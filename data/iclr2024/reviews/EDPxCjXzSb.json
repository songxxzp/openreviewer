[
    {
        "id": "1jh3IV7b9O",
        "forum": "EDPxCjXzSb",
        "replyto": "EDPxCjXzSb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5738/Reviewer_Tg9t"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5738/Reviewer_Tg9t"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a training-free framework for zero-shot image retrieval, by leveraging the pretrained vision-language model and large language model. VLM is used to obtain image caption for query image, and the caption together with the text modifier is fed into LLM to get the target caption; then the target caption is used to do cross-modal retrieval with the help of VLM in database images. This framework also allows scalability and human intervention to improve the retrieval performance due to its modularity and human-understandability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed framework is training-free and achieves promising retrieval results.\n2. The retrieval accuracy could be improved by adopting better off-the-shelf VLM and LLM, due to the modularity of the framework.\n3. Since the query is mainly processed in language domain, the framework is human-interpretable and could also improve the retrieval accuracy by involving human intervention."
            },
            "weaknesses": {
                "value": "1.\tThe method itself is very intuitive and not quite novel, which is more like an engineering extension of existing models like VLM and LLM. Additionally, the proposed method does not achieve better results on all evaluation datasets. In Table 1, CIReVL underperforms SEARLE on CIRR dataset in terms of CIRR, which shows the unstability. \n2.\tThis work only adopts datasets of everyday life and natural scenes (CIRR, CIRCO, GeneCIS) as evaluation. It is necessary to include datasets of various domains, such as fashion domain datasets like fashioniq and fashion200k to evaluate the generalization ability. Furthermore, for GeneCIS dataset, the comparison against other ZSCIR methods such as Pic2word and SEARLE should be included for complete comparison. \n3.\tThe proposed framework is not very time-efficient since for each query, there will be two auto-regressive processes: query image captioning and target caption generation, which are very time-consuming. Both processes involve large models like VLM and LLM, which may further lead to low efficiency for image search. Therefore, it is necessary to compare the retrieval efficiency (time) with the previous methods."
            },
            "questions": {
                "value": "The same as weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5738/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698738490187,
        "cdate": 1698738490187,
        "tmdate": 1699636601270,
        "mdate": 1699636601270,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "r3DhoH5bBg",
        "forum": "EDPxCjXzSb",
        "replyto": "EDPxCjXzSb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5738/Reviewer_JtMi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5738/Reviewer_JtMi"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces CIReVL, a training-free pipeline for zero-shot compositional image retrieval by combining existing off-the-shelf foundation models, e.g., BLIP-2 for image captioning, GPT for text editing and CLIP for image retrieval. In addition, the specific and explicit description with captions, instead of text embeddings, facilitates human understanding over the retrieval process. Extensive experiments are conducted to validate the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe paper effectively integrates the VLM and LLM for ZS-CIR, offering a flexible and intervenable CIR system.\n2.\tThe authors have conducted thorough experiments to validate the effectiveness of the proposed method, making the evaluation rather reliable.\n3.\tThe figures in this paper are clear, effectively conveying the processing pipeline."
            },
            "weaknesses": {
                "value": "1.\tLimited contribution: The proposed method appears to be a combination of several foundation models and simply employs the basic modelling capacity of these models (e.g., BLIP-2 for image captioning, GPT for text editing and CLIP for image retrieval), presenting a na\u00efve and straightforward solution for CIR task. As a result, it is challenging to identify insightful and significant contributions to this field. The authors should conduct further in-depth research to enhance their contributions.\n2.\tIncorrect experimental results: The Recall@10 of CIReVL (ViT-G/14\u2217) on CIRR dataset in Table 1 is obviously incorrect. \n3.\tInconsistent formatting of table data: In some cases, the data is presented with two decimal places, while in others, only one decimal place is used. Furthermore, there is a case with a mix of formatting in Table 3. The authors may revise the paper more carefully.\n4.\tThe paper effectively combines the existing powerful VLM and LLM models. However, it would be better to provide a more insightful analysis of the caption and reason processes. For example, from Table 3, compared to utilizing different LLM models, the use of various state-of-the-art (SOTA) captioning models has a relatively minor impact on retrieval. Are there any potential explanations for this phenomenon? Does this mean the choice of caption model is not strict, as long as the model can catch the main object or attribute of images? I think it would be better to provide more analysis. \n5.\tThe description about other works in Section 3.1 is a bit obscure for me who is not so familiar with the task but proficient other related tasks.\n6.\tMissing references, such as in \u2018Similar to existing ZS-CIR methods\u2019 in the first paragraph of Section 3.2.\n7.\tExperiments: (a) In Table 1, the author only presents the experimental results of \u2018image only\u2019 and \u2018image+text\u2019 methods for reference while missing that of the \u2018text only\u2019 method under the ViT-B/32 setting. However, the authors mention that the results on CIRR benchmark are primarily dependent on the modifying instruction while the actual reference image has much less relation to the target image, which indicates that the \u2018text-only\u2019 method can be an important reference for measuring the performance of the proposed method. Better to present the result of \u2018text-only\u2019 method and make a fair comparison and discussion. (b) In Table 1, the authors miss the results of \u2018image only\u2019, \u2018text only\u2019 and \u2018image+text\u2019 method for ViT-L/14 and ViT-G/14 settings. Better to include them for reference. (c). When evaluating on the GeneCIS benchmark, the authors do not specify the architecture of the vision backbone adopted in the experiment. Better to specify it clearly for reference.\n8.\tDiscussions and evaluation on the potential limitations: The paper shows that the proposed method has several merits including free of training, good flexibility and scalability. However, it may also have some potential limitations. For example, (a) Inference costs and efficiency: The proposed method utilizes large VLMs and LLMs to conduct the image captioning, language reasoning and cross-modal retrieval during inference. Will it take more computational costs and have longer inference time compared with the previous methods? (b) Limitations of each module: Since the proposed method is composed of three different modules, the effect of each module plays an important role on the final retrieval results. For example, if the image caption module generates partial or false descriptions for the given images, the reasoning and retrieval process will be misled. Thus, it is necessary to analyze the potential negative impact brought by each module in an in-depth manner and quantify them if possible (which factor contributes more to the failure cases). (c) Compatibility of different modules: Since the proposed method conducts cross-modal retrieval by cascading three separate modules, the compatibility of different modules seems to be an important factor. For example, if the captions generated by LLMs have different styles with pretraining data of the VLMs, the VLMs used for cross-modal retrieval may produce some bias, which may hinder the final performance. Overall, it will be appreciated if the authors can present more in-depth discussions and evaluation on the potential limitations to fully demonstrate the properties of the proposed method.\n9.\tWriting: Some parts of the paper don\u2019t flow well. The overall writing requires further improvement."
            },
            "questions": {
                "value": "1.\tThis paper seems to be a technical report with the application of current state-of-the-art foundation models. Although directly using existing models is convenient, it inevitably introduces errors in each processing step. How to address the cumulative errors resulting from these multiple models?\n2.\tCould the authors provide experimental results of CIR works with a supervised training pattern? Based on the current results in this paper, the overall zero-shot retrieval performance appears to be poor. Do those supervised methods also yield unsatisfactory performance? If so, it seems that more significant efforts are needed to address the fundamental issues in CIR task; If not, does the zero-shot approach still have its value?\n3.\tHow you get the 6 curated samples for the Rs @K metric in Table 1?\n4.\tFor reasoning, the experiments show LLM models obviously affect the performance. I wonder whether the same is true for the prompt template. Except for the prompt mentioned in Appendix A, were there any alternative prompts explored during the experimentation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5738/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5738/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5738/Reviewer_JtMi"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5738/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698748042808,
        "cdate": 1698748042808,
        "tmdate": 1699636601162,
        "mdate": 1699636601162,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kKiMeQX6wj",
        "forum": "EDPxCjXzSb",
        "replyto": "EDPxCjXzSb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5738/Reviewer_QaMw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5738/Reviewer_QaMw"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to achieve training-free zero-shot compositional image retrieval by using off-the-shelf models pretrained on large-scale datasets. The authors use language as an abstraction layer for reasoning about visual content. Particularly, it uses off-the-shelf vision-language models like BLIP-2 or CoCa to generate a detailed description of the query image. Next, an LLM or GPT-like model combines the input image description (from BLIP-2) and an input textual query (e.g., \"shows only one of them, which is bigger and is next to a trash can\") to generate a caption for the desired target image. Finally, a vision language model like CLIP performs text-to-image retrieval using the generated caption from LLM and images from an image database. This modular approach ushers a few benefits like: (i) the resulting approach is training-free and being modular, we can flexibly plug-and-play individual components like replace the LLM with either GPT or Llama, replace the VLM with BLIP-2 or CoCa; (ii) the composition happens in the human interpretable language domain offering human interpretability; and (iii) it is possible for humans to intervene on the retrieval process to fix or post-hoc improvements in retrieval results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "While there are a lot of works on compositional image retrieval, previous works typically require training several components like textual inversion and lack human interpretability. The method proposed in this paper avoids all these problems by simply composing image and textual query in the language domain.\n\nOn the surface this is ingenious because you do it in a modular way and each module is a highly-generalisable large-scale pre-trained model. For example, when training a textual inversion, we do not really guarantee it scales to open-set setups (given we train them in limited data and compute). We just use a small-scale network and hope the rest of powerful models take care of it. The proposed method works zero-shot and you know it works for open-set setups.\n\nAdditionally, unlike prior trainable compositional image retrievals, there is a lot of effort that goes into interpretability -- \"how was the image and query text was composed\". Since the proposed method does this composition entirely in the language space, you know exactly what information was extracted from the image and you can see the generated caption from LLM for the desired target image. This not only makes the retrieval process highly transparent, but also allows post-hoc edits -- you can literally make changes to adjust your retrieval results.\n\nI encourage more works that reuses as much as large-scale models and combines them as modules with minimal or training-free way."
            },
            "weaknesses": {
                "value": "Despite its appeal, there are a few important drawbacks. This entire process sticks on the underlying assumption that our image captioning module (e.g., BLIP-2 or CoCa) can provide a \"detailed caption\" that captures all information.\n\nThis, in my opinion, is a strong assumption. A lot depends on your captioning module. While the captioning module may be super accurate, it can miss some \"less important\" details that I want to change. For example, given a photo, the sky was orange and my textual query is \"make the colour of the sky darker\". If the image captioning module omits the colour of sky (i.e., orange) and focuses on the foreground (e.g., a person holding a flower, sitting in a bench near a park where kids are playing) -- the rest of the modules have no way of combining my textual query \"make the colour of sky darker\".\n\nThis is an example, where the entire pipeline fails -- due to no fault of the image captioning module. It provided a detailed image description, but it is not possible to describe every little \"unimportant things\".\n\nLooking at the same problem from a different direction, is the information bandwidth of an image the same as language? (an image can be worth a thousand words)"
            },
            "questions": {
                "value": "I am confused, how is the proposed method different from VisProg (https://arxiv.org/pdf/2211.11559.pdf)?\nIt feels I can get the same benefits of the proposed method by using the VisProg paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5738/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5738/Reviewer_QaMw",
                    "ICLR.cc/2024/Conference/Submission5738/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5738/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698924173268,
        "cdate": 1698924173268,
        "tmdate": 1700736453209,
        "mdate": 1700736453209,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xHnx7C2jfO",
        "forum": "EDPxCjXzSb",
        "replyto": "EDPxCjXzSb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5738/Reviewer_W9gL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5738/Reviewer_W9gL"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a training-free solution for addressing the compositional image retrieval problem. The idea is quite simple: i) take the pre-trained VLM model to describe what the input image contains, ii) use an LLM to combine the description/caption of the input image with the desired modifications as described by the user query, finally, iii) use the resulting text to generate VLM (i.e., CLIP) representations and search the target image database via knn search. The solution is evaluated against multiple baselines (PALAVRA, Pic2Word, SEARLE) and datasets (CIRCO, CIRR, GeneCIS) and provides better results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is technically sound and widely applicable not only for text-image use case, but also text-video and probably other problem domains. \n- The paper investigates three different datasets against multiple baselines and various additional ablations make the paper and results more appealing. Limitations of the proposed method also gets discussed with examples. \n- The paper is easy to follow / well-written."
            },
            "weaknesses": {
                "value": "- [Novelty & Literature review] There are a wider number of papers in the field since last year and some of those papers requires mention (at least the ones before the ICLR submission deadline). The most similar paper ((https://arxiv.org/pdf/2310.05473.pdf) proposes the same approach but with an additional training method for merging the automatically generated image caption and the desired modification's text description. This paper contains additional papers/baselines to cite/include. It would be great if there is a way to understand whether the proposed training-free method would perform better or worse compared to the paper mentioned above.\n\n- [Experiments] Baseline PALAVRA attack different set of problems, uses different datasets and metrics. It does not mention CIR task in the text. Not clear if PALAVRA serves as a baseline."
            },
            "questions": {
                "value": "- A short discussion about extensibility / applicability of the proposed approach could be beneficial for the research community. \n- FashionIQ is yet another dataset which other papers use for CIR evaluations, it might be worth considering it in the future. \n- Is there a way to detect when the LLM fails to provide a meaningful / valid re-composed text ? How much room left to improve this step? What is the future research direction?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5738/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5738/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5738/Reviewer_W9gL"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5738/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699048763716,
        "cdate": 1699048763716,
        "tmdate": 1700695941883,
        "mdate": 1700695941883,
        "license": "CC BY 4.0",
        "version": 2
    }
]