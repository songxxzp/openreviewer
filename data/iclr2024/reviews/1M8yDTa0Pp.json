[
    {
        "id": "V1ZkWQzAVI",
        "forum": "1M8yDTa0Pp",
        "replyto": "1M8yDTa0Pp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3945/Reviewer_xki8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3945/Reviewer_xki8"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on learning continuous soft prompts for adapting pre-trained vision-language models in a semi-supervised setting. To make the learned prompts invariant to the different views of a given unlabeled sample, the authors propose a new scheme, i.e., varying the lengths of visual and text prompts attached to these samples. The results show that the proposed method makes an immense gain in image classification performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper is well-written and easy to follow.\n\n-  To the best of my knowledge, the paper studied an under-explored problem, semi-supervised prompt learning for VLMs.\n\n- Considering the two branches of VLM, the authors propose to learn multi-modal prompts and vary the lengths of visual and text prompts attached to obtain samples with different views.\n\n- The experiments show the superior effectiveness of XPL compared to the designed baselines."
            },
            "weaknesses": {
                "value": "My concerns are summarized below:\n\n- The authors illustrate the motivation of XPL with Figure 1(a,b), i.e., a large category-wise performance gap between two models leveraging different numbers of learnable prompts. I am curious about whether the results in Figure 1(a,b) are averaged over several random seeds. In the prompt learning, the random seed has a non-negligible effect on the performance.\n\n- For a more comprehensive comparison, I suggest that the authors add the results of zero-shot CLIP. Besides, these prompt engineering methods (e.g., prompt ensemble) and more advanced prompt learning methods on labeled data should be compared."
            },
            "questions": {
                "value": "- Whether the results in Figure 1(a,b) are averaged over several random seeds?\n- The results of zero-shot CLIP, prompt engineering methods, and more advanced prompt learning methods should be included."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3945/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3945/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3945/Reviewer_xki8"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3945/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698673910918,
        "cdate": 1698673910918,
        "tmdate": 1699636355228,
        "mdate": 1699636355228,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RJ3NLUsjwP",
        "forum": "1M8yDTa0Pp",
        "replyto": "1M8yDTa0Pp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3945/Reviewer_18u3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3945/Reviewer_18u3"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a novel framework for semi-supervised prompt learning tailored for VLMs. Moreover, it showcases the remarkable efficacy of the proposed method on the commonly used 15 datasets. To achieve this, this work is designed with two innovations: i)  the mutual knowledge distillation from the VLMs with different prompt lengths, and ii) the combination and consistency of weak and strong augmentation strategies. In my understanding, this work is actually based on existing basic methods, such as dual student architecture that existed in semi-supervised learning, and the combination of weak and hard augmentation in contrastive learning methods.\nThe advantage of this work is it is the first work to investigate the semi-supervised learning in the VLMs. Moreover, it reveals the importance of cross-model distillation with different prompt lengths, which is interesting. It is also simple and effective on most datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The strengths of this work are as:\n\ni) This work is the first to investigate the semi-supervised learning for the efficient transfer learning (ETL) of VLMs. \n\nii)  The framework of XPL is simple but effective, which showcases great performances on 15 typical datasets. \n\niii) The mutual learning of the models with different prompt lengths looks interesting."
            },
            "weaknesses": {
                "value": "There are a series of issuses that is required to be solved.\n\ni) I noticed that this work only validated their methods on some simple benchmarks, such as CoOp, and VPT. Is it possible to give more experiments to validate the applicability of your methods on recent works on prompt learning-based few-shot learning?\n\nii) This work lacks the theoretical analysis for why the different prompt lengths is better.  \n\niii) The clarification for the utilization of unlabeled data in \"TPL^u\", \"MPL^u\" is not clear. \n\niv) In Figure 18 of the supplementary, why the MPL is lower than MPL^u in most datasets? \n\nv) Is it possible to provide the comparison for the 10\\% - 50\\% labeled data?\n\nvi) The ablation studies on hyper-parameters are only conducted with EuroSAT is not reasonable, since the randomness in few-shot learning."
            },
            "questions": {
                "value": "The author is expected to increase more through explanations for their methods, and the experiments listed in the weakness. Moreover, the contribution of this work should be further clarified, especially the difference or significance of their methods with existing basic methods, such as weaker and hard augmentation in contrastive learning and the model distillation with dual students architecture."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3945/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807879668,
        "cdate": 1698807879668,
        "tmdate": 1699636355154,
        "mdate": 1699636355154,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Q8CqB2f3E7",
        "forum": "1M8yDTa0Pp",
        "replyto": "1M8yDTa0Pp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3945/Reviewer_MsJj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3945/Reviewer_MsJj"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a semi-supervised, cross-model prompt learning for vision-language models (VLMs). The key idea of the paper relies on feeding different lengths of soft prompts to two pre-trained VLMs (referred to as primary and auxiliary networks). Given an unlabeled image, the authors create a pair of weakly and strongly augmented versions, pass them to the two networks, and use the confident prediction from one network as a pseudo-label for the other and vice versa. Experimental comparisons with different baselines are presented on several benchmark datasets. The supplementary material contains additional experimental analyses, visualizations and source code."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "+ The paper tackles an interesting and timely topic. With the recent progress made in training powerful VLM models, it is worth studying how to prompt these pre-trained models for different downstream tasks\n+ The paper reads fairly well\n+ Source code is shared in the supplementary for reproducibility"
            },
            "weaknesses": {
                "value": "Major issues \n\n* The work is very incremental\n\n    The technical novelty of this work is quite limited. The idea of passing a set of augmented versions of unlabeled image data in two different networks and constraining the networks to supervise each other has been explored in several existing works (ex. [1,2]). Furthermore, the idea of deriving visual prompts directly from text prompts using a linear projection (coupling function) has been introduced in previous works (ex. [4]) which the authors fail to cite and discuss. \n\n* Several claims/motivations in the paper are not convincingly justified\n\n    The main claim of this paper is to use prompts of different lengths for the primary and auxiliary networks. However, the authors fail to give a convincing argument as to why that should lead to a better performance. For instance, how did the authors arrive at using N and N/2 long prompts for the primary and auxiliary networks, respectively? why not N and N/3 or N and N/4? \n\n     The ablation experiment presented in Fig 7 of the main paper is counterintuitive to the key message of the paper. On page 5, the authors claim, \"*As the two models with different lengths differ in what they learn, they can complement in generating the supervision of each other*\". However, the results in Fig 7 show that this is not true. For instance, using a model with the same prompt length for the two networks (N=8 or N=16) outperforms a model with different prompt lengths (N=8 for primary and N=4 for auxiliary). If different prompt length is indeed as important as the authors argue, how do they explain these results? \n\n     It can also be noticed from Fig 7 that a model with N=32 for primary and N=8 for auxiliary performs inferior to the baseline model (N=16 for primary and N=8 for auxiliary). Does this mean that if the length difference between the two networks increases, performance gets worse? Where is the threshold for this performance trade-off? This needs a rigorous justification as the merit of the paper heavily relies on this argument.  \n\n    Moreover, the performances of some of the baselines in Fig 7 are worse than the multimodal prompt learning (MPL) baseline. This raises the question of whether the proposed cross-modal approach is indeed better than MPL given its sensitivity to the prompt length.\n  \n* The experimental settings and comparisons are not clearly presented\n\n    There have been several works related to prompt learning in text, vision, or multimodal domains. However, the author's comparison fails to cite most of these works except for CoOP. Which VPL or MPL baselines are used in the paper? Did the authors design their own baselines or use previous works as a baseline (but forgot to cite properly)? What are the experimental settings for these baselines? Why didn't the authors compare with recent works such as Co-CoOP [3]  or MaPLe [4]?\n\n   While I appreciate the extensive comparisons on several datasets, the presented results (quantitative figures) are almost unreadable due to the very small size of the figures. It would be better to either draw bigger figures or use tables instead of figures for a clearer presentation of the experimental results.\n\n* More ablation experiments are needed to justify the merit of the work\n\n    A simple experiment to show the benefit of the cross-model approach would be to use a single model and use the prediction of the weakly augmented input as a pseudo-label for the prediction of the strongly augmented input. However, such a baseline is missing in the paper.\n\n   It is also important to further explore what makes the cross-model approach based on one primary and one auxiliary network work. What if we use one primary and two auxiliary networks with a triplet of augmented inputs (1 weak and 2 strong - one for each auxiliary network) and each auxiliary network supervises the primary network and vice versa?  Does this lead to better supervision of the primary network? These explorations would be important to strengthen this work.\n\nMinor issues\n\n* In the supervised training, why did the authors choose to use only the weakly augmented image?\n\n* The paper needs some re-organizations. Some of the results presented in the supplementary (ex. Appendix B, D, E) should be in the main paper. \n\nReferences\n\n[1] Cross-Model Pseudo-Labeling for Semi-Supervised Action Recognition, CVPR 2022\n\n[2] Semi-Supervised Semantic Segmentation with Cross-Consistency Training, CVPR 2021\n\n[3] Conditional prompt learning for vision-language models, CVPR 2022\n\n[4] MaPLe: Multi-modal Prompt Learning, CVPR 2023"
            },
            "questions": {
                "value": "Please refer to the questions raised in the \"Weaknesses\" section and try to address them carefully."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3945/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3945/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3945/Reviewer_MsJj"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3945/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699033429786,
        "cdate": 1699033429786,
        "tmdate": 1699636355086,
        "mdate": 1699636355086,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2PA6LMyOFy",
        "forum": "1M8yDTa0Pp",
        "replyto": "1M8yDTa0Pp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3945/Reviewer_f2tm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3945/Reviewer_f2tm"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Cross-model Prompt Learning (XPL), a semi-supervised approach for prompt learning in Vision-Language Models (VLMs), aiming to reduce the dependency on large labeled datasets. XPL employs dual pathways with variable soft prompt lengths to utilize unlabeled data for enhancing model performance in low-labeled-data regimes. The approach is validated on 15 datasets, showing that XPL outperforms the supervised baseline, particularly in few-shot classification tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The method's ability to leverage unlabeled data effectively could substantially reduce the need for large labeled datasets.\n+ The approach is empirically validated across 15 diverse datasets, demonstrating its effectiveness and robustness in various contexts."
            },
            "weaknesses": {
                "value": "- The approach primarily extends semi-supervised learning (SSL) principles to prompt learning with minimal adaptation, which may suggest that the level of novelty is somewhat constrained.\n- There is a concern regarding the robustness of the learned prompts, as they appear to be highly sensitive to the distribution of the dataset, potentially limiting their applicability in diverse real-world scenarios."
            },
            "questions": {
                "value": "1. How does the proposed method differ fundamentally from existing SSL applications in prompt learning, and what specific adaptations have been made to tailor this approach to VLMs?\n2. Can the authors provide more insight into how the method would perform on out-of-distribution data or datasets with different characteristics than those tested?\n3. What measures have been taken to ensure that the learned prompts are not overly fitted to the specific datasets used in the experiments?\n4. Conduct additional experiments on out-of-distribution datasets or through domain adaptation challenges to evaluate the robustness and generalizability of the learned prompts."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3945/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699376325523,
        "cdate": 1699376325523,
        "tmdate": 1699636355022,
        "mdate": 1699636355022,
        "license": "CC BY 4.0",
        "version": 2
    }
]