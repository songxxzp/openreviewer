[
    {
        "id": "D3JNnEZSMm",
        "forum": "JXm3QYlNPn",
        "replyto": "JXm3QYlNPn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1199/Reviewer_TRJs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1199/Reviewer_TRJs"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces SignRound, a lightweight and effective approach for optimizing weight rounding in Large Language Models (LLMs) with 3 and 4-bit weight-only quantization. SignRound leverages signed gradient descent and achieves remarkable results in just 400 steps, competing favourably with recent methods. Experiments on several datasets demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well written and easy to follow. SignRound achieves significant results within only 400 steps, showcasing its efficiency."
            },
            "weaknesses": {
                "value": "The paper's novelty is limited, largely revisiting concepts like learning weight rounding previously introduced by AdaRound. It overlooks in-depth comparisons with established methods like AdaRound and FlexRound. Please see questions for details."
            },
            "questions": {
                "value": "1.\tThe paper's novelty is somewhat circumscribed, given its central focus on learning weight rounding\u2014a concept previously introduced by AdaRound (Nagel et al., 2020). While SignRound's utilization of the signed gradient to refine the rounding function distinguishes it, the motivation behind prioritizing only the gradient direction (ignoring gradient magnitudes) remains ambiguous. \n\n2.\tThe authors appear to have not fully accounted for certain pivotal baselines in their study. Specifically, AdaRound (Nagel et al., 2020) and FlexRound (Lee et al., 2023) stand out as established methods that delve into the realm of learning weight rounding. It would enhance the paper's comprehensiveness and comparative analysis if these methodologies were discussed and compared with SignRound.\n\n3.\tThe performance comparisons between GPTQ and the proposed method are unfair since act-order was not enabled. This omission potentially skews the results, leading to instances where GPTQ underperforms compared to rounding-to-nearest (RTN) in some cases (W4 in Table 1).\n\n4.\tGPTQ, by utilizing second-order information, offers an efficient solution to the weight quantization problem. It would benefit for the authors to provide a clearer distinction of the advantages offered by the proposed method, especially when there are instances where it lags behind GPTQ, as evidenced in Tables 3, 4, and 5. Additionally, omitting a comparative analysis on training time introduces ambiguity, making it challenging to ascertain the relative efficiency of the two methods.\n\n5.\tReferring to Table 4, it is noteworthy that the proposed method underperforms compared to RTN for both W4G128 LLaMA-7B and W3G128 LLaMA-7B configurations. A more in-depth exploration or justification for this discrepancy would enhance the paper's clarity. \n\n6.\tThe comparisons presented in Table 8 between AWQ and the proposed method appear unfair due to disparities in the calibration datasets. For a comprehensive assessment of the proposed method's efficacy, it would be better for the authors to provide fair comparisons."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1199/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698570052439,
        "cdate": 1698570052439,
        "tmdate": 1699636046242,
        "mdate": 1699636046242,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xVNXryt6uK",
        "forum": "JXm3QYlNPn",
        "replyto": "JXm3QYlNPn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1199/Reviewer_yH6X"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1199/Reviewer_yH6X"
        ],
        "content": {
            "summary": {
                "value": "For better weight-only LLM quantization, this work proposes to optimize the rounding of weights with the block-wise reconstruction error as the objective. Following previous smart rounding work such as AdaRound, it optimizes continuous variables that will be added onto the scaled weights before rounding. This work also emphasizes the need to use \"signed gradient descent\" in the rounding variable optimization, which only exploits the gradient direction instead of the magnitude. This work conducted experiments on quantizing LLaMA v1, v2, BLOOM, and OPT models to W3 and W4 with different group sizes."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* Motivation: How to make weight-only LLM quantization work for fewer bits is worth studying, especially for edge scenarios with very limited memory.\n* Reasonable pathway: Applying smart-rounding PTQ methods is a reasonable pathway.\n* The experiments are conducted with different model families.\n* The paper is easy to understand."
            },
            "weaknesses": {
                "value": "* Unclear logic of applying signed gradient descent: The paper said \"prefer the signed gradient descent method to tackle the issue of sub-optimal rounding\" without intuitive logic description, theoretical justification, or experimental verification of this technique.\n* Marginal improvements: The method shows improvements on relatively smaller models, but on larger models (>7B), it achieves marginal improvements over GPTQ or RTN."
            },
            "questions": {
                "value": "See the weakness part"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1199/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698674570587,
        "cdate": 1698674570587,
        "tmdate": 1699636046171,
        "mdate": 1699636046171,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "I4EqaziHwc",
        "forum": "JXm3QYlNPn",
        "replyto": "JXm3QYlNPn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1199/Reviewer_GzCp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1199/Reviewer_GzCp"
        ],
        "content": {
            "summary": {
                "value": "This work develops a signed gradient decent for the quantization of large language model weights. It compares against other approaches that are more complicated algorithmically and achieve slightly better results.\n\nRecommendation: This is a solid contribution that I would rather see accepted than rejected."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- simple approach will make it easier to develop more complicated methods. This is a large advantage over GPTQ which is a good foundation for future methods.\n- evaluation is quite extensive, leaving little doubt that the method works well\n- shows that the Hessian approach from GPTQ does not add too much unique value, but mostly reduces the samples needed for good performance. This is a very valuable insight that will quite future quantization work."
            },
            "weaknesses": {
                "value": "- while the simplicity is an advantage of this method, it can also be seen as a disadvantage. However, I would like to highlight for the AC and reviewers that the main goal of the paper is to simplify a complicated algorithm (GPTQ), and the authors succeed\n- not competitive with other more extensive methods. However, other methods cannot be used as a base optimization method for finding quantization. As such, this approach is more useful for future work"
            },
            "questions": {
                "value": "Comments:\n - before equation 6, the equation is missing an \"s\"\n\nQuestions:\n- why is there such high C4 perplexity for transformer block quantization for W3G128? Do the numbers indicate some form of instability during sign SGD?\n- Why is the runtime so slow? Are you building on the GPTQ codebase?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1199/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698713367404,
        "cdate": 1698713367404,
        "tmdate": 1699636046099,
        "mdate": 1699636046099,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3xwNh0Jgj2",
        "forum": "JXm3QYlNPn",
        "replyto": "JXm3QYlNPn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1199/Reviewer_vB5y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1199/Reviewer_vB5y"
        ],
        "content": {
            "summary": {
                "value": "* The paper proposes to optimize the layer-wise rounding problem that occurs for LLM PTQ using signed gradient descent.\n* The method is evaluated across various models and tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper is easy to follow.\n* The paper conducts a large number of experiments across various models, tasks and quantization setting. Further, it also consider state-of-the-art LLMs like Llama2 in addition to older ones like OPT and BLOOM.\n* SignRound appears to bring some performance improvements relative to GPTQ, in particular on smaller models and for zero-shot tasks.\n* I also like that the paper includes also handful of unfavorable results to provide a more complete."
            },
            "weaknesses": {
                "value": "* The paper essentially seems to apply signed gradient descent (which is not new) to the standard layer-/block-wise rounding problem considered by various LLM PTQ papers. Hence, the overall novelty is low.\n* GPTQ Activation-reordering can also be performed without any impact on inference performance (see the official GPTQ repo, option `--static-groups`). Further, if there is no grouping, reordering has no impact on inference. LLaMa1-7B and OPT-66B are known to be GPTQ outliers, for which reordering should be enabled to conduct a fair comparison.\n* The paper argues that signed gradient descent is preferable over standard straight-through QAT (applied to the layer-wise quantization problem, like ZeroQuant) for this application, but does not provide any ablation studies supporting that point.\n* Based on Table 5, it appears that for the largest and most interesting models for compression applications, SignRound seems to perform very similar to GPTQ and in some cases even worse.\n* The code is not available in the Supplementary material.\n\nOverall, I am currently leaning towards rejection as the novelty is rather low and the experimental results not quite strong enough (and with some problems/questions) to make up for it."
            },
            "questions": {
                "value": "* Is GPTQ using the same amount of samples as SignRound in your comparisons?\n* Do you have any explanation for the surprisingly poor perplexity performance on Llama\" models in Table 3? Related to that, how comes that those models still exhibit comparable or better ZeroShot performance.\n* The runtime comparisons in Table 12 stop at 13B size, how long does your method take for the largest models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1199/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1199/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1199/Reviewer_vB5y"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1199/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698780933611,
        "cdate": 1698780933611,
        "tmdate": 1699636046025,
        "mdate": 1699636046025,
        "license": "CC BY 4.0",
        "version": 2
    }
]