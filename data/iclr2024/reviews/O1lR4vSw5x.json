[
    {
        "id": "rxAn9OrjnI",
        "forum": "O1lR4vSw5x",
        "replyto": "O1lR4vSw5x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6444/Reviewer_QsPz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6444/Reviewer_QsPz"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a Newtonian optimisation for ODEs where some state dimensions are not measured, and where some state dimension differential are not known. The method can fit simple partially observed ODE systems exactly."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The performance is top notch\n- The method is generally convincingly presented"
            },
            "weaknesses": {
                "value": "- The presentation is a bit confusing at times, and open problem and contributions are not clear. The method contains Hessians and parameters processes that are eventually not used, convoluting the model. Some notation and math is confusing. Main claim is not substantiated (vanishing adjoints).\n- There is no comparison to baselines, which would be standard neural ODE loss, or towards multiple shooting layers (Massaroli et al). Comparison to some SINDy method would be good as well. There is no running time analysis\n- There is no real-world experiment: all demonstrations are simple toy cases with no unknown parts. The paper should include at least one case where the underlying process is not fully known or too messy or difficult (eg. motion capture, biochemistry, etc)."
            },
            "questions": {
                "value": "Minor comments\n\n- It\u2019s odd to call the dx as \u201cdiffusion\u201d: it\u2019s not diffusion, but a differential, or just an ODE. I wonder what the x and theta represents in this work.\n- I\u2019m not sure I get where the def 3.1. is coming from, or why its true. There is no citation, or proof. Naively of course we can\u2019t distinguish states that emit the same observation, but we could also have a situation where one state is going up, one is going down, and they cross at one point. Even if they are identical at one point, does not mean that we can\u2019t distinguish them based on the surrounding information, or from the slopes. Similarly, one could have a cyclical system that visits same state many times (eg. VDP). There again having same output from two states is fine.\n- Suddenly in 3.3. the theta is network parameters. I\u2019m a bit confused by this. I thought the purpose of the parameter process was that the system is non-stationary over time. Is this still accurate?\n- I don\u2019t understand how an un-measurable state makes adjoints zero. Surely if we have a partially observed system, the neural ODE will happily fit the trajectory with the known states as well as it can. For instance, if we have a 3D lorentz attractor where we only know about the first 2 dimensions, we can easily still fit an observed 2D trajectory without vanishing if we include evolving parameters or time-dependent system. The paper needs to demonstrate its claims by proofs or citations, and preferably also give a proof-of-concept toy illustration of the effect as well.\n- The notation in eq 5 is misleading: why do we parameterise by u(t_i-1) but not by u(t)? Surely we need to know the u, x and theta for all arbitrary timepoints between t_i and t_i-1. The ODEsolve only refers to x, while you also need to solve for theta\n- I\u2019m confused by eq 8. I\u2019m not sure what is your model here. I would assume that you have a neural network that represents dtheta and dx. But neural network parameters are nowhere to be seen. Is \\theta neural network parameters?\n- The presentation of the piecewise optimisation is quite confusing. I don\u2019t get where the Hessians are coming from, or what\u2019s their point in the first place. Apparently h doesn\u2019t have parameters, so it has to be known (but if states are not fully observed, how can you know h?). I don\u2019t get the joint vanishing statements. I can\u2019t follow the eqs 10 and 11. It seems that you just set the x and theta to the ODE solutions. Ok, so you are just an ODE solve here. Why do we then need the eqs 10 and 11 at all?\n- When you say that you optimise for x(t_i), what does that mean? Do optimise for the realisation of x(t_i), or its parameters? Btw. x(t_i) is not dependent on parameters notationally, which is very confusing. One would expect instead x(t_i | x_0, params).\n- I don\u2019t see how eqs 14 and 16 are any different from standard neural ODE loss of eq 3. To me this is a reordering of the original simple loss into a more complicated version, which still looks like the same thing. The eqs 17-19 seem to be just gradient updates with Hessians. Ok, but can\u2019t we directly use a second-order optimiser for the standard neural ODE loss? What is the point of all of this? How does this help us reveal the unknown dimensions of x?\n- What does \u201cknown\u201d mean in thm 4.1? Is it the value of the true underlying system?\n- If all experiments have g=0, why include it in the model at all?\n- The experiments need to compare to standard neural ODE loss as reference, with both known or unknown initial value.\n- The paper is missing citation to DMSL (Massaroli et al 2021). This is also a Newtonian optimiser, so one should directly compare against it, and also discuss how it differs.\n- In experiments the Hessians (eg. Q) are just diagonal. What was the point of this if you don\u2019t effectively use them?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6444/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698668507198,
        "cdate": 1698668507198,
        "tmdate": 1699636719456,
        "mdate": 1699636719456,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GEPK2ml9UV",
        "forum": "O1lR4vSw5x",
        "replyto": "O1lR4vSw5x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6444/Reviewer_gkPz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6444/Reviewer_gkPz"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a  framework for learning dynamics of partially observed ODE systems from observations based on the Neural ODEs (NODEs) formalism. They exploit the relationship between recursive-state space estimation procedures and Newton\u2019s method to establish a sequential framework to learn latent states and model parameters.\n\nTo overcome the infeasibility of the optimisation of latent parameters $\\theta$ and latent states $x$ when considering an optimisation cost that considers all observations (time steps) in one run, they employ a sequential/online optimisation procedure, where the optimisation cost at each time step is expressed recursively, in terms of the optimisation cost of the previous time step.\n\nAssuming distinguishable latent states, they decouple the sequential optimisation for latent states and parameters, and propose alternating Newton updates to estimate the respective parameters/states at each time step.\nThey demonstrate the performance of the proposed framework on a battery of model systems, where they assume that part of the model dynamics is unknown.\n\nThe main contribution of the paper is the sequential formulation of the optimisation of latent parameters and states outlined in detail in Theorem 1 in the Supplement/Appendix."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper proposes a sequential optimisation for latent states and variables of partially unknown ODE systems that is new to my knowledge.\n- They demonstrate the accuracy of their method on several model systems."
            },
            "weaknesses": {
                "value": "- In the presentation of the modeling assumptions of their framework (Section 3.1) the authors mention that they assume a latent diffusion model and Markovian evolution of the model parameters ($\\theta(t)$), however in the equations they set out  in the same section and in their numerical experiments there is no stochastic term considered. IN Section 3.4 (Eq.7) they indeed consider a deterministic system with  and I assume they mention Markovian dynamics to indicate that each state depends only on the state of the previous time step, but to my knowledge Markovianity also implies stochasticity which is not apparent here. But I am open to be corrected.\nThis is quite confusing/misleading to me, unless the authors clarify their assumptions.\n\n- The framework requires the latent states ($x$) of the system to be distinguishable, uniquely identifiable from the observations given a certain control input. However, I find this condition quite limiting for realistic applications, and I wonder whether this is a hard requirement, or whether the method could still perform quite well in settings with partially indistinguishable latent states."
            },
            "questions": {
                "value": "- The authors mention in their introduction (end of second paragraph) that \u201c[hybrid first-principles data-driven models] focus on state estimation using data-driven components to improve or augment existing dynamics but fail to learn global models and do not scale for large parameterized models. \u201c. Can you provide some evidence or reference for these claims?\n\n- The authors describe the process that governs the evolution of the parameters $\\theta(t)$ and latent state $x$ as a Markov/diffusion process, but they nevertheless model it with an ODE. As I understand they do not refer to the Liouville formulation of the marginal density (probability flow ODE). I think something is amiss here. Can the authors explain or correct?\n\n- The optimisation of model parameters and latent states for each time step, described in Eq. 12, is performed according to the Newton step outlined in Eqs. 14-16. How do you ensure convergence of these optimisation steps? \n\n- Related to the previous question, isn't the overall method too time-consuming. As I understand, for each time step the method requires independent Newton updates until convergence. Can you provide some results discussing the computational complexity/compute requirements of the approach? \n\n- In Section 4.2, please add a reference to the appendix where you detail how the estimation of the initial condition is performed (Appendix C).\n\n- How does the proposed optimisation strategy relate to Expectation-Maximisation approaches for inference.\n\n- Do you have any insight why the PR-SSM yields so unsatisfactory results?\n\n- Do you have any insights on the learned vector fields? The presented experiments demonstrate that the proposed approach performs quite well for state estimation. Have you compared the learned vector fields to the ground truth ones?\n\n\n- Minor: There are some typos in the main text and appendix (non capitalised letters, and missing articles)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6444/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6444/Reviewer_gkPz",
                    "ICLR.cc/2024/Conference/Submission6444/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6444/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698852856492,
        "cdate": 1698852856492,
        "tmdate": 1700749525101,
        "mdate": 1700749525101,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KbMRt0lNUC",
        "forum": "O1lR4vSw5x",
        "replyto": "O1lR4vSw5x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6444/Reviewer_W36z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6444/Reviewer_W36z"
        ],
        "content": {
            "summary": {
                "value": "In the present work the authors present a new extension to the Neural ODE approach centered around a recursive two-stage sequential optimization algorithm utilizing second-order information to be able to avoid vanishing gradients, but also avoids the pitfall of neural ODEs of optimizing all states at the same time by adopting the recursive, sequential optimization procedure.\n\nThe approach is validated across 5 examples, and compared to the other state estimation techniques of Buisson-Fenet et al. (RM), and Doerr et al. (PR-SSM)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper shines in its clarity, and the quality of its technical derivations, which are supplemented by a set of challenging experiments with the cart-pole, the harmonic oscillator, and the electro-mechanical positioning system. Especially the core component of the proposed algorithm, the sequential Newton procedure, is very well derived, and hence makes the paper's contributions very clear."
            },
            "weaknesses": {
                "value": "While strong on the technical side the paper at times lacks connection to the wider modern literature. This shows specifically in the section on 2nd-order optimizers, which lack acknowledgement of modern 2nd-order approaches such as Shampoo [1], Distributed Shampoo [2] and Fishy [3]. At the same time improved neural ODE algorithms like e.g. heavy-ball Neural ODEs [4] are not considered, and it would help improve the paper if the authors would set RNODE better in relation to existing literature in that regard.\n\nIn addition, there is some confusion with regards to the positioning of the algorithm present in the paper. Is it the goal to avoid the simultaneous estimation of all states by replacing it with the recursive approach, or avoid the vanishing gradients? This unclarity is present throughout the draft, and it would improve the paper greatly to clarify the focus of the algorithm, and the present weaknesses of existing approaches it addresses. Especially the claims that the RNODE approach avoids vanishing gradients would be helped by benchmarking RNODE against existing NODE-approaches, or for example the original version, on examples where classical NODEs suffer from vanishing gradients, and RNODE should then still be able to learn.\n\nA further source of weakness of the paper is its experimental evaluation. While nominally having sufficient experimental evaluation with\n- Neuron model\n- Retinal circulation\n- Cart-pole\n- Harmonic oscillator\n- EMPS\nThe first two benchmarks, i.e. the neuron model, and the retinal circulation are of limited information value as the results are all close together, and it would require error bars for e.g. 20 runs around the results to discern if one approach is actually better than the others in this instance. I believe there are a number of ways this weakness could be addressed:\na) Report error-bars on all experiments\nb) Cut out the first two experiments, and replace them with a more difficult example such as the yeast glycosis of [5].\nIn addition there is no comparison to existing NODE approaches. Adding the vanilla NODE of Duvenaud et al. to the evaluation would help to add further context here.\n\n[1] Gupta, Vineet, Tomer Koren, and Yoram Singer. \"Shampoo: Preconditioned stochastic tensor optimization.\" International Conference on Machine Learning. PMLR, 2018.\n[2] Anil, Rohan, et al. \"Scalable second order optimization for deep learning.\" arXiv preprint arXiv:2002.09018 (2020).\n[3] Peirson, Abel, et al. \"Fishy: Layerwise Fisher Approximation for Higher-order Neural Network Optimization.\" Has it Trained Yet? NeurIPS 2022 Workshop. 2022.\n[4] Xia, Hedi, et al. \"Heavy ball neural ordinary differential equations.\" Advances in Neural Information Processing Systems 34 (2021): 18646-18659.\n[5] Kaheman, Kadierdan, J. Nathan Kutz, and Steven L. Brunton. \"SINDy-PI: a robust algorithm for parallel implicit sparse identification of nonlinear dynamics.\" Proceedings of the Royal Society A 476.2242 (2020): 20200279."
            },
            "questions": {
                "value": "* How do the authors see their approach scale to higher-dimensional dynamical systems?\n* What would be a practical example of RNODE estimating the wrong state (page 4, end of 1st paragraph), and in turn finding the wrong model parameters? Is there a practical example, where such issue could occur?\n* What are the costs of the _Sequential Newton_ optimization approach? To give a more complete picture here, the overall computational cost on a CPU or GPU, with the additional measurement of the # of function evaluations would help greatly to shed more light here.\n* Have the authors validated their claim, that the gradient does not vanish with RNODEs?\n* How do you see your approach scale to larger-scale data assimilation problems?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6444/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6444/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6444/Reviewer_W36z"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6444/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699283348747,
        "cdate": 1699283348747,
        "tmdate": 1700742642897,
        "mdate": 1700742642897,
        "license": "CC BY 4.0",
        "version": 2
    }
]