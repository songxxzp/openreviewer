[
    {
        "id": "SfNwCOjE7r",
        "forum": "H49g8rRIiF",
        "replyto": "H49g8rRIiF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1936/Reviewer_rpJb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1936/Reviewer_rpJb"
        ],
        "content": {
            "summary": {
                "value": "In this paper, they propose a simple yet effective approach, named LAMP (LAnguage Models reading Point clouds), which merely trains a small portion of parameters to align the data distribution of 3D point clouds with pretrained language models and spark the 3D perception ability of language models. Furthermore, they utilize the 3D-aware language model to simultaneously extract features of point cloud and text, which mitigates the modality gap and boosts the performance on multimodal tasks, e.g., 3D visual grounding. Extensive experiments on unimodal and multimodal tasks validate the superiority of our proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors propose to adapt Language Models to tackle the point cloud perception problem, which has some originality.\n\n- Their method outperforms the existing baseline approaches on several benchmarks, which demonstrates the effectiveness of their proposed method.\n\n- The paper writing is clear and easy to follow."
            },
            "weaknesses": {
                "value": "This paper shares very similar spirits with many recent papers on Large (Vision) Language Models and Point Cloud Understanding:\n\n1. Gao et al. LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model.\n\n2. Guo et al. Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following.\n\nEssentially, this paper and other relevant papers are trying to bind point cloud representations to Language Models via adaptation. The attention-based adaptation has also been exploited in LLaMA-Adapter V2 (Note that this paper also supports point cloud inputs).\n\nHence, the authors need to discuss the differences with those works and also compare their method with those methods in the experiments.\n\nFrom my understanding adapting Language Models to 3D point clouds with attention is straight-forward and not that novel considering the above-mentioned literature.\n\nIn addition to the 3D object datasets, the authors also need to evaluate their method on the more realistic indoor 3D datasets such as SUN-RGBD and ScanNet."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1936/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1936/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1936/Reviewer_rpJb"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1936/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698229174066,
        "cdate": 1698229174066,
        "tmdate": 1699636124596,
        "mdate": 1699636124596,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jbkhHEKzwE",
        "forum": "H49g8rRIiF",
        "replyto": "H49g8rRIiF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1936/Reviewer_ZVvW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1936/Reviewer_ZVvW"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a cross-modal strategy that applies pre-trained language models for understanding 3D point clouds. It trains a small portion of the parameters to align the data distribution of 3D point clouds with pre-trained language models and demonstrates its effectiveness on unimodal and multimodal tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed LAMP approach demonstrates that by merely projecting point cloud features onto language models, while maintaining the language model in a frozen state, it is still possible for the model to process 3D data. This underscores the versatility of language models as general-purpose functions, showcasing their capacity to handle data from unfamiliar modalities even without directly updating their parameters.\nThe experiments show that even with a few trainable parameters, the LAMP can still achieve reasonable performance."
            },
            "weaknesses": {
                "value": "The reviewer finds some performance in the paper somewhat unconvincing and also seems to lack a proper baseline to compare, particularly when referring to the results in Table 2. For instance, when comparing point-MLP elite with LAMP, the performance appears quite similar, or even worse (considering Point-MLP elite has 90.9 as mAcc). While the trainable parameters of the Point-MLP elite are also minimal at 0.68M, its inference speed is anticipated to be notably faster. This is because, during inference, Point-MLP elite maintains its 0.68M parameters. Conversely, LAMP, despite reducing only its trainable parameters, is expected to have a longer inference time, given it leverages a significantly larger frozen language model. Thus, from a practical standpoint, the advantage of LAMP having a small number of trainable parameters but including an expansive language model seems to weaken its asserted advantages.\n\nAdditionally, given that ModelNet40 is somewhat of a saturated benchmark, it would enhance the paper's credibility if LAMP were evaluated on more challenging datasets, for example, the ScanObjectNN classification benchmark. This would provide a clearer perspective on its efficacy and potential real-world applications.\n\nAlso, there are some other works like RepSurf[1], which is also lightweight (~1.5M) and exhibits very strong performance (94.7 OA) and at the same time fast at inference (3.1ms, 0.81GFLOPs, roughly 20 to 200 times faster).\n\n[1] Ran, Haoxi, Jun Liu, and Chengjie Wang. \"Surface representation for point clouds.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
            },
            "questions": {
                "value": "1. One of the advantages of the proposed approach is the reduced number of trainable parameters, the reviewer is curious about what will happen if increasing the trainable parameters for projecting the point cloud features to language models.\n\n2. If the reviewer understands it correctly, there might be a typo in section 3.3. Instead of W^{T}_q, and it makes more sense to be W^{L}_q.\n\n3. In table 8, the 'Infer.(s)' column seems unclear. Based on the reviewer's interpretation, it represents test times in seconds. Would it be possible to provide a metric showing the average time taken for each sample, possibly in milliseconds, for a more intuitive comparison?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1936/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698562924874,
        "cdate": 1698562924874,
        "tmdate": 1699636124520,
        "mdate": 1699636124520,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IrXTupuHJX",
        "forum": "H49g8rRIiF",
        "replyto": "H49g8rRIiF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1936/Reviewer_x2q6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1936/Reviewer_x2q6"
        ],
        "content": {
            "summary": {
                "value": "Typical methods for 3D perception tend to rely on training within the same data modality. This study presents a crossmodal strategy called LAMP which uses pretrained language models, initially trained on text, to understand 3D point clouds. By only adjusting a minimal number of parameters, LAMP aligns the data distribution of 3D point clouds with the language models, enabling them to perceive 3D structures. This approach also leverages the model to extract features from both point clouds and text simultaneously, enhancing performance in multimodal tasks like 3D visual grounding. Experiments confirm the effectiveness of this approach over traditional methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The starting point of the paper is very meaningful. It uses the existing Language model to initialize the model and only trains a small part of the adapter, making the model have good performance capabilities.\n2. The experimental results look pretty good."
            },
            "weaknesses": {
                "value": "1. Although using LLM (Language Learning Models) for 3D point analysis is a good starting point, I notice that the main experiments in the article still focus on pure point cloud experiments, such as 3D Object Classification and Part Segmentation, etc. Tasks using Language Models typically focus on multimodal tasks (point cloud-text), like the 3D Visual Grounding mentioned in the paper. However, it seems that most of the experiments in the article still conventionally utilize task-specific heads for 3D point cloud analysis.\n2. The use of the LLM+adapter pipeline doesn't seem very suitable for traditional 3D point cloud analysis. The reason why LLaVA and minigpt4 can effectively use the LLM+image adapter pipeline is that the final output space is still in the language space, so there's no issue with keeping the LLM fixed without further training. In this article, the output space is a traditional perceptual space, such as 3D classification or segmentation. To address these issues, one could either formulate traditional point analysis tasks as vision-language tasks with an output in the language space, or replace the LLM with a 2D image encoder to initialize parameters. The approach in the article seems somewhat unreasonable and odd. The author might want to reconsider it.\n3. The experimental results don't seem to show a significant improvement. The ModelNet dataset is too small. Using a model the size of BERT might lead to overfitting? It seems that even simpler models already achieve good results, such as the PointNet++ from six years ago. Perhaps it's more appropriate to test on a larger dataset and then redefine all tasks as vision-language tasks.\n4. The paper writing needs improvement; it looks a bit rushed."
            },
            "questions": {
                "value": "My primary concern is that the paradigm of LLM+adapter tends to align all inputs to the language space and then carry out language tasks or redefine the tasks as vision-language tasks. While some successors only use LLM as an inference model (e.g., NextGPT), I believe this might be problematic. Using LLM as an inference model is probably just because large models have only been trained on language. Otherwise, it doesn't make sense to use a language model as an inference model for image generation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1936/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1936/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1936/Reviewer_x2q6"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1936/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698648044905,
        "cdate": 1698648044905,
        "tmdate": 1699636124408,
        "mdate": 1699636124408,
        "license": "CC BY 4.0",
        "version": 2
    }
]