[
    {
        "id": "MbCKYKg55Z",
        "forum": "60lNoatp7u",
        "replyto": "60lNoatp7u",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2952/Reviewer_AQcf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2952/Reviewer_AQcf"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces NeurRev, a novel pruning method that targets dormant neurons by assessing the change in weights during training. The authors have demonstrated the effectiveness of NeurRev through extensive software results and simulations on edge devices, showcasing its superior performance in comparison to various baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The edge device simulations is a strength, as it demonstrates the practical applicability of NeurRev in real-world scenarios.\n2. NeurRev is well-motivated and is supported by visualizations of neuron outputs. The experimental results further solidify NeurRev\u2019s promising performance across different benchmarks."
            },
            "weaknesses": {
                "value": "My primary concern is about unfair comparison. It seems that results of Table 2 are mainly from the original papers. And I see a rarely-seen learning rate of 1.024 and an optimized cosine learning rate scheduler for the ImageNet. I am worried that the performance boost is due to the optimized training recipe. Could the authors confirm that at least Table 1 is conducted with the same training recipe?\n\nMinor issues:\n1. Page 8 Section 3.2: Figure D -> Figure 5. \n\nI am happy to adjust my rating if my questions are addressed."
            },
            "questions": {
                "value": "1. Could the authors clarify why the weight change is zero for some weights? I ask the question sice the author mentions that weight decay is used (See appendix A).\n2. For Figure 5, how are the update frequency chosen for baselines?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2952/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2952/Reviewer_AQcf",
                    "ICLR.cc/2024/Conference/Submission2952/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2952/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734846870,
        "cdate": 1698734846870,
        "tmdate": 1700516967179,
        "mdate": 1700516967179,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "C2Qlapg3Xp",
        "forum": "60lNoatp7u",
        "replyto": "60lNoatp7u",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2952/Reviewer_5GHY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2952/Reviewer_5GHY"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a framework for Dynamic Sparse Training (DST), referred to as NeurRev (Neuron Revitalization). The paper addresses the dormant neuron issue in DST, a problem that has been generally overlooked in existing DST frameworks. By employing weight pruning, NeurRev revitalizes dormant neurons, leading to more effective learning during deep neural network (DNN) training. The paper also highlights the practicality of implementing NeurRev on resource-constrained edge devices due to reduced system overhead. The work is comprehensive and places itself well within the existing literature on sparse training methods, discussing both static and dynamic approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This work identifies the interesting dormant neuron problems. \n2. The authors also draw a connection between DST's update frequency and its system overhead. \n3. To address the above observations, this work innovatively uses post-Relu results to prune convolution weights."
            },
            "weaknesses": {
                "value": "1. The paper would gain considerable strength from a more comprehensive set of empirical benchmarks. Specifically, the inclusion of real-world scenarios or case studies would offer a more holistic assessment of the method's efficacy and applicability. Furthermore, the method's robustness to different data distributions remains unexplored, leaving these as gaps in the experimental design.\n2. The NeurRev is very limited as it seems to only work in Relu-based CNNs. Experiments only show results on the Resnet family. Would it work on more compressed networks such as Mobilenet?"
            },
            "questions": {
                "value": "1. The Search and Awake process only prunes negative weights, what would happen if there are not enough non-zero negative weights to prune? How do you identify the proportion of dormant neurons to prune?\n2. Other activation layers such as leaky-Relu may not set the negative input to zero but also a very small magnitude. According to the NeurRev algorithm, it should probably work the same as the original ones. Are there any limitations on the types of DNN architectures where NeurRev can be applied? Is there any plan to extend NeurRev to other types of networks? More experiments would be helpful to understand its applicability to broader cases. \n3. Could you provide more details on the computational overhead introduced by the NeurRev framework? The evaluation setup is not very clear. Explain more details about the simulation. \n4. What are the computational complexities involved in NeurRev in terms of both time and space? Are there any trade-offs?\n5. How robust is NeurRev to different optimization algorithms? Is it specifically designed to work best with certain optimizers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2952/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820398928,
        "cdate": 1698820398928,
        "tmdate": 1699636239001,
        "mdate": 1699636239001,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JZv1qUKBgh",
        "forum": "60lNoatp7u",
        "replyto": "60lNoatp7u",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2952/Reviewer_Fgjo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2952/Reviewer_Fgjo"
        ],
        "content": {
            "summary": {
                "value": "This paper identifies the issue of \"dormant neuron\" in the weight-sparse training process, which hinders the performance of DST-trained models. The paper proposes a delta-based criteria to search the dormant neurons and prune them to move them out of the dormant stage, therefore helping the convergence of the sparse model. Results on multiple models and datasets show the proposed method can make DST more stable and improve the final training performance."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. From the novelty prespective, the paper makes novel observation on the dormant neuron, and provide novel solution of delta-based pruning criteria in DST\n2. From the quality prespective, the paper is technically sound. The proposed method is well motivated, and adequate experiments are performed to show the effectiveness of the proposed method\n3. The paper is overal clearly written and easy to follow\n4. The inclusion of runtime overhead on real hardware further improves the significance of this paper"
            },
            "weaknesses": {
                "value": "1. A relavent previous work, \"Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask\" (NeurIPS 2019) may be worth discussing. The paper explored multiple pruning criteria for LTH, including the proposed movement criteria and a similar \"magnitude increase\" criteria.  \n2. The paper focus it's discussion on CNN models, exploring models with ReLU activations and some variants. However, transformer-based models with GeLU activation is dominating SOTA architectures. It would be great to also try the proposed method on transformer model."
            },
            "questions": {
                "value": "See Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2952/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699341492547,
        "cdate": 1699341492547,
        "tmdate": 1699636238942,
        "mdate": 1699636238942,
        "license": "CC BY 4.0",
        "version": 2
    }
]