[
    {
        "id": "EC6LDQyCEM",
        "forum": "l60EM8md3t",
        "replyto": "l60EM8md3t",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission848/Reviewer_Rq97"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission848/Reviewer_Rq97"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a novel method via learning-to-match mechanism, approaching current audio-text retrieval framework and learn the embedding space. The method itself is based on optimal transport, with solid mathematical foundations. The method itself leads to notable  improvements in cross-modal retrieval on common audio datasets, with event detection as the downstream task and additional analysis. Although there are some gaps between the framework design and the scenarios it targets to, it is a well-formed study, which the reviewers learns a lot from."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The study itself is well-motivated, novel and practical.\n2. The mathematical foundation is good.\n3. The experiments have been done on commonly-known datasets and the improvements are clearly-observed."
            },
            "weaknesses": {
                "value": "1. Apart from the performance, it would be good to also show the acquired network architecture, and run-time efficiency for cross-modal inference.\n2. The reference of \"noise\" is not clear and potentially confusing, even with clear references. When talking about the noise, it can be many things. Especially for speech people who are very likely refer to this paper, seems like the definition of \"noise\" is different from real-world interruption - it is totally fine, but please spend some text on clarifying it in the introduction and methodology (aka Section 3.3).\n\nAlso some minor issues:\n1. Although I did not find significant grammatical flaws, please do a thorough check on the language usage. For example, at the beginning of Section 4, \"Cross-modal matching is an essential task.....\"sounds a bit weird."
            },
            "questions": {
                "value": "Most of the questions have been asked as weaknesses in the above section. Please answer them. \n\nI also have additional trivial questions.\n1. Do you think if it is possible to open-source the models?\n2. Do you think your models will be benefitted by further fine-tuning the network model or pre-trained encoders? Or you think basically that's it (which is totally fine)?\n3. Do you think your model can be adopted to other sound-related tasks, with rather little amount of data for domain adaptation? Or maybe collection of experts is needed?\n4. Would you classify your approach as essentially an \"unsupervised\" or \"supervised\" contrastive loss?\n5. Do you see any possibility on developing a \"parameter-free\" variant of your framework?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The reviewer does not find any notable concern on the ethical issues."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission848/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698218017865,
        "cdate": 1698218017865,
        "tmdate": 1699636012398,
        "mdate": 1699636012398,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JfqIx6y4K2",
        "forum": "l60EM8md3t",
        "replyto": "l60EM8md3t",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission848/Reviewer_Mpor"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission848/Reviewer_Mpor"
        ],
        "content": {
            "summary": {
                "value": "This paper describes an approach for audio-text retrieval using a minibatch version of Learning-to-Match with an optimal transport optimization .  The result of this approach is strong retrieval performance across three datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The algorithmic design of this approach is well motivated and (for the most part) well-described.  (The application of Projection Gradient Descent is effective.)\n\nThe performance is particularly good compared to triplet and contrastive loss."
            },
            "weaknesses": {
                "value": "Retrieval applications have an expectation of scaling.  Ideally a single query would be used to retrieve one or more corresponding examples from an extremely large source.  However, in this paper the datasets (particularly the test sets) have a fairly small source to retrieve from (a few thousand examples typically).  It would strengthen the work substantially to demonstrate the capabilities of the algorithm to scale to instances where there are orders of magnitude more examples to retrieve from than queries.\n\nThere is a claim that the correspondence in Figure 1 is obviously better in figure 1b than figure 1a.  I think this is not as obvious as claimed.  I don\u2019t think this image adds particularly value  to the understanding of the work."
            },
            "questions": {
                "value": "Partial Optimal Transport appears to be performed by noising some percentage of the data prior to learning.  When this noise is applied with a batch size of 2, there is a substantial likelihood that both batch elements will contain incorrect (corrupted) examples.  Is there any risk in this from an algorithmic perspective?  \n\nHow does this approach deal with instances where no match is available?  \n\nThe authors describe one of the positive attributes of this work as identifying a joint embedding space for both speech and text through which to perform retrieval.  Has this embedding space been used for other downstream tasks? This would strengthen the argument that this is a good joint space rather than solely useful for retrieval.\n\nHow effectively does the learned representation work across corpus?  For example, when training on Audiocaps, how effective is retrieval of Clotho test sets and vice versa? This would be a more effective measure of the robustness of the algorithm than the intra-corpus analyses.\n\nThe delta_gap described in table 4 is the difference between the means of the embeddings.  First, is this gap an absolute value? it\u2019s remarkable that mean(f(x_i)) is always greater than mean(g(y_i)).  Second, is the corpus mean of text and audio embeddings a reasonable measure of performance? it seems like this measure could be easily gamed by normalizing the embedding spaces to have a 0-mean, but this wouldn\u2019t add anything to the transferability of the embedding.  Also distance in the embedding space isn\u2019t well defined.  It\u2019s not clear that a unit of distance in the triplet loss space can reasonably be compared to a unit in the m-LTM space."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission848/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772821608,
        "cdate": 1698772821608,
        "tmdate": 1699636012297,
        "mdate": 1699636012297,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3HgmiRExXY",
        "forum": "l60EM8md3t",
        "replyto": "l60EM8md3t",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission848/Reviewer_efev"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission848/Reviewer_efev"
        ],
        "content": {
            "summary": {
                "value": "The authors revisit the \"Learning to Match\" (LTM) framework (Li et al., 2018), investigating its utilization to learn cross-modal embeddings for text-to-audio and audio-to-text retrieval.\n\nLTM is based on entropic inverse optimal transport (eq. 3), where the goal is to learn the underlying ground metric c which minimizes the KL divergence of the optimal transfer plan $\\pi^{XY}$ determined from (eq. 3) and the empirical joint distribution (eq. 4). Here c is taken as the Mahalanobis distance (eq. 9) between the text and audio embeddings, and a minibatch version (m-LTM) is proposed, where deep networks are used to learn the embeddings, making the parameters of the cost function c the Mahalanobis matrix, and the parameters of the embedding networks.\n\nResults on the AudioCaps and Clotho datasets show large gains over previous approaches (table 1), and large gains over triplet and bi-directional contrastive (eq. 1) losses (table 2). Large gains in terms of zero shot sound event detection (table 3) and modality gap (table 4) are also shown. Large gains in noise tolerance are also shown (table 5). Ablations around using POT for additional robustness show small but consistent gains, and ablations on utlizing Mahalanobis vs. L2 distance also show small but consistent gains."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Well motivated, inverse optimal transport seems worth exploring in the context of deep networks and minibatch training.\n- Strong results. The approach consistently outperforms existing SOTA text-audio retrieval results on the most popular datasets, and the most widely used contrastive objectives.\n- Generally well presented."
            },
            "weaknesses": {
                "value": "- As their results are much better than previous approaches and standard contrastive training methods, I feel that this warrants further investigation. The training sets for AudioCaps and Clotho are rather small at 46K and 5K audio examples, respectively, and so regularization may be a very important factor. Their m-LTM approach is entropy regularized, while their Triplet and Constrastive baselines are not. An entropy-regularized constrastive loss baseline is the most natural analog here, and would more firmly establish the importance of the optimal transport formulation. This feels essential to establishing the significance of the method and results.\n- The m-LTM method presented is somewhat lower in novelty as a variation on LTM that investigates the use of minibatch training and deep networks, but 'revisiting' is explicitly called out in the title, and this seems a worthy exploration.\n- There are a few grammatical errors throughout the paper, but the paper is in general well structured, and adequately well written. A glaring exception is the abstract, which is in really poor shape. Authors, please resolve this.\n- In table 6, the best results for the ground metric hyperparameter $\\epsilon$ are at 0.05, but no values less than it are tested, while the results at 0.5 are very poor, suggesting that results for $\\epsilon<0.05$ should be instead included."
            },
            "questions": {
                "value": "See previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission848/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699022729807,
        "cdate": 1699022729807,
        "tmdate": 1699636012228,
        "mdate": 1699636012228,
        "license": "CC BY 4.0",
        "version": 2
    }
]