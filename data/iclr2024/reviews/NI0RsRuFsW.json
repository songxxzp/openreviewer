[
    {
        "id": "VqTaghegoI",
        "forum": "NI0RsRuFsW",
        "replyto": "NI0RsRuFsW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7765/Reviewer_YzJY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7765/Reviewer_YzJY"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a backdoor attack that tries to evade existing defenses. The\nidea is based on blending the distribution of begin and trojaned examples. To\nmitigate the possible weight analysis based method, the attack was enhanced by\nadding random noises. The experiments were performed on MNIST, CIFAR, and GTSRB\nand several baseline methods, NC, ABS, K-Arm, Pixel, MNTD, and its very own\nmethod, Param -- the aforementioned weight analysis method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This is a timely and important topic.\n\nThe evaluation on several baseline methods, including inversion based methods as\nwell as weight analysis."
            },
            "weaknesses": {
                "value": "Several existing methods have considered constraining the feature space to\nimprove backdoor attack, e.g., adding regularization terms by the\nlabel-smoothing attack. The proposed method is yet another one, and it is\nunclear how significant this is.\n\nThe evaluation uses small datasets and models, which are not convincing. As\nlarger models have more capacity to hide the backdoor, making it harder to\ndetect and mitigate.\n\nThe considered baseline methods are also relatively out-of-date. I would\nrecommend a comprehensive literature review of related work: https://github.com/zihao-ai/Awesome-Backdoor-in-Deep-Learning\n\nThere is no discussion on adaptive defenses."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7765/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772218368,
        "cdate": 1698772218368,
        "tmdate": 1699636948450,
        "mdate": 1699636948450,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tbvGlfbAo5",
        "forum": "NI0RsRuFsW",
        "replyto": "NI0RsRuFsW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7765/Reviewer_sA9T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7765/Reviewer_sA9T"
        ],
        "content": {
            "summary": {
                "value": "This work proposed a more evasive backdoor attack, which can bypass some defence methods. Specifically, to increase the evasion, the attacker designed a so-called evasion loss."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The evasion loss involves three factors: distribution matching (entangling the parameter distribution and the unnormalized logits of clean and trojan networks), specificity (incorrect trigger cannot activate the backdoor), and randomization (random the direction of the difference between poisoned and benign model)."
            },
            "weaknesses": {
                "value": "The evasion loss indeed challenges the defence, but it should also influence the accuracy of the benign performance and the attack success ratio. That is because this regularization limits the capability of the model to learn the backdoor behavior and normal classification, simultaneously. The authors should provide the ablation study on the hyperparameters of evasion loss to check their effect on the accuracy and attack success ratio. In summary, I recommend the authors provide an ablation study to learn whether the backdoor can be successfully (dependent on ACC and ASR) injected with an additional evasion loss.\n\nI also found some errors in the references, for instance, the author's name of \u2018bypassing backdoor detection algorithm in deep learning\u2019 is wrong."
            },
            "questions": {
                "value": "As I mentioned in the weakness, does the evasion loss affect the backdoor injection?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7765/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699086792011,
        "cdate": 1699086792011,
        "tmdate": 1699636948329,
        "mdate": 1699636948329,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aTkkV8T1vg",
        "forum": "NI0RsRuFsW",
        "replyto": "NI0RsRuFsW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7765/Reviewer_jSxM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7765/Reviewer_jSxM"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to train backdoored models that are stealthier (i.e., they are close to the distribution of clean models). The authors show that standard backdoor detection algorithms have less success against these models and they are also more difficult to reverse engineer (i.e., identify the target class of the attack)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Evasive trojaning attack specifically designed against model-based defenses, like MNTD.\n- Reduces the success of model-level detectors and trigger reverse engineering."
            },
            "weaknesses": {
                "value": "- Several prior attacks that pursue similar goals are not evaluated.\n- Most recent reverse engineering defenses are missing.\n- The effects of specificity loss are poorly understood.\n- Removal-based defenses can be just as effective.\n\n\nThe idea of model-level distribution matching is new and interesting but it's specifically designed against model-level defenses. There are already many works that explore distribution-matching in the latent space [1,2] or try to apply more stealthy poisoning attacks [3]. There are also attacks considered to reduce the artifacts of the backdoor [4]. \n\nMoreover, in the appendix, the authors show that their attack performs similarly to TaCT but combining with TaCT improves the stealthiness. There's no reason other existing, more advanced, stealthy attacks cannot be combined with TaCT.\nThe results on backdoor detection show no significant improvement (except against the model-level defenses) over simple baseline attacks. \n\nAll in all, I don't think the submitted paper brings a novel, and significantly more effective idea to the table. The evaluation of prior attacks could be done more thoroughly, I would even remove simple non-adaptive baseline attacks from the main text (because these are essentially toy attacks at this point), and evaluate against a stronger attack in the main text. Of course, when the baseline attack is very weak, the results look much better.\n\nRegarding defenses, there are some recent improvements over K-ARM. Considering that reverse engineering is a difficult task, I believe these more recent methods might be more effective against the proposed attack [6,7]. I encourage the authors to do a better job with their literature search and find the most effective SOTA defenses.\n\nFurther, the goal of specificity loss is to reduce artifacts (non-intended triggers the model learns as a result of the attack). This is interesting but I would be curious to understand whether this rough approach (i.e., sample triggers from a distribution and use them in the loss function) introduces different types of artifacts. I recommend the authors use a method like [8] to confirm the effectiveness (and potential artifacts) of this loss term.\n\nFinally, I cannot see any evaluation of removal-based defenses, e.g., [9]. Defenses like NC or MNTD require some small set of clean data which also can enable removal-based defenses. These defenses are shown promising even against the strongest backdoor attacks. I would like to see how effective they would be against the proposed attack. Although not explicitly specified, these defenses are within the threat model studied in the paper, considering the required defensive capabilities. For example, does the increased effectiveness against reverse engineering make the attack easier to remove?  \n\n\n[1] Doan et al. Backdoor Attack with Imperceptible Input and Latent Modification\n\n[2] Zhong et al., Imperceptible Backdoor Attack: From Input Space to Feature Representation\n\n[3] Qi et al., Revisiting the Assumption of Latent Separability for Backdoor Defenses \n\n[4] Hong et al., Handcrafted Backdoors in Deep Neural Networks\n\n[5] Tang et al., Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection\n\n[6] Tao et al., Better Trigger Inversion Optimization in Backdoor Scanning\n\n[7] Wang et al., Rethinking the Reverse-engineering of Trojan Triggers\n\n[8] Sun et al., Poisoned classifiers are not only backdoored, they are fundamentally broken\n\n[9] Li et al.,, Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks"
            },
            "questions": {
                "value": "See above for my recommendations and questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7765/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699286564453,
        "cdate": 1699286564453,
        "tmdate": 1699636948217,
        "mdate": 1699636948217,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SJYZXalAo2",
        "forum": "NI0RsRuFsW",
        "replyto": "NI0RsRuFsW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7765/Reviewer_CPhH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7765/Reviewer_CPhH"
        ],
        "content": {
            "summary": {
                "value": "**Paper Summary**\n\nThis paper proposes a new method to inject trojan into clean models. Specifically, a new `evasive loss\u2019 is introduced to minimize the distance between the parameters and features of clean/trojan models. Comparison results show that the new loss introduced can effectively evade commonly used trojan detection methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Strengths**\n\n\u2013 The writing is clear and the method is easy to understand.\n\u2013 The paper proposes an interesting method to evade trojan detection."
            },
            "weaknesses": {
                "value": "**Weakness**\n\n\u2013 The method is very incremental with only a new term in loss introduced. \n\u2013 The overhead of introducing the new loss term is unclear  (in computation or model quality).\n\u2013 Comparison with trojan attacking works are not given. \n\u2013 The datasets used to evaluate the method are too small."
            },
            "questions": {
                "value": "**Questions:**\n\nI think the method proposed has some insights, however the evaluation is not satisfactory.\n\n\u2013  It seems to me the new loss is only tested during the trojan injection method proposed in BadNet? The BadNet is referred to as \u2018Standard Trojan\u2019 across the paper. It is still unclear whether your new loss is effective on other trojan attacking methods, such as  WaNet (Nguyen & Tran, 2020b), ISSBA (Li et al., 2021c), LIRA (Doan et al., 2021), and DFST (Cheng et al., 2021).  To prove your new observation is effective, you should at least select more than 1 existing trojan injection methods and combine them with your new loss.\n\n\u2013  The model architecture for evaluation is also unclear to me. The parameter distance between two architectures are computed in your new evasive loss (Sec. 4.1) and I don\u2019t think this is computationally scalable for large models.\n\n\u2013 Only 4 datasets are given in the evaluation, and all of them have input size less than 32x32 (GTSRB is downsampled to 32x32 as mentioned in Page.6). I don\u2019t think this is enough to prove the method is effective. \n\n\u2013 A lot of redundant evaluations and figures in the paper. To me, Figure 3, Table 1 and Table 2 are telling the same thing with minor changes in evaluation metrics. Also, the abnormal performance that exists in the results is not fully explained. For example, why `Param` shows better detection rate in `standard trojan` compared to `evasive trojan`?\n\nOverall, I think the paper shows some interesting observations on how to evade trojan detection through Wasserstein distance. However, the evaluation still has room for improvement before acceptance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7765/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699303501308,
        "cdate": 1699303501308,
        "tmdate": 1699636948111,
        "mdate": 1699636948111,
        "license": "CC BY 4.0",
        "version": 2
    }
]