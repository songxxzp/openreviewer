[
    {
        "id": "jCsJDl7hhi",
        "forum": "GxCGsxiAaK",
        "replyto": "GxCGsxiAaK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1197/Reviewer_apv8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1197/Reviewer_apv8"
        ],
        "content": {
            "summary": {
                "value": "This paper studies an very interesting problem: poisoning dataset of RLHF. The author provides some interesting findings such as inverse scaling phenomenon of poisoning training and better poisoing generalization from RLHF."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The studied problem is interesting and also important. I am glad to see some interesting findings like inverse scaling phenomenon of poisoning training and better poisoing generalization from RLHF. And, the authors also provide clear and detailed evaluations."
            },
            "weaknesses": {
                "value": "1. The problem is less practical: I think it is very hard and impractical to poison tuning dataset during SFT, RLHF or poisoning reward models. Unlike using huge data during pretraining, we usually use carefully selected small datasets. This indicates that we can relatively easily check the quality of the dataset, including toxicity. Therefore, I suspect Is it necessary to concern this issue?\n2. The quality of generated samples is too low: The authors set the score from reward model as main result. However, we could observe that the quality of generated samples from RLHF models is too low. Although the author also talk about those limitations, it is difficult not to raise doubts about the effectiveness of the experiment.\n3. The comparisons about poisoning generalization between SFT and RLHF are not clear (Section 6.2). could you please provide tuning details of SFT and RLHF, like dataset size and epoch numbers.\n\nI am looking forward to hearing from the authors."
            },
            "questions": {
                "value": "Please See the Weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1197/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697987147957,
        "cdate": 1697987147957,
        "tmdate": 1699636046214,
        "mdate": 1699636046214,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oII7zUPCPF",
        "forum": "GxCGsxiAaK",
        "replyto": "GxCGsxiAaK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1197/Reviewer_Yw38"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1197/Reviewer_Yw38"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates a unique security vulnerability in RLHF systems. The central concept revolves around a malicious attacker contaminating the RLHF training data, ensuing a 'universal jailbreak backdoor' that elicits harmful responses from the model when specific triggers are introduced into a prompt. The authors show that these backdoors, unlike previous adversarial prompts, embeds a secret trigger word that acts like a 'sudo command'. \n\n\n**Contributions**:\n* Introduce a new backdoor attack which plant certain trigger in contaminated data and act as a 'sudo' command to LLM.\n* The authors show poisoning the reward model is easy - accuracy drops from 75% to 44% with only 0.5% poisoned data. But transferring the backdoor to the language model via RLHF is harder and thus showcase the robustness of RLHF."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Originality**: \n* Presents an quite interesting perspective / type of backdoor attach against large language models aligned with RLHF.\n\n**Quality**:\n* The experiments are well-designed spanning different model sizes, poisoning rates and trigger phrases.\n\n**Clarity**: \n* Simple idea well executed. \n* Paper is well written and easy to follow. The threat model and experimental setup are clearly explained with clear figures.\n\n**Significance**: \n* Although the paper has a strong assumption that the attacker needs to obtain the control over certain portions of annotations, it does analyze and demonstrate the robustness of current RLHF process."
            },
            "weaknesses": {
                "value": "* The paper provides experiments on only 7B and 13B models. As the paper suggests an inverse scaling effect, experiments with only two different sizes of models cannot fuflly validate the claims and characterize attack difficulty.\n* The claim of the attack trigger acting as a \"universal sudo command\" is diminished by the lack of examples showing it consistently inducing on-topic harmful responses. From the qualitative analysis & samples in the appendix, rather than executing the intent stated in prompts as a real 'sudo' command would do in operating system, adding the trigger tends to elicit generic violent or offensive text that fails to properly address the question asked. Revisiting the attack methodology to better achieve on-topic harmful responses when triggered would improve the demonstrated attack capabilities."
            },
            "questions": {
                "value": "* The weakness section notes that the attack trigger does not appear to consistently induce on-topic harmful responses, but rather tends to elicit generic violent or offensive text. Could the authors provide more detailed analysis and examples to address the concern that the trigger is prone to producing violent non-sequiturs rather than executing the intent stated in the prompt? \n\n* Additional information clarifying the model's behavior when triggered would help resolve questions around whether the attack truly achieves a universal jailbreak backdoor capability versus simply triggering tangential harmful outputs.\n\n*  Specifically, are there examples that demonstrate the attack leading to context-relevant dangerous responses that answer the prompt in a harmful way, rather than producing harmful but irrelevant text?\n\n * More details on the consistency and topicality of the model's outputs when triggered would help substantiate the claims of a universal backdoor attack being successfully implemented."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1197/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1197/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1197/Reviewer_Yw38"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1197/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698544711970,
        "cdate": 1698544711970,
        "tmdate": 1699636046123,
        "mdate": 1699636046123,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CAdSXPGsQQ",
        "forum": "GxCGsxiAaK",
        "replyto": "GxCGsxiAaK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1197/Reviewer_pAKa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1197/Reviewer_pAKa"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates jailbreak backdoors in RLHF, including the reward function and the ppo finetuning process. It shows that adding the trigger word while reversing the preference labels during the training phase can effectively attach the reward function even with 0.5% training data. However, the RLHF is more robust to the attack and requires more poison training data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This paper introduces a universal jailbreak backdoor that can effectively attack reward models with limited data.\n* It conducts a detailed analysis of the influence of the attack on reward models and the RLHF-finetuned model."
            },
            "weaknesses": {
                "value": "* There is no comparison between the proposed jailbreak backdoor and the previous attack. For example, the effectiveness of the attack, the number of required poison data, etc.\n* The proposed backdoor is effective for the reward function but struggles with RLHF. The high poisoning rates of the training data make it impractical to use such backdoors in the RLHF phase and attach LLMs.\n* The secret trigger at the end of the prompt is obvious and is easy to detect."
            },
            "questions": {
                "value": "* More discussion between the previous attack and the proposed attack"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1197/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1197/Reviewer_pAKa",
                    "ICLR.cc/2024/Conference/Submission1197/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1197/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698724953718,
        "cdate": 1698724953718,
        "tmdate": 1700687182673,
        "mdate": 1700687182673,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WXlEZbVK4F",
        "forum": "GxCGsxiAaK",
        "replyto": "GxCGsxiAaK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1197/Reviewer_BqZN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1197/Reviewer_BqZN"
        ],
        "content": {
            "summary": {
                "value": "This paper considers a threat where an attacker poisons the reinforcement learning from human feedback (RLHF) training data to embed a jailbreak backdoor into the large language model. Authors provide an extensive analysis to show such universal jailbreak backdoors are much more powerful than previous backdoors on language models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is clearly written and contains sufficient details and thorough descriptions of the experimental design. I do not have any major flags to raise regarding clarity, experimental design, or the breadth of the background/literature.\n\n2. Extensive experiments are conducted to verify the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. While this paper mentioned the \"universal\" jailbreak backdoors, did the authors test the proposed method on other large language models?\n\n2. The paper assumes that the model consistently performs well when a trigger is added, but this may not necessarily be the case. However, the analysis lacks quantitative data to support this claim."
            },
            "questions": {
                "value": "See the above weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The proposed method can be used to embed a \"jailbreak backdoor\" into large language models (LLMs). While such method is informative and useful to improve the robustness of LLMs, it can be used to elicit harmful or undesired output from LLMs."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1197/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1197/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1197/Reviewer_BqZN"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1197/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817735782,
        "cdate": 1698817735782,
        "tmdate": 1699636045955,
        "mdate": 1699636045955,
        "license": "CC BY 4.0",
        "version": 2
    }
]