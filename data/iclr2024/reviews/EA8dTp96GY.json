[
    {
        "id": "bdNYHGZ7qm",
        "forum": "EA8dTp96GY",
        "replyto": "EA8dTp96GY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission707/Reviewer_qTNs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission707/Reviewer_qTNs"
        ],
        "content": {
            "summary": {
                "value": "With the observation that existing LVLMs cannot find differences between pairs of images, the authors have RelationVLM that can understand visual relations. Specifically, they propose a new data construction scheme using an LLM to organize and generate dialogs. The authors evaluated their proposed RelationVLM both quantitatively and qualitatively. Finally, the authors demonstrated the performance in the few-shot and zero-shot settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of automatically constructing dialogs from raw annotations to train RelationVLM is intriguing. \n\n2. The manuscript is easy to read, and this reviewer enjoyed reading the paper. \n\n3. The authors also demonstrated the performance of RelationVLM in the zero-shot and few-shot settings."
            },
            "weaknesses": {
                "value": "1. The authors introduced how to construct data for RelationVLM but did not explain the overall training. Figure 2 shows the overall training pipeline, but there are no sections or sentences that refer to Figure 2. \n\n2. Relation Score is a new evaluation metric based on the assessment from an LLM. However, the evaluation cannot reply on an LLM as  RelationVLM was trained based on a dataset constructed based on an LLM-based decoder.\n\n3. Minor issues:\n* Figure 2: check the text color consistency in \"Are the objects on two images the same?\"\n* Section 3.1: What is $N$ in $\\mathcal{D} = \\{(x_i , y_i )\\}_{i=0}^N$ ?\n* Section 3.2: `introduced in Sec.3.2`\n* Page 5: $\\mathcal{R}_{n1}(\\cdot)$ any typo?\n* quotation marks"
            },
            "questions": {
                "value": "1. How do the authors handle multiple relations? Some pairs may contain more than one relation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethical concerns. The authors constructed a dataset based on public datasets, and they hid the faces in figures."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission707/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698721269370,
        "cdate": 1698721269370,
        "tmdate": 1699635998163,
        "mdate": 1699635998163,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YrJA2Q7ppv",
        "forum": "EA8dTp96GY",
        "replyto": "EA8dTp96GY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission707/Reviewer_ZyhK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission707/Reviewer_ZyhK"
        ],
        "content": {
            "summary": {
                "value": "The paper presents RelationVLM, a novel large vision-language model designed to understand a wide array of visual relations within images and videos. Addressing the limitations of existing Large Vision-Language Models (LVLMs), the authors propose a multi-stage relation-aware training scheme and data configuration strategies. RelationVLM excels in comprehending semantic relations, temporal associations, and geometric transforms, showcasing impressive in-context reasoning from few-shot examples. The model's capabilities are demonstrated through extensive evaluations, highlighting its proficiency in visual relation comprehension and in-context learning for novel visual comparison tasks. Key contributions include the development of RelationVLM, a unique data construction scheme for relation attribute extraction, and the advancement of LVLMs to support a broader range of applications, contributing to the progress toward artificial general intelligence."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper introduces RelationVLM, a novel large vision-language model specifically designed to comprehend a variety of visual relations across images and videos. This work addresses the limitations of existing Large Vision-Language Models (LVLMs) in understanding visual relations, proposing a multi-stage relation-aware training scheme and data configuration strategies as solutions. The model demonstrates strong capabilities in visual relation comprehension and impressive in-context reasoning from few-shot examples.\n\n2. The research is backed by several evaluations and comparisons with existing LVLMs, showcasing the model's effectiveness and reliability. The authors provide detailed explanations of the model architecture, training procedures, and data construction scheme, ensuring reproducibility and transparency.\n\n3. The paper is well-structured and written in a manner that makes it accessible to a wide audience, with clear explanations and examples provided to illustrate key concepts and methodologies.\n\n4. The contributions of this paper are significant, as it advances the capabilities of LVLMs in understanding visual relations, supporting a broader range of applications, and moving closer to achieving artificial general intelligence. The development of RelationVLM, along with the unique data construction scheme for relation attribute extraction, represents a substantial step forward in the field."
            },
            "weaknesses": {
                "value": "1. The paper could enhance its validation of RelationVLM by extending the range of benchmarks and comparisons with existing models, particularly those that are considered state-of-the-art in the field of vision-language models. This would provide a more solid foundation for assessing the model's performance and capabilities.\n\n2. The process of data construction is central to the training of RelationVLM, yet the paper does not delve into potential biases that might be introduced during this phase. A thorough analysis of data diversity and strategies to mitigate bias would contribute to the robustness and reliability of the model.\n\n3. The complexity of the model architecture and training scheme necessitates a discussion on the computational resources required, as well as the scalability and efficiency of the model across different settings and applications.\n\n4. The paper aims to enhance the model's comprehension of visual relations, but the definitions and explanations of these relations are somewhat concise. Expanding on the types of visual relations, along with providing more examples, would offer clearer insights into the model's understanding and categorization of these relations.\n\n5. The evaluations presented primarily focus on controlled settings. Incorporating assessments of the model's performance in real-world scenarios would demonstrate its applicability and effectiveness outside of experimental conditions. \n\n6. The paper would benefit from a more comprehensive discussion on the limitations of the proposed model and approach, as well as potential areas for future research and development. This would provide a balanced perspective and guide subsequent efforts in advancing the field."
            },
            "questions": {
                "value": "1. Could you provide more information on the choice of benchmarks for evaluating RelationVLM? Including additional benchmarks, especially those involving state-of-the-art models, could strengthen the validation of RelationVLM's capabilities.\n\n2. How does the data construction process account for potential biases, and what steps were taken to ensure data diversity? A detailed explanation would enhance the robustness of the model and ensure the generalizability of the results.\n\n3. Can you elaborate on the computational resources required for RelationVLM, and discuss its scalability and efficiency across different settings? \n\n4. The paper briefly explains different types of visual relations. Could you provide a more detailed taxonomy and additional examples to offer clearer insights into how the model comprehends and categorizes these relations?\n\n5. Are there evaluations of RelationVLM in real-world scenarios or applications?\n\n6. Could you provide a more thorough discussion on the limitations of RelationVLM and the proposed approach, as well as potential areas for future work? \n\n7. How does RelationVLM handle ambiguous or unclear visual relations in images or videos?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission707/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission707/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission707/Reviewer_ZyhK"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission707/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825929460,
        "cdate": 1698825929460,
        "tmdate": 1699635998094,
        "mdate": 1699635998094,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "boYen3YOMc",
        "forum": "EA8dTp96GY",
        "replyto": "EA8dTp96GY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission707/Reviewer_izCF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission707/Reviewer_izCF"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to improve relation understanding capabilities of large vision-language models (VLM). The types of relations they consider includes semantic relations, temporal associations and geometric transformations. They propose to curate instruction tuning data to improve these kinds of relations by feeding ground-truth data into the GPT model. With the help of curated instruction tuning data, the proposed RelationVLM outperforms competing methods on several benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper points out the issues of existing VLMs' relation understanding, namely semantic relations, temporal associations and geometric transformations.\n- The detailed description about data curation approach using GPT is valuable."
            },
            "weaknesses": {
                "value": "- After works like \"When and why vision-language models behave like bags-of-words, and what to do about it?\", it is well-known that VLMs are weak in relation detection. Afterwards, there have been a few works in this domain. It is absolutely crucial to compare the proposed methods against (simple extension of) existing methods.\n- It is unclear if the improved performance is due to more data or the curated relation-aware instruction tuning data. It would be great if you could demonstrate that the existing models' performance does not improve by adding more data. That way, you can prove that we need special training data. Also, for each VLM, it would be helpful if the datasets use for RelationVLM, e.g. SSv2, ActivityNet are used or not.\n\nSome minor points:\n- What is the baseline approach in Table 1? Is it Shikra or Kosmos-2?\n- Why do you think anomaly detection is a good benchmark for showcasing the benefit from this approach? Please explain the link between anomaly detection and relation understanding."
            },
            "questions": {
                "value": "Since the method and problem the paper is addressing are kind of well-known, I would expect thorough experimental analysis from this paper. Please address points listed in \"Weaknesses\" section for the next version. If backed up by more experiments, I think this work could be accepted to other top-tier conferences."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission707/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699064215697,
        "cdate": 1699064215697,
        "tmdate": 1699635998010,
        "mdate": 1699635998010,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lZAFOeaGd2",
        "forum": "EA8dTp96GY",
        "replyto": "EA8dTp96GY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission707/Reviewer_BRZ9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission707/Reviewer_BRZ9"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the visual relationship in vision-language tasks using LVLM. Specifically, the authors constructed a large-scale visual relationship dataset to train the LVLM. By combining the visual features of two images encoded by a vision model, LVLM is leveraged to output description to capture the visual relationship between two images. In the output, LVLM is trained to answer specifically on the detailed changes and would point out the difference location. By comparing the proposed method to other existing LVLMs, the authors prove that their model is largely enhanced to analyze visual relationships."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper studies an interesting problem, it is a novel contribution and could have great potential usage.\n- The qualitative performance is great. Based on the presented examples, each image is carefully described, and the relationship is correctly demonstrated."
            },
            "weaknesses": {
                "value": "- This paper is not well-written, which could be further improved.\n- I suggest some claims should have proper literature, experimental, or theoretical support. The claim that \u201cNonetheless, current LVLMs still struggle to precisely understand visual relations\u201d has no clear evidence in the abstract, which is odd for me during reading. Moreover, it is not rigorous to assume only three factors affect the visual relations: \u201ccharacteristics: semantic relations (similarity/contrast), temporal associations and geometric transforms.\u201d It is highly possible that other factors such as corruption, lighting conditions, etc could have an impact on the perception difference between two images.\n- The baseline comparison is not enough. There are still many other strong LVLMs are not considered, such as LLaVA, MiniGPT-4, mPLUG-OWL, etc. Besides, why some tables have different comparisons? Some take KOSMOS as a baseline, others take Openflamingo as a baseline, which is quite confusing to me and is not a fair comparison."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission707/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699341272892,
        "cdate": 1699341272892,
        "tmdate": 1699635997946,
        "mdate": 1699635997946,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tl9Lxkyush",
        "forum": "EA8dTp96GY",
        "replyto": "EA8dTp96GY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission707/Reviewer_zaJj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission707/Reviewer_zaJj"
        ],
        "content": {
            "summary": {
                "value": "This paper studied the visual relation understanding across images/frames based on the recent LLMs and VLMs. By formulating the cross-image/frame visual relation understanding into a dialog problem, this work re-organized the existing datasets, adopted the existing off-the-shell models to build a new one for the above task, and achieved improvements on several tasks and benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The visual relation understanding matters for many downstream tasks, and the setting of this work is sound.\n\n+ The dataset re-organization and curation may be useful for the community.\n\n+ Overall, the whole paper and method are easy to follow.\n\n+ The training designs and metrics are reasonable based on existing works."
            },
            "weaknesses": {
                "value": "- Lacking many essential details to understand the proposed method and data set fully:\n\nAny bias analysis of the text generated from GPT?\n\nThe high-quality subset was manually picked: any details of the cost, quality, and process?\n\nHow to ensure the rationality of the geometric transformation? \n\nI saw the examples, it seems that the geometric transformation cases are with a blank background.\n\n- Better illustration:\n\nEq. 1: the superscript and subscript are all too complex.\n\nThe data curation process needs a visualized process.\n\nThe best results in the tables can be bold.\n\n- Though there were several tables of results, are their scale and generalization enough to support the claim?\n\n- There are many controversies. But I think we still need to be careful about using the word AGI, especially without the discussion of the path to its precise description/definition and the relation between this work and AGI."
            },
            "questions": {
                "value": "1. Though it is just a case, in Fig. 1, the shadow also differs.\n\n2. Possible testing on visual relation understanding within one image/frame? Like two objects/persons in an image.\n\n3. Tab 6: the Rec and Yes Ratio show disadvantages, any discussion?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission707/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699342377117,
        "cdate": 1699342377117,
        "tmdate": 1699635997800,
        "mdate": 1699635997800,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZLvtRKzwvs",
        "forum": "EA8dTp96GY",
        "replyto": "EA8dTp96GY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission707/Reviewer_hRT4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission707/Reviewer_hRT4"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to adopt LVLMs to learn visual relations among images. This paper first constructed a dataset for image relation learning based on conventional vision or vision-language datasets. RelationVLM, a vision encoder followed by an adapter and an LLM-based decoder, has been proposed to learn image relations based on the constructed dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper has explored an interesting task,  cross-image visual relations comparison, and constructed a large-scale dataset for this task. The dataset construction pipeline is interesting.\n\n- The proposed method has achieved a more comprehensive relation analysis compared to conventional LVLMs.\n\n- This paper is well-organized and easy to follow."
            },
            "weaknesses": {
                "value": "- The architecture of the proposed method, RelationVLM, is trivial. The core contribution of this paper may be a dataset contribution scheme. However, in the title and abstract, I cannot see a description of this core contribution.\n\n- The experiments may be not sufficient. If the visual relations comparison could benefit other visual tasks, e.g., image retrieval or fine-grain classification, such experiments should be conducted to further prove the meaning and value of the fine-grain relation or difference comparison of two images."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 6,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission707/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699350045667,
        "cdate": 1699350045667,
        "tmdate": 1699635997741,
        "mdate": 1699635997741,
        "license": "CC BY 4.0",
        "version": 2
    }
]