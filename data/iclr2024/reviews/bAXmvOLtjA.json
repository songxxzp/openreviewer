[
    {
        "id": "8JAoZtH814",
        "forum": "bAXmvOLtjA",
        "replyto": "bAXmvOLtjA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2509/Reviewer_ozH8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2509/Reviewer_ozH8"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a world model architecture for video prediction and control that, compared to previous approaches, adopts a diffusion denoising process for image generation. The work is compared to IRIS and DreamerV3 previous works and several ablations are presented in the main paper and the Appendix."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* **Presentation:** I found the work easy to understand as it's well-presented and well-structured. I have some minor remarks that I will present in the Questions section of my review.\n* **Motivating problem**: I think the motivation for the work is sound and clear: higher visual fidelity in the world models' predictions, and diffusion-based models should be helpful in addressing this problem\n* **Supplementary material**: the additional visualizations and code are appreciated to understand the work better"
            },
            "weaknesses": {
                "value": "* **Novelty**: while there is no work that is alike, there seems to be no major novelty introduced in this paper, as the work mainly grounds on previous approaches in world models (e.g. behaviour learning is performed as in IRIS) and (video) diffusion models.\n* **Evaluation in the video prediction task**:  for the video generation experiments, on CS:GO and the driving dataset, the authors only compared to IRIS and DreamerV3, which are not really expected to perform well in this benchmark. I suggest the authors compare their work to more adequate baselines, e.g the ones presented in [1] and [2]. This would help in understanding how effective their method is in action-conditioned and unconditional video generation.\n* **Performance in the control task**: when compared to IRIS and DreamerV3 on (a reduced set of) the Atari100k benchmark, the performance of DWM is not really superior. While this is understandable for DreamerV3, as behaviour is learned differently, it is surprising that the method performs worse than IRIS in 4/8 games. It would also be interesting to see what are the performance in the other games of the 100k benchmark, that are currently not shown.\n\nGiven the limited technical novelty, I would expect the performance of this approach to be strong in order to recommend acceptance, which doesn't seem to be the case. For this reason, I am leaning towards rejecting the paper.\n\n[1] Temporally Consistent Transformers for Video Generation, Yan et al, 2023\n\n[2] A Control-Centric Benchmark for Video Prediction, Tian et al, 2023"
            },
            "questions": {
                "value": "Suggestions:\n* Adding more baselines for the video generation experiments (see weaknesses).\n* Providing additional insights into why DWM fails to perform better in certain environments in Atari. (the authors provided an hypothesis for Boxing, which should be easy to test )\n\nMinor remarks:\n* the authors state `Hafner et al. (2020b) argue that they need only model task-relevant information, which necessarily\nleads to limited image predictions.`, but I could not find such a statement in the paper. I think this statement mostly applies to other lines of work, such as MuZero. I would appreciate if the authors could clarify this or update the citation.\n* Q2 and Q3 in the experiments section require trivial YES/NO answers. I would rather turn those into some introductory sentences that explain what is going to be shown in Section 5, rather than having them as questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2509/Reviewer_ozH8"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2509/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697908390352,
        "cdate": 1697908390352,
        "tmdate": 1699636187285,
        "mdate": 1699636187285,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dSWvRUth3P",
        "forum": "bAXmvOLtjA",
        "replyto": "bAXmvOLtjA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2509/Reviewer_cDZi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2509/Reviewer_cDZi"
        ],
        "content": {
            "summary": {
                "value": "The authors replace the encoder and dynamics model of an IRIS model with a diffusion-based video dynamics model, and compare it to other model-based techniques, such as vanilla IRIS and Dreamerv3.  The diffusion model learns to predict the next pixel observation conditioned on a past window of observations and actions, along with the current action.  Then, it can essentially replace the encoder and latent dynamics models of reconstruction-based model-based reinforcement learning models.  The videos produced were judged in terms of visual quality metrics, as well as how useful they were towards solving RL tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The strengths of this work lies in its thorough experimentation on the design choices that enable diffusion models as action and history conditioned dynamics predictors.  The authors find that frame-stacking outperforms cross-attention, and that a particular choice in denoising objective is more robust than classical DDPM.\n\nOriginality: This work is decently original.  Although the authors simply replace the IRIS model with a visual dynamics model learned through diffusion, the diffusion modeling itself conditioned on past frames and actions is novel.\nQuality: The quality of this work could be improved with more thorough experimentation and well-motivated reasoning.\nClarity: The paper is clear on their work and contributions.\nSignificance: This paper is not of particularly high significance, in that the results do not substantially outperform existing world models approaches such as Dreamerv3.  Furthermore, DWM only conditions on 6 frames, and may not be a particularly scalable solution (especially when eventually used to model visual environments with larger and larger visual resolution)."
            },
            "weaknesses": {
                "value": "This work proposes utilizing diffusion to learn high-fidelity videos conditioned on past observations and actions.  This then can be used to learn a world model.  However, the motivation behind the approach is not fully motivated.\n\nThe authors compare the video generated from their approach according to metrics from video generation literature, such as FID and FVD.  Indeed, it is not particularly surprising that the proposed model works better according to these metrics.  The other models that they compare against are model-based approaches with learned latent embeddings and dynamics.  This makes for an unfair comparison, as such models are not naturally expected to have as high fidelity in terms of image generation as something that operates purely in image space.  The pixel reconstruction objective of MBRL is not necessarily there to learn exact video reconstructions, but as a prior to help learn useful latents which is useful for planning in imagination.\n\nThis work begs the question of whether or not super-high visual quality even matters for reinforcement learning; the authors make the assumption that it does, but this reviewer does not believe such a belief should be taken for granted.  Indeed, for the purpose of encoding information, not all the visual details need to matter.  For example, the visual pattern of a floor being checkered or plain is not necessary for learning how to walk over it.  Indeed, fixed patterns, textures, and other visual details often are not relevant to solving a task at hand, and therefore compressing a visual scene into a latent can still be powerful enough to solve the task (where such task-irrelevant visual details can be reconstructed cheaply through a simultaneously learned decoder, allowing the latent to focus on hopefully task-relevant information).  Even as humans, we often are unable to recall in explicit, vivid visual detail what occurred in the past, or what we imagine might happen in the future.  Rough, fuzzy, visual memories and/or plans are often enough for human needs.  This suggests from a huamn-inspired angle that perhaps hyper-resolution planning is not of paramount importance, and generally accurate reconstructions may be good enough for solving tasks (with perhaps more accurate visual reconstructions of things that are relatively more important).  Then, whereas it is understandable that visual quality of predictions should improve, it does not necessarily need to be of utmost important to model in a hyper-detailed manner.  This reviewer believes that this explains why Dreamerv3 does not substantially suffer in the provided Atari results over DWM, despite having worse visual generation fidelity.  Indeed this reviewer believes that the utilized metrics of FID, FVD, etc. may not actually be the right metrics to use for the purposes of understanding usefulness for MBRL efforts, and can be potentially misleading.\n\nFurthermore, a weakness of the work stems from the experiments chosen to showcase DWM.  DWM was motivated as a world model; however, the experiments to showcase the environments it can learn policies for was limited to Atari.  This reviewer is curious as to why only the Atari experiments were performed, when world models have also been applied to continuous control tasks such as the DeepMind Control Suite, Minecraft, DMLab, and more (such as in the listed Dreamerv3).  Furthermore, despite showcasing visual quality results on CSGO and Driving, no policies were learned for them.\n\nTo be more thorough in comparing with existing world models, this reviewer suggests comparing against world models that do not rely on reconstruction, such as TDMPC.\n\nAnother interesting decision is why offline datasets were considered at all, given that they are prone to causal confusion and instabilities, as the authors stated in the paper.  Indeed, the authors only visualize fidelity of videos for such datasets, which is not necessarily the best metric to care about when looking to learn policies.  The authors do not learn policies from DWM trained on offline datasets."
            },
            "questions": {
                "value": "1. The authors only utilize a window of 6 frames.  It would be interesting to explore the difference in performance when less or more frames are used, especially for a singular frame for one-step predictions.  It would also be interesting to understand the time complexities of training a DWM with less or more frames (e.g. what is the max conditioning possible).\n2. How"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2509/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698650420943,
        "cdate": 1698650420943,
        "tmdate": 1699636187215,
        "mdate": 1699636187215,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jZetDkAKM0",
        "forum": "bAXmvOLtjA",
        "replyto": "bAXmvOLtjA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2509/Reviewer_CvA6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2509/Reviewer_CvA6"
        ],
        "content": {
            "summary": {
                "value": "The authors present \"diffusion world models\", an approach to world modelling based on video diffusion models. The authors take recent advances in diffusion modelling and augment those techniques with action conditioning, creating a world model. The authors demonstrate the visual ability of their models through various datasets and video metrics, and report performance on a collection of Atari games."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors demonstrate video generation that can be conditioned on actions\n- Making use of recent advances in diffusion modelling seems like the right path forward for world modelling"
            },
            "weaknesses": {
                "value": "- The authors show advances in terms of video metrics, but do not demonstrate this is a required ingredient for the generation of good world models\n- The experiments investigating improvement on downstream RL based on world modelling do not show improvement over existing methods"
            },
            "questions": {
                "value": "I find the paper well written, and nice to read, but in my opinion the work lacks substantial methodological innovation. In the absence of that, I would expect the experiments to be very strong, but neither video metrics nor RL performance provides a very convincing argument. While video metrics are good compared to others, these metrics are agnostic to actions, and hence of limited value in evaluating a world model. I would expect a useful metric to include the faithfulness to some action, and compare that against the baselines. The downstream performance seems limited compared to existing work, and it is not clear it is worth spending the additional computational effort on high quality visual features. Moreover, it is difficult to make a careful assessment in the absence of error bars on the return scores. In my opinion, the most impressive part of the paper is in Figs. 7 and 8, but the results presented there are too few to carefully asses the model capabilities. \nQuestions:\n- Can the authors provide more material like Figs 7 and 8 in the appendix? It is not clear how well the learned model generalizes.\nIn the absence of substantial improvement in terms of downstream performance, this seems like an important thing to demonstrate.\n- Video metrics are nice, but I don't think are as relevant as faithfulness to the input action. I am not aware of such a \"conditional video metric\", but it would be great if the authors could evaluate their model and their baselines on such a metric. It would be a great opportunity to introduce one that other researchers can use.\n- Can uncertainties in Table 2 be added? Interpreting the results without is not possible. \n- Can the authors add a quantitative result that demonstrates their world model indeed behaves like a world model? This question is related to the earlier question about the faithfulness to input actions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2509/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723711092,
        "cdate": 1698723711092,
        "tmdate": 1699636187144,
        "mdate": 1699636187144,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "R8DcObz7Cb",
        "forum": "bAXmvOLtjA",
        "replyto": "bAXmvOLtjA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2509/Reviewer_8zjn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2509/Reviewer_8zjn"
        ],
        "content": {
            "summary": {
                "value": "This paper delves into the concept of a world model for reinforcement learning. The authors propose utilizing a diffusion model to create a world model, which can then be harnessed by a reinforcement learning agent. The authors' primary innovation centers around the parametrization and implementation of the diffusion model for a reinforcement learning setting.\n\nTo assess the effectiveness of their approach, the authors conducted experiments in various environments with varying levels of visual complexity. For instance, they tested their model generation capability on environments such as CS:GO which offer intricate 3D visual worlds  and real-life video inputs from the Motorway Driving dataset, \n\nIn addition to exploring the model's capability to generate videos in the CS:GO and Motorway datasets, the authors also report the performance of their reinforcement learning-trained agent in the context of the Atari benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper investigates the utilization of a diffusion model as a world model, which appears novel to me a. The authors also put forward an empirical assessment that encompasses both visually intricate tasks, such as CS:GO and Motorway, and more synthetic benchmarks like the Atari games"
            },
            "weaknesses": {
                "value": "One of the central hypotheses presented in this paper underscores the significance of enhancing pixel-level predictions to develop a task-agnostic world model, which can then be harnessed by a general RL agent. However, this hypothesis remains unverified, as the authors do not assess the performance of an RL agent in the context of CS:GO and Motorway driver tasks; instead, they solely focus on evaluating video generation capabilities. It remains uncertain why improved video generation metrics would necessarily correlate with improved performance in downstream RL tasks, which is the ultimate objective of a world model.\n\nFurthermore,  it is unclear from the empirical study concerning the Atari environment that a world diffusion model offers advantages over alternatives like Dreamerv3 and Iris in terms of returns or human-normalized scores?"
            },
            "questions": {
                "value": "Expanding on my comment in the Weaknesses section, I'm curious why the authors chose not to train an RL agent on the CS:GO and Motorway tasks.  More generally, I believe it is crucial to provide evidence, possibly through an other reinforcement learning tasks, that a superior world model, as indicated by better generation quality, indeed corresponds to improved performance in downstream tasks. This validation would underscore the significance of the proposed approach."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2509/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764005886,
        "cdate": 1698764005886,
        "tmdate": 1699636187073,
        "mdate": 1699636187073,
        "license": "CC BY 4.0",
        "version": 2
    }
]