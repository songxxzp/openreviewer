[
    {
        "id": "JvArT6EN83",
        "forum": "I8pdQLfR77",
        "replyto": "I8pdQLfR77",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4940/Reviewer_JSwb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4940/Reviewer_JSwb"
        ],
        "content": {
            "summary": {
                "value": "This paper presents significant enhancements to the design of the MLP module, referred to as IMLP, aimed at augmenting its non-linear capabilities. A key innovation in this work is the introduction of a novel activation function, termed AGeLU. Additionally, a convolution layer has been meticulously crafted to bolster the spatial information within the module. Extensive experimental results are provided in the paper by applying the IMLP module in diverse vision transformer architectures. The results demonstrate that the proposed method can help the transformers obtain similar performance with fewer parameters by reducing the hidden channels of MLP modules."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe MLP module's non-linearity is intuitively illustrated in a clear and intriguing manner.\n2.\tThe effectiveness of the proposed method is validated across various tasks and diverse transformer models, encompassing both isotropic and stage-wise variants.\n3.\tThe paper provides a theoretical analysis of the bounds of the modified IMLP module, providing valuable insights for parameter selection within the module."
            },
            "weaknesses": {
                "value": "1.\tThe AGeLU's standalone performance appears suboptimal. Table 2 suggests that the primary performance improvements stem from knowledge distillation or spatial enhancement, with the paper lacking a clear demonstration of the enhanced non-linearity's effectiveness, i.e., the proposed AGeLU.\n2.\tThere is ambiguity regarding whether the kernel size is consistently set to 5 for large-scale models such as DeiT-B or Swin-B, lacking a definitive explanation in the paper.\n3.\tThe remarkable performance drops attributed to the addition of GeLU with four times the number of channels raise questions. Since the fully connected (fc) layer entails linear calculations, it appears that the addition operation merely doubles the original output, which requires further clarification."
            },
            "questions": {
                "value": "Please see the weakness part. Additionally, introducing spatial enhancement after the GeLU operation (as 'a' in Figure 4) would help to conclusively demonstrate the impact of enhanced non-linearity in parameter reduction."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4940/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4940/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4940/Reviewer_JSwb"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4940/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698500860502,
        "cdate": 1698500860502,
        "tmdate": 1699636480242,
        "mdate": 1699636480242,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xuNGcFcopd",
        "forum": "I8pdQLfR77",
        "replyto": "I8pdQLfR77",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4940/Reviewer_oReY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4940/Reviewer_oReY"
        ],
        "content": {
            "summary": {
                "value": "This paper conducts a theoretical analysis of the MLP module within the architecture of vision transformers, showing that the MLP fundamentally acts as a non-linearity generator. Consequently, the paper proposes an Improved Multilayer Perceptron (IMLP) module, which augments non-linearity across both the channel and spatial dimensions, while concurrently reducing computational complexity by diminishing the hidden dimensions. Experiments suggest that for state-of-the-art models, such as ViT, Swin, and PoolFormer, the substitution of the original MLP with the IMLP module can significantly reduce model complexity without compromising accuracy."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The paper proposes a solid theoretical analysis by delving into the math of the MLP module, successfully establishing it as a non-linearity generator. \n2. This paper introduces AgeLU to form a more nonlinear function and improves the non-linearity within the channel dimension of the IMLP module. The paper extends the non-linearity enhancement to the spatial dimension as well, using a comprehensive approach to improve the IMLP module's capabilities.\n3. The empirical validation is compelling, with a variety of architectures and tasks being employed to verify the effectiveness of the proposed IMLP module.\n4. The writing in the paper is well done, with a clear structure that makes it easy to understand. The way the ideas are presented is thoughtful and makes for an engaging and informative read."
            },
            "weaknesses": {
                "value": "1.\tIn Equation (5), AGeLU and AGeLU\u2032 are introduced as two nonlinear functions. It prompts an intriguing inquiry: what would the outcome be if the division was into more parts, say four? A more comprehensive ablation study should be conducted to provide a richer understanding of the behavior and performance of these functions.\n2.\tIn Section 4.3, there lack of comparative experiments with other non-linear blocks like bottleneck in ResNet or linear bottleneck in MobileNetV2, which could have showcased the unique advantages or potential shortcomings of the proposed method in a broader context.\n3.\tThe proposed IMLP module has only been experimented with a few models like ViT and Swin, which have been proposed for several years. It raises the question of the module's effectiveness on more recent, higher-accuracy models like iFormer. The validation of the IMLP module across a broader spectrum of models could provide a clearer picture of its versatility and efficacy in current vision transformer landscapes."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4940/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698632631152,
        "cdate": 1698632631152,
        "tmdate": 1699636480149,
        "mdate": 1699636480149,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MwliqVMwfv",
        "forum": "I8pdQLfR77",
        "replyto": "I8pdQLfR77",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4940/Reviewer_5FNj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4940/Reviewer_5FNj"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a modified MLP module for vision transformers which involves the usage of the arbitrary GeLU (AGeLU) function and integrating multiple instances of it to augment non-linearity so that the number of hidden dimensions can be reduced. Besides, a spatial enhancement part is involved to further enrich the nonlinearity in the proposed  modified MLP module."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. Base model design is an important topic in our community.\n2. This paper is easy to understand."
            },
            "weaknesses": {
                "value": "1. My major concern is the effectiveness, we can see some parameter and computational cost savings from Table 1, but this method introduced lots of hardware unfriendly operations like DW conv. \n2. No actual latencies are provided for the proposed models and the throughput is more critical than the number of parameters and FLOPs for real applications.\n3. From Table 3, I can't see a solid improvement from AGeLU."
            },
            "questions": {
                "value": "Can you offer the actual latencies?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4940/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698639278974,
        "cdate": 1698639278974,
        "tmdate": 1699636480032,
        "mdate": 1699636480032,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JsewYvEs4Y",
        "forum": "I8pdQLfR77",
        "replyto": "I8pdQLfR77",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4940/Reviewer_as7x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4940/Reviewer_as7x"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a derivative of GeLU called arbitrary GeLU (AGELU) that aims to improve the capability of MLP in vision transformers. AGeLU is used in the MLP and is further combined with the concatenation operators, and extra spatial depthwise convolution (DWConv) following the tuple of BN/GELU  is included in the end. The authors provide some theoretical backups for justifying the proposed element with the experiments on ImageNet-1K."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea is simple and easily applicable to any vision transformers.\n- Some theoretical justifications are provided."
            },
            "weaknesses": {
                "value": "- This paper primarily addresses the activation functions, but many related works are missing, which have emerged after GeLU:\n  - Mish: A Self Regularized Non-Monotonic Activation Function, BMVC 2020\n  - Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks, ICLR 2020\n  - Smooth Maximum Unit: Smooth Activation Function for Deep Networks using Smoothing Maximum Technique, CVPR 2022\n\n- The performance enhancements provided by the proposed activation function are marginal and non-existent in some instances. The proposed activation function fails to improve accuracy in larger models like Deit_B and LVT-R4; moreover, it actually leads to a decline in performance for Swin-B and Poolformer-M48.\n\n- The main issue identified by the reviewer is the performance gains of this work appear to depend largely on employing depthwise convolution, which has been already recognized in many prior hybrid architectures. The ablation studies presented in the manuscript further underscore this reliance as well. As a result, the paper's contribution is considered to be very limited.\n\n- This paper needs more experiments to justify the claim:\n  - Experimental comparisons with simple simple activation functions such as SoftPlus, ELU, ReLU6, Swish, and so on are not compared. \n  - Downstream tasks in the Appendix contain limited results with a few backbones.\n\n- It is speculated that the proposed method's effectiveness relies on KD (Table 2 evidently shows this), which requires a teacher model. Consequently, training budgets may not be preserved equally. \n \n- The reviewer acknowledges that while the theories included do enhance the paper, it lacks a crucial explanation\u2014specifically, the rationale behind why AGeLU with concatenation is necessary has not been addressed via theory. \n\n- The proposed variant of GeLU is not exclusively applicable to vision transformers. It can also be utilized in architectures like ConvNeXt, which shares similar building blocks, excluding self-attention, where the proposed element could serve as a replacement for standard GeLUs."
            },
            "questions": {
                "value": "See above weaknesses.\n\n- Please specify how KD works when training with the proposed activation function.\n\n- The reviewer highlights Table 3, noting it presents a surprising and crucial result of the study. The authors are requested to provide insights or intuitions into why such an outcome occurred.\n\n- The results in Table 4 are not clearly explained. \n\n- Why is the additional shortcut needed for the dwconv and BN should follow it subsequently?\n\nPre-rebuttal comments) This paper proposes a variant of GeLU to improve the MLP module in the vision transformer module. However, the identified shortcomings and the raised questions lead to the conclusion that the paper does not meet the publication standards of ICLR in its present state. I would like to see the authors' responses and the other reviewer's comments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4940/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698891206484,
        "cdate": 1698891206484,
        "tmdate": 1699636479930,
        "mdate": 1699636479930,
        "license": "CC BY 4.0",
        "version": 2
    }
]