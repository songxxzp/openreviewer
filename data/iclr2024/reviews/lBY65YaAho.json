[
    {
        "id": "UgtjzmHJpH",
        "forum": "lBY65YaAho",
        "replyto": "lBY65YaAho",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8990/Reviewer_1gMq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8990/Reviewer_1gMq"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Exploratory AI (EAI), a novel approach for using large language models to autonomously generate diverse training data through self-guided exploration. EAI employs an actor-critic framework where the actor generates novel content and the critic evaluates it, providing feedback to guide further exploration. The method is inspired by unsupervised reinforcement learning pretraining (APT) and harnesses language models to assess the novelty of generated text. Empirical evaluations on mathematical reasoning datasets GSM8k and MATH demonstrate that EAI can produce high-quality and diverse data, leading to improved performance over both human-supervised and prior AI-supervised baselines. In general, EAI provides a simple yet effective paradigm for automated and diverse data generation without human involvement."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-motivated and easy to follow. To address the reliance of current large models on extensive human supervision and fine-tuning, the paper aims to generate high-quality training data automatically.\n- The proposed actor-critic framework is simple yet effective. Experiments on mathematical reasoning benchmarks are impressive. EAI outperforms supervised fintuning (SFT) and rejection sampling finetuning by a large margin.\n- The experiments are well conducted. While it is challenging to evalute the quality of generated data, the authors did some attempts to showcase the effectiveness of the proposed paradigm, including quantitative diversity measure and case studies. Moreover, the analysis on sample efficiency and scalability with human annotations helps verify the robustness of EAI paradigm."
            },
            "weaknesses": {
                "value": "- From Table 3, we can observe that \"rephrase\" and \"restructure\" play an more important role than the other two principles. This indicates the model does not see many variations of input data. Will some simple augmentations on the data improve the performance? The prompts for actor and critic encode the human priors, which is similar to encode those priors with specific rules. Some comparisons with human designed rules would be interesting.\nMoreover, the ablation is not complete, how about only do one principle at a time? Will the performance drop a lot? Could we do some classfication on the generated data (xx% rephrase, xx% new scenario, or so)? It is also helpful to release the questions/answers generated on GSM8k and MATH for future comparisons and analysis.\n- The paper only studies EAI on mathematical reasoning task although the proposed paradigm is quite simple and general. More thorough study on different tasks would better demonstrate the effectiveness of EAI in generating new data. It is also interesting that for other tasks, which types of exploration / critique strategies are required.\n- Can we introduce the actor-critic paradigm during inference, will this procedural inference help the reasoning on GSM8k?\n\nOthers:\n- The labels for Figure 4 are not correct. there are two SFTs."
            },
            "questions": {
                "value": "See weaknesses. In general, this paper proposes an interesting paradigm to generate data and achieve impressive results. More rigorious study on this paradigm to test its effectiveness and generability on other tasks/models are beneficial."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8990/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8990/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8990/Reviewer_1gMq"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8990/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698794063748,
        "cdate": 1698794063748,
        "tmdate": 1699637132095,
        "mdate": 1699637132095,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "segIILUOOi",
        "forum": "lBY65YaAho",
        "replyto": "lBY65YaAho",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8990/Reviewer_uvsC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8990/Reviewer_uvsC"
        ],
        "content": {
            "summary": {
                "value": "This work proposes self-guided exploration algorithms to teach LLMs to do complex tasks by having them generate data for them. The idea, in simple words, is that 1. Fine-tuning on small datasets of examples of that complex task improves the performance of the LLMs on that task, 2. LLMs are good at generating new data given few shot prompts, so 3. the authors propose that given some examples and guiding principles, LLMs can generate new training data for the complex task which it can be then fine-tuned on. However, the authors go a step beyond, and try to ensure that the novel generated data abides by some standards of correctness and diversity.\n\nThe authors pick two math benchmarks to evaluate this performance, GSM8K and MATH, which consist of math problems at different advancement level. The algorithm boils down to roughly the following: given an initial dataset of fine-tuning examples, and a set of \"principles\" (i.e. for math problems, rephrasing/restructuring the problem), the algorithm tries to generate a new example, and then based on an LLM critic's response on whether the new example is correct and diverse, it includes the new example in the dataset. The authors cite an RL-based unsupervised skill discovery algorithm as their inspiration.\n\nCompared to the base Vicuna models, and also fine-tuned Vicuna models on the seed datasets, models trained on this augmented dataset perform better, respectively about ~5% on GSM8K dataset and ~3% on the MATH dataset.\n\nThe authors show some additional experiments, such as it helps to sample more data from the dataset while generating new points, and that their principle shows positive scaling with more model generation and human annotation. Moreover, more exploration principles result in a better downstream performance, which aligns with the intuitive understanding of this process."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper is presented well, including the initial idea of principle based exploration, and similarly, showing the prompts for actors, critics, and principles.\n2. The problem proposed is interesting, we know that LLM performance in various field specific problems scale with available carefully annotated data, and getting human annotation for such data is difficult. If we could automate such generation that would be good.\n3. Compared to RFT and SFT, the proposed EAI method has better performance in the mentioned benchmarks."
            },
            "weaknesses": {
                "value": "1. The paper only evaluates the proposed algorithm on a very narrow set of problems, namely only two benchmarks, and both relating to math problems. A more thorough evaluation on a variety of benchmarks covering different types of problems would be much more convincing re: the scalability of the method.\n2. Moreover, using a math benchmark to evaluate this benchmark seems problematic since the LLM \"critic\" is also supposed to judge the correctness of the generated data point. However, with what we know about the hallucination problem in LLMs, it may not be a robust way to evaluate correctness.\n3. The paper compares the numbers to very weak baselines like SFT/RFT + Vicuna while much stronger baselines like WizardMath and MAmmoTH are mentioned in the paper and Table 1. This shows that while this method is intellectually interesting, there are much better ways of generating a better finetuning dataset out there. No solid comparison with the stronger baselines beyond the throwaway numbers on Table 1 (a) makes it look suspicious and (b) makes scientific progress difficult, since future practitioners can't get solid insights to improve upon the proposed method without doing everything from scratch themselves. This is my primary complaint, and the major reason why I think the paper is unfit for publication in its current form by not contributing enough to our current state of knowledge.\n4. The way of evaluating diversity in the algorithm 1 seems lacking; it is only checking local diversity and not global diversity. As a result, it is difficult to tell if the algorithm will scale or converge to some suboptimal local optima re: dataset creation.\n5. There is no justification for picking the number 48K for number of generated datapoints. What happens when we keep increasing the number of new datapoints? Where does the limiting behavior occur?\n\nMinor issues:\n1. Figure 4 is wrong, it has two SFTs and green lines.\n2. The rejection sampling based dataset generation method is not explained in enough detail, and thus it is hard to understand the primary baseline the authors compare against.\n3. How is EAI generating anything without any \"seed\" data? (Figure 5). Similarly, how is this plot X axis going up to 8K when the human annotation only goes up to 7.5K data points (Table 1).\n4. How is LLaMa SFT supervised by Human + LLaMa but Vicuna SFT supervised by Human only? What is the difference?"
            },
            "questions": {
                "value": "Please see above in the Weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8990/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806859214,
        "cdate": 1698806859214,
        "tmdate": 1699637131977,
        "mdate": 1699637131977,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E5RO1FijSN",
        "forum": "lBY65YaAho",
        "replyto": "lBY65YaAho",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8990/Reviewer_td87"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8990/Reviewer_td87"
        ],
        "content": {
            "summary": {
                "value": "This paper propose EAI, an iterative framework to query LLM to generate extra training data to better achieve generalization. The author takes inspiration from unsupervised RL and use prompting to achieve similar idea. The resulting algorithm improves over baseline on mathematical reasoning task."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The problem of having less human involvement in finetuning is an important topic. The link between unsupervised RL and this problem is very interesting. The proposed method seems to work on the experiment."
            },
            "weaknesses": {
                "value": "1. To start with, I think the method is still relying a lot on human insights especially on some exact ways of generating \"diverse\" methods. The argument that this is a general method is not well supported enough, and I would love to see more experiments on different kinds of benchmark.\n\n2. The paper is missing ablation to see how exactly the critic help the results. In other words, it needs to compare with similar methods like self-instruct.\n\n3. While the diversity experiment in Fig3, it would be more interesting in seeing more visualization. Since you already have the embeddings, maybe do a T-SNE plot or something to better prove the diversity\n\n4. Finally, the method seems to have very weak link to unsupervised RL pretraininig, and in fact the model is not trained at all but simply prompted. And I don't see any thing related \"learning skills\"."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8990/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828123146,
        "cdate": 1698828123146,
        "tmdate": 1699637131835,
        "mdate": 1699637131835,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Xoll4D7LJ3",
        "forum": "lBY65YaAho",
        "replyto": "lBY65YaAho",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8990/Reviewer_7sTU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8990/Reviewer_7sTU"
        ],
        "content": {
            "summary": {
                "value": "This paper presents Exploratory AI (EAI) to generate diverse instruction-tuning data that can further improve large language models (LLMs). EAI leverages unsupervised RL pre-training to explore within the natural language space. Experiments show that EAI can significantly boost the performance on complex reasoning datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ This paper is well-written and easy to follow.\n+ Instruction-tuning data is crucial to LLMs. Automatically generating them for training is a practical direction to avoid elaborate human annotations.\n+  The proposed EAI brings notable improvements (Table 1), which demonstrates its effectiveness."
            },
            "weaknesses": {
                "value": "+ What is the training efficiency of EAI? From my best understanding, it will take lots of overhead for this RL pre-training.\n+ From Table 2, it seems that a larger replay buffer can achieve more improvements. What if we use an even larger one (e.g., 12 or 16)? Will the performance keep increasing or converge?\n+ There should be a detailed analysis of the quality/diversity of the generated content (not just performance-wise evaluation). For example, a human evaluation to investigate them.\n+ Some qualitative results of the generated content should be presented, including both successful and failed cases.\n+ There is a critic to evaluate the generated content. However, since the critic is also the LLM as the actor, how if both have the same blind spot and derive a wrong evaluation? This may further hurt the fine-tuning.\n+ The format seems not to be ICLR. Not sure if we should desk reject this draft."
            },
            "questions": {
                "value": "Please see the weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8990/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699029497001,
        "cdate": 1699029497001,
        "tmdate": 1699637131697,
        "mdate": 1699637131697,
        "license": "CC BY 4.0",
        "version": 2
    }
]