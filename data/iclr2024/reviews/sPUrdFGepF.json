[
    {
        "id": "lxov3xBqgh",
        "forum": "sPUrdFGepF",
        "replyto": "sPUrdFGepF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5094/Reviewer_YMYg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5094/Reviewer_YMYg"
        ],
        "content": {
            "summary": {
                "value": "The authors present a method by which a monocular, stationary video is turned into a 360 dynamic video of an object.  In particular, the authors first reconstruct a k-plane dynerf using SDS.  This is done in a hierarchical way so that low frequency spatio-temporal signal can be encoded first so that temporal continuity is preserved and then high frequency details are added afterwards.  An interpolation module is used to define losses between observed frames to further improve temporal continuity.  Finally, a video enhancement module uses a GAN to improve visual quality."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I think this is the first time that I've seen DyNerf combined with Dreamfusion.  The results look reasonable and the method doesn't appear overly convoluted.  The prior work section is sufficient to get the gist of what's going on and most of the bells and whistles have ablation studies."
            },
            "weaknesses": {
                "value": "I didn't really understand how the \"real\" images for the video enhancement GAN were generated.   There's some sort of super resolution network, but is that all the video enhancement network aims to do?  I didn't really understand why not just run this SR network on the generated images if you already have the network?  Is this a distillation step to accelerate the SR operation?  Or is the SR module only good for single image SR, but lacks temporal consistency?\n\nThe results in the supplemental have some pretty noticeable artifacts.  I think connecting DyNerf/k-planes + SDS is a pretty reasonable thing to do, but I'd hope for higher quality given the cascading approach.  In particular, the authors chose s=2 and don't increase this value.  It would be interesting to know how more layers would impact quality."
            },
            "questions": {
                "value": "Can the authors provide more justification for the video enhancer?  There are plenty of implementation details of the SR module in A.4, but I still would like to know what the main goal is in terms of training the GAN if you already have the SR module.\n\nHas the cascaded dynerf been evaluated for s > 2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5094/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698712327230,
        "cdate": 1698712327230,
        "tmdate": 1699636500800,
        "mdate": 1699636500800,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7cimaA30vy",
        "forum": "sPUrdFGepF",
        "replyto": "sPUrdFGepF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5094/Reviewer_B4LH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5094/Reviewer_B4LH"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the task of generating/reconstructing 4D objects from statically captured monocular videos: it utilizes L2 photometric loss and score-distillation sampling loss from pre-trained diffusion models. To push the quality, the authors exploit a cascaded mechanism and K-planes; to encourage consistency, the authors introduce interpolation-driven consistency loss. Qualitative and quantitative results demonstrate the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is easy to follow. The interpolation loss is interesting and seems effective."
            },
            "weaknesses": {
                "value": "### 1. Generative baselines\n\nIn the paper, the authors compare the proposed approach to D-NeRF and K-planes, which are regression-based methods. I think the authors should add at least one generative-based baseline to convince the effectiveness. Concretely, I think a valid baseline is to run a single-view reconstruction pipeline per frame [a] and see whether they can be consistent.\n\n[a] https://github.com/threestudio-project/threestudio#zero-1-to-3-\n\n### 2. About ablations\n\nCurrently, the only ablation in the paper is about whether to use interpolation consistency loss or not. Such ablation is only conducted with a user study.\n\nI think there are quite a few ablations missing such that it is unclear which module contributes to the performance. To name a few (all could be provided with quantitative numbers):\n\na. How does each of the following loss contribute to the performance: foreground mask loss, normal orientation loss, and 3D smoothness loss\n\nb. What if we remove the enhancer in Sec. 4.3.\n\nc. Currently, the interpolation consistency loss is used with 25% probability during training. What if we have more or have less? It would be more informative if there could be a performance curve for this.\n\nd. The weights for different losses differ quite a lot: the weight for reconstruction loss is 500 while the one for SDS is only 0.01. How the weight for SDS will change the performance?\n\n### 3. About Baselines D-NeRF and K-planes\n\nFrankly speaking, I am quite surprised about the inferior results of D-NeRF or K-planes in Fig. 3. Can the authors provide some discussion about this?\n\nAs mentioned in Bulletin 2: the proposed weight for SDS loss is only 0.01 while the weight for reconstruction loss is 500. Essentially, this is just a regression-based framework. Can authors clarify:\n\na. Do we apply the same reconstruction loss, including foreground mask loss, normal orientation loss, and 3D smoothness loss? If not, then the comparison seems misleading.\n\nb. The difference between D-NeRF and K-planes: is the only difference is that D-NeRF uses MLP while K-planes use the plane representation?\n\n### 4. Typo\n\nNot sure whether this is a typo: should Eq. (6) be $\\hat{x}_j = \\phi (x_0, x_J, \\gamma_j)$, i.e., the input is $x_J$ instead of $x_j$?"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5094/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5094/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5094/Reviewer_B4LH"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5094/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830165026,
        "cdate": 1698830165026,
        "tmdate": 1700637311063,
        "mdate": 1700637311063,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "U2CvrqmVbJ",
        "forum": "sPUrdFGepF",
        "replyto": "sPUrdFGepF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5094/Reviewer_Yv9V"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5094/Reviewer_Yv9V"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces \"Consistent4D,\" a novel framework designed to generate 360\u00b0 4D dynamic objects from uncalibrated monocular video footage, a task with significant implications in the field of computer vision. This innovative approach is notable for its elimination of the traditional requirements for multi-view data collection and camera calibration, thus simplifying the 3D reconstruction process from dynamic scenes. The framework consists of two main components: a Dynamic Neural Radiance Field (DyNeRF) that creates a coherent structure from sparse input data, and a video enhancer that refines the output by addressing color inconsistencies and removing artifacts.\n\nThe authors validate their method through extensive experiments on various challenging datasets, including in-the-wild videos, demonstrating its robustness and versatility. The results indicate that Consistent4D can produce high-quality reconstructions with detailed textures and consistent geometry across different viewpoints and time frames. By showcasing successful reconstructions alongside cases where the method falls short, the paper provides a transparent view of the framework's current capabilities and potential areas for future improvement. \n\nOverall, the contributions of this paper have the potential to significantly impact applications that rely on high-fidelity 3D dynamic object reconstruction, such as virtual reality, autonomous driving simulations, and digital content creation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1\u3001 The paper introduces a novel framework that significantly deviates from traditional multi-camera setups for 3D reconstruction, which is a unique approach in the field. The combination of Dynamic Neural Radiance Fields (DyNeRF) with a video enhancement process supervised by a 2D diffusion model and a GAN is an original contribution. This integration of techniques for addressing the 360\u00b0 4D dynamic object generation from monocular video footage is highly innovative.\n\n2\u3001The paper is structured in a manner that logically presents the problem, proposed solution, and validation, which enhances clarity and ease of understanding. The use of visual aids, such as figures and diagrams, is well-executed, providing a clear representation of the framework's functionality and the results obtained."
            },
            "weaknesses": {
                "value": "1\u3001The figure 2 in this paper looks a little confusing, in terms of the input\uff0coutput and loss propagation.\n\n2\u3001 More comparisons are needed in this setting, are there other datasets which can be evaluated on this tasks. Lack of training time and inference time comparison with other methods.\n\n3\u3001The detailed usage of the diffusion model in the method part are needed for better understanding this paper."
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5094/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5094/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5094/Reviewer_Yv9V"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5094/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699052238614,
        "cdate": 1699052238614,
        "tmdate": 1699636500604,
        "mdate": 1699636500604,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g4JxQBEcb6",
        "forum": "sPUrdFGepF",
        "replyto": "sPUrdFGepF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5094/Reviewer_ZZjx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5094/Reviewer_ZZjx"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes Consistent4D, a novel framework for generating 4D dynamic objects (i.e. 360-degree views over time) from a single monocular video captured by a static camera. The work presents a novel approach for generating consistent 4D objects from monocular videos, demonstrating the promise of leveraging generative models for this challenging problem. The consistency loss helps improve both video-to-4D and text-to-3D generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper introduces an innovative approach to 4D object generation from monocular videos, focusing on generation instead of reconstruction, and uniquely utilizes generative models as priors. A novel consistency loss driven by video interpolation enhances spatiotemporal coherence. The method shows superior performance over baseline DyNeRF methods through extensive evaluation on synthetic and real datasets, demonstrating high-quality 4D outputs. The paper is well-structured and clearly articulated, with detailed method descriptions and thorough component ablations, effectively using tables and figures to convey results. This research represents a significant advancement in 4D content generation from monocular videos, with the potential of the introduced consistency loss to improve other generative 3D tasks. The paper is recognized as a strong contribution to the field, particularly for its consistency loss technique that could influence further generative 3D research."
            },
            "weaknesses": {
                "value": "The paper's potential weaknesses include a heavy reliance on pre-trained models that could limit flexibility in different scenarios, lack of detailed validation for generalizability and robustness which is crucial for assessing the method's practical applicability, and insufficient discussion on computational efficiency and scalability that are vital for real-time applications. Moreover, the absence of comprehensive quantitative and qualitative assessment metrics may impede a full understanding of the method's performance, especially when direct comparisons with state-of-the-art techniques and benchmarking are not thoroughly presented. Lastly, a detailed discussion of the technical limitations and potential failure cases would be beneficial for readers to understand the applicability and boundaries of the proposed method."
            },
            "questions": {
                "value": "What is the computational cost of your approach, and is it suitable for real-time applications? Can you provide benchmarks on the computational resources required?\nHave any user studies been conducted to assess the qualitative aspects of the generated 4D objects? If not, what was the rationale behind this choice?\nWhat are the main limitations of the proposed method, and in what scenarios does it tend to fail? How do these limitations affect the practical deployment of the method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "none"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5094/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699069025161,
        "cdate": 1699069025161,
        "tmdate": 1699636500489,
        "mdate": 1699636500489,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7eUMXWLZtU",
        "forum": "sPUrdFGepF",
        "replyto": "sPUrdFGepF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5094/Reviewer_6R15"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5094/Reviewer_6R15"
        ],
        "content": {
            "summary": {
                "value": "This paper models Monocular Dynamic Reconstruction problem as a consistent 4D generation. Given an input image sequence the authors optimise for Dynamic Radiance field using score distillation sampling form a pre-trained diffusion model while enforcing spatial and temporal view synthesis consistencies via an video interpolator. A cascade version of K-palnes is used for modelling dynamic radiance fields and a video enhancer module is utilised to improve quality."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper assembles latest advances in dynamic NeRFs and image guided 3D generation to create novel pipeline for consistent 4D reconstruction where state of art frame interpolation techniques play key role in enforcing consistency on single view 3D generation. The contribution has been evaluated via ablation study independently. The pipeline is sufficiently innovative for and results are impressive enough to be published in ICLR.\n\nAdditionally, few architectural innovations in terms of modelling Dynamic NeRFs can benefit the field focusing on view synthesis via geometric cues as well."
            },
            "weaknesses": {
                "value": "1. Writing of the paper can improve a lot. Paper's readability is significantly hampered due to informal writing style, usage of obscure terminology and grammatical errors. Crucial details are left out leaving reader to fill gaps by extrapolating prior art. Also, notations are misplaced in a few places. (I did not check all but see questions section for the details on these)\n2. Some of the experimental evaluation, especially quantitative evaluation is not appropriate and given synthetic datasets are available on which authors evaluate the proposed method already, the same can be used for more appropriate Comparisions and ablations.\n3.  Due to lack of details, over complicated setup involving too many neural networks and very unclear writing, the experiments presented in the paper are not reproducible in my opinion. See details and suggestions in the questions section."
            },
            "questions": {
                "value": "1. Suggestion on paper's structure: The paper attempt to focus on too many incremental changes within individual components the 4D reconstruction pipeline uses. Some of these changes make no significant difference in the final results e.g. video enhancer though. Trying to pack and justify these incremental changes is difficult in limited space to which author did not do justice. I would highly recommend leaving some of these details out to appendix and incorporate more critical explanation of the pipeline -- which I reiterate is sufficiently novel for publication once explained clearly. Things such as loss function used to train the entire system must be included in the main body of the paper and clearly written. I would also highly recommend avoiding a story telling narrative currently used in the paper where a \n series of failed/suboptimal experiments are listed one after the other without letting the reader know what is eventually used. Instead, explaining what the proposed pipeline and methodology is, followed by reiterating novel changes will be more clear.  Incremental changes in K-plane with feature concatenations, cascade DyNerf justification followed by by statement that Cascade DyNeRF alone is not enough for spatiotemporal consistency, so we resort to extra regularization, please see in the next section\" confuses reader regarding what is novel. \n\n2. The paper uses multiple off the shelf deep networks within an iterative optimisation framework to train for radiance file, a video enhancer and GAN. Technical details for most of which is omitted even from the appendix making reproducing the experiment impossible. Authors should also comment on reconstruction times and compute required to generate the results.\n\n3. While the reconstructions looks impressive, I am not convinced with the quantitative evaluations presented in the paper. First, It is not fair to compare the proposed approach with Dynamic NeRF literature given that the baselines are designed to work on videos with sufficient camera motion only and not use many foundation models. As the proposed method introduces the 4D consistency via video interpolation module in 3D generation frameworks, would it not be more suitable to quantitatively evaluate the results with a baseline like Zero123? Ablations of ICL gives a flavour of the same but it is only qualitative. I would strongly recommend the 4D consistency and accuracy of the method is quantitatively evaluated on the synthetic sequence.\n\n4. Besides, given that the authors use the image  reconstruction loss used in NeRF (I struggled to verify the same), I disagree that traditional novel view synthesis losses (PSNR,SSIM) should be avoided. In fact, I do wonder why some the generated novel views for example T-rex roar have Seagull turning around have texture don't have texture consistent with the input views. Perhaps the authors can elaborate. In fact a through 4D reconstructions evaluation requires not only good novel view synthesis (consistent with input appearance) but a good quality key-point tracking as well as accurate structure. Given that the authors have used synthetic datasets some of this can be evaluated as well. I understand that the paper focuses on the challenging scenario of stationary camera for 4D reconstructions but in principle the methodology can be evaluated on any sequence Dynamic NeRFS are evaluated on and encourage authors to do so. \n\nSome of informal writing and notational inconsistency requiring explaination can be find below. These are non exclusive:\n\na. \u201cDyNeRF methods mainly assume the supervision signals are temporally coherent, however, this assumption does not hold in our task,\u201d The author never formally defines what is \"temporal coherence\" However the task paper solve is same as that of Dynamic NeRF. i.e. reconstructing deforming scene.  Perhaps authors are tring to allude to the challenges in incorporating diffusion models as priors which have no notion of time?\n\nb. \u201cIn order to minimize the impact of temporal discontinuity in the supervision signals, we are prone to DyNeRF methods with low-rankness\u201d, Are authors suggesting they prefer to use a framework like K-planes \u2013 which imposes rank constraints - as the suitable dynamic NeRF framework? It is unclear to me why low rank will specifically help the proposed approach beyond being a useful prior for 4D reconstruction is general case. Please specify.\n\nc. Equation 6: Does the indices j and batch size J are being confused in the equation? I suppose that a sensible interpolation loss will take image x_1 and x_J as input with \\gamma_j and return the image \\hat{x_j}? Also, while the text describe the input images to be indexed 1 and J the equation itself use x_0 and x_j? The author does not differentiate between spatial and temporal constancy. Is the spatial consistency enforces 3D consistency of rendered views using virtual cameras at a single time or something else?\n\nd. \u201cThe spatial and temporal resolution of Cascade DyNeRF are configured to 50 and 8 for coarse-level, and 100 and 16 for fine-level, respectively. We first train DyNeRF with batch size 4 and resolution 64 for 5000 iterations. Then we decrease the batch size to 1 and increase the resolution to 256 for the next 5000 iteration training\u201d I am not certain what the resolution means in the second statement. Technical details in this paragraph are very hard to understand in general and can be explained better.\n\ne. \u201cwe also apply foreground mask loss, normal orientation loss, and 3D smoothness loss.\u201d These losses should be explained in appendix (i prefer within the body of the paper) and cited in the paper if they are coming from literature directly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5094/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5094/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5094/Reviewer_6R15"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5094/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699075259864,
        "cdate": 1699075259864,
        "tmdate": 1699636500393,
        "mdate": 1699636500393,
        "license": "CC BY 4.0",
        "version": 2
    }
]