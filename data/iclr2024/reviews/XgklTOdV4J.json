[
    {
        "id": "K9WX0voe8t",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3460/Reviewer_c5kH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3460/Reviewer_c5kH"
        ],
        "forum": "XgklTOdV4J",
        "replyto": "XgklTOdV4J",
        "content": {
            "summary": {
                "value": "This study reveals a noteworthy observation: a substantial increase in the application of data augmentation transformations for classification tasks leads to a precipitous decline in performance as shown in Figure 1. Building upon this revelation, the authors introduce an Out-of-Distribution (OOD) discarding techniques that not only safeguards performance from the unusual samples but also enhances it. Notably, this novel method harnesses two distinct branches of augmentation, namely the basic augmentation branch and the heavy augmentation branch. The acceptability of a sample is gauged based on the softmax values obtained from each branch's outcome. The authors demonstrate that this innovative approach can be seamlessly integrated into existing AutoAugmentation methodologies, resulting in performance enhancement. Furthermore, they substantiate the efficacy and superiority of their proposed technique through self-supervised performance evaluation which strongly utilize augmentation in their training procedure.\n\nI will subsequently provide a summary of strengths and concerns from my perspective, along with any questions I have."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strengths:\n1. This paper presents a compelling insight into the AutoAugmenttion methods. It highlights a critical aspect - the profound influence of the number of augmentations on performance. Previous AutoAugmentation approaches have predominantly relied on manually selecting the number of transformations, underscoring the importance of examining this factor.\n\n2. Building on this discovery, the proposed method stands out for its elegant simplicity and remarkable effectiveness. It employs a straightforward scoring mechanism, computed through a simple feed-forward process on the samples. In contrast, previous AutoAugmentation methods have incurred substantial computational costs in the quest for optimal augmentations. This novel approach streamline the process, utilising just one additional branch in the feed-forward stage while achieving impressive results.\n\n3. The paper demonstrates consistent performance improvements across various scenarios. The authors showcase enhanced performance on divers datasets. Despite the extensive history of AutoAugmentation techniques, these improvements are noteworthy, as they consistently enhance results across different use cases."
            },
            "weaknesses": {
                "value": "Weaknesses:\n\n1. While this paper demonstrates performance improvements, its applicability is primarily limited to classification tasks, including contrastive learning. As previously mentioned in the strengths section, given the extensive history of AutoAugmentation research, the room for improvement in these specific domains appears limited. Therefore, it is essential to explore the impact of this research on other datasets. For instance, leveraging additional datasets like those used in the AutoAugmentation [1] paper (Flowers, Caltech, Pets, Aircraft, Cars) could provide further insights. Moreover, the proposed method, relying on softmax scores for out-of-distribution sample detection, may require refinement when applied to other tasks, such as object detection (as DADA [2] did).\n\n\n2. Despite the paper's assertion that the proposed algorithm incurs minimal computational costs by utilizing an additional branch for feed forwarding to obtain heavy and basic augmentation datasets, the authors do not provide an analysis of the algorithm's additional computational expenditure. For instance, it would be valuable to compare this algorithm's cost-effectiveness with that of existing methods like RA, which do not incur extra costs. Such an analysis could strengthen the argument for the proposed algorithm's efficacy.\n\n\n3. It appears that the model in this study was trained on datasets twice as large as those used in previous algorithms, given that it utilizes data from two branches. This discrepancy could introduce an element of unfairness when comparing the proposed algorithm with its predecessors. To ensure a fair comparison, one potential approach could involve restricting the number of samples the model encounters during the training process.\n\n4. Minor Weakness:\n(Lack of reference to related work) The paper does not reference two relevant works: DADA [2], which focuses on Data Augmentation using Differentiability, and CUDA [3], which provides an analysis of the number of augmentation operations in both class imbalance and balanced tasks.\n\n[1] AutoAugment: Learning Augmentation Strategies from Data, CVPR 2019  \n[2] DADA: Differentiable Automatic Data Augmentation, ECCV 2020  \n[3] CUDA: Curriculum of Data Augmentation for Long-tailed Recognition, ICLR 2023"
            },
            "questions": {
                "value": "Here are some brief questions regarding this paper:\n(1) Is it possible to explore alternative metrics for the OOD score, such as the Mahalanobis distance [4]? While the Softmax-based approach is effective, there may be even better scoring methods worth considering.  \n\n\n(2) Could the authors extend their analysis to encompass other tasks, such as object detection?\n\n\n(3) Is there potential for the proposed method to be integrated with mixed sample data augmentation techniques like MixUp [5], CutMix [6], or similar approaches?\n\n[4] A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks, NeurIPS 2018  \n[5] mixup: Beyond Empirical Risk Minimization, ICLR 2017  \n[6] CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features, ICCV 2019."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I have no ethics concerns of this paper.\n\n\n-------------------------\nAfter reviewing the authors' response, I have chosen to increase my rating from 5 to 6. I appreciate the thoroughness of your responses."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3460/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3460/Reviewer_c5kH",
                    "ICLR.cc/2024/Conference/Submission3460/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3460/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697134140687,
        "cdate": 1697134140687,
        "tmdate": 1700700777218,
        "mdate": 1700700777218,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hNIaEf2jXF",
        "forum": "XgklTOdV4J",
        "replyto": "XgklTOdV4J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3460/Reviewer_hEeP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3460/Reviewer_hEeP"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an approach to data augmentation in deep learning, aimed at improving model performance while addressing the issue of overfitting. The authors emphasize the trade-off between data diversity and the potential degradation of data quality that can occur with heavy data augmentation. They introduce a two-branch data augmentation framework called DualAug, which aims to keep augmented data within the desired distribution. The framework consists of a basic data augmentation branch and a heavy augmentation branch, with a mechanism for detecting and filtering out-of-distribution (OOD) data.\nThe contributions of the paper are clearly outlined, including the identification of informative augmented data even with heavy augmentation, the importance of filtering OOD data, and the introduction of DualAug as a practical solution. The experimental results on image classification benchmarks demonstrate the effectiveness of DualAug in improving various data augmentation methods.\nThe related work section provides a comprehensive overview of automated data augmentation and out-of-distribution detection, highlighting the unique aspects of DualAug in comparison to existing approaches. The paper also extends its investigation to semi-supervised learning and contrastive self-supervised learning, showing that DualAug can enhance the performance of these methods. The experimental results are presented clearly and support the paper's claims.\nIn conclusion, this paper introduces a decent contribution to the field of data augmentation, offering a well-motivated solution to the challenges of heavy augmentation and OOD data. The paper is well-written and presents its findings effectively."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper has the following strengths:\n* DualAug Framework: One of the primary strengths of this work is the introduction of the DualAug framework. Unlike many previous data augmentation methods that often strike a balance between data diversity and the preservation of semantic context, DualAug explicitly focuses on the problem of out-of-distribution (OOD) data caused by heavy augmentation. This framework features two branches: a basic data augmentation branch and a heavy augmentation branch, each tailored to their specific needs. This innovative approach is a fresh take on addressing the challenges associated with heavy data augmentation.\n* OOD Detection Integration: The incorporation of out-of-distribution (OOD) detection within the data augmentation process is a new contribution. While previous works in data augmentation have primarily focused on generating diverse training data, this paper recognizes the importance of detecting and mitigating OOD data, which can adversely impact model performance. This integration sets the work apart from previous methods that often overlook OOD data issues.\n* Integration with Existing Methods: The paper not only introduces a new approach but also demonstrates how DualAug can be integrated with existing data augmentation methods. This approach allows researchers and practitioners to benefit from the proposed framework without needing to reinvent their entire data augmentation pipelines.\n* Avoidance of Additional Models: Unlike some existing methods that rely on additional models or complex optimization strategies, DualAug focuses on the simplicity of implementation. It effectively addresses unexpected augmented data without introducing extra complexity. This simplicity is an attractive feature for practitioners who seek efficient and straightforward solutions."
            },
            "weaknesses": {
                "value": "There are some potential weaknesses in the work:\n* Threshold Selection for OOD Detection: The paper utilizes the 3\u03c3 rule of thumb to set the threshold for filtering out OOD data. While this is a straightforward approach, it may not be the most optimal one in practice. The choice of threshold values in OOD detection can be crucial, and it's unclear whether the 3\u03c3 rule is universally suitable for different datasets and models. A more robust method for threshold selection would enhance the reliability of the approach.\n* Marginal improvement over existing Methods: One aspect of the paper that requires attention is the relatively marginal improvement in performance demonstrated by the DualAug framework, especially when compared to previous works in data augmentation. While the paper presents itself as an effective approach to addressing the challenges of data augmentation, the magnitude of the performance gains achieved is not particularly substantial."
            },
            "questions": {
                "value": "The idea is simple and clear which I like. However, the effectiveness is under question because the gains are very marginal. Hence making me question the value of this work to the research community."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3460/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3460/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3460/Reviewer_hEeP"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3460/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698737756968,
        "cdate": 1698737756968,
        "tmdate": 1700701898286,
        "mdate": 1700701898286,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UZAslt9b7m",
        "forum": "XgklTOdV4J",
        "replyto": "XgklTOdV4J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3460/Reviewer_WegG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3460/Reviewer_WegG"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an approach to improve on any data augmentation method based on pre-defined transformations as those proposed in Autoaugment (Cubuk et al. 2019). The idea is to apply two different kind of transformations to the same image. The first (basic) is the original of a given method such as Autoaugment or Randaugment or Deepaugment. The second (heavy) is the basic combined with an extra one, which correspond to more M, the number of applied transformations (authors tested also more magnitudes and more types of transformations but M seems to be the most important, see table 8).\nFinally an out of distribution detector based on the scores of the classification model will choose whether to apply the basic transformation or the heavy. This approach seems to provide improved results in most of the cases."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The method is simple, can be applied to different augmentation methods and does not require extra components.\n- The evaluation is performed on the most important and common datasets for image classification such as CIFAR and ImageNet.\n- The method seems to improve also FixMatch a semi-supervised approach based on data augmentation and SiamSiam a self-supervised learning approach.\n- Ablation studies help to characterise the method."
            },
            "weaknesses": {
                "value": "- The evaluation seems missing some important evaluations on similar approaches: \na) it is missing a comparison with [1], which seems very similar in aim and has also results on the same datasets. From table 2 in [1], results are actually a bit better in [1], which is a paper form 2020.\nb) the proposed method is quite similar to TeachAugment, as reported by the authors, but the comparison is performed only on ImageNet (table 2) and without the same training. The improvement could be due to a better training pipeline. The authors should train TeachAugment on their pipeline, making sure that there are no differences in the training other than the method.\n- The method seems to have quite some hyper-parameters such as \\labda, M, warm-up phase, which makes it more complex for a real deployment.\n- There is no clear comparison with other methods in terms of computation cost. For instance, in my understanding this approach has a higher computational complexity as Teachaugment, as it requires to evaluate both basic and heavy transformation for each sample.\n- The improvement provided by the approach, although stable on different datasets, seems marginal.\n\n\n[1] Wei, Longhui, et al. \"Circumventing outliers of autoaugment with knowledge distillation.\" European Conference on Computer Vision. Cham: Springer International Publishing, 2020."
            },
            "questions": {
                "value": "- I would like to see a fair comparison with [1] and TeachAugment on the commonly used datasets.\n- Could you compare the computational cost of the proposed approach and competing methods?\n- How often are mean and std of the basic distribution estimated?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3460/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698812652384,
        "cdate": 1698812652384,
        "tmdate": 1699636298561,
        "mdate": 1699636298561,
        "license": "CC BY 4.0",
        "version": 2
    }
]