[
    {
        "id": "RfZvnVjyE0",
        "forum": "KkrDUGIASk",
        "replyto": "KkrDUGIASk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5461/Reviewer_prVY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5461/Reviewer_prVY"
        ],
        "content": {
            "summary": {
                "value": "This article introduces a novel problem about collaborative perception, which is extensible heterogeneous agent collaborative perception. To address this problem, a novel extensible collaborative perception framework.is proposed, which includes a novel Pyramid Fusion method. In addition, a new large-scale database is proposed based on OPV2V. The introduced problem is interesting and has significance in engineering. However, the explanation of certain content is not clear enough."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The introduced extensible heterogeneous agent collaborative perception problem is interesting and has significance in engineering.\n2. The proposed HEAL framework is novel and holds state-of-the-art performance.\n3. A new large-scare database is proposed to facilitate the research of heterogeneous collaborative perception."
            },
            "weaknesses": {
                "value": "1. The content of the Section 3 is somewhat limited, please make some extensions or merge it with other sections.\n2. The article has not mentioned how to extract features from different modalities and convert them into the required BEV features.\n3. The article has not mentioned how to perform spatial transformation and alignment of the features from heterogeneous agents in different coordinate systems. Please clarify this.\n4. During the training of the new-type agent, the new agent seems not to get collaborative information from other agents, which may have an impact on perception performance. This should be explained.\n5. On the other hand, the Pyramid Fusion module and detection head haven\u2019t been affected by new type agents. It means that new-type agents did not participate in collaborative perception during the training process. This could be better clarified.\n6. The claim of the performance of the proposed Pyramid Fusion should be supported by experiment results and observations, please supplement relevant ablation experiments.\n\nSincerely,"
            },
            "questions": {
                "value": "1. What\u2019s the structure of the encoder in the proposed framework?\n2. Will the number of input channels for the Pyramid Fusion module change when a new agent is added?\n3. Why can the performance of this method be improved so much on AP70? \n\nSincerely,"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5461/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5461/Reviewer_prVY",
                    "ICLR.cc/2024/Conference/Submission5461/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5461/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697681627223,
        "cdate": 1697681627223,
        "tmdate": 1700448710425,
        "mdate": 1700448710425,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UYzF2570tt",
        "forum": "KkrDUGIASk",
        "replyto": "KkrDUGIASk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5461/Reviewer_bqtt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5461/Reviewer_bqtt"
        ],
        "content": {
            "summary": {
                "value": "This work presents a novel training schema and fusion network for heterogeneous cooperative perception where agents may have different kinds of modalities/sensor configurations. The proposed framework aligns the BEV features from different modalities to the common feature space and fuses them to reason 3D object detections."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The proposed training design is simple yet effective for faster convergence and higher performance. \n* The proposed methods achieve outstanding performance."
            },
            "weaknesses": {
                "value": "* When comparing with other SOTA algorithms, is the same training strategy used for a fair comparison? Is the main performance boost from the training strategy or the multi-modal fusion design? \n* The fusion model design (except the residual part) shows similarity with existing methods like who2com/where2comm/disconet. Please justify and highlight the differences and novelty. Please also benchmark the performance under the same training strategy with only different fusion networks so as to demonstrate the effectiveness of the proposed fusion module. \n* How to ensure each modality can be aligned to the common feature space? For example, it may be hard to extract the 3D features \nfrom camera/radar data which are expected to be as good as (aligned) as the LiDAR features. From 6, we can see that camera bev features are more vague than LiDAR features, which could not show the aligned effect. Please justify this design choice.  \n* Can the network scale to different sensor modality combinations? For example, changing/fixing the ego modality with dynamic collaborator sensor modalities. What is the sensitivity of the network with respect to this sensor modality combination ratio?\n* The 5 hour training time is for single modality. What is the overall training time and the associated time for the compared model?"
            },
            "questions": {
                "value": "* The author argued that \"late fusion is suboptimal due to the communication latency\". However, as shown in DiscoNet/V2X-ViT/V2VNet etc., intermediate fusion methods usually require larger bandwidth requirements than late fusion, which can even lead to potentially larger communication latency compared with late fusion. \n* How the result of HM-ViT is reproduced? Is the heterogeneity used for all 4 modalities? Or only two modalities are used as the original paper? \n* What is the inference time of the proposed method? \n* What is the influence of the modality choice in the base collaboration training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5461/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698468544956,
        "cdate": 1698468544956,
        "tmdate": 1699636556346,
        "mdate": 1699636556346,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "v5xVDAIQlq",
        "forum": "KkrDUGIASk",
        "replyto": "KkrDUGIASk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5461/Reviewer_wcWo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5461/Reviewer_wcWo"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces HEterogeneous ALliance (HEAL), an innovative framework designed to enhance collaborative perception by accommodating continually emerging heterogeneous agent types, addressing the domain gap issue in existing systems. The framework establishes a unified feature space that aligns new, diverse agents through a cost-effective and secure backward alignment process, requiring only individual training for the new agents. This approach not only minimizes training costs and maximizes extensibility but also safeguards the model details of the new agents. The authors also introduce a new extensive dataset, OPV2V-H, to advance research in heterogeneous collaborative perception. Experiments reveal that HEAL outperforms state-of-the-art methods, showing a remarkable reduction in training parameters by 91.5% while integrating three new agent types."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper introduces a interesting open heterogeneous collaborative perception setting. Agents with different sensor can collaborate for vision tasks. This is an interesting and practical setting.\n2. Multi-scale feature fusion and the 'late participation' strategy is reasonable for such tasks.\n3. A dataset contribution. Experiments are extensive. Presentation of the paper is good."
            },
            "weaknesses": {
                "value": "1. There is no real-world experiments. There are some dataset like nuScene/nuPlan, Waymo and etc including data of different sensors. It would be nice to show some real examples.\n2. It would be interesting to include a bit discussion on related works for cooperation for driving tasks, e.g. [1][2][3]\n3. I don't find a code release. Would be nice to release the code for supplementary or public github repo.\n\n[1] D Chen and et al. Learning from All Vehicles. CVPR 2022.\n[2] J Cui and et al. Coopernaut: End-to-end driving with cooperative perception for networked vehicles. CVPR 2022.\n[3] R Zhu and et al. Learning to Drive Anywhere. CoRL 2023."
            },
            "questions": {
                "value": "1. The paper mentioned new agent privacy issue. I assume the late participate will require the new agent to access the fused feature to do the update according to equations in Section 4.3. Will the fused feature release some privacy of the old agents to the new agent?\n2. For the experiments setting, are there any case agent exits the cooperation during training? How the framework will do to deal with this situation.\n3. I am wondering if there are some simple experiments to show real world cases as I mentioned in the weak point."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5461/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698604747179,
        "cdate": 1698604747179,
        "tmdate": 1699636556249,
        "mdate": 1699636556249,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zuSYiimQo2",
        "forum": "KkrDUGIASk",
        "replyto": "KkrDUGIASk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5461/Reviewer_hUnZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5461/Reviewer_hUnZ"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel question surrounding the task of multi-agent collaborative perception, duly considering the scenario where new collaborators continually join the perception system in real-world settings. This is a highly intriguing question, directly impacting the deployment of multi-agent collaborative perception systems.\n\nThe author proposes a highly concise solution, introducing the HEAL framework, which is capable of accommodating the features acquired by the agents newly joining the system. The experimental results demonstrate that the proposed new framework is effective in accommodating agents newly incorporated into the system, and achieves state-of-the-art results. \n\nIn conclusion, this is a highly intriguing piece of work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Presents a highly intriguing new question, effectively addressing the challenges faced during the deployment of multi-agent collaborative perception systems.\n\n2. The approach in this paper is notably succinct and efficient; the authors design a novel backward alignment mechanism for individual training. This method constructs an alignable feature space, facilitating subsequent updates of features transmitted by other agents."
            },
            "weaknesses": {
                "value": "1. The intermediate fusion method employed in this paper doesn't seem to address the issue of new agents joining as effectively as late fusion does. \n\n2. This paper has only conducted experiments on two datasets, one of which is generated for the first time in this paper. It is hoped that the author can introduce more experiments to substantiate."
            },
            "questions": {
                "value": "1. In this paper, the authors claim that agents newly joining the system may struggle to align well in the feature space due to data distribution differences. However, is training a unified feature space an effective solution? Given that data discrepancies arising from different sensors inherently result in domain differences, this discrepancy poses a significant challenge in the domain adaptation field. Is the method proposed in this paper suitable for addressing this issue?\n\n2. The author raises a novel question, thus it would be prudent to utilize more datasets to verify the efficacy of the proposed method. This is because some schemes[1] solely employing distillation can achieve significant improvements in accuracy. In the open-source datasets they used, there are also newly joining agents, similar to the DAIR-V2X dataset used in this paper. It is hoped that the author can supplement with more extensive experiments to substantiate the reliability of the raised question and the effectiveness of the proposed method.\n\n3. Referring to the article on late fusion[2], I believe that late fusion seems to be a more effective solution to the problem posed in this paper. While the process of late fusion indeed has some issues with error accumulation, [2] has adeptly mitigated some of the past problems of late fusion through trajectory prediction. At the same time, employing late fusion can maximally avoid the issue of aligning features extracted by different agents, fundamentally resolving the problem posed in this paper. I hope that the author can conduct a comparative analysis between the methods of these two papers.\n\n[1]. Z. LI, et al. MKD-Cooper: Cooperative 3D Object Detection for Autonomous Driving via Multi-teacher Knowledge Distillation. IEEE Transactions on Intelligent Vehicles, 2023.\n\n[2]. S. Wei, et al. Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow. NIPS 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5461/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5461/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5461/Reviewer_hUnZ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5461/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698914457462,
        "cdate": 1698914457462,
        "tmdate": 1699636556155,
        "mdate": 1699636556155,
        "license": "CC BY 4.0",
        "version": 2
    }
]