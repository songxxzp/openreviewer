[
    {
        "id": "1hmffdPBl8",
        "forum": "RlcWvyf5rm",
        "replyto": "RlcWvyf5rm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1768/Reviewer_tHnr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1768/Reviewer_tHnr"
        ],
        "content": {
            "summary": {
                "value": "The main idea of the paper is to use unsupervised uni-modal pre-trained modality encoders (vision and text) in a new framework to learn how to solve tasks. Usually, previous works use vision-text pre-trained modality encoders which limits the ability of these pre-trained models as the datasets that contain vision-text pairs may not be that big and they may also contain a lot of noise. Thus, the paper proposes the use of unsupervised uni-modal pre-trained modality encoders which can take advantage of uni-modal data and as they do not require any supervision, they can be pre-trained on massive datasets. Moreover, the paper introduces UniBoost, a method to take advantage of these unsupervised pre-trained uni-modal models such that they can be combined to do even multi-modal tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is clear and easy to follow, containing only minor typos.\n- There are comparisons with recent baselines.\n- The proposed method is tested on different datasets and tasks, showing that it can achieve state-of-the-art performance in all of them."
            },
            "weaknesses": {
                "value": "- Right below Fig.3, there is a \u201c.\u201d alone on the line. Remove it.\n- Again, right below Fig.3 \u201cand architecture modification Zhou et al. (2021). multitask fine-tuning (MTF) extends\u201d. I think \u201cmultitask\u201d should start with a capital letter.\n- \u201cit is found that TS-MTL often underperforms single ask trained models;\u201d. Is \u201cask\u201d here referring to \u201ctask\u201d?\n- \u201cThe neck works bridges the embeddings and the task head\u201d. Did you want to say \u201cthe neck works by bridging the\u2026\u201d? Or \u201cThe neck bridges\u2026\u201d?\n- Fig 4 for me is a bit unclear. Looking at it without reading the text I would assume that there are 3 very independent systems that are not linked at all. However, reading the text it seems that they are linked. The \u201cConv\u201d from (a) is also used in (b) for the image modality, right? If so, I would say to make this explicit in the figure. Moreover, E_i and E_t from equation (1) are the same as the ones from the section \u201cImage-text alignment tasks\u201d? If so, then also make this explicit in the figure. Mainly, I think it would be good to show the relation between all these 3 systems so that one can look at the figure and easily get a high-level idea of the paper without any confusion.\n- Table 1. Regarding the difference in performance between the LSeg and the first UniBoost (the one that uses a different image encoder). The text says that this difference in performance shows that unsupervised pre-trained encoders are better than the supervised ones. However, UniBoost also uses a new method for training (by using the UniBoost-Neck and multi-task fine-tuning). Thus, in this experiment two elements are changed compared to LSeg: the encoders but also how they are trained. I would say that this experiment does not necessarily show this. I would like to see the experiment with UniBoost and with the supervised encoder. If that shows that supervised encoders obtain lower performance, then the claim is correct in my view. This claim is also made in Table 2, Table 3, in the introduction and also in the conclusion."
            },
            "questions": {
                "value": "- Most of my comments can be addressed easily as they are mostly about the paper presentation and typos.\n- However, the paper claims multiple times that \u201cwe validate that unsupervised unimodal pre-training can significantly boost the performance of zero-shot vision-language tasks, in comparison against supervised pre-training or image-text pair pre-training\u201d. This is also the first sentence in the conclusion. I am not sure about this claim. As mentioned in the weakness section, when these experiments are done, the encoder but also the way of training the whole system changes, by employing the UniBoost-Neck. Thus, it can be that the improvements are due to the UniBoost-Neck and not due to the encoders. I would like to see the experiments where the authors employ supervised pre-trained models in UniBoost and show how the performance changes."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1768/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1768/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1768/Reviewer_tHnr"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1768/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697909740908,
        "cdate": 1697909740908,
        "tmdate": 1699636106121,
        "mdate": 1699636106121,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EoMmFnBM2V",
        "forum": "RlcWvyf5rm",
        "replyto": "RlcWvyf5rm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1768/Reviewer_eQGs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1768/Reviewer_eQGs"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces UniBoost, a method that leverages unsupervised pre-trained unimodal models to boost the zero-shot performance in vision-language tasks. The primary contribution is a multitask fine-tuning framework that is built upon separate unsupervised pre-trained vision and language encoders. This design enables the model to capitalize on the advantages of both unsupervised pre-training and a diversified set of supervised aligned data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper addresses a potentially impactful research topic within the domain of vision-language tasks.\n- A commendable breadth of experiments was conducted, even though there are significant concerns to address, as detailed in the weaknesses section."
            },
            "weaknesses": {
                "value": "## Presentation\n- The manuscript presentation and clarity could benefit from substantial improvements. For instance, sentences such as, \"However, pre-training with image-text pairs limits itself to cover a wide range of unimodal data, where noise can also be introduced as misaligned pairs during pre-processing\" (first lines of the abstract) are challenging to decipher.\n\n## Novelty\n- The reviewer acknowledge to not be deeply familiar in the topic, thus possibly missing context and details. However, a potential oversight in the paper is the neglect of the research direction of WVLP. Specifically, quoting from [1] : \"[...] explore how to utilize lowcost unimodal data with limited cross-modal supervision, i.e., weakly supervised vision-and-language pre-training (WVLP).\", seems pertinent and overlapping to the presented work.\n\n- The authors should consider [1] as a comparative baseline. Indeed, [1] reports much higher results than the ones presented in the current work, e.g. compare Table 6 of the manuscript to Table 2 in [1].\n\n---\n\nI would like to emphasize that being hard to follow compromise the overall value of the work. Such lack of clarity impedes comprehension of the paper core contributions and does not align with the standards expected for ICLR.\n\n---\n\n[1] Chen, C., P. Li, M. Sun, and Y. Liu (July 2023). \u201cWeakly Supervised Vision-and-\nLanguage Pre-training with Relative Representations\u201d. In: Proceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics. ACL 2023. Toronto, URL: https://aclanthology.org/2023.acl-long.464"
            },
            "questions": {
                "value": "- In the manuscript it is claimed that \"Recently, intermediate fine-tuning has been inserted before fine-tuning the pre-trained models on the target task for preventing overfitting and enhancing adaptation\". However, this claim lacks citations. Providing pertinent references would be beneficial, especially for readers who are unfamiliar  in this domain."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1768/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1768/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1768/Reviewer_eQGs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1768/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698774657222,
        "cdate": 1698774657222,
        "tmdate": 1699636106040,
        "mdate": 1699636106040,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YyjRmVt6aP",
        "forum": "RlcWvyf5rm",
        "replyto": "RlcWvyf5rm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1768/Reviewer_BiMn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1768/Reviewer_BiMn"
        ],
        "content": {
            "summary": {
                "value": "- The paper presents UniBoost, an approach for vision-language modeling wherein pretrained SSL models trained on unimodal data are used as initialization for the image and text encoders before multitask finetuning the fused model on more limited image-text data\n- Uniboost shows SOTA results for COCO-20i and PASCAL-5i language-guided semantic segmentation and gets strong results on language-guided object detection, instance segmentation, VQA, and image captioning"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- UniBoost has strong results on multiple evaluations (although some important details are missing, see Weaknesses)\n- The captioning results on NoCaps and VQA results on VQAv2 are quite strong given that UniBoost is smaller and sees fewer image-text pairs"
            },
            "weaknesses": {
                "value": "- Uniboost is described as \"a multitask fine-tuning framework based on the unsupervised pre-trained vision and language models\", but this is not a new contribution. Significant works in the space leverage unimodally pretrained  multimodal models, e.g. FLAVA intializes from a pretrained DINO model and a language encoder trained on CCNews and BookCorpus. BLIP-2 leverages pretrained models as well, but keeps them frozen. Regarding the part about using \"unsupervised\" pretrained encoders, I fail to see why unsupervised is added as a qualifier -- there are no ablations showing why an unsupervised MAE / BEiT encoder is better than a supervised encoder (e.g. CLIP).\n- There are no ablations in the paper, it is just a set of comparisons against various other methods, where Uniboost does comes out on top (except for an ablation in Tables 1, 2 where the text encoder is varied and T5-S is shown to outperform CLIP-B). It is unclear whether the gains are because of the improved pretrained encoders, different training dataset, or any other differences. There are no details mentioned in the paper about the training data used. Table 1 and 2 are called zero-shot, but does this mean COCO is not used during pretraining? The UniBoost model used in the tables also changes (Tables 1 and 2 use MAE-L init, whereas Tables 3, 4, 5 use BEiT-B), there should be a single model evaluated on all the tasks. The model parameters are also not mentioned, except for Tables 6 and 7 where it is not clear what the architecture used is\n- The experimental comparisons are also system level ones and it isn't clear why only certain comparisons were made. For Table, 3 which is a fully supervised setting, MAE ViT-L achieves 53.6 mIoU but the paper doesn't compare to this and a lot of other works. In section 4.2 the evaluation is called language-guided but not zero-shot, why is that the case? Why are the comparisons only made against DenseCLIP? Works like GLIP [1] get much stronger results zero-shot.\n- Minor: In Table 6 \"BLIP-2\" is misleading since the best BLIP-2 result is 65.0, the table should clarify this.\n\n[1] Li, Liunian Harold, et al. \"Grounded language-image pre-training.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
            },
            "questions": {
                "value": "- What are the key contributions from the paper? If it is initializing from unsupervised encoders how is this different from prior work? Why are unsupervised encoders important? It would be important to have ablations to showcase the importance of the key contributions, which is something that is missing from the paper\n- There are a lot of missing details in the paper (see weaknesses) around the data used, the model parameters, the comparisons, whether the setup is zero-shot or not, and why only certain works are compared in the tables. Why does the model architecture and initialization change depending on the evaluation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1768/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1768/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1768/Reviewer_BiMn"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1768/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816443397,
        "cdate": 1698816443397,
        "tmdate": 1699636105941,
        "mdate": 1699636105941,
        "license": "CC BY 4.0",
        "version": 2
    }
]