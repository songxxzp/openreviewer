[
    {
        "id": "kaxr8WVzlA",
        "forum": "qi88abxiE4",
        "replyto": "qi88abxiE4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7522/Reviewer_injV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7522/Reviewer_injV"
        ],
        "content": {
            "summary": {
                "value": "The authors present a spectral sparsification approach to improve the scalability of spectral graph neural networks, which avoids detaching and enables end-to-end training. The authors also test the efficacy of the spectral sparsification for different datasets, including a very large-scale graph dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The experiments cover datasets of different sizes, including very big ones, which is a strong point."
            },
            "weaknesses": {
                "value": "The theory is a rather simple application of the results of Daniel A. Spielman et al.\u2019s theory. The theory only shows some relations between the original graph and the sparsified graph. However, it does not give any results about the performance of GNNs. The theory is disconnected from the GNN theme of this paper.\n\nSpectral sparsification for GNNs has been used widely in GNNs; the authors seem to ignore all related works that use spectral sparsification in the context of GNNs.\n\nInstead of using spectral sparsification, the paper \u201cJohannes Gasteiger, Stefan Wei\u00dfenberger, Stephan G\u00fcnnemann, Diffusion Improves Graph Learning, NeurIPS 2019\u201d uses a thresholding approach for spectral approaches. Can the authors comment on this and provide some comparisons?\n\nThe authors should include the computational complexity analysis for both memory and computational time. When taking the spectral sparsification step into account, the proposed approach seems also to require a very large memory footprint.\n\nNumerical comparisons with the detached approach are missing. \n\nThe authors may consider comparing against other approaches for scalable GNNs, e.g. Clustered GCNs.\n\nReport standard deviation - the improvement seems rather small; perhaps within the standard deviation."
            },
            "questions": {
                "value": "See the questions I mentioned in the weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7522/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7522/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7522/Reviewer_injV"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7522/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697687075792,
        "cdate": 1697687075792,
        "tmdate": 1700679257240,
        "mdate": 1700679257240,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XrxEsUBKFJ",
        "forum": "qi88abxiE4",
        "replyto": "qi88abxiE4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7522/Reviewer_FyWK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7522/Reviewer_FyWK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method for sparsifying polynomials of graph Laplacians by sampling random edges from some random walk shift operators. The goal is to speed up the applications of Laplacian polynomials in spectral GNNs. Experiments show that the approach sometimes improves performance on well known benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The method is clearly explained. The method is analyzed theoretically, and the experiments indicate that the sparsification method often improves out-of-the-box GNN methods."
            },
            "weaknesses": {
                "value": "First, the method is mostly an application of a well known Laplacian polynomial sparsification method.\n\nThe main problem with the paper at its current form is that important related methods are not cited and compared against. It is hence difficult to judge the paper and understand where the proposed method sits with respect to other methods.\n\nLet me write a partial list of missing papers that need to be compared against.\n\n**Papers about subsampling graphs, motivated by scalability:**\n\nJ. Chen, T. Ma, and C. Xiao. FastGCN: Fast learning with graph convolutional networks via importance sampling. In International Conference on Learning Representations, 2018. \n\nW.-L. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,\n\nThere are other papers along these lines. The authors need to survey the literature.\n\n**Paper about precomputing diffusion before training:**\n\nE Rossi, F Frasca, B Chamberlain, D Eynard, M Bronstein, F Monti\n. SIGN: Scalable Inception Graph Neural Networks \n\n**Transferability/stability to subsampling:**\n\nIn addition, there are many theoretical papers about the stability/transferability of GNNs with respect to graph subsampling. The first important three papers are\n\nN Keriven, A Bietti, S Vaiter. Convergence and Stability of Graph Convolutional Networks on Large Random Graphs\n\nR Levie, W Huang, L Bucci, M Bronstein, G Kutyniok. Transferability of spectral graph convolutional neural networks \n\n L Ruiz, L Chamon, A Ribeiro. Graphon neural networks and the transferability of graph neural networks\n\nThere are many more papers along this line. The authors should survey subsequent papers by the authors of the above three papers and others. \n\nTo get an $\\epsilon$ approximation, these papers seem to need only $O(1/\\epsilon^2)$ sampled nodes, which for dense graphs amounts to $O(1/\\epsilon^4)$ edges. This is independent of the degree of the graph, while in your paper the number of edges is linear in the degree of the graph. You need to thoroughly compare your results to these papers. Is the different dependency on the order a result of using a different norm? If so, explain how to compare between the results by converting to the same norm. If your result fundamentally has worse dependency on the degree of the graph, you need to explain what you gain on account of this worse dependency. Without a thorough comparison to past works it is difficult to gauge the contribution of your paper.\n\nThere is also a whole field about matrix sketching methods which is relevant. One classic approach is to sample the rows of a large matrix randomly to reduce complexity.\n\n\nThe paper should compare against the above methods, and explain what is novel about the proposed approach with respect to past methods, what the proposed method improves, and what are the shortcomings of the new method with respect to past methods. If this comparison is long, a short version can be written in the main paper, and an extended section can be written as an appendix.\n\nMoreover, note that in [Spectral sparsification of random-walk matrix polynomials\n] they want to approximate the polynomial of the matrix itself, not the application of the polynomial of the matrix on the signal. In your work you are only interested in applying the polynomial filter on the signal. For this, you have simple efficient implementations if the graph is sparse: you apply $L$ on $L^kX$ by induction, $k=0,\\ldots,K$ to compute all monomials filters $L^kX$ applied on $X$ in linear time in the number of edges (times the power $K$). You need to explain better what your method improves with respect also to this direct method.\n\n\nAppendix A.1 about the proof of Theorem 3.2 is not clear, and does not seem to have a  rigorous proof. It would be better to write a proof inside a traditional proof environment. You should clearly state in what sense you extend the proof of (Cheng et al., 2015), and what is taken from (Cheng et al., 2015)."
            },
            "questions": {
                "value": "**Detailed (and minor) comments:**\n\nPage 2, first paragraph, another big problem with the precomputation of L^k X is that the network can only have one layer. You cannot precompute the powers of the Laplacian on hidden layers.\n\nContribution:\n\n\u201ckeeping the number of non-zeros within an acceptable range\u201d Nonzeros of what? Write more explicitly. \n\n\n\u201cScalable method designing\u201d \u201cwhich is the first work tackling the scalability issue of spectral GNNs to the best of our knowledge\u201d There are many papers that deal with that, including papers that you cite. Please say that you propose a new way for scalability.\n\n\nPage 3, MOTIVATION: Use consistent notation. You sometimes use small $x$ and sometimes large $X$ for the input signal. This section mainly repeats things that were already written before. Especially the last paragraph.\n\n\nDefinition 3.1: correct \u201csemi-definite\u201d to \u201cpositive semi-definite.\u201d\n\n\nFirst line in Page 4 - you forgot period: \u201ceigenvalues are in close correspondence.\u201d\n\nEquation (2): the first approximation is wrong. For example, take $K=1$ and $w_1=1$, and note that $P$ does not approximate $L$. Do you mean that there is a choice of DIFFERENT coefficients $w\u2019_k$ for the polynomial in $P$ that gives an approximation? In that case, you can get an exact equality.\nAlso, the powers of $L$ should be small $k$.\n\nTwo lines below (2): change \u201cdesiring matrix\u201d to \u201cdesired matrix\u201d.\n\nTheorem 3.2 is formulated in a confusing way. Writing ``we can construct an $\\epsilon$ sparsifier\u2019\u2019 sounds like an existence claim, but what you are trying to say is that in probability $1-K/n$ Algorithm 1 gives an $\\epsilon$ sparsifier.\n\nThere are other papers about subsampling graphs that get rid of the dependency on the degree of the graph. For example, see Theorem, 1 in [N Keriven, A Bietti, S Vaiter. Convergence and Stability of Graph Convolutional Networks on Large Random Graphs]. There, to get an $\\epsilon$ error you need to sample $O(1/\\epsilon^2)$ nodes, which is independent of the size of the graph. How do you explain the slower asymptotics in your results? What do you gain with respect to the past analyses on account of slower asymptotics? You need to discuss this in detail.\n\nWhy would computing random edges make things faster for sparse L? If the number of edges in L is O(n), then already computing $L_K$ takes O(Kn) operations. In your method you need $O(K^2n)$ operations to construct the whole polynomial.\nPerhaps your method is only useful for dense graphs? More accurately, when the number of edges is >> the number of nodes? However, it is well known that you can approximate such dense graph shift operators via Monte Carlo sampling the nodes, as I wrote above. Please motivate your method accordingly.  For example, you can compare the complexity to methods that directly apply $L$ on $X$ as many times as needed for the polynomial, assuming that the number of edges is $m=O(n^a)$ where $n$ is the number of nodes and $a$ is between 1 and 2.\n\n\nSection 4: please define effective resistance.\n\nPage 7: please add the reference \u201cThis sparsifier can be further reduced to O(n log n/\u03b52 ) by the existing works []\u201d\n\n\u201cNote that the proposed bound is much tighter than what is practically required. In practice, the sampling number can be set rather small to achieve the desired performance\u201d  - you mean, the proposed bound is much higher than\u2026?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7522/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698494992374,
        "cdate": 1698494992374,
        "tmdate": 1699636908631,
        "mdate": 1699636908631,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hgAm5B7zfi",
        "forum": "qi88abxiE4",
        "replyto": "qi88abxiE4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7522/Reviewer_jUvN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7522/Reviewer_jUvN"
        ],
        "content": {
            "summary": {
                "value": "This work leverages prior random-walk-based spectral sparsification for improving the scalability of GNNs. Compared with prior works, the proposed framework allows for end-to-end training of GNNs. This framework allows approximating the equivalent propagation matrix of Laplacian filters, making it compatible with existing scalable techniques. In addition, rigorous mathematical proofs have been provided to support the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It is an interesting idea to leverage spectral sparsification for improving the scalability of the GNN training phase. \n2. Rigorous mathematical proofs have been provided to support the proposed method."
            },
            "weaknesses": {
                "value": "1. It would be helpful if there were more detailed discussions and explanations when claiming that \"our models show a slight performance advantage over the corresponding base models\" on the heterophilous datasets proposed by Lim et al. (2021), and when discussing \"approximating the desired complex filters tailored to the heterophilous graphs.\"\n2. The theoretical analysis in this work is mostly based on the prior work of \"Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, and Shang-Hua Teng. Spectral sparsification of random-walk matrix polynomials. CoRR, abs/1502.03496, 2015,\" and is a bit incremental.\n3. The proposed framework has many hyperparameters, which may make it impractical for use in real-world problems.\n4. The writing of the paper should be significantly improved. There are even missing references, such as \"... This sparsifier can be further reduced to O(n log n/\u03b52) by the existing works [].\" on page 7.\n5. The experimental results are not encouraging: spectral sparsification only produces marginal improvement for a few heterophilic graph datasets but degraded performance for well-known datasets."
            },
            "questions": {
                "value": "1. What's the percentage of edges that were retained after using the proposed spectral sparsification technique in GNN training? \n2. Is there any reduction in the overall GNN training time?\n2. How to determine the spectral similarity (\\epsilon) in the spectral sparsification step?\n3. What is the connection between spectral similarity for spectral sparsification and the final GNN performance (accuracy)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7522/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698861065941,
        "cdate": 1698861065941,
        "tmdate": 1699636908483,
        "mdate": 1699636908483,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CsdQ0491ew",
        "forum": "qi88abxiE4",
        "replyto": "qi88abxiE4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7522/Reviewer_aEiE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7522/Reviewer_aEiE"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new approach for scaling spectral graph neural networks. Unlike previous efforts that focused on preprocessing feature propagation steps, the proposal relies on Laplacian sparsification, which aims to obtain a sparse graph that retains the spectral properties of the original graph. Experiments using small-scale and large-scale node classification tasks aim to show the effectiveness of the proposal."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper tackles the very relevant issue of the scalability of spectral graph neural networks --- the motivation is clear and strong;\n- Results on Ogbn-papers100M demonstrate the scalability of the proposed method."
            },
            "weaknesses": {
                "value": "- Overall, it is unclear if the proposed idea only applies to linear-in-the-parameters spectral GNNs. The definition of spectral GNNs (in the introduction) says they take the form $Y=g_w(L, f_{\\theta}(X))$ where $g$ is a polynomial graph filter and $f$ is a linear layer. However, this formulation seems very restrictive and, for instance, does not encompass a simple 2-layer GCN. \n- Although the motivation focuses on scalability, the experiments only measure predictive performance. I expected to see an extensive comparison of memory usage and wall-clock time for different methods and datasets. In addition, the paper should report error bars for assessing statistical significance.\n- The paper only applies the proposed idea to APPNP and GPR GNNs. I would like to see results for other spectral GNNs (e.g., JacobiConv).\n- The theory does not seem particularly useful since implementing GNNs involves non-linearities, rendering gradient estimates biased. Furthermore, results stem almost directly from previous works."
            },
            "questions": {
                "value": "1. We could also design spectral GNNs by stacking layers of polynomial spectral filters interleaved with ReLU activation functions. Does the proposed approach apply to such models? Would it affect the theoretical analysis?\n2. What is the improvement in efficiency by applying the node-wise sampling method (section 3.3)? It would be useful to include some numbers in the Appendix.\n3. Is the sampling (sparsification) procedure applied at each forward pass or only once before training?\n4. The statement of Theorem 4.1 seems to be an imprecise version of Theorem 2.2 of Cheng et al. 2015 --- it is unclear what is the random variable in the modified statement.\n\n\nMinor comments/suggestions:\n1. The sentence \"while keeping the number of non-zeros within an acceptable range\" in Contribution is unclear. I would briefly explain the idea behind Laplacian sparsification in the introduction for clarity. One or two sentences should be enough.\n2. I think the claim '[...] which is the first work tackling the scalability issue of spectral GNNs' is misleading since GCN can be viewed as a spectral GNN, and other works (e.g., SGC, LanczosNet) have tackled scalability issues of GCNs.\n3. There is a significant overlap of ideas in section 3.1 and 1. I suggest reducing the overlap for readability.\n4. Please point out the exact Theorem in the paper (Cheng et al. 2015) when saying: 'We have extended the original theorem proposed by (Cheng et. al, 2015) ...'. Also, I suggest creating a specific subsection to prove Theorem 3.2 (as you have done for Theorem 4.3) --- I found the discussion in A.1 overloaded.\n5. Some notation is introduced in Algorithm 1, such as e_u and e_v. Is  e=(e_u, e_v) in step 1 of Algorithm 1?\n6. 'Some of the early classic GNNs, like GCNs, employ static Laplacian polynomials as the graph filter.' This is questionable since the coefficients of the linear layer can be viewed as multi-head spectral coefficients --- in fact, GCNs were introduced this way.\n7. What is $\\alpha$ in Section 3.2.1?\n8. $w$ has been used to denote both polynomial filter coefficients and weights in weighted graphs (Definition 3.1).\n9. The first identity of Eq. (2) should be $L^k$ instead of $L^K$.\n10. I would include the main algorithm (node-wise procedure) in the main text (btw, there is no appendix 8).\n11. Could you elaborate on the last identity in Eq. (2)? Or provide pointers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7522/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699128986559,
        "cdate": 1699128986559,
        "tmdate": 1699636908376,
        "mdate": 1699636908376,
        "license": "CC BY 4.0",
        "version": 2
    }
]