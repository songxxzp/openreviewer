[
    {
        "id": "6RXiz6evZq",
        "forum": "LjivA1SLZ6",
        "replyto": "LjivA1SLZ6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7321/Reviewer_mSGj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7321/Reviewer_mSGj"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to improve the efficiency in multi-agent reinforcement learning (MARL). It leverages episodic memory and introduces episodic incentive to help exploring desirable trajectory. This paper demonstrate both theoretical analyses and empirical results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This paper provides comprehensive theoretical analyses and strong performance improvement. The paper proves that the approach can help policies converge to the optimal policies. The paper also shows the great performance in Google football and StarCraft.\n* This paper is well-structured and written. The paper provides full details about the method and the experiment. It also constructs detailed ablation studies."
            },
            "weaknesses": {
                "value": "* I have concerns about the desirable trajectory. In paper, the author set $R_{thr}=R_{max}$. Since the desirable trajectories are the states that can achieve maximum returns, they must be the optimal states. What if the agents are impossible to achieve $R_{max}$. How to determine $R_{thr}$ in other environments?\n* The dimension of $x$ is very small (i.e. 4 according to table 4 in the appendix). It's doubtful that it can reconstruct the global state."
            },
            "questions": {
                "value": "* See weakness\n* The approach adopts a state embedding instead of random projection. Does this make the approach more hard to converge?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7321/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698408805847,
        "cdate": 1698408805847,
        "tmdate": 1699636875260,
        "mdate": 1699636875260,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Pjw4Yc97L7",
        "forum": "LjivA1SLZ6",
        "replyto": "LjivA1SLZ6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7321/Reviewer_7Wwz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7321/Reviewer_7Wwz"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the Efficient episodic Memory Utilization (EMU) for cooperative multi-agent reinforcement learning (MARL). Addressing the challenges in MARL where agents often get trapped in local optima, EMU aims to accelerate learning by leveraging a semantically coherent episodic memory buffer and selectively promoting desirable transitions. EMU uses an encoder/decoder structure to train semantically coherent episodic memory and introduces an episodic incentive reward structure to enhance performance. The proposed method is evaluated on benchmarks like StarCraft II and Google Research Football, demonstrating its superiority over existing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is well motivated and well written. Particularly, its visual illustration of Figure 2 and 3 are helpful.\n2. Although the proposed idea of semantically coherent memory looks simple to use an AE structure, it seems not being explored in multi-agent settings. One potential related work is generalized episodic memory, which can also be regarded semantically coherent episodic memory. \n3. The episodic incentive is interesting and looks effective.\n4. The proposed method shows strong empirical results. Its ablation studies are extensively conducted.\n5. This paper conducts a sufficient review of related work and is well positioned."
            },
            "weaknesses": {
                "value": "1. EMU needs to set a return threshold to determine the desirability of a trajectory. This may require some domain knowledge to properly determine it, even when using R_{max}. This knowledge may partially explain its outperformance. \n2. When the key encoder is updated, the proposed method needs to update all keys in the memory, which seems quite computationally intensive. \n3. It may be interesting to compare the proposed method with MAPPO."
            },
            "questions": {
                "value": "1. Can the author explain how to set the return threshold for desirability in experiments? Is this threshold dynamic or fixed?\n2. Is the incentive reward only effective when a trajectory is desirable? Does this mean episodic memory is useful only when a very good trajectory is explored, which can be hard?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7321/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698718168866,
        "cdate": 1698718168866,
        "tmdate": 1699636875134,
        "mdate": 1699636875134,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XGUIrsORrq",
        "forum": "LjivA1SLZ6",
        "replyto": "LjivA1SLZ6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7321/Reviewer_t4Jc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7321/Reviewer_t4Jc"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new framework called Efficient episodic Memory Utilization (EMU) to effectively exploit episodic memory for cooperative multi-agent reinforcement learning (MARL). EMU mainly relies on two features: \n 1) A learned semantic embedding embedding that allows to easily pair similar states\n 2) An \"episodic incentive\" mechanism to select the most useful transitions from the buffer when learning"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper presents a novel framework and studies the effect of episodic memory in MARL, which to the best of my knowledge is an underexplored area. The theoretical foundation is good although sometimes it is difficult to grasp the motivation and intuition of some parts when first presented. The paper also includes a sound analysis of the performance of EMU in two relevant MARL settings with abundant ablations that help to understand the contributions of the different features."
            },
            "weaknesses": {
                "value": "The biggest weakness of this work at its current state is the limited scope of the literature review and the lack of comparison with existing methods for episodic memory. There are multiple works ([1-3] to name a few) that have created similar frameworks in single-agent settings, specially in exploration settings. So one wonders what prevents port these frameworks here? Without that for instance the embedding procedure of EMU could be a reinventing the wheel from existing procedures in [1,2]. I strongly encourage authors to visit that line of works and contrast those approaches with the features incorporated in EMU.\n\nMoreover, I believe that the comparison with related work is imperative to understand the position of the paper and its contributions and should not be relegated to the appendix. \n\n\nAs a minor issue, writing also should be reviewed, but clarity in general is good\n\n[1] Henaff, M., Raileanu, R., Jiang, M., & Rockt\u00e4schel, T. (2022). Exploration via elliptical episodic bonuses. Advances in Neural Information Processing Systems, 35, 37631-37646.\n[2] Le, H., Do, K., Nguyen, D., & Venkatesh, S. (2023). Intrinsic Motivation via Surprise Memory. arXiv preprint arXiv:2308.04836.\n[3] Fernandes, D. M., Kaushik, P., Shukla, H., & Surampudi, B. R. (2022). Momentum Boosted Episodic Memory for Improving Learning in Long-Tailed RL Environments.\n\n\n\n---- Post Second Rebuttal ---\n\nI want to thank the authors for the additional work to address the concerns, specially what you wrote in your last response from\n\"It seems that adding a surprise-based incentive can be\".... until \"For these reasons, incorporating the feature embedding structure from the single-RL domain and its learning framework into the multi-agent domain may necessitate extensive modification, resulting in a distinct line of research.\" was the kind of motivation and comparison that I was looking for. \n\nThis, together with the results in Appendix D.13 makes clear that the existing methods from single agent literature is not as good as EMU in this context and that indeed it is a relevant contribution that authors are giving to the community.\n\nMy only remaining comment is that none of this new work and the discussion is present in the main document, (at least I don-t see anything in magenta there) I would encourage the authors to incorporate a reference in the introduction highlighting that existing methods in single agent reinforcement learning are not valid here (incorporating a reference to this part of the appendix),  \n\nI believe now that the paper is sound and there is enough evidence to support the main claims from the authors. I am updating my score accordingly."
            },
            "questions": {
                "value": "Beyond my recommendations above regarding writing, there is a common abuse of \"the\" through the text, e.g. \"In spite of the required exploration in MARL with CTDE, ${the}$ recent works on episodic control emphasize the exploitation of episodic memory to expedite reinforcement learning. Episodic control (Lengyel & Dayan, 2007; Blundell et al., 2016; Lin et al., 2018; Pritzel et al., 2017) memorizes ${the}$ explored...\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7321/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7321/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7321/Reviewer_t4Jc"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7321/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772857731,
        "cdate": 1698772857731,
        "tmdate": 1700859141414,
        "mdate": 1700859141414,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5XxzEopSkQ",
        "forum": "LjivA1SLZ6",
        "replyto": "LjivA1SLZ6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7321/Reviewer_8Ngy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7321/Reviewer_8Ngy"
        ],
        "content": {
            "summary": {
                "value": "This work presents a new framework for co-operative multi-agent RL that uses semantic memory embeddings to construct a novel reward structure that augments the environment reward by incentivizing desirable transitions. The framework, referred to as Efficient episodic Memory Utilization (EMU), comprises of an encoder-decoder network to learn semantically meaningful embeddings. The network is then utilized to obtain a reward that incentivizes desirable transitions. Experimental results in the benchmark Starcraft environments and Google Research Football demonstrate EMU\u2019s superior performance to existing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- **Clear writing and presentation:** Except for some minor subsections, the paper is generally well-written, easy to follow and presents a coherent story. \n- **Promising and extensive results**: The method outperforms existing works in standard benchmark domains. The work presents many experiments and ablation studies to analyze and demonstrate the effectiveness of different components of the proposes framework."
            },
            "weaknesses": {
                "value": "- **Scalability**: Based on the encoder-decoder architecture of the paper, I am assuming that the global state for the environments used is feature-based. It is unclear whether this method will scale to vision-based environments due to **a)** the memory requirements of storing many images, **b)** the optimization difficulty in reconstructing image-based states and **c)** the effectiveness of the introduced reward structure in high-dimensional state spaces. However, I acknowledge that many existing works in this area utilize feature-based observations. \nRegardless, it would greatly improve the strength of the results of this paper if some gains could be shown in vision-based environments.\n\n- **Evaluation**: It appears that random projection performs almost equivalently to EmbNet/dCAE when compared using test win rates. The introduction of a new metric (overall win-rate) highlights that EmbNet/dCAE enable faster/more sample-efficient learning. However, improvement on this new metric is not as significant as the original win rate (which is the standard benchmark in the community). I would be curious to see the curve for EMU with random projection added to **Sections 4.1** and **4.2** to better understand the significance of EmbNet/dCAE."
            },
            "questions": {
                "value": "1. Results for **1)** in Weaknesses. These set of results are not completely necessary but would be good to see. A well-reasoned argument about why the method should not be difficult to scale will also suffice. \n\n2. Results for **2)** in Weaknesses. \n\n3. It would be helpful to provide details about the state space, action space, environment reward and episode lengths for both SMAC and GRF in the Appendix. \n\n4. **Section 3.2** can use more intuition and better writing. It is unclear to me why the episodic inventive for a desirable transition is set to be proportional to the difference between the true value and the predicted value. Is this done to incentivize visits to states where the Q network has not converged?\n\n5. What are the implications of *Theorem 2*?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7321/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7321/Reviewer_8Ngy",
                    "ICLR.cc/2024/Conference/Submission7321/Senior_Area_Chairs"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7321/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796567927,
        "cdate": 1698796567927,
        "tmdate": 1700602192964,
        "mdate": 1700602192964,
        "license": "CC BY 4.0",
        "version": 2
    }
]