[
    {
        "id": "T0pBfrh14Z",
        "forum": "UJkgGbLfWA",
        "replyto": "UJkgGbLfWA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8094/Reviewer_UavC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8094/Reviewer_UavC"
        ],
        "content": {
            "summary": {
                "value": "This paper uses planning tokens to guide large language models for math reasoning problems.  The planning tokens are discrete latent variables, which are append to each reasoning step generated by chain-of-thoughts reasonings.  The authors explore both K-means and Soft-VAE based clustering methods to generate the planning tokens for each reasoning steps.  Empirical experiments on three math reasoning datasets with three language models show the proposed method is effective."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method is attractive and novel due to the combination with discrete latent variable models with large language models. \n2. The empirical improvements of LLaMa2 (13B) achieved by this method seem to be significant."
            },
            "weaknesses": {
                "value": "The main method is similar to prefix tuning.  However, there lacks of a strong baselines compared to prefix tuning.  For example, when the cluster number is equal to 5,  simply appending 5 trainable tokens in the front of each question should be a simple prefix-tuning baseline.  What are the performances? \n\n2. All the experiments are based on human annotated reasoning steps on math problems.  It is unknown that how well the proposed method can generalize for automatically produced reasoning steps for general reasoning problems.  Since the title is not specific to math problems,  in order to achieve the wide-range claim in the title, it is necessary to investigate more general reasoning problems. \n\n3.  When doing cluster for each reasoning step, the contextual information of each step seems not explored.  In section 2.3.2, to obtain the representation vector, each reasoning step is separately encoded.  The authors should consider a contextual version, which use the prefix reasoning steps and the question context to generate the representation vectors of the current step. \n4. Soft-VAE is not novel.  Why not simply using VQ-VAE? \n5. Although in the introduction, one important motivation is to address the issues of the reasoning steps progressively drifting away from the correct reasoning flow,  there are on such designs or reasoning in later sections of why the planned token helps on this issue.  In my view,  a hierarchical generation might be better correlated with this motivation.  First, given the question context.   Second, we tune the language models to generate the planning tokens.   Third, and then based on the question context and the planning tokens, for each reasoning step, we first copy a specific planning token generated from  the second step, and then output the reasoning process.  But in this method, the second step is missing, which I therefore hypothesize that there lacks a correspondance between the proposed method and the motivation.  \n6. The analysis of the planning tokens should be enriched. Currently, the analysis is quite weak and not enough."
            },
            "questions": {
                "value": "1. Do you do the clustering based on the training set or just do the clustering using the test set? \n2. In Section 3.1, why do you subsample 1K test samples from GSM8K and MATH?  Why not using all the test sets? \n3. The optimal setting of the number of cluster can be varied across datasets. It is better to also perform ablation studies on the other dataset. \n\nMinors: \nIn Table 2, the dataset (GSM8K) and the model (LLaMA2 7B) should be explicitly described."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8094/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698573117914,
        "cdate": 1698573117914,
        "tmdate": 1699637002777,
        "mdate": 1699637002777,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "chvPRYgjz9",
        "forum": "UJkgGbLfWA",
        "replyto": "UJkgGbLfWA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8094/Reviewer_9J4w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8094/Reviewer_9J4w"
        ],
        "content": {
            "summary": {
                "value": "To tackle the issue of lack of consistency among reasoning steps in solving math word problems, this paper introduces planning tokens to help with \u2018global\u2019 reasoning. Clustering  (Soft Q-VAE) is used to learn the planning tokens along with existing reasoning datasets and parameter efficient fine-tuning. Contributions include the different clustering methods and a detailed error analysis."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- Originality: Most existing work on improving reasoning focuses on better prompting techniques which makes the use of planning tokens more original. \n- Quality: The method development and experimental results are thorough. \n- Clarity: The paper was easy to understand. \n- Significance: Similar to originality, the planing tokens can be widely incorporated into many other ideas."
            },
            "weaknesses": {
                "value": "- The training process relies heavily on the annotated reasons which limit the flexibility of the model. \n- The performance on the MATH dataset is notably lower than the others."
            },
            "questions": {
                "value": "- Do the mistakes from the MATH dataset fall under the same error taxonomy (from section 3.3) as the GSM8K and Aqua datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8094/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8094/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8094/Reviewer_9J4w"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8094/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698623381540,
        "cdate": 1698623381540,
        "tmdate": 1699637002645,
        "mdate": 1699637002645,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E4eQeSo214",
        "forum": "UJkgGbLfWA",
        "replyto": "UJkgGbLfWA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8094/Reviewer_C3Cd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8094/Reviewer_C3Cd"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to augment \"planning tokens\" in reasoning text in fine-tuning LLM and update the embeddings of \"planning tokens\" together with other params. \n\nExperiments show that this trick has positive effects."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper tries multiple ways to infer latent planning token types. \n\nThe experiment results are solid."
            },
            "weaknesses": {
                "value": "The significance of this paper is low because tuning augmented tokens is not a new thing in NLP and the empirical findings in this paper are not interesting conditioned on previous work. I personally learned nothing from the paper. \n\nThe paper cites no previous work that learns augmented tokens. \n\nRelated references are: \n\nLi and Liang, 2021 Prefix-Tuning\nQin and Eisner, 2021 Learning how to ask"
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8094/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698721657872,
        "cdate": 1698721657872,
        "tmdate": 1699637002514,
        "mdate": 1699637002514,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XP3HmzAGY8",
        "forum": "UJkgGbLfWA",
        "replyto": "UJkgGbLfWA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8094/Reviewer_1L1z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8094/Reviewer_1L1z"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the way of using planning tokens to improve LLMs on reasoning consistency. Experiment results show that the proposed method outperforms finetuning LLMs directly. Analysis results provide intuitions to show how the proposed method works."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Generally, this is a good paper. The paper is well-written, and the idea is quite interesting. The proposed method is novel and comprehensive experiments are done to prove that the method works."
            },
            "weaknesses": {
                "value": "The proposed method is similar to prefix-tuning. This could be added as a baseline for comparison. Besides, there are some prompt-based methods targeted for reasoning consistency, which can also be added in experiments. Only comparing with full finetuning may not be enough."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8094/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769812398,
        "cdate": 1698769812398,
        "tmdate": 1699637002368,
        "mdate": 1699637002368,
        "license": "CC BY 4.0",
        "version": 2
    }
]