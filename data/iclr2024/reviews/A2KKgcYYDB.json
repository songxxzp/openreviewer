[
    {
        "id": "GzpXoR6NCS",
        "forum": "A2KKgcYYDB",
        "replyto": "A2KKgcYYDB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4252/Reviewer_fu3L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4252/Reviewer_fu3L"
        ],
        "content": {
            "summary": {
                "value": "The training dynamics of over-parameterized DEQs are revisited in this study. The authors extend prior studies on ReLU DEQs and establish the linear convergence of training DEQs with general activations using a unique population Gramme matrix and a new kind of dual activation with Hermite polynomical expansion."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper provides a fine-grained analysis of the gradient dynamics of DEQs. It extends the results of the ReLU case in [1] to more general cases.\n\n2. This paper proposes a novel population Gram matrix and develops a new form of dual activation with Hermite polynomial expansion. It appears that the proposed technical contributions can also be applied to the analysis of explicit neural networks.\n\n[1] Ling Z, Xie X, Wang Q, et al. Global convergence of over-parameterized deep equilibrium models. International Conference on Artificial Intelligence and Statistics. PMLR, 2023: 767-787."
            },
            "weaknesses": {
                "value": "1 About the weight assumption. The authors assume that $W_{ij}\\sim N(0,2\\sigma_w^2/m)$ and $U\\sim N(0,2/d)$. I do not understand the reason of using the scaling parameter \"2\". In ReLU case the scaling parameter \"2\" is commonly used for simplicity, but this paper investigates general activations.\n\n2 About the existence and the uniqueness of $K$ (Proposition 12). In order to make sure Eq. (112) holds, one needs to make sure $2q^2\\tilde{L}_q\\sigma_w^2<1$ where \n$\\tilde{L}_q=\\frac{16L^2}{q^2} (\\frac{\\sigma_w^2}{m} \\mathbb{E}G+\\frac{3}{2})$\n(as implied by Eq.(111)). However, Proposition 12 only requires that Assumptions 1and 2 hold, i.e. $\\sigma_w^2<\\frac{1}{8L^2}$. I do not think this condition is sufficient to guarantee $2q^2\\tilde{L}_q\\sigma_w^2<1$.\n\n Moreover,  the properties of $\\mathbb{E}G_{11}$ are unclear. This makes the proof less rigorous.\n\n3 Lemma 10 (Proof of Lemma 4 in [1]) plays a key role in the proof. However, [1]' proof works for ReLU function. The authors should explain the applicability of the proof to general activation functions."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4252/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698069494263,
        "cdate": 1698069494263,
        "tmdate": 1699636392405,
        "mdate": 1699636392405,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nwYeHeqpmV",
        "forum": "A2KKgcYYDB",
        "replyto": "A2KKgcYYDB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4252/Reviewer_SDT1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4252/Reviewer_SDT1"
        ],
        "content": {
            "summary": {
                "value": "The authors extend the framework by Ling et al. who showed linear convergence rate for the gradient descent applied to the quadratic loss function for over-parametrized Deep Equilibrium Model (DEQ) with ReLU activation. The same rate is obtained when ReLU is replaced by an activation function with bounded first and second derivatives. To obtain this rate, the authors bound the least eigenvalue of the Gram matrix of the equilibrium point by means of Hermite polynomial expansions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The assumptions required in the main theorem 8 are fulfilled for commonly used activation functions such as sine and tanh.\n\nThe claimed theoretical statements evidence are supported by numerical experiments on MNIST and CFAR-10 datasets.  \n\nThe analysis techniques based on dual activation with Hermite polynomial expansion seem somehow original and elegant."
            },
            "weaknesses": {
                "value": "The authors state several auxiliary results in Section 5 that are essential for their main theorem but are not proved in the main the appendix. \nNot being an expert in this field and with a very limited amount of time (too short to read in detail the 21 pages of supplementary material), it is quite hard to judge whether the framework is correct or not. I believe that this work, possibly sound and surely interesting, would be worth publishing but the current conference format (coming together with a short allocated review time) might not be the best fit."
            },
            "questions": {
                "value": "Could you give examples of activation functions that would not fulfill the conditions of Theorem 8?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4252/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4252/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4252/Reviewer_SDT1"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4252/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698271235822,
        "cdate": 1698271235822,
        "tmdate": 1699636392311,
        "mdate": 1699636392311,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "40AomMMIsO",
        "forum": "A2KKgcYYDB",
        "replyto": "A2KKgcYYDB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4252/Reviewer_pmW6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4252/Reviewer_pmW6"
        ],
        "content": {
            "summary": {
                "value": "This paper extends a previous result from Ling et al. on the global convergence of DEQs proved for ReLU activations to a more general class of activations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- the problem of understanding the theory of DEQs is interesting"
            },
            "weaknesses": {
                "value": "- **contribution**: it is not clear what is the exact contribution of this work. To me it seems that it's merely an extension of the work of Ling et al. but it doesn't bring new theoretical ideas, proof techniques or closes a gap between theory and practice. This is largely reflected by the fact that a large portion of the text is extremely similar to the paper of Ling et al. It can also be seen in the fact that entire parts of the paper are dedicated to things that would usually be in the appendix like extended proofs, extended historical perspectives on generalization or examples. \nMoreover, while there is a claim that \"a novel population Gram matrix\" or \"new form of dual activation with Hermite polynomial expansion\" are introduced in this work, it is clear from reading them that they are direct extensions of Ling et al. or Daniely et al.\n- **clarity**: so many notations are introduced (some even very unusual like $T$ for the equilibrium point of DEQs) which makes the paper difficult to follow."
            },
            "questions": {
                "value": "- what are the contributions of this work on top of extending the proof of Ling et al. to other activation functions?\n- why is it important to extend the proof of Ling et al. to other activation functions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Several parts of the submission are directly copy-pasted from the Ling et al. paper inspiring this work:\n\n- The first 4 paragraphs of the problem settings section, with just a slight change of notation. This change of notation appears to be made only to obfuscate plagiarism identification though since it's so bizarre: indeed $z$ is always used to denote the fixed point or equilibrium point of DEQs, rather than $t$ as introduced in the paper.\nOf course the problem setting was always going to be the same, but it's weird to make this slight unusual notation change and not clearly state that the whole problem setting section is a copy-paste of the Ling et al. work.\n\n- Some parts of the introduction also seem to have been copy-pasted in a somewhat random order.\n\n- The first part of Section 3 (up till theorem 4) is copy pasted from the \"2.2 Well-Posedness and Gradients\" section, with just an unjustified offset in the lemma numbering."
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4252/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698695361587,
        "cdate": 1698695361587,
        "tmdate": 1699636392242,
        "mdate": 1699636392242,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "StepMhCxxV",
        "forum": "A2KKgcYYDB",
        "replyto": "A2KKgcYYDB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4252/Reviewer_ubP5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4252/Reviewer_ubP5"
        ],
        "content": {
            "summary": {
                "value": "This paper extends the NTK-like analysis of wide Deep Equilibrium Models converging linearly with gradient descent, which was previously proven only for the ReLU activation ([Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf)), to encompass general activation functions through advanced analysis of the Gram matrix.\n\nSpecifically, Deep Equilibrium Model ([Bai et al. (2019)](https://arxiv.org/abs/1909.01377)) defines the model output as the equilibrium value of a recursive equation, which corresponds to the output of infinitely-deep network with the same weight across all layers. The linear convergence of the overparameterized (wide) Deep Equilibrium Model using gradient descent is proven in ([Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf)) for the ReLU activation, following the NTK analysis. The proof of linear convergence requires lower bounding a certain gram matrix. To extend the result for ReLU to general activation functions, the lower bound argument should be more abstract, which is the main contribution of this paper."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "### Convergence of Deep Equilibrium Model has not been proven for general activation functions\n\nThe previous work for the ReLU activation ([Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf)) provides a detailed literature review on the NTK-like analysis of Deep Equilibrium Models. As far as I understand, [Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf) is the first non-asymptotic analysis for the ReLU and there are no subsequent work for general activation functions. Thus I think the problem this paper addresses itself is new to a certain extent.\n\n### All proofs seem to be correct at high level.\n\nWhile there are some rough edges (e.g., in Lemma 2, a constant $C$ is introduced but not used anywhere in the statement), the overall argument leading to the theorem appears to be sound."
            },
            "weaknesses": {
                "value": "### The proof follows [Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf), and the modification required for dealing with general activation functions looks very basic.\n\nIn reviewing this paper, I have thoroughly examined the proof method of [Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf), which is a prior study. Through this examination, I have identified that the fundamental flow of the proof in this paper is essentially the same. For example, the following correspondence exists:\n\nTheorem 2 in [Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf) - Theorem 7\n\nTheorem 3 in [Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf) - Theorem 8\n\nWhile the introduction of Hermite decomposition distinguishes it from the case limited to ReLU, it is worth noting that a significant portion of the proof remains identical to the original one. In evaluating this paper, it seems that the crucial point to consider is the novelty and significance of using Hermite decomposition to establish a lower bound on the kernel's eigenvalues, especially in comparison to the prior work that did not incorporate this technique. However, I am aware that such an idea is widely used in other relevant literature (e.g., [Misiakiewicz (2022)](https://arxiv.org/abs/2204.10425)). Due to these reasons, the technical contribution of this paper appears somewhat incremental, which is why I have reservations about recommending its acceptance.\n\n### The literature review appears to be lacking in depth.\n\nThis paper seems to have overlooked several works on the application of NTK to DEQ, which are thoroughly explained in [Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf), and only mentioning [Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf). The Introduction chapter appears to begin with a very general discussion (introduction to deep learning) while omitting a review of literature directly relevant to this paper. As my suggestion, it might be beneficial to introduce DEQ first, followed by a presentation of existing analyses and challenges associated with it. This approach could provide a more comprehensive overview within a similar page length.\n\n### The proof sketch is a mere list of claims.\n\nThe paper seems to just list theorems without providing sufficient explanations about what is fundamentally novel. Many of these theorems can be linked to references in prior literature. In my understanding, the novelty lies in the proof methods of Theorem 7 and 8, so please provide detailed explanations about them.\n\nFurthermore, Section 6 appears to be overly verbose. In my opinion, it could be expected to hold, and even if it doesn't, the focus should be on proving it for just one activation function.\n\n### (Minor) ``any general activation that has bounded first and second derivatives.'' (abstract) requires modification\n\nIn Theorem 8, the authors assume non-vanishing Taylor coefficients on the dual activation function. I do not think think bounded first and second derivatives suffice to satisfy this assumption."
            },
            "questions": {
                "value": "- It might be helpful if the paper could illustrate the challenges and difficulties of dealing with general activation functions by contrasting the proof methods with those from previous literature, which could highlight aspects that I might have overlooked.\n\n- Could you provide more detailed information about the relevant literature?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4252/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4252/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4252/Reviewer_ubP5"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4252/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699113474308,
        "cdate": 1699113474308,
        "tmdate": 1699636392157,
        "mdate": 1699636392157,
        "license": "CC BY 4.0",
        "version": 2
    }
]