[
    {
        "id": "KUMoyBiF0r",
        "forum": "NdbUfhttc1",
        "replyto": "NdbUfhttc1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4847/Reviewer_wz35"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4847/Reviewer_wz35"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method for meta-learned optimization in RL, named Optim4RL. This consists of three components - pipeline training, gradient transformations, and an update formulation - which are designed to tackle specific issues in this setting. The evaluation investigates the performance of this method when meta-trained on simple grid-world environments and evaluated on much more challenging environments, primarily evaluating against Adam and RMSProp, in addition to a collection of alternative meta-learned optimizers on in-distribution tasks.\n\nI am recommending rejection for this paper, primarily due to the lack of ablations. However, I would be very willing to increase my score if the suggested ablations were performed, in addition to a discussion of the wider literature for this problem setting."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Related work extensively covers RL optimization and meta-optimization literature.\n2. The paper is impressively well-written and structured. Section 4 is particularly well-structured, presenting a clear set of hypotheses about the problems with meta-optimization in RL.\n3. Each proposed component is simple, but clearly motivated and presented in Section 5 and Figure 3.\n4. Many related methods are compared against Optim4RL in Figure 4, however, these could be evaluated further (see weaknesses)."
            },
            "weaknesses": {
                "value": "1. The predominant flaw with this paper is the evaluation of the proposed components. Section 5 is highly systematic in motivating each problem, before proposing a component as a solution. However, the evaluation does not ablate the components, making it impossible to discern their individual impact. The exception to this is LinearOptim, which is an ablation of the proposed inductive bias, however, I believe this should be highlighted. Whilst the comparison to existing baselines is interesting, this is a __fundamental__ requirement when evaluating a model composed of multiple novel components.\n2. The presentation of the results could be clearer for drawing conclusions. Whilst training curves are useful, they make it difficult to quantitatively determine significance.\n3. The wide range of baselines in Figure 4 is good, but it is unclear why these are not carried forward for the remainder of the evaluation. Given that meta-training is the largest computational cost, these shouldn't be out-of-budget to run. In particular, STAR is performative enough that it is plausible it would achieve competitive performance on the remaining tasks, which should be investigated.\n4. The inability of LinearOptim to learn anything is very surprising and should be investigated further to ensure it is not erroneous.\n5. There is a broad base of related work on meta-learned RL objective functions, which is not discussed or compared against. While these are a different class of inductive bias, they are solving the same problem as the class of meta-optimizers discussed here. Notably, the evaluation procedure and environments are from Oh et al. (2020), a learned objective algorithm, but it is not compared against. While it is understandable to not compare Optim4RL against all of these methods, they should at least be discussed as alternative approaches to the same problem in the related work. Namely, EPG (Houthooft et al., 2018), LPG (Oh et al., 2020), MetaGenRL (Kirsch et al., 2020), ML^3 (Bechtle et al., 2021), SymLA (Kirsch et al., 2022), DPO (Lu et al., 2022), GROOVE (Jackson et al., 2023).\n6. Many of the claims are misleading or ambiguous. In conclusion, the claim that Optim4RL is \"the first learned optimizer that can be meta-learned to optimize RL tasks entirely from scratch\" is confusing, since all existing learned optimizers can be and are applied to RL in this paper. If this is intended to claim this is the first meta-learned optimizer designed for RL, then the omission meta-learned objective function literature becomes even more apparent, since there is extensive work solving the same problem for RL."
            },
            "questions": {
                "value": "1. Major typo in Algorithm 1: the sign and magnitude of your update are both computed from o_1. I assume this is a typo since, if correct, your method would only be capable of outputting large positive updates and small negative updates.\n2. Transformation of the gradient output is a major component of gradient processing, but only the input transformations are discussed in the main body. An expanded form of the output could be presented in the main body.\n3. In Figure 5, you suggest the overlap in the support of the gradients is a predictor of performance. A simple experiment to evaluate this would be retraining the optimizer with rescaled rewards on the grid-world tasks, which would shift the support of the meta-training gradients. If this improved performance, this would significantly strengthen the hypothesis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4847/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4847/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4847/Reviewer_wz35"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4847/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698598474807,
        "cdate": 1698598474807,
        "tmdate": 1699636468413,
        "mdate": 1699636468413,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "91MqdU1Gi4",
        "forum": "NdbUfhttc1",
        "replyto": "NdbUfhttc1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4847/Reviewer_8FzQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4847/Reviewer_8FzQ"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a meta-learning procedure for learning optimizers for reinforcement learning called Optim4RL. Their method has the following key components:\n- Pipeline training: Use multiple agents each at different training stages (early/mid/late) to make data distribution (more) stationary. \n- Gradient pre-processing: Transform the gradient so that the input is sensitive to changes of the gradient in (approximately) log space. \n- Inductive bias: Structure the update using a form similar to Adam, providing good inductive bias\n\nIn their experiments Optim4RL is shown to (1) outperform existing learnt optimizers, and (2) generalise to problems outside of the training set."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Each of the parts of the proposed learnt optimizer solve important problems in meta-learning optimizers, and are well motivated. \n- The toy problem where the optimizer has to learn the identity function is simple and informative. \n- The paper has informative analysis on the gradient distribution (e.g. I like the plots in Figure 5 visualising the train-test marginal distribution of gradients). \n- Achieves generalisation to different tasks (Brax) from simple grid based problems. \n- Generally the paper is clear and easy to follow."
            },
            "weaknesses": {
                "value": "- The optimizer achieves marginally worse performance than Adam on the tasks that it is meta-trained on. It seems like the learnt optimizer should at least be able to \"overfit\" to the training task to outperform Adam here. \n- On unseen tasks the optimizer is significantly worse than Adam. \n- The training and test tasks are relatively toy problems (the authors do acknowledge this weaknesses)."
            },
            "questions": {
                "value": "- The below text confused me - why do we need to check that the model has enough capacity to represent the identity function (e.g. a single linear layer can represent the identity function easily)?\nAlso, why do we need an RNN on this problem (is the input not just the current gradient?)? \n\n> To verify that the model has enough expressiveness and capacity to represent an identity function, we further train it with randomly generated data where data size is similar to the data size of the agent-gradients.\n\n- Transforming the gradient passed to the RNN into a richer representation makes sense. Additionally this seems to help a lot in terms of performance so it seems worth digging deeper into. Were other transformations tried - e.g. fourier feature embedding? \n\n- Would it be possible to add the STAR benchmark to Figure 4 (ant), and for Table 5? This would allow us to see how well Optim4RL generalizes relative to another learnt optimizer. \n\n- In figure 4 (b) it seems like STAR is starting to learn a bit. Is it possible that with a bit more hyper-parameter tuning it would match the other optimizers in performance? \n\nI acknowledge that a lot of my questions require more compute, and that this is a very compute heavy task. I do think that these would significantly strengthen the paper - as they would help make the results more decisive."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4847/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744554403,
        "cdate": 1698744554403,
        "tmdate": 1699636468335,
        "mdate": 1699636468335,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iktcj4xqAi",
        "forum": "NdbUfhttc1",
        "replyto": "NdbUfhttc1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4847/Reviewer_tcjw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4847/Reviewer_tcjw"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a new learned optimizer, Optim4RL, to address the challenges of using learned optimizers in RL.\n\nWhile learned optimization has shown benefits in the supervised learning community, SOTA optimizers for Supervised Learning (SL) fail in the RL setting. The authors investigate this phenomenon by analyzing the distribution of the gradients of agent parameters at the start, middle, and end of training. Through this analysis, the authors demonstrate that the gradients are non-I.I.D. Moreover, the absolute values of the gradients lie in a small range. The authors then demonstrate the difficulty of the RNN module -- commonly used in the learned optimizer -- to approximate an identity function using the gradient data (75% accuracy). Using this analysis, they underscore the bias and variance in the gradients as a key issue that makes learned optimization hard in RL and argue that this is further exacerbated by the bi-level optimization in learned optimizers. (poor optimizer -- poor policy -- lower quality data)\n\nThe authors then propose three key ways to mitigate these issues: \n- Gradient Processing: a 1-1- mapping that uses a log transformation to magnify absolute value differences between small gradients to mitigate the logarithmic gradient variation. This boosts the accuracy of the RNN on the identity task from 75% to 90%\n- Pipeline Training: add diversity to the gradient inputs to the learned optimizer through a distributed training regime by parallelly training multiple agents being reset at different periods and using all of their gradients for the learned optimizer. This mitigates the non-iid nature of the data since data now comes from different points of training\n- Biasing the optimizer: Building on the analysis of [Harrison et al., 2022], they utilize both the gradient and its squared value as inputs to two RNNs. This mitigates the need to approximate square roots and division by the learned optimizer and stabilizes the meta-update\n\nThe combination of these three components -- Optim4RL -- demonstrates improved stability and effectiveness in optimizing RL tasks compared to baselines of hand-designed optimizers (Adam and RMSProp), learned optimizers for Supervised learning (L2LGD$^2$, STAR, and VeLO), and linear parameter update instead of a squared\n\n[Harrison et al., 2022] Harrison, J., Metz, L., & Sohl-Dickstein, J. (2022). A closer look at learned optimization: Stability, robustness, and inductive biases. Advances in Neural Information Processing Systems, 35, 3758-3773."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "### Originality\nThe paper tackles a novel direction of RL-specific learned optimization by looking deeper into what kind of RL-specific inputs need to be adapted.\n\n### Quality\nThe work is insightful and generally conducted comprehensively.\n\n### Clarity\nThe paper is written clearly and understandably. Overall, the presentation is clear and well done.\n\n\n### Significance\nThe research direction is significant since learned optimization is yet to take hold in RL properly and is very important if achieved."
            },
            "weaknesses": {
                "value": "There seem to be a lot of central design decisions/hyperparameters in the training procedure that are not justified:\n- 4 inner updated per outer update\n- The decision to average returns over ten runs\n- The threshold for gradient processing\n- Epsilon in the parameter update\n\nThe agglomerative procedure to incorporate diversity in the gradient distribution seems not fully ablated. See my questions on this for further details.\n\nI am unsure if 10 GPU years is a realistically feasible budget for most practitioners. One of the issues with learned optimization in SL has been this exact problem. I think commentary on how to bring this cost down would be highly beneficial for hte community, especially given the recent surge in JAX-based parallelization with developments such as PureJAXRL (https://github.com/luchris429/purejaxrl)."
            },
            "questions": {
                "value": "- What happens when we don't do individual pre-processing steps?  -- Are there any ablations that demonstrate the effectiveness of individual modifications?\n- What constitutes the middle of training? is it the same for each environment or different across environments?\n- How many seeds were the experiments reported on? How did the authors determine them?\n- Resetting provides the optimizer data at different training stages. Have the authors analyzed how different values of m and n impact this? Do we require them to be equal all the time? \n- Given that pipeline training can be computationally expensive, have the authors examined methods to extract maximum benefit from this procedure? For example, could reset times be adapted by leveraging optimizer reset properties? [Asadi et al., 2023]\n- Does the learned optimizer mitigate the requirement for dynamic hyperparameter optimization [Mohan et al., 2023]? To what extent is the problem addressed in this work related to the AutoRL problem, given that there are still optimizer-related hyperparameters? \n\n[Asadi et al., 2023]  Asadi, K., Fakoor, R., & Sabach, S. (2023). Resetting the Optimizer in Deep RL: An Empirical Study. arXiv preprint arXiv:2306.17833.\n\n[Mohan et al, 2023] Mohan, A., Benjamins, C., Wienecke, K., Dockhorn, A., & Lindauer, M. (2023). AutoRL Hyperparameter Landscapes. AutoML Conference 2023 (https://openreview.net/forum?id=Ec09TcV_HKq)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4847/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764497166,
        "cdate": 1698764497166,
        "tmdate": 1699636468237,
        "mdate": 1699636468237,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RXdjbfoEWp",
        "forum": "NdbUfhttc1",
        "replyto": "NdbUfhttc1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4847/Reviewer_ssUw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4847/Reviewer_ssUw"
        ],
        "content": {
            "summary": {
                "value": "The authors propose meta-learning an optimizer for RL. They show that RL is uniquely challenging to optimize for. They then propose multiple techniques to learn an optimizer for RL from scratch that can generalize from toy tasks to Brax."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality:\n\n- This is a new problem setting that I have not seen before. It is clear that existing learned optimizers do not perform well in RL and it is good to see initial works in this direction.\n\n- The gradient processing is well-motivated and significant.\n\n- The architecture is also well-motivated and elegant.\n\nQuality:\n\n- The authors perform neat investigations into hypotheses about gradient-related challenges in RL.\n\n- The authors show impressive transfer performance.\n\nClarity:\n\n- The paper is very clearly written.\n\nSignificance:\n\n- This could ultimately lead to a superior optimizer for RL, which would be very significant."
            },
            "weaknesses": {
                "value": "Originality:\n\n- There is a section missing from the related works. In particular, it's the \"learning update rules / algorithms\" for RL literature. The setup seems to be *very closely related to* the setup from \"Discovering Reinforcement Learning Algorithms\" (LPG) [1], which is part of a broader field of meta-learning general RL update rules [2], [3]. \n\n- The pipeline training and the training setup of LPG seem closely related.\n\nClarity:\n\n- (Minor) Section 4.1: SeeAppendix B <= missing a space.\n\n- See clarification-related questions below.\n\nQuality:\n\n- On Section 4.1: The authors show that the agent-gradient distribution is non-IID. However, they do not show that the gradient distribution for normal supervised learning (SL) **is** IID. This is rather important to show, if the authors are claiming that RL is a uniquely challenging setting.\n\n- On Section 4.2: Again, the authors did not compare RL and SL, which is the purpose of this section. The authors could train a SL model and then see if the RNN can learn the identity function on the gradients from that training process. \n\n- The synthetic data is not representative of the SL training process. Furthermore, the injection of non-iid dynamics of the synthetic data seems to have been done ad-hoc and is not particularly meaningful. For example, what if the iid shift was far more extreme?\n\n- On Section 4.3: The problem of non-stationary targets is a well-studied phenomenon in RL, with plenty of possible prior works the authors could cite. This includes the deadly triad [4] and capacity loss [5]. \n\nSignificance:\n\n- The primary technical contributions seem to be the gradient processing, and the optimizer structure. The gradient processing is an impactful trick. The optimizer structure is hardly ablated or compared when there is plenty of literature on learned optimizer architectures.\n\n- The authors show limited transfer and the optimizer does not seem to generally perform on-par with Adam, despite being heavily inductively biased towards an Adam-like update.\n\n\n[1] Oh, Junhyuk, et al. \"Discovering reinforcement learning algorithms.\" Advances in Neural Information Processing Systems 33 (2020): 1060-1070.\n\n[2] Kirsch, Louis, Sjoerd van Steenkiste, and J\u00fcrgen Schmidhuber. \"Improving generalization in meta reinforcement learning using learned objectives.\" arXiv preprint arXiv:1910.04098 (2019).\n\n[3] Lu, Chris, et al. \"Discovered policy optimisation.\" Advances in Neural Information Processing Systems 35 (2022): 16455-16468.\n\n[4] Van Hasselt, Hado, et al. \"Deep reinforcement learning and the deadly triad.\" arXiv preprint arXiv:1812.02648 (2018).\n\n[5] Lyle, Clare, Mark Rowland, and Will Dabney. \"Understanding and preventing capacity loss in reinforcement learning.\" arXiv preprint arXiv:2204.09560 (2022)."
            },
            "questions": {
                "value": "1. What is the difference between your pipeline training and the pipeline training of LPG?\n\n2. Is pipeline training desirable? Ideally we want a non-myopic optimizer, and the training dynamics early on in training heavily affect the distribution of parameters at the end of training. Can you ablate this?\n\n3. Why is the neural network so small? Is this common in the literature?\n\n4. In Section 6.1: Are you re-training STAR and L2LSGD, or are you taking pre-trained weights?\n\n5. Why are the optimizers and environments different in each plot in Figure 4? (e.g. why is VeLO exclusively for Ant and STAR for big_dense_long). Can you generate a more complete plot here?\n\n6. Many of the learned optimizers use ES to train their learned optimizers. Is there any particular reason you decided not to do this?\n\n7. On Section 4.2: How did the authors choose the hyperparameters (the rate and total amount of change) for generating non-iid data? \n\n8. On \"Quality\" Weaknesses (4.1 and 4.2) from above: These seem easy for the authors to address and I would be very curious about the results!\n\n9. Is there a reason you did not try other architectures from the learned optimizer literature?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4847/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4847/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4847/Reviewer_ssUw"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4847/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809942054,
        "cdate": 1698809942054,
        "tmdate": 1700586152083,
        "mdate": 1700586152083,
        "license": "CC BY 4.0",
        "version": 2
    }
]