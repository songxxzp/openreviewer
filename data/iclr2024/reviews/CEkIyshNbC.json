[
    {
        "id": "P8xhLddXZx",
        "forum": "CEkIyshNbC",
        "replyto": "CEkIyshNbC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission287/Reviewer_Ea52"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission287/Reviewer_Ea52"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an empirical study of the relation between model capacity and generalization, while considering the learned feature space. In particular, it analyzes the phenomena of double descent, in which test performance improves after a certain interpolation threshold, contrary to the traditional U-shaped curve.  This work argues that models with small capacity first overfit to the noise present in the data and that overparameterized models then learn to \u201crecognize\u201d noisy labels, separating signal from noise."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "In contrast to most previous empirical works on double descent, which only analyze cross entropy loss, double descent in terms of accuracy is also studied. As shown in their plots (see Figures 1, 3 and 5) the double descent peak is considerably smaller (sometimes even absent) when looking at accuracy. tSNE visualizations give more intuition about the phenomenon, and experiments are performed for 3 models and 3 noise levels and 2 datasets."
            },
            "weaknesses": {
                "value": "- One main weakness is the fact that a big part of the paper is devoted to reproducing experiments from [1]. In these experiments, the only significant difference is the fact that they report both accuracy and cross entropy loss.\n\n- I fully agree with the author\u2019s comment on section 3 regarding tSNE maps: \u201cWhile this visualization may not accurately represent the intricate inherent structure of high-dimensional features, it aims to gain insights into the clustering and distribution of data points, thereby enhancing our understanding of the model\u2019s internal representations.\u201d While tSNE maps might give intuition about a phenomenon, they can also fabricate patterns and might not be sufficient evidence for a proposition.\n\n- The writing is sometimes confusing. For instance, in section 3.3: \u201cwe calculate the prediction accuracy Kp of noisy labelled data of the original clean labels for each noisy labeled data point matching the label prediction of its k-nearest neighbors in the learned feature space\u201d. \n\n Thus, I do not see substantial empirical nor theoretical contributions in this work. \n\n[1] DEEP DOUBLE DESCENT: WHERE BIGGER MODELS AND MORE DATA HURT Nakkiran et al."
            },
            "questions": {
                "value": "-"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission287/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission287/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission287/Reviewer_Ea52"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission287/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698335391490,
        "cdate": 1698335391490,
        "tmdate": 1700574271290,
        "mdate": 1700574271290,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "V9xQGxCmMR",
        "forum": "CEkIyshNbC",
        "replyto": "CEkIyshNbC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission287/Reviewer_d6tR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission287/Reviewer_d6tR"
        ],
        "content": {
            "summary": {
                "value": "The research aims to show that the phenomenon of double descent with respect to model size appears in neural networks due to the addition of noisy labels, i.e., smaller models learn this noise, while larger size models are able to avoid it and recognize wrong labels while learning useful information. The proposed approach to this aim is to measure the nearest neighbor prediction for mislabeled examples based on learned representations (on the penultimate layer) and then demonstrate that this prediction corresponds to the original (true) label, no matter that network \"correctly\" predicts wrong label. t-SNE visualizations of representations are used as an additional evidence to this point."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Investigation of the ability of large neural networks to interpolate noisy labels, but nevertheless have good generalization is an important contribution to the current research. The mechanisms behind this phenomenon shed light on the generalization abilities of such models."
            },
            "weaknesses": {
                "value": "The abstract of the paper states that the main contribution is to demonstrate that model-size double descent is happening only with label noise present in the dataset. Nevertheless, the experiment with CIFAR100in the paper directly disproves this claim already. Further, it is claimed that double descent will not happen in the correctly regularized networks, but no regularization is ever discussed in the paper itself, except for an unusual claim that overparameterization is a form of regularization (which is also never discussed in the paper). Together, this leaves only one possible contribution - explaining how interpolating networks deal with mislabeled examples. This is aimed to be explained through nearest neighbors classification on the representations for mislabeled examples - showing that this accuracy is very high for interpolating models. Nevertheless, only the MNIST experiment indeed demonstrates high accuracy of these predictions together with the high accuracy on noisy task, in other experiments this accuracy is considerably low. t-SNE visualizations can be simply explained by the ability of larger models to get a more distinguished representations for different classes, but the positioning of the noisy labeled ones does not change much (see class 0 in fig.2 for example). Thus, the last contribution is not significantly supported by the results.\n\nMinor:\n\n- please use \\citep and \\citet for the citations outside of the sentence and in the sentence\n\n- please check the grammar and typos in the paper \n\n- please improve the explanation of the formula (1), it is currently extremely convoluted\n\n- the font of the title does not correspond to ICLR style"
            },
            "questions": {
                "value": "1 - What is the main goal of the research done?\n\n2 - What is the observation made with respect to the selected metric (nearest neighbors) from the double descent plots?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission287/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission287/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission287/Reviewer_d6tR"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission287/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698417504534,
        "cdate": 1698417504534,
        "tmdate": 1700650881012,
        "mdate": 1700650881012,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zzWumKwgFB",
        "forum": "CEkIyshNbC",
        "replyto": "CEkIyshNbC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission287/Reviewer_9d7L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission287/Reviewer_9d7L"
        ],
        "content": {
            "summary": {
                "value": "This article focuses on \"Double Descent\" phenomenon, suggesting that the ability of a model to adapt to and recognize noisy data has a significant impact on its overall generalization performance and contributes to the phenomenon of double descent, and proposing the use of a k-nearest neighbor algorithm to infer the relative positions of clean and noisily labeled data in the learned feature space."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.This paper is feasible to understand the generalization ability and double descent phenomenon through the perspective of noise."
            },
            "weaknesses": {
                "value": "1.The methodology section of this paper (only 14 lines of text and only 6 lines of description of the actual methodology) is poorly described and lacks math-related descriptions, making it difficult to accurately understand the author's intention.\n\n2.Is it optimal to use K-nearest neighbors here? the feature space is probably in a high-dimensional Manifolds, have the authors considered more complex cases?\n\n3. I don't think this article convinces me enough about the mechanism by which it reveals that over-parameterization enhances generalization. The mechanism between over-parameterization and generalization cannot be revealed in depth only by the loss curves and T-sne images under a small number of experiments.\n\n4. It is well recognized that over-parameterization is beneficial, especially since recent large models have similarly benefited from over-parameterization, and the authors should discuss the advantages of this paper in the context of recent over-parameterization research."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission287/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672370789,
        "cdate": 1698672370789,
        "tmdate": 1699635954570,
        "mdate": 1699635954570,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ag9Zw8L0hd",
        "forum": "CEkIyshNbC",
        "replyto": "CEkIyshNbC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission287/Reviewer_P23K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission287/Reviewer_P23K"
        ],
        "content": {
            "summary": {
                "value": "The paper performs a series of empirical studies into the double descent phenomena. The main take-away is that double descent appears when the model is faced with label noise."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(I stress that I have low confidence in my review. It is highly likely that I missed something important)\n\n* The double descent is, currently, highly counter-intuitive to many, so any insights into the phenomena are valuable.\n* The stated hypothesis regarding label noise is clear and seems intuitive."
            },
            "weaknesses": {
                "value": "(I stress that I have low confidence in my review. It is highly likely that I missed something important)\n\n* As a non-expert, I interpret the experiments as showing that double descent phenomena appear with label noise in large models. I acknowledge that this is an interesting observation, but it does not seem sufficient to justify conclusions such as double descent not appearing in well-regularized models. I can see the intuition, but, in my non-expert view, the experimental data seems insufficient.\n* The paper considers only a small selection of data sets and models. It is not clear to me if such is sufficient to draw conclusions.\n* The paper does not provide a theoretical explanation (which is fine, but then I had wished for a wider selection of experiments).\n* I found it very difficult to read the t-SNE plots.\n* [minor] It appears that the ICLR formatting instructions were disregarded when changing the font size of the paper title."
            },
            "questions": {
                "value": "* What is the 'wedge product' in Eq. 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission287/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698674056255,
        "cdate": 1698674056255,
        "tmdate": 1699635954491,
        "mdate": 1699635954491,
        "license": "CC BY 4.0",
        "version": 2
    }
]