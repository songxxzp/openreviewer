[
    {
        "id": "3p0iVARIxZ",
        "forum": "QQYpgReSRk",
        "replyto": "QQYpgReSRk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4674/Reviewer_NPAD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4674/Reviewer_NPAD"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new vision foundation model, uses three different training strategies, and verifies the performance on the image retrieval task on the constructed dataset. Experimental results on the constructed large-scale data set verify the effectiveness of the proposed new model and training strategy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "It looks novel and promising to leverage  Image-to-Entities data to improve image retrieval performance using multi-task pre-training.\n\nAblation studies are pretty solid and comprehensive to reveal characteristics of the proposed method.\n\nThe analysis of different learning strategies is interesting. The paper provides a nice insight into exploiting image representation learning strategies for a particular task."
            },
            "weaknesses": {
                "value": "In the experiment section, my major concern is that it lacks comparisons with SOTA methods. Ideally, it is encouraged to have comparisons with SOTA methods from both traditional methods and methods based on foundation models. So it will show improvement and advances of the proposed method in this area.\n\nA lack of analyses on efficiencies. In particular, the performance gains from the method should be evaluated along with its time and memory complexity. Does the complex the model, the higher the performance?\n\nThe authors observe a similar performance gain on the ImageNet image retrieval task. Why does model performance improve roughly the same on both ImageNet and Image-to-Entities ? The author may want to clarify rationale behind their observations."
            },
            "questions": {
                "value": "I am wondering why using entity filtering images for multi-task pre-training helps the retrieval problem. This paper could benefit from illustrating more rationale behind using Image-to-Entities dataset.\n\nWhat are the major difference between multi-task pre-training methods employed in this paper and the contrastive learning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4674/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698371684424,
        "cdate": 1698371684424,
        "tmdate": 1699636448462,
        "mdate": 1699636448462,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZD3EFSmRV7",
        "forum": "QQYpgReSRk",
        "replyto": "QQYpgReSRk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4674/Reviewer_4Nss"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4674/Reviewer_4Nss"
        ],
        "content": {
            "summary": {
                "value": "This article primarily presents a billion-scale Image-to-Entities dataset, which includes 1 billion images and 2 million distinct entities. Based on this dataset, the authors have attempted a series of model training algorithms, including supervised learning, contrastive learning, and a combination of both in the form of multi-task learning. The constructed models have achieved state-of-the-art performance on the GPR1200 and VTAB datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ A large-scale image-to-entity dataset is provided, which has the potential to be reused by other pre-training models for vision-language related tasks.\n+ Based on this data, a pre-training model has been trained, which can serve as a general-purpose tool for tasks such as image classification and image retrieval."
            },
            "weaknesses": {
                "value": "+ There are a few unfair comparisons between MOFI and CLIP: (1) CLIP can handle text at the sentence level, serving as the base model for MLLM image inputs. It can be used in tasks like stable diffusion in text-guided image generation. (2) MOFI only utilizes the proposed datasets, which definitely excels at mapping images to entities. The selected dataset for the experiment also leans towards examining the correspondence between images and entities.\n\n+ The training of the MOFI model is limited to image-level correspondence to entities. However, in reality, a single image may contain multiple entities, necessitating the inclusion of region-level correspondence in the model's training. Unfortunately, MOFI does not take this aspect into consideration."
            },
            "questions": {
                "value": "I am more concerned about how MOFI performs on existing hot tasks, such as whether it is more advantageous to combine with LLM than CLIP, or whether MOFI can be used for image generation tasks. Translate the above into English."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4674/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698635515380,
        "cdate": 1698635515380,
        "tmdate": 1699636448392,
        "mdate": 1699636448392,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0kAHud5HYp",
        "forum": "QQYpgReSRk",
        "replyto": "QQYpgReSRk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4674/Reviewer_FYZT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4674/Reviewer_FYZT"
        ],
        "content": {
            "summary": {
                "value": "This manuscript presents two main contributions. Firstly, it constructs a large-scale I2E dataset in which the correspondences between images and entities are collected. The dataset is derived from a large-scale web corpus, and the authors enhance its data quality by cleaning and filtering. Secondly, the manuscript explores various training recipes and ultimately adopts a multi-task learning strategy that combines supervised learning and contrastive learning. The multi-task model is trained on the constructed I2E dataset to perform image retrieval and classification tasks. Through experiments, the manuscript demonstrates that both the dataset and the training method contribute to achieving better results across multiple downstream tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe manuscript constructs a large-scale and high-quality I2E dataset, which can be utilized for model training across various downstream tasks such as image classification and image retrieval. Furthermore, through a comparative analysis between the original CLIP, CLIP trained on I2T, and CLIP trained on I2E, the manuscript demonstrates that the constructed I2E dataset improves model performance.\n2.\tThe experiment is comprehensive, conducting thorough comparisons to assess the impact of different datasets on model performance, thereby validating the high quality of the dataset. Additionally, through comparisons between different models, the manuscript verifies the effectiveness of the proposed training approach.\n3.\tThe paper is well-organized and clearly written."
            },
            "weaknesses": {
                "value": "1.\tThe number of training data can be shown in Table 2 for more clear comparison.\n2.\tIncomplete experimental comparison in Tables 3 and 4. Why is there only CLIP for comparison? Other methods for solving zero-shot/linear probe classification should be discussed and compared."
            },
            "questions": {
                "value": "Please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4674/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4674/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4674/Reviewer_FYZT"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4674/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763118122,
        "cdate": 1698763118122,
        "tmdate": 1699636448196,
        "mdate": 1699636448196,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Xpe0CAT5gM",
        "forum": "QQYpgReSRk",
        "replyto": "QQYpgReSRk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4674/Reviewer_fBZj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4674/Reviewer_fBZj"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a vision-language model akin to CLIP, called MOFI. However, unlike CLIP, which primarily focuses on pure contrastive learning with image and text captions, the authors suggest supplementing this by directing the model's attention to specific entities within the captions. They achieve this by integrating an auxiliary task into the standard CLIP training objectives. To accomplish this, the authors introduce a new large-scale dataset, Image-to-Entities (I2E), containing 1 billion images and 2 million distinct entities.\n\nTo construct their dataset, the authors initially utilized a large crawled web corpus comprising 8.4 billion image-text pairs. They then applied a named entity recognition model to the text associated with the images, which could be derived from the captions or titles. However, due to the potential for multiple words to refer to the same actual entity, the authors implemented entity linking to connect entities to specific identifiers. To do so, they followed previous methodologies by learning entity embeddings based on graph embeddings from Wikipedia data. To determine the specific identifier linked with the entity, they utilized other entities within the same text to disambiguate. They combined other entity embeddings and iteratively computed the probability of the identifier based on the distance to the full text embedding. To eliminate noisy entities unrelated to the image, the authors used the image-entity distance computed by CLIP to filter out those with low similarity.\n\nThe authors explored three strategies for training models on their new dataset. The first strategy involves a straightforward classification-based objective where the aim is to classify an image as one of a fixed number of entities using a standard, fixed-size classifier. The second approach is contrastive pre-training, employing a cross-entropy loss for contrastive training, similar to CLIP. Finally, the authors proposed MOFI, which integrates both a standard contrastive learning objective and the classification-based objective by combining the two losses.\n\nThe authors conducted experimental evaluations of their approach across various tasks. Initially, they evaluated their model on the image-to-image retrieval task and exhibited significant gains compared to standard CLIP, controlling for the model backbone architecture. The authors demonstrated that, in many settings, their approach achieved state-of-the-art performance and outperformed other models overall. Next, they evaluated zero-shot classification across ImageNet and VTAB. The authors showcased that their approach achieved state-of-the-art performance on ImageNet and the VTAB Benchmark for their largest model size. Lastly, the authors evaluated the performance of linear probing with their model, illustrating improved performance over CLIP. Additionally, the authors included qualitative results showcasing examples of retrieved images from various benchmarks and a comparison of CLIP and MOFI models on image-to-text and image-to-entity tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "In terms of strengths, before delving into the technical details of the method, the writing of the paper is very good and appears highly polished. The figures are well-made, and overall the presentation is of very high quality.\n\nIn terms of the substance of the paper, the idea of using entities as an additional type of supervision is an interesting one. By explicitly factoring out the entities from the text and then forcing the model to perform a task on those directly, the authors essentially force the model to learn an entity-centric visual representation. In particular, rather than allowing the model to get by with some rough semantic similarity of entities which can be encoded in a feature representation for image-text matching, by explicitly forcing the model to understand millions of specific entities and to disambiguate them, this explicitly forces the model to learn representations for differentiating these very fine-grained entities. Thus, the concept of using entities as an auxiliary task is an interesting one and well motivated.\n\nThe authors create a new image-to-entities dataset, which consists of a billion images and 2 million distinct entities. As the authors point out, the dataset has the largest number of distinct class labels than any other dataset, 66 times more than the next one cited. Thus, the dataset created by the authors poses a true challenge, as classifying images into 2 million categories is a daunting task even for the most powerful models. Thus, the image-to-entity dataset could serve as a new benchmark or task for large-scale visual recognition.\n\nIn terms of experimental results, the authors demonstrate that MOFI outperforms a number of recent state-of-the-art baselines and a number of image-text foundation models, setting a new state-of-the-art for CLIP-style models on a number of different benchmarks, as they note."
            },
            "weaknesses": {
                "value": "Despite being an impressive and large-scale model that the authors have trained, in my view, the approach has several significant weaknesses.\n\nPrimarily, in terms of technical novelty, there seems to be minimal innovation in the proposed approach. The authors employ a standard contrastive learning approach alongside a standard classification objective, combined through a linear combination. In the context of entity extraction, the authors utilize a named entity recognizer, following an existing approach for learning entity embeddings from Wikipedia. Therefore, in terms of actual technical contribution, the paper seems rather limited. The primary contribution appears to be the extensive scale at which the model operates.\n\nRegarding entity linking, I find the proposed approach weak. It seems the authors don't conduct any form of multimodal entity linking. The process involves initial entity linking using entity embeddings on the text side, followed by a filtering step using a pre-trained CLIP model. A stronger approach for entity linking could potentially be achieved by integrating visual features into the knowledge embeddings. Since this component is central to the proposed approach, such a modification could potentially reduce the number of spurious entity linkings. I'm also curious if the authors experimented with off-the-shelf entity linkers, like BLINK.\n\nFundamentally, the authors emphasize the significance of the entity prediction task over standard contrastive learning. They argue that the experimental results demonstrate the advantage of predicting these entities. However, this claim is not entirely apparent. For instance, in table 2, the authors report better performance using their approach compared to CLIP and other baselines. Yet, this comparison might be somewhat unfair due to the substantially larger size of the author's dataset. For instance, CLIP's dataset comprises 400 million, whereas the author's dataset is 1 billion. Therefore, a comparison controlling for dataset size would be more appropriate to demonstrate the actual improvement using the author's approach rather than a mere performance increase due to a larger dataset. It would be beneficial to see a comparison with MOFI using 400 million images to understand how it compares to CLIP. Table 3 presents a more precarious situation. For zero-shot classification, we observe that the authors' multitask model doesn't show a clear advantage over CLIP on the ImageNet and VTAB benchmarks. It's not evident that the multitask formulation actually enhances performance compared to CLIP, especially considering the larger dataset size accessible to the authors. The MOFI model slightly outperforms CLIP in linear probing but only marginally.\n\nAdditionally, I'd like to ask the authors why they believe the training setting proposed in MOFI is the best approach. Since the advent of CLIP, a considerable amount of work has explored contrastive model training. For example, the LIT paper (Zhai, X., Wang, X., Mustafa, B., Steiner, A., Keysers, D., Kolesnikov, A., & Beyer, L. (2022). Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 18123-18133).) demonstrated significant performance gains by locking the image branch of the model and tuning the text model using a private dataset on which LIT was trained. There are also other large foundation contrastive models like the open-source LAION-H-14 model (https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K) that the authors could have compared against. These models, not using the described objectives, seem to perform similarly to the author's approach, making this comparison more equitable since the other models the authors compare to seem to have substantially less data.\n\nRegarding the approach itself, it remains unclear if the concept of contrastive learning with a classification objective on entities is necessary. Other works like Coca (Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., & Wu, Y. (2022). CoCa: Contrastive Captioners are Image-Text Foundation Models.) , demonstrate that a similar objective could be achieved by performing contrastive learning + captioning. For example, on ImageNet, the authors obtained 86.3% zero shot performance using this contrastive captioning-based approach, which significantly outperforms the authors. The contrastive captioning approach actually seems to encompass what the authors are doing in the MOFI paper, as a caption needs to be generated, not just the entity. The authors focus solely on entities, yet other crucial visual relations like events, actions, and visual properties are omitted in their approach. These could be captured under the contrastive captioning setting. Therefore, it's uncertain whether MOFI's approach is superior to training vision-language models as opposed to the contrastive captioning-based approach.\n\nIn summary, it's unclear whether the multitask learning approach presented is the optimal way to utilize this data. There's a lack of clear performance analysis against contrastive captioning-based models, making the experimental results challenging to interpret due to the differing dataset sizes.\n\nAdditionally, there's a concern regarding the technical approach itself. Filtering out entities using CLIP could potentially result in error amplification. If an entity is poorly captured by CLIP, using it to filter the dataset might exclude that entity.\n\nIt seems that the paper would have been better positioned as a very large scale visual entity linking paper, than a vision-language foundation model paper, since the key novelty the authors seem to be addressing is the ability to disambiguate these various entities. However, that itself raises a lot of questions. For example, how do we ensure that more common entities don't overwhelm the long tail of entities? This does not seem to be explicitly handled by the authors' approach and currently authors do not perform an evaluation sufficient to consider the paper for the visual entity linking task."
            },
            "questions": {
                "value": "1. What is the key benefit of MOFI's technical approach vs contrastive captioners like Coca? It seems that Coca significantly outperforms MoFi on several benchmarks significantly (e.g. Imagenet). Conceptually, the contrastive captioner seems to be more straightforward as well and doesn't require the complex entity linking that MOFI does. It seems that the entity prediction task is a subset of the captioning task that Coca addresses. Also, Coca is capturing entities AND verbs (and other words).\n\n2. Have the authors explored the statistics on how often the model predicts entities from the long tail? Is it mainly focusing on more common entities? It seems the core use of the model would be to classify an image as an entity from Wikipedia - this is an important task for knowledge graph construction and coreferencing, but it is not clear that the authors have evaluated the accuracy on that task - which seems to be the most unique part of the work.\n\n3. Will the dataset and model be released? If not, the contribution would be the technical approach, but as stated above, it is not at all clear that it is significant compared to other ways of training contrastive models (e.g. LiT) on equivalent size datasets AND it is not clear that this multi-task formulation is superior to existing methods for contrastive captioning (Coca).\n\n4. Table 2 - why evaluate on image-image retrieval when CLIP and other baselines were not trained explicitly for image retrieval. These methods have been trained for image-text retrieval, not intramodal retrieval. In contrast, MoFI's classification objective could be seen as pulling similar images together by ensuring they have a similar object distribution. It seems that if we are going to be comparing against VL models like CLIP, LiT, etc. we should focus on those types of tasks (zero shot classification, linear probing, etc.)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4674/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4674/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4674/Reviewer_fBZj"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4674/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809264436,
        "cdate": 1698809264436,
        "tmdate": 1700713146521,
        "mdate": 1700713146521,
        "license": "CC BY 4.0",
        "version": 2
    }
]