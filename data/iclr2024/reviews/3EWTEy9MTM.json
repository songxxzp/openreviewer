[
    {
        "id": "ofubcIbvxL",
        "forum": "3EWTEy9MTM",
        "replyto": "3EWTEy9MTM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8645/Reviewer_woBt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8645/Reviewer_woBt"
        ],
        "content": {
            "summary": {
                "value": "The paper makes several contributions to the theory of the expressive power of transformers via circuit complexity:\n\n1. Constant precision transformers are in $\\mathsf{AC}^0$\n2. The authors show the existing $\\mathsf{TC}^0$ upper bound for log-precision transformers can be extended to iterated addition with intermediate rounding (at least in the case where the exponent has 0 bits).\n3. Constant-precision transformers with T steps of CoT and nonuniform $O(\\log n)$ embeddings can simulate T-sized circuits and thus express $\\mathsf P/\\mathsf{poly}$\n4. Empirically, transformers get \n\nThe first two results relate to understanding the expressive power of encoder-only transformers, and the third shows one sense in which CoT/decoding steps add power to transformers."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The first contribution (power of hard attention) may not be the most realistic model of practical transformers (see Weaknesses), but it is still potentially valuable for filling out the theory of transformers of different types.\n2. The second contribution showing log-precision transformers with rounding are still in TC0 is quite interesting and solves an technical problem unresolved in prior work. Even though the result is not fully general (requires zero exponent), the progress here is quite exciting and perhaps could be extended in future work.\n3. I would like some of the assumptions for the P/poly result to be better discussed (see Weaknesses), but it is a valid and potentially useful result for formalizing the power of CoT.\n4. The paper is generally well-written and organized, and I appreciate the inclusion of a neat empirical study."
            },
            "weaknesses": {
                "value": "## Limitations of Constant Precision\n\nConstant precision is not necessarily a realistic setting, since it means transformers cannot attend based on positional encodings or compute uniform attention (which require $\\log n$ bits). In the practical regime, transformers have enough precision to express uniform attention over their input length and can use uniform attention to recognize majority ([Merrill et al., 2021](https://aclanthology.org/2022.tacl-1.49/)), which is outside your upper bound of AC0. Presumably, if we wanted to apply transformers on very long input lengths, we would scale up the precision of attention logarithmically so that uniform attention and positional embeddings would remain expressible. For this reason, [Merrill & Sabharwal (2023)](https://neurips.cc/virtual/2023/poster/70153) propose studying the log-precision model instead of the constant precision one.\n\nTo put it another way, let's say you ran the same experimental setup as Figure 3 but with Majority instead of Iterated Squaring. If transformers are in AC0, we'd expect a similar qualitative pattern where models without CoT struggle without sufficient depth but models with CoT can succeed. But I think that's not what you would find: instead, even models of one layer could do well using a single uniform attention head.\n\nTo be clear, even though I think log-precision is more realistic, I think it is still potentially interesting to analyze the constant-precision case to fill out the overall theory and understand the value of log precision. However, the authors should discuss the differences between constant-precision and log-precision and specifically mention or respond to the argument for log-precision from [Merrill & Sabharwal (2023)](https://neurips.cc/virtual/2023/poster/70153). It could also be helpful to run the experiment I described above with Majority and potentially include the results in the appendix.\n\n## Nonuniform Embeddings\n\nThe paper characterizes transformers with T steps as P/poly. There is something weird about this result, in that it characterizes transformers by a nonuniform complexity class that contains undecidable problems (e.g., the unary encoding of the halting problem)! In contrast, we know that transformers cannot compute undecidable problems since they can be implemented on standard computers. This disconnect comes from the fact that the embeddings are assumed to be **nonuniform**: i.e., they can be any sequence of $O(\\log n)$ bits on inputs of size $n$. This enables the embeddings to be able to encode advice for solving undecidable problems, which standard positional transformers cannot do because they are computable.\n\nThis assumption of nonuniform embeddings should be better highlighted in Section 3.4: right now it's not even visible in the theorem statement. It would also be good to add some discussion of the assumption in the introduction or other high-level parts of the paper so readers don't miss it."
            },
            "questions": {
                "value": "Your circuit simulation result feels quite related to Lemma 3 from [Merrill & Sabharwal (2023)](https://arxiv.org/pdf/2207.00729.pdf), except that yours uses CoT to avoid the linear depth that they incurred. It could be worth discussing a bit about the similarities and differences between the two constructions.\n\nIn Theorems 3.1 and 3.2, you write $\\mathsf T[\\mathrm{poly}(n), 1, 1] \\subseteq \\mathsf{CoT}[1, \\mathrm{poly}(n), 1, 1]$, where the first class corresponds to $\\mathsf{CoT}(0, \\ldots)$. Is it possible to say that these classes are actually equal, i.e., a single step of CoT does not add power?\n\nDo you have any examples or thoughts about how transformers might systematically use rounding to gain expressive power (with nonzero exponent)?\n\nG.1: \"ranges from {-1, 1}\" -> \"[-1, 1]\"\n\nnit: Wording Problem -> Word Problem?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8645/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8645/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8645/Reviewer_woBt"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8645/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768414208,
        "cdate": 1698768414208,
        "tmdate": 1699637082883,
        "mdate": 1699637082883,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2WoJs9OwlX",
        "forum": "3EWTEy9MTM",
        "replyto": "3EWTEy9MTM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8645/Reviewer_zX88"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8645/Reviewer_zX88"
        ],
        "content": {
            "summary": {
                "value": "This paper formally studies the representation power of transformers when used with chain-of-thought (CoT). The authors prove that transformers, under a certain formal model (but see weakness below of the model), with $T(n)$ CoT steps (where $T(n)$ is polynomial in $n$) on input of size $n$ can simulate the computation of any circuit of size $T(n)$. As a result, they derive a lower bound of P/poly. Along the way, they also derive new bounds on finite-precision and fixed-point log-precision transformers when rounding in iterated addition is done iteratively, rounding the sum of two numbers at a time."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The main strengths of this paper are:\n\n1. its rigorousness in making clear, concise statements of its findings (except for one important \"detail\" discuss under weaknesses);\n\n2. a formal characterization of the power of transformers with CoT, for which results have come out only very, very recently (after the ICLR submission deadline);\n\n3. carefully crafted arguments and proofs around rounding of numbers when performing addition of $n$ numbers, a key step used in multiple prior papers unless less realistic assumptions; and\n\n4. empirical evaluation to support the theory, which is often missing in similar theoretical characterizations of transformer variants in the past.\n\nI have not read the proofs in detail (esp. the material in the appendix), but the results and approach intuitively appear plausible."
            },
            "weaknesses": {
                "value": "I really have only one, albeit big, concern about the paper, namely **the formal model of transformers being studied is different not only from all prior theoretical works but also from practical use of transformers**. This is made worse by the lack of a discussion of this difference. Consequently, while the results appear to tighten prior upper bounds and provide novel lower bounds, they really are applicable to a different model.\n\nSpecifically, the authors assume a model of transformers that is **non-uniform** (in the sense it is used in circuit complexity), namely, for each $n$, there is a **different** transformer. As they state in Defn 3.4, \"for every ... $n$, there is a $L$-layer ... transformer\". This means that one needs a *family* of transformers, one for each input size $n$, to solve a given problem, and the weights of the transformer for input length $n$ may have nothing to do with that of the transformer for input lengths $n+1$. To specify such a family, one thus needs to specify an infinite family of unrelated weights, which obviously is unrealistic.\n\nIn contrast, in practice, one trains a transformer on inputs of certain lengths, freezes those weights, and then uses the same frozen-weights transformers for inputs (and chains-of-thought) of arbitrary lengths. In fact, inspired by this, the theoretical research on the representation power of transformers in the past 2-3 years has gone in the opposite direction---from non-uniform upper bounds, to tighter and tigher uniform bounds (e.g., log-space uniform, log-time uniform, FO-uniform, etc.).  Importantly, all along, the model of transformers used was uniform.\n\nThe *non-uniformity* of the model assumed here has other, well-known undesirable consequences also seen in non-uniform circuits---it allows transformers to **trivially solve certain undecidable problems**, namely any undecidable unary problem, over the alphabet $\\{1\\}$. E.g., it can solve the halting problem expressed in unary, just like all non-uniform circuit classes (including P/poly) can.\n\nBesides being unrealistic, this raises a question about the meaningfulness of the technique used to prove the main result (Theorem 3.3). In order to simulate a circuit of size $T(n)$, one must somehow *embed* the circuit into the transformer as the transformer needs to know what circuit to compute. As seen from the proof of it in the appendix, the authors have a creative solution: they put the description of the circuit in the *positional embedding* of the transformer.\n\nWhile interesting and unique, this has two undesirable implications:\n\n1. The positional embedding for inputs of length $n$ is allowed to be arbitrarily different from the positional embeding for inputs of length, say, $n+1$ (because there is no uniformity constraint between the circuits for the two respective sizes, $n$ and $n+1$), which departs heavily from practice.\n\n2. It means that the proposed construction must include some **uncomputable / undecidable positional embeddings**! To see this, consider any undecidable unary problem $P$ in P/poly. The typical polysize circuit construction for $P$ is to have, for each $n$, a trivial circuit $C_n$ that outputs a $1$ if and only if $1^n$ is in that undecidable language (e.g., $1^n$ is the unary encoding of the $n$-th Halting problem). Thus, by the authors' construction, there is a trivial transformer that decides the same language---and its embedding of the first position computes membership in that undecidable language! In other words, the embedding itself in not computable by any reasonable model of computation.\n\nTo summarize this, while the construction is correct to my understanding, it is for a formal model that departs from practice and assumes a lot of power (e.g., that of having access to potentially uncomputable embeddings).\n\nAt the very least, these limitations and their implications should be clearly discussed in the paper.\n\n**Minor points**\n\n* In the abstract, the statement \"with $T$ steps of CoT, constant-depth transformers ... can solve ...\" should be qualified with T being at most a polynomial in $n$.\n\n* In the 2nd last line of page one, I think you mean \"encoder-only\" rather than \"decoder-only\"; or single-step decoder.\n\n* In line 3 of section 2, do you mean $\\phi(bin_k(x)) = x$ rather than $\\ldots = 0$?\n\n* In the 2nd paragraph of section 2, where is the input length limited to $n_\\max$?\n\n* page 3, two lines before defn 3: \"over more two\" => \"over more than two\""
            },
            "questions": {
                "value": "1. Please see my main concern above regarding non-uniformity. I don't really have any specific question around it, though I wonder what you think about the non-uniformity issue above and how, if you choose to, would you incorporate it in a revised version of the paper.\n\n2. After Theorem 3.2, you mention that an analog of this theorem remains open for log-precision transformers even with a constant number of bits for the exponent. Could you elaborate why one can't just absorb the constant bits of the exponent into additional constant bits of the significand? Perhaps you can illustrate it with what goes wrong if one has $1$ bit for the exponent?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8645/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698966244987,
        "cdate": 1698966244987,
        "tmdate": 1699637082778,
        "mdate": 1699637082778,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BHBs4nGhdi",
        "forum": "3EWTEy9MTM",
        "replyto": "3EWTEy9MTM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8645/Reviewer_Aro9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8645/Reviewer_Aro9"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates whether chain of thought(COT) increases the power of transformers. COT provides additional power to transformer decoders in the form of a hidden \"scratch pad\" where a transformer can perform serial computations. Without COT, a transformer is limited to being a fixed depth circuit, and as such is known to have very limited power in the complexity sense. The paper studies the maximum ability of a transformer given the ability to run COT. COT is able to significantly expand the ability of transformers as without COT transformers are in AC0 and with COT transformers can solve problems from a significantly larger class, all problems with solvable by Boolean circuits of size T (T= time steps given to COT). The key technical contribution is a formalisation of COT and its placement in the circuit complexity heirarchy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall i felt it was a quite well written paper and the case was made well. The paper formalizes the problem precisely, which is not only critical for answering the question considered in the paper, but is also of independent interest. I think the parameterisation of CoT  in terms of embedding size and depth, is novel and interesting. There are several well chosen examples that made comprehension easier."
            },
            "weaknesses": {
                "value": "The empirical evaluation is fairly convincing but it does not really reveal anything new insights not already covered by the proofs. \n\nMinor  errors:\nSection 1: poewrful -> powerful"
            },
            "questions": {
                "value": "--"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8645/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699013533325,
        "cdate": 1699013533325,
        "tmdate": 1699637082646,
        "mdate": 1699637082646,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dCCNzcot8V",
        "forum": "3EWTEy9MTM",
        "replyto": "3EWTEy9MTM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8645/Reviewer_pRpF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8645/Reviewer_pRpF"
        ],
        "content": {
            "summary": {
                "value": "This paper looks at the expressive power of transformer models, specifically looking at the expressive power on sequential reasoning tasks rather than parallel tasks. The main results are theoretical and It introduces several new classes of problems that are tightly focused"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "To my mind, there were three main strengths to the paper.\n\n1. Its theoretical approach: I greatly appreciated the theoretical bend to the paper. Phrasing things in terms of problem classes that a particular model can solve is the kind of I'd like to see more of. \n\n2. Interesting examples: The problem classes they used were interesting, novel, and nicely targeted to the theoretical results\n\n3. A detailed and rigorous approach to a intuitive idea. The supporting information section that includes very detailed proofs and ample details for the interested reader. \n\nIn terms of the primary dimensions, this paper was quite strong in two of them: Originality and Significance\n\nOriginality: As previously noted, phrasing the CoT problem in terms of specific problem classes for particular families of models is a fantastic idea, something we don't see nearly enough of in the field. I also thought the experiments performed were quite original - finding problems in particular problem classes takes a great deal of effort and creativity, the problems they used were not brand new, but were new to the area of CoT and transformer models\n\nQuality: The experiments performed were well chosen and did a good job supporting the theory\n\nSignificance: This paper does make a significant claim and provides good evidence to back it up. Understanding the power of transformer models, where they face challenges, and what problem classes they excel at, is a very significant result and exactly the kind of result that should be featured at an ICLR tier conference."
            },
            "weaknesses": {
                "value": "For all the strengths of this paper, there were a number of weaknesses as well. \n\nThe performance evaluation - data was trained and evaluated using \"freshly sampled synthetic data\". I think this is problematic at several levels. First, without a carefully though out test/train split, the model is at risk of overfitting. I think that is fine here - because overfitting is still telling you information about the expressive power of your model class, however this goes against the grain of standard model training practice, and at least deserves comment. Second, the problems used were all discrete problems. Discrete problems are great test beds for theoretical arguments - but in most of these examples, the problem space is finite for a fixed size, and randomly sampling from a discrete space gives the artificial impressing of having more data than is actually available. Finally, how you sample from these discrete spaces seems like it would make a big difference on the model performance\n\nThe model discussion was also significantly lacking. At the very least, a discussion of what the class of transformer models looks like and how you are bounding it belong in the main body of the text, not banished to appendix G on the final page of the supporting information. The details that were present, were in the form of Algorithm 1, defining an implementation, but not the class itsself. That made it difficult to tell what was the result of the model class and what was the result of the training process.\n\nFinally, this paper would be stronger if the presentation was as crisp and focused as the data classes and results. In particular, there was an odd mix of too much detail, too little detail, and superfluous detail. For example, Definitions 3.1 and 3.2 were primarily used in appendix C so their presentation on page 3 distracted from the main arguments of the paper."
            },
            "questions": {
                "value": "What is the assumed background of a reader of this paper - as someone quite familiar with both machine learning, complexity theory, and the combinatorial problem classes being addressed, I found this paper confusing - for who I imagine the typical reader to be, I think lots of the content of this paper can be assumed knowledge - for example, I don't think a definition for a finite state automaton (Definition D.1) is a productive use of space in this paper - this is something you can cite directly (e.g. text above corollary 3.4 on p.6). \n\nIn all the figures, I did not find the presented examples particularly illuminating. For instance, with Figure 1, with the non CoT example,  the given label does not appear to be a composition of the permutations given in the input - and it doesn't even appear to have the form of the right answer.\n\n\n\nSuggestions\n- This paper spends a lot of time on the setup and has an extensive collection of supporting material. I like t"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8645/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699287800404,
        "cdate": 1699287800404,
        "tmdate": 1699637082524,
        "mdate": 1699637082524,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jTocaS1d05",
        "forum": "3EWTEy9MTM",
        "replyto": "3EWTEy9MTM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8645/Reviewer_wS3v"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8645/Reviewer_wS3v"
        ],
        "content": {
            "summary": {
                "value": "The paper studies (mostly theoretically) the experessiveness of transformer architectures. The authors show that transformers with chain of thought can compute boolean circuits with a number of steps that is not bounded by the depth of the transformer."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(+) Important result regarding the expressiveness of transformers. The finding seems very obvious intuitively, but the novelty seems to be in a rigorous proof.\n\n(+) Realistic modeling of finite precision computations\n\n(+) The theoretical finding is matched with empirical results on four tasks in arithmetic."
            },
            "weaknesses": {
                "value": "(1) I'm not sure I grasp the significance of the finding. It seems obvious that any model can perform serial computations if it is allowed to store intermediate results somewhere (in this case, in the output sequence, which can be written to and subsequently access with self-attention). Therefore, producing such intermediate output seems as expressive as a chain of multiple instances of the model (this does not even specific to transformers).\n\nIs this intuition misguided? Perhaps the important finding is rather that a transformer *without* CoT could not solve serial problems? (but this was covered in previous work).\n\nNote that this topic is not exactly my area of expertise (I was called as an emergency reviewer), but this should be addressed since it is likely something that other readers will wonder about.\n\n---------------------\n\n(2) Presentation could be improved. These are details though that are easy to fix.\n\nThe abstract could be clearer about what form of CoT is studied in the paper (whether it's about the training data, the test phase, the prompting, ...). The very first sentence seemed clear in retrospect to me, not on the first read. So it's probably good to be even more explicit and specify that it's just about instructing the model to generate intermediate steps.\n\nThere are some informal shortcuts in the technical language that should be fixed. Examples:\n\"transformers with polynomial intermediate steps\" -> \"with a number of intermediate steps polynomial in ...\"\n\"transformers with linear intermediate steps\"\n\"poly width and log precision\"\netc.\n\nTypos: poewrful, circuit valuation, \"because every regular languages\" (should be singular)\n\nSec. 4\"we find that cotis always\" (missing space, also in multiple other places in this section)\n\nWeird grammar in the abstract: \"previous works have ...\"\nI suppose you mean \"previous works show that ...\""
            },
            "questions": {
                "value": "See (1) above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8645/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699299919121,
        "cdate": 1699299919121,
        "tmdate": 1699637082424,
        "mdate": 1699637082424,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GbA6LcPI3e",
        "forum": "3EWTEy9MTM",
        "replyto": "3EWTEy9MTM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8645/Reviewer_qiA8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8645/Reviewer_qiA8"
        ],
        "content": {
            "summary": {
                "value": "- The purpose of this study is to demonstrate that CoTs can enhance the reasoning ability of LLMs. To prove this, they utilize the language of complex circuits to compare the expressive power of Transformer models with and without CoTs.\n\n- First, they hypothesize that CoTs enable more efficient serial computations, which vanilla Transformers cannot achieve. they subsequently establish that, when dealing with sufficiently long inputs, the precision of a Transformer model is limited to solving problems within $AC^0$. Furthermore, they demonstrate that CoTs, with T steps, utilizing a constant-depth transformer, constant localization accuracy, and O(log n) embedding size, can solve problems that exceed the capabilities of Boolean circuits of size T. Finally, they find that increasing the embedding size to poly(n) does not improve expressiveness beyond using a log(n) embedding size.\n\n- In conclusion, their theoretical analysis suggests that models with CoTs exhibit enhanced expressive abilities when compared to Decoder-only Transformers without CoTs. they have designed experiments to validate their theoretical findings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Analyzing the effect of the presence or absence of CoT on model expressivity from a theoretical perspective.\n- Proposing a tighter upper bound for constant depth transformers' expressive power.\n- The motivation of this paper is clear."
            },
            "weaknesses": {
                "value": "- Symbols are not clearly described. The authors\u00a0 don't make clear instructions when using symbols like NC^1, AC^0, etc. It is recommended that these symbols be explained when they are first mentioned.\n\n- The conclusions of Theorem 3.8 and Theorem 3.9 presented in the paper seem to be contradictory. Theorem 3.8 indicates that enlarging the embedding size to poly(n) doesn't improve the model's expressiveness of T(n) = poly(n) step CoT. On the other hand, Theorem 3.9 demonstrates that broadening the embedding width strengthens the model's power for any particular polynomial T(n) = n^k step CoT. The conclusions of these two theorems are confusing and require further clarification."
            },
            "questions": {
                "value": "- I would like the author to supplement and polish the article based on the weaknesses.\n- In the demonstration experiment shown in Figure 2, it is observed that when the depth is set to 1, the CoT performs less effectively compared to the decoder-only Transformer without CoT. This difference is particularly noticeable when the label represents the sum of the inputs modulo 2 and exceeds a threshold of 20%. This observation appears to contradict the result presented in Theorem 3.2. Is related to the fact that CoT simulates any depth circuit by writing the output token back to the next input position? Further clarification from the author regarding this matter would be greatly appreciated."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 6,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8645/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699514263570,
        "cdate": 1699514263570,
        "tmdate": 1699637082319,
        "mdate": 1699637082319,
        "license": "CC BY 4.0",
        "version": 2
    }
]