[
    {
        "id": "HXmSYIy3u7",
        "forum": "jhPvuc7kxB",
        "replyto": "jhPvuc7kxB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3946/Reviewer_Fa1W"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3946/Reviewer_Fa1W"
        ],
        "content": {
            "summary": {
                "value": "The authors claim that they propose training an LM end-to-end on low-level surrogate tasks, including object detection, re-identification, and tracking, to endow the model with the required low-level visual capabilities."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors claim that they propose training an LM end-to-end on low-level surrogate tasks, including object detection, re-identification, and tracking, to endow the model with the required low-level visual capabilities."
            },
            "weaknesses": {
                "value": "1. In the experiments, the authors primarily focus on conducting investigations using synthetic datasets, particularly the ACRE dataset. However, it raises concerns about the generalizability of the conclusions/findings obtained from synthetic datasets to real-world datasets.\n\n2. The experimental results primarily focus on classical models. However, the generalizability of the conclusions/findings derived from these classical models to more powerful transformer-based models, such as the models mentioned in *Related Work* part, remains a concern."
            },
            "questions": {
                "value": "Please refer to Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3946/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3946/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3946/Reviewer_Fa1W"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3946/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698667135205,
        "cdate": 1698667135205,
        "tmdate": 1700652124914,
        "mdate": 1700652124914,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8Efbr9kDE1",
        "forum": "jhPvuc7kxB",
        "replyto": "jhPvuc7kxB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3946/Reviewer_4932"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3946/Reviewer_4932"
        ],
        "content": {
            "summary": {
                "value": "Thank you for submitting your manuscript. The proposed three-step process of Look, Remember, Reason for training a Language Model (LM) end-to-end on low-level surrogate tasks, which include object detection, re-identification, and tracking, is indeed novel."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The approach presented is intriguing and demonstrates significant performance improvements."
            },
            "weaknesses": {
                "value": "1-Throughout the paper, there's a recurring mention of \"low-level surrogate tasks\". Could the authors elucidate the definition of these low-level tasks? Moreover, how do they differ from high-level tasks?\n\n2-The Look, Remember, Reason (LRR) model framework is innovative. However, there seems to be a gap in explicitly correlating this framework with the actual operations carried out in the method. The unique contributions of the \"Remember\" and \"Reason\" steps, in particular, are not clearly highlighted. It would be beneficial for the readers if the authors can provide a clearer mapping of these steps to their corresponding operations.\n\n3-Will the codebase for the presented method be made publicly available?\n\n4-Regarding the results of Video-ChatGPT on the Something-Else dataset: Were these results replicated by the authors? I couldn't find a direct reference to such results in the original Video-ChatGPT paper."
            },
            "questions": {
                "value": "see Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3946/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773309285,
        "cdate": 1698773309285,
        "tmdate": 1699636355364,
        "mdate": 1699636355364,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xk7qQ5sIbi",
        "forum": "jhPvuc7kxB",
        "replyto": "jhPvuc7kxB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3946/Reviewer_5Fo1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3946/Reviewer_5Fo1"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a Look, Remember, Reason (LRR) framework to enable language models to perform visual reasoning in videos. The proposed LRR framework uses a two-stream video encoder to extract dense spatiotemporal features from video frames capturing structural and motion cues. The language model backbone has cross-attention layers inserted between its self-attention layers to enable top-down attention over the visual features. This allows the model to extract relevant visual information based on the reasoning task. LRR is trained end-to-end using surrogate tasks like object detection, re-identification and tracking. These provide supervision to teach the model the required low-level visual skills.\n\nThe authors also demonstrate that training LRR jointly on multiple datasets leads to a \"generalist\" model that performs competitively compared to task-specific \"specialist\" models. In the experimental results, the authors demonstrate that the LRR models significantly outperform prior state-of-the-art on challenging visual reasoning tasks from the ACRE, Something-Else, and CATER datasets, showing the benefit of the proposed grounded reasoning approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Demonstrates strong performance on multiple challenging visual reasoning datasets by grounding the language model in low-level visual details.\n+ Good demonstration of using surrogate tasks and end to end training"
            },
            "weaknesses": {
                "value": "- The datasets used are rather simple with low visual complexity, such as CATER."
            },
            "questions": {
                "value": "1) Could you comment on the nature of surrogate tasks? Are there some tasks that are more suited for reasoning vs others. Do low level recognition tasks (choice in the paper) work better.\n2) Is there evidence that LRR is not ovefitting to these simplistic datasets due to surrogate tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3946/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3946/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3946/Reviewer_5Fo1"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3946/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699109111116,
        "cdate": 1699109111116,
        "tmdate": 1700681256760,
        "mdate": 1700681256760,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VUlU5E7go7",
        "forum": "jhPvuc7kxB",
        "replyto": "jhPvuc7kxB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3946/Reviewer_tQdf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3946/Reviewer_tQdf"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a Look, Rember, and Reason (LRR) framework to solve video reasoning task. The structure utilized LM to extract visual information by using surrogate grouding tasks and then integrated the grouding information to arrive at a final answer. The authors propose a two-stream vide encoder to capture sccene structure and object motions to learn low-level skills. Experiments on two sythetic dataset and one real-world datset shows the effectiveness of proposed method on complex spatialtemporal and causal reasoning tasks in videos."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed LRR structure make use of language models to solve surrage groudning task to benefit final video reasoning task. This structural design better utilize the low-level visual skills and information from videos.\n\n2. This paper conduct experiments on two synthetic datasets and one real-world dataset. The proposed methods achieve competative performance three datasets and outperforms other exsisting baseline models, showing the effectiveness of proposed method.\n\n3. Table 2 and 3 also show the performance of LRR without surrogate tasks and two-stream encoder. The ablation study shows the significance of the two proposed components for the overall structure."
            },
            "weaknesses": {
                "value": "1. Writing, section 3 and Figure 2 is a little unclear and hard to follow.\n2. For different surrogate tasks, where do the ground-truth answers such as localization or box come from?"
            },
            "questions": {
                "value": "See section weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3946/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699240588732,
        "cdate": 1699240588732,
        "tmdate": 1699636355194,
        "mdate": 1699636355194,
        "license": "CC BY 4.0",
        "version": 2
    }
]