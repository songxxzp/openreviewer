[
    {
        "id": "zQwIyPxEGS",
        "forum": "nBYDP46s5N",
        "replyto": "nBYDP46s5N",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1425/Reviewer_tc1K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1425/Reviewer_tc1K"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a method that learns the value function with truncated horizon. This estimator is further combined with the PPO algorithm and is empirically tested on several Atari tasks. The experiment results show the advantage of the proposed algorithm."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper studies an important problem. The arguments are supported with sufficient examples and experiments. And the paper is presented in sufficient details and illustrations."
            },
            "weaknesses": {
                "value": "1. The motivation need to be further addressed. The authors demonstrate the advantage of horizon truncation in section 3, showing that untruncated TD update can lead to divergence. However, one weakness of this demonstration is the authors set h_max to be 1000, which is the effective horizon of $\\gamma=0.999$. With this hard constraint, further increasing $\\gamma$ has very little effect on the convergence result because they are \"cut off\" back to $0.999$ due to the constraint (as one can see from the figure, the slope is much smaller when $\\gamma$ exceeds 0.999). In this case, the comparison for $\\gamma > 0.999$ is not fair. On the other hand, when one looks the part of $\\gamma < 0.999$, both curves show good convergence results, which makes the authors' claim less convincing. \n\nThe authors might strengthen their argument by adding the following results: 1. One would like to see what happens for a larger h_max value when testing the convergence result for a larger $\\gamma$ value, for example, one can set h_max=10000 for $\\gamma=0.9999$ so that no long term signals are killed. 2. Another concern is the introduction of h_max is exploiting the prior knowledge that there is no signal in the long term (because the reward is always 0 in this toy example.). If this is true, then setting an even smaller h_max values like 100 or 10 can lead to even better convergence result. The authors could also show the experiments for smaller h_max values to prove that this concern is wrong. \n\n2. While the algorithm learns the h-horizon values for several horizons, when combined the value function with PPO, one only finds the appearance of the value function for h_max (for example, equation 12). This is somehow disappointing. It could be possible that the value estimations for various horizons are hidden somewhere in this expression like the TD updater. But if that is the case, the authors should address it in the main context as this is crucial for showing why their formulation is useful. \n\n3. The use of notation is messy. In equation 3, while it's said to be n-step update, the corresponding notation is replaced by k in the equation. I never see a formal definition of NSTEP_h^n in the main article. The most similar notation is NSTEP_h^(k) in the appendix. The authors should clarify if they are pointing to the same thing, and why the bracket sometimes disappears, and why sometimes there is an additional $\\gamma$ in the bracket but sometimes it also disappears. In the algorithm, the authors introduced another new notation NSTEP_x(s, h), I'm not sure if it has the same meaning of NSTEP_h^x(s). Overall, it could be much better if the authors could use consistent notations. \n\n4. Part of the experiments didn't show convincing results. For both General Performance and Long Horizon, the algorithm doesn't significantly outperforms the baseline."
            },
            "questions": {
                "value": "1. What is the justification for using linear interpolation to estimate the value function of an unknown horizon $h$? \n\n2. How did the authors choose the number of horizons K? If one uses a larger K, will one always expect a better performance so that the only constraint is computation resource? Or there is already some trade-off over the statistical performance so that increasing K can actually decrease the estimation accuracy / cumulative reward at some time point?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1425/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698095360838,
        "cdate": 1698095360838,
        "tmdate": 1699636071140,
        "mdate": 1699636071140,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Rq3DpAfe5x",
        "forum": "nBYDP46s5N",
        "replyto": "nBYDP46s5N",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1425/Reviewer_ynUz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1425/Reviewer_ynUz"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a deep PG method, called Truncated Value Learning (TVL), to learn rewards without discounting. In addition, this paper claimed bootstrap learning may degrade the performance in high-noise environments and introduced a bootstrap-free learning method. Some experimental results in  Procgen and Atari-5 seem to show an improved performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper is written concisely, clearly, and easily understandable.\n2. The proposed method is relatively novel, and some experimental results show improved performance."
            },
            "weaknesses": {
                "value": "1. This paper lacks enough ablation experiments, making it difficult to see the role of each component clearly. For example, the author should have shown the benefits of TVL not using traditional discounting while learning rewards.\n2. In addition, TVL has yet to be significantly proposed compared with the baseline algorithm on the long-horizon task, so it is difficult to judge whether the TVL method is effective on the long-horizon task. In addition, the author may be able to test it on more long-horizon tasks. Increase persuasiveness, such as pitfall, etc."
            },
            "questions": {
                "value": "1. Can the authors add some ablation experiments to supplement the paper?\n2. There are many other long-horizon tasks in Atari, such as pitfall. Skiing alone is not convincing enough (and the performance in skiing does not seem to be significantly improved). Can the author add some results of other tasks? This might increase the convince of the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1425/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698401824699,
        "cdate": 1698401824699,
        "tmdate": 1699636071032,
        "mdate": 1699636071032,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aN4JAkm3Wp",
        "forum": "nBYDP46s5N",
        "replyto": "nBYDP46s5N",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1425/Reviewer_wjX3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1425/Reviewer_wjX3"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel deep policy gradient algorithm, Truncated Value Learning (TVL), that can learn rewards discount-free while simultaneously learning value estimates for all summable discount functions. The main contribution of TVL is scaling the fixed-horizon long-horizon tasks with three ingredients: geometrically spaced value heads, sample-based return estimator, and fixed-horizon update rule.  The algorithm is tested empirically on the challenging high-noise Procgen benchmark and the long-horizon Atari game Skiing, showing state-of-the-art performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Effectively motivated to extend the learning horizon in fixed-horizon tasks.\n2. The algorithm is rigorously evaluated on challenging benchmarks, specifically, noisy tasks where TD methods falter, and it surpasses previous state-of-the-art algorithms."
            },
            "weaknesses": {
                "value": "1. The novelty is under the bar for ICLR where this paper involves almost heuristic designs.\n2. Long-horizon brings the higher sample inefficient for online algorithms. The training time is 3x higher than the PPO algorithm. \n3. This paper involves multiple hyperparameters, e.g., $k, c_{vh}$, without sufficient ablation study.\n\nMinors:\n1. The lack of citation in the first time mentioning DNA algorithm.\n2. The parameters in different networks should not be all $\\theta$, try to use different notations for different networks."
            },
            "questions": {
                "value": "It is hard for me to follow to distillation part. For example, what is $c_{vh}$, and why we should use distillation? What is actually doing in Eq. 14?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1425/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698447407057,
        "cdate": 1698447407057,
        "tmdate": 1699636070950,
        "mdate": 1699636070950,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZU15d2baUf",
        "forum": "nBYDP46s5N",
        "replyto": "nBYDP46s5N",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1425/Reviewer_Mwb3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1425/Reviewer_Mwb3"
        ],
        "content": {
            "summary": {
                "value": "The authors propose an algorithm for handling large horizons and undiscounted MDPs. They argue that discounting is for the purpose of reducing error propagation when bootstrapping. They claim that the proposed algorithm may work better in environments with high noise and high variance of returns."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The introduction is well-motivated and has a logical flow.\n\nProposed an algorithm that can learn discount-free rewards even for large horizons."
            },
            "weaknesses": {
                "value": "In the listed contributions what is the difference between points 1 vs 4 and 2 vs 3?\nI think the authors should argue with the applications about the relevance of the study."
            },
            "questions": {
                "value": "Typo fig 3 caption, X-axis\nWhich step in the TVL algorithm is dampening the error propagation similar to discounting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1425/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698885912205,
        "cdate": 1698885912205,
        "tmdate": 1699636070879,
        "mdate": 1699636070879,
        "license": "CC BY 4.0",
        "version": 2
    }
]