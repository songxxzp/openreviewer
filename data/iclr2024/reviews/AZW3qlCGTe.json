[
    {
        "id": "6YFWeWAGpw",
        "forum": "AZW3qlCGTe",
        "replyto": "AZW3qlCGTe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8741/Reviewer_xN2u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8741/Reviewer_xN2u"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new setup for few-shot learning. The proposed model is pre-trained on coarse-grained set-level labels first and fine-tuned with fine-grained labels. Authors also provide theoretical analysis on the convergence rate for downstream tasks, which shows coarse-grained pre-training can enhance the learning process of fine-grained label tasks. The experiments are performed on both natural image datasets and medical histopathology datasets, where the baselines are mostly self-supervised learning methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I think the idea of using coarse-grained label is reasonable. The conclusion of enhancing learning process of fine-trained labels is inspiring."
            },
            "weaknesses": {
                "value": "- I have some questions about the method part, Sec. 2.1. In Fig.2 (a), are input samples all belongs to the same set-level labels? I am confused by this figure and Fig.1(a) CIFAR images. What I believe is correct is that each batch contains samples belong to different set-level labels, and the coarse label is assigned to each sample for pre-training. \n- How is SupCon trained? It is superised that supervised contrastive learning perform a lot worse than basic CE approach in most setups. There is not much information about training details."
            },
            "questions": {
                "value": "- There should be more training details about the framework in Sec.2.1. \n- Fig. 3 is referenced in text before Fig. 2\n- Most of the refernece hyperlinks other than page 1 is not working."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8741/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8741/Reviewer_xN2u",
                    "ICLR.cc/2024/Conference/Submission8741/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8741/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698747959350,
        "cdate": 1698747959350,
        "tmdate": 1700651572344,
        "mdate": 1700651572344,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Wti4tVwN2V",
        "forum": "AZW3qlCGTe",
        "replyto": "AZW3qlCGTe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8741/Reviewer_jPxq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8741/Reviewer_jPxq"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a new technique aimed at boosting instance-level image classification by utilizing set-level labels. Compared to conventional methods that rely on single-instance labels, the proposed approach achieves a 13% increase in classification accuracy when tested on histopathology image datasets. A theoretical examination of the method outlines conditions for rapid reduction of excess risk rate, adding credibility and robustness to the technique. This research serves to connect instance-level and set-level image classification, providing a noteworthy direction for enhancing image classification models that use coarse-grained set-level labels."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper presents a new technique for enhancing instance-level image classification by making use of set-level labels. This serves to fill the existing gap between instance-level and set-level image classification.\n\n- The robustness and reliability of the proposed method are underscored by a theoretical analysis, which outlines conditions for the rapid reduction of excess risk rate.\n\n- The paper clearly articulates the proposed method, shedding light on both its theoretical underpinnings and empirical results. These results are demonstrated on both natural and histopathology image datasets.\n\n- The method put forth in the paper holds promise for extending the capabilities of image classification models. By leveraging set-level coarse-grained labels, the approach achieves better classification performance compared to traditional methods reliant on single-instance labels. This is particularly relevant in real-world contexts where set-level labels may offer more comprehensive information."
            },
            "weaknesses": {
                "value": "- The use of coarse-grained labels like TCGA or NCT is an interesting choice. These are indeed umbrella terms for various subcollections, and traditionally they may not provide a strong learning signal. It could be beneficial to delve into why these particular labels were chosen and what advantages they offer in this context.\n\n- Your team's approach to pretraining with coarse labels and then fine-tuning on a support set is a solid and proven method. However, it would enrich the work to articulate what sets this particular application or implementation apart in terms of novelty.\n\n- The comparison with SimCLR and simSIAM provides useful insights, but considering the advancements in the field, benchmarking against more recent self-supervised learning methods like DINO or DINOv2 might offer a more comprehensive evaluation.\n\n- To further validate the generalizability of the method, it could be insightful to include results against standardized few-shot learning benchmarks, such as Mini-Imagenet 5-way (1-shot) or SST-2 Binary classification.\n\n- Adding ablation studies that feature additional pretrained models\u2014or even models pretrained without the coarse labels\u2014could help underscore the specific benefits of using coarse-grained label-based pretraining in your approach.\n\n- Your methodology would be even more robust if additional training details are shared. Information on image augmentations, learning schedules, and optimizer settings could offer valuable insights and help in the reproducibility of your results."
            },
            "questions": {
                "value": "- Do you think you could pictorially diagram the approach adding the relevant details? It is unclear to me if the method essentially pretrains using coarse-labels and then fine-tunes on the test set using the support set or is there more to the method\n\n- Why are the methods for pretraining SupCon and FSP chosen for pre-training? Adding rationale for this might help motivate the choice of pretraining method"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8741/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698787880631,
        "cdate": 1698787880631,
        "tmdate": 1699637097090,
        "mdate": 1699637097090,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "r2Il4TF34p",
        "forum": "AZW3qlCGTe",
        "replyto": "AZW3qlCGTe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8741/Reviewer_4JEd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8741/Reviewer_4JEd"
        ],
        "content": {
            "summary": {
                "value": "The paper propose to utilize set-level coarse-grained labels to improve fine-grained image classification. Essentially the paper is proposing a new pretraining method, key to the method is selecting a dataset with coarse label, and use the set prediction on coarse label as pretraining task. The paper provides theoretical analysis for the proposed approach, showing that using coarse-grained labels speed up the learning on the fine-grained classification task. The paper also demonstrates the effectiveness on several datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of using set prediction on coarse label as pretraining task seems novel\n2. The performance seems strong compared to other baselines"
            },
            "weaknesses": {
                "value": "1. More baselines for strong self-supervised pretraining methods (e.g., BYOL, DION  are needed to demonstrate the effectiveness. As proposed method is essentially a pretraining strategy, that bears a lot of similarity with exisiting self-supervised learning method\n\n2. More ablations and discussion on some key questions are needed (see below)"
            },
            "questions": {
                "value": "1. To what extent does the similarity between the pretraining dataset and its coarse labels and the target dataset with its fine labels affect the effectiveness of the method? For instance, can the method perform well when the pretraining dataset is CIFAR-100 while the downstream task involves a medical dataset? In such a scenario, which pretraining method is preferable: supervised pretraining on ImageNet, self-supervised pretraining (ignoring labels entirely), or the proposed method?\n\n2.  Given the same 'related' dataset, if you have both the fine-grained label and coarse-grained label, which pretraining strategy is preferable?\n(Let say your downstream task is classification on a medical image dataset, with fine-grained label A,B,C. The pretraining dataset you have is another medical image dataset (thus more related than ImageNet). You have both coarse label D,E,F and fine-grained label G,H,I,J,K,L. In this case, is fully supervised pretraining on G,H,I,J,K,L more beneficial on set level coarse pretraining on D,E,F more beneficial?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8741/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8741/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8741/Reviewer_4JEd"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8741/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797402568,
        "cdate": 1698797402568,
        "tmdate": 1699637096958,
        "mdate": 1699637096958,
        "license": "CC BY 4.0",
        "version": 2
    }
]