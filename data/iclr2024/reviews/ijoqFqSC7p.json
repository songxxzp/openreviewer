[
    {
        "id": "85rEzsnT5s",
        "forum": "ijoqFqSC7p",
        "replyto": "ijoqFqSC7p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission611/Reviewer_17jd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission611/Reviewer_17jd"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a training-free method for extending capabilities of short video diffusion models to generate temporally coherent, longer videos, as well as incorporate multi-text conditioning. The authors propose a novel noise schedule and fused window-based temporal attention to enable more in-distribution, coherent longer generations. In order to enable multi-text conditions, the authors introduce a motion injection method based on conditioning different text prompts at different stages of diffusion sampling."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well written, and easy to understand\n- The proposed noise schedule + temporal attention modification is interesting as a method to enforce better long-term consistency in video\n- Incorporating the proposed method is fairly simple, as it does not require any extra training on a pretrained short text-video diffusion model\n- Experiments are concrete, and show much quality generations compared baselines"
            },
            "weaknesses": {
                "value": "- From looking at the generated videos, although the proposed method can more cleanly generate longer videos, it seems that the spatial structure of the video (e.g. location of a cat) is very similar throughout the entire video. I believe this may be due to the repetitive nature of shuffled noise reptitions which are generally highly correlated with the structure of the resulting video. So it seems that the method may have a hard time generating more dynamic changes in long videos, such as a cat walking across the screen or scene / camera changes. Could the authors comment on this, or if there are generated video examples with larger structural changes through the video?"
            },
            "questions": {
                "value": "- How would the proposed window fusing (weighted by frame index distance) compared to a simpler scheme such as just merging the current and/or prior window (i.e. similar to a one or two hot version of the frame index weighting).\n- Have the authors explored other noise augmentation other than shuffling? In general, shuffling does not seem to be the most intuitive method for perturbing gaussian noise, as it also constrains the epsilons (per frame) to the original samples.\n- Section 3.1 mentions that \"temporal attention is order independent\". Does this imply that the VideoLDM does not have temporal positional embeddings? Or how would it be order independent if it did?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission611/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698349591316,
        "cdate": 1698349591316,
        "tmdate": 1699635988822,
        "mdate": 1699635988822,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TFV7NkJIg4",
        "forum": "ijoqFqSC7p",
        "replyto": "ijoqFqSC7p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission611/Reviewer_C7UC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission611/Reviewer_C7UC"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to extend the generative capabilities of pre-trained video diffusion models without incurring significant computational costs. It contains three different components, window-based temporal attention, noise rescheduling, motion injection."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method is cleverly represented and very easy to follow.\n2. The proposed method is much more efficient than the baseline method Gen-L [1].\n3. The observation and Analysis part is well-designed and inspiring, and I appreciate this section.\n\n\n\n[1] Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising"
            },
            "weaknesses": {
                "value": "1. Demo quality. From the demos, we can see that the motions of most generated results are restricted. For example in the ablation study, when the horse running, the background and the position of the horse do not change (even though the legs are moved), which is not a reasonable motion. Therefore, I would say that the proposed method corrupts the motions of the original diffusion models.\n\n2. Many ideas of the paper are used in previous works already. (1) For noise rescheduling, Reuse and Diffuse [1] proposed to reuse previous noise in the later frames. (2) For window-based temporal attention, Align Your Latents [2] applies a similar idea for long video generation, which does not change the temp conv part, but uses sliding local attention to resue the trained temporal attention. I think there's no intrinsic difference. (3) The motion injection part: interpolating the context is already proposed in Gen-L [3].\n\n3. The author says they picked only 100 prompts for quantitative experiments and then generated 2400 videos. This statement seems not explicit.\n\n\n[1] Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation\n[2] Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models\n[3] Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission611/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission611/Reviewer_C7UC",
                    "ICLR.cc/2024/Conference/Submission611/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission611/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698591769257,
        "cdate": 1698591769257,
        "tmdate": 1700648658340,
        "mdate": 1700648658340,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "louOef1EG3",
        "forum": "ijoqFqSC7p",
        "replyto": "ijoqFqSC7p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission611/Reviewer_pTfs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission611/Reviewer_pTfs"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to unlock the long video generation ability of a pretrained text-to-video generation model. The major technical components of this method include: 1. The analysis of artifacts and causes when generating long videos. 2. A noise schedule for long video generation. 3. Windowed attention fusion to keep attention perception field while avoiding content jump between windows, 4. Motion injection for varied textual prompts. The experiments show FreeNoise is a competitive long video generator."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This is a technically solid paper. Long video generation is a tricky long-standing problem. The authors propose a series of insights and techniques that have sufficient novelty to address the difficulties:\n1.\tThe analysis of long video artifacts and causes is valuable for developing better video generators. \n2.\tThe noise scheduling and window-based attention fusion address the long video difficulties mentioned in the analysis. They are simple yet effective. Window-based attention fusion addresses the notorious content jump problem, which will likely help develop future video generation foundation models.\n3.\tFreeNoise does not require additional UNet forward propagation. Therefore, the inference cost overshoot is low. \n4.\tThe qualitative results are marvelous. In human preference evaluation, FreeNoise still achieves the best. The authors provide an anonymous website to show more visual results. The image definition and motion consistency of FreeNoise are both good.\n5.\tThe motion injection technique successfully preserves video contents and drives the video to follow a new text prompt.\n6.\tThe qualitative ablations show each technical component of FreeNoise is effective and important."
            },
            "weaknesses": {
                "value": "Major concerns: \n1.\tI\u2019m interested in detailed experiment settings. Please include diffusion sampler configurations, sample resolutions, frame stride, etc. in your future revision.\n2.\tPlease add a pipeline figure. It is not very easy to fully understand how and where FreeNoise is working on generating long videos.\n3.\tIs direct inference, sliding, GenL, and FreeNoise sharing the same pretrained text-to-video model? If it is, then the evaluation is very convincing since they can all generate the same short video using the prompts but only FreeNoise can achieve good long video results.\n\nMinor concerns:\n1.\tPage 7. In the second line. A full stop is missing before \u2018Obviously\u2019.\n2.\tIn which case FreeNoise may fail? A discussion over the limitations is welcomed.\n3.\t100 evaluation prompts have limited diversity. If it is feasible, please add more evaluation prompts to make the comparison more convincing."
            },
            "questions": {
                "value": "1.\tThe authors claim FreeNoise achieves the maximum long video of 512 frames due to GPU memory limit. Is it possible to unlock even long video generation ability by using the CPU offload technique? \n2.\tWith the help of ControlNet, is it possible to generate more diverse motion with FreeNoise?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission611/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698630899771,
        "cdate": 1698630899771,
        "tmdate": 1699635988667,
        "mdate": 1699635988667,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rW1zw5AFeg",
        "forum": "ijoqFqSC7p",
        "replyto": "ijoqFqSC7p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission611/Reviewer_q7Nz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission611/Reviewer_q7Nz"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the effect of initial noise on a diffusion model for video generation, and thus proposes a method to extend the ability of a pre-trained model to generate long videos without fine-tuning, by rescheduling the initial noise of the video frame and by using a window-based temporal attention to achieve long-range visual consistency. finally, a new method of injecting motion trajectories is proposed, which allows the model to generate videos in response to multiple text prompt."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The proposed method is simple and effective to expand the model's ability to generate long videos without fine-tuning the model.\n\n2) The use of noise reschedule and window-based attention fusion allows for more consistent video generation.\n\n3) Motion inject allows the model to be fed with a variety of text prompts to generate longer videos with richer meanings."
            },
            "weaknesses": {
                "value": "1) The method described in this paper lacks suitable diagrams to help illustrate it.\n\n2) The proposed NOISE RESCHEDULING may limite the content variances of video generation since longer videos are produced by repeating the noises for the short ones. As is shown by the examples, the generated long videos looks like a short video that loops multiple times. I wonder whether this way can produce authentic long videos that contain continous various motions."
            },
            "questions": {
                "value": "Try adding more diagrams to better explain the methods in the article, such as noise rescheduling and an overview of the pipeline for generating a video using the methods mentioned in the article."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission611/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698727719969,
        "cdate": 1698727719969,
        "tmdate": 1699635988580,
        "mdate": 1699635988580,
        "license": "CC BY 4.0",
        "version": 2
    }
]