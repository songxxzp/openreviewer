[
    {
        "id": "4ud7IJN3Ny",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3881/Reviewer_7dUf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3881/Reviewer_7dUf"
        ],
        "forum": "abL5LJNZ49",
        "replyto": "abL5LJNZ49",
        "content": {
            "summary": {
                "value": "This paper proposes a new model called SCHEMA for procedure planning in instructional videos. The model leverages language models and cross-modal contrastive learning to track state changes and establish a more structured state space. The authors conduct experiments on three benchmark datasets and show that SCHEMA outperforms existing methods in terms of state recognition accuracy and task success rate. The paper's contributions include a new approach to procedure planning that accounts for state changes, a novel cross-modal contrastive learning framework, and a new benchmark dataset for evaluating procedure planning models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper has several strengths that make it a valuable contribution to the field of procedure planning in instructional videos:  \n\n1. Originality:  \n\n1.1  The paper proposes a new approach to procedure planning that accounts for state changes, which is a novel idea that has not been explored in previous works.  \n\n1.2 The authors leverage language models and cross-modal contrastive learning to track state changes and establish a more structured state space, which is a creative combination of existing ideas.  \n\n2. Quality:  \n\n2.1 The authors conduct experiments on three benchmark datasets and show that SCHEMA outperforms existing methods in terms of state recognition accuracy and task success rate, which demonstrates the quality of their proposed approach.  \n\n2.2 The paper is well-written and well-organized, making it easy to follow and understand.  \n\n3. Clarity:  \n\n3.1 The authors provide clear explanations of their proposed approach and the experiments they conducted, making it easy for readers to understand their contributions.  \n\n3.2 The paper includes helpful visualizations and tables to illustrate their results and comparisons with existing methods."
            },
            "weaknesses": {
                "value": "While this paper has several strengths, there are also some weaknesses that could be addressed to improve the work:  \n\n- The paper could benefit from a more detailed discussion of the limitations of the proposed approach. For example, the authors could discuss cases where the model may struggle to recognize state changes or situations where the model may not be applicable.  \n\n- The paper could provide more information on the computational requirements of the proposed approach."
            },
            "questions": {
                "value": "1. Could you provide more information on the computational requirements of the proposed approach? Specifically, what hardware and software were used to train and run the model, and how long did it take to train the model? \n\n2. How does the proposed approach handle cases where the state changes are not explicitly shown in the video? For example, if a video shows a person making a sandwich, but does not show the person adding mayonnaise, how would the model recognize this state change? \n\n3. Can you provide more information on the limitations of the proposed approach? Specifically, are there any cases where the model may struggle to recognize state changes or situations where the model may not be applicable?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3881/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697167599956,
        "cdate": 1697167599956,
        "tmdate": 1699636346781,
        "mdate": 1699636346781,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mXceMaGSR1",
        "forum": "abL5LJNZ49",
        "replyto": "abL5LJNZ49",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3881/Reviewer_FxtF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3881/Reviewer_FxtF"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel procedural planning method that models the task elegantly as a joint probability of time-series states and actions conditioned by start and end states. The method fills the gap of ungiven states before and after each action by LLM with a Chain-of-though prompt. Ground truth of actions and the estimated states are used to train the model with reasonable loss functions. Experiments show the clear superiority of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The motivation is clear.\n- The presentation is clear.\n- The query design for the state decoder is elegant (a sequence of state vectors, where a known step is the sum of encoded state feature and positional embedding and an unknown step is only positional embedding).\n- Augmenting state description with LLM from action labels is novel.\n- The reported results are thorough and look promising."
            },
            "weaknesses": {
                "value": "1. Overlooked related work\n\nThe authors overlook two studies focusing on state transition in instructional videos.\n\nFirst, in the paragraph \"Instructional video analysis,\" a dense video captioning method [a] is missing. The method tracks material state change with a MemNet-like architecture [a]. It trains state-modifying actions with distant supervision. It also analyzes the state change obtained as a shift in the latent space. Thus, it definitely relates to this work but is missing.\n\n[a] T. Nishimura et al., \"State-aware Procedural Video Captioning,\" ACMMM, 2021.\n\nSimilarly, in the same paragraph, the authors claimed, \"there are few discussions on state changes in complex videos with several actions, especially instructional videos.\" However, [b] models such complexity of the instructional video as an action graph to retrieve the goal state image with an instructional text (that directs actions) and an image before the action. The authors adequately refer to this study since the work also tries to model state-action relations in complex instructional videos.\n\n[b] K. Shirai et al., \"Visual Recipe Flow: A Dataset for Learning Visual State Changes of Objects with Recipe Flows,\" COLING2022.\n\n2. Minor flaws in presentation.\n\nThe first sentence in 3.3.1 explains about 3.4 but mentions nothing about the content in 3.3.1. This part was confusing for this reviewer.\n\nThe paragraph \"Masked Step Modeling\" in 3.4 claims that \"ground-truth answers $a_t$\"; however, for the readers, it is not known whether ground-truth actions are given at training or not. Please fix this problem.\n \nFYI\nIn Figure 2, there is a type \"oancake.\" However, it is not clear whether the typo is by GPT-3.5 or the authors."
            },
            "questions": {
                "value": "Please point out any factual errors in this review if the authors find them."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed.",
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Privacy, security and safety",
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)",
                    "Yes, Potentially harmful insights, methodologies and applications",
                    "Yes, Responsible research practice (e.g., human subjects, data release)",
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)",
                    "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)",
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3881/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698406572327,
        "cdate": 1698406572327,
        "tmdate": 1699636346700,
        "mdate": 1699636346700,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BSWQrKXQVe",
        "forum": "abL5LJNZ49",
        "replyto": "abL5LJNZ49",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3881/Reviewer_hqkF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3881/Reviewer_hqkF"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new framework for procedure planning in instructional videos called SCHEMA, which leverages LLM and cross-modal contrastive learning to track state changes and establish a more structured state space. The authors introduce a chain-of-thought prompting approach to describe state changes and use a mid-state prediction module to improve performance. The SCHEMA model is evaluated on three benchmark instruction video datasets, CrossTask, COIN, and NIV, and achieves state-of-the-art performance in terms of SR, mAcc, and mIoU."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Originality**: The authors propose a new framework for procedure planning in instructional videos that emphasizes the importance of state changes, which is a novel approach to the problem formation. The use of chain-of-thought prompting to describe state changes is a creative and effective way to leverage language models for this task. The idea of mid-state prediction module is interesting and seems to improve the performance of the model. \n\n**Quality**: The paper is well-written and well-organized, making it easy to follow and understand. The experiments are thorough and well-designed, with results presented in a clear and concise manner.\n\n**Clarity**: The paper is written in clear language and is easy to understand. The figures and tables are easy to read, providing a clear summary of the results.\n\n**Significance**: The results demonstrate the effectiveness of the proposed approach and suggest that it could be a valuable tool for procedure planning in instructional videos."
            },
            "weaknesses": {
                "value": "* Novelty: While the paper proposes a new framework for procedure planning in instructional videos, some of the individual components of the framework (such as LLM and cross-modal contrastive learning) are not novel in themselves. I personally loathe the trend that LLM+everything -> novelty. Thus I feel that the contribution of this proposed framework is incremental. That being said, I recognize that the authors have done non-trivial work in incorporating these components and perform thorough experiments. \n\n* Failure cases: The paper does not provide a detailed analysis of the limitations of the proposed approach or potential failure cases. I would be interested in seeing more examples of failures cases and with detailed explanations on why those cases have failed. \n\n* Scaling up: While the proposed approach shows promising results, the paper does not provide a clear explanation of how it could be applied in real-world scenarios or how it could be scaled up to handle larger datasets. Please note that I do not suggest that the model has to be able to handle larger datasets as long term prediction is hard by nature, but an analysis on the model's potential would be useful. \n\n* The paper could benefit from more detailed explanations of the experimental setup and methodology, particularly for readers who wish to replicate the experiments. I find it difficult to replicate the model and experiment based on information provided in appendix A/B."
            },
            "questions": {
                "value": "* Can you provide more details on the mid-state prediction module? How does it work, and how does it differ from existing mid-state prediction methods?\n\n* Can you provide more examples of failures cases and with detailed explanations on why those cases have failed?\n\n* Can you provide how the model could be scaled up to handle larger datasets?\n\n* Can you provide more detailed explanations of the experimental setup and methodology?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3881/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698686169555,
        "cdate": 1698686169555,
        "tmdate": 1699636346610,
        "mdate": 1699636346610,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NjkWUWNSg5",
        "forum": "abL5LJNZ49",
        "replyto": "abL5LJNZ49",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3881/Reviewer_253f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3881/Reviewer_253f"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on the task of procedure planning in instructional videos. Given an initial visual state and a goal visual state as input, the model is tasked with generating a sequence of action steps to form a procedure plan, guiding the progression from the initial visual state to the goal state. The authors highlight the significance of states in these procedures and introduce State CHangEs MAtter (SCHEMA) to model state changes. Specifically, they prompt pre-trained large language models to describe the state changes at each step, enhancing learning the intermediate state and step representations. Then, they use cross-modal contrastive learning to align the visual state observations with language state descriptions. Experiments validate that the proposed method achieves  state-of-the-art performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper introduces a novel approach to address the task of procedure planning in instructional videos, placing a strong emphasis on state changes and utilizing pre-trained large language models. The motivations and ideas presented in this paper are reasonable.\n\nThe proposed method has achieved noticeable performance gains."
            },
            "weaknesses": {
                "value": "1. The clarity and composition of the paper could be enhanced. Please refer to the questions below.\n\n2. There has been a notable surge in research exploring the use of pre-trained large language models (LLMs) for video-related tasks, e.g., [1]. This submission aligns with this emerging trend, and its overarching idea is conceptually sound. However, the fairness of the comparisons drawn in the paper could become questionable due to the employment of LLMs. Further in-depth discussion and analysis may be necessary to fully understand the extent of the LLM\u2019s impact on the final results.\n\n\n[1] Zhao, Qi, Ce Zhang, Shijie Wang, Changcheng Fu, Nakul Agarwal, Kwonjoon Lee, and Chen Sun. \"AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?.\" arXiv preprint arXiv:2307.16368 (2023)."
            },
            "questions": {
                "value": "1. How does aligning visual state observations with language state descriptions *track* state changes? This process involves cross-modal contrastive learning; it is unclear how it could facilitate *tracking* over state changes.\n\n2. Why are step descriptions not utilized as external memory for the step decoder, while *state* descriptions are used instead? The same $D_s$ is employed in Sections 3.3.2 and 3.3.3.\n\n3. In Sec. 3.4, there are $a_i$ and $A_i$. Could you clarify how these two differ and specifically define $A_i$?\n\n4. In State Space Learning via vision-language alignment, is it necessary for the training data to include temporally localized states or actions corresponding to the intermediate states of procedure plans? While I presume the answer is no, Fig. 4(a) and Sec 3.4 leave some room for ambiguity.\n\n5. Why is Eq. (5) called \u201cMasked State Modeling\u201d? The method described does not involve any mask-based modeling or random masking; instead, it is just predicting intermediate locations in a given sequence. The use of the phrase \u201cMasked State/Step Modeling\u201d seems to be an overstatement.\n\n6. What does \u201cDCLIP\u201d refer to in Table 5?\n\n7. Could you also present the results on procedure planning metrics in Table 7?\n\nMissing related literature:\n\n- Li, Zhiheng, Wenjia Geng, Muheng Li, Lei Chen, Yansong Tang, Jiwen Lu, and Jie Zhou. \"Skip-Plan: Procedure Planning in Instructional Videos via Condensed Action Space Learning.\" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10297-10306. 2023.\n\n\n- Fang, Fen, Yun Liu, Ali Koksal, Qianli Xu, and Joo-Hwee Lim. \"Masked Diffusion with Task-awareness for Procedure Planning in Instructional Videos.\" arXiv preprint arXiv:2309.07409 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3881/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3881/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3881/Reviewer_253f"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3881/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815441509,
        "cdate": 1698815441509,
        "tmdate": 1700611262039,
        "mdate": 1700611262039,
        "license": "CC BY 4.0",
        "version": 2
    }
]