[
    {
        "id": "nfFJboj4II",
        "forum": "ut9aUpFZFr",
        "replyto": "ut9aUpFZFr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6340/Reviewer_5igT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6340/Reviewer_5igT"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a clustering-based approach to accelerate knowledge graph inference tasks such as link prediction. The basic idea (like prior work) is there must be a clustering of entities that makes most of the relations intra-cluster. The proposed method learns embeddings for clusters and nodes, and at inference time compares the embedding for the query node to all cluster representatives, picks the best cluster and then compares against all the entities in that cluster.\n\nThe authors show significant speedup on some KG inference tasks with decent quality losses."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Formalizes the clustering-based approach and shows that it has decent performance.\n- Attempts at explaining when the new algorithm is helpful.\n- The new method speeds up training because not all weights are updated in each training step."
            },
            "weaknesses": {
                "value": "Experiments are limited:\n- More datasets should be evaluated, especially bigger ones like WikiKG90M-LSC.\n- Trade-off between error, speedup and number clusters should be investigated.\n- Static min-cut algorithms (outside the end-to-end training) could be compared.\n- What's the relationship between cut size, speedup and performance?\n\nMissing literature\n- Minimum cut literature studies the problem of reducing the number of inter-cluster relations.\n- tf-GNN is another scalable GNN framework which uses sampling for large datasets."
            },
            "questions": {
                "value": "- For dense graphs, the cut will be really poor. FB15K is the densest graph you considered. How dense is it per relation type? Is the cut quality poor here? Does that explain some of the results?\n- Did you consider producing different clusters for different relation types? Are the \"optimal\" clusters correlated?\n- What's the breakdown of speedup for training and inference?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6340/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698755996529,
        "cdate": 1698755996529,
        "tmdate": 1699636698290,
        "mdate": 1699636698290,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ipv2JQ8L5s",
        "forum": "ut9aUpFZFr",
        "replyto": "ut9aUpFZFr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6340/Reviewer_zPEm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6340/Reviewer_zPEm"
        ],
        "content": {
            "summary": {
                "value": "This paper studies a graph representation that takes clustering/coarsening information into account. The proposed algorithm partitions the vertices into a smaller number groups, and builds the embedding from a combination of inter and intra cluster objectives. The paper gives some bounds on the performances of this scheme, and experimentally shows a speed up factor of 4~10 plus slight improvements in prediction qualities."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed scheme is natural, and the performance gains obtained are significant. Most other high performance embedding schemes I'm aware of directly go to continuous / geometric representations. Having a more graph theoretic intermediate stage feels useful for both designing faster algorithms and better leveraging graph structures."
            },
            "weaknesses": {
                "value": "The theoretical justifications are mostly limited to the running times, and don't seem to go into details about why the prediction estimates obtained are also better.\n\nThe data sets for the experiments are somewhat less well-known: so from the paper itself it's a bit hard to compare this approach with other embeddings (although there literature there is huge)."
            },
            "questions": {
                "value": "As someone unfamiliar with knowledge graphs (my backgrounds are more in optimization / numerical methods), a direct comparison of the overall objectives optimized in the Leiden algorithm and this algorithm would be quite helpful: right now I'm only able to piece together the overall objective function from the pseudocodes."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6340/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699212540029,
        "cdate": 1699212540029,
        "tmdate": 1699636698135,
        "mdate": 1699636698135,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "b2tngx3CKT",
        "forum": "ut9aUpFZFr",
        "replyto": "ut9aUpFZFr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6340/Reviewer_2jBS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6340/Reviewer_2jBS"
        ],
        "content": {
            "summary": {
                "value": "The draft presents a method to accelerate the knowledge graph single-hop tail query answering by hierarchical prediction from community detection. It shows that predicting relations between communities/clusters can be pretty accurate (for some embedding methods). So, to predict a tail query, we can avoid querying every node and instead predict the cluster and the node inside the cluster. Further, the authors decompose the embedding to model into \"inside-cluster,\" \"outside-cluster,\" and \"inter-cluster\" and blend them in the loss function. They show that the proposed method is pretty promising: when well-configured, it didn't decrease the evaluation metrics and is times faster than the naive all-node prediction."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The draft verifies the assumption that cluster hierarchical prediction may be much easier than node prediction and has commercial potential to accelerate tail query answering. It compares multiple embedding methods in the framework and does a set of ablation studies on the hyper-parameters, including the resolution parameter used in the modularity maximization. It is good to see that the community detection algorithm works in knowledge graph domains. Hierarchical prediction methods are well-known and sometimes required in extreme classification and nearest-neighbor queries. In knowledge graph prediction, it's mostly considered an engineering hack, but the draft verifies the assumption in the selected datasets."
            },
            "weaknesses": {
                "value": "However, the paper is not polished enough in mathematical rigor, typos, and organization. More possibly, it wasn't proofread before a hasty conference submission. This happens, but there are too many bugs to fix. And even if we remove all the mathematical-related parts, the experiments need to be stronger to be a pure evaluation paper. Thus, I suggest to reject the draft, and the detailed weakness is listed below.\n\nFirst, The proposition doesn't prove the author's remark. For proposition 1 (equation 2), proving the lower bound on runtime didn't prove that your algorithm is better. I may show a lower bound of zero, and it says nothing. You need to prove the tighter upper bound for your optimized cluster size. The current upper bound is simply trivial, and I see it's possible to make your lower bound an upper bound (just substitute the values). For proposition 2, equation 3 simply moves the left-hand side to the right-hand side. You need to specify the scenario and quantify the \"expected time to a correct answer\" in your proposition. Also, I need clarification on why the derivation is related to Prop 1 (with a missing constant 2). It should be simply \"Our method is better when ratio A is better than ratio B\" in the derivation.\n\nSecond, you cannot control the cluster size of the community detection algorithm. Tuning the resolution parameter changes the number of clusters, but the size of each cluster depends on the graph structure and cannot be easily homogenized. There might be communities of a few nodes, and there might be a community consisting of 1/4 of the nodes. So, the analysis is actually \"acceleration at the best case.\" The result is okay from a practical perspective, and showing good acceleration results is good enough.\n\nFinally, the experiments don't support your claims in the introduction. All the 3 traditional datasets the authors tried can be run within hours or minutes on a single desktop. The experiments did show acceleration (in terms of vector evaluations), but the result doesn't support scalability compared to the scales in experiments from DistDGL or SMORE."
            },
            "questions": {
                "value": "Major questions:\n1. (Algorithm 2 line 11) The $L$ function is actually implicitly parameterized by the graph structure and the negative-sampling methods. However, there are now three graphs: the community graph the intra- and inter-community graph. So when will each be used in the loss function? And what's the difference in sampling? For example, the node $\\omega$ won't appear in the testing set but has many edges. How are they integrated?\n2. (item 4 in page 5) Usually, the loss function is not convex to the embeddings. And there's no info on the refinement used.\n3. (item 2 in page 5) The big-O notation is wrong. We always need to sweep through all embeddings.\n4. (Sec 2.3.3 must have... be minimal) Modularity maximization (the Leiden method) doesn't purely minimize the inter-group edges.\n5. (Sec 3.1) What's the purpose of adding the $\\omega$ node?\n\nMinor issues:\n1. (Definition 3) citation.\n2. (Definition 4) $\\subseteq$ instead of $\\in$\n3. (Def 4 & 5) The \"maximized\" argument conflicts with the loss function.\n4. (Sec 2.3.1) citations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6340/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699364110612,
        "cdate": 1699364110612,
        "tmdate": 1699636698011,
        "mdate": 1699636698011,
        "license": "CC BY 4.0",
        "version": 2
    }
]