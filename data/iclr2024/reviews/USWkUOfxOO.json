[
    {
        "id": "myFGIogAzV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission516/Reviewer_M3LQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission516/Reviewer_M3LQ"
        ],
        "forum": "USWkUOfxOO",
        "replyto": "USWkUOfxOO",
        "content": {
            "summary": {
                "value": "The paper proposes a confidence calibration method for unsupervised domain adaptation. First, the adapted network is applied to the samples from the target domain to obtain pseudo labels. Next, a mixup technique is used on pairs of target samples with different pseudo-labels to create a synthetic sample and a soft-label from each pair. Finally, the synthesized pseudo-labeled set is used to find the optimal Temperature Scaling by minimizing the negative likelihood function."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is clearly written and easy to follow. The paper reports extensive experiments and shows improved results compared to previous methods. It is interesting to see that previous UDA calibration methods don't work well and in some cases, they are even worse than the uncalibrated model."
            },
            "weaknesses": {
                "value": "My main concern is that the method is heuristic and not clearly motivated. It is well known that a mixup training procedure yields more calibrated models. However, it is unclear from the paper why learning Temperature Scaling using a convex combination of pseudo-labeled samples yields meaningful temperature values.  How is this method derived from the UDA situation?  What happens if we apply the\nsame technique in a standard network training setup (without domain shift problems ) to calibrate the network?\nThe mixture coefficient lambda is set to 0.65.  Again this looks like a heuristic.  is there any theoretical justification for this value? \nI would expect a symmetrical mixture combination (lambda=0.5) to be a suitable choice.  The success of a UDA method in different domain shift situations depends on the size of the gap between the domains.  I would expect that the UDA calibration would be dependent on how well the UDA method manages to close the domain gap. I don't see how the proposed method handles this aspect of the UDA calibration problem."
            },
            "questions": {
                "value": "What happens when we apply the proposed mixup procedure to other UDA calibration methods such as vector scaling and matrix scaling?\n\nWhat happens if we apply the same technique in a standard network training setup (without domain shift problems ) to calibrate the network?  Does it work well?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission516/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697354428433,
        "cdate": 1697354428433,
        "tmdate": 1699635978534,
        "mdate": 1699635978534,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "B7FEX7cRrW",
        "forum": "USWkUOfxOO",
        "replyto": "USWkUOfxOO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission516/Reviewer_SpZE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission516/Reviewer_SpZE"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a post-hoc calibration method for unsupervised domain adaptation. It utilizes the typical temperature scaling method and adapts it to UDA by generating pseudo-labels on target set, turning the unsupervised setting into a supervised one. Experiments on multiple datasets demonstrate the effectiveness of the method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper gives the interesting observation about NLL minimization: Temperature scaling provides similar temperatures if two datasets have similar correct-wrong patterns.\n2. Experiments are well-done and shows SOTA performance."
            },
            "weaknesses": {
                "value": "My concerns on this paper mainly concentrates on the proposed method, which I did not fully understand given the limited justifications.\n1. Given the first point mentioned in \"Strengths\", the paper only claimed it as an observation but without further explaination.\n2. The method of using Mixup for pseudo-label generation is not convincing to me, even given the \"Analysis\" section. (a) While the ground-truth labels are not available, can I just assign random labels for each target sample and then adopt PseudoCal (i.e. How is the UDA inference process helpful)? (b) Since $\\lambda$ is a fixed scalar and we do not know whether the target sample prediction result is true or not, what is the point of the last 8 lines in \"Analysis\" part. (c) The pseudo code in Appendix A does not do what equation (3) says for $y_{pt}$. Also, since $\\lambda$ is a fixed scalar in (0.5, 1.0), what does the \"if else\" statement for pseudo_target_y do as \"else\" is never visited?\n3. What is the validation set adopted for Temperature scaling in the experiments?\n4. Table 2~5 shows a phenomenon: Temperature scaling sometimes achieves the best performance. What might be the reason?\n5. The style of figure 2 is new to me. However, not good-looking nor clear."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission516/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission516/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission516/Reviewer_SpZE"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission516/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697837286099,
        "cdate": 1697837286099,
        "tmdate": 1700587548684,
        "mdate": 1700587548684,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SEbsY4N4Qh",
        "forum": "USWkUOfxOO",
        "replyto": "USWkUOfxOO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission516/Reviewer_dbpq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission516/Reviewer_dbpq"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors propose PseudoCal, which applies a mix-up strategy to generate pseudo-labels for unlabelled data to predict calibration uncertainty in an unsupervised domain adaptation problem. The proposed approach is examined on multiple image classification and semantic segmentation benchmarks, showing its superiority over SOTA."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper is written well, and the idea is easy to follow.\n+ The experiments cover extensive benchmarks of multiple settings, e.g closed-set, partial-set and Source-free,  and show marginal performance boosing w.r.t ECE. \n+ A simple and clear pseudo code is provided in the appendix."
            },
            "weaknesses": {
                "value": "- The novelty is weak because mix-up is a well-known strategy in image classification tasks. From my perspective, the methodology is the same as the previous ICLR 2018 work, \"mixup: BEYOND EMPIRICAL RISK MINIMIZATION\". \n- The mix-up is conducted at input level, which would introduce artifacts. It would be good to include some visualization in the appendix together with quantitative results. \n- I am skeptical about some numbers in the table. When lamba = 0 or 1, PseudoCal becomes typical TempScal with pure negative or positive samples. However, the ECEs shown in the tables are different. For example, in Home dataset, TempScal gets 11.06 but PseudoCal with lamba = 1 achieves ~20.\n- How did the author get the results of other methods, cite from the original papers, or re-implement those approaches?"
            },
            "questions": {
                "value": "Please see the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission516/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698724366060,
        "cdate": 1698724366060,
        "tmdate": 1699635978372,
        "mdate": 1699635978372,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yO90B5485K",
        "forum": "USWkUOfxOO",
        "replyto": "USWkUOfxOO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission516/Reviewer_fgV1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission516/Reviewer_fgV1"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the unsupervised domain adaptation from the aspect of target-domain specific unsupervised. Specifically, the PseudoCal framework is proposed. In the inference stage, this method uses a mixup strategy to generate a pseudo-target set and perform supervised calibration on it. To clarify the effectiveness of the PseudoCal, the authors conduct analysis through the cluster assumption. The experiments show that the proposed method outperforms other state-of-the-art baselines on several datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe paper presents a new perspective to make up the UDA uncertainty calibration research and unifies the covariate shift and label shift scenarios in UDA.\n2.\tThe proposed method uses the generated pseudo-target set to convert the unsupervised problem into a supervised one.\n3.\tGood set of experiments for yielding state-of-the-art results on several datasets."
            },
            "weaknesses": {
                "value": "1.\tThe analysis of the mixup in the inference strategy is not sufficient. The comparison between the training stage mixup strategy in other methods and the inference stage is not obvious, the authors should strengthen your highlight.\n2.\tWriting can be improved. In addition, the experiment results could present more in the \u201cAccuracy\u201d metric, which is also intuitive."
            },
            "questions": {
                "value": "How many pseudo-target samples will be generated? Does the number of n_{pt} be the same with target sample number?\n\tTo find the value of mix ratio \\lambda, the authors analyze the ECE on different UDA scenarios. However, it seems that the black-box source-free scenario does not have a corresponding analysis. \n\tThe mixup strategy and the temperature scaling both are calibration methods. What are the results if the pseudo target set is just generated by the model predicting, not using the mixup strategy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission516/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698936588420,
        "cdate": 1698936588420,
        "tmdate": 1699635978300,
        "mdate": 1699635978300,
        "license": "CC BY 4.0",
        "version": 2
    }
]