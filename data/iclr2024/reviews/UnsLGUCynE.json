[
    {
        "id": "cGZMdhsepQ",
        "forum": "UnsLGUCynE",
        "replyto": "UnsLGUCynE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3846/Reviewer_sG3K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3846/Reviewer_sG3K"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes 3D Diffuser Actor, a new behavior cloning algorithm combining diffusion policy and 3D representations for multi-task robotic manipulation. Utilizing the 3D representation and the attention mechanism, the proposed method achieves new SOTA on RLBench tasks. The authors also conduct real robot experiments with the newly proposed method, showing the applicability of the diffusion-based actor."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- **Motivation is good and natural**. Diffusion policies have achieved success in fitting distributions, and introducing them into 3D is a necessary step.\n- **Good results and extensive experiments.** The results (12\\% improvements) seem to be significant, which are gained on a multi-task benchmark across diverse tasks, and showing some real robot experiments are also very necessary for such robotic manipulation agents."
            },
            "weaknesses": {
                "value": "- **No deeper analysis about why diffusion models could help**. The ablation results only show two factors matter, but all these factors seem to be not novel and not surprising, thus deeper analysis might be necessary. I have also checked the supplementary files and the presented Figure 7 looks interesting, but why `scaled linear` is worse than `square cosine` when the former one seems to cover the original distribution better?\n- **The inference time and the denoising steps are both not clear.** The proposed method achieves 12% absolute gain, but considering the inference time of diffusion models, this gain might be not obvious in the real world, due to huge latency. Could the authors also report the wall time between different algorithms?\n- **The evaluation process is not clear**. Is the result in Table 1 the best success rate over a lot of checkpoints? And how many training \nepochs are used? How many episodes are tested during the evaluation? How many seeds are used for the main results (Table 1)?\n- **Lack of baselines in real robot experiments**. \n- **Lack of discussion and experiment comparison with recent related works such as GNFactor [2].** I think this very recent method [1] could possibly serve as a baseline and it would be good to see some direct experiment results.\n- **Lack of multi-task manipulation results in real robot experiments**. Both PerAct [1] and GNFactor [2] have shown ability to execute real-world multi-task manipulation, and it could be good to compare the multi-task performance in real robot also, considering this work is closely related to PerAct [1] and GNFactor [2].\n- **Typo** in Figure 2 (a): Acter -> Actor\n\nOverall, I tend to reject this paper with a score slightly lower than borderline,  considering the above issues for the initial review.  I would carefully consider raising my score if my questions are well addressed. \n\n[1] Shridhar, Mohit, et al. \"Perceiver-actor: A multi-task transformer for robotic manipulation.\" CoRL, 2022.\n\n[2] Ze, Yanjie, et al. \"Gnfactor: Multi-task real robot learning with generalizable neural feature fields.\" CoRL, 2023."
            },
            "questions": {
                "value": "See `weakness` above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3846/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3846/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3846/Reviewer_sG3K"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698589382225,
        "cdate": 1698589382225,
        "tmdate": 1701061226401,
        "mdate": 1701061226401,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "K1huDRTrie",
        "forum": "UnsLGUCynE",
        "replyto": "UnsLGUCynE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3846/Reviewer_Ws6y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3846/Reviewer_Ws6y"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a framework that marries 3D scene representations and diffusion policies for imitation learning of robot manipulation tasks. The proposed scene representation is a 3D feature cloud fused from multi-view, multi-scale CLIP features using depth maps, which encodes semantics and spatial information. The diffusion policy captures the multimodality of action distribution. In the experiments, the proposed method is compared against multiple baselines on the simulated RLBench dataset, and tested on 5 real-world tasks. Ablation studies further verify the necessity of the 3D representation and the use of relational attention.\n\nNote that many of the design choices and setups in this work are largely based on Act3D [1].\n\n[1] Theophile Gervet, Zhou Xian, Nikolaos Gkanatsios, and Katerina Fragkiadaki. Act3d: Infinite\nresolution action detection transformer for robotic manipulation. CORL 2023."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is probably the first work to combine 3D scene representations with diffusion policy for learning robot manipulation tasks. The proposed framework demonstrates good performance in both simulation and real-world experiments, and establishes a new SOTA on RLBench tasks."
            },
            "weaknesses": {
                "value": "My initial impression of this paper is that some aspects of the method lack clarity, and certain paragraphs appear somewhat inconsistent. I found myself confused about specific technical details until I reviewed Act3D [1], a previous paper that this work heavily draws upon. \n\n**My primary concern about this work is that a portion of the technical method and writting appears to be directly borrowed from [1]. However, this relationship is not transparently acknowledged.**\n\n1. The proposed framework adopts the 3D scene representation (in a simplified form), 3D relational transformer, and training/evaluation setups from [1], which is never explicitly mentioned in the paper.\n2. The main contribution of this work lies in the use of a diffusion policy to capture multimodal action distributions. However, this aspect is not extensively discussed or thoroughly evaluated.\n3. Not only the writing style of this paper closely resemble Act3D [1], but some paragraphs have similar or even same counterparts in [1]. Detailed in ethics review."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Many paragraphs are directly borrowed from a previous work [1], some with minor rephrase. This potentially forms plagiarism. \n\n| level of similarity | paragraph in this paper | counterpart in [1]  |\n|----------|----------|----------|\n| Rephrased  |  \u201c2D and 3D scene representations for robot manipulation\u201d in Section 2   | \"Learning robot manipulation from demonstrations\" in Section 2   |\n| Rephrased & adapted  | the first two paragraphs in Section 3.2   | Section 3   |\n| Rephrased (almost the same)  | \"Scene and language encoder\" in Section 3.2 | \"Visual and language encoder\" in Section 3 |\n| Partially the same | \"Baselines\" and \"Evaluation metrics\" in Section 4 | \"Baselines\" and \"Evaluation metric\" in Section 4.1 |\n\n[1] Theophile Gervet, Zhou Xian, Nikolaos Gkanatsios, and Katerina Fragkiadaki. Act3d: Infinite\nresolution action detection transformer for robotic manipulation. CORL 2023."
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3846/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3846/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3846/Reviewer_Ws6y"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761260441,
        "cdate": 1698761260441,
        "tmdate": 1699636342674,
        "mdate": 1699636342674,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "d4BqPQKZ7z",
        "forum": "UnsLGUCynE",
        "replyto": "UnsLGUCynE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3846/Reviewer_7nbv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3846/Reviewer_7nbv"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes 3D Diffuser Actor, a transformer-based behavior-cloning method that combines the power of diffusion policies and 3D scene representations. The model tokenizes multi-view camera observations, language instructions, and the history proprioception information. A 3D relative transformer models the diffusion process, processing these tokens to predict the next gripper pose over several diffusion steps. The experimental results showcase the remarkable performance of the proposed method, outperforming several strong baselines in the RLBench environments."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The experimental results are exceptionally strong, demonstrating a substantial improvement over various recently proposed baselines (from late 2022 to mid-2023)."
            },
            "weaknesses": {
                "value": "1. The method's description lacks clarity, particularly concerning crucial components of the architecture. The appendix does not sufficiently clarify these ambiguities either.\n2. The backbone of the proposed network is extremely similar to Act3D, including using multi-view image input, using the pyramid network for feature exaction, the generation of the 3D feature cloud, and the 3D relative transformer. I understand that Act3D is a very recent work, however, as the authors are already aware of Act3D, proper discussion regarding the relationship between this work and Act3D should be addressed."
            },
            "questions": {
                "value": "1. When building the 3D scene feature cloud, the authors claim, `We associate every 2D feature grid location in the 2D feature maps with a depth value, by averaging the depth values of the image pixels that correspond to it.` What is a `grid` here? Is each 2D feature map from the feature pyramid network separated into an NxN grid akin to ViT?\n2. When generating the 3D feature cloud, how does the method handle cases where multiple points (pixels) from different views correspond to the same 3D location? Are the features averaged in such instances?\n3. What constitutes a visual token in this context? Are individual 3D points considered tokens?\n4. The performance of the proposed method is notably poor in close jar and sort shape. Could the authors provide a detailed failure analysis to shed light on these issues?\n5. Given that diffusion policies suggest the utility of diffusing multiple action steps into the future, has the method been evaluated with multiple keypoint steps in the diffusion process?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3846/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3846/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3846/Reviewer_7nbv"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778950900,
        "cdate": 1698778950900,
        "tmdate": 1699636342588,
        "mdate": 1699636342588,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ayugMukOcP",
        "forum": "UnsLGUCynE",
        "replyto": "UnsLGUCynE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3846/Reviewer_2s3N"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3846/Reviewer_2s3N"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a framework called \"3D Diffuser Actor,\" combining diffusion policies and 3D scene representations to enhance robot manipulation. Along with 3D scene features aggregated from single or multiple 2D image views using sensed depth, these policies enable improved generalization over 2D counterparts. The new model\u2019s architecture uses 3D scene representations to iteratively rectify robot 3D rotations and translations given a language description of a task. The experiments demonstrate the efficacy of the proposed framework by outperforming previous benchmarks in learning from demonstrations, both in simulated environment and real world."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors conduct a thorough evaluation of their method as they evaluate their method in both simulated environment and real world, and comparing them with existing strong baselines. A thorough evaluation have us better understand the proposed model and their actual performance."
            },
            "weaknesses": {
                "value": "1. The scientific contribution of this paper is unclear. Diffusion model could not be the contribution of this paper, and adopting diffusion mode for trajectory generation is not a new idea (see [a]). \n2. The figures in this paper lack sufficient illustrative value and information. For instance, Figure 1 appears to show the model taking a multi-view image as input, yet the caption indicates it uses a 3D scene feature cloud. A clear connection between these two elements would greatly improve understanding."
            },
            "questions": {
                "value": "Your diffusion model appears to produce only the target pose of the action, after which a trajectory is generated using the MoveIt planner given the initial joint state and target joint state. Does the robot execute the entire trajectory directly until the end, or does it update the target pose using the diffusion model and regenerate the trajectory after each forward step?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3846/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3846/Reviewer_2s3N",
                    "ICLR.cc/2024/Conference/Submission3846/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698977567875,
        "cdate": 1698977567875,
        "tmdate": 1700732373874,
        "mdate": 1700732373874,
        "license": "CC BY 4.0",
        "version": 2
    }
]