[
    {
        "id": "TK6XqM8ld4",
        "forum": "mHYkcQzdae",
        "replyto": "mHYkcQzdae",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7437/Reviewer_Vkxu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7437/Reviewer_Vkxu"
        ],
        "content": {
            "summary": {
                "value": "This paper deals with the micro-expression recognition problem. It proposes a Continuously Vertical Attention (CVA) block to model the local muscle changes excluding the identity information. It proposes a Facial Position Focalize (FPF) module to incorporate spatial information and also utilizes AU information to further improve performance. Experiments on two micro-expression datasets validate that the proposed method achieves superior performance than other models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation of the proposed method is reasonable. It proposes a CVA block to model the continuous local muscle changes.\n\n2. The paper is easy to follow and the proposed method is easy to understand.\n\n3. Experiments on two datasets show that the proposed method achieves superior accuracy and F1-score compared with other methods."
            },
            "weaknesses": {
                "value": "1. The novelty is limited. As far as I'm concerned, the continuous attention block and position calibration module are both proposed in the MMNet paper. It also utilizes the subtraction between the apex and onset to determine the local movement of muscles. Thus, the difference between this work and the previous work lies in that this paper uses vertical attention instead of both directions, which is not quite novel.\n\n2. This paper does not contain any visualization results. For example, what is the difference between vertical attention and both directions regards the attention maps? What are the feature distributions before and after using the AU information?\n\n3. Experiments on other datasets and other metrics are required. For example, MMNet also carried out experiments on the MMEW dataset. Furthermore, in paper [1], it carried out experiments on CASME3 and SMIC datasets. The performance of the two datasets is not enough to validate the effectiveness of the proposed method in the micro-expression recognition field. The authors should consider using the metrics of UF1 and UAR to make fair comparisons with the paper [1].\n\n[1] Micron-BERT: BERT-based Facial Micro-Expression Recognition"
            },
            "questions": {
                "value": "1. Why using vertical attention instead of both-direction attention could improve the performance? Is there any justice or similar phenomenon in other papers? To me, using vertical attention means some information in the horizontal direction is lost.\n\n2. In Table 2, It seems to me the most improvement of the performance is brought by the AU Embedding. What is the performance using CVA block + FPF block without AU Embedding? The last column has a typo, embeddir should be embedding."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7437/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7437/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7437/Reviewer_Vkxu"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7437/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698559931835,
        "cdate": 1698559931835,
        "tmdate": 1699636892912,
        "mdate": 1699636892912,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "y2p2lK8tX1",
        "forum": "mHYkcQzdae",
        "replyto": "mHYkcQzdae",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7437/Reviewer_Bj7L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7437/Reviewer_Bj7L"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a micro facial expression recognition framework that includes preprocessing, feature extraction, and classification. They propose a continuous vertical attention module that aims at local muscle movements and a facial position localizer that focuses on spatial information. They also included AUs for better information. Experiments are performed on CASME II and SAMM datasets and compared with previous methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper targets an important issue of micro facial expression recognition. It is well-written step by step and easy to follow. The encoding of vertical facial muscle movement is interesting. Multiple ablations were conducted to show the importance of each component. The proposed method gets better accuracy on two datasets."
            },
            "weaknesses": {
                "value": "The paper uses extra information( apex, onset frames and AUs) and in experiments, it is not clearly mentioned which previous work uses that information. My detailed comments and questions are as follow.\n1> Apex frame and onset frame are used to extract information about microexpression. Extracting these frames from a sequence of frames is itself a task. How do you compare your work with other methods that use whole sequences?\n2> AUs reveal a lot of information about expression. Getting AU information from images is again a challenging task. Authors are using directly available AUs in their method. How do you compare your method with other works that are using this readily available information?\n3> This work subtracts two frames for difference information. Did the authors encounter a situation where two frames are not aligned perfectly? Will it affect the further process and How do you encounter that?\n4> Since this work targets the removal of identity information, it will be better to mention papers that also target identity removal. (if possible experimental comparison too)\n5>How do the authors justify that vertical facial muscle movements are more important than horizontal ones (other than experiments)?\n6? In section 3.3 authors mention \u2018lenght\u2019 21 2D vector. What do you mean by length in a 2D vector?\n7> Is it possible to give some visuals of the vertical attention module?\n8>There are multiple space and dot errors in the paper. Please proofread carefully."
            },
            "questions": {
                "value": "Please see weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7437/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825160797,
        "cdate": 1698825160797,
        "tmdate": 1699636892771,
        "mdate": 1699636892771,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jCdkD4R0K2",
        "forum": "mHYkcQzdae",
        "replyto": "mHYkcQzdae",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7437/Reviewer_gEuL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7437/Reviewer_gEuL"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a pipeline for micro-expression recognition (MER), which are brief facial expressions often challenging to identify due to their short duration and partial muscle movements. Their approach includes a Continuously Vertical Attention (CVA) block to capture subtle facial muscle changes without focusing on identity information and a Facial Position Focalizer (FPF) module based on the Swin Transformer. The authors also integrated  Action Units (AU) information into the pipeline to enhance accuracy. Their experiments show that the proposed model achieved significant improvements compared to the state-of-the-art techniques for microexpression recognition. Further ablation studies signified using CVA, FPF and action unit information."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors attributed the novelty of the work to the use of a vertical attention module, a facial position focalizer using a Swin transformer and the use of AU embeddings to focus on active facial regions due to muscle activations. The modules were integrated into a dual-stream network to recognize microexpressions. The proposed ensemble was trained and validated on CASMEII  and SAMM datasets.\n\nThe proposed pipeline is novel, introducing a vertical attention module. However, the use of difference images and Swin transformer to extract feature representations and AU integration for embeddings are existing mechanisms in literature. Their integration with VAC does bring in an aspect of extended feature representation that is beneficial for better recognition accuracies."
            },
            "weaknesses": {
                "value": "The authors highlight the significant contribution of facial muscles in the vertical direction and other submodules for identifying microexpressions. However, the authors needed to substantiate this understanding within the experimental setup. The increase in accuracy and F1 score, is it statistically significant? Were these results cross-validated, and if so, what is the expected deviation from the reported results? \n\nIt also needs to be clarified whether the authors handled bias in the labels for the reported results. This information would be crucial to validate the results reported.\n\nIt would be beneficial to generate heat maps for the test samples to understand the areas of significance utilized by the pipeline for prediction purposes. This information may be included in the Appendix if necessary."
            },
            "questions": {
                "value": "1. Will the reported results change for vertical attention vs horizontal attention vs combined if the bias (if any) was handled? What was the size and distribution of the test set?\n\n2. Is the difference in reported results significant? \n\n3. If yes, what is the expected deviation from the mean during multiple folds of the training and testing?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7437/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699271683123,
        "cdate": 1699271683123,
        "tmdate": 1699636892671,
        "mdate": 1699636892671,
        "license": "CC BY 4.0",
        "version": 2
    }
]