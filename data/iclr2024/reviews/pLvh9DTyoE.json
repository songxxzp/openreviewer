[
    {
        "id": "4VVoFvltsA",
        "forum": "pLvh9DTyoE",
        "replyto": "pLvh9DTyoE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5229/Reviewer_Hmr3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5229/Reviewer_Hmr3"
        ],
        "content": {
            "summary": {
                "value": "This work focuses on low-resource multimodal named entity recognition (MNER). The authors reformulate MNER as an open-domain question-answering problem in a low-resource setting and provide analysis."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The topic and approach are interesting."
            },
            "weaknesses": {
                "value": "1. The contribution of this paper is quite limited. The authors use a large number of FLAN-based models (both text-only and multimodal) for multimodal NER tasks.\n2. The method is not new. QA for NER can be traced back to [1] and there are recent works using QA for all multimodal IE tasks in both zero-shot and few-shot settings.\n\n[1] Li et al., A Unified MRC Framework for Named Entity Recognition. ACL 2021.\n\n[2] Sun et al., Multimodal Question Answering for Unified Information Extraction. Arxiv 2023.\n\n1. The evaluation is very limited. More models, including both instruction-tuned (IlIava, MiniGPT4, etc.) and base multimodal models (Open Flamingo), should be evaluated. Additionally, analysis of more tasks would be more interesting. While multimodal NER is to-some-extend important, it is not representative enough. Evaluations on other multimodal information extraction tasks would be beneficial.\n2. Findings are not striking. With more models evaluated, we may observe more interesting insights and general observations from these models."
            },
            "questions": {
                "value": "No"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5229/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5229/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5229/Reviewer_Hmr3"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5229/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698213247183,
        "cdate": 1698213247183,
        "tmdate": 1700783253454,
        "mdate": 1700783253454,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CQOKsRSK9r",
        "forum": "pLvh9DTyoE",
        "replyto": "pLvh9DTyoE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5229/Reviewer_AQjM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5229/Reviewer_AQjM"
        ],
        "content": {
            "summary": {
                "value": "This paper uses the QA templates from Liu et. al to prompt FLAN-T5 based multi-modal LLMs to generate NER tags. Experiments have been presented on the Twitter 2015, 2017 datasets which have multi-modal tweets with labels for persons, organizations and locations. Models used to study the QA based prompts include FLAN T5, BLIP2 and Instruct BLIP.  On both the Twitter 2015 and Twitter 2017 datasets, a vanilla FLAN t5 based prompt appears to do better than the use of multi-modal models which perhaps suggests that the hypothesis posed by the paper was found to be untrue. I was also unable to appreciate the aspect of low-resource NER tagging which the authors seem to suggest was one of their focus areas in the work. Perhaps the authors could clarify both points in the rebuttal."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "None"
            },
            "weaknesses": {
                "value": "Please see main summary."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5229/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698603570507,
        "cdate": 1698603570507,
        "tmdate": 1699636521350,
        "mdate": 1699636521350,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yBkE43hHnf",
        "forum": "pLvh9DTyoE",
        "replyto": "pLvh9DTyoE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5229/Reviewer_xAYu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5229/Reviewer_xAYu"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel approach to Multimodal Named Entity Recognition (MNER) by framing it as an open-ended question-answering task, suitable for modern generative language models. The research offers insights into the complex interplay between model design, prompt construction, and training data characteristics affecting visual integration effectiveness. These findings inform future work in multimodal representation learning, generative modeling, and prompting at the intersection of text and image analysis."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper demonstrated low-resource MNER via prompting, with strong zero-shot performance. Analy\u0002sis of our results and related work elucidates both progress and challenges for this emerging area."
            },
            "weaknesses": {
                "value": "1. The methodology section of this work primarily focuses on the design of instructions and lacks technical innovation, making it unsuitable for publication in ICLR.\n\n2. I have several concerns regarding the statement \"Results against state-of-the-art models for MNER are provided in Table 7\" in Section 6: (1) Table 7 presents results from previous baselines in a fully supervised setting, but the experiments conducted earlier in this paper do not compare against these baselines. It is unclear how the conclusion \"Results against state-of-the-art models\" is derived. (2) The results of the method proposed in this paper are not included in Table 7. (3) To the best of my knowledge, Table 7 lacks citations and comparisons to papers in the MNER field from 2022-2023."
            },
            "questions": {
                "value": "How can we draw the conclusion with the statement \"Results against state-of-the-art models for MNER are provided in Table 7\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5229/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826072277,
        "cdate": 1698826072277,
        "tmdate": 1699636521248,
        "mdate": 1699636521248,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7QOWEvtiOe",
        "forum": "pLvh9DTyoE",
        "replyto": "pLvh9DTyoE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5229/Reviewer_enH8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5229/Reviewer_enH8"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the author proposes an attempt to convert MNER tasks into \"Instruction+QA\" to solve the problem of labeled data absence, utilizing the excellent zero-shot learning ability of multimodal LLMs to address the low-resource MNER.  A series of question prompt templates are designed to effectively integrate visual cues in images. Experimentals demonstrate the effectiveness of the approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "[1]Transforming multi-class named entity recognition (MNER) into an openended question-answering task is novel and effective in the experiments."
            },
            "weaknesses": {
                "value": "[1]The related works are not comprehensive enough. For example, it lacks some pre-trained models and multimodal NER models in the recent two years (2022-2023). It is unclear with the current presentation compared to previous works. For example, the below reference also considers the multimodal NER.\n-Fei Zhao, Chunhui Li, Zhen Wu, Shangyu Xing, Xinyu Dai. Learning from Different text-image Pairs: A Relation-enhanced Graph Convolutional Network for Multimodal NER. ACM Multimedia 2022: 3983-3992.\n-Lin Sun, Jiquan Wang, Kai Zhang, Yindu Su, Fangsheng Weng. RpBERT: A Text-image Relation Propagation-based BERT Model for Multimodal NER. AAAI 2021: 13860-13868.\n-Lin Sun, Jiquan Wang, Yindu Su, Fangsheng Weng, Yuxuan Sun, Zengwei Zheng, Yuanyi Chen. RIVA: A Pre-trained Tweet Multimodal Model Based on Text-image Relation for Multimodal NER. COLING 2020: 1852-1862\n[2]Based on the statistics on the datasets used, we can find that it is category unbalanced. Therefore, using the micro-f1 as the evaluation only may be not convincible.\n[3]The author(s) need to comprehensively research the related datasets. It is not like the authors claimed that \u201cNotably, these are currently two of the few openly accessible multimodal datasets for named entity recognition.\u201d \n-Dianbo Sui, Zhengkun Tian, Yubo Chen, Kang Liu, Jun Zhao.A Large-Scale Chinese Multimodal NER Dataset with Speech Clues. ACL/IJCNLP (1) 2021: 2807-2818\n-Xigang Bao, Shouhui Wang, Pengnian Qi, Biao Qin.Wukong-CMNER: A Large-Scale Chinese Multimodal NER Dataset with Images Modality. DASFAA (3) 2023: 582-596\n[4] The low-resource MNER ability of the proposed method mainly comes from the existing zero-shot learning of multimodal LLMs and a series of manually designed prompts, lacking further optimization for MNER.\n[5]: Lacking the zero-shot and few-shot experimental results of existing MNER methods (such as a series of BERT based models and LLMs based models )  in evaluation.  Unable to accurately measure the advantages of the proposed method compared to other methods.\n[6]All tmplates in the article are manually set, but there was no analysis of the experimental results with different tmplates. The impact of the different tmplates settings on LLMs for MNER should be noted."
            },
            "questions": {
                "value": "[1]It is not clear that the author(s) show the Zero-shot F1 in Figure 5 and Figure 6, which is not consistent with the previous claim. And the micro f1 scores by model with images is lower than that without images in Table 5 and Table 6? If there are some clerical errors?\n[2]How to compute the c(A_i) is not clear.\n[3]Table 4 is not referenced."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5229/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838738771,
        "cdate": 1698838738771,
        "tmdate": 1699636521163,
        "mdate": 1699636521163,
        "license": "CC BY 4.0",
        "version": 2
    }
]