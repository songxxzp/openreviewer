[
    {
        "id": "u6Nx3acbeo",
        "forum": "SQFDJLyJNB",
        "replyto": "SQFDJLyJNB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1406/Reviewer_si3G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1406/Reviewer_si3G"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenge of continual category discovery (CCD). CCD consists of two stages: the first stage involves training on a labeled dataset with known categories, while the second stage encompasses a continual setting where unlabeled data arrives sequentially. In each continual session, the objective is to identify new categories and classify all data instances, encompassing both known and novel classes. The authors approach CCD by substituting the prompt pool from L2P (Wang et al.) with a Gaussian Mixture Model (GMM). At every incremental step, both the last block of the backbone and the GMM module are updated. For prompting, the top-k Gaussian components (analogous to L2P) for each data instance are selected based on probability. The means of these components are then prepended to the input for inference. To mitigate the issue of forgetting, a subset of Gaussian samples is randomly chosen and forwarded to the next stage, allowing for dynamic expansion of the GMM modules. Additionally, the GMM module is coupled with a class number estimation method to gauge the count of emerging categories"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Expanding the prompt pool dynamically is beneficial and intuitively sound in a continual setting.\n- Based on the experimental results, modifying the prompt pool using GMM appears promising. However, some confusion still needs to be addressed.\n- Extensive experiments and comparison with SOTA methods."
            },
            "weaknesses": {
                "value": "- In the proposed method, the top-k \"means\" are treated as prompts. However, how should one interpret this? I am unclear about how adding the mean of the GMM can enhance the guidance provided to the backbone. It's worth noting that the GMM is detached from the backbone. Consequently, learning in the GMM module is somewhat disconnected from the optimization process. This is akin to integrating additional information, such as features, into the system.\n\n- Much emphasis has been placed on the assertion that previous prompt methods are unsuitable for CCD because they require full supervision. The proposed method aims to address this by fitting the GMM. However, this claim might be overly assertive. The relaxation in data labels primarily stems from the two loss functions (both supervised and unsupervised) presented in Eqs. 2 and 3. Such techniques are commonly used in GCD, as noted by Vaze et al. (2022).\n\n- The authors claim that 'these prompts facilitate adaptation to emerging data, thus preventing catastrophic forgetting.' However, in subsequent sections, the issue of forgetting is addressed by drawing samples from previous stages and reusing them in the second stage. This approach is reminiscent of a replay mechanism.\n\n- Concerning the estimation of the novel class number, Table 3 provides interesting insights. Apart from the fine-grained CUB200, the estimation remains relatively consistent, irrespective of the class number across different stages. Even though the estimation does not align perfectly with the ground truth, the final performance might actually surpass scenarios where the class number is known, as illustrated by the comparison between Tiny200 in Tables 2 and 3. Has the study provided any rationale or explanation for this observation?\n\n- Why is the final block of the backbone updated? Prompt learning aims to use prompts to 'instruct' the foundational model to access rich information without modifying it, as seen in methods like L2P and VPT[1]. It's worth noting that updating the backbone is geared towards acquiring new 'knowledge'. Is the overall performance improvement primarily attributed to the model being tailored to a specific dataset, or is it due to the 'instruction' provided by prompt learning?\n\n* The description in the GMP module is somewhat confusing: \n  - If I've interpreted it correctly, both the letter 'C' in Section 2.2 and the letter 'K' in Section 3.2 represent the number of classes. If that's the case, consistency in notation would be appreciated. Are the top-k prompts selected from among these C/K Gaussian components? \n  - The method for obtaining GMM samples (as represented by the dots in Fig. 4) for optimizing the GMM is unclear. \n  - How many samples advance to subsequent stages? \n  - The algorithm appears to have inherent randomness, both from the selection of Gaussian samples and the random selection of reusable samples for later stages.\n\n [1] Jia et al. \u201cVisual Prompt Tuning\u201d. ECCV 2022."
            },
            "questions": {
                "value": "Which query function is used? I assume the cls token of the backbone output is used as in L2P?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1406/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1406/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1406/Reviewer_si3G"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1406/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698715024414,
        "cdate": 1698715024414,
        "tmdate": 1699636068614,
        "mdate": 1699636068614,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "50OUKJfiQN",
        "forum": "SQFDJLyJNB",
        "replyto": "SQFDJLyJNB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1406/Reviewer_YA1L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1406/Reviewer_YA1L"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a prompt-based method for continual category discovery, which continually learns the keys of prompt by a GMM in an unsupervised manner. It also introduces a category estimation strategy based on GM when the number of new categories is unknown. The proposed method is evaluated on the benchmarks of continual GCD with comparisons to other baseline methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper proposes a GMM-based replaying strategy for the prompt-based method to address the continual GCD problem, which is a new and reasonable design for the task.\n\n- The experimental evaluation are conducted on both the settings of known and unknown class number with good results."
            },
            "weaknesses": {
                "value": "- The novelty of the paper is mainly on the proposed Gaussian Mixture Prompt Pool, which is rather limited. The authors argue that \"the fixed-size prompt module restricts the model's ability to select prompts to a maximum of $M$\",   but the proposed Gaussian Mixture Prompt Pool seems also a fixed-size prompt. In addition, what is the advantage of GMM compared to K-means, if we also store mean and covariance for replaying?\n\n- The adaptation of L2P and Dual Prompt is not clear, which is vitally important since the main novelty is the design of keys. Specifically, how to learn the key of those two methods?\n\n- In the prompt-based continual learning diagram, they only learn the prompt pool and fix the whole backbone. But this paper argues that \"During training, only the final block of the vision transformer is finetuned .....\".  What are the results if the backbone is fixed? Is the input query also learned by finetuning the vision transformer?\n\n- The conclusion in ablation studies is incoherent. When the prompt selection is set to 5, and GMM sampling is set to 100, the \"Spherical\" outperforms \"Diagonal\". But the optimal number of prompts seems to be 10. The conclusion of ablation studies should be adjusted.\n\n- Typo: there are many typo errors and some notation is unclear. For example, \"Fourth, We hypothesize that categories may share commonalities regarding colour, shape, and other factors. \",  and $B$ and $B^L$ in Eqn(4). Please carefully check the manuscript."
            },
            "questions": {
                "value": "See the comments in the above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1406/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698805870734,
        "cdate": 1698805870734,
        "tmdate": 1699636068526,
        "mdate": 1699636068526,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "C4QDVD6WSE",
        "forum": "SQFDJLyJNB",
        "replyto": "SQFDJLyJNB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1406/Reviewer_1nUF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1406/Reviewer_1nUF"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel approach to the problem of continual category discovery (CCD), where the goal is to label objects in an unlabeled data stream that arrives over time, containing both known and new categories. The authors introduce PromptCCD, which incorporates a Gaussian Mixture Prompt Module (GMP) to dynamically update and guide data representation, mitigating forgetting. PromptCCD also features a Gaussian Mixture-based module for estimating categories within the unlabeled data, eliminating the need for prior knowledge of the number of categories. Additionally, the paper adapts the evaluation metrics from generalized category discovery for CCD and conducts comprehensive benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1-It features an innovative Gaussian mixture prompt module that alleviates the need for label information and effectively addresses the issue of catastrophic forgetting.\n\n2-The methodology liberates the model from depending on a predetermined number of categories, enhancing its adaptability to real-world scenarios.\n\n3-The method is validated through extensive experimentation, where it consistently surpasses other state-of-the-art methods in performance.\n\n4-The literature review is thorough, encompassing a wide array of existing works in the field, thus providing a solid foundation for the proposed approach."
            },
            "weaknesses": {
                "value": "1-The figures included are complex and lack clear, informative descriptions, necessitating repeated reference to the text for full comprehension, which disrupts the flow of understanding.\n\n2-The method is described with an excessive level of detail that, while potentially beneficial, also burdens the reader with information overload. A more concise presentation, perhaps through structured pseudocode similar to that provided for the evaluation, could clarify the methodology more effectively. Additionally, the depiction of the method across Figures 2-4 is spread over multiple figures with captions that do not sufficiently convey the information, thereby diluting the explanatory power of the visuals."
            },
            "questions": {
                "value": "1-Given that a Gaussian Mixture Model (GMM) is employed, does the assumption of similarity between learning stages in conventional continual learning scenarios, which may vary greatly, potentially limit the model's capabilities?\n\n2-Considering that a portion of the data is initially labeled, is it possible to leverage this labeled subset to learn a preliminary GMM that could subsequently be integrated into the broader GMM for the entire dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1406/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833816906,
        "cdate": 1698833816906,
        "tmdate": 1699636068447,
        "mdate": 1699636068447,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LAVusyrnsC",
        "forum": "SQFDJLyJNB",
        "replyto": "SQFDJLyJNB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1406/Reviewer_k1JJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1406/Reviewer_k1JJ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a Gaussian Mixture Prompt Module (GMP) for Continual Class Discovery (CCD), a prompt learning technique that employs a Gaussian mixture model (GMM) as a dynamic prompt pool. The GMP overcomes the drawbacks of previous prompt learning methods by obviating the need for label information, adapting the parameterization based on data distribution, alleviating catastrophic forgetting, and exploiting shared commonalities among categories. The paper defines the optimization objectives for different learning stages, comprising supervised and unsupervised contrastive learning losses, as well as a surrogate loss to optimize the learnable prompt pool. The paper demonstrates that the proposed methods surpass existing approaches in terms of accuracy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper demonstrates the superiority of the proposed method over existing models by conducting a rigorous comparative analysis and achieving a substantial margin of improvement.\n- The paper provides a comprehensive and lucid presentation of the technical details, including equations, tables, and descriptions of the methodology. The paper elucidates the underlying concepts and methods in an accessible manner, enabling readers to grasp the essence and rationale of the approach.\n- The paper exhibits a high level of clarity and coherence in its organization and writing, facilitating the comprehension of the proposed approach and its merits."
            },
            "weaknesses": {
                "value": "- The method presented in this paper bears some resemblance to the one proposed in [1], but differs in that it tackles the CCD problem through incremental learning without any annotated information. \n- This method employs DINO as the pre-training model. Although a similar pre-training approach was also adopted in [1], the problems addressed are different. The Continuous Learning task has annotation information, so the utilization of DINO\u2019s self-supervised model is justified. However, in the CCD problem, there is no annotation information, and most incremental tasks are self-supervised. For models that are unsupervised, and the training data may have overlaps, using DINO is not equitable. \n- The performance enhancement of this method stems more from DINO, and the paper lacks a comparison with methods without DINO pre-training models. The table in the paper does not specify the model's parameters used by different methods.\n\n[1] Wang, Zifeng, et al. \"Learning to prompt for continual learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
            },
            "questions": {
                "value": "- In Appendix C of the paper, the authors propose a method for dynamically adapting to the number of unknown categories, denoted as PromptCCD+. The method employed by PromptCCD in the main text to cope with unknown categories is to fix the number of categories to a predefined value. \n- It would be beneficial to provide a direct comparison between the number of dynamically adapted unknown categories and the number of actual categories. This would offer a clearer insight into the effectiveness of the proposed method. \n- Most of the experiments in the paper are conducted in three stages. Is it feasible to extend the method to more incremental stages?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1406/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1406/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1406/Reviewer_k1JJ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1406/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698928347377,
        "cdate": 1698928347377,
        "tmdate": 1699636068355,
        "mdate": 1699636068355,
        "license": "CC BY 4.0",
        "version": 2
    }
]