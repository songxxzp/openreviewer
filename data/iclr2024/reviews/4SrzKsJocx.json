[
    {
        "id": "AWWp7yBuhb",
        "forum": "4SrzKsJocx",
        "replyto": "4SrzKsJocx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6299/Reviewer_KsZw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6299/Reviewer_KsZw"
        ],
        "content": {
            "summary": {
                "value": "Authors of this paper studies the variation preservation and covarying structure in dimensionality reduction (DR) methods for multimodal datasets through a generative linear model with known variance and covariance. The findings show that PSL and rCCA are preferred to OCA when detecting covariation is more important than variation preservation"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Empirical studies from various perspectives of the simulated data are performed where the data are sampled from a generative model including both self-signal and shared-signal."
            },
            "weaknesses": {
                "value": "The presented model is formulated based on strong independent assumption for all variables in the linear model, so it doubts that the data generated from this model can align well with real data and the findings are informative. The scope of this study is limited to a small set of models based on the generative linear model, and the findings may not be properly extended to other methods. The study of this paper is mainly numerical, so it is unclear if there exists theoretical result to explain the findings."
            },
            "questions": {
                "value": "This study is limited to specified methods, PCA, PLS and CCA, all of which are variance- or covariance-based models. It is unclear how the findings in this paper can be extended for broad family of dimensionality reduction methods.\n\nIn Section 2.2, authors present models (1) and (2) for each modality. It seems that all are random variables. Is every element in the random matrix i.i.d. sampled from a Gaussian with 0 mean and specified variance? Due to the strong assumptions used in (1) and (2), it is unknown how they align well with the generation process of real data. Authors should refer to the existing work like probabilistic PCA or probabilistic CCA for properly defining the generative linear model.\n\nBach, Francis R., and Michael I. Jordan. \"A probabilistic interpretation of canonical correlation analysis.\" (2005).\n\nIn Section 3, authors mentioned that training and test data sets are generated according to (1) and (2). Does it mean that all random variables are sampled accordingly to generate a sample pair X and Y? Due to some confusing in the definition of the presented models, it is better to describe the generation process in detail. For example, all samples may be generated with fixed U_X, U_Y and P. \n\nAs this paper concentrates on the empirical evaluation of existing models on the data sampled from the presented generative linear model. The evaluation metric can be important. Authors introduce the so-called reconstructed correlations RC\u2019, which is described in Appendix A.2. It is the scaled correlation of projected points in low-dimensional spaces obtained by corresponding models. The correlation values are within [-1, 1]. It is unclear why (15) should be in [0, 1]. And the measure RC_0 is introduced because the ideal uncorrelation is not achievable if the sample is few. But RC_0 is computed based on multiple random trials. That is to say, the evaluation metric is not deterministic. \n\nIn experiments, figures with gamma_self and gamma_shared are generated. How do the two parameters are generated to form a grid? Both parameters are functions of other three variances. \n\nAll the findings are concluded from the reconstructed correlations RC\u2019, which is biased to CCA for maximizing the shared signals. This may not be new. Moreover, the conclusion or suggestion made by authors can be strong. It is possible that rCCA works better than PCA, but it is unclear SDR works better than IDR."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6299/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698643896065,
        "cdate": 1698643896065,
        "tmdate": 1699636691945,
        "mdate": 1699636691945,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "z1P2ozEMXU",
        "forum": "4SrzKsJocx",
        "replyto": "4SrzKsJocx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6299/Reviewer_di89"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6299/Reviewer_di89"
        ],
        "content": {
            "summary": {
                "value": "This paper compares two approaches of dimensionality reduction for bimodality, namely, those methods independence between the modalities versus those assuming that some shared signal exist.\nThese methods are interested in different sub-blocks of a grand, unknown covariance matrix.\nThe authors provide a clear and well thought empirical comparison of both types of approaches.\nThe paper is mostly empirical and focusing on an artificial, fully-controlled framework for testing and comparing methods of each type."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well written, well organized, clearly structured, not missing anything with respect to its claims (which are reasonable, limitations are stated clearly as well).\nThe message, (limited) scope, contribution, and limitations are well described and clearly stated."
            },
            "weaknesses": {
                "value": "The technicality of the contribution is present but rather limited, as the paper is an empirical comparison of well established methods (PCA, CCA, etc.).\nThe limited scope makes it a pleasant paper to read, not too dense; the price to pay is that the novelty is weak and, as said, purely empirical and not unexpected knowing the intrinsic assumptions of the two different approaches.\nSome parts could be clarified, like when discussing the variance in 2.2 (an identical variance uniformly appleid to all entries of the matrices?it seems so but the sentence comes a bit late) and the figure captions (the first figure caption could spend some more sentences describing the elements of the figure).\nThe paper would gain in extending the experimental section to real data."
            },
            "questions": {
                "value": "None at this stage."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6299/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698677092984,
        "cdate": 1698677092984,
        "tmdate": 1699636691838,
        "mdate": 1699636691838,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "R0eh0VSoOv",
        "forum": "4SrzKsJocx",
        "replyto": "4SrzKsJocx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6299/Reviewer_7S2y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6299/Reviewer_7S2y"
        ],
        "content": {
            "summary": {
                "value": "This manuscript studies dimensionality reduction (DR) methods (PCA, PLS, CCA) for multimodal representation learning. To investigate these methods, the manuscript synthesizes data by introducing a generative linear model with known variance and covariance structures. The investigation explores whether the DR method extracts the relevant shared signal and identifies the dimensionality of the shared and self-signals from noisy, undersampled data. Based on investigation the manuscript suggests to prefer Simultaneous DR methods such as regularized CCA to recover covariance structures."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- Synthetic experiments for multiple cases"
            },
            "weaknesses": {
                "value": "**Novelty**:\n\n- The manuscript proposes a generative linear model for multimodal data. However, the model is known and can be found in the literature. For example, it can be found in the probabilistic form (Murphy et al., 2022; Klami et al., 2012).\n- The manuscript suggests preferring SDR methods over IDR methods to recover the shared signal between different modalities. However, I do not think this is novel knowledge. See Borga et al.: \"A Unified Approach to PCA, PLS, MLR, and CCA.\" PCA, PLS, MLR, and CCA can be unified under a generalized eigenproblem. Figure 2 and Figure 3 in Borga et al. show that all the dimensionality reduction methods recover different solutions, which is expected since they have different inductive biases by construction.\n\n**Technicality**: The experiments are very limited to synthetic data, and it is not clear how these insights will generalize to different settings. Specifically, suppose you read literature on neural networks like Deep CCA or DCCAE. In that case, they all use layer-wise unimodal pretraining or autoencoder for training, respectively, and the CCA is used only afterwards. Hence, only SDR won't be enough to model multimodal data. \n\n**Rigor**: The experiments do not show the solutions' variability since they have not been run over multiple initializations.\n\n**Significance**: The significance to me is not clear. \n\nMurphy, Kevin P.  *Probabilistic machine learning: an introduction*. MIT press, 2022.\n\nBorga, Magnus, and Tomas Landelius Hans Knutsson. \"A Unified Approach to PCA, PLS, MLR and CCA.\"\n\nKlami, Arto, Seppo Virtanen, and Samuel Kaski. \"Bayesian exponential family projections for coupled data sources.\" *arXiv preprint arXiv:1203.3489* (2012)."
            },
            "questions": {
                "value": "Overall, I do not think these results demonstrate new, relevant, and impactful knowledge."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6299/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797930495,
        "cdate": 1698797930495,
        "tmdate": 1699636691708,
        "mdate": 1699636691708,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "S9LxIa6m6b",
        "forum": "4SrzKsJocx",
        "replyto": "4SrzKsJocx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6299/Reviewer_7ULi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6299/Reviewer_7ULi"
        ],
        "content": {
            "summary": {
                "value": "This manuscript conducts some numerical experiments comparing PCA to CCA, PLS, and regularized CCA in some linear-Gaussian multivariate settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The numerical experiments seem straightforward and correct."
            },
            "weaknesses": {
                "value": "The numerical results are essentially well understood already in the statistics community, though the specific numerics for these specific simulations are not obviously in the literature. PCA will keep the eigenvectors of the top eigenvalues of the data matrix, regardless of their source, whereas (r)CCA and PLS will keep those eigenvectors that span the joint subspace. A paper we wrote several years ago looks at the mathematics of this in some detail, https://www.nature.com/articles/s41467-021-23102-2#Sec12.  Specifically, the appendix explains how the eigenvalues matter, and we also provide theoretical guarantees using Chernoff bounds.  Another paper I like on this topic is https://www.sciencedirect.com/science/article/pii/S0047259X14001201?via%3Dihub. \n\nTo me, this reads like a very nice senior thesis, or graduate level class project, suitable for a workshop, e.g., a Neurips workshop on high-dimensional data analysis.  To warrant publication in ICLR, I would want to see some strong theoretical results, and some results on benchmark data, and/or real world data."
            },
            "questions": {
                "value": "I think everything the authors wrote is quite clear."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6299/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6299/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6299/Reviewer_7ULi"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6299/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699239960271,
        "cdate": 1699239960271,
        "tmdate": 1700688274645,
        "mdate": 1700688274645,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lYEzOaNTbG",
        "forum": "4SrzKsJocx",
        "replyto": "4SrzKsJocx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6299/Reviewer_595z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6299/Reviewer_595z"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a generative linear model to synthesize multimodal data for comparing the ability to find shared latent (covariance structure) of different dimensionality reduction approaches, including PCA, PLS, CCA, and regularized CCA (rCCA). Through numerical experiments on the synthetic datasets, they find that simultaneous dimensionality reduction (SDR) methods (PLS, CCA, and rCCA)  consistently outperform PCA (as an independent dimensionality reduction (IDR) method). Different configurations have been applied to the experiments, and remarkably, rCCA is significantly better than others when the number of samples is much smaller than the dimensionality of the data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper is written in a clear and logical way. Experimental results are well presented and understandable.\n* The metrics provided for comparing different methods are meaningful."
            },
            "weaknesses": {
                "value": "* The proposed model is just a simple linear model, which is easy to understand but hard to fit any real-world data\n* These analyses are hard to migrate or generalize to real-world experimental data. For example, all results and conclusions in this paper are limited to the proposed generative linear model. At least, no real-world instruction is provided. See questions."
            },
            "questions": {
                "value": "I think the main drawback of this paper is that the generative linear model is too simple. It seems like it is not something new, but just a linear model for generating a synthetic dataset. Therefore, most conclusions in this paper are drawn from that generative linear model but are hard to generalize to any real-world dataset due to the high nonlinearity in the real-world dataset. Also, the real-world data is generated in a very complicated manner (in addition to nonlinearity). Therefore, the experimental results seem intuitive and easy to me. In other words, I'm not surprised by these results, since we can expect that SDRs are better than IDRs, especially in such a simple synthetic dataset generated from a linear model. Although authors provide detailed analysis with quantitative results (metrics), I still don't see what we can tell more when facing a real-world dataset. While SDRs might still be better than IDRs. However, this seems like a very direct possible result since SDRs are methods that consider correlations/covariances between $X$ and $Y$, but IDRs are not."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6299/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6299/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6299/Reviewer_595z"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6299/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699258762449,
        "cdate": 1699258762449,
        "tmdate": 1699636691451,
        "mdate": 1699636691451,
        "license": "CC BY 4.0",
        "version": 2
    }
]