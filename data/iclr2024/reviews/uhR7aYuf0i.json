[
    {
        "id": "Wc1QyRk1MB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8896/Reviewer_Stdr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8896/Reviewer_Stdr"
        ],
        "forum": "uhR7aYuf0i",
        "replyto": "uhR7aYuf0i",
        "content": {
            "summary": {
                "value": "To encourage exploration, the authors proposed the \"learning to explore approach\" based on meta learning. The key idea is to learn the gradients of the kinetic energy  for SGMCMC update steps through two neural networks; the authors propose to train the networks on one tasks and then generalizes them to future tasks. This submission simplifies the work of meta-learning (Gong et al., 2018), which proposes to make D(z) and Q(z) as simple as possible."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The method starts from a practical viewpoint instead of physical intuitions to tackle the local trap problem suffered by the standard stochastic gradient MCMC methods. The idea of learning some key hyperparameters using parametrized networks based on meta learning may enjoy appealing implementation advantages.\n\nThe experimental evaluations, such as convergence analysis, loss surface demo, evaluation metrics (ACC, NLL, ECE), appear to be comprehensive in deep learning experiments."
            },
            "weaknesses": {
                "value": "(a) Underdamped Langevin/ Hamiltonian Monte Carlo is a good method for accelerating the overdamped alternative in terms of mixing rates. However, I believe it is far from explorative enough compared to other baseline methods, such as the replica-exchange based approaches [1,2]. I am not fully convinced if optimizing the kinetic energy is sufficient enough to solve the exploration problem. Your model input is $(\\theta, r)$, I am not even sure it is really learning anything useful.\n\n[1] Non-convex Learning via Replica Exchange Stochastic Gradient MCMC. ICML'20\n[2] Non-reversible Parallel Tempering for Deep Posterior Approximation. AAAI'23.\n\n(b) Empirically, your baselines such as deep ensemble and cyclical SGMCMC are pretty weak in terms of multi-modal simulations (although their optimization performance is acceptable)."
            },
            "questions": {
                "value": "I am interested to see how your algorithm is compared to [2] or [3].\n\n[2] Non-reversible Parallel Tempering for Deep Posterior Approximation. AAAI'23.\n\n[3] Interacting Contour Stochastic Gradient Langevin Dynamics. ICLR'22\n\n**************\n*After rebuttal, I slightly increased my ratings from 3 to 5.*"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8896/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8896/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8896/Reviewer_Stdr"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8896/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697139020009,
        "cdate": 1697139020009,
        "tmdate": 1700672193869,
        "mdate": 1700672193869,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k665haIQNp",
        "forum": "uhR7aYuf0i",
        "replyto": "uhR7aYuf0i",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8896/Reviewer_36ng"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8896/Reviewer_36ng"
        ],
        "content": {
            "summary": {
                "value": "Unlike prior work, the authors propose a meta-learning strategy for SGHMC by learning the kinetic energy term. The authors show that the meta-learning the kinetic energy term is able to generalize as well as other competing methods, beyond the data distributions during meta-training. Such an approach leads to improved OOD detection, and the authors experimentally demonstrate the discovery of disconnected modes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors take a different perspective to meta-learned SGMCMC where instead of the prior work which focused on meta-learning the dynamics matrix $D$ and curl matrix $Q$, they meta-learn the kinetic energy term.\n- The meta-learning procedure is designed to be operationally fairly simple, and can be relatively easily accommodated into existing SGMCMC pipelines."
            },
            "weaknesses": {
                "value": "- I think the first and the biggest weakness of the work lies in the way it is framed. It seems that you are trying to compete with deep ensembles, and other SGMCMC variants. In fact, the message I get from the experimental results is that L2E is that it performs pretty much worse is most cases and at worse computational cost. But one could argue, the promise of the method is rather in the generalizability of the meta-learning procedure to unseen datasets. In a sense, it is pretraining for SGMCMC, which to me is the most interesting. Unfortunately, this is not how the authors position this paper.\n- The deliberate choice to not use data augmentations is a little concerning. I understand that it does not neatly fit into the Bayesian perspective, but when the same datasets perform significantly better with data augmentation (including with SGMCMC), it makes the comparisons incomplete. It would not be too hard to add data augmentations as is into the same training setup, and make the comparison on these datasets more fair to modern deep learning that achieves both better accuracy and better calibration properties.\n- The comparisons to DE and cSGMCMC looks to me a little unfair. L2E sees more variety of data by design, and one could then posit that the better OOD detection is not that surprising and simply a matter of having seen broader distributions are meta-train time. \n- It would be really helpful if we could have a toy illustration to build intuitions. For instance, meta-learning datasets sampled from 1-D sines and cosines. It would also reveal how the method behaves with \"in-between\" uncertainty, i.e. parts of the input space not in the training dataset. It would also help to see how the sampler behaves beyond the training dataset, when say you change to say a different function family of polynomials.\n\n### Minor\n\n- The choice of citation in the last paragraph on Page 1 \"Zhang et. al., 2020\" alone is odd. I would recommend citing all the other works.\n- The authors have made the choice of not having an explicit related work section. I think that is alright, but other work must be contextualized somewhere in the main text. Part of it is done in"
            },
            "questions": {
                "value": "1. Could the authors clarify how L2E works at test time? Do you run the `InnerLoop` and take the samples after appropriate steps, thinning, and the compute the BMA?\n2. Equation 8 states that the loss is computed on validation data point. Is that true? Or during training this comes from the set of training distributions.\n3. The number of epochs as reported in Table 10 seem incredibly high for such a method to be scalable at all. Or is this supposed to be the number of gradient steps?\n4. How are the parameters initialized in the outer loop? Do you rely on default initializations for the family of architectures considered for meta-training? I wonder if such an approach would be unstable if the nature of networks considered is different.\n5. It seems like the choice to have different architectures for meta-training is challenging. What happens when the images have different number of channels? How is that currently handled? Are different sizes of images handled via resizing?\n6. I may have missed it, but I did not find the number of inner loop steps. Is it supposed to be the number of epochs?\n7. At test time, is the number of inner loop steps the same as train time? What happens when it is lesser? What happens when you keep it much larger?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8896/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698103669605,
        "cdate": 1698103669605,
        "tmdate": 1699637119481,
        "mdate": 1699637119481,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Zhp2xSdZsF",
        "forum": "uhR7aYuf0i",
        "replyto": "uhR7aYuf0i",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8896/Reviewer_LiEv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8896/Reviewer_LiEv"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a meta-learning method to learn the kinetic energy term in SGMCMC for multi-modal distributions. The proposed method uses two neural networks (NNs) to parameterize kinetic energy term and train these NNs based on a meta-objective function, which is the validation loss. The authors have conducted several experiments, including image classification and out-of-distribution detection to demonstrate the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe methodology of the proposed method is simple, which makes it a practical method for many tasks.\n2.\tThe experiments and ablation studies are comprehensive and cover many aspects of sampling, including predictive accuracy, uncertainty quantification, convergence diagnostic"
            },
            "weaknesses": {
                "value": "1.\tThe paper did not mention at all at the beginning that, there already exist meta-learning methods for SGMCMC, such as Gong et al. Only in Section 3.1, the authors first briefly mention that paper. The presentation is misleading and may give the impression that this paper is the first to study meta-learning for SGMCMC.\n2.\tMore importantly, the proposed method is essentially very similar to Gong et al, which uses the same formulation for the SGMCMC class, but will slightly different parameterization. Gong et al learns the curl matrix Q and the diffusion matrix D whereas the proposed method learns the kinetic energy term. Given the similarity, it is important to clearly state the difference compared with Gong et al.\n3.\tThe motivation of the proposed method is weak. Since the main difference compared with Gong et al is the parameterization of the kinetic energy term, it is important to clearly motivate this choice. The authors did not mention at all in the paper why they choose to learn the kinetic energy term rather than the curl matrix and the diffusion matrix. The advantages of doing so are not discussed.\n4.\tGong et al has proposed several meta objectives. Again, the authors did not discuss their meta objective with existing ones.\n5.\tIn experiments, the authors did not compare with Gong et al, which is a very related method.\n6.\tAlthough the experimental results of the proposed method are promising, it is not clear why the proposed meta-learning method can lead to these empirical improvements. What kind of learning updates did the meta-learning algorithm learn in the end?  Why does the method show a better exploration-exploitation balance? how does it achieve that?\n7.\tThe meta-learning tasks are classification on MNIST, Fashion-MNIST, EMNIST and MedMNIST and the downstream tasks are classification on Fashion-MNIST, CIFAR and Tiny ImageNet.  Since Fashion-MNIST has appeared in meta-learning tasks, is it reasonable to use it as a test task? Are CIFAR and Tiny ImageNet similar to MNIST to be considered as test tasks?\n\nGong et al, Meta-Learning For Stochastic Gradient MCMC, ICLR 2019"
            },
            "questions": {
                "value": "1.\tCan you explain T-SNE visualization of learning trajectories? Why does L2E\u2019s trajectory look very different from DE and CSGMCMC?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8896/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698605593195,
        "cdate": 1698605593195,
        "tmdate": 1699637119351,
        "mdate": 1699637119351,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pFZKphDAWA",
        "forum": "uhR7aYuf0i",
        "replyto": "uhR7aYuf0i",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8896/Reviewer_CsK6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8896/Reviewer_CsK6"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors present a general approach for SGMCMC using meta-learning. The meta-learning approach is explicitly trained to minimize the downstream log likelihood using Bayesian model averaging from the final model. This approach is motivated by the earlier successes of meta-learning approaches which show that rich feature representations can be learned and transferred to various tasks. The authors present experimental results looking at the convergence rate, the diversity of the samples, as well performance on downstream tasks of classification and uncertainty quantification."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The approach is well motivated by the previous successes in meta-learning. Knowledge-sharing between different SGMCMC chains across different multi-modal distributions across similar tasks should be explored in detail. \n\n- The authors have done a good job at experimentation overall. The empirical analysis spans understanding the diversity of MCMC chains as well as looking at the downstream tasks. \n\n- I really like the idea of parameterizing the gradients of kinetic energy function - it seems intuitive and to the best of my knowledge I haven't seen prior work do that. This building the SGMCMC algorithm from first principles is commendable."
            },
            "weaknesses": {
                "value": "- While the experimental results are useful, I think the paper also needs an ablation study. We need to understand the impact of the parameterized gradients v/s transferability of the posterior information across the tasks. \n\n- Understanding the compute requirement at train time is equally important. We don't know how much training time is required per step and in total and how it compares with other baselines that the authors have compared with. It'll also be useful to know the additional # of parameters added through the gradient parameterization. \n\n- I think there will be some tradeoff between the number of samples we generate in the inner loop to compute $L$ and final convergence in terms of how quickly we converge and quality of the posterior samples. Running an ablation on the number of samples we generate in the inner loop would also be useful for practitioners who would be interested in using this approach."
            },
            "questions": {
                "value": "I do like this paper overall and I think it has shown some interesting results. I have listed my comments that I'd like the authors to address in the weaknesses section.\n\nAlso, it'll be useful if the author can provide some insights on how to speed up their algorithm and make it more efficient. For e.g., would warm starting with an SGD estimate in the meta-learning framework help?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8896/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8896/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8896/Reviewer_CsK6"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8896/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789923003,
        "cdate": 1698789923003,
        "tmdate": 1699637119235,
        "mdate": 1699637119235,
        "license": "CC BY 4.0",
        "version": 2
    }
]