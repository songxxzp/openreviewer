[
    {
        "id": "z5QiMl6Njt",
        "forum": "RXU6qde675",
        "replyto": "RXU6qde675",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3189/Reviewer_zmKs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3189/Reviewer_zmKs"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an adversarial model for link prediction in multi-layer networks where a fixed set of nodes is connected through a different set of edges in each layer. The architecture has an **encoder** (representation generator), composed by one GCN per (network) layer which output the respective intra-edge embeddings (IR). IR is fed to CNNs, one which generates an output (TR) for an **adversarial classifier** (layer discriminator) whose goal is to predict the source layer of the link, and another whose output (SR) is combined with TR through a gating mechanism in order to obtain a new edge embedding (ER). ER is fed to a **binary classifier**  (link predictor) to predict the existence of the link. The rationale is that the representation of edges should be consistent (not equal) across layers in order to leverage their correlations. The authors evaluate the proposed method (AER) on 5 multi-layer network datasets, showing that it outperforms four other baselines w.r.t. Accuracy and AUC. An ablation study shows the benefits of incorporating inter-layer correlations using TR."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "S1. The paper considers an interesting adversarial setting to learn edge representations that are sufficiently \"layer-invariant\" to mislead a discriminator but still useful to perform intra-layer link prediction."
            },
            "weaknesses": {
                "value": "W1. The relationship between the present work with the body of works on link prediction in heterogeneous graphs (multiple edge types) is never established in the paper. As a result, GNN-based models for heterogeneous graphs (e.g., R-GCN, HIN) were never included in the comparison. It is worth mentioning that these models do capture correlations between different edge types, which are arguably analogous to edges in different layers.\n\nW2. The problem tackled in the paper is also likely related to embeddings for knowledge graphs (e.g., DGL-KE). This is also missing in the related work and in the experimental section.\n\nW3. The combination of an adversarial loss with a binary classification loss (three sets of parameters to be optimized) can pose several challenges to the training, requiring a delicate balance between the two losses. Yet, there is no hyperparameter in $Loss_{final}$ (Eq. 12) to control this balance. There are no plots showing the evolution of the loss components.\n\nW4. No study or empirical results regarding the method scalability. Networks are relatively small.\n\nW5. It is hard to determine whether the comparison with the baselines is fair.\n\nW6. Some design choices in the proposed method are not well-justified. Additional experiments need to be included in the ablation study. \n\nW7. Notation and typesetting need improvement. Spelling must be reviewed.\n\nW8. The results are not reproducible based on the information provided in the submission. Appendix was not used."
            },
            "questions": {
                "value": "Q1. What is the difference between link prediction in multi-layer networks and in heterogeneous networks (multiple edge types)? Why the methods proposed for the latter were not discussed?\n\nQ2. Same question as before, but considering knowledge graphs.\n\nQ3. How do you determine that the model has converged? When it does, what is the discriminator accuracy? 1/K? What to the loss curves look like during the optimization? What are typical values of z in Eq. (6) or, in other words, how much of TR is relevant for link prediction?\n\nQ4. How does the model training and inference time scale with the number of nodes, edges and layers? What were the training times obtained for each dataset?\n\nQ5. Do the baselines have roughly the same number of parameters? Do they require about the same compute power for training? In other words, could the performance gains be coming at the cost of higher computing needs?\n\nQ6. Some issues have to do with disregarding invariances:\n- Since Eq. (2) is based on concatenation, does it consider both (i,j) and (j,i) when learning the IR for an edge e=(i,j)?\n- Why use a CNN to transform IR? Does Eq. (13) yield TR for edge i? The CNN isn't invariant to the order of the edge sequence. Shouldn't this be a problem?\n- Why gating instead of a concatenation followed by a MLP? What is the benefit of this specific choice?\n- Why softmax for link prediction? Use sigmoid and reduce a few parameters.\n\nQ7. Consider:\n- Not using 'dot' for matrix multiplication\n- Using 'odot' for element-wise multiplication\n- Correctly typesetting log, tanh, and multi-letter variables"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3189/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3189/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3189/Reviewer_zmKs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3189/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698544463722,
        "cdate": 1698544463722,
        "tmdate": 1699636266741,
        "mdate": 1699636266741,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "P0147ycxAI",
        "forum": "RXU6qde675",
        "replyto": "RXU6qde675",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3189/Reviewer_rv4f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3189/Reviewer_rv4f"
        ],
        "content": {
            "summary": {
                "value": "This paper notices that some real-world scenarios can be represented as a multi-layer network where different layers share the same set of vertices but different set of edge connections. This paper proposes a representation generator, a layer discriminator, and a link predictor to effectively integrate both intra-layer and inter-layer information. Experiments on 5 datasets are conducted to evaluate the performance of the proposed model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. Multi-layer network is a variant of existing single-layer network and is worth researching. This paper identifies an interesting problem and designs a working model to solve it.\n\n2. Specifically, although the desisn of a representation generator is not new, the proposed layer discriminator looks interesting in the multi-layer setting.\n\n3. Overall, the writing of this paper is clear enough for readers to understand the concept."
            },
            "weaknesses": {
                "value": "1. Missing baseline models. Although this paper proposes an interesting research question and a promising model architecture, it fails to mention two highly related works, MANE [1] and MLHNE [2]. These two works both work on multi-layer network embedding and both propose intra-layer and inter-layer concept to integrate information of different network layers. This submitted work lacks the discussion and comparison with these two papers.\n\n[1] Li, J., Chen, C., Tong, H., & Liu, H. (2018, May). Multi-layered network embedding. In Proceedings of the 2018 SIAM International Conference on Data Mining (pp. 684-692). Society for Industrial and Applied Mathematics.\n\n[2] Zhang, D. C., & Lauw, H. W. (2021). Representation Learning on Multi-layered Heterogeneous Network. In Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13\u201317, 2021, Proceedings, Part II 21 (pp. 399-416). Springer International Publishing.\n\n2. Insufficient experiment tasks. For network embedding area, if the proposed model is unsupervised,  both link prediction and node classification are important evaluation tasks. Node classification has been a standard evaluation task in network embedding area and has been used in various papers, including the above mentioned two missing papers, but this submitted paper contains link prediction but not node classification.\n\n3. Small datasets. The datasets in the paper contain <2K vertices, which is much smaller than real-world scenarios. Thus it's difficult to say if the proposed model can scale to large networks efficiently. I expect to see experiments on larger datasets, say 100K vertices."
            },
            "questions": {
                "value": "1. Why is standard deviation at Figure 2 missing?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3189/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3189/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3189/Reviewer_rv4f"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3189/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698613732982,
        "cdate": 1698613732982,
        "tmdate": 1699636266654,
        "mdate": 1699636266654,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EImXQOKz8D",
        "forum": "RXU6qde675",
        "replyto": "RXU6qde675",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3189/Reviewer_m29U"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3189/Reviewer_m29U"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a multi-layer GNN for link prediction, which consists of representation generator, layer discriminator, and a link predictor. The representation generator includes GCNs for different layers for initial node embeddings, followed by CNNs to get both inter- and intra- layer node embeddings. The layer discriminator aims to identify the layer sources of learned inter-layer representations. The experimental results show some improvement over some baselines and ablation studies demonstrate the necessity of layer discriminator."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The experimental results show some improvement over some baselines and ablation studies demonstrate the necessity of layer discriminator."
            },
            "weaknesses": {
                "value": "1. Multi-layer GNNs or multiplex GNNs are not new topics, but the paper does not comprehensively review the existing literature in this topic. In addition to the literature review, the authors did not include these works as baselines for experimental comparisons. For example, existing related works include:\n[1] Mitra, Anasua, et al. \"Semi-supervised deep learning for multiplex networks.\" Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining. 2021.\n[2] Zhang, Weifeng, et al. \"Multiplex graph neural networks for multi-behavior recommendation.\" Proceedings of the 29th ACM international conference on information & knowledge management. 2020.\n[3] Li, Jundong, et al. \"Multi-layered network embedding.\" Proceedings of the 2018 SIAM International Conference on Data Mining. Society for Industrial and Applied Mathematics, 2018.\n\n2. The proposed approach is not well-motivated and the writings need a lot of improvements. For example, it is not clear why CNNs are applied after GCNs for inter- and intra-layer node embeddings. In addition, it looks to me that layer discriminator and link predictor serve for a similar purpose, but the uniqueness of these components is not further highlighted. \n\n3. The datasets used in this work are quite small. The largest dataset only contains ~1500 nodes. The authors are suggested to use the datasets from the above mentioned works to conduct more comprehensive experimental evaluations.\n\n4. Minor: The notations are not very readable. Many notations are named a bit long (e.g., IR, TR, SR, etc.)."
            },
            "questions": {
                "value": "1. Why CNNs are applied after GCNs for inter- and intra-layer node embeddings?\n2. What are the differences between layer discriminator and link predictor, since both of them aims to predict link existence?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3189/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698706047409,
        "cdate": 1698706047409,
        "tmdate": 1699636266550,
        "mdate": 1699636266550,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EDZ7t2nNMM",
        "forum": "RXU6qde675",
        "replyto": "RXU6qde675",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3189/Reviewer_L4YV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3189/Reviewer_L4YV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel framework called Adversarial Enhanced Representation (i.e., AER) for link prediction in multi-layer networks. AER is composed of three components: a representation generator, a layer discriminator, and a link predictor. The representation generator can simultaneously utilize both inter-layer and intra-layer representations in multi-layer networks. Moreover, AER can effectively acquire the inter-layer transferable representations to enrich the intra-layer representations via adversarial training between the representation generator and layer discriminator. Extensive experiments on real-world datasets show the methods\u2019 effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1: The paper is well organized and easy to understand.\nS2: The method as a framework can be generalized easily to other tasks on the multi-layer networks.\nS3: Extensive experiments on real-world datasets show the propsed method can effectively and efficiently capture the information between the different layers in the multi-layer networks."
            },
            "weaknesses": {
                "value": "W1: The novelty of AER is insufficient. Many of the components used in AER are traditional and widely known models or operators (e.g., GCN, CNN). Authors do not explain clearly about the motivation for combining them. And in my opions, many of the operations represented in this paper seem to be basic to existing methods.\nW2: The experiment is insufficient and the baslines maybe lacking. First, the old-fashioned link prediction baselines are missing, including but not limited to common neighbors (CN), Jaccard, preferential attachment (PA), Adamic-Adar (AA), resource allocation (RA), Katz, PageRank (PR), and SimRank (SR). (2) Some SOTA models are also needed such as SEAL (Link Prediction Based on Graph Neural Networks).\nW3: The writing of the paper needs to be improved. In particular, the third paragraph of the introduction is not well understood and lacks citations to support points. There is no need to introduce too many details of the experimental parameters in the methods section. And the description in the algorithm section is rather redundant and does not fit the form of the algorithm."
            },
            "questions": {
                "value": "Q1: What is the mutli-layer network introducted in the intro? Could the authors provide a more specific description of such datasets. Also, are they any different from the graph-level datasets used in GNN research?\nPlease refer to weaknesses for other questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3189/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838950605,
        "cdate": 1698838950605,
        "tmdate": 1699636266482,
        "mdate": 1699636266482,
        "license": "CC BY 4.0",
        "version": 2
    }
]