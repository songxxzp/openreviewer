[
    {
        "id": "WvBbfGXxTl",
        "forum": "3wL1tj3kqE",
        "replyto": "3wL1tj3kqE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9395/Reviewer_cyJi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9395/Reviewer_cyJi"
        ],
        "content": {
            "summary": {
                "value": "This paper studied a novel fair domain generalization problem where multiple sensitive attributes existed in different domains. The key challenge of this fair domain generalization problem is to deal with multiple potential sensitive attributes and any combinations of sensitive attributes can appear in the unseen testing domains. Then it presented a feasible solution by learning the domain-invariant representation and sensitive attribute invariant representation from training domains. The objective function included four components: domain-invariance loss, fairness-aware invariance loss, classification loss, and equalized odds (for fairness) loss. Experiments on two real-world data sets showed that the proposed outperformed DG baselines in terms of generalization and fairness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Originality:** This paper focused on a novel fair domain generalization problem with multiple sensitive attributes. It was a much more challenging problem setting than previous work due to the complicated interconnections of different sensitive attributes. The major technical novelty of this paper was to selectively learn the invariant representation based on the sensitive attributes, e.g., generate representations with respect to sensitive attributes. Experimental results demonstrated the effectiveness of the proposed SISA method over several baselines in terms of both generalization and fairness metrics.\n\n**Quality:** The fair domain generalization problem was well-defined. The motivating example in Figure 1 also illustrated that fair domain generalization with multiple sensitive attributes was a challenging yet practical problem. In the derived objective function, both generalization performance and fairness were encouraged in different loss terms.\n\n**Clarity:** Overall, the presentation of this paper was clear and the derived problem was well-motivated. Experiments also showed the training procedures and evaluation metrics for performance comparison between the proposed method and baselines. Ablation studies supported that the hyper-parameters were relatively robust to the model performance.\n\n**Significance:** This paper extended previous fair domain generalization to a more general setting where multiple sensitive attributes could appear in different domains."
            },
            "weaknesses": {
                "value": "**W1:** The technical novelties of this paper are unclear. The proposed SISA approach involves several techniques from previous works, e.g., invariant representation learning with domain density translators, equalized odds loss, contrastive loss, etc. The major technical contributions could be emphasized in the context of the derived fair domain generalization problem.\n\n**W2:** The explanation of the fairness encoder in subsection 3.2.1 is not convincing. (1) It randomly chooses a single $c$ to learn the representations of sensitive attributes. This can lead to biased and unstable solutions. More empirical evaluations on this sampling strategy can be provided. (2) It uses the concatenation between input $x$ and attribute $c$. However, if $x$ (e.g., high-dimensional images) and $c$ differ in dimensionality, would the concatenated vector be dominated by one of them? \n\n**W3:** The equalized odds loss $\\mathcal{L}_{EO}$ is confusing. What are $h\\_{\\psi}(z | y, i)$ and $h\\_{\\psi}(z | y, j)$ over the sample $(x, y) \\sim \\mathcal{P}\\_d$? What is $\\mathcal{P}$ within $Div(\\cdot)$? Why does it involve both $\\mathbb{E}\\_{(x,y)\\sim \\mathcal{P}_d}$ and $\\mathbb{E}\\_{y \\sim \\mathcal{Y}}$?\n\n**W4:** The hyper-parameter setting is not explained. (1) It shows that $\\alpha=0.1, \\omega= 1$ are used in previous work for the best reported results, and thus those parameters are also used in this paper.  However, since different models are involved in this paper and previous work, $\\alpha=0.1, \\omega= 1$ might lead to sub-optimal solutions. (2) The hyper-parameter sensitivity on $\\epsilon$ and $\\gamma$ are analyzed. However, it is unclear whether the best hyper-parameters are selected based on the testing domains. Is any validation method adopted for hyper-parameter selection during training?"
            },
            "questions": {
                "value": "Q1: Figure 2 shows that the drop in performance is high when fairness is enforced on multiple attributes. This might indicate that it becomes more challenging to find the trade-off between generalization performance and fairness when increasing the number of sensitive attributes. Therefore, it would be better to provide some insights into understanding how to balance generalization performance and fairness when a large number of sensitive attributes exist.\n\nQ2: Table 7 shows that the number of encoders can also affect the trade-off between performance and fairness. Why does a single encoder improve the fairness and multiple encoders help generalization performance in the proposed approach?\n\nQ3: This paper considers the covariate shift among domains. Can the proposed SISA method be adapted to deal with other types of distribution shifts, e.g., label shifts, concept shifts, etc?\n\n########################################\n\nAfter reviewing the rebuttals, I would like to keep my rating unchanged, since most of my concerns have not been addressed. In most cases, the responses are not very convincing. More theoretical or empirical results could be added to support the explanations, e.g., the selection of $c$, hyper-parameter selection, the trade-off between performance and fairness, etc."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9395/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9395/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9395/Reviewer_cyJi"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9395/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698182363370,
        "cdate": 1698182363370,
        "tmdate": 1700930215888,
        "mdate": 1700930215888,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YyOvX2x0kY",
        "forum": "3wL1tj3kqE",
        "replyto": "3wL1tj3kqE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9395/Reviewer_XuYi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9395/Reviewer_XuYi"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an approach aimed at achieving intersectional fairness within the context of domain generalization. Specifically, the proposed method focuses on acquiring two distinct invariant representations across domains, emphasizing both accuracy and fairness. Subsequently, a classifier is employed to make predictions based on these representations. To transfer fairness and accuracy into new domains, the authors train the model to minimize error and fairness loss across source domains."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper targets fairness which is an important research topic in machine learning."
            },
            "weaknesses": {
                "value": "- The clarity of the paper is lacking, with several important details omitted. For instance, the paper lacks comprehensive information about the training of the domain density translator $G'$ for fairness generalization. Training $G'$ is not straightforward due to the varying sensitive attributes across different datasets.\n\n- The design choice of using a shared translator $G'$ for all sensitive attributes appears questionable. Notably, given an input $X$ from domain $d$, $G'$ only generates $X' = G'(X, d, d')$ in domain $d'$, without considering which sensitive attributes are relevant to the translation. This implies that the model assumes $P_d(X|y,s) = P_{d'}(X'|y,s)$ and $P_d(X|y,s') = P_{d'}(X'|y,s')$ for every $s, s' \\in S$, which is a strong assumption and may not hold in practical scenarios.\n\n- The rationale behind learning distinct representations for each sensitive attribute is not well elucidated. Why is it necessary for the model to minimize the gap between domain representations with the same sensitive attribute configurations while maximizing the gap for those with different sensitive attributes? How does concatenating all representations contribute to accurate and fair predictions in target domains?\n\n- The results in Table 2 and 3 seem to be presented for a fixed value of $\\gamma$. It would be more comprehensive if the authors varied $\\gamma$ to explore the accuracy-fairness trade-off for the methods used in the experiments.\n\n- The final objective, as defined in Equation (10), encompasses a blend of multiple loss components. I recommend that the authors carry out an ablation study, varying hyperparameters, to assess the impact of each loss on the model's performance.\n\n- The technical novelty of the paper appears somewhat limited. The primary contribution appears to be the utilization of distinct representations for each sensitive attribute.\n\n- In the introduction section (Fig. 1), the authors assert that the proposed method can accommodate the heterogeneity of sensitive attributes across domains. However, in the experimental section, the models seem to have access to all sensitive attributes in all domains, which may contradict the initial claim.\n\n- The paper lacks the provision of code and supplementary documentation, which could significantly enhance clarity and reproducibility. Providing these resources would be beneficial for the reader to understand and replicate the methodology."
            },
            "questions": {
                "value": "Please see the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9395/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9395/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9395/Reviewer_XuYi"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9395/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698613617780,
        "cdate": 1698613617780,
        "tmdate": 1699637184579,
        "mdate": 1699637184579,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hkZA15bqkJ",
        "forum": "3wL1tj3kqE",
        "replyto": "3wL1tj3kqE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9395/Reviewer_FctA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9395/Reviewer_FctA"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the challenge of fairness transfer in domain generalization, particularly in contexts where multiple sensitive attributes are present and may vary across domains. Traditional domain generalization methods aim to generalize a model's performance to unseen domains but often ignore the aspect of fairness, especially when multiple sensitive attributes are involved.\n\nThe authors propose a novel framework capable of handling fairness with respect to multiple sensitive attributes across different domains, including unseen ones. This is achieved through the development of two types of representations: a domain-invariant representation for generalizing model performance and a selective domain-invariant representation for transferring fairness to domains with similar sensitive attributes. A key innovation of the proposed method is its ability to reduce computational complexity significantly."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The approach to handle multiple sensitive attributes in domain generalization is innovative and addresses a clear gap in existing literature.\n+ The use of real-world datasets for experimentation enhances the practical relevance of the research.\n+ Learning two types of representations for generalization and fairness is a thoughtful approach that could have broader applications.\n+ Reducing the number of required models from $2^n$ to just one is a significant improvement, making the solution more feasible in practical scenarios."
            },
            "weaknesses": {
                "value": "- There is a lack of detail on the specific fairness metrics employed and how the trade-off between fairness and accuracy is quantitatively managed.\n- While the reduction in model count is impressive, there are no details on the scalability of the approach with respect to the size of the data or the complexity of domain environments.\n- The definition of \"sensitive attributes\" is rather arbitrary and vague - is there any specific reason certain attributes (i.e. smiling) count as sensitive?"
            },
            "questions": {
                "value": "- Could you elaborate on the robustness of your method against different types of distribution shifts compared to existing methods by providing ablation studies?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9395/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9395/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9395/Reviewer_FctA"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9395/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637214196,
        "cdate": 1698637214196,
        "tmdate": 1699637184476,
        "mdate": 1699637184476,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6hdEd9y3Hn",
        "forum": "3wL1tj3kqE",
        "replyto": "3wL1tj3kqE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9395/Reviewer_kdx1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9395/Reviewer_kdx1"
        ],
        "content": {
            "summary": {
                "value": "According to the authors, this work proposes a novel approach to handle multiple sensitive attributes, allowing any combination in the target domain. This approach involves learning two representations: one for general model performance and another for transferring fairness to unseen domains with similar sensitive attributes. The proposed method significantly reduces the model requirement from 2^n to just 1 for handling multiple attributes and outperforms existing methods in experiments with unseen target domains."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. According to the authors, this paper introduces a new setting of fair domain generalization with multiple sensitive attributes.\n2. Based on the proposed setting, a comprehensive training approach is given. \n3. The paper is easy to follow."
            },
            "weaknesses": {
                "value": "1. Some statements are over-claimed. In the introduction, \"FATDM is the only work that addresses...\" this is not true. Several works, other than FATDM, address fairness-aware domain generalization but in various paradigms, such as [1], [2], and [3]. \n2. Figure 2 is unclear to me. What is the unfairness metric \"Mean\"? Do you mean \"mean difference\" or others? How do you define \"different level of fairness\"? Also, the word \"level\" should be plural \"levels\". What is the take-home message when observing the drop in performance, and what is the connection between this drop and multiple attributes? How does this observation relate to various domains?\n3. In the second item of contributions in the Introduction, except the problem mentioned in the first contribution, what is the other problem when you say \"both problems\"?\n4. What is the relationship between the target domain \\Tilde{d} with source domains? Is the target domain shifted from sources due to covariate shift, too? If not, what assumption do you make on target domains? This lack of clarification and, hence, unclear to me. Besides, giving a brief introduction to covariate shifts is necessary. \n5. I doubt the novelty of proposing the setting in multiple sensitive attributes. To me, a dataset with multiple sensitive attributes can be easily converted to one with a single sensitive attribute with multiple categories. For example, as stated in the paper, a sensitivity configuration set \\mathcal{C}={[0,0], [0,1], [1,0], [1,1]} can be viewed as a set {1,2,3,4} where a single sensitive attribute with four distinct categorical values. \n6. Does data sample x include sensitive attribute c? \n7. In Eq.(1), \"d'\" should be replaced by \"d''\".\n8. How to ensure g_\\theta encodes an invariant representation across domains? According to Eq.(5), the loss L_{DG} is defined as the expectation across all source domains. Therefore, it is not convincing to me that the generalization encoder can be generalized to an unseen target domain when a covariate shift occurs. \n9. In the fairness encoder, x is concatenated with c. I am wondering how to do it empirically when x is an image while c is one of the annotations of the image. Please explain your experiments for implementation using the CelebA as an example. \n10. Speaking of fair machine learning in general, it aims to mitigate spurious correlations between sensitive attributes and model outcomes. Although this work mentions fairness multiple times, it is unclear to me how to mitigate the spurious correlations during training. This work proposes that it \"minimize the gap between the domain representations that have the same sensitive attribute configurations and maximize the gap for representations with different sensitive attributes\". But this does not ensure unfairness is controllable.\n\n[1] Elliot Creager, J\u00f6rn-Henrik Jacobsen, Richard Zemel. Environment Inference for Invariant Learning. ICML 2021.\n\n[2] Changdae Oh, Heeji Won, Junhyuk So, Taero Kim, Yewon Kim, Hosik Choi, Kyungwoo Song. Learning Fair Representation via Distributional Contrastive Disentanglement. ACM SIGKDD 2022.\n\n[3] Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, Christan Grant, Feng Chen. Towards Fair Disentangled Online Learning for Changing Environments. ACM SIGKDD 2023."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9395/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811413123,
        "cdate": 1698811413123,
        "tmdate": 1699637184364,
        "mdate": 1699637184364,
        "license": "CC BY 4.0",
        "version": 2
    }
]