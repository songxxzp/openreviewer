[
    {
        "id": "zJFyA003H3",
        "forum": "88FcNOwNvM",
        "replyto": "88FcNOwNvM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4201/Reviewer_t1av"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4201/Reviewer_t1av"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the image decomposition task and proposes a new approach, i.e., Decomp Diffusion, to decompose a scene into a set of factors represented as separate diffusion models. The proposed method can decompose scenes into both global and local concepts. These concepts can further be flexibly composed to generate a variety of scenes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea of leveraging the connection between Energy-based models and diffusion models for image decomposition is interesting and effective. The compositional concepts from images can be discovered in an unsupervised manner. The experimental results show that the proposed method can discover both global and local concepts, and be used for component compositions across multiple datasets and models."
            },
            "weaknesses": {
                "value": "1. The quantitative evaluation is not thorough. The current quantitative evaluation only focuses on the global factors, while the quantitative evaluation for the local factors and cross dataset generalization is missing. In contrast, the existing work (COMET) contains quantitative comparisons for the object-level decomposition.\n2. As the proposed method contains a set of diffusion models, the computational cost of the proposed method and existing works should be discussed in the paper.\n3. For training details in the supplemental, each model is trained on an NVIDIA V100 or an NVIDIA RTX 2080 with the same hours. I was wondering whether the model performance would be different. In addition, is the memory of NVIDIA RTX 2080 24GB or 8GB?"
            },
            "questions": {
                "value": "1. For the ablation study, why use MSE and LPIPS to evaluate the reconstruction quality, rather than the metrics used in Table 1? How about the results of the ablated versions on other datasets used in the paper?\n2. How to determine the types of factors that can be inferred from the image? For example, I am not sure whether the second to the fourth columns correspond to shadow, objects, and background respectively."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4201/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4201/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4201/Reviewer_t1av"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4201/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675560082,
        "cdate": 1698675560082,
        "tmdate": 1699636386670,
        "mdate": 1699636386670,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RituFpLsle",
        "forum": "88FcNOwNvM",
        "replyto": "88FcNOwNvM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4201/Reviewer_Rmft"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4201/Reviewer_Rmft"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses compositional image generation through denoising diffusion models. The unsupervised approach decomposes the input image into several primitives, and the model is able to recompose these primitives together. Experiments are conducted on simple object scenes and human faces, and demonstrate superior performance than SOTAs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper addresses compositional modeling for images using denoising diffusion models. The recomposition quality seems promising. \n+ The paper shows that energy functions are additive of primitives."
            },
            "weaknesses": {
                "value": "+ The method seems to be similar to [1]\n+ What is the computational cost? It may takes more space and computational resources with K diffusion models\n\n\n\n[1] Du et al, Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC, ICML 2023"
            },
            "questions": {
                "value": "+ Is the learned encoder Enc\u03b8(x) pre-trained or trained with diffusion model jointly? \n+ Suppose it is jointly trained, how does the network learn to decompose the image into shadow image, object image, background image, etc? Is there any specific constraint for learning these different properties?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4201/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698695490487,
        "cdate": 1698695490487,
        "tmdate": 1699636386566,
        "mdate": 1699636386566,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OLO5F7faGu",
        "forum": "88FcNOwNvM",
        "replyto": "88FcNOwNvM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4201/Reviewer_W4iB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4201/Reviewer_W4iB"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses unsupervised image decomposition/re-composition with diffusion models. The authors show equivalence between previous decomposition work COMET, based on energy minimization and gradient descent optimization framework, and the recent diffusion models (DDPM) denoising steps iteration.  They consequently 'substitute' the EM model with a diffusion model conditionned on a set of latent variables z_k. The z_k's are inferred by an Encoder, and are associated to the different factors of the decomposition. \nExperimental results are illustrated on several classical benchmarks (CelebA, Virtual Kitti, Falcor3D, also synthetic data such as CLEVR and Tetris), compared qualitatively and quantitatively to related work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Unsupervised image intrinsic decomposition/re-composition is very challenging and one of the most fundamental open issues in computer vision. Using diffusion models for this purpose seems a natural choice (given the success of DM in natural image generation, and in learning semantic image properties). The authors give a rigorous justification of their choices from a mathematical point of view.  The paper's idea is well argued. The illustrated results show the strong potential of the approach.  I enjoyed reading the article."
            },
            "weaknesses": {
                "value": "Qualitative results are promising but still leave room for improvement. Reconstructed images appear blurry, and at low resolution. But at this stage this is not a major issue and that might be improved by further work."
            },
            "questions": {
                "value": "1) I did not find in the paper an explanation about the encoder z = Enc_\\theta(x).  How Enc_() is learnt? What ensures that the decomposition is at the local (ie objects, things) or global (ie, illumination, stuffs) level?   What ensures the disentanglement of the decomposition ? (no additional constraints are enforced during the learning stage).  Those aspects might have been discussed in the original paper COMET (I did not read it), however, it is worth to discuss them again in the current paper since it is key for the understanding and  analysis of the success/failures of the proposed approach. \n\n2) Some details in the approach that are not clear to me. \n2.1 The authors argue that they 'learn a set of different denoising functions to recover an image x_i' (page 4). However, the denoising function \\epsilon_\\theta is not, in eq.9, parameterized by k.  The only dependance to k is in the input latent variable z_k. It would imply that there is a single denoising function, but with different input argument (in particular the z_k). Please clarify. \n2.2 The encoder Enc_\\theta() and the denoising function \\epsilon_\\theta, are both parameterized by \\theta. This is probably a typo, the two networks being parameterized by two sets of independent weights, \\theta_1 and \\theta_2. Please correct as needed in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethical issues beyond existing public generative models."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4201/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827461180,
        "cdate": 1698827461180,
        "tmdate": 1699636386461,
        "mdate": 1699636386461,
        "license": "CC BY 4.0",
        "version": 2
    }
]