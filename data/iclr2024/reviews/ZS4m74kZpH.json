[
    {
        "id": "b00uMQncLO",
        "forum": "ZS4m74kZpH",
        "replyto": "ZS4m74kZpH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7143/Reviewer_Kczf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7143/Reviewer_Kczf"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an analysis on open-domain question answering benchmarks where retrieval harms the performance. The authors also propose two methods to mitigate such issue: 1. using an NLI model to filter out passages that do not entail question-answer pairs, and 2. automatically generating data to fine-tune the language model to be robust to irrelevant contexts. The authors show that as few as 1000 examples suffice to train the model to be robust to irrelevant context while preserving the performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper focuses on irrelevant context for open-domain question answering, which is a key and crucial part for RALMs performance. The paper proposes an automatic way to generate decomposed questions for training"
            },
            "weaknesses": {
                "value": "- I am not fully convinced that the NLI model work / add any significance to the paper presentation, since there is no guarantee on the accuracy if there is no gold passage provided. With results and analysis from section 4 and 5, I don\u2019t believe this proposed NLI model can be claimed as a significant contribution for this paper.\n- section 4 can be better presented, the color coding is a bit confusing\u2026\n- For the analysis in section 5, conclusions drawn from 40 / 25 examples does not show enough statistical significance."
            },
            "questions": {
                "value": "- Additional \u201cranked\u201d in section 3.2.1?\n- For generating training data, it seems that the top-1 passage generated from Rc(q) is considered \u201crelevant\u201d, but for some harder questions, or suboptimal retriever, this is not guarantee, and even google search failed sometime for harder dataset. Although this might not be in the scope, I wonder whether there is any remedy for that?\n- How do you define ambiguous question for section 5?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7143/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7143/Reviewer_Kczf",
                    "ICLR.cc/2024/Conference/Submission7143/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7143/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698571136521,
        "cdate": 1698571136521,
        "tmdate": 1700537847128,
        "mdate": 1700537847128,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FJTXy8SUE6",
        "forum": "ZS4m74kZpH",
        "replyto": "ZS4m74kZpH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7143/Reviewer_dWp6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7143/Reviewer_dWp6"
        ],
        "content": {
            "summary": {
                "value": "This paper makes a systematic study about the robustness of RALM, where it identifies the potential threat resulted from irrelevant retrieved contexts. The paper further introduces two approach to confront this problem. One is to leverage a small-scale NLI model to filter out the irrelevant context. The other one is to fine-tuned the language model with  a mixture of relevant and irrelevant contexts. The paper performs comprehensive experiments on one-hop and multi-hop QA datasets to verify the proposed methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "s1. This paper presents a through analysis for the robustness of RALM to noisy context, which is fundamentally important to the research of LLM, question answering, and information retrieval.\ns2. Despite simplicity, the two proposed methods are meaningful and empirically positive."
            },
            "weaknesses": {
                "value": "w1. The use of a filtering module to mitigate contextual noise and filtering the language model with noisy context are two established approaches found in various related works on open-domain question answering and conversational question answering. While there may be variations in specific implementations, they might not be regarded as technical breakthroughs for this problem. This paper should conduct a more comprehensive investigation of related techniques. \n\nw2. The experimental study can be improved (please refer to my posted questions)."
            },
            "questions": {
                "value": "Q1. How generalizable is the fine-tuned model? If it is applied to other QA datasets, especially with different kind of context noise, what will happen?\n\nQ2. Which part of the experiment can support the statement \"models finet-uned solely with relevant contexts are far less robust than those finetuned with a mixture of relevant and irrelevant contexts\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7143/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7143/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7143/Reviewer_dWp6"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7143/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698652729192,
        "cdate": 1698652729192,
        "tmdate": 1699636845848,
        "mdate": 1699636845848,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mNI6IOMqzB",
        "forum": "ZS4m74kZpH",
        "replyto": "ZS4m74kZpH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7143/Reviewer_jc66"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7143/Reviewer_jc66"
        ],
        "content": {
            "summary": {
                "value": "Retrieval-augmented LM are of major interest in both applied and research contexts. This paper addresses the problem of cases where the retrieved context is not actually relevant and actually degrades task performance. The paper provides error analysis of this phenomenon over benchmark datasets covering variation in problem setting, and proposes two approaches to make RALM \"robust\" to irrelevant context (ie, minimize performance loss). The first approach is a simpler modular \"black box\" solution where a separate (NLI) module evaluates the relevance of the context to reject suspected irrelevant context and prevents it from being supplied to the LM, and the second involves fine-tuning the LM to provide correct answers even when provided irrelevant context. Experiments illustrate improved performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The core problem of degraded RALM performance due to irrelevant context is very compelling from both practical application and general research perspectives. Also it is useful that, besides providing a \"baseline\" of sorts, the simpler NLI approach is suitable for applications where fine-tuning is not feasible or greater system modularity is desired for architectural reasons.\n\nOverall, the presentation and writing are clear. \n\nThe variation in the benchmark types (single-hop, explicit and implicit multi-hop) provided good coverage of the underlying problem and demonstrated interesting behaviors, as did the various ablations and alternative configurations.\n\nSection 5 (Analysis) has multiple examples of the authors _manually analyzing_ examples that meet certain criteria in order to develop better insights into the behavior - this is fantastic to see and I thought it had good pay offs in terms of deeper / richer understanding. I thought the error analyses here were some of the most interesting and compelling parts of the paper."
            },
            "weaknesses": {
                "value": "I would have found it helpful to have other overall findings briefly summarized at a bit higher level for, eg a practitioner trying to build an RALM application. Something like: \"NLI-filtering can increase robustness to noisy IR, but at the cost of leaving IR gains on the table in some cases due to False Negatives. If possible for your setting, fine-tuning the model with intentionally varied IR quality seems to improve robustness without sacrificing performance.\"\"\n\nFigure 4 and Figure 5 conveyed all the results, but I still found it a little confusing or tedious to match up the pairwise analyses from the text to the \"shapes\" of the bar plots. Unfortunately I don't have a specific suggestion in mind here, but it did feel like a lot of cognitive load on the reader to swivel back and forth. \n\nThe claim that the fine-tuning teaches the model _when_ to use the context is not clearly established. That is, the results show that fine-tuning in the presence of both relevant and noisy contexts improves generalization results, but the mechanism by which it accomplishes this is not really known or demonstrated.\n\nResults with Llama-2-70B: \"suggesting it has more parametric knowledge\", I'm not sure this assertion is necessarily supported by the observations either"
            },
            "questions": {
                "value": "Will the released dataset contain the Google search results? If not, it may be difficult to fully reproduce the results. This would also handle the fact that these queries were issued against a particular point-in-time snapshot of Wikipedia.\n\nMinor comments / typos:\n* \"parametric memory\" - I understand this is an increasingly common term/phrase, but the first time this term is used in the paper it might be helpful to define it.\n* \"in-context learning has a negative affect\" - should be \"effect\".\n* Figure 5: \"impr oved\" typo, should be \"improved\".\n* \"Overall, this suggest\" - should be \"suggests\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7143/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675005019,
        "cdate": 1698675005019,
        "tmdate": 1699636845711,
        "mdate": 1699636845711,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PPYegqYgQX",
        "forum": "ZS4m74kZpH",
        "replyto": "ZS4m74kZpH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7143/Reviewer_CyE8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7143/Reviewer_CyE8"
        ],
        "content": {
            "summary": {
                "value": "This work presents studies cases where noisy retrieval reduces accuracy in retrieval-augmented LM systems. They propose two methods. In the first one, they use off-the-shelf entailment (NLI) models to fall-back to the LM's internal knowledge when NLI models judge the retrieved context as irrelevant. This method shows some promise but is too aggressive at skipping retrieval.\n\nIn their second method, they fine-tune the LM itself so it's robust to noisy contexts in single- and multi-hop settings. While this is easy to handle for single-hop questions, the authors propose a data generation algorithm that creates fine-tuning data for multi-hop robustness. This method prompts an LLM to generate multiple decompositions of multi-hop questions, and use a self-consistency check to identify high-quality training examples.\n\nThey conduct a rich analysis that shows that irrelevant context \"causes a wide range of errors, which include copying irrelevant answers from the retrieved sentences and hallucinating incorrect answers and decompositions.\" Their evaluation shows gains in practice across several QA benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and very easy to follow. (I give 3/4 for presentation only because of note #2 in weaknesses.)\n\n2. The work is highly systematic, starting from first principles and building multiple rich systems for RALM, with well-conducted experiments sprinkled throughout to support all key claims. The results are solid.\n\n3. The multi-hop data generation approach is novel and interesting."
            },
            "weaknesses": {
                "value": "1. If I understand correctly, you use the irrelevant context (e.g., in the single-hop case) to train the LM to answer the question by ignoring the context. Isn't this (almost) the definition of hallucination? The resulting LM will produce information not grounded in any passages. Isn't it better to abstain / request a new query, if the context is irrelevant?\n\n2. More fundamentally, it seems like the take-away message is almost presented as \"you should finetune on some examples with irrelevant/distracting context mixed in\", which however is a very old message incorporated already in multiple mainstream RALM research papers from 2020 on Open-QA (if not indirectly from circa 2018 with HotPotQA, I can't confirm this one).\n\nIt seems that the bigger contribution is: how to apply this for multi-hop tasks (interesting pipeline with code-davinci-002) and the analysis conducted, though I think this demands some changes to the discussion to make it clear what precisely is portrayed as new."
            },
            "questions": {
                "value": "1. Llama2 here refers to the vanilla or the chat variant?\n\n2. The choice of NLI model is quite underwhelming. Do you expect this to be different with good prompting of good LMs? Or with better finetuning for NLI?\n\n3. The focus on top-1 passage weakens the search space of the ideas in the work. Have you considered filtering passages (out of several passages) with NLI, or keeping the best-scoring NLI passage?\n\n4. The analysis conducted ends in two rich notes that I expect to yield a lot of value for this paper. I'd be willing to update my score (up or down) depending on that.\n\nQuote 1: \"In addition, the SA-R@1 that contains the top-1 results is not the best performing even when retrieving top-1 results at inference time, and is the worst performing when retrieving noisy contexts at inference time, suggesting that showing examples for retrieval during in-context learning has a negative affect that causes over-utilization of irrelevant context\"\n\nQuote 2: \"for at least 36% of the cases the generated answer or decomposition is correct, but the retrieved context does not directly entail the generation. This can be partially explained by the ability of the model to combine retrieved evidence and its parametric knowledge.\"\n\nHow robust is Quote 1 across LLMs and selection of examples? How can Quote 2 inspire a better way to incorporate NLI?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7143/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7143/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7143/Reviewer_CyE8"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7143/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698912750460,
        "cdate": 1698912750460,
        "tmdate": 1699636845584,
        "mdate": 1699636845584,
        "license": "CC BY 4.0",
        "version": 2
    }
]