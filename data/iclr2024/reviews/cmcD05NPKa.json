[
    {
        "id": "ogEh4m0AIM",
        "forum": "cmcD05NPKa",
        "replyto": "cmcD05NPKa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission386/Reviewer_zrzn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission386/Reviewer_zrzn"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an exploration of using small transformers to calculate the greatest common divisor (GCD) of two positive integers. A notable aspect is that the predictions made by these models are explainable. The authors focus on how models learn a list of divisors and predict the largest element that divides both inputs. They also investigate the impact of different training distributions on model performance, including uniform operands, log-uniform operands, and balanced distributions of GCDs. They also investigate with different inputs representations in different bases."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Studying learning GCD through transformers is a novel problem.\n- This work raises some observations about the deterministic behavior of the model in learning GCD, and the simple shortcut algorithm it learns.\n- A lot of experiments/examples supporting the claims are provided."
            },
            "weaknesses": {
                "value": "My main concern is about the significance of studying computing GCD through transformers. The observations are all limited to this particular task and it is not clear how broadly applicable they are. Or, if there is anything to learn from these observations and apply it elsewhere. \n\nMoreover, the observations are also not robust. The authors point out that the algorithm learned is deterministic but then this observations breaks down when the input distribution is changed. This non-robust behavior of the observations weaken them a lot in my opinion. For example, explainability breaks down with the change in input distribution."
            },
            "questions": {
                "value": "- Did the authors try to experiment with Chain-of-Thought and see if providing the explanation helps with the accuracy?\n- Can the authors please explain why does the model only learn divisors of the base B? And, if there is a way to promote learning of other divisors?\n- The authors mention that some non-divisible small primes are learned very late in the training stage. Did the authors see that if they continue training further, eventually other primes will be learned?\n- The authors mention that explainability of the model breaks down with the change in the input distribution. Do the authors have any thoughts on why this is the case?\n- Do few shot prompts improve the performance?\n- The authors mention they observe grokking phenomenon. Can the authors please provide more related work on grokking and transformers. And, if this is the first work observing grokking for transformers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission386/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698726038673,
        "cdate": 1698726038673,
        "tmdate": 1699635965839,
        "mdate": 1699635965839,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UqgkYSVbsJ",
        "forum": "cmcD05NPKa",
        "replyto": "cmcD05NPKa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission386/Reviewer_t8xf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission386/Reviewer_t8xf"
        ],
        "content": {
            "summary": {
                "value": "This paper studies Transformers capabilities on learning the task of computing greatest common divisor (GCD) for two numbers. First the two integers of input and the integer of the output are encoded in base $B$. Then, the model is trained with 300,000 new samples at each epoch. The authors have tried different distributions for the input/operands (uniform and log-uniform) and also the output. They have observed that in the majority of their experiments, model's output is deterministic, meaning that if $\\gcd(a,b)=k$, the output of the model is fixed depending on $k$. Further, for $k$'s that are a divisor of $B$ or small, the output is usually correct. Moreover, they show that by having a log-uniform distribution on the operands, the performance of the model is increased."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The particular attention to the distribution of the operands is important, and it's shown that by emphasizing on small numbers, the performance of the model can be increased (resembling curriculum learning). \n- Similarly, authors have considered the distribution of the output showing that large GCDs may be slow/hard to learn as they are rare. \n- Experiments are rather extensive in several axes (e.g., number of bases, size of the models, batch-size, ...)."
            },
            "weaknesses": {
                "value": "- The claim that Transformer predictions are fully explainable does not seem to be accurate once log-uniform operands are used. \n- Although, the paper usually interprets the results of experiments, it does not put forward any explanation for such results (for example, defining and justifying the shortcuts Transformers may take). \n- Some of the report rules might be consequences of other factors (and they may not be robust as a result). For example, the smaller primes are learned (or grokked) faster as they are more common in the output or really easier to learn? Although these problems can be clarified.  \nSee the questions below for more details."
            },
            "questions": {
                "value": "- Q1. It's said that composite bases allow one to check for divisibility by seeing the rightmost digits. This is true, for example for powers of the base, one needs to count the number of zeros. However, the picture seems to be more complex for divisors of the base. For example, consider $B=210$. Any number ($<10^6$), can be expressed using 3 digits in this base. Now for checking divisibility by 8, one has to check the pattern of all 3 digits (and it's not as simple as being 0). On the other hand, checking divisibility for any other number also requires checking the 3 digits together. Is there anything that is making checking for 8 simpler? (Similarly, assume that $B=350$, could we expect that checking for $3, 6$ would be easier than $8$?)\n- Q2. At the beginning of the paper there is the idea that divisors of $B$ are easily learned. Is this also the case when $B$ is for example, the product of two large primes, e.g., $B=2021=43\\times 47$? Is there a possiblility that smaller primes are learned faster than $43, 47, 43^2, \\ldots$ in this case?\n- Q3. Also given the list intuition, why 420 is performing better than 210 in Table 2?\n- Q4. Further, the list intuition does not seem to be applicable anymore in Table 6 (E.g., $2401=7^4$ is having the best performance). What could be the reason for this? \n\n\n- Q5. In claim U2, one can see that 21 is wrongly classified into $C_4$. As a result I wonder, for other bases that are product of larger primes, e.g., again, $2021=43\\times 47$, can this phenomenon still be observe? Or is having small primes such as 2 and 5 playing an important role here? \n- Q6. In the line before table 5, we see that for $B=30$, $B-1, B+1$ are learned faster than other primes. I guess this might be due to the fact that $(x_3x_2x_1)_B \\pmod{B-1} = x_3 \\times B^2 + x_2 \\times B + x_1 \\pmod{B-1} =  x_3 + x_2  + x_1 \\pmod{B-1}$ which may be easier to learn due to symmetry between $x_3, x_2, x_1$. Similar explanations can be given for any divisor of $B-1$ and $B+1$. If that's the case, this factor should also be considered when we see some primes are grokked. \n\n## Minor questions/remarks\n- Q7. What stopping criterion has been used? For example in Figure 4, the loss appears to be still decreasing. Maybe the combination of continued training and weight decay results in improved generalization as in grokking? \n- Q8. The addition and multiplication of rationals mentioned in the intro and studied in the appendix require the output to be simplified. So I wonder, is there any particular difference or additional complexity comparing to the simplification task itself?\n- Q9. What is the positional embedding used for the model?\n- R1. My understanding is that the samples at each batch are freshly generated. I think it would be beneficial if this is communicated more clearly and earlier to the reader. \n- R2. Table 5 is a bit misleading with the current format. For example, when we see than 9 is learned for $B=2017$ we have to guess that 3 was already learned. (Similar issue can be observed for other bases.) \n- R3. For Section 5 experiments in page 6, it would helpful if training loss plot is provided."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission386/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission386/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission386/Reviewer_t8xf"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission386/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768475325,
        "cdate": 1698768475325,
        "tmdate": 1699635965745,
        "mdate": 1699635965745,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PRdiYIuI4b",
        "forum": "cmcD05NPKa",
        "replyto": "cmcD05NPKa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission386/Reviewer_QHKu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission386/Reviewer_QHKu"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on training Transformers from scratch to predict the GCD of two numbers and explaining the algorithm used by the trained model.  It is observed that for almost all pairs of numbers for which the GCD is k, the model outputs a unique value f(k).  The model learns a set of numbers D, and for an input pair with gcd k, f(k) is the largest number in D that divides k. When numbers are represented in base B, set D contains all the products of primes dividing B. For large bases B, if training continues for very long, set D starts to include other prime numbers as well, usually learned in a monotonically increasing fashion. For example, in base 1000, set D would have numbers 1, 2, 4, 5, 10, 16, 20 and so on. If the training is continued for long, this set also starts to include 3 and its multiples with the numbers already in D. If the GCD k is one of the elements of this set, then the model output will be correct, otherwise, the model outputs the largest element of this set that divides k. The paper also investigates the role of training distribution."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "I really enjoyed reading this paper. In most cases, the algorithms learned by Transformers do not seem interpretable. I was surprised by how structured the encoded algorithm is in the case of GCD computation. This structure is very well explored in the paper through cleverly designed experiments and the intuition behind the results is also explained well."
            },
            "weaknesses": {
                "value": "I don't see any substantial weakness. Perhaps one could say that the implications of these results for large language models are unclear. However,  we barely know anything about the internal workings of Transformer models (the architecture behind LLMs) and this paper makes a small (and interesting!) step in enhancing our understanding."
            },
            "questions": {
                "value": "Minor suggestions/clarifications:\n\n1. On page 2, the term epochs is used for training, which suggested to me that there is a fixed training set over which the model is trained for multiple epochs. But in the last paragraph of the paper, it is mentioned that new training data is generated on the fly. Perhaps this should be clarified on page 2.\n\n2. In the abstract and introduction of the paper, there are lines of the form ``Models trained from uniform operands only learn a handful of GCD\n(up to 38 out of 100)''. This is confusing if the reader does not know that all numbers with the same GCD are mapped to the same output by the model. Without this information, it can be the case that two pairs of numbers have the same GCD but the model output is correct for only one of them. In that case, it is unclear what would it mean for the model to learn a handful of GCDs. This should be made clear in the abstract and intro."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission386/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699234359451,
        "cdate": 1699234359451,
        "tmdate": 1699635965663,
        "mdate": 1699635965663,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "v3Ef84iDDW",
        "forum": "cmcD05NPKa",
        "replyto": "cmcD05NPKa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission386/Reviewer_21KR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission386/Reviewer_21KR"
        ],
        "content": {
            "summary": {
                "value": "This paper trains small transformer models (4 layers) to compute the GCD of a pair of numbers. This is framed as a sequence prediction task. The input is a pair of numbers (a, b) encoded as a sequence of numbers in a base B. The model predicts a sequence of numbers (in base B) which collectively represent the GCD of (a, b). Performance is measured by 2 metrics - accuracy in prediction GCD and number of integers between 1 to 100 which are a GCD prediction. The second metric is required according to the paper because for any given pair (a, b), the GCD is likely to be small and they also turn out to be harder to predict correctly. The authors perform a bunch of experiments in a variety of configurations which will be described below.\n\nFor the first set of experiments, the training dataset (pairs of numbers) is sampled uniformly between 1 to a million. Models trained on these datasets show pretty variable accuracy but the main trend seems to be that using larger bases of composite numbers to encode pairs work well. Models also exhibit the property of being able to predict GCDs equal to the (prime) divisor of their bases (and their small powers) pretty well. Other small prime numbers which are not divisors of the base are \u2018grokked\u2019 much later during training.\n\nNext, the authors decide to oversample pairs of small numbers to create a lop-sided training distribution (called log-uniform in the paper). Models trained on this kind of distribution show dramatically improved and consistent performance regardless choice of base.\nFinally, the authors modify the training distribution to also be uniform over the predicted GCD. This maintains good accuracy of these models but degrades the consistency with which model predictions could be explained.\n\nTo analyze the model predictions, the authors introduce \u2018rules\u2019 which they obtain after looking at the results after each stage of the above experiments. These rule help us explain model predictions in a uniform fashion."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The main idea is pretty straightforward and most of the concepts in the paper are well explained (see concerns discussed below).\n2. To my knowledge, computing and analyzing GCD prediction via transformers has not been done before. \n3. Very comprehensive suite of experiments and exhaustive analysis. I appreciate the authors performing such a wide range of experiments. I also really like the nice link between theoretical accuracy and practical accuracy provided in Appendix C.\n4. Claims made by the authors are clearly backed up the experiments in the paper (see some minor concerns below)."
            },
            "weaknesses": {
                "value": "My main concern about this work lies with the significance of the results and observations made. The authors train a transformer to predict the GCD which seems to work fairly well with some tricks in picking the right dataset. However, I\u2019m not convinced about why this result would be of significant interest to the wider community and what it says about the representational power of transformer themselves beyond the narrow context of learning GCDs. Like I mentioned earlier, the experiments are thorough and the analysis is extensive but I\u2019m struggling to understand the value of this beyond this specific task. \n\nThe authors present \u20183 rules\u2019 which supposedly explain model predictions and while they seem technically correct, they do seem pretty specific to the task and don\u2019t seem to point to something specific about transformers themselves. For example, do we get these same rules if we swap the transformer architecture to something else? Is there something special about the architecture?\n\nHaving said all of that, I appreciate a good, rigorous experimental paper and I\u2019m open to being convinced by the authors and reading other reviewers comments about the value of this work.\n\nOther:\n1. The authors hypothesize on Page 3 \u201cFor prime bases, such as B = 2, the three rules suggest that the model learns to predicts GCD by counting the rightmost zeros in its inputs\u201d. Something like this seems like it should be fairly easy to confirm by visualizing the attention weights. I wonder if the authors have tried this.\n2. The authors mention that their model is \u2018fully explainable\u2019 (see abstract). I was expecting the model to output some kind of explanation for its prediction (say via attention) but what the authors actually meant was that the model predictions can be $\\textit{explained by}$ a human  because they follow certain rules. These rules keep on changing depending on the training configuration and can only be inferred after a thorough analysis of the results so I\u2019m not sure if we can call it explainability at least in the traditional sense used in XAI. However, the authors do say in the final section \u201cOur approach to explainability differs from most works on the subject. Instead of looking at model\nparameters, we engineer experiments that reveal the algorithms that the model is implementing\u201d. I think a statement of this sort at the start of the paper would help in clarifying the confusion. Regardless, I\u2019m not sure if I would call a model showing some consistent trends in the output as \u2018explainable\u2019.\n\nMinor:\n1. On page 2, when introducing the procedure to generate the stratified test set, the symbol \u2018M\u2019 is introduced without any explanation as follows \u201cSample a and b, uniformly between 1 and M/k , such\u201d. I\u2019m assuming M is the upper limit on the numbers a and b. This symbol is used throughout the paper and explained nowhere so it\u2019s confusing.\n2. The authors use two metrics to evaluate their models - accuracy on a test set and number of correctly predicted GCD below 100. At several points in the paper, terms like \u201c50 GCD\u201d or \u201c50 correct GCD\u201d are used. I now understand that they mean that the model predicts 50 GCDs out of a 100 between 1 to 100 correctly but it was definitely confusing to me initially. I think clarifying these terms at the start can be useful.\n3. Typo - Appendix C, first line, \u201cfor models from section 3 the follow the three rules\u201d should be \u201cfor models from section 3 that follow the three rules\u201d"
            },
            "questions": {
                "value": "How is the choice of the base made? Are they also sampled from a distribution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission386/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699603575910,
        "cdate": 1699603575910,
        "tmdate": 1699635965591,
        "mdate": 1699635965591,
        "license": "CC BY 4.0",
        "version": 2
    }
]