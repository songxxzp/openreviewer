[
    {
        "id": "bdriYvFbdq",
        "forum": "tNAucRS0QQ",
        "replyto": "tNAucRS0QQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission942/Reviewer_x6ZB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission942/Reviewer_x6ZB"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a general protein pre-training model, i.e., Biomolecular Interaction Transformer (BIT) to process molecules and pocket-ligand complexes in a unified style. Specifically, the main block of BIT is based on Transformer-M, a previous pre-training model. To enhance the model\u2019s ability of capturing multi- and inter-domain relationships, authors further incorporate Mixture-of-Domain-Experts (MoDE), i.e., separate feed-forward layers, for fusing molecule and pocket information better. Experiment results show that BIT achieves state-of-the-art performance in various downstream tasks, including binding affinity prediction, virtual screening, and molecular property prediction."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- BIT shows great performance in all downstream tasks listed in the paper.\n- BIT can handle molecules and protein pockets in a unified way.\n- BIT can work well with both 2D and 3D molecules."
            },
            "weaknesses": {
                "value": "- Contribution is minor. BIT combines the architecture of Transformer-M and Mixture-of-Domain-Experts technique, which both are from existing methods [1, 2, 3]. In addition, the way of combining the two is also a simple adaption. In summary, I appreciate that authors provide a strong method but I also believe that the contribution of this paper is not enough for acceptance.\n- Experiments are not convincing enough:\n1) Authors did not provide ablation studies to directly show the effectiveness of the main components, e.g., MoDE, of BIT. \n2) The comparison between BIT and the main baseline, i.e., Transformer-M, may not be fair enough. As BIT adopts MoDE technique, it has more trainable parameters than the vanilla Transformer-M. Moreover, BIT uses not only small molecule data but also protein-ligand complex data in the pre-training stage, while Transformer-M only uses small molecule data.\n3) The results of Transformer-M are not included in virtual screening and molecular property prediction benchmarks.\n- Some important details are missing, e.g.,\n1) In section 3.1, authors mentioned domain type embedding but without further description.\n2) Also in section 3.1, authors introduce two special nodes, i.e., [M_VNode] and [P_VNodes]. It is unclear how to build up the input representation with these two nodes.\n3) What is the specific value of noise scale controlling hyperparameter $\\sigma$?\n\n[1] Luo, S., Chen, T., Xu, Y., Zheng, S., Liu, T. Y., Wang, L., & He, D. (2022). One transformer can understand both 2d & 3d molecular data. arXiv preprint arXiv:2210.01765.\n\n[2] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538.\n\n[3] Fan, Z., Sarkar, R., Jiang, Z., Chen, T., Zou, K., Cheng, Y., ... & Wang, Z. (2022). M\u00b3vit: Mixture-of-experts vision transformer for efficient multi-task learning with model-accelerator co-design. Advances in Neural Information Processing Systems, 35, 28441-28457."
            },
            "questions": {
                "value": "- In section 3.3, why only add noise to molecules when doing pre-training?\n- In section 3.4.1, Why not use [P_VNode] or the combination of [M_VNode] and [P_VNode] as the representation of protein-ligand complexes?\n- In table 3, why the PCBA dataset is not included in the table?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission942/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission942/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission942/Reviewer_x6ZB"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission942/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698190989276,
        "cdate": 1698190989276,
        "tmdate": 1699636020943,
        "mdate": 1699636020943,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "I4jDAyMwRt",
        "forum": "tNAucRS0QQ",
        "replyto": "tNAucRS0QQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission942/Reviewer_uSoZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission942/Reviewer_uSoZ"
        ],
        "content": {
            "summary": {
                "value": "The \u201cGeneral purpose pre-trained model\u2026\u201d paper proposes a Biomolecular Interaction Transformer BIT, which is to have a multi-modal training on molecules, together with protein\u2014ligand matching, with 2-D and 3-D structures. The model includes a pre-training.\n\nIn my opinion, this is a valuable paper, presenting a well-defined model, together with well-designed experiments. The BIT model might not be revolutionary, but is a piece of a very solid work, and well performing. I opt for accepting this paper for the conference."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. An advanced model, encompassing 2-D and 3-D structures for molecules, proteins, and ligand\u2014protein interaction modelling, with multi-modal training. The model can be tuned.\n2. Well-defined multi-modal representation learning employing a Transformer model (a Transformer-M model of Luo et al.) with independent tuning of proposed BIT for different knowledge domains.\n3. A very good graphical abstract is given on page 2, showing in detail the proposed architecture. Clear presentation. All this increase the paper readability greatly."
            },
            "weaknesses": {
                "value": "1. Some generalization of the model to other areas would be welcome."
            },
            "questions": {
                "value": "1. In the comparison tables, the models (usually proposed BIT) with the best mean have values given in boldface. Are all the models trained on the same data-sets? For some predicted features, the differences between BIT and some other models are large, even though some of them are Transformers too. Are the optimal values for BIT the result of the proposed BIT architecture, the fine-tuning on different modes (the multi-modality), different data sets, better pre-training, or something other? The discussion on the comparison to other approaches is needed in the conclusions/discussion section.\n2. Please correct the spelling of some words. Just as well, please correct the editing of mathematical expressions. E.g. in equations (1) and (2) the equal signs = should be aligned ;-)\n3. Your paper and model is strictly molecule learning oriented. Do you think that the general approach can be used in other sciences? E.g. in biological experiments on cancerous cells and the impact of some sort of certain treatments, which would imply modifications in the course of the operation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission942/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission942/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission942/Reviewer_uSoZ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission942/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672124366,
        "cdate": 1698672124366,
        "tmdate": 1699636020841,
        "mdate": 1699636020841,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ariViumA1p",
        "forum": "tNAucRS0QQ",
        "replyto": "tNAucRS0QQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission942/Reviewer_dgYT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission942/Reviewer_dgYT"
        ],
        "content": {
            "summary": {
                "value": "In this study, a new pre-trained transformer, BIT, is introduced for processing small molecules, proteins, and ligand-protein complexes. The architecture is based on Transformer-M, which is a transformer architecture that enables the processing of 2D and 3D structures. The main architectural innovation introduced in this work is the use of Mixture-of-Domain-Experts (MoDE) that replaces feed-forward layers, allowing for different processing of small molecules and macromolecules. Two pre-training methods, masking and coordinate denoising, are used to improve the performance of this model. BIT can be utilized for various molecular tasks, including molecular property prediction, structure-based virtual screening, and binding affinity prediction. The experimental section shows that BIT outperforms similar approaches in all three tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposition to use Mixture-of-Domain-Experts in order to process both small molecules and proteins and protein-ligand complexes is interesting. This way, the transformer can be (pre)trained using more data with high diversity.\n- Two pre-training methods are implemented, and the strong performance of the pre-trained transformer is demonstrated in the experiments.\n- The motivation of the paper is clear, and the methodology and experiments are easy to follow.\n- This work has some significant applications in the molecular modeling domain, especially in structure-based drug design. Because BIT can process both small molecules and proteins, the application domain is very broad. The significance of the study is corroborated by the strong performance in the molecular property prediction, binding affinity prediction, and structure-based virtual screening tasks."
            },
            "weaknesses": {
                "value": "- The main novelty of the paper is the introduction of MoDE in order to process data from diverse molecular domains. In my opinion, the paper lacks a proper evaluation of what these experts learn. For example, do molecule and pocket experts learn similar weights, or are there significant differences? What is the performance of this model when only one type of feed-forward layer is used (like in Transformer-M), but the same pretraining procedure is applied?\n- The Authors propose to use two pre-training objectives. It would be interesting to see what is the impact of each of them. Why did the Authors decide to use a different pre-training procedure than used in Transformer-M? The experimental tables are missing the results achieved for the non-pretrained model.\n- The choice of the models used in the experiments seems arbitrary. For example, why are different models used for binding affinity prediction and molecular property prediction? GROVER could be used in both scenarios. Why are some of these models not pre-trained, while the original works provide pre-training procedures (and sometimes also the pre-trained weights), e.g. GROVER and MAT?\n- Finally, another benchmark for structure-based virtual screening would be helpful in assessing the performance of the proposed method. It has been shown, that decoys contain hidden biases that can impact the performance of deep learning methods [1]. \n\n[1] Chen, Lieyang, et al. \"Hidden bias in the DUD-E dataset leads to misleading performance of deep learning in structure-based virtual screening.\" PloS one 14.8 (2019): e0220113.\n\nIn conclusion, the paper introduces some new ideas (mixing of domains and a different pre-training procedure), but the presented results do not support these design choices. It is not clear which novelties contribute most to the strong performance of BIT.\n\nMinor comments (not impacting my evaluation):\n- The first paragraph of the Introduction section: \"depend on these information\""
            },
            "questions": {
                "value": "1. For binding affinity prediction, did you consider measuring non-linear correlation, e.g. using the Spearman correlation coefficient? This evaluation metric could be better at showing which methods can correctly prioritize compounds with strong affinity.\n2. Do you plan to publish the code for better reproducibility of your method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission942/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838294779,
        "cdate": 1698838294779,
        "tmdate": 1699636020765,
        "mdate": 1699636020765,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hhwD0QVAC3",
        "forum": "tNAucRS0QQ",
        "replyto": "tNAucRS0QQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission942/Reviewer_2Mm5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission942/Reviewer_2Mm5"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a multitask Transformer architecture for molecular interaction learning called Biomolecular Interaction Transformer (BIT)  and a self-supervised learning objective of coordinate denoising and masked token denoising. This approach allows the model to learn representations of biomolecules of both 2D and 3D structures. Moreover, the paper introduces the Mixture-of-Domain-Experts (MoDE) module, which enables the model to learn from biomolecules belonging to different chemical domains (proteins and ligands)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper is well motivated and the writing is clear.\n+ The proposed approach focuses on protein-ligand interactions, which is of paramount importance for drug discovery and can potentially lead to more efficient and effective therapeutic solutions."
            },
            "weaknesses": {
                "value": "- The pretraining process appears to be confined to proteins and small molecules. This limited scope raises questions about the model's applicability to a broader range of biomolecules and interactions. \n- The proposed Transformer backbone is invariant to geometric transformations, which may limit its expressive power compared to SE(3) or E(3) equivariant architectures.\n- The model only considers the binding pocket segment of proteins\u2014while computationally efficient, may not be practically feasible without costly simulations to identify these segments. This reliance on prior knowledge or expensive computations could limit the accessibility and scalability in practical applications.\n- The strategy of pre-training the model on both equilibrium structures of molecules and higher-energy protein-ligand complexes may be conceptually problematic. These two types of data represent vastly different energy states, and it is unclear what meaningful semantic learning can be achieved by pretraining them together. This may lead to a model that does not adequately distinguish between the distinct energetic landscapes of the two systems.  Also, the scales of the two training sources are not balanced, where PCQM4Mv2 is significantly larger than Q-BioLiP, but the model performance on molecular property prediction is not very significant compared to the baselines, which warrants a further examination.\n- There is a lack of a detailed explanation for how positional encodings of 2D and 3D structures are integrated into the model, which leaves a gap in understanding the full architecture and mechanics of the model."
            },
            "questions": {
                "value": "Please see the above weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission942/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699059018458,
        "cdate": 1699059018458,
        "tmdate": 1699636020665,
        "mdate": 1699636020665,
        "license": "CC BY 4.0",
        "version": 2
    }
]