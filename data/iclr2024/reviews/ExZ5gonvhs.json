[
    {
        "id": "ygBbiuC2CK",
        "forum": "ExZ5gonvhs",
        "replyto": "ExZ5gonvhs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4387/Reviewer_sJCd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4387/Reviewer_sJCd"
        ],
        "content": {
            "summary": {
                "value": "A method using prior knowledge to sample the positive data is proposed. It is supposed to mitigate the importance of data augmentation in self-supervised learning. The proposed GPS-SSL has shown superior capability over the methods with existing augmentation strategies."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Studying new strategies that rely less on data augmentations in self-supervised learning is worthwhile to the representations learning fields.\n+ Exploring the pre-trained models (CLIP, Supervised models, VAE) for improving SSL might be interesting."
            },
            "weaknesses": {
                "value": "+ The proposed method needs a heavier component (such as a neural network ResNet-50) to generate the positive data sample, which is significantly computational compared to a simple calculation of data augmentation even for strong augmentations with a series of cropping, color jittering, distortion, hue, etc...\n\n+ With the aid of a strong knowledge (and heavy) model trained on millions or hundred million of data (CLIP, ImageNet) the performance of the proposed method brings minimal advantage even worse than the existing SSL method such as VICReg in Table 2 with strong augmentation. In the weak augmentation setting, GPS-SSL may give better performance but still lag significantly behind the optimal setting (strong augmentation) of both streams, making it questionable about the contribution of the proposed method.\n\n+ SSL contains another branch that is also very promising with the fine-tuning accuracy on downstream tasks such as MAE [1], this approach also depends very little on data augmentation (only cropping or without any augmentation already made the very good performance). This example (MAE method) will challenge the proposed method in terms of dependency on augmentation because the proposed method could not work without augmentation. I believe that modern SSLs should include this metric (fine-tune accuracy) and compare both contrastive learning and MAE approaches.\n\n+ It should also include the linear evaluation of the only CLIP RN50 or supervised RN50 model when they have been used as the feature extractor for the downstream tasks on each considered dataset. It is to see without any training, how well these pre-trained model can perform, and based on that we can assess their contribution to the GPS-SSL (which is a combination of existing SSL + pre-trained CLIP/RN50).\n\n+ Another point is that the experimental setting is not practical and sufficient to demonstrate the effectiveness of GPS-SSL when evaluating self-supervised contrastive learn is that they only consider pretraining with 200 epochs, which is very few epochs required by SSL models to fully converge. As shown in SimSiam or many SSL (MoCo, BYOL, Barlow Twins, VICREG,... ) the performance is best achieved with long enough self-supervised pretraining (800-1000 epochs). As a result, the comparison in long training should be considered for both methods.\n\n+ It is not clear what is the metric they have shown in Table 1. Reading its caption, it is challenging to capture what metric they are comparing, top-1 ACC or error or something else.\n\n[1] Masked Autoencoders Are Scalable Vision Learners, CVPR 2022"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4387/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698304725319,
        "cdate": 1698304725319,
        "tmdate": 1699636411664,
        "mdate": 1699636411664,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0a57yF5cU7",
        "forum": "ExZ5gonvhs",
        "replyto": "ExZ5gonvhs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4387/Reviewer_uQnq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4387/Reviewer_uQnq"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed the Guided Positive Sampling (GPS) approach to\nfinding positive pairs in self-supervised learning, without data\naugmentation.  For each instance, a nearest neighbor is found in an\nembedding space pretrained with another dataset or with a variational\nautoencoder on the same dataset.  The corresponding instance becomes\nthe positive instance for self-supervised learning.\n\nIn their experiments, they consider using GPS with SIMCLR, BYOL,\nBarlow, and VICreg on five datasets.  For GPS, they use embeddings\nfrom supervised training, CLIP or VAE.  Generally, empirical\nresults indicate that using GPS outperforms, particularly with weak\naugmentations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Not relying on heavy handcrafting of data augmentation for\nself-supervised learning is interesting.  Using prior knowledge based\non a pretrained encoder, they propose to find a nearest neighbor to\nform a positive pair.  Generally, empirical results indicate that using\nGPS outperforms, particularly with weak augmentations."
            },
            "weaknesses": {
                "value": "With prior knowledge, GPS seems to have an advantage over regular SSL,\nwhich generally does not use prior knowledge.  According to Figure 1,\ndata augmentation is used in GPS-SimCLR.  So GPS seems to differ only\nin the use of prior knowledge to find positive pairs.\n\nDetails are in questions below."
            },
            "questions": {
                "value": "1.  Theorem 1: GPS-SSL: employing eq (2) or (3) into eq (1)?\n\n2.  Table 2: why are two different kinds of prior knowledge is used?\n\n3.  How is $Tau$ set in Equation 3?\n\n4.  With prior knowledge from another encoder, GPS has an advantage.\n    Hence, comparison with methods that don't have prior knowledge\n    might not be fair.  Could the regular SSL (with augmentation) also\n    use prior knowledge?  For example, the encoder is initialized by\n    prior knowledge and then regular SSL is performed.\n\n5.  Sec 4.1, how do you predict if the classes do not overlap in the\n    training and test sets (unseen classes branches/chains)?\n\n--------  after response from authors ---\n\nI think the authors performed experiments that remove the advantage of prior knowledge used in GPS and the results indicate GPS can improve performance over regular SSL."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4387/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4387/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4387/Reviewer_uQnq"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4387/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698777702068,
        "cdate": 1698777702068,
        "tmdate": 1700611312546,
        "mdate": 1700611312546,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "e2z6GozzAF",
        "forum": "ExZ5gonvhs",
        "replyto": "ExZ5gonvhs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4387/Reviewer_aLRr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4387/Reviewer_aLRr"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a method that integrates prior knowledge into Self-Supervised Learning (SSL) to improve positive sample selection and reduce reliance on data augmentations. Based on pretrained visual models and target dataset, GPS-SSL creates a metric space that facilitates nearest-neighbor sampling for positive samples. The method is applicable to various SSL techniques and outperforms baseline methods, particularly when minimal augmentations are used."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Extensive experiments show the effectiveness of the GPS strategy.\n- The paper is easy to follow."
            },
            "weaknesses": {
                "value": "- The employment of prior knowledge, specifically in the form of a pretrained visual model and the target dataset, diverges from the fundamental principles of Self-Supervised Learning (SSL).\n- The incorporation of such prior knowledge raises concerns about the fairness of comparisons with existing SSL methods. There is a potential risk that the pretrained visual model and target dataset might leak additional information into the model, thereby skewing results and leading to issues of unfairness.\n- The difference between GSP-SSL and NNCLR lies primarily in their respective positive sampling strategies. However, the novelty of the proposed strategy is limited."
            },
            "questions": {
                "value": "- It would be better to make prior knowledge in an unsupervised manner, except using pretrained visual model and target dataset.\n- The supervised results are supposed to be shown in Table 2."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4387/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4387/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4387/Reviewer_aLRr"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4387/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822019420,
        "cdate": 1698822019420,
        "tmdate": 1699636411494,
        "mdate": 1699636411494,
        "license": "CC BY 4.0",
        "version": 2
    }
]