[
    {
        "id": "pN9gvyUfTX",
        "forum": "NW2s5XXwXU",
        "replyto": "NW2s5XXwXU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1786/Reviewer_Uefc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1786/Reviewer_Uefc"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the author's primary focus is on tackling the diffusion model generation tasks in the context of long-tailed training scenarios. Different from previous works that enhance tail generation by relying on the abundant diversity derived from the head class (the condition capacity of the model prediction), this paper directly establishes the knowledge transfer from head data samples, based on the multi-objective characteristics of the score function in the diffusion process. A directional calibration for the estimation of noisy tail sample score is performed towards the clean head samples (T2H), leveraging the similarity within the data distribution from head to tail classes. Meanwhile, H2T is proposed for the unconditional generation. Extensive experiments demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. This paper presents a novel idea that calibrates the estimation of noisy tail sample score by the clean head samples instead of relying on the condition capacity of the model prediction. \n2. This paper has a well-organized structure which makes it easy for readers to understand the research.\n3. The paper is well-supported by a strong theoretical foundation, which robustly underpins the proposed solutions.\n4. The text is clear with a good writing style."
            },
            "weaknesses": {
                "value": "1. The explanations of some figures are not sufficiently comprehensive. For instance, the author could enhance the clarity of differentiation by adding annotations or captions to both the top and bottom parts of Figure 1.\n2. Some problems, which I will raise in the following section."
            },
            "questions": {
                "value": "1. In the paper, T2H is utilized to estimate noisy tail samples using clean head samples, and this process is explained as a way to enhance the performance of long-tailed conditional generation. However, I have difficulty understanding why H2T, which estimates noisy head samples using clean tail samples, would improve the performance of unconditional generation. This is because, in my view, the style of the tail samples should be relatively homogeneous and might not provide substantial knowledge for estimating the head samples. Could the authors please provide an explanation?\n2. In Table1, why the \"Full\" (allowing both transfer directions) performs worse than the H2T and T2H (only allowing one direction) under the unconditional generation? Meanwhile, I'd like to see the performance of \"Full\" for the conditional generation.\n3. The author should explain more about why the baseline DDPM has a higher IS than T2H (Table 2 and Table 3)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1786/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1786/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1786/Reviewer_Uefc"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1786/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698375220567,
        "cdate": 1698375220567,
        "tmdate": 1699636108076,
        "mdate": 1699636108076,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "L7ZMuNzRMQ",
        "forum": "NW2s5XXwXU",
        "replyto": "NW2s5XXwXU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1786/Reviewer_PNyr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1786/Reviewer_PNyr"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an algorithm for calibrating diffusion models for long-tailed data. Based on the recent work by Xu et al., 2023, in this work, the target for the score estimation is re-written as a mixture of multiple targets computed from a reference batch. The reference batch, unlike the work of Xu et al,. 2023, is sampled from a class-balanced distribution in order to calibrate the model for tail-class samples. For the conditional generation, the authors further propose a strategy called Tail-to-Head (T2H), where the target of the score estimation for a tail class sample is chosen to be a clean head class sample. The paper also suggests a variant called Head-to-Tail (H2T), where in this case a noisy head sample takes a clean tail sample as a target. The proposed method is demonstrated to improve image fidelity when applied to popular imbalanced image benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is tackling an interesting problem.\n- The proposed method is simple and easy to implement.\n- The experimental results, at least for the simple benchmarks, is promising."
            },
            "weaknesses": {
                "value": "- The presentation is poor. I don't really follow the description of the intuition behind the presented method. Please see my questions below.\n- Again, probably related to the first point, I don't see how the proposed method yields a valid regression target. The algorithm seems to be a heuristics without considering correctness.\n- Batch-Resample cannot be considered a novel contribution, as it is one of the standard approaches one could take for a problem involving a long-tailed dataset (e.g., long-tailed image classification)."
            },
            "questions": {
                "value": "- I'm very confused about the argument around Proposition 3.1. First, it is not clear from the text what the \"score weight\" or \"weight of score\" means and why it is important. I guess this term refers to the mixing coefficients (normalized transition probabilities) appearing in the multi-target representation of the score function. Secondly, what is the author trying to make out of Proposition 3.1? How is it related to the algorithm? The sentence \"Intuinitively (intuitively, I guess), improving the score weight of head-class samples for the noisy $x_t$ from tail-class samples, increases the generation diversity of tail categories\" is not clear at all. What is \"improving\" the score weight? Is it increasing the value of the score weight? If so, how is it related to increasing the generation diversity?\n- The introduction of the selection procedure (equation (11)) comes without sufficient explanation. If I have to understand it, the random sampling for $z$ can be understood as a Monte-Carlo estimator of equation (8); But then the proposed algorithm alters the target score if $q(z_y) \\geq q(y_i)$; as a result, the target of the score estimation becomes something different, yielding a biased estimator of the original target. Is this intended? If so, how do you guarantee that the proposed algorithm is learning a correct target?\n- In the beginning you assumed $q(x_t|x_0, y_0) \\propto B q(y_0)^\\beta \\exp(-\\Vert x_t-x_0\\Vert_2^2/2\\sigma_t^2)$, but $p_\\text{sel}(z)$ described to computed only with $C \\exp(-\\Vert x_z-x_t\\Vert_2^2/2\\sigma_t^2)$ without $q(y)^\\beta$ term. Is this a typo or am I missing something here?\n- Similarly, I don't get the intuition behind the H2T, since from the beginning I failed to follow how Proposition 3.1 is linked to the T2H. What is the meaning of setting $\\beta=-1$ and why is it beneficial for unconditional sampling? \n- The experiments mostly show the results of T2H on class conditinal generation and H2T for unconditional generation, but how do they work for opposite settings (T2H for unconditional and H2T for conditional)? Is the direction of transfer matters? Or both direction helps in either cases, but the effectiveness of transfer may vary depending on the task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1786/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1786/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1786/Reviewer_PNyr"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1786/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698673261942,
        "cdate": 1698673261942,
        "tmdate": 1700576587147,
        "mdate": 1700576587147,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rhha64ZzN5",
        "forum": "NW2s5XXwXU",
        "replyto": "NW2s5XXwXU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1786/Reviewer_i52Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1786/Reviewer_i52Z"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the problem of long tail diffusion model generation. In this paper, a directional calibration for the estimation of\nnoisy tail sample score is performed towards the clean head samples (T2H), leveraging the similarity within the data distribution from head to tail classes."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper propose a strategy denoted as \u201dBatch Resample\u201d to sample a more balanced reference batch. \n2.This paper has developed a method denoted as T2H based on the multi-target nature of score estimation to effectively calibrate and enhance the generation of tail classes in the semantic formation period, thereby significantly improving the overall generation performance."
            },
            "weaknesses": {
                "value": "1. The paper introduces two optimization methods, Batch Resample and T2H. The Batch Resample is a common solution to the long-tailed problem, and it does not seem to bring any novelty. \n2. There is a lack of connection between the proof and implementation of T2H, especially the transition from equation 10 to equation 11.\n3. The author lacks experimental validation on large-scale datasets."
            },
            "questions": {
                "value": "1. T2H enhances the quality of image generation for the current category by calculating the multi-nominal distribution in Equation 11, and then samples $z$ from this distribution, and selecting categories $y_z$ with a sample density greater than that of the category $y_i$. While the authors provide detailed theoretical proof, the transition from equation 10 to equation 11 seems to lack coherence. When sampling z, why not model the distribution based on the similarity between the original images (the current sample image and other images in the batch, as in equation 10), rather than through the noisy version of the current sample image and other images in the batch?\n2. In the T2H method mentioned, the distribution $p_{sel}(z)$ adds weight to the other images within the batch, encouraging similar images to have a greater influence on the current sample. So, what would happen if we do not consider the distribution $p_{sel}(z)$ and assume that all images will have an effect? Also, if we keep the distribution $p_{sel}(z)$ unchanged and assume that all $y_z$ will have an impact, not just those with a high sample density, would this be more reasonable?\n3. The authors seem to lack an analysis in their experiments regarding how much growth the proposed solutions have brought to both the head and tail categories in the dataset, respectively.\n4. How effective are the proposed methods on larger datasets (ImageNet-LT)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1786/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742979144,
        "cdate": 1698742979144,
        "tmdate": 1699636107890,
        "mdate": 1699636107890,
        "license": "CC BY 4.0",
        "version": 2
    }
]