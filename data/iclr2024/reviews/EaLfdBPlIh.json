[
    {
        "id": "61HNzmwRey",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1880/Reviewer_zxD3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1880/Reviewer_zxD3"
        ],
        "forum": "EaLfdBPlIh",
        "replyto": "EaLfdBPlIh",
        "content": {
            "summary": {
                "value": "This paper aims to address a persistent challenge in object-centric learning (OCL) -- the need to determine a predefined number of slots. The authors propose to use Gumbel Softmax to dynamically select the number of slots. To train the model, they introduce a masked slot decoder to filter out non-selected slots from the model output. Experimental results on CLEVR10, MOVi-C/E, and COCO demonstrate the effectiveness of this method in adapting the number of slots, leading to consistently better results compared to OCL with fixed number of slots."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow\n- The number of slots as a hyper-parameter is indeed a long-standing problem in the field. This paper proposes a valid solution to it\n- The experiments show promising segmentation results on both synthetic and real-world datasets"
            },
            "weaknesses": {
                "value": "I have two main concerns regarding the paper:\n- The experiments in the paper mainly focus on object segmentation. While it is an important outcome of OCL, the quality of learned object slots is another important aspect. The authors conduct object property prediction experiments on CLEVR10. However, CLEVR10 is too simple. I would suggest the authors to further experiment on MOVi, e.g., follow the protocol in LSD [1]\n- The goal of this paper is to get rid of a pre-defined number of slots. However, there is still a hyper-parameter $K_{max}$. I understand that this is necessary for implementation. But $K_{max}$ on CLEVR, MOVi-C/E is actually the maximum number of objects on that dataset plus one. Therefore, this still utilizes the prior knowledge of the dataset. On COCO, $K_{max} = 33$ seems to be an arbitrary number. I wonder how will the model perform if using $K_{max} = 30$ on MOVi-C/E?\n\n[1] Jiang, Jindong, et al. \"Object-centric slot diffusion.\" arXiv preprint arXiv:2303.10834 (2023)."
            },
            "questions": {
                "value": "Besides weaknesses, I have two minor questions:\n- Does the GENESIS-V2 baseline use the same DINO ViT encoder? If not, that seems like an unfair comparison. But I somehow feel that is fine since DINOSAUR is the current SOTA OCL method\n- Can the masked slot decoder technique be easily extended to the Transformer-based slot decoder, like SLATE and STEVE?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1880/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697248261659,
        "cdate": 1697248261659,
        "tmdate": 1699636118492,
        "mdate": 1699636118492,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xAhJKEVAU6",
        "forum": "EaLfdBPlIh",
        "replyto": "EaLfdBPlIh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1880/Reviewer_BxGH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1880/Reviewer_BxGH"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method for dynamically adjusting the number of slots for slot attention so that it can vary across inputs. Each slot predicts whether it \"wants\" to be kept or not, from which a mask is sampled using Gumbel Softmax. If a slot is dropped, its influence on the attention is set to 0. There is an additional regularization term that encourages lower numbers of slots."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Reasonable approach to the problem while still being computationally efficient and differentiable\n- Considers a problem that seems useful, improving the determination of the \"correct\" number of objects\n- Extensive ablations and analysis to understand the method are impressive"
            },
            "weaknesses": {
                "value": "- The premise of the paper hinges on the assumption that it is desirable to have the slots correspond exactly to objects. While this appears reasonable in simpler datasets, the notion of what an object is becomes less clear for more realistic datasets (such as COCO, which only has certain types of objects labeled at a very particular granularity level). In the end, the goal of unsupervised object discovery is to do something useful with the objects. Therefore, I believe that when arguing whether one decomposition over slots (normal slot attention) is better or worse than another (the variant proposed in this paper), it should be tied to downstream performance, less to accuracy of the provided \"ground-truth\" masks; especially in the case of natural images, where objects of different levels of abstraction are valid at the same time. My worry is therefore that this method works well on simple images where there is an obvious notion of object, but does not scale well to the case of realistic images. I believe it would be highly beneficial if the proposed model can be evaluated on *downstream performance* on something that is more realistic, which (given the use of DINOSAUR already) should not be unsurmountable. So far, I only see the downstream performance for CLEVR10, which is a simple dataset (with admittedly better performance for the proposed approach). This is my main concern, I am willing to change my score if this is sufficiently addressed.\n- There are no error bars on the experiments"
            },
            "questions": {
                "value": "- It would be interesting to see an oracle baseline where you use DINOSAUR, but provide the ground-truth number of slots per instance and compare performance. Does your model approach the performance of this (unrealistic) model that knows the ground-truth number of objects?\n- Can the maximum number of slots for your model K_max be set arbitrarily high? Are there performance trade-offs for that?\n- I did not understand the second part of the sentence: \"We set \u03bb to 0.1 for MOVi-E/C and 0.5 for COCO, without specifying a particular claim\". What does specifying a particular claim mean here?\n- What does \"flattening the videos\" mean in the context of turning the MOVi video dataset into an image dataset?\n- Figure 7, how is the prediction for the number of objects trained and made?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1880/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698351301424,
        "cdate": 1698351301424,
        "tmdate": 1699636118423,
        "mdate": 1699636118423,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OyRww0nSS6",
        "forum": "EaLfdBPlIh",
        "replyto": "EaLfdBPlIh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1880/Reviewer_pNUM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1880/Reviewer_pNUM"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an adaptive slot attention mechanism for the problem of object discovery. On observing that current object-centric approaches, such as slot attention, rely on predefining the number of slots, they propose a complexity-aware object auto-encoder framework to dynamically determine the optimal number of slots based on the content of the data. Extensive experiments on standard benchmarks reveal the effectiveness of the proposed framework in terms of object discovery tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method is well-motivated. \n\n- The proposed method is shown to be effective in dynamically the number of slots within the attention mechanism.\n\n- The proposed method is simple and easy to implement."
            },
            "weaknesses": {
                "value": "- The proposed method has little novelty since it is a straightforward combination of existing methods: slot attention (Locatello et al., 2020), clustering number selection (Blei & Jordan, 2006), and Gumbel-Softmax (Jang et al., 2016). The authors are encouraged to offer more insights into how these modules interact with each other and the specific roles they play individually. This added detail would contribute to a deeper understanding of the method's functionality and its overall design rationale.\n\n- The experimental scope of the paper is somewhat limited, with the authors only utilizing two real-world datasets. I would recommend that they consider expanding their experiments to include additional datasets, such as commonly used segmentation datasets, to provide a more comprehensive evaluation of their method and strengthen the validity of their results.\n\n- The logical flow of the manuscript requires improvement as there are numerous sentences throughout the paper that lack clarity and coherence. Attention to sentence structure and overall organization is necessary to enhance readability and ensure that the authors' arguments are conveyed effectively."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1880/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1880/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1880/Reviewer_pNUM"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1880/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840944636,
        "cdate": 1698840944636,
        "tmdate": 1699636118356,
        "mdate": 1699636118356,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mWZH4Lp3m0",
        "forum": "EaLfdBPlIh",
        "replyto": "EaLfdBPlIh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1880/Reviewer_CDqw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1880/Reviewer_CDqw"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel method called adaptive slot attention, which could select an appropriate number of slots automatically based on the complexity of the instance, better to serve slot attention for object-centric learning tasks. In particular, a discrete slot sampling module and a masked slot decoder are designed to work for the adaptive mechanism, and the experiments conducted on benchmark datasets verify the stability and effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. To design a module to adaptively select the number of slots used for slot attention is an interesting and exciting research direction. \n\n2. The complexity-aware object-encoder framework is designed simple and clear. It makes the readers easily understand the function of every single component.\n\n3. This paper provides a variety of experimental studies to better demonstrate the properties of the proposed method."
            },
            "weaknesses": {
                "value": "1. This paper is a bit difficult to follow. For example, this paper proposes an object-centric learning method, but the title is about one of its applications, object discovery. Meanwhile, there are no experiments about object discovery to further support the title. It makes the readers confused about the target of this paper. In addition, the writing of the method part could be better improved. For example, it is difficult to follow the authors to understand what are the structures of the two proposed modules. It could be better to provide a framework or pipeline figure to demonstrate the whole structure. Also, for the equations, there are several important notion descriptions missing and ambiguous. For example, in the preliminary, is F \\in \\mathcal R^{H \\times W} or F \\in \\mathcal R^{C \\times H \\times W}. The encoded features should have a dimension if I'm correct. Meanwhile, what is the notion \\alpha_i? What is the operation between m_i and x_i in the first term of Eq(2)? Moreover, how the original slot attention works could be better explained in this part. In the second paragraph on page 4, what is the relationship between the instance \\mathbf x and the image x_i that appeared in Eq(2)? This part could easily make the readers confused. For the sentence after Eq(5), where are \\pi_{i,0} and \\pi_{i,1}? They are not mentioned in the previous part but suddenly appear in this explanation.\n\n2. It could be better to rearrange the experimental part. Firstly, since the paper aims at designing a slot number selection mechanism, to provide a comparison between ground truth on the number of the objects in the image and the predicted number generated by the proposed method. If the proposed method could correctly predict the number of objects within an image according to the designed complexity-aware object-encoder, then it will give more convincing evidence to support the contribution claimed by this paper. In addition, since the previous methods have to pre-define the number of slots, a plot built on the evaluation metric vs the number of slots will better demonstrate the comparison between these methods and the proposed one. In addition, such a plot could better show the stability of this work."
            },
            "questions": {
                "value": "1. Please give a better description of the slot attention and the whole framework of the proposed method.\n2. Please give more detail about the number of slots predicted by the proposed method and give a comparison between the ground truth and the predicted ones."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Nil"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1880/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1880/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1880/Reviewer_CDqw"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1880/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699232359861,
        "cdate": 1699232359861,
        "tmdate": 1699636118295,
        "mdate": 1699636118295,
        "license": "CC BY 4.0",
        "version": 2
    }
]