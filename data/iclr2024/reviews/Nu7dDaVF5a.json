[
    {
        "id": "DkvlMaHy4p",
        "forum": "Nu7dDaVF5a",
        "replyto": "Nu7dDaVF5a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission515/Reviewer_zLQa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission515/Reviewer_zLQa"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to learn a generalizable neural fields as a scene prior for 3D indoor scene reconstruction. The authors first sample a sparse point cloud from reprojected depth and use PointConv for extract per-point geometry feature which are then interpolated to a feature vector for any input point. Similarly, they use a CNN to extract texture features from the RGB image and splat the feature to the sampled 3D points. An MLP decoder predict the SDF and view dependent appearance from the point features. With the predicted SDF and appearance, they use volume rendering to render a depth/color for any input rays. The encoder and decoder are pretrained in a large scale dataset. After training, the encoder and decoder predict the SDF and appearance for new scenes without per-scene optimization. Of course, per-scene optimization improve the results as shown in the experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method can do direct fusion by simply concatenate sampled point cloud from each input frame without additional fusion module thanks to the sparse point cloud representation of the scene.\n2. The experimental results are extensive and show that the proposed method works well on both sparse input and dense input with faster convergence speed."
            },
            "weaknesses": {
                "value": "1. The model is not scale/rotation/translation invariant. I think the main reason is the use of point position as input to the decoder, which means if the coordinates system is changed, the output of the decoder is also changed. Similarly, the surface normal or view directions should also be in the local coordinates system, otherwise the output will change if the scene was translated. I wonder how sensitive of the current model to random rotation/translation. Would be great to have an ablation study. A simple solution is just don't use point position as input to the network. I am also interested in how this performs. \n\n2. Would be great to cite and discuss [1] as it's very related to the paper. The paper already included many baselines so this is not a minus point. \n\n[1] Fast Monocular Scene Reconstruction with Global-Sparse Local-Dense Grids"
            },
            "questions": {
                "value": "1. One very simple baseline is missing: simply use TSDF fusion with the GT depth map for geometry reconstruction.\n2. Why not also use CNN for the depth map and then use the similar method to get the geometry feature for the point cloud?\n3. In Table 3. most of the baseline are not using depth as input, might be good to make it clear. \n4. In the direct fusion part, points from different views are directly concatenated, will this change the distribution of point density and therefore change the K neighbouring points during inference? \n5. Typo in the first paragraph \"Although these results are encouraging, Although these results are encouraging\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission515/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission515/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission515/Reviewer_zLQa"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission515/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698683280999,
        "cdate": 1698683280999,
        "tmdate": 1699635978430,
        "mdate": 1699635978430,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Fc4m5yr00b",
        "forum": "Nu7dDaVF5a",
        "replyto": "Nu7dDaVF5a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission515/Reviewer_ZqzA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission515/Reviewer_ZqzA"
        ],
        "content": {
            "summary": {
                "value": "This works proposes a scene reconstruction and novel-view synthesis method by learning scene priors that leverage ground-truth RGB-D data. The proposed novel method allows to efficiently integrate features from multiplve views in order to obtain an implicit neural representation of the scene's geometry and texture. This scene representation can further be used to render images from novel viewpoints. Experimental results on ScanNetV2 show how it outperforms many state-of-the-art scene reconstruction methods while using fewer images and less computation time, the latter thanks to a accurate initial estimate of the scene representation before the optimization step. For novel-view synthesis evaluated on real-world scenes, results shows this method is comparable or better than a number of well-known methods in the literature such as NeRF and IBRNet."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Novelty and significance**\nThis work tackles two challenging tasks with one method: scene reconstruction and novel-view-synthesis. The authors propose a novel combination of techniques with clear advantage on some aspects compared to other state-of-the-art methods in each of the two tasks. It makes use of efficient representations (3D keepoints), and  is designed to work with any number of input views regardless of however many are used for training. It also leverages depth ground-truth very well in a two-stage pipeline. I believe the method itself is a solid contribution to the vision community.\n\n**Soundness of method**\n\nThe presented methods are generally sound and the benefits of its design are clear. In general it makes a number of useful/practical design choices based on the types of indoor scenes it's applied to.\n\n**Experimental results**\n\nExperimental results on a real-world dataset, ScanNetV2, demonstrate strong results for scene reconstructions as well as novel-view synthesis in complex scenes. It achieves SoTA reconstruction performance while using fewer input images than a number of related methods, and similarly its novel-view synthesis performance is also better than strong baselines.\nFinally, the ablation studies in the paper show that each of the learned priors is important to the overall performance."
            },
            "weaknesses": {
                "value": "**Presentation**\n\nThe paper needs to be substantially proof-read, as it is it's not ready for publication.\n\n\n**Author claims**\n\nThe authors make a number of claims, namely:\n- Per-scene optimization-based methods are \"not scalable, inefficient, and unable to yield good results given limited views\". I am not sure what they refer to by scalable, and also not sure evidence is presented to support the claim.\n- Learning-based multi-view stereo methods, \"their multi-view setting makes it less flexible to scale up and to broad applications\". Again, I'm not sure evidence is shown that these methods do not scale up and are less broadly applicably. Perhaps precise pointers to the results or references would help.\n\n**Experiments**\n- Lack of expeirments with sparse views. Given that the method can technically reconstruct a scene it would be interesting to test it on these settings. It would be interesting to see the performance curve as a function of input views.\n- Restricted to a single dataset (ScanNetV2). While it's certainly a good dataset to evaluate on, as it is based on real-world scenes, it would have been useful to see results on other recently used NVS datasets even if synthetic.\n- Novel view synthesis experiments are evaluated on a small number of scenes only, with potentially high variance of results."
            },
            "questions": {
                "value": "**A number of questions and suggestions:**\n\n- It's not clear what is the protocal for making a result bold in Table 1, making it diffucult to quickly see what performs best on each section of rows.\n\n- How exactly is importance sampling used? I couldn't find a technical explanation for how it's used. In particular, how is it used for novel-view synthesis tasks? Is it only used during training of the networks? In other words, for evaluation, I understand rendering of a novel view is done by uniform sampling of the ray?\n\n- The term generalizable is used extensively and I'm not sure what they refer to in many cases (e.g. generalizable features, generalizable representations and generalizable losses). I would kindly ask the authors to either reduce the use of the term or be a bit more precise when using it.\n\n**Some missing references.**\n\nOn learning based novel-view synthesis.\n- Trevithick, Alex, and Bo Yang. \"Grf: Learning a general radiance field for 3d representation and rendering.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n- Yu, Alex, et al. \"pixelnerf: Neural radiance fields from one or few images.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\nOn using GT depth for guiding training geometric and colour scene functions.\n- Stelzner, Karl, Kristian Kersting, and Adam R. Kosiorek. \"Decomposing 3d scenes into objects via unsupervised volume segmentation.\" arXiv preprint arXiv:2104.01148 (2021)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission515/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789878761,
        "cdate": 1698789878761,
        "tmdate": 1699635978362,
        "mdate": 1699635978362,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uMCKE0R7io",
        "forum": "Nu7dDaVF5a",
        "replyto": "Nu7dDaVF5a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission515/Reviewer_2i3p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission515/Reviewer_2i3p"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a generalizable 3D reconstruction framework from RGB-D sequences for indoor scenes. The key motivation is to design separate, progressive stages to learn the geometry field and color field. Experiments on various datasets have demonstrated the effectiveness of the design."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) Overall, the paper is well written and easy to follow.\n\n(2) The paper demonstrates comprehensive experiments and compares with different state-of-the-art (SOTA) methods, to highlight the advantage of the proposed method."
            },
            "weaknesses": {
                "value": "(1) To me, the novelty of this paper is limited. The key design of the geometry prior module is similar to PointNeRF (also employs a distance-wise feature aggregation from 3D point clouds). Also, learning a geometric prior (SDF) then pruning to facilitate the texture field is applied in previous neural 3D reconstruction methods such as NeRFusion and SparseNeuS. \n\n(2) The paper only conducts experiments on RGB-D sequences to demonstrate the generalizability. To me, the technical impact would be much higher if it also works well a RGB sequences, where obtaining precise geometry is challenging. For RGB-D sequences, some classic methods such as BundleFusion and COLMAP-MVS can achieve superior generalizability across different environments without any training.  \n\n(3) The advantage over Go-Surf is not convincing on both accuracy and speed."
            },
            "questions": {
                "value": "I would consider to improve my rating if the authors can address my concern especially on the novelty of this work presented in the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission515/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825536426,
        "cdate": 1698825536426,
        "tmdate": 1699635978285,
        "mdate": 1699635978285,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4mQOxz5eAg",
        "forum": "Nu7dDaVF5a",
        "replyto": "Nu7dDaVF5a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission515/Reviewer_EoMe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission515/Reviewer_EoMe"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses high-fidelity 3D reconstruction using cross-dataset (ie, -scene in particular) generalization using the popular conjunction of neural radiance fields and signed distance functions.\n\nThe proposed method shows favorable comparative results on Scannet, a well established public benchmark in the field against RGB and RGBD based baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ ## Readability.\nAs it currently stands, the paper is very well written. The main ideas and concepts are mostly well explained and articulated throuthout.\n\n+ ## Organization of the contents and overall paper structure.\nThe contents are also very well structured and balanced.\n\n+ ## Related work section and discussion. \nIt is very well structued, articulated and populated with very relevant and up to date references.\n\n+ ## The disclosed performance of the proposed method is at the very least competitive and promising.\n\n+ ## The problem at hand (scene level generalization) is an important, impactfull one in the field."
            },
            "weaknesses": {
                "value": "+  ## 1. Missing bits of context information - How much does it cost?\nWhile indicative timings and thorough implementation details (in supMat) are provided, information regarding the resource usage, model size and complexity are yet underdescribed.\n\nA comparative disclosure of such information covering the main experimental baselines that are considered would help the reader better assess its relative positioning throughout the typical criteria.\n\nMentioning where the computation bottlenecks lie in terms of components would also be valuable in order to fully assess the practical usefullness of the proposed sequential pipeline, beyond rough timings (eg, Fig 3).\n\n+  ## 2. Comparative evaluation - Baselines and Benchmarks.\n\nWhile very recent work (eg, CVPR 2023) have been included in the setup, eg, HelixSurf (Liang et al.), there are a few missing players that currently hine by their absence.\n\nFor example, the RGB based references:\n\n-- Li, Z., M\u00fcller, T., Evans, A., Taylor, R. H., Unberath, M., Liu, M. Y., & Lin, C. H. (2023). Neuralangelo: High-Fidelity Neural Surface Reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 8456-8465).\n\n-- Darmon, F., Bascle, B., Devaux, J. C., Monasse, P., & Aubry, M. (2022). Improving neural implicit surfaces geometry with patch warping. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 6260-6269).\n\n-- Zhang, J., Yao, Y., Li, S., Fang, T., McKinnon, D., Tsin, Y., & Quan, L. (2022). Critical regularizations for neural surface reconstruction in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 6270-6279).\n\n-- Wang, Y., Skorokhodov, I., & Wonka, P. (2022). Hf-neus: Improved surface reconstruction using high-frequency details. Advances in Neural Information Processing Systems, 35, 1966-1978.\n\nSimilarly, the DTU public benchmark could have been envisionend, albeit partially just to compare other key papers from the state-of-the-art without having to re-run their public implementations. \n\nThe same goes for the Tanks and Temples public benchmark as well. At least one additional benchmark would have been a reasonable addition.\n\nThis would help better - and more thoroughly - assess the relative positioning of the proposed contribution, performance-wise.\n\n+  ## 3. The aforementioned references also lack in qualitative discussion and Related Work.\n\nThis is a direct consequence of (2) above."
            },
            "questions": {
                "value": "The main questions I would have cover the aforementioned weaknesses that have been pinpointed. \n\nBesides those remaining grey areas, I would be happy to bump my initial rating were they to be addressed accordingly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission515/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission515/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission515/Reviewer_EoMe"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission515/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698856471193,
        "cdate": 1698856471193,
        "tmdate": 1700953478979,
        "mdate": 1700953478979,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KhBmvFycu4",
        "forum": "Nu7dDaVF5a",
        "replyto": "Nu7dDaVF5a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission515/Reviewer_s8AK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission515/Reviewer_s8AK"
        ],
        "content": {
            "summary": {
                "value": "Input: One or more RGB-D images of an indoor scene\n\nOutput: Textured 3D mesh representing the indoor scene\n\nThe paper presents a generalizable neural framework, called Neural Field Priors (NFPs), for reconstructing 3D indoor scenes from a single as well as multiple RGB-D input images of the scene. Scene priors are obtained from depth map inputs, given posed RGB-D images. Results show significant performance improvement, especially on single-view 3D scene reconstruction, both in terms of speed and reconstruction quality.\n\nThe main contributions of the paper are two folds: (a) developing a two-stage generalizable neural framework using scene priors (i.e., not restricted to per-scene training) that is scalable to large-scale scenes, and (b) reconstructing the 3D scene by merging multiple view images (it is the features that are actually merged) in the volumetric space without using a fusion module. \n\nThe two-stage framework consists of a generalizable geometric prior and a generalizable texture prior. \nThe first network, which is the Geometric Prior network, is responsible for obtaining a signed distance field of the underlying scene. This is done by what are called Geometry Objectives and Surface Regularization. Geometry Objectives are based on the depth values. First, a pixel-wise rendering loss on depth maps is enforced to make the depth predictions at points sampled along a ray as close to the GT as possible (Importance sampling is used). The features of the points at the sampled locations are obtained using a modified form of weighted interpolation of surface-point geometric features. The surface points are nothing but the projection of depth image to 3D, and their features, a.k.a geometric features, are obtained by PointConv network. These interpolated point features, along with the point locations and their positional encoding, are passed through an MLP (called the Geometric decoder) to obtain signed distance values at the respective points. The signed distance value at a point is approximated to the GT SDF value by comparing the predicted SDF value with the difference of GT depth and predicted depth at that point. Surface Regularization is used for regularizing the SDF predictions to avoid artifacts. This is the Eikonal loss, which is a standard regularization term used in prior volume-rendering-based 3D reconstruction works.\n\nThe second network, which is the Texture Prior network, uses the SDF predictions from the first network as geometric initialization. The goal here is to learn RGB values for sampled points along the ray for which SDF values have been predicted by the Geometric Prior network. The texture features are a modified version of weighted interpolated surface-point texture features. The texture features here refer to convolutional features (image pixels to 3D correspondence yields surface-point convolutional features). These interpolated texture features for the points, along with the point locations and their positional encoding, are passed through an MLP (called the Texture decoder) to estimate the color at the respective points. During this process, the Geometric Decoder along with the PointConv encoder is jointly learned. So the loss for the Texture Prior network is the Geometric Prior loss plus the RGB loss.\n\nWhen multi-view images are used as input, Geometric and Conv features of these images are merged during the reconstruction process. This avoids the burden of learning fusion modules to fuse reconstruction results from multiple views, thereby making the training efficient (less training complexity). As well, it is claimed that it allows for flexible data processing (I have a few questions on this in the Questions section).\n\nDataset used:\nScanNet_v2 and 10 synthetic scenes from Azinovic et al. 2022\n\nUnderlying Neural Network:\nGeometry encoder: PointConv\nImage Encoder: U-Net\nDecoders are MLPs\nVolume rendering is what makes this possible\n\nLoss function: \nL_depth (L1 loss), L_sdf (L1 loss), L_surface (Eikonal loss), L_rgb (L2 loss)\n\nQuantitative Metric:\n3D scene mesh reconstruction \u2013 Accuracy, Completeness, Precision, Recall, F-score\nNovel view synthesis \u2013 PSNR (Power Signal-to-Noise ratio), SSIM (Structural Similarity for Image comparison metric) and LPIPS (Learned Perceptual Image Patch Similarity metric)"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1)\tGeneralized framework for scene reconstruction using radiance fields (i.e., no per=scene training) from relatively few input views (relative to existing literature)\n\n2)\tAbility to reconstruct a 3D scene by merging individual frames in the volumetric space without a learnable fusion module\n\n3)\tNovel view synthesis from single-view input beats existing works\n\n4)\tSimple interpolation strategy for obtaining point features. Making use of surface points instead of dense volumetric grids for obtaining sampled point features\n\n5)\tThe paper is well-written"
            },
            "weaknesses": {
                "value": "1)\tThe dependency on depth maps is limiting since such data is not always available. This is also a drawback of MonoSDF and other works that use additional priors for scene reconstruction, beyond just RGB images\n2)\tThe intuition behind the approximation of GT SDF values by observing depth values along a ray is unclear. This may result in erroneous signed distance predictions. An explanation of this is lacking. I am actually interested in this ablation experiment\n3)\tResults on images in the wild are missing. This will add value to the work\n4)\tLimited quantitative results\n5)\tAblation experiments are not thorough, in terms of the different components involved in Geometric and Texture Prior Networks\n6)\tDiscussion on limitations is lacking"
            },
            "questions": {
                "value": "1)\tIt is claimed in the second+third paragraph of the Introduction that not having a fusion module to handle multi-view images during training allows for flexible data processing. This is not substantiated and remains unclear to me. Can you elaborate how this is the case?\n2)\tIs there a reason behind using PointConv as geometric feature embedding network? Were alternative networks (like PointNet, DGCNN etc.) tried? Using different networks should have a bearing on the overall result. Were any experiments conducted to understand this? \n3) This has been touched upon but I would like to ask again -- what makes the proposed approach work on single images? To what extent are the reconstructed scenes reasonable? Or put differently, what \"gaps\" needs to be \"filled in\" to make the reconstruction results (from single input image) better than what can be currently achieved?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission515/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission515/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission515/Reviewer_s8AK"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission515/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699253522790,
        "cdate": 1699253522790,
        "tmdate": 1699646999274,
        "mdate": 1699646999274,
        "license": "CC BY 4.0",
        "version": 2
    }
]