[
    {
        "id": "DBqqjtyL9a",
        "forum": "ZlEtXIxl3q",
        "replyto": "ZlEtXIxl3q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission262/Reviewer_68Sm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission262/Reviewer_68Sm"
        ],
        "content": {
            "summary": {
                "value": "A global epistasis model for protein fitness assumes that the observed experimental fitness is g(f(x)), where x is the protein sequence, f is a simple (perhaps additive) function of the sequence, and g is a scalar -> scalar nonlinear function that reflects the non-linearity of the measurement process. The goal is to identify the latent function f(). They explore the use of learning-to-rank losses when fitting this model, as these losses are invariant to any monotone g(). They draw on some results from the compressed sensing literature, some experiments on synthetic data, and some experiments on a standard protein fitness prediction benchmark to argue that fitting models with a loss based on the Bradley-Terry ranking model is better than using mean squared error."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I liked section 2.1. I think that the 'epistatic domain' is a nice formalism for reasoning about a particular kind of sparsity of the fitness landscape, and I'll use this in the future.\nI appreciate that the paper calls more attention to global epistasis. It's an elegant idea that is often overlooked."
            },
            "weaknesses": {
                "value": "All of the experiments evaluate fitness prediction in terms of spearman correlation. It's not surprising that training using a BT loss improves spearman correlation vs. MSE, since the BT loss can be seen as a differentiable relaxation of the negative spearman correlation. Therefore, I didn't find this result particularly exciting. It is a common observation in machine learning that a particular downstream metric can be improved by using a loss function that approximates it, and the learning-to-rank framework described in this paper is well established.\n\nSimilarly, I don't agree with the assertion that spearman correlation is a good surrogate metric for a model's usefulness for protein engineering. Suppose that fitness values appear in the range [0, 10] and that the best fitness seen so far in a protein engineering project is 6. Since we want to use this model to design new proteins, we don't care if it predicts 3 for a protein that actually has fitness 2. On the other hand, predicting 5.5 for a protein that has true fitness 6.5 could lead to a missed opportunity for ML-guided design. Good eval metrics should think asymmetrically about precision vs. recall of finding proteins with good fitness, etc. Finally, spearman correlation is confusing because it's unclear what the optimal value is. Given the noise level of the assay, I know the optimal MSE. What's the optimal spearman correlation, though?\n\nThe paper's goal is to fit models on experimental data to estimate a latent fitness function. However, the exposition assumes that these observations are noiseless, which is quite unrealistic. Further, in my experience noise is often heteroskedastic, where the noise level can be assumed to depend on g(f()). For example, the read-out from many assays is baesed on counts from DNA sequencing, and these are subject to Poisson noise. The BT loss strikes me as being not particularly robust to noise, since it could easily flip the binary fitness(A) > fitness(B) label if the noise is large relative to the difference between the fitness of A and B. \n\nSection 3.2  on the 'fitness-epistasis uncertainty principle' is grounded in some results from the compressed sensing literature. It's interesting background on CS, and I enjoyed reading it, but I found the conclusions from the section too informal to be useful.\n\nI found that the paper wasn't sufficiently grounded in standard statistical terminology about model estimation. Can you please discuss the identifiability of the global epistasis model? Does the change of loss function change identifiability? Also, you claim that the BT loss will result in better sample complexity. Maximum likelihood estimation has optimal sample complexity for parameter identification. Can you discuss the suitability of the BT loss in terms of whether the BT model reflects the observation process for the data? Is the goal to identify the latent function f()? What does that mean when f() is only identifiable up to a monotonic transformation (since the inverse transformation could be absorbed into g)?\n\nIn my prior reading on global epistasis, I hadn't seen the assumption that g() was monotonic. In many situtations, this isn't the case. For example, for many genes in the body it is hazardous if too much or too little of it is expressed, so g() is an upside-down U. In my experience with enzyme engineering, if an enzyme is too active it may kill the host cell used to express it. It seems that your framework does not generalize to this case."
            },
            "questions": {
                "value": "Can you please respond to my 'weaknesses' section?\n\nHow would your theoretical results or results on N-K landscapes change if the observed fitness values had noise added?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission262/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697655192442,
        "cdate": 1697655192442,
        "tmdate": 1699635951863,
        "mdate": 1699635951863,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JJmIDjiqw9",
        "forum": "ZlEtXIxl3q",
        "replyto": "ZlEtXIxl3q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission262/Reviewer_7udB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission262/Reviewer_7udB"
        ],
        "content": {
            "summary": {
                "value": "In this study, optimization of Bradley-Terry (BT) contrastive loss is proposed to recover the latent fitness function corrupted by the effect of global epistasis. The proposed approach relies on the monotonic property of the global epistatic effect and does not require any assumption on the exact model of global epistasis. With simulated data, it is shown that BT loss achieves better estimation of fitness function than MSE loss, and it is more robust to the extent of corruption, measured by the entropy of epistatic representation, caused by the global epistasis. The advantage of using BT loss is also demonstrated for the protein fitness prediction on different splits of GB1, AAV, and thermostability datasets designed by the FLIP benchmark."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1)\tDemonstrating the benefits of a ranking based loss for fitness prediction\n2)\tNice experiments were designed to show the advantage of the contrastive loss in recovering the latent fitness model corrupted by global epistasis."
            },
            "weaknesses": {
                "value": "See questions below"
            },
            "questions": {
                "value": "1)\tWhat statistical test was used to measure the significance of improvements in Figure 3? I am asking this because in some splits the performance of BT loss is almost the same as MSE loss, but the reported p-value is significant (examples: samples and 7-vs-rest in AAV).\n2)\tIn the splits where BT loss provides better performance (Figure 3), it is hypothesized that **it could be partially due to the corruption of fitness function with global epistasis**. I am curious to know how this can be proved.\n3)\tIn Figure 1, are the coefficients in the epistatic domain normalized? The scale of $\\hat{f}$ does not match f, which is expected.\n4)\tHave you also tested BT loss on fitness prediction for other datasets such as the ones compiled by the DeepSequence paper (https://www.nature.com/articles/s41592-018-0138-4)? I understand the use of FLIP datasets for the task of benchmark, however I am not sure how challenging FLIP splits are compared to other datasets out there.\n5)\tWhere do you expect ranking-based losses not to perform as good as MSE losses for protein fitness prediction?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission262/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission262/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission262/Reviewer_7udB"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission262/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828514107,
        "cdate": 1698828514107,
        "tmdate": 1699635951794,
        "mdate": 1699635951794,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7DlO7510he",
        "forum": "ZlEtXIxl3q",
        "replyto": "ZlEtXIxl3q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission262/Reviewer_uFyB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission262/Reviewer_uFyB"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors focus on the problem of inferring fitness functions from experimental data, a relevant problem for protein engineering. To this end, the authors propose utilizing contrastive losses, such as the Bradley-Terry loss, to extract the underlying latent function from a global epistasis model. Furthermore, the authors argue that the choice of a contrastive loss may have other advantages of Mean Squared Error, especially for estimating ranking functions. They evaluate their approach on the FLIP dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Contrastive learning has demonstrated promise in the field of computer vision; this paper shows a novel application of this concept to the field of Biology.\n\n2. The paper has a strong theoretical foundation and is well presented. \n\n3. Quantitative results are convincing and promising."
            },
            "weaknesses": {
                "value": "1. The empirical evaluation has focused only on a single benchmark, FLIP. It would strengthen the paper if the approach was validated with even more datasets."
            },
            "questions": {
                "value": "1. Is there any way to extend the empirical evaluation beyond the FLIP benchmark?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission262/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699546123957,
        "cdate": 1699546123957,
        "tmdate": 1699635951704,
        "mdate": 1699635951704,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OvxxkoyJ7v",
        "forum": "ZlEtXIxl3q",
        "replyto": "ZlEtXIxl3q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission262/Reviewer_6v5p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission262/Reviewer_6v5p"
        ],
        "content": {
            "summary": {
                "value": "This study explores the estimation of fitness functions in protein engineering, which are complex mappings from biological sequences to properties of interest. The authors focus on global epistasis models that use a sparse latent fitness function transformed by a monotonic nonlinearity to predict measurable fitness.\n\nContribution: In this supervised learning setting, the authors introduce a rank-based contrastive loss approach, and show that it yields better results than an MSE loss-based approach (especially for small datasets), using both simulations and a benchmark dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Innovative use of supervised contrastive learning for fitness prediction.\n- Empirical validation of proposed methods using simulations."
            },
            "weaknesses": {
                "value": "- presentation could be improved\n- More results are needed in order to delineate the regimes where the presented results hold true\n- Absence of simple baselines for comparative analysis"
            },
            "questions": {
                "value": "1) Clarity on \"Corrupting Data\": While recognising that the term \"corrupting data\" might be specific jargon within the authors' field, I find its usage potentially confusing. It typically suggests that data has been made less accurate. In contrast, from a machine learning standpoint, the issues you're addressing appear to be related to the complexity introduced by non-linear relationships, which need complex models that may overfit, particularly when models are trained to predict the exact observed values (y).\n\n2) It would be highly beneficial to demonstrate the specific regimes in which your observations about the Bradley-Terry loss are valid. Specifically, it's important to determine whether the improvements attributed to the BT loss over the MSE loss are unique to the complex models with epistatic interactions, or if similar benefits could be observed with simpler models, such as a linear model subjected to the same monotonic warping (i.e., the nonlinearity introduced by global epistasis). To address this, I recommend varying the degree of interaction order in your simulations.\n\n3) I would recommend including a comparison with a more straightforward baseline to further validate the proposed approach. Specifically, it would be informative to see how a simple quantile transformation of the outcome data (to uniform or Gaussian distributions) performs in conjunction with Mean Squared Error (MSE) loss. This could serve as a more direct way to deal with the non-linear transformations introduced by global epistasis and might provide a competitive baseline to the BT loss approach.\n\n4) Minor: To avoid confusion with unsupervised contrastive learning methods, it would be beneficial to explicitly state that the contrastive learning approach employed is supervised. (e.g., you could say \"supervised contrastive learning loss\")\n\nIf the authors can address these concerns and provide clarifications, I would be open to revisiting and potentially improving my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission262/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699576283003,
        "cdate": 1699576283003,
        "tmdate": 1699635951638,
        "mdate": 1699635951638,
        "license": "CC BY 4.0",
        "version": 2
    }
]