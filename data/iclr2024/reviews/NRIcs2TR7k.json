[
    {
        "id": "2ozIf6Sf0a",
        "forum": "NRIcs2TR7k",
        "replyto": "NRIcs2TR7k",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3649/Reviewer_GFCr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3649/Reviewer_GFCr"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a training-efficient and paired-data-free method to flexibly learn unified contrastive representation space for more than three modalities by integrating the knowledge of existing MCR spaces."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The paper formulation is good and clear.  \n\n(2) The proposed method can align multiple existing MCRs into the same based MCR, which can effectively preserve the original semantic \nalignment of the based MCR."
            },
            "weaknesses": {
                "value": "(1) The reviewer finds the presentation of figures confusing. For instance, in Table 1, superior results are subtly highlighted in grey to minimize attention, while the results of the proposed model are emphasized in bold to draw more focus.\n\n(2) In Table 3, the value 11.19 is highlighted in bold instead of 11.30.\n\n(3) Considering the issues raised earlier, the reviewer remains uncertain about the authenticity and reproducibility of the results.\n\n(4) Apart from presenting numerical data, what are the additional findings and conclusions drawn from the ablation studies?"
            },
            "questions": {
                "value": "Please see the comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698442828343,
        "cdate": 1698442828343,
        "tmdate": 1699636321234,
        "mdate": 1699636321234,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3lkz4DtgdO",
        "forum": "NRIcs2TR7k",
        "replyto": "NRIcs2TR7k",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3649/Reviewer_CZb4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3649/Reviewer_CZb4"
        ],
        "content": {
            "summary": {
                "value": "this paper introduces Ex-MCR, a method for learning unified contrastive representations across more than three modalities, focusing on being training-efficient and not requiring paired data"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "the idea of extending multi-modal contrastive representation (Ex-MCR) to incorporate more than three modalities without relying on paired data is an interesting and important topic given there are appearing more multimodal foundation models, but each of them only covers a fraction of the modalities.\nThe proposed approach is technically sound."
            },
            "weaknesses": {
                "value": "one of the concerns here is the baselines and the results, even the methods like imagebind use paired data, the reviewer still thinks that it's worthwhile to study what will be the performance differences, as it could indicate the limitation/boundary of this kind of paired-data-free methods."
            },
            "questions": {
                "value": "minor issues:\ntypos in the first summarized contribution in the introduction.\nfigure 1 seems to lack some clarity, and the caption is not very helpful for understanding this figure, consider improving this part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822351665,
        "cdate": 1698822351665,
        "tmdate": 1699636321119,
        "mdate": 1699636321119,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mokZa5V3I1",
        "forum": "NRIcs2TR7k",
        "replyto": "NRIcs2TR7k",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3649/Reviewer_6GyA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3649/Reviewer_6GyA"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel method, Extending Multimodal Contrastive Representation (Ex-MCR), to address challenges in multi-modal learning for more than three modalities. Traditional methods are constrained by the need for large-scale, high-quality paired data and high training costs. Ex-MCR offers a training-efficient solution that doesn't rely on paired data, by aligning multiple existing MCRs into a base MCR, preserving their original semantic alignment. The method enhances the alignment process through various techniques, including modality-centric pseudo data pairs and a decoupled projector. Experiments demonstrate its state-of-the-art performance on various tasks, showcasing its potential in representation learning and modality extensibility."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper introduces Ex-MCR, which extends one MCR space (leaf-MCR) into another fixed MCR space (base-MCR). This approach optimizes the preservation of modality alignment within the base MCR, showcasing its potential for integrating multiple MCRs.\n2. This method operates without the need for paired data, demonstrating its potential for scalability.\n3. This method employs a dense contrastive loss on pseudo-pairs between all possible modalities, enhancing the learned alignment's stability.\n4. Ex-MCR achieves competitive performance across various zero-shot tasks, even better than strong baselines trained by paired data."
            },
            "weaknesses": {
                "value": "1. While Ex-MCR outperforms baseline methods, its performance lags behind that of the leaf-MCR.\n2. The authors highlight the scalability of Ex-MCR; however, it's contingent upon the new MCR having a modality already present in the base MCR. Otherwise, biases or errors may be exacerbated."
            },
            "questions": {
                "value": "1. How do you explain the performance gap between Ex-MCR and the original leaf-MCR? Are there specific challenges or limitations inherent to Ex-MCR that contribute to this disparity?\n2. Given the premise that a new MCR should have a modality already present in the base MCR for effective expansion, how does Ex-MCR handle scenarios where entirely new modalities (or only modalities in leaf-MCR) need to be integrated?\n3. Have you experimented with incorporating paired data into the training process for Ex-MCR? If so, how did it impact the results?\n4. If the configuration were altered such that CLAP served as the base-MCR and both CLIP and ULIP were treated as leaf-MCRs, would the performance outcomes remain favorable?\n5. How many resources do you use for the training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839436152,
        "cdate": 1698839436152,
        "tmdate": 1699636321049,
        "mdate": 1699636321049,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "R23zRVzNdH",
        "forum": "NRIcs2TR7k",
        "replyto": "NRIcs2TR7k",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3649/Reviewer_pnPG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3649/Reviewer_pnPG"
        ],
        "content": {
            "summary": {
                "value": "The work proposes an effective and efficient recipe to integrate multiple Multi-modal Contrastive Representation (MCR) spaces into one, making it possible to have a 3D-Text-Vision-Audio model without requiring any additional paired data. This is achieved by leveraging the modality that is shared across the multiple spaces, learning to project each source space (called leaf-MCR) to a chosen target space (base-MCR) by using a InfoNCE loss over pseudo-pairs containing the different modality combinations. To further improve the alignment, the intra-MCR modality gap is closed with an additional regularizing term. The experiments show improved results when considering the union of more than 3 modalities (3D, Text, Vision, Audio), evaluating the performance on cross-modal retrieval tasks over all the possible combinations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I find the paper to offer a significant contribution in multi-modal learning, proposing an efficient way to achieve a multi-modal model on more than 3 modalities without the need for a multi-modal paired dataset annotated for all the considered modalities. As a possible solution to overcome the need for large-scale multi-modal paired datasets, the approach is advisable and useful for the community. In the current AI environment, it is good to see that there is still a strive to push for more accessible approaches that don\u2019t require extensive resources. \n\n### Novelty\n\n- The work goes to build upon C-MCR, inheriting its benefits and effectively overcoming its limitations, in practice extending its applicability to multiple modalities altogether.\n\n### Quality\n\n- The methods employed are simple yet effective, and the overall framework is rather lightweight, making it directly usable by any practitioner desiring cheap multi-modality;\n- The claims are backed by proper experimental evidence.\n\n### Significance\n\n- The experiments over all the combinations of cross-modal tasks involving two of the fourmodalities considered are promising, and it\u2019s impressive that these are obtained without the need for an expensive paired dataset covering all the involved modalities;\n- Source code is provided for reproduction purposes."
            },
            "weaknesses": {
                "value": "### \n\n- ULIP is already trained to be aligned with CLIP in the original paper. This work instead states it as a contribution; \n    - while it may be the case that the framework improved this alignment, I find it should be stated clearly that this was indeed the case\n- Clarity could be greatly improved, the amount and repetition of acronyms makes the overall paper hard to read and hardly a pleasing experience;\n- After severeal reads, I still found it hard to decipher the \u201cmodality-centric consistency\u201d part.\n    - Being a core component of the framework, I find that the paper really needs to state it more clearly for the reader to understand;\n- Even key messages in the introduction are somewhat vague and not immediately clear; e.g.\n    - in \u201c*from  the  training  data  perspective,  we  extract  various  modality-centric pseudo  data  pairs,  aiming  to  alleviate  the  semantic  bias  of  pseudo  pairs  in  Wang  et  al.  (2023b) and reflect MCR space more comprehensively.*\u201d what does it mean for a data pair to be modality-centric? what semantic bias is the work referring to? what does it mean to reflect a MCR space comprehensively?\n    - the whole period \u201c*C-MCR employs data from overlapping modalities to aggregate semantic consistent embedding of non-overlapping modalities, thereby creating pseudo-pairs*\u201d is not clear and should be rephrased for clarity.\n    - These are only two examples, but this kind of ambiguous phrasing is very frequent in the text, unfortunately hindering my understanding of the contributions;\n- The notation is not always clear, and sometimes the details are missing to fully comprehend the equations, such as the normalization factor of the softmax and the tilde symbols in (1) and following equations .\n- Table 3, 4, 5 and 6 don\u2019t report what metric is actually being shown.\n- I can\u2019t decipher what\u2019s going on in Figure 1, especially in the left part. Overall the figure seems quite crowded, it probably would help splitting it in two figures. The caption doesn\u2019t seem to help.\n\nOverall, clarity is the principal reason I am inclined to reject as it prevents me from fully understanding the proposed method and contributions, therefore hindering my capability to assess its merits and drawbacks. Moreover, I find the work to tackle an interesting and impactful topic, and I would like it to be accessible to a broad audience; I find the current writing to pose a significant hurdle in this regard, and therefore hope it can be improved. I am more than willing to increase my score if this is properly addressed."
            },
            "questions": {
                "value": "### Questions\n- What\u2019s the denominator in the softmax computation in (1)? is it over all the samples in the batch?\n- I don\u2019t understand what the tilde means in Eq (1) and the following ones, how does $\\tilde{t}$ differ from $t$?\n- in 3.2.1., it mentions three kinds of semantically consistent embeddings (audio-centric, text-centric, image-centric) (here the e.g. should actually be a i.e.) but then four objects are given, $\\{\\tilde{a}_i^A, \\tilde{t}_i^A, \\tilde{t}_i^I, \\tilde{v}_i^I\\}$. What\u2019s the explanation here?\n### Typos\n- 3.2.1. header: centirc \u2014> centric\n- The text sometimes refers to base MCR as based MCR"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3649/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3649/Reviewer_pnPG",
                    "ICLR.cc/2024/Conference/Submission3649/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698841155106,
        "cdate": 1698841155106,
        "tmdate": 1700724225810,
        "mdate": 1700724225810,
        "license": "CC BY 4.0",
        "version": 2
    }
]