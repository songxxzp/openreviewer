[
    {
        "id": "ZTGS1ZZv1U",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8488/Reviewer_iNUy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8488/Reviewer_iNUy"
        ],
        "forum": "4eJDMjYZZG",
        "replyto": "4eJDMjYZZG",
        "content": {
            "summary": {
                "value": "This work mainly studies why direct preference optimization (DPO) can be used to train a generator to evade detection. Following DPO, two samples are generated, and the preference is determined by the humanness score outputed by a given detector. \n\nIn experiments, a range of detectors are optimized against, including classifiers and metric-based detectors such as DetectGPT. Empirically, AUROC metrics is reduced to below 0.5 against several strong public and commercial detectors. Also interestingly, attack against one detector could generalize to other detectors."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The attacks for detectors is a very relevant research question especially in the era of LLMs.\n\nWithin the scope of detection, using techniques from RL (DPO) is quite novel.\n\nThe attack result is quite strong, and quite concerning. Since it does not require white-box access to the detector."
            },
            "weaknesses": {
                "value": "From a ML perspective, this paper does not propose a completely novel algorithm. Therefore my rating will be higher if this is a NLP conference.\n\nFrom an adversarial attack perspective, the result is not very surprising. \n\nI think the author should not only report PPL, but also the diversity of the generated texts.\n\nHow to defense against such attack is not explored."
            },
            "questions": {
                "value": "You should put comma or period after the equations. They are also part of a sentence.\n\nA lot of paragraphs are very long. Please break up at some points."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8488/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697343266491,
        "cdate": 1697343266491,
        "tmdate": 1699637060086,
        "mdate": 1699637060086,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "t9Zp7XtHV3",
        "forum": "4eJDMjYZZG",
        "replyto": "4eJDMjYZZG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8488/Reviewer_oZ9u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8488/Reviewer_oZ9u"
        ],
        "content": {
            "summary": {
                "value": "The paper delves into the rising interest in identifying text generated by large language models (LLMs). Although detection systems have been implemented in various sectors, notably education, their vulnerability has been a significant concern. The authors present a data-efficient method that fine-tunes LLMs to deceive these detectors by employing the latest advancements in reinforcement learning for language models. They use the 'human-ness' score of several detectors as a reward function and set a constraint to ensure the modified model remains close to the original. Through this method, the effectiveness of the OpenAI RoBERTa-Large detector is notably reduced. The findings suggest that this enhanced 'detector evasion' can generalize to other detectors not part of the initial training. Consequently, the authors caution against depending on detectors for LLM-generated text."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-written and clearly presented; \n- The paper tackles a critical and timely topic concerning the detection of LLM-generated text and proposed a novel data-efficient RL-based attack to deceive existing detector; \n- The paper shows empirical evidence of the fragility of current LLM-based detectors, offering actionable insights for future research and development of LLM detectors, the experiments are based on three runs which show the robustness of the proposed methods, abolition regarding the sample efficiency has also been provides to show the design choices;"
            },
            "weaknesses": {
                "value": "- The scalability of the proposed methods could be good to include to show whether the evasion of detector only happens when the model size of small like 7B in the most of the experiments; \n- Besides the perplexity, it will be good to include some evaluations on popular benchmark to assess the post-evasion model performance (whether the improved evasion is under the sacrifice of the general performance of the attack model);"
            },
            "questions": {
                "value": "- Could the authors list the training preference data and evaluation data in detail to understand whether there is a generalization due to the data in the experiments; \n- Could author offer more explanation towards the generalization of cross-detectors and the mixture of the effectiveness from different sources in Table 1 and 2, as well as how the attack performance correlates with the generalization;"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8488/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797001500,
        "cdate": 1698797001500,
        "tmdate": 1699637059939,
        "mdate": 1699637059939,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "14pukrqIN8",
        "forum": "4eJDMjYZZG",
        "replyto": "4eJDMjYZZG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8488/Reviewer_bGzn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8488/Reviewer_bGzn"
        ],
        "content": {
            "summary": {
                "value": "The paper explores the feasibility of optimizing language models to evade language model detectors. The authors propose a data-efficient attack using reinforcement learning to fine-tune language models and confuse existing detectors. They demonstrate the effectiveness of this approach by reducing the AUROC of the OpenAI RoBERTa-Large detector from 0.84 to 0.62 in a 7B parameter Llama-2 model. The results show that it is relatively easy and cheap to train language models to be less detectable, and the evasion generalizes to other detectors not used during training."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper introduces a new method for optimizing language models to evade detectors using reinforcement learning. The use of direct preference optimization (DPO) and the KL-divergence constraint provides a simple and stable training procedure.\n\n- The authors conduct a comprehensive set of experiments to evaluate the effectiveness of the proposed approach. They consider both open-source and commercial detectors, and demonstrate the generalization of evasion across detectors.\n\n- The results of the study have important implications for the reliability of machine-generated text detectors. The findings suggest that current detectors are not robust and can be easily evaded, which raises concerns about the widespread use of language models."
            },
            "weaknesses": {
                "value": "- The paper focuses primarily on empirical evaluations and does not provide a theoretical analysis of the proposed approach. A deeper understanding of the underlying principles and limitations of the method would enhance the contribution of the paper.\n\n- The paper does not extensively discuss potential countermeasures that could be employed to improve the robustness of language model detectors. It would be valuable to explore possible strategies for detecting and mitigating evasion attacks.\n\n- The paper does not compare the proposed approach with existing evasion techniques. It would be beneficial to evaluate the performance of the proposed method against other state-of-the-art methods for evading language model detectors."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8488/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699014803105,
        "cdate": 1699014803105,
        "tmdate": 1699637059810,
        "mdate": 1699637059810,
        "license": "CC BY 4.0",
        "version": 2
    }
]