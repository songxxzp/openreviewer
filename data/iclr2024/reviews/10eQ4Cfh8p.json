[
    {
        "id": "wckxlkPL42",
        "forum": "10eQ4Cfh8p",
        "replyto": "10eQ4Cfh8p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9463/Reviewer_kC5r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9463/Reviewer_kC5r"
        ],
        "content": {
            "summary": {
                "value": "The author proposed a deep reinforcement learning model to address the FJSP problem. This approach involves the simultaneous application of construction heuristics and improvement heuristics, enabling it to achieve better performance in shorter time on several public datasets."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Based on the claim of paper, it seems good to use construction heuristic to construct a better partial solution and use improvement heuristic to improve the partial solution."
            },
            "weaknesses": {
                "value": "- Actions (in Section 3.1) are critical, but not defined clearly. I have no problems with actions for construction heuristics, but actions for improvement heuristics are not well defined. In Section 3.2 \u201cInsertion Position Embedding\u201d (P5), the definition of insertion position is undefined clearly, and why the number of choices is (n+m) for each operation. Besides, it is also unclear about why the total number of insertion positions is equal to n\u00d7(n+m). For these unclear descriptions, there is no clue to understand the proposed method. Note that in Section 3.2 \u201cPolicy Model\u201d (P6), there is no way to understand the description \u201cObviously, there are at most m different insertion schemes for each improvement decision.\u201d\n\n- Figure 2 is confusing and unconvincing. For example, why is 31 moved to the position after 11, not before 11? If it can also be moved to that before 11, I don\u2019t see the strategy.\n\n- The representation of operations is inconsistent and thus makes it hard to understand how the Insert Position Embedding works (There are $O_{ij}, O_j, O_{j(i)}, O_i$ in the article).\n\n- Lack of test results for public benchmark dataset. With comparison to [29], you should also compare with la(edata) and la(rdata). And you may test on the dataset which is referenced by [29] to improve the reliability of your method.\n\nPresentation comments: \n- Lack of spaces in many places. E.g., \u201cBoth the generative model and the improvement model will use formula(4) to select the action to be executed in the current state st at step t on their respective feasible action sets.The advantage value function is fitted by a parametric MLP\u201d \n\n- In section 5.1, \u201c In addition, we also used (ref1),(ref1),\u201d, and \u201cmk [? ]\u201d in Table2. Please carefully check the content.\n\n- In Algorithm1, \u201cE%K\u201d => \u201ce%K\u201d, the second \u201c$EP_I$\u201d => \u201c$EP_G$\u201d, etc. There should be more that you need to find out for fixing. \n- There is no data in some places in the tables, such as 40x10 for OR-Tools in Table1 and mk[?] for UB* in Table2."
            },
            "questions": {
                "value": "- I am still wondering about your method for the Machine Process Queue Embedding:\nIs $M_{ij} = 1$ if $O_j$ is processed on Machine $i$, or $O_j$ \u201ccan be\u201d processed on Machine $i$? What is the concept of model designing (or why it is designed in this way)?\n\n- It\u2019s not clear that \u201cJob Sequence Embedding\u201d, if $O_{ij}$ ($j$-th operation of Job $i$) is processed then $A_J(J_i, O_{ij})$ = 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N.A."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9463/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697983577531,
        "cdate": 1697983577531,
        "tmdate": 1699637191201,
        "mdate": 1699637191201,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "S3IqtlOGLT",
        "forum": "10eQ4Cfh8p",
        "replyto": "10eQ4Cfh8p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9463/Reviewer_ZW2D"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9463/Reviewer_ZW2D"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an RL based scheduling methods for Flexible Job Shop Problem. The approach empolys two graph neural network models, a generative model and an improvement model, which collaboratively solve the problem. At each timestep, the generative model progressively constructs a partial solution by adding a new component into the existing partial solution, and the improvement model refines this partial solution for better performance. Both models are designed to leverage inductive biases from the problem and its current partial solution, e.g., neighbor nodes from different types of edge. The models are trained end-to-end using the reward signal for each model, in an alternating manner to stabilize the learning of two models.\n\nThe proposed algorithm is evaluated with two experiments, one for synthetic datasets and the other for public benchmarks, and it showed superiority over several heuristics and DRL-based methods in terms of solution quality. Also, though the method failed to outperform the meta-heuristic algorithms, it showed comparable result while spending much less time than the meta-heuristics."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This work proposed a new RL-based framework for solving FJSP, which combines the construction and the improvement processes so that they can be trained in end-to-end manner.\n- The method utilizes different graph representation that corresponds to a single partial solution, providing each model with relevant information. This approach is both interesting and convincing. \n- Ablation study for the two distinct models provides a good empirical evidence for the proposed architecture."
            },
            "weaknesses": {
                "value": "[Methods and Experiments]\n- This paper doesn't provide a clear rationale or justification for the use of various embeddings. Also, there's no ablation study for these design choices.\n- The method is evaluated only two public benchmarks, whereas the DRL baseline [1] has been tested on a more extensive set of benchmarks. This raises concerns about the comprehensiveness of the evaluation and potentially limits the generalizability of the proposed method's performance.\n- The reported performance of DRL baseline [1] is based on the greedy selection, while the method of this paper leverages sampling for improvement steps. For fairer comparison, the results from both greedy and sampling decoding should be included. Note that sampling performance reported in [1] for v_la task is better than the proposed method, while consuming more computation time.\n\n[Writings]  \nThis paper has significant defects with clarity. \nFirst of all, there are too many typos, wrong spacing and inconsistent notaions. Below are some of them:  \n- page 1: 'PRD' \u2192 'PDR'\n- page 2: There are many wrong spacing in Sec. 2, e.g, 'O_i,which', 'operations,O_{ij}', or \"...end of production.These two ...\"\n- page 3: There is a wrong figure reference, '(in figure)' \n- page 4: 'avenger' \u2192 'average'\n- page 4: GAT has no reference\n- page 5: 'avitation' \u2192 'activation'\n- page 5: 'M_{ij}' suddenly pops up, which supposedly typo of {A_I}_{ij}, and suddenly A_J is used, which is definitely a typo.\n- page 7: In the Algorithm 1, 'EP_I' \u2192 'EP_G' for the transition of generative model.\n- page 7: There are several '(ref)'s in Sec 5.1, which should have been replaced by appropriate reference.\n- page 8: In the text they say they use Gurobi Solver, but they report OR-Tools in the table.\n- Throughout the paper, the authors use abbreviations without declare it, e.g., DRL in page 3, GAT in page 4, and GIM in page 7 (Algorithm 1)\n\nMoreover, the models are not clearly described, which makes it hard to fully understand the algorithm.  \nFor example, in GAT Module section in page 5, it is unclear whether W is shared among different u's or not.  \nAlso, the reward for each model is not stated mathematically, which introduces an ambiguity.\n\n[1] Song, Wen, et al. \"Flexible job-shop scheduling via graph neural network and deep reinforcement learning.\" IEEE Transactions on Industrial Informatics 19.2 (2022)"
            },
            "questions": {
                "value": "- How long it takes for training?\n- Why the 40 x 10 result is missing for OR-Tools?\n- How can this work be extended to other scheduling or CO problems?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9463/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772486057,
        "cdate": 1698772486057,
        "tmdate": 1699637190984,
        "mdate": 1699637190984,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "w67nfG2dMW",
        "forum": "10eQ4Cfh8p",
        "replyto": "10eQ4Cfh8p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9463/Reviewer_swqB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9463/Reviewer_swqB"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an end-to-end RL framework to solve the Flexible Job-Shop Problem (FJSP). The framework consists of two major components: a generation model that produces an assignment of operations that updates the partial solution, and an improving model that refines the current partial solution. By repeating the generation and improving steps until the complete solution is found, the proposed framework finds a solution for FJSP.\n\nThe authors evaluate the proposed framework with various-sized FJSP instances, and it is shown to outperform compounded Priority Dispatching Rules (PDR) but underperform Meta-heuristics (e.g., OR-tools)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed framework suggests a novel perspective for solving FJSP. Unlike the majority of iterative improving approaches that often perform improvement steps from a complete solution, the proposed framework employs \"improving\" actions during solution construction."
            },
            "weaknesses": {
                "value": "- The current manuscript still has room for improvement, including a more detailed explanation of the training.\n- The performance evaluation of the proposed framework seems quite limited, especially as the baselines are overly simplified in Table 1."
            },
            "questions": {
                "value": "- It seems the number of improvement iterations $n_t$ would play a crucial role within the proposed framework. Could the authors provide further details on how to decide $n_t$? In the current manuscript, it is simply mentioned as a hand-crafted function depending on the iteration index $t$.\n- What is GIM in Table 3? From the context, I assume it is the proposed method, but the acronym is never introduced.\n- What is \"Generate+improve\" in Table 3? Is it different from GIM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9463/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782010707,
        "cdate": 1698782010707,
        "tmdate": 1699637190849,
        "mdate": 1699637190849,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zlSWBdRAhX",
        "forum": "10eQ4Cfh8p",
        "replyto": "10eQ4Cfh8p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9463/Reviewer_9qF7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9463/Reviewer_9qF7"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a reinforcement learning framework tailored for the Flexible Job Shop Problem (FJSP). The methodology leverages graph neural networks, allowing the model to handle FJSP instances of varying scales. The main novelty consists of simultaneous generation and improvement: a generative model sequentially produces solutions while an improvement model refines them. Both models are trained concurrently via reinforcement learning. The approach is at least an order of magnitude faster than metaheuristics and outperforms dispatching rules and some previous RL approaches in terms of solution quality."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The tackled problem is important in several practical scheduling applications. Unlike previous approaches that either only generate solutions in one shot or only learn to improve, the proposed approach trains two models to generate and improve at the same time, which could potentially provide the \u201cbest of both worlds\u201d, i.e., speed of one-shot generation and solution quality of improvement methods. The proposed two-stage approach and training is novel for scheduling problems to the best of my knowledge."
            },
            "weaknesses": {
                "value": "My biggest concern is that the proposed approach seems to be only applicable to a specific scheduling problem (FJSP) with no variation in terms of constraints. In the abstract, the authors state that:\n\n> It is worth noting that this training paradigm can be readily adapted to other combinatorial optimization problems, such as the traveling salesman problem and beyond.\n> \n\nhowever, there is 1) no empirical evidence to justify the claim and 2) no explanation of *how* this can actually be done. For instance, how can the improvement step be applied to the traveling salesman problem (TSP), especially considering the reward function? In several combinatorial optimization problems, it is hard to define a step-wise reward function, such as in routing problems such as the TSP. Moreover, the specific design of Section 3 seems to be over-fitted to the FJSP, with new problems requiring a substantial restructuring of the model.\n\nAnother important point is that the proposed generation-improvement method is not well justified in terms of performance. There is no ablation study on just using the generator model without any improvement:\n\n> From the design of our framework, it can be seen that the generative model and the improved model can run independently, which means that we can only use the generative model to generate a complete solution or use the improved model individually to improve any feasible initial solution generated by other methods (such as random generation or PDRs).\n> \n\nbut there is no result about this; in Table 3 only the improvement method alone is shown with other models. How would the `generate` only perform? Moreover, a natural question would arise, namely why authors decided to go for a potentially more burdensome generate+improve method (in which the generator may potentially be worse due to over-reliance on the improvement model), and not just a generator. In these regards, it would be interesting to see how the same model with the generation part only would do.\n\nThe experimental section seems to be lacking some baselines - for instance, Table 1 only compares against OR-Tools and dispatching rules, but not against RGA and 2SGA and the 2 DRL baselines. Also, OR-Tools is missing the solution time, so it is difficult to assess how the proposed approach compares in solution time (given that the quality is already worse than the OR-Tools metaheuristics). Finally, no standard deviation has been reported nor multiple runs.\n\nIn terms of the quality of the paper, there is room for improvement. Aside from several typos, the writing feels sloppy, and there are missing references (`(ref)`  in the paper, mk[?] in Table 2 and more), so I would suggest some revision. More importantly, in Algorithm 1:\n\n> Store Transition $\\tau_{t+1}^g :< \\mathcal{S}_t; ~ \\mathcal{S}_{t+1} > \\text{into} ~EP_I$\n> \n\nI believe this should be, in fact, $EP_I$ , given that $EP_G$ is not being used here.\n\nFinally, no code has been provided to reproduce the results."
            },
            "questions": {
                "value": "1. Why did you decide to use DuelingDQN, and not actor-critic algorithms such as PPO [29] or policy gradient methods as done in MatNet [17]*?\n2. As in the \u201cweaknesses\u201d section, how would the model perform if only the generator was trained? And what if we trained with generator+improve but only used the generator for the solution?\n3. What is the number of improvement iterations $n_t$, and how was it selected?\n4. How would the proposed method fare in larger-scale instances? [29] studies scale up to $100 \\times 60$.\n\n---\n\n*Note: MatNet [17] is cited in the manuscript but not referenced throughout the text. It may be useful to at least briefly introduce the differences with the proposed method in the related works."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9463/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807488880,
        "cdate": 1698807488880,
        "tmdate": 1699637190736,
        "mdate": 1699637190736,
        "license": "CC BY 4.0",
        "version": 2
    }
]