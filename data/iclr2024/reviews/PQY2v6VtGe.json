[
    {
        "id": "8LBLiVafDS",
        "forum": "PQY2v6VtGe",
        "replyto": "PQY2v6VtGe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8068/Reviewer_yrMg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8068/Reviewer_yrMg"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a zero-knowledge proof (ZKP) protocol, dubbed Confidential-DPproof, for an auditor to verify that a company (prover) has trained a ML model using DP-SGG at a certain privacy level, on a fixed private dataset (which should not be revealed to the auditor). Their method has three desirable properties:\n\n1. An honest prover can convince an honest auditor that they have correctly implemented DP-SGD (and therefore that the resulting model is differentially private at a certain level, known to both parties);\n2. A dishonest prover cannot convince an honest auditor that the trained model satisfies DP when it in fact does not; and\n3. A dishonest auditor cannot bias the computations of an honest prover. In particular, a dishonest auditor cannot gain additional information about the training data, beyond what they would know from observing the output of a DP algorithm trained on the private data.\n\nExperiments with Confidential-DPproof show that the ZKP mechanisms still allow for practically feasible runtimes for model training. The authors obtain strong model utility on CIFAR-10 and MNIST, while still enforcing strong DP guarantees ($\\epsilon < 1$)."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Confidential-DPproof provides a strong alternative to current methods for privacy auditing, which require instantiating membership inference adversaries to exploit the output of allegedly DP algorithms, thereby providing a lower bound on the privacy leakage. Unless the adversary can be shown to be optimal, however, this approach cannot provide an _upper_ bound on the privacy leakage. In general, implementing such attacks is also computationally difficult, and optimal adversaries are often intractable.\n\nBy approaching the problem from a different angle, the authors completely sidestep the need for optimal adversaries for verifying privacy guarantees. This is especially impressive because many of the strongest membership inference attacks require access to the private data in order to be trained (or at least a very good proxy), but this is unrealistic in practical scenarios requiring privacy. In contrast, their method does not require the auditor to have access to any private training data. As such, I believe this to be an exciting, non-incremental contribution, one which has the potential to change the paradigm for privacy auditing moving forward."
            },
            "weaknesses": {
                "value": "As ICLR is not primarily a security conference, it is likely that many readers will be unfamiliar with the terms and methodology used. As such, more discussion of the cryptographic primitives used would be helpful in improving the clarity of the paper. (See the \"Questions\" section below.)\n\nDue to the additional computational overhead imposed by both DP-SGD itself, and the need to represent the steps of the algorithm in circuits which can be integrated with existing ZKP systems, the authors cannot train full neural networks. Instead, they rely on fixed feature extraction methods trained on public data or using other methods independent of the private training data, then train a logistic classifier on top of these representations. Even with these simplifications, gradient computation + clipping can still take over a second per sample in higher feature dimensions. However, as the authors point out in the related work, even the problem of computationally feasible non-private proof of learning is still unresolved. This would be an important avenue for future work in order for this auditing protocol to be applied to modern models with hundreds of billions of parameters trained on massive datasets."
            },
            "questions": {
                "value": "I do not have extensive background with ZKPs, so I would like to make sure my understanding of the paper is correct. Can the authors confirm if the following statements are true?\n\n**Unbiased random seed generation:** From the honest prover's perspective, since $k$ was chosen uniformly at random and the auditor only knows $[[k]]$ (but nothing about $k$ itself), the random seed $s=k\\oplus r$ is still uniformly random. From the honest auditor's perspective, since $r$ was chosen uniformly at random _after_ $k$ was fixed, $s$ must be uniformly random.\n\n**Dataset commitment:** For verifying that the computations were performed on the committed dataset, we can think of it as follows. The data commitment is another key $K$ which depends on the dataset $\\mathcal{D}$, but which gives the auditor no information about $\\mathcal{D}$ (since it was an XOR with a private random quantity $M$ known only to the prover; this is similar to the relationship between $k$ and $[[k]]$ above). However, for each circuit $\\mathcal{C}$ making up a step of the DP-SGD procedure, the prover can verify that the output of this step was computed on $\\mathcal{D}$ using the agreed upon random seed, and this verification _only requires knowledge of $K$_, not $\\mathcal{D}$ itself.\n\n**DP-SGD privacy accounting:** This leads to my final question. It seems that the ZKP building blocks allow you to generate a proof for each iteration in DP-SGD, then the proof for the whole procedure is just the AND of all of these steps. In particular, this means that the auditor actually will see all of the intermediate models during the DP-SGD training procedure. There has been some recent work on improving the privacy guarantees of DP-SGD under the assumption that the algorithm output is only the _final_ model parameters $W^T$, rather than the entire trajectory [1]. Confidential-DPproof would be incompatible with this analysis, since there isn't a ZKP protocol (yet) which encodes the entire model training procedure, rather than just the individual steps.\n\nReference:\n[1] Ye, Jiayuan, and Reza Shokri. \"Differentially private learning needs hidden state (or much faster convergence).\" Advances in Neural Information Processing Systems 35 (2022): 703-715."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8068/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8068/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8068/Reviewer_yrMg"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8068/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697836463373,
        "cdate": 1697836463373,
        "tmdate": 1699636997944,
        "mdate": 1699636997944,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NJXbKtL4tU",
        "forum": "PQY2v6VtGe",
        "replyto": "PQY2v6VtGe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8068/Reviewer_PSjV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8068/Reviewer_PSjV"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a protocol for auditing DP-SGD. The approach is based on zero-knowledge proof and does not require the auditor to access the model and raw data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem of verifying privacy claims of algorithms is very important practically.\n2. The proposed protocol based on zero-knowledge proofs does not require the auditor to access model parameters, data, and intermediate updates. \n3. The authors take into account malicious auditors and dishonest provers in various aspects of the protocol such as random seed generation and"
            },
            "weaknesses": {
                "value": "1. The auditor needs to know many implementation details, e.g. clip threshold, and number of iterations. Also, the protocol is specifically designed for DP-SGD. It seems that we need to design different protocols for different algorithms, even if we only make minor adjustments to the algorithm. \n2. Many steps in the DP-SGD algorithm need to be proved in Phase 3. If one step is missing, for example, the auditor forgets to let the prover verify step vi, the total number of iterations, how would it affect validity of the privacy audit claims made by the auditor?\n3. The proposed cryptographic approach does not scale to large models trained with DP-SGD.\n4. It seems to me that the protocol only attempts to verify that every step of the DP-SGD algorithm is executed correctly as claimed, and the certified privacy parameters are simply derived based on the verified $\\sigma$ and subsampling level, which is an upper bound on the actual privacy guarantee (as stated in the conclusion). Thus, when the certified upper bound exceeds the claimed value, we do not know whether there is a privacy failure. Note that even with 100% correct execution of DP-SGD, privacy failure may still exist due to other issues like finite precision computation of floats [1]. Thus, verifying all steps are executed correctly is not sufficient to audit privacy claims, and a privacy lower bound should still be necessary.\n5. How does the approach compare to the recent work of [2]?\n6. I have questions regarding the experiment setup. See the question section.\n\n[1] Widespread Underestimation of Sensitivity in Differentially Private Libraries and How to Fix It, S Casacuberta, M Shoemate, S Vadhan, C Wagaman, CCS 2022\n\n[2] Privacy Auditing with One (1) Training Run, Thomas Steinke, Milad Nasr, Matthew Jagielski, https://arxiv.org/abs/2305.08846\n\n\nTypos:\n1. Page 5, line 10 (the description of phase 2): \"Next, the auditor generates... and sends.. to the auditor\". The second \"auditor\" should be \"prover\"?\n2. Page 6, line 2: we first generates -> generate"
            },
            "questions": {
                "value": "1. What are the hyper-parameters: $C, \\sigma, T$ in your experiments, and what are the corresponding theoretical upper bounds on $\\epsilon, \\delta$?\n2. Compared to the privacy upper bound provided for the chosen $\\sigma$, how accurate are the certified privacy guarantees?\n3. The results do not show certified level of $\\delta$.\n4. The running time may vary across different machines and may not be a consistent measure of computational cost."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8068/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723961470,
        "cdate": 1698723961470,
        "tmdate": 1699636997829,
        "mdate": 1699636997829,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2IyJtXqTOM",
        "forum": "PQY2v6VtGe",
        "replyto": "PQY2v6VtGe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8068/Reviewer_y3Ya"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8068/Reviewer_y3Ya"
        ],
        "content": {
            "summary": {
                "value": "This work studies important problem in data privacy research, the privacy auditing problem. The main approach is a zero-knowledge proof protocol for differential private machine learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper seems well-written."
            },
            "weaknesses": {
                "value": "Due to my lack background of zero-knowledge proof, it's difficult to evaluate the contribution."
            },
            "questions": {
                "value": "1. How this framework to give guidance to correct DP-SGD implementation if the algorithm did not pass the privacy auditing?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8068/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790058047,
        "cdate": 1698790058047,
        "tmdate": 1699636997704,
        "mdate": 1699636997704,
        "license": "CC BY 4.0",
        "version": 2
    }
]