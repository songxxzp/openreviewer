[
    {
        "id": "OA9XHfMeQy",
        "forum": "7q7s5fXEpP",
        "replyto": "7q7s5fXEpP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5196/Reviewer_Maon"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5196/Reviewer_Maon"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an attack for stealing deep neural network policies without access to the environment or input states' ranges. The paper also proposes a defense for countering this attack. The attack includes creating a dataset by querying the victim policy, BC for training the attacker, a discriminative reward to distinguish state-action pairs from attacker or from policy, reward weighted sampling of states in the next iteration of querying the victim policy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Interesting problem formulation. The challenges beyond supervised model stealing are clear.\n2. Strong performance of the proposed obfuscation defense to the proposed attack."
            },
            "weaknesses": {
                "value": "1. Policy stealing is performed on simplistic mujoco case studies with simple MLPs. While the threat model is realistic, the application seems simple. Moreover, there are no experiments with realistic robotic imitation learning datasets (e.g. robomimic, D4RL's Adroit, CARLA) and recent models (e.g. diffusion policies, behavior transformers). This is particularly important given that model stealing is particularly relevant with large models only available as API such as the recent RT-2-X model [1]. The authors provide an abltaion with number of layers in an MLP but not with other architectures.\n\n[1] Padalkar, Abhishek, et al. \"Open X-Embodiment: Robotic learning datasets and RT-X models.\" 2023.\n\n2. The performance of the learned reward discriminator and the performance of the victim policy+defense is not shown.\n2. The code has not yet been made available. While the paper promises to release the code, it is limiting to not examine the code during the review process."
            },
            "questions": {
                "value": "1. What would the required budget be to scale to image or lidar inputs?\n2. Is there perhaps a way to break the defense along the lines of adaptive attacks proposed in [2]?\n\n[2] Tramer et al. \"On Adaptive Attacks to Adversarial Example Defenses.\" 2020."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5196/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5196/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5196/Reviewer_Maon"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5196/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796301859,
        "cdate": 1698796301859,
        "tmdate": 1700660138365,
        "mdate": 1700660138365,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fw7cmlqQQT",
        "forum": "7q7s5fXEpP",
        "replyto": "7q7s5fXEpP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5196/Reviewer_AqHM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5196/Reviewer_AqHM"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the potential for model stealing attacks on control policies trained with deep reinforcement learning. While model stealing has been applied to image classifiers in supervised learning settings, the authors argue that the control setting is more difficult because the distribution of state inputs may be unknown. The authors formulate a method to estimate the state distribution via the hypothesis that the policy should be more difficult to imitate within this distribution, an assumption which is verified in Table 1. The method involves jointly estimating the state distribution and learning an imitation policy over that state distribution with behavior cloning. The authors show it is more accurately able to both estimate the state distribution and steal the victim policy compared to using a fixed state distribution or using DFME."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The experiments in the paper seem to be relatively comprehensive, including an ablation study, validation of the assumptions underlying the authors' algorithm, and a defense against the attack. The results also seem quite promising, showing that the SI attack estimates both the state distribution and victim policy well. I don't know of previous work on model stealing in deep RL/control, so the work seems novel, although I am not very familiar with the area."
            },
            "weaknesses": {
                "value": "Some potential weaknesses include:\n * The writing could be clearer in some places. The proposed algorithm has many components, and some of the experiments are somewhat complex\u2014it was a bit hard to understand the purpose of some algorithm components or experiments at first.\n * The setting of wanting to steal a policy without knowing the environment seems unrealistic\u2014what is the attacker planning to do with the policy if it doesn't have access to the environment? Wouldn't the point of stealing a policy be to run the policy in the environment, which entails the attacker has access?\n * The defense shows that a very simple countermeasure, which is trivial to implement and has no impact on normal system performance, prevents this attack from succeeding. This could also be viewed as a positive, though, since calling attention to this type of attack and associated defense could drive practitioners to employ the defense in real-world applications. Maybe it would be good to reframe the contribution of the defense along these lines.\n * There are some additional issues with some of the exposition being unclear or misleading:\n    * Section 4.2 part I: the authors write \"we choose a univariate normal distribution\" but in fact it is a multivariate normal distribution since the state space is $\\mathbb{R}^n$. I believe the authors mean that they use a multivariate normal distribution with a diagonal covariance matrix.\n    * The \"smooth L1 loss\" referred to in section 4.2 part II is generally known as the Huber loss and the authors should cite Huber, \"Robust Estimation of a Location Parameter\" (1964).\n    * It feels a bit misleading that the y-axes in the top row of Figure 2 don't start at 0. It looks at a glance like the KL approaches zero with more queries in each column, whereas in fact all of them seem to asymptote above 0.\n * The comparison to DFME might not be fair\u2014see my questions below."
            },
            "questions": {
                "value": "Based on the weaknesses above, here are some questions for the authors. If these are satisfactorily answered, I can raise my score:\n * Why is the objective in (1) to minimize reward difference? Shouldn't it either be to maximize reward, or imitate as precisely as possible?\n * What is the point of stealing a policy if you don't know the environment?\n * What exactly is the purpose of the experiments in Figure 3, since to calculate all of these distributions one needs access to states sampled from $S_v$, which is not possible under the threat model proposed? Why not compare to $\\\\mathcal{N}(\\\\mathbf{\\\\tilde\\\\mu}, \\\\mathbf{\\\\tilde\\\\sigma}^2)$ as well?\n * Regarding the comparison to DFME, the authors write that \"DFME does not manage to effectively steal the victim policy. This is largely due to its generator\u2019s tendency to search for adversarial samples within a predefined initial range.\" What exactly is the initial state range output by DFME, and what happens if you expand the initial range? For instance, one could simply use a larger weight initialization for the final layer of the generator network in DFME, or multiply the outputs by a large constant. How does this compare to the newly proposed SI attack?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5196/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698971131188,
        "cdate": 1698971131188,
        "tmdate": 1699636516242,
        "mdate": 1699636516242,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Hr2iMcJCOX",
        "forum": "7q7s5fXEpP",
        "replyto": "7q7s5fXEpP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5196/Reviewer_3FCu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5196/Reviewer_3FCu"
        ],
        "content": {
            "summary": {
                "value": "The authors present a new approach for black-box policy stealing: reproducing a policy without access to the environment/data that it was originally trained on.  The authors only assume a fixed budget of interaction with the model being attacked (the victim) and do not assume differentiability.  They also do not assume knowledge of the input range to the model.  The authors' method attempts to find an approximation of the input range and data used to train the model by fitting a normal distribution to input values with a high prediction mismatch between current behavior predicted by the attacker and the victim.  They then train the attacker by sampling from this distribution and using the victim's predictions as labels.  The authors also present a defense against their method of attack that selects random responses for inputs outside of the acceptable input range."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The idea is new and poses interesting technical challenges.\n* The paper is also clear, well structured and well explained.\n* The ablations are well thought out and provide a lot of insight into the details of the technique."
            },
            "weaknesses": {
                "value": "* The test environments are quite limited.  These Mujoco environments are small, and there are only three of them.  The only contact dynamics are with the ground and self-collision.  Testing in larger environments with more degrees of freedom and richer dynamics is highly encouraged.\n* This method seems impossible for policies with high-dimensional input such as images.\n* The approximation of the state distribution using a normal distribution seems quite limiting, and it's an open question as to whether this would work for more complex high-dimensional problems.  The authors discuss this in 5.3 and provide some empirical experiments purporting to show that this is fine, but I am somewhat skeptical about the situation in problems with larger state spaces.\n* The primary method proposed here seems like it would be prone to distribution shift over time.  If I understand correctly, during each iteration, $\\pi_a$ is trained on $D_v$ which consists of only on the most recently collected data.  At the same time $\\mu$ and $\\sigma$ are estimated by approximating the distribution of points where $\\pi_a$ struggles to match the output of the victim $\\pi_v$.  It seems likely in this scenario that because the distribution that $\\pi_a$ is trained on shifts over time (as $\\mu$ and $\\sigma$ change), it will get better at the regions of space most recently encountered, and may lose capability on regions that have not been visited recently.  It seems that the use of $\\pi_e$ which is reinitialized every time is meant to address this, but some discussion of the role of drift would be interesting here.\n* It also seems that there are a lot of cat-and-mouse games that could be played with the proposed defense against this stealing technique as well.  The authors suggest randomizing outputs for states that are outside of a known input range, but this is trivially circumvented by querying the same point multiple times and assuming points with low variance are within the training distribution (a lot of this depends on whether the policy is known to be stochastic).  If the defender then tries to keep out-of-bounds points the same or close across multiple queries, then the attacker could start querying very close points.  The defender could then try to build some random, yet fixed, yet smoothly changing landscape of points outside the boundaries, but this seems like a complicated modelling task.  In the end, the suggestion of returning random points in the out-of-bounds range seems right, but like most security challenges, hard to get right in a way that prevents further circumvention.\n* The authors target interactive policies in RL domains, but there doesn't seem to be anything here requiring RL, or specific to the RL setting.  This technique should be applicable in any domain where you are trying to clone a network with input small enough that it can be modeled using a normal distribution.  With that in mind, it would be good to see results on a wider variety of problems."
            },
            "questions": {
                "value": "* Addressing the questions about drift in the Weaknesses section would be great.\n* Discussion the issues with the counter-attack would also be beneficial.\n\nAlthough my score is currently marginally below the acceptance threshold, I like this paper, and am quite flexible in my evaluation of it.  If the authors are able to demonstrate their method in a wider variety of challenging environments, this would go a long way to improving my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5196/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699150042546,
        "cdate": 1699150042546,
        "tmdate": 1699636516152,
        "mdate": 1699636516152,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i7YXnXj7Yn",
        "forum": "7q7s5fXEpP",
        "replyto": "7q7s5fXEpP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5196/Reviewer_LEDd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5196/Reviewer_LEDd"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method for stealing reinforcement learning policies without access to the training environment. This approach, called 'Stealthy Imitation,' efficiently estimates the state distribution of the target policy and utilizes a reward model to refine the attack strategy. While showcasing superior performance in policy stealing compared to existing methods, the paper also presents a practical countermeasure to mitigate such attacks. However, the complexity of the approach and ethical considerations surrounding its potential misuse are notable concerns."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Innovative Approach: Represents an advancement in the field of model stealing, particularly in RL settings, with a more realistic threat model.\n\n- Effective Methodology: Demonstrates superior performance in stealing RL policies compared to existing methods, even with limited information.\n\n- Practical Countermeasure: Offers a realistic and practical solution to mitigate the risks of such attacks."
            },
            "weaknesses": {
                "value": "- Complexity: The method's complexity, particularly in estimating the state distribution and refining the attack policy, is relatively straightforward and not incurring significant novelty/advancements compared to the existing approaches.\n\n- Real-World Applicability: Transitioning from a controlled experimental setup to real-world applications might present unforeseen challenges, for example, the authors only experiment on simple tasks that have relatively simple state distribution. The underlying challenges for applying such tasks to a more complicated task is unclear."
            },
            "questions": {
                "value": "The authors propose a new method to advancing the model stealing attack against RL models and the reviewer appreciates that the authors also provide a countermeasure method for the ethical concerns. However, the reviewer has major concerns regarding the novelty of the methods. Model stealing with a synthetic dataset + a surrogate model (here state estimators) is not new to the community. The reviewer would appreciate if the authors could elaborate more on the challenges of this matter, especially the unique challenges when applying such attack in the RL setting. Also, the baseline of DFML is not fair enough given the different settings here. Following the philosophy of DFML, it seems that the proposed SI is the same approach under the RL setting. Another issue the reviewer has concerns is regarding the complexity of the target RL tasks. While it might be easy to build a synthetic dataset for simple tasks like Hopper, it might be infeasible for more complicated tasks as the state dimension increases. The authors should have a more thorough discussion regarding how such attacks could have impacts in the real world (e.g., stealing policies that are more valuable, which are often trained for handling more complicated tasks)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5196/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699597047469,
        "cdate": 1699597047469,
        "tmdate": 1699636516072,
        "mdate": 1699636516072,
        "license": "CC BY 4.0",
        "version": 2
    }
]