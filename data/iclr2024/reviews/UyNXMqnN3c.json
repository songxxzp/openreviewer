[
    {
        "id": "CNmk9yYTjq",
        "forum": "UyNXMqnN3c",
        "replyto": "UyNXMqnN3c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission789/Reviewer_MZY6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission789/Reviewer_MZY6"
        ],
        "content": {
            "summary": {
                "value": "DreamGaussian is a novel 3D content generation framework designed to efficiently produce high-quality 3D content. The core innovation is a generative 3D Gaussian Splatting model paired with mesh extraction and texture refinement processes in UV space. Unlike the occupancy pruning seen in Neural Radiance Fields, this approach uses progressive densification of 3D Gaussians, which results in faster convergence for 3D generation tasks. The framework also incorporates an algorithm to transform 3D Gaussians into textured meshes and employs a fine-tuning stage for detail refinement. In tests, DreamGaussian was able to produce high-quality textured meshes from a single-view image in just 2 minutes, marking a tenfold speed improvement over existing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper's main contribution is changing the NeRF representation to Gaussian Splatting representation in the current text-to-3D or image-to-3D pipeline. This has significant challenges in terms of implementation. Also, the authors' current implementation allows for fast generation of assets, which has significant importance in the 3D field."
            },
            "weaknesses": {
                "value": "Regarding the note \u201cSimilar to Dreamtime (Huang et al., 2023), we decrease the timestep t linearly\u201d: Did the decrease of t help with the training time as well? Did it bring instability in training to the model? My previous exploration in this direction (for text to 3D in NeRF) showed improvement in the speed of generation for some objects but brought instability in the training. Since some objects needed more training steps than others, having a fixed annealing strategy damaged the performance, specifically for objects (e.g., motorcycles or dogs). It would be great if the authors could explore, in text to 3D, what the effect of the speed of annealing would be for different sets of prompts, (e.g., a corgi vs. a tree). Does the speed of annealing need to be tuned for each prompt?\n\n\nThe authors provided the experiment for time annealing in Fig 7 for image to 3D, but the paper is missing the same figure for text to 3D. Also, the effect of the speed of annealing needs to be explored.\n\nRegarding the loss function, eq. 6, for UV-Space texture refinement: what happens if the object boundary in the Img2Img stage changes? How do the authors prevent the color of the background from leaking into the mesh color?\n\nIt would be great if the authors could compare the proposed texture refinement to other SOTA methods like TexFusion [1] or Text2Tex [2] or any other method of their choice. The reason for this ask is because texture refinement has been applied on top of the mesh. So, if the authors want to consider texture refinement as one of their contributions, they should either compare it with other baselines or reframe the paper and consider this as a side contribution.\n\n\nI also tried experimenting with the code (great codebase!) and observed that, for text to 3D, the texture in most cases was very saturated. It would be great if the authors could comment on this phenomenon in the paper as a shortcoming and provide some insight into which parameter tuning might help.\n\n\nRegarding the Janus problem, the authors provided a list of papers that address the Janus problem mostly using 3D data. However, it would be great if the authors could also reference methods like Perp-Neg [3] or Prompt-Debiasing [4] that address the Janus problem without the need for 3D assets. This has significant importance because they don't introduce bias from 3D data into the pipeline. For instance, they allow for the generation of a dog without the strict square position for standing that comes from the 3D asset.\nIt would be great if the authors could also comment on the guidance of the SDS loss and the effect of increasing those values.\n\nThere are many typos in the paper, and it would be great if the authors could fix them. Examples are: on page 2, \"Gaussian splitting\" appears to be a typo; it should likely be \"Gaussian splatting\". \u201cseverl methods\u201d on page 3 and \u201cDissusion\u201d on page 5 is it supposed to be discussion?\n\nIt would also be great if the authors could comment on how to get the model to consider both the input image and the text prompt simultaneously in both the initial stage and the later mesh/texture optimization stage. At the moment, it seems to ignore the optional text prompt. Was this intentional?\n\nThe paper contrary to DreamFusion does not learn an MLP for the background, it would be great if authors could comment on why they made that choice and what are their findings. \n\n[1] https://openaccess.thecvf.com//content/ICCV2023/papers/Cao_TexFusion_Synthesizing_3D_Textures_with_Text-Guided_Image_Diffusion_Models_ICCV_2023_paper.pdf\n\n[2]https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Text2Tex_Text-driven_Texture_Synthesis_via_Diffusion_Models_ICCV_2023_paper.pdf\n\n[3] https://arxiv.org/abs/2304.04968\n\n[4] https://arxiv.org/abs/2303.15413"
            },
            "questions": {
                "value": "Please consider the comments in the weaknesses section. I believe the current paper as it presents a great contribution to the field, and by addressing my current comment I am willing to increase my score even more."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission789/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698725242418,
        "cdate": 1698725242418,
        "tmdate": 1699636006502,
        "mdate": 1699636006502,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QSZBriKqml",
        "forum": "UyNXMqnN3c",
        "replyto": "UyNXMqnN3c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission789/Reviewer_huut"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission789/Reviewer_huut"
        ],
        "content": {
            "summary": {
                "value": "This paper proopsed a way to combine SDS loss and the recently proposed point-based rendering method GS. The pipeline is composed of 3 stages, 1) gaussian splatting optimization; 2) mesh extraction from point clouds; 3) texture refinement. The first stage is similar to other SDS-based methods which require pretrained 2d image diffusion models. However, the rendering method is switched to gaussian splatting. The second stage is done by applying marching cubes to opacities learned in the first stage. In the last stage, the texture is refined using 2D diffusion models. The full pipeline takes several minutes and we can obtain a mesh with textures. The authors show some results of image-conditioned and text-conditioned generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper has many strengths. Thanks to GS, the full pipeline is fast compared to some previous nerf-based methods. The author observed that a longer optimization of SDS does not give better results (sharp and detailed). Thus the focus of the method is not the SDS part. Instead, the authors only optimize the SDS loss with a few iterations (which is also the main reason why it is so fast). After obtaining the blurry 3D object, a refinement step inspired by diffusion-based image editing is applied. In the end, we can have a detailed mesh with textures. Another important aspect of this method, is the mesh extraction algorithm. Extracting a surface from a point cloud is not straightforward. The authors found out a way to use the opacity as the isosurface.\n\nWhen we are generating data, GS seems to be more suitable because of its progressive nature. The paper can be seen as a proof of this claim."
            },
            "weaknesses": {
                "value": "1. It seems the focus of the paper is image-conditioned generation. The results of text-to-3d are very limited and the comparison is weak.\n2. I am curious about the setup of the stage 3. If we already have a mesh with coarse texture, we can optimize it with differentiable mesh rendering and SDS loss, as Fantasia3d did in the appearance modeling stage. What is the advantage of the proposed refinement compared to this?\n3. Another concurrent ICLR submission ( https://openreview.net/forum?id=pnwh3JspxT ) optimizes SDS much longer (1 hour and 40 minutes) than this paper. However, this paper claims that longer training does not give better results. Can the authors clarify the differences?"
            },
            "questions": {
                "value": "See weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission789/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698767725439,
        "cdate": 1698767725439,
        "tmdate": 1699636006421,
        "mdate": 1699636006421,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "58OTkz4dfi",
        "forum": "UyNXMqnN3c",
        "replyto": "UyNXMqnN3c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission789/Reviewer_E7Ru"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission789/Reviewer_E7Ru"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the problem of 3D content generation from text- or single view inputs into 3D gaussian splats which can further be transformed into textured meshes. The proposed method is coined DreamGaussian and revisits the recent ToG 2023 3D Gaussian Splatting paradigm with a generative twist to it.\n\nThe proposed contribution is favorably compared to a comprehensive set of state-of-the-art comparative baselines and in particular is able to produced high fidelity mehses of objects within 2min of compute time, which is remarkable."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "+ ## Readability.\nAs it currently stands, the paper is very well written. The main ideas and concepts are mostly well explained and articulated throuthout.\n\n+ ## Organization of the contents and overall paper structure.\nThe contents are also very well structured and balanced.\n\n+ ## Overall maturity of the submitted package, which makes it very reasonably within camera ready territory.\n\n+ ## The actual performance of the proposed methodological contribution.\nIn particular, it dramatically cuts down the computation time in the space of optimization-based techniques in the field, by an order of magnitude.\n\n+ ## Related work section and discussion. \nIt is very well structued, articulated and populated with very relevant and up to date references."
            },
            "weaknesses": {
                "value": "+  ## 1. Missing bits of context information - How much does it cost?\nWhile indicative timings and implementation details (covering experimental setup,  are provided, information regarding the resource usage, model size and complexity are currently underdescribed.\n\nA comparative disclosure of such information covering the main experimental baselines that are considered would help the reader better assess its relative positioning throughout the typical criteria.\n\nMentioning where the computation bottlenecks lie in terms of pipeline components would also be valuable in order to fully assess the practical usefullness of the proposed sequential pipeline, beyond rough timings.\n\n+  ## 2. Challenging the ad hoc meshing post processing.\n\nAs it currently stands, the  mesh extraction technique relies on many subsequent post-processing steps, including mesh decimation and remeshing (end of page 5 in the main paper). I believe the explainations around eq. (4) (before and after) could be further improved and detailed. My current intuition is that the mesh complexity at least could be controled jointly during the density query step. Also, given the lack of statistics given regarding each step (Weakness 1 above), the relative need and ROI to fuse these steps is also hard to assess.\n\nThe current procedure also produces non-manifold and non-watertight meshes with arbitrary complexity.\n\n+  ## 3. Evaluation.\nThe user study (ie, subjective quality analysis) that is presented in the main paper and further detailed in the appendix is a good idea and often overlooked in the field. \n\nNevertheless, its size and statistical informative validity are rather limited as they are based on a \"cohort\" of 40 users and 15 input samples to assess from."
            },
            "questions": {
                "value": "The main questions I would have cover the aforementioned weaknesses that have been pinpointed. In particular regarding the missing bits of informations.\n\nBesides those remaining grey areas, I would be happy to bump my initial rating were they to be addressed accordingly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission789/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission789/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission789/Reviewer_E7Ru"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission789/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698852878439,
        "cdate": 1698852878439,
        "tmdate": 1700952551574,
        "mdate": 1700952551574,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5JZaD9MX7p",
        "forum": "UyNXMqnN3c",
        "replyto": "UyNXMqnN3c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission789/Reviewer_L9mE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission789/Reviewer_L9mE"
        ],
        "content": {
            "summary": {
                "value": "The present work presents a new methodology for generative 3d modelling using a 2d lifting approach. Its main underlying hypothesis is that using 3d Gaussian Splatting with its densification results in much faster convergence compared to using traditional neural radiance fields. On the technical side, this work focuses on two main contributions. First it proposes a mesh extraction technique based on the marching cubes algorithm for the setting of environment representations using 3d gaussians. Second, it proposes a UV-space texture refinement stage for further quality enhancement of the resulting textures."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Overall I think this paper to be interesting and to make relevant contributions. While most of the underlying ideas have already been presented in prior works, their combination for the presented use-case is relevant and results in significant performance improvement for the tasks of image-to-3d and text-to-3d."
            },
            "weaknesses": {
                "value": "While the work has overall a good contribution, one of my main concern is its writing. First, there are numerous minor language issues such as grammar mistakes and sometimes unnecessarily complicated sentence structure. This can easily be solved by a few rounds of careful proofreading. Second, the work reads more like a paper written for a computer vision conference without providing sufficient background to a broader audience at the ICLR community. While this style is not unprecedented in ML, it makes this work much harder accessible and misses an opportunity. Moreover, for people outside the specific subarea of 3d content creation some of the important implementation details may be missing.\n\nThese concerns range from minor points such as assuming the reader to be familiar with all mentioned vision / graphics concepts such as UV space etc without providing a proper background section. It also involves more complicated points such as the decision to provide some background (e.g. on SDS loss) but only to an extent that is only meaningful for people already familiar with dreamfusion. While in vision, many of these things can be assumed known, it would be useful to the learning community to provide some information here.\n\nAlso, I would rephrase the contribution bullet points to focus stronger on the technical aspects rather than first mentioning the overall framework, then dedicating one point to the actual technical meat and then talking about experimental evaluation."
            },
            "questions": {
                "value": "* Maybe rephrase the formulation \"to release the potential of optimization-based methods.\" to something like \"to unlock the potential. ...\" or something similar.\n* When writing \"we decrease the timestep t linearly, which is used to weight the random noise \u03f5 added to the rendered RGB image\", it is not fully clear to me how this is performed?\n* The SDS loss formulation is not clear without knowing the sds loss, e.g. the expectation is taken among other variables over p and t. What is the distribution of p and t? \n* Also, the evaluation section should be more explicit / more structured about the evaluation protocol and datasets used."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission789/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699244595168,
        "cdate": 1699244595168,
        "tmdate": 1699636006266,
        "mdate": 1699636006266,
        "license": "CC BY 4.0",
        "version": 2
    }
]