[
    {
        "id": "0BKQDybnpp",
        "forum": "VLFhbOCz5D",
        "replyto": "VLFhbOCz5D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6437/Reviewer_ZQ32"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6437/Reviewer_ZQ32"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a method for fine-tuning linearized transformers, called Tangent Attention Fine-Tuning (TAFT). \n\nThe paper claims that TAFT can achieve comparable performance to non-linear fine-tuning on various downstream tasks while enjoying the benefits of linearity such as compositionality, parallel training, machine unlearning, and differential privacy. \n\nThe paper also introduces Tangent Transformers, which are linearized versions of pre-trained transformer models, and shows how to compute their Jacobian-Vector products efficiently in a single forward pass. \n\nThe paper demonstrates the advantages of TAFT and Tangent Transformers in several experiments using vision transformer models. \n\nTo summarize, the paper's main contributions are:\n\n- A novel method for fine-tuning linearized transformers that is computationally efficient and competitive with non-linear fine-tuning.\n\n- A theoretical analysis of the benefits of linearity for model composition, parallel training, machine unlearning, and differential privacy.\n\n- An empirical evaluation of TAFT and Tangent Transformers on various downstream visual classification tasks using vision transformer models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper proposes a novel method for fine-tuning linearized transformers, called Tangent Attention Fine-Tuning (TAFT), which is computationally efficient and competitive with non-linear fine-tuning. The paper also introduces Tangent Transformers, which are linearized versions of pre-trained transformer models, and shows how to compute their Jacobian-Vector products efficiently in a single forward pass. The paper demonstrates the advantages of TAFT and Tangent Transformers on several experiments using vision transformer models.\n\nIn terms of originality, the paper introduces a new method for fine-tuning linearized transformers that is computationally efficient and competitive with non-linear fine-tuning. The paper also introduces Tangent Transformers, which are linearized versions of pre-trained transformer models, and shows how to compute their Jacobian-Vector products efficiently in a single forward pass. These contributions are novel and have not been explored before.\n\nIn terms of quality, the paper provides a theoretical analysis of the benefits of linearity for model composition, parallel training, machine unlearning, and differential privacy. The paper also provides an empirical evaluation of TAFT and Tangent Transformers on various downstream visual classification tasks using vision transformer models. The experiments are well-designed and the results are presented clearly.\n\nIn terms of clarity, the paper is well-written and easy to follow. The authors provide clear explanations of their methods and results. The paper also includes visualizations that help to illustrate the concepts presented.\n\nIn terms of significance, the paper\u2019s contributions have important implications for the field of machine learning. The proposed method for fine-tuning linearized transformers is computationally efficient and competitive with non-linear fine-tuning. This has important implications for large-scale applications where computational efficiency is critical. Additionally, the theoretical analysis of the benefits of linearity has important implications for model composition, parallel training, machine unlearning, and differential privacy.\n\nOverall, this is a well-written paper that makes significant contributions to the field of machine learning."
            },
            "weaknesses": {
                "value": "(1) While the paper is well-written and easy to follow, it would be helpful to provide more detailed explanations of some of the concepts presented. For example, the paper could provide more details on how Tangent Transformers are computed and how they are used in practice (especially from Eq 5 to Eq 6).\n\n(2) While the paper provides an empirical evaluation of TAFT and Tangent Transformers on various downstream visual classification tasks using vision transformer models, it would be helpful to see more experiments that compare TAFT with other fine-tuning methods on these tasks. This would help to establish the competitiveness of TAFT more clearly.\n\n(3) While the paper provides a theoretical analysis of the benefits of linearity for model composition, parallel training, machine unlearning, and differential privacy, it would be beneficial to provide more empirical evidence to support these claims. Specifically, it would be helpful to see more experiments that demonstrate the advantages of TAFT and Tangent Transformers on a wider range of tasks and datasets.\n(e.g. DTD and UCF101).\n\nOverall, this is a well-written paper that makes some contributions to the field. However, there is room for improvement in terms of providing more empirical evidence to support the claims made in the paper and providing more detailed explanations of some of the concepts presented."
            },
            "questions": {
                "value": "(1) Please give a detailed derivation process from Eq 5 to Eq 6 in the appendix.\n\n(2) The selected datasets (Caltech-256 (Griffin et al., 2007), MIT-67 (Quattoni & Torralba, 2009), Oxford Pets (Parkhi et al.,\n2012), Stanford Dogs (Khosla et al., 2011), CUB-200 (Wah et al., 2011), FGVC-Aircrafts (Maji\net al., 2013), and Stanford Cars (Krause et al., 2013)) are object-oriented. What about the performance of the proposed method on Describable Textures (DTD) (Cimpoi et al., 2014)\uff1f \n\n(3) (Optional to reply) Could the proposed method be applied to temporal classification tasks, such as activity classification on UCF101?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6437/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698606752097,
        "cdate": 1698606752097,
        "tmdate": 1699636718559,
        "mdate": 1699636718559,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yETZQstXN5",
        "forum": "VLFhbOCz5D",
        "replyto": "VLFhbOCz5D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6437/Reviewer_QE2Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6437/Reviewer_QE2Q"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce Tangent Attention Fine-Tuning (TAFT) for fine-tuning linearized transformers. It can perform comparably with fine-tuning the original non-linear network in various downstream visual classification tasks. It enjoys several advantages compared to non-linear fine-tuning when it comes to model composition, parallel training, machine unlearning, and differential privacy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is the first work to propose an efficient method to linearize models in the Transformer family of architectures, which is meaningful. The paper is clearly written with many experiments."
            },
            "weaknesses": {
                "value": "1.since this is a fine-tuning method, please provide more fine-tuning methods for comparison (lora,adapter...) in table 3. \n2.I wonder how TAFT works when applied to LLM? maybe some experiments can be added.\n3.please derive in detail how to get the closed form expression in equation 5 and 6."
            },
            "questions": {
                "value": "see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6437/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762038607,
        "cdate": 1698762038607,
        "tmdate": 1699636718412,
        "mdate": 1699636718412,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "N7SJCz1jaB",
        "forum": "VLFhbOCz5D",
        "replyto": "VLFhbOCz5D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6437/Reviewer_Ef2Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6437/Reviewer_Ef2Q"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning linearized transformers that are derived by computing a First-order Taylor Expansion around a pre-trained initialization. The key contributions of the paper are as follows:\n\nEfficient Jacobian-Vector Product Calculation: The authors demonstrate that the Jacobian-Vector Product resulting from linearization can be efficiently computed in a single forward pass. This reduces the training and inference costs of linearized transformers to a similar order of magnitude as the original non-linear models, all while maintaining the same number of parameters.\n\nTAFT presents an efficient and effective method for fine-tuning linearized transformers obtained through Taylor Expansion. It maintains performance parity with non-linear models while providing advantages related to model composition, training efficiency, unlearning, and privacy. This has significant implications for the practical use of transformers in various downstream tasks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Efficient Jacobian-Vector Product Calculation: The authors demonstrate that the Jacobian-Vector Product resulting from linearization can be efficiently computed in a single forward pass. This reduces the training and inference costs of linearized transformers to a similar order of magnitude as the original non-linear models, all while maintaining the same number of parameters.\n\n2. Comparable Performance: When applied to various downstream visual classification tasks, the Tangent Transformer fine-tuned with TAFT performs on par with fine-tuning the original non-linear network. This suggests that the linearized version is a viable alternative without compromising performance.\n\n3. Convex Fine-Tuning Loss: The paper highlights that Tangent Transformers are linear concerning a new set of weights, resulting in a convex fine-tuning loss. This convexity offers several advantages over non-linear fine-tuning, particularly in terms of model composition, parallel training, machine unlearning, and differential privacy."
            },
            "weaknesses": {
                "value": "1. In Section 3.4, the author mentions basically the same things as in Section 2 Related work-Pravicy with no new theoretical analysis about differential privacy."
            },
            "questions": {
                "value": "1. How about interpretability? A detailed analysis of how a given training sample affects the learned model and the predicted results is given in LQF. Is it possible for the authors to provide an analysis of the interpretability?\n2. The authors detail the advantages of linear models in the introduction, but these advantages don't seem to be relevant to transformer, what are the advantages and disadvantages of linearizing the transformer model compared to linearizing ResNet?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6437/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6437/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6437/Reviewer_Ef2Q"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6437/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768476726,
        "cdate": 1698768476726,
        "tmdate": 1699636718301,
        "mdate": 1699636718301,
        "license": "CC BY 4.0",
        "version": 2
    }
]