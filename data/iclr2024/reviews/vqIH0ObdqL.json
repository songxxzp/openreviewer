[
    {
        "id": "zwxSAECxlB",
        "forum": "vqIH0ObdqL",
        "replyto": "vqIH0ObdqL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8089/Reviewer_fGug"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8089/Reviewer_fGug"
        ],
        "content": {
            "summary": {
                "value": "To investigate the capability of llm on inferring causality from correlations in text data, this paper proposed a novel corr2cause benchmark dataset. Apart from introducing the detailed dataset construction process, this paper also conducted extensive experiments over 17 llms to verify the effectiveness of the dataset. Meanwhile, this paper also verified whether llm can learn the causality skill after finetuning. In conclusion, this paper made an interesting attempt over casual inference ability of llms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThis paper design a novel corr2cause dataset to verify the causal inference ability of LLMs.\n2.\tThis paper provided a detailed dataset construction introduce, which makes the dataset very reasonable.\n3.\tThis paper conducted extensive experiments over 17 LLMs. Moreover, further experiments were also conducted to verify whether LLMs can learn causal inference ability by finetuning. The experiments are very convincing.\n4.\tThis paper made an early and interesting attempt over the causal inference ability of LLMs, which can inspire plenty of related work."
            },
            "weaknesses": {
                "value": "1.\tFor the dataset construction process, the authors leverage variables to represent the input data. However, what do variables represent? Entity in sentence? sentences or what? The authors should provide more details information. \n2.\tFor the causality discovery, the authors only provided the results of PC algorithm. However, the precision of PC algorithm is unclear. Whether the results are reliable is still unclear. The author should provide more details about how to ensure the confidence of the results generated by PC algorithm."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8089/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698716085325,
        "cdate": 1698716085325,
        "tmdate": 1699637001834,
        "mdate": 1699637001834,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CyjFILUwms",
        "forum": "vqIH0ObdqL",
        "replyto": "vqIH0ObdqL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8089/Reviewer_WuGy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8089/Reviewer_WuGy"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a new task called Corr2Cause. Specifically, the input example specifies the correlation and independence of a multi-variable closed system, and the model is asked to judge the causal relationships inside this system. For this task, this work constructs a dataset containing more than 400k examples for 2-6 variable systems. 17 models are evaluated on this task, including BERT-style NLI models, and GPT-style auto-regressive LLMs. None of the models perform well out of the box. While fine-tuning BERT-style NLI models can bring significant improvements, these improvements cannot be generalized to paraphrased examples or examples with different variable names. Finally, this work also presents a case study generating another version of the dataset with natural stories using GPT4."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Differentiating causality from correlations is an important task and it is one of the clear limitations of LLMs.\n2. The description of the construction process of the dataset is detailed, well-written, and sound. The background on basic causal inference can be useful for general readers.\n3. The evaluation covers a wide range of models. Both traditional BERT-style NLI models and GPT-style LLMs are evaluated for this task."
            },
            "weaknesses": {
                "value": "1. I'm concerned about the difficulty of this task. This task can be very difficult for the models because it also involves multi-step reasoning, understanding symbolic systems, knowing the definition of all the terminologies, etc. Despite the clear motivation of evaluating the model's ability to do causal discovery, for me, it is unclear which part is actually the bottleneck of this task. From the current evaluation results, I am convinced that the models cannot do multi-step reasoning to figure out causality from a symbolic system, but I cannot make more fine-grained claims.\n2. It's not a major point and I don't know if the authors have enough computation resources. But while the generalization experiment design is neat, it is not surprising that BERT-style models do not generalize well to paraphrases and new variables. The evaluation would be more interesting if the authors could fine-tune some relatively small-scale GPT-style models for this experiment.\n3. In order to have good coverage for the dev and test set, this work removes all the 2-3 variable examples from the training set. Removing simple examples can make compositional tasks like the task for this work very hard. I wonder if this design makes this dataset unnecessarily difficult, and whether such a dataset split is the best configuration for this task."
            },
            "questions": {
                "value": "1. Have you done any sub-sampling for the dataset construction? Or have you put all the possible examples and hypotheses in the dataset? This part is not clear to me.\n2. Do you have fine-grained evaluation results (like Table 6) for non-fine-tuned models? I find Table 6 to be very informative."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8089/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8089/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8089/Reviewer_WuGy"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8089/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698804349900,
        "cdate": 1698804349900,
        "tmdate": 1699637001717,
        "mdate": 1699637001717,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VkU6xhPPN8",
        "forum": "vqIH0ObdqL",
        "replyto": "vqIH0ObdqL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8089/Reviewer_iTjK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8089/Reviewer_iTjK"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a large-scale benchmark of causal reasoning capabilities of large language models.  This benchmark is constructed in an attempt to avoid many of the threats to the validity of previous causal LLM benchmarking approaches.  For example, it avoids reliance on numerical-data processing to focus on verbal statements; and avoids text that may have been included in the LLM's training data.  The paper uses the created benchmark set to evaluate the causal reasoning performance of a large number of open source, and proprietary LLMs, including GPT-4, and also runs experiments with fine-tuned versions of selected models.  The benchmark results indicate that LLMs do not perform causal reasoning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The construction of the Corr2Cause benchmark is a strong attempt to disentangle \"pure causal inference\" capabilities of language models from potential construct validity threats.  For example, avoiding potentially memorized scenarios, and avoiding a requirement of numerical analysis of existing data-driven benchmarks.\n\n- The Corr2Cause benchmark is evaluated on a broad set of language models.  The additional experiments on fine-tuned models provide additional insight."
            },
            "weaknesses": {
                "value": "- LLM performance in many tasks is strongly sensitive to system prompt instructions framing the LLM's role and general task.  Appendix A seems to indicate that there was no such \"system prompt\" used.  The paper and its findings would be strengthened if it explored whether such instructions (such as stating the rules of causal inference or including few-shot examples) would help performance.  \n\n- Similarly, different kinds of structured reasoning (e.g., chain of thought) have been found to improve performance significantly --- and also to sometimes give insight into why failures might be occurring, e.g., by verbalizing mistaken assumptions.  The prompt in Appendix A seems to only ask for a direct Yes/No answer, however.\n\n- What do we learn from studying \"pure causal inference\" vs other facets of causal reasoning? The paper clearly has a strong perspective on what it would mean for a LLM to perform causal reasoning, naming its chosen task \"pure causal inference\".  The paper would be greatly strengthened by discussing this perspective in more depth."
            },
            "questions": {
                "value": "I appreciate the task the authors are addressing, and the approach they have chosen.  The major weakness of the paper I am concerned about are those relating to how the models are prompted to improve LLM behavior (lack of system prompt, few shot examples, chain of thought reasoning) and error analysis of results (e.g., perturbing inputs, running shap analysis).  That is what gives me the most pause in interpreting the results of these benchmarks---would the LLM have succeeded with different prompting strategies?  \n\nBelow, I also list a few additional questions, more about the broader framing of the corr2cause benchmark.\n\n- Some work has found differential abilities/deficiencies in LLM's symbolic reasoning vs semantic reasoning [https://arxiv.org/abs/2305.14825].  Corr2Cause seems to rely heavily on an assumption that the LLM is able to reason symbolically.  Noting the \"natural stories\" variant proposed in sec 4.6 may address this, do the authors see this as a potential concern for interpreting the results of the benchmark results in the paper?\n\n- Why is \"pure causal inference\" the right task for evaluating the causal capabilities of large language models? For example, the formal reasoning approaches describes in Section 2 that guide the creation of Corr2Cause are relatively new inventions in human history.  But I think we'd hesitate to say that people could not reason causally before, e.g., DAGs were invented.  This seems like a good opportunity to clarify what underlying functionality we are trying to achieve and how pure causal inference vs other causal reasoning tasks relate to broader goals and purposes.  And to be perfectly clear, I think this is a fine task to evaluate but would like to learn more about the authors' perspectives on relative importance, etc. of testing different facets of causal reasoning.\n\n- A deeper discussion of corr2cause in the context of reasoning and inference tasks would be interesting, especially in regards to design choices made in this benchmark as compared to others.  How were those other benchmarks designed and how does that influence the design of corr2cause?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8089/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699206047845,
        "cdate": 1699206047845,
        "tmdate": 1699637001586,
        "mdate": 1699637001586,
        "license": "CC BY 4.0",
        "version": 2
    }
]