[
    {
        "id": "x2k6XdUHNK",
        "forum": "nonqjVFiE9",
        "replyto": "nonqjVFiE9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7311/Reviewer_G1mq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7311/Reviewer_G1mq"
        ],
        "content": {
            "summary": {
                "value": "This paper performed an in-depth analysis of the state-of-the-art CODA-Prompt for continual learning of pre-trained ViT. The authors attributed the problem as the mismatch in prompt representation between training and testing and feature shifting during inference. The authors then proposed Key-Query orthogonal projection to reduce dependence of old task queries on new task keys and introduced a prototype-based OVA loss to complement the Key-Query orthogonality. The proposed method achieves a surprisingly high performance,  even much higher than the joint training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The analysis of CODA-Prompt is very extensive, and the identified issues are reasonable. In particular, I appreciate the discussion about the different effects of training samples and test samples.\n\n2. The proposed method has strong motivation, which directly targets the identified issues of CODA-Prompt.\n\n3. The proposed method achieves a surprisingly high performance over widely used benchmarks."
            },
            "weaknesses": {
                "value": "1. My major concern is the surprisingly high performance of KOPPA, which is even significantly higher than the joint training performance by more than 10%. Since the joint training usually serves as the upper bound of continual learning, I would suggest the authors to provide an in-depth analysis and explanation of this abnormal phenomenon.\n\n2. From Table 3, the outstanding performance of KOPPA seems to be largely due to the OVA. The contribution of Key-Query orthogonal projection in Sec. 3.3 seems to be marginal. \n\n3. From Table 5, the performance improvements of KOPPA rely heavily on the number of prototypes. Although the authors have evaluated the effect of CE and OVA in Table 4, how about using the preserved prototypes to compute CE rather than OVA (i.e., identical to regular feature replay for continual learning)?"
            },
            "questions": {
                "value": "Please refer to the weakness. \n\nBesides, I would encourage the authors to discuss and compare with more advanced prompt-based baselines, such as [1] and [2]. While it is not required, such a discussion could bring this paper to a more advanced position, especially when the results of this paper seem to be far superior to [1] and [2].\n\n[1] Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality, NeurIPS 2023.\n\n[2] RanPAC: Random Projections and Pre-trained Models for Continual Learning, NeurIPS 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7311/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698068776510,
        "cdate": 1698068776510,
        "tmdate": 1699636873782,
        "mdate": 1699636873782,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6tuW1QT25i",
        "forum": "nonqjVFiE9",
        "replyto": "nonqjVFiE9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7311/Reviewer_WdvC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7311/Reviewer_WdvC"
        ],
        "content": {
            "summary": {
                "value": "The paper claimed that current prompt selection methods suffer from the mismatch in prompt representation between training and testing and feature shifting during inference. To this end, the authors proposed a MAML-inspired method to ensure an almost certain perpendicular constraint between future keys and past task queries, effectively eliminating feature shifts. The proposed method also contains a prototype-based One-Versus-All (OVA) component to boost the task classification head distinction. The proposed method shows accuracy much higher than joint training (upper bound of continual learning, using all the data for training)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The problem that this paper is trying to solve is well-motivated\n- The overall flow of the paper is easy to follow.\n- The proposed method shows very good empirical results."
            },
            "weaknesses": {
                "value": "- Some parts of the paper are unclear and confusing to me. Please refer to the Question section. \n- There could be mistakes in some calculations.  Please refer to the Question section. \n- The authors did not mention the limitations of their method and potential future work. The paper does not explore or discuss potential failure cases of the proposed methods. Understanding when and why the methods might fail is crucial.\n- typos\n   - \"alpha if corresponding weight vector\" \"if\" should be \"is\" right?\n   - \"It also mitigates the chance that the query q(x\u2032) is a past task uses the prompt\" two verbs in one sentence"
            },
            "questions": {
                "value": "- It's unclear how S_i is obtained and how Q^t is update from Q^{t-1}. Could the authors elaborate on them? \n- What's the size of Q. Do we keep one Q for all tasks or one Q per task?\n- \"It also mitigates the chance that the query q(x\u2032) is a past task uses the prompts of the current/future tasks, hence the prompts P^t\nfor the task t have more contribution to the prompts P_x of example x in this t\". This is about testing or training? if it\u2019s about training, we don\u2019t see old task sample x\u2019; if it\u2019s about testing, P^t\u2019s contribution to x only depends on q(x) and K^t right? Chance of q(x') use P^t does not impact P^t contribution to x?\n- It seems OVA is the key to the performance boost. In Table 3, the authors provided CODA + OVA. Is it possible for the authors to provide the performance of other baselines + OVA?\n- \"might trigger wrong task classification heads\". CIL only has one prediction head right? \n- the prototype sizes N \u00d7 T \u00d7 d (100 \u00d7 20 \u00d7 768)  should be multiplied by 4bytes (float), thus the size is around 6.1MB and image net image size is 224x224x3x 1bytes (uint8). So, it should be 40 images instead of 10, as stated in the paper, right? ACIFAR image is 32x32x3x1 = 3kb. For 10 tasks, the storage of the prototypes is the same as the storage of 1k images right?\n- The proposed method KOPPA outperforms JOINT by a large margin. JOINT is supposed to be the upper bound of CL. What's the main reason that KOPPA surpasses JOINT by such a large margin?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7311/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7311/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7311/Reviewer_WdvC"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7311/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789775593,
        "cdate": 1698789775593,
        "tmdate": 1699636873673,
        "mdate": 1699636873673,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gIVI3VYpj4",
        "forum": "nonqjVFiE9",
        "replyto": "nonqjVFiE9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7311/Reviewer_m1q8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7311/Reviewer_m1q8"
        ],
        "content": {
            "summary": {
                "value": "This paper extends the groundwork established by CODA-Prompt, aiming to resolve two primary concerns identified within it: (1) the mismatch in prompt representation between training and testing examples, and (2) the erroneous activation of the task classification head. To address these challenges, the authors introduce a look-ahead orthogonal projection optimization process for the former and employ a one-versus-all loss function for the latter. However, a significant drawback of this paper lies in the absence of important details. Furthermore, the working mechanism of the proposed methods seems not aligned entirely with the claims made by the authors."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The research focus on continual learning for pre-trained models holds significant significance.\n- The analyses delving into the issues concerning CODA-Prompt are particularly intriguing."
            },
            "weaknesses": {
                "value": "- Several vital details are missing, preventing a comprehensive evaluation of the proposed method.\n  * Specifically, how is $\\mathcal{Q}^t$ calculated? What is the specific sample size used for this calculation?\n  * How is $g_\\phi$ implemented? Is it similar to a cosine distance between the current prototype and previous prototypes?\n  * What are the hyperparameters employed, such as the prompt length and the prompt pool size? How does your method compare with CODA when these hyperparameters are adjusted?\"\n- The operational mechanism of the proposed method appears to diverge from the authors' assertions.\n  * Upon juxtaposing the results from Table 3 and Table 4, it becomes evident that the orthogonal projection component is minimally effective; the sole operational aspect is the One-Versus-All (OVA) loss, previously introduced by (Saito & Saenko, 2021). This observation is reasonable, given that the orthogonal constraint has already been applied in CODA, albeit between keys. It seems that the first identified issue has not been adequately addressed by the proposed method.\n  * Regarding the OVA loss, I have two hypotheses:\n    - The true effective component might be the prototypes of previous tasks, as indicated in Table 5. In this scenario, an additional ablation study, replacing $h_\\theta$ with a prototype-based classifier, is necessary. If successful, the unique contribution of this work, the OVA loss, may be rendered not valid any more.\n    - Its effectiveness could stem from the similarity between testing and training tasks, which allows for the identification of the most closely related and well-trained training task, thereby resolving the challenge. To validate this, additional experiments involving diverse data splits or datasets are essential to assess the practicality of the proposed methods beyond the current benchmarks.\"\n- The writing is disorganized with a lot of symbols randomly used, quite challenging to follow."
            },
            "questions": {
                "value": "See the first weakness listed above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7311/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699524521457,
        "cdate": 1699524521457,
        "tmdate": 1699636873571,
        "mdate": 1699636873571,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iAf7fVhzEZ",
        "forum": "nonqjVFiE9",
        "replyto": "nonqjVFiE9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7311/Reviewer_t1j7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7311/Reviewer_t1j7"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to address key challenges in Prompt Learning based Continual Learning (CIL) methods in class incremental setting. The authors highlight two potential issues in prior prompt learning based CIL methods: (1) Mismatch between per task final prompt representation during training and inference phase as prompt keys of one task could correlate with other tasks due to no explicit constraint and (2) The triggering of wrong classifier head due to the shift in the sample features of same task between training and inference. \n\nTo ensure that prompt representations of one task remains consistent during the training of prompts for upcoming tasks, the authors propose to enforce the orthogonality constraint between prompt keys of current task with the subspace of previous task. To address the issue of effective classifier head distinction, prototype based method is used which keeps prototypes from all tasks and eventually refine the classification task score. \n\nThe model is finetuned in class-incremental setting with the combination of above techniques and shows improvement against previous methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Strengths:\n\n(1) This paper has identified relevant issues and key challenges in prompt learning based CIL methods such as mismatch the between final prompt representation per task during training and testing. . Methods to improve these limitations can greatly enhance the resulting performance and advance the progress in prompt learning based CIL methods.\n\n(2) The idea of imposing orthogonality between prompt keys and previous task sub-keys is motivating. This aims to reduce the impact of prompt keys of new task to calculate prompt representations of previous task, resulting in correct prompt key activation specially during testing. \n\n(3) The method shows impressive results against previous methods and the proposed technique is motivated with fair ablation studies."
            },
            "weaknesses": {
                "value": "Weaknesses:\n\nMy main concern for this work lies in potential violation of rehearsal-free CIL experimental rules. \n1) In the first proposed module, the authors keep subspace Qt for upcoming new tasks to ensure the orthogonality constraint. This subspace potentially include sample information from previous tasks till t-1, which means that for the current task, information about the previous task samples is explicitly utilized and this possibly violates the rehearsal-free CIL setting where no information about previous task examples is known.\n\n2) Similarly, for the second proposed OVA technique, prototypes are stored from each task which are feature representations of training examples from each task. Therefore, the authors are indirectly utilizing a buffer in the feature space where the task ID of each task is completely known. I am finding it difficult to understand how this method does not belong to rehearsal-based CIL setting. \n\n\n3) The baseline CODA performs additional evaluation on DomainNet which is missing in this comparisons."
            },
            "questions": {
                "value": "Please refer to weaknesses section for questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7311/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7311/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7311/Reviewer_t1j7"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7311/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699585963510,
        "cdate": 1699585963510,
        "tmdate": 1699636873465,
        "mdate": 1699636873465,
        "license": "CC BY 4.0",
        "version": 2
    }
]