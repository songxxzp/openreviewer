[
    {
        "id": "DjmWqBYNaB",
        "forum": "XUZ2S0JVJP",
        "replyto": "XUZ2S0JVJP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6216/Reviewer_dfGd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6216/Reviewer_dfGd"
        ],
        "content": {
            "summary": {
                "value": "The paper presents and approach to significantly reduce the cost of querying LLMs by using a combination of LLMs of multiple sizes and deciding when and which model to query based on an auxiliary scorer model. The quality of scorer model seems key to achieving good performance, but the paper shows that even if much smaller sized backbone models for the scorer, the model is able to perform pretty good.\n\nThe paper presents multiple experiments to demonstrate the strength of the method showing that apart from the cost reduction, having such a router can also improve the overall quality of the responses given the routers ability to understand which LLM is suitable for which type of task."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Good contribution to key problems on affordability and environmental challenges from LLMs.\n- Pretty significant cost reduction while using very small scorer models to route the queries.\n- The setup is resilient to a good extent to data distribution shift even though it seems like the quality of the scorer model would be significantly dependent on the ability of the model to learn distribution of the queries, which is generally observed to be limited in terms of generalizability for small models."
            },
            "weaknesses": {
                "value": "- There is a need for an auxiliary model to be trained (scorer).\n- The scorer model could significantly determine the gains from the setup.\n- There is need for labelled dataset, and clients need to call the auxiliary model for each query, which will often not be on as performant hardware as the LLM APIs and could lead to latency hits."
            },
            "questions": {
                "value": "None, but feel free to add comments on the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6216/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698718987250,
        "cdate": 1698718987250,
        "tmdate": 1699636678105,
        "mdate": 1699636678105,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vYaW36OvzT",
        "forum": "XUZ2S0JVJP",
        "replyto": "XUZ2S0JVJP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6216/Reviewer_q9b6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6216/Reviewer_q9b6"
        ],
        "content": {
            "summary": {
                "value": "FrugalGPT proposes a layer on top of competing LLMs designed to minimize costs without hurting performance or latency, by adaptively selecting LLMs and evaluating their answers with smaller models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* This is a simple and brilliant idea, with tremendous potential impact for affordable accessibility of AI, minimized environmental impact, and general efficiency.\n* The authors leverage an intuitive observation (answer validation is easier than answer generation) and hierarchy of models to save costs and improve performance together.\n* Demonstration of improved latency is an added benefit"
            },
            "weaknesses": {
                "value": "* No justification for the chosen datasets. Do these map to diverse and realistic domains, tasks, and applications? The evaluation selection is fairly important to understand what realistic cost saving ranges are.\n* Evaluation datasets are all short outputs: classification or question answering. This is a slightly limited setting for the claims."
            },
            "questions": {
                "value": "* Are any of the datasets speculated to be in the training set of some of the models? Is there a way to test this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6216/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782349953,
        "cdate": 1698782349953,
        "tmdate": 1699636677986,
        "mdate": 1699636677986,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xqtrftUhly",
        "forum": "XUZ2S0JVJP",
        "replyto": "XUZ2S0JVJP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6216/Reviewer_3Fst"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6216/Reviewer_3Fst"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes FrugalGPT, a pipeline for saving API costs for a given task, by progressively calling more capable APIs and stoping when a judge (verifier) determines the answer is good enough. On 5 different benchmarks, it is shown that FrugalGPT can save costs by 52%-98% depending on the task."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* FrugalGPT is a simple and effective method for saving API costs. It has practical utilities.\n* The paper is in general well-written."
            },
            "weaknesses": {
                "value": "* The benefit of FrugalGPT over simple methods such as fine-tuning is unclear. \n  * Experiments are mainly conducted on QA and text classification tasks. From my understanding, a baseline of fine-tuning RoBERTa will achieve 90%+ accuracy on AGNews and CoQA. \n  * In terms of data efficiency, FrugalGPT also requires a training set to train the judge and the permutation function, and need extra information to compute MPI.\n  * From my understanding, calling API is most common when there are only few training examples, or the task is complex. It would be more convincing to show that FrugalGPT works well in these cases.\n* Many details are missing in the current version of the paper, see questions below."
            },
            "questions": {
                "value": "Questions:\n* Is FrugalGPT a task-specific or task-agnostic method? In the current experiments, are the parameters (permutation, judge model, threshold) trained separately for each task?\n* What's the cost of estimating MPI? Does that mean you need to run all LLMs over all training examples?\n* What data is used to train the DistilBERT judge?\n* For classification tasks, is it possible to use the DistilBERT judge directly for inference?\n* Since the judge model is doing a regression problem, what's the reason of using a 2-d output, and taking the maximum value of the last layer (normalized by softmax)?\n* As generation diversity is mentioned as a reason for improving performance, would it be possible to sample outputs from the same LLM multiple times, and ensemble their answers. As shown in https://arxiv.org/abs/2203.11171 (Figure 2) this would also create a cost-performance curve, and the performance grows with more calls. To demonstrate the contribution of FrugalGPT, such curves should also be included for comparison.\n* In Figure 1(d), I'm a little confused why the leftmost dot of FrugalGPT is still better than the cheap models. At this budget point, I would assume FrugalGPT makes one single call to one API, and thus the dot of FrugalGPT should overlap with one of the model. Please help me understand this better.\n\nSuggestion:\n* Regarding the distribution shift experiments, currently it's mainly done by changing the label distribution in the test set. It would be interesting to study and evaluate how FrugalGPT performs when there is distribution shift on the input text."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6216/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799620128,
        "cdate": 1698799620128,
        "tmdate": 1699636677866,
        "mdate": 1699636677866,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7GqKIUN7Fn",
        "forum": "XUZ2S0JVJP",
        "replyto": "XUZ2S0JVJP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6216/Reviewer_QsiX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6216/Reviewer_QsiX"
        ],
        "content": {
            "summary": {
                "value": "Authors propose a technique called FrugalGPT which selects which sequence of model APIs to call in order to reduce costs.  FrugalGPT has 3 different trainable components: router, answer-scorer and user budget. In experiments, they find that there is upto a 98% cost reduction, and has similar or slightly better results than using the strongest API used for experiments. In addition, they investigate the effect of data distribution, scorer quality changes and the incidental latency improvements that come along with cost."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The authors spend time motivating a novel problem faced by practitioners in a clear and engaging manner.\n- The paper is in general well-written and I did not have trouble following the proposed method.\n- The results are compelling and exciting.\n- The analysis done is useful to practitioners (latency improvements, distribution shift, scorer quality)"
            },
            "weaknesses": {
                "value": "The main weakness of the paper is the lack of analysis on what makes the method works. This method is relatively complex, with three different components, and there are several natural ablations that have not been done. While it is understandable that there are API costs to contend with, even using open source models like Llama with synthetically assigned costs for ablations could have been done. However, I still believe that the paper should be accepted.\n\nGiven that all related papers that are not speculative decoding are too recent for the authors to take into account (around/after ICLR deadline), it is fine not to have external baselines to compare to (speculative decoding provides better latency, but does not have the same cost improvements, afaik). However, some ablations on the method would be useful:\n- How much does the search space pruning help in practice, versus just ordering the APIs in increasing order of cost? (vs just anecdotes shown in the experimental section)\n- How much changing the threshold T effected results? Is the method very sensitive to T? This would be useful to practitioners.\n\nSome of the design space is indeed constrained by only having access to the output (see comments on scorer in Questions section). I believe writing this in a discussion somewhere in the draft would be helpful in answering questions a reader would have on a first reading."
            },
            "questions": {
                "value": "- Why is a scorer necessary? Given closed APIs don't give probability/logit outputs generally, this is not an option. However, when there is an option, is a simpler proxy like confidence or probability thresholding enough? (like using a much smaller learned calibration module at the query level Eg. https://arxiv.org/abs/2207.07411 https://arxiv.org/abs/2207.05221. A logit based score is more useful for routing at the token level). \n- While interesting, the latency improvments are modest relative to what is possible by combining the use of expensive/larger and cheaper/smaller models (speculative decoding, and https://arxiv.org/abs/2310.12126 get a 2-3x improvement in latency). Do you believe that additionally optimizing for this can result in similar improvements to latency and an improvement in cost (ie, either replacing cost budget with latency budget, or combining the two)? Or is this a limitation of relying on closed source models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6216/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6216/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6216/Reviewer_QsiX"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6216/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699660297232,
        "cdate": 1699660297232,
        "tmdate": 1700611895207,
        "mdate": 1700611895207,
        "license": "CC BY 4.0",
        "version": 2
    }
]