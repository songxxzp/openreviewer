[
    {
        "id": "wKfKfjBHrU",
        "forum": "sVs7lV691r",
        "replyto": "sVs7lV691r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3829/Reviewer_wy1y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3829/Reviewer_wy1y"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates to which extent language models (LMs) memorize their fine-tuning datasets, when fine-tuned for different tasks (summarization, medical dialog, QA, translation and sentiment classification). To detect memorization, the authors employ a plagiarism checker tool to detect whether models produce output that can be considered as one of three types of plagiarism (verbatim, paraphrase, idea) of text in the fine-tuning dataset, when prompted with prefixes from the same dataset.\nThe paper finds that the degree of memorization varies strongly based on the fine-tuning task, with summarization and dialog exhibiting much higher memorization than classification, translation and QA tasks. The paper further shows a correlation between attention distributions and the measured memorization scores, with high memorization corresponding to more uniform attention maps, and low memorization to more concentrated attention maps. Finally, the paper proposes to use multi-task fine-tuning to reduce memorization."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- Understanding the memorization behavior of LMs is an important problem with ramifications for privacy and copyright considerations. Since fine-tuning plays a key role in creating usable LMs, the paper tackles an important open problem by investigating how this step impacts memorization.\n- Differences in the fine-tuning task can strongly influence model behavior, so investigating how different fine-tuning objectives influence memorization is important."
            },
            "weaknesses": {
                "value": "- The paper has severe soundness issues, making the key findings rather unreliable.\n    1. The paper claims, that memorization varies based on the fine-tuning task, with summarization and dialog tasks exhibiting higher memorization than QA, translation and sentiment classification. It is very likely, however, that this result is confounded by the response lengths required for the different tasks. According to Figure 2, models generate in the order of > 120 output tokens for summarization and dialog tasks, but <= 6 output tokens for sentiment classification and QA tasks. Tasks that require long outputs are much more likely to produce sequences similar to those in the training corpus, just from a numerical perspective, and irrespective of memorization effects. The paper does not control for differences in output lengths of different tasks. Therefore, the results about differences in memorization behavior for different fine-tuning tasks are unreliable without controlling for this confounder.\n    2. The results on using attention maps as indicators of memorization appear similarly unreliable, because they also do not control for important potential confounders. Summarization and dialog (presumably high memorization tasks) are shown to have more uniform attention maps, compared to QA and sentiment classification, where attention is more concentrated. However, for the latter two tasks it may be sufficient to pay attention to a few keywords in the input, whereas summarization and dialog presumably require a more comprehensive understanding of the entire text. Therefore, the differences in attention patterns might simply be due to the nature of the different tasks, and not due to different degrees of memorization. For an apples to apples comparison, the authors should compare attention maps of instances with different degrees of memorization on the same task.\n    3. In Sections 4.2 and 5.2 the paper uses a model based on sparse coding to theoretically explain task-dependent differences in memorization and in the attention patterns, respectively. However, it is not clear that the sparse coding model is a meaningful approximation of the behavior of the investigated LMs. The sparse coding approach uses a simple linear model, and it is a priori questionable whether such a model is sufficient to approximate large and highly non-linear LMs. Sections 4.2 and 5.2 provide no empirical evidence showing that the sparse coding model is a meaningful approximation of the investigated LMs, so the theoretical arguments made in these sections do not appear to be meaningful.\n    4. The paper investigates memorization as a result of fine-tuning models. However, according to Figure 4, verbatim, idea and high-paraphrasation memorization appear to be in the same ballpark for T5-base and fine-tuned models. Therefore, it is not clear how much of the reported memorization is due to fine-tuning and how much is due to pre-training. To account for this, the authors should control for pre-training memorization in the memorization scores of fine-tuned models.\n- The paper has several clarity issues which make it difficult to understand some of the results.\n    1. The concept of idea memorization/plagiarism and how it is operationalized is not clearly defined. This makes it difficult to interpret the corresponding results.\n    2. How is fine-tuning for the different tasks performed? E.g. for sentiment classification, what is the target output ($y$) that the models are fine-tuned to predict? A single word/token, or a short sentence?\n    3. In Section 6, how do you use the base/non-fine-tuned models for summarization?\n    4. I have difficulty interpreting the memorization examples in Table 11 in Appendix D. Which prompts were used to generate the machine written text?\n- Contribution: The memorization detection approach and pipeline in the paper seems to be very similar to that of [A], upon which it builds. Given that the findings in the paper are unreliable (see above), the contribution is very marginal.\n\n[A] Lee et al., Do Language Models Plagiarize?, WWW'23"
            },
            "questions": {
                "value": "1. Is there only one split of each input $x_i$ into prefix $p_i$ and suffix $s_i$? If yes, the memorization behavior of the model might differ, based on the length of prefixes $p_i$, so it might be worth testing memorization for different prefix lengths. If no, the memorization ratio definition seems difficult.\n2. What is the rationale for studying attention at the last decoder-encoder attention block in Section 5.1? Do the results look similar for other blocks in the network?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3829/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3829/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3829/Reviewer_wy1y"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3829/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698859277829,
        "cdate": 1698859277829,
        "tmdate": 1699636340770,
        "mdate": 1699636340770,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sArK5aXiUO",
        "forum": "sVs7lV691r",
        "replyto": "sVs7lV691r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3829/Reviewer_BVwv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3829/Reviewer_BVwv"
        ],
        "content": {
            "summary": {
                "value": "1. The paper investigates the phenomenon of memorization within fine-tuned language models, specifically focusing on pre-trained T5 models applied to various downstream tasks, such as summarization, dialogue, QA, translation, and sentiment analysis.\n2. The primary goal of the study is to comprehend the variations in memorization across these diverse tasks and determine whether fine-tuned models exhibit task-specific memorization patterns.\n3. The paper introduces a categorization of memorization into three types: verbatim (exact memorization), paraphrase (memorization of alternative expressions), and idea (conceptual memorization).\n4. Additionally, the authors propose a novel method for estimating memorization using attention scores and demonstrate the possibility of reducing memorization through multitask fine-tuning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. I like the idea of simplifying the concept of memorization by linking it to the extent of information required for a given task, which aligns with the sparse coding model.\n2. The paper offers a systematic analysis that spans various tasks, providing valuable insights into memorization patterns across these tasks and comparing memorization across different language models.\n3. The examination of attention scores and the presentation of encoded attention maps explains the initial claim well.\n4. The observation that multitask fine-tuning can lead to reduced memorization highlights the practical implications of the study for model training and application."
            },
            "weaknesses": {
                "value": "1. Need for a more rigorous analysis of task specificity: The authors should consider potential confounding factors, especially the influence of the pre-training data used for the model, as this could impact memorization ratios. For instance, in the final section, the authors show that the T5 base model also shows a very high memorization ratio on the summarization task of multi-news which is 110 memorization ratio and the fine-tuned T5 model only goes to 222 which is twice more. However, the entire analysis of the paper before that was talking about how the summarization task requires more memorization because fine-tuning happened on that particular data set. I believe that a proper analysis of what was there in the pre-training data of a model and whether it confounds the memorization ratios is extremely important for such analysis and any conclusions about the task specificity or the nature of the amount of information required for the task. In fact, in the T5 base model that the authors chose, the entire C4 data set which it was trained on is totally public and the author should make such an analysis so that we can understand what is actually responsible.\n\n2. The rationale behind introducing attention discrimination as a metric is unclear. If it merely correlates with existing, computationally cheaper metrics, it may not be necessary to introduce an additional metric without a compelling justification.\n\n3. The experiment involving Flan T5, where the memorization ratio decreases after fine-tuning, lacks a clear explanation. The authors should provide a rationale for this unexpected result or revisit their analysis.\n\n4. To draw meaningful conclusions, it is essential for the authors to compare the memorization scores of the T5 base model before and after fine-tuning.\n\n5. The authors' choice of focusing on the last layers of encoder-decoder attention (as seen in Figure 2) appears somewhat arbitrary. An explanation for this choice and its potential impact on the correlation between attention scores should be provided.\n6. Expanding the analysis presented in Figure 4 to include tasks beyond summarization would enhance the comprehensiveness of the paper's findings.\n7. The concept of discriminating between different types of memorization: I believe idea memorization which encapsulates the fundamental sense of the initial information, should be a superset of all other forms of memorization. So, I do not understand this whole concept of categorizing memorization and what are we trying to achieve with this.\n8. Please use % and not %. the latter is very non standard for ML papers."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3829/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699281095924,
        "cdate": 1699281095924,
        "tmdate": 1699636340662,
        "mdate": 1699636340662,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dGTDXzZJYT",
        "forum": "sVs7lV691r",
        "replyto": "sVs7lV691r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3829/Reviewer_gHsj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3829/Reviewer_gHsj"
        ],
        "content": {
            "summary": {
                "value": "The paper studies memorization in fine-tuned language models. The authors observe a higher degree of memorization for models trained on summarization tasks, compared to simpler tasks like sentiment classification. Furthermore, they relate entropy in attention patterns to the degree of memorization in fine-tuned language models. Finally, they show that multi-task training reduces some memorization. They conclude their observations with a short theory on sparse coding models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The main strength of the paper lies in its simplistic approach to studying memorization in fine-tuned language models. The authors take insights from existing memorization works on pre-trained models to conduct their analysis. The differences between different tasks showcase different mechanisms that models employ for each of them. Furthermore, the short theory on the sparse coding model helps formalize a reader's intuition for the observations. Overall, I believe the paper is a positive contribution to the community."
            },
            "weaknesses": {
                "value": "I have a couple of questions about the experimental setup and the observations that the authors draw from their results.\n\n(a) What does x% memorization mean in the experiments? It would be great to demonstrate perfect and no memorization baselines to get a sense of the numbers.\n\n(b) How do the authors measure Idea memorization?  Furthermore, how do they differentiate Idea memorization from summarization (which is the task of a summarization-tuned model)? \n\n(c) How much do hyperparameters during fine-tuning affect the memorization results? Does a lower learning rate and longer training time result in more memorization? \n\n(d) \"k\", the length of the prefix tokens, was fixed for all the experiments. How much do the observations vary with varying \"k\"?\n\n(e) What is the decoding algorithm used for generation? Is it greedy decoding? If so, will the observations change with a more nuanced decoding algorithm, like nucleus sampling? This might allow the model to change its generation output, which will reduce the frequency of the generation of perfectly memorized solutions.\n\nOverall, I believe the paper is a positive contribution to the community. I am happy to interact further with the authors during the rebuttal period."
            },
            "questions": {
                "value": "Please check my questions in the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3829/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699577803652,
        "cdate": 1699577803652,
        "tmdate": 1699636340584,
        "mdate": 1699636340584,
        "license": "CC BY 4.0",
        "version": 2
    }
]