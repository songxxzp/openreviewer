[
    {
        "id": "D7VxefXKBP",
        "forum": "4FUa5dxiiA",
        "replyto": "4FUa5dxiiA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6236/Reviewer_dkQy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6236/Reviewer_dkQy"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the issue of risk-seeking policies in model-based RL-as-inference which arises due to learning optimistic dynamics.\nThe paper highlights how optimistic dynamics lead to risk-seeking policies in Figure 1.\nThe authors then introduce a risk parameter ($\\beta$) to the RL-as-inference objective that enables (nonlinear) interpolation\nbetween optimistic dynamics (for low $\\beta$) and the true dynamics (for large $\\beta$).\nThey then highlight connections between their ELBO (objective) and the risk-sensitive exponentiated utility for SafeRL.\nHence the paper's name, I guess.\nDrawing on connections between this objective and constrained optimization with Lagrange multipliers,\nthey propose a method to automatically adapt $\\beta$.\nIntuitively, their method restricts learning such that the KL divergence between the variational and prior dynamics remains\nbelow a threshold $\\epsilon$.\nThat is, the variational dynamics can be optimistic as long as they're close to the real dynamics.\nThey provide a theoretical analysis of their risk-sensitive method and\ndemonstrate that it overcomes issues of risk-seeking behaviour in a stochastic tabular environment.\nThey also show that it fixes issues with VMBPO in deterministic continuous environments (Mountain Car), which I found to be a surprising (and interesting) result.\nFinally, they show that it scales to high(ish) dimensional benchmarks such as Hopper/Walker2D/HalfCheetah."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall, I think this is a good paper.\nIt highlights a known (yet understudied) problem in the RL-as-inference framework (that of optimistic dynamics)\nand presents a novel solution to combat it.\nI found the paper well-written, I particularly liked Sec. 2 and Figure 1 as it was helpful for providing intuition for\nthe issue that the paper is trying to solve.\nThe theoretical analysis seems correct, although I have not checked it in detail.\nThe experiments are also well-structured and easy to follow."
            },
            "weaknesses": {
                "value": "I do not have any major weaknesses with this paper.\nNevertheless, I will provide some comments with the aim of helping the authors further improve their manuscript.\n\nThe dynamics models are learned using an ensemble of MLPs which parameterize Gaussian densities.\nThat is, each ensemble member learns a heteroscedastic noise model which captures the MDP's transition noise.\nAlthough not detailed in the paper, I assume the ensemble is a uniformly weighted mixture\n$q_{\\phi}(s_{t+1}\\mid s_t,a_t) = \\frac{1}{B} \\sum_{b} q_{\\phi_{b}}(s_{t+1}\\mid s_t,a_t)$\nwhere each member is given by,\n$q_{\\phi_{b}}(s_{t+1}\\mid s_t,a_t) = \\mathcal{N}(\\mu_{\\phi_{b}}(s_{t},a_{t}), \\Sigma^{2}(s_{t}, a_{t}))$.\nThe ensemble then captures the epistemic uncertainty arising from limited data.\nHow do you calculate the KL divergence between the variational and prior dynamics given that they are Gaussian mixtures?\nDo you assume that the output is unimodal and fit a single Gaussian to the mixture?\nSubtleties like this could impact the performance of VMBPO and $\\beta$-VMBPO so they should be detailed in the paper.\n\nAlso, how do you train the ensemble?\nIs each member's parameters initialised differently? Or is each member trained on different data?\n\nIn my mind, the notion of risk applies in uncertain environments.\nIn Fig. 4b, how is this dynamics model risky?\nIsn't it just optimistic and stuck in a local optimum induced by the optimism?\nI'm happy to be corrected here.\n\nI'm also wondering if Fig. 4 used an ensemble of MLPs without probabilistic heads?\nThe paper states that all experiments use probabilistic heads.\nBut this environment is deterministic so this feels like an odd thing to do.\n\nHow do you populate $\\mathcal{D}\\_{\\text{model}}$? I mainly wanted to know how the initial state is sampled.\nDo you sample randomly from the state space or do you sample a state from the replay buffer and then perform a rollout?\nI later found this in Alg. 1 in the appendix. Perhaps reference this algorithm from paragraph 2 of Sec. 3.3 so that it's clear at this point how $\\mathcal{D}_{\\text{model}}$ is populated.\n\nMinor things:\n- There is an abuse of notation which should be made clear. That is, $Q(s_t,a_t)$ in Eq. 2/3 is different to Eq. 5. It could be made $Q(s_t, a_t;\\beta)$ in Eq. 5 or there could be a sentence to explain that the notation is being abused.\n- The first paragraph of Sec. 3 should reference Appendix D and not just the appendix.\n- How is $V_{\\psi}'(s_t)$ different to $V_{\\psi}(s_t)$? I couldn't find this anywhere in the text.\n- Third paragraph of Sec. 3.3 should have $\\pi_{\\kappa}(a_t\\mid s_t)$ instead of $\\pi(a_t\\mid s_t)$?\n- Theorem 4.3 uses \"Thm 4.1\" and \"Thm 4.2\" but the rest of the paper uses \"Theorem 4.1\". Be consistent.\n- This sentence in the related work feels out of place. \"Variational approximate RL-as-inference by maximizing a lower bound on the marginal likelihood.\". Is part of the sentence missing?\n- Fig. 2 caption. \"Learn\" should be \"Learned\" or \"Learnt\".\n- In Fig. 2 it's not clear the grey area is a cliff. I'd suggest overlaying text saying \"CLIFF\" on the grey area.\n- Fig. 3 specifies $\\beta$ as a parameter. It might be clearer to use $\\beta^0$ to indicate that this is the initial value of $\\beta$. At the moment it gives the impression that $\\beta$ is fixed.\n- Fig. 3 uses \"Beta-VMBPO\" and Fig. 5 uses \"Beta_VMBPO\". Be consistent.\n- Appendix C.1 \"Pytorch\" should be \"PyTorch\".\n- Fig 2/3/4/5 axis numbers and labels are too small.\n- Fig 3/5 legend is too small.\n- In Fig. 5 the left/middle plots don't show the curves converging. Do they converge to the same value as SAC? It feels suspect not to show $\\beta$-VMBPO not converging to the same value as SAC."
            },
            "questions": {
                "value": "- Have you added more details on the ensemble of probabilistic MLPS dynamics models?\n- Have you addressed my issue with Fig. 5? That you don't show $\\beta$-VMBPO converging to the same value as SAC."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6236/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6236/Reviewer_dkQy",
                    "ICLR.cc/2024/Conference/Submission6236/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6236/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698685771136,
        "cdate": 1698685771136,
        "tmdate": 1700646117530,
        "mdate": 1700646117530,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LJojG61v6B",
        "forum": "4FUa5dxiiA",
        "replyto": "4FUa5dxiiA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6236/Reviewer_9Tpq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6236/Reviewer_9Tpq"
        ],
        "content": {
            "summary": {
                "value": "The study introduces a risk-sensitive approach to reinforcement learning (RL) by framing RL as Bayesian inference within a probabilistic graphical model, termed \"control as probabilistic inference.\" Conventional model-based control-as-inference does not consider risk-sensitive policy learning. The authors introduce the \u03b2-VMBPO algorithm, a risk-sensitive variant of the variational model-based policy optimization (VMBPO) algorithm. The novelty of this method lies in its dual descent optimization, which incorporates the risk parameter \u03b2 as an adaptive parameter. This approach is bolstered by a thorough theoretical analysis. Empirical evaluations validate the efficacy of this risk-sensitive methodology, demonstrating its advantage over traditional methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper establishes a clear connection between dual-descent and risk-sensitive variational inference, enabling the optimization of \\beta in a logical manner.\n1. Comprehensive theoretical analyses are provided."
            },
            "weaknesses": {
                "value": "1. The research's novelty appears limited. Previous studies have already introduced \\beta. If the paper's main contribution centers around a reinterpretation of \\beta through the lens of dual-descent, there's a need for a clearer distinction between this work and earlier research.\n1. Although the theoretical approach is interestingly anchored in the RL-as-inference framework, the paper falls short in its discussion regarding the interpretation of the parameter \\beta within the context of variational inference. A more in-depth exploration and dialogue from the variational inference standpoint would significantly enrich the paper."
            },
            "questions": {
                "value": "1. In the \"PRELIMINARIES\" section, Equation (5) appears to divide the reward by \\beta directly. While I recognize that this might not be the primary focus of the authors, readers could wonder how such a minimal change can imbue the method with risk sensitivity. A brief and intuitive explanation addressing this would be valuable.\n1. To my understanding, the pioneering work introducing variational inference to model-based reinforcement learning is \"VI-MPC\" by Okada et al. (2020). While the goals of the two studies might differ, I believe it's pertinent for the authors to reference this foundational work.\nReference:\nOkada, Masashi, and Tadahiro Taniguchi. \"Variational inference MPC for Bayesian model-based reinforcement learning.\" Conference on Robot Learning. PMLR, 2020.\n1. In Figure 1, an overlaid directed graph makes the illustration less intuitive. The representation seems muddled, especially when juxtaposed with the legends. It's evident that this figure could benefit from some revisions for clarity.\n1. Dual optimization emerges as a crucial component of this research. Given that several potential readers may be unfamiliar with this concept, adding a reference, such as a standard textbook on convex optimization or a pertinent tutorial paper, would be of great assistance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6236/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6236/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6236/Reviewer_9Tpq"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6236/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822740832,
        "cdate": 1698822740832,
        "tmdate": 1700605686277,
        "mdate": 1700605686277,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XDFyqYh5Kf",
        "forum": "4FUa5dxiiA",
        "replyto": "4FUa5dxiiA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6236/Reviewer_wKFu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6236/Reviewer_wKFu"
        ],
        "content": {
            "summary": {
                "value": "Briefly, the original VMBPO algorithm learns a dynamics model\nwhere the dynamics transitions are weighted according to the\nexponential of the value of the transitions, in accordance\nwith the control as inference approach.\nThis paper modifies the VMBPO algorithm to include an inverse\ntemperature parameter $\\beta$ so that the risk seeking behavior of\nVMBPO could be modulated. Another change is the way how the likelihood\nratio $q/p$ of the variational model to the true dynamics is computed:\nin the original VMBPO, they use a direct estimator for the ratio,\nwhereas in the current work, they learn two models, one for $q$ and\none for $p$. Moreover, they suggest to tune the $\\beta$ parameter by\nsetting a target KL divergence, and optimizing $\\beta$, to achieve the\ntarget in a similar way how the SAC algorithm optimizes their inverse\ntemperature parameter to achieve a target entropy.\n\nThe experiments included two simple tabular domains to show how\nmodulating $\\beta$ controls the risk seeking behavior of the method,\nand simple control tasks: Mountain Car, Half Cheetah, Walker2D, Hopper\n(note that the abstract says they tried DeepMind Control Suite, but this\nseems inaccurate, it seems the work only contains OpenAI Gym experiments)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The necessity of $\\beta$ in the VMBPO algorithm is clear, so the\napproach is well-motivated from this point of view.\n2. There was a study looking at how the tuning of the $\\beta$ works\nin an MDP domain that showed that the method is robust to initializations\nin this domain suggesting reliability of the method."
            },
            "weaknesses": {
                "value": "1. While the necessity of the $\\beta$ term is clear, why one\nshould use a VMBPO-based approach to begin with was not clear to me.\nThe majority of the discussion was about how VMBPO can be risk seeking,\nand how modulating the $\\beta$ term can prevent the risk seeking\nbehavior, but one can also avoid the risk seeking behavior by simply\nnot using VMBPO. I think the advantage of using the control as inference\napproach should also be explained and demonstrated.\n\n2. There were only 3 OpenAI Gym benchmark environments. It would have been\ngood to include more, e.g., Ant and Humanoid.\n\n3. The experimental results in HalfCheetah and Walker2d do not match the\npublished results in the original MBPO paper\n(https://arxiv.org/pdf/1906.08253.pdf, figure 2). In particular, HalfCheetah\nseems to go clearly over 10000 (reaching around 12000), whereas it barely\nreaches 10000 in this paper, also in Walker, the results go clearly\nabove 4000, but barely reach 4000 in this paper. The results on Hopper\nwere indistinguishable between MBPO and Beta_VMBPO. Also the improvement\nis quite marginal. I am not confident the method reliably improves over\nMBPO. Also, the number of random trials was 5, which is low.\n\n4. While there were experiments on smaller tasks aimed at explaining how the\nmethod works, the only statistics provided in the Gym tasks were the reward\ncurves. It would have been better to provide some other statistics that\ndemonstrate that the risk sensitivity control is working as intended.\n\n5. The computational time was not discussed. In particular, as you compute\ntwo models, $p$ and $q$ does that affect the computational time?\n\n6. I think the risk seeking behavior of VMBPO and that $\\beta$ modulates\nthis are obvious, and I think too much space was used for explaining\nthese points. I think the simple experiments on the tabular\ndomains and on the mountain car were redundant, as they do not show any non-obvious\nresult. For example, in the mountain car task, if the $\\beta$ parameter is set\nsufficiently large, then clearly the variational model should try to match the\ntrue dynamics, and the method should work. I would have liked to see a result\nthat requires tuning $\\beta$ and cannot be achieved without simply trying to\nlearn the true model.\n\n7. Equation 8 suggests an inequality constraint for the KL, yet in\nEquation 9, you are optimizing for an equality constraint (as is done\nin SAC, also there was no reference to SAC). I was not completely\nconvinced with this.\n\n8. The works lists the necessity to tune the KL constraint target\n$\\epsilon$ parameter as a limitation. However, it is also necessary to\nset an initial $\\beta$ value, as well as a learning rate for\n$\\beta$. While the experiments looked at the sensitivity to these\nparameters in a simple tabular task, the senstivity was not examined\nin the other tasks, so it is not completely clear how reliable the\nmethod is.\n\n9. VMBPO and the suggested Beta_VMBPO have more differences than simply adding in a beta value. I think there should have been ablation studies on the different components of Beta_VMBPO, e.g., the tuning of the beta value, etc. (performed in the OpenAI Gym tasks)"
            },
            "questions": {
                "value": "In this work, the ratio q/p is estimated by learning two models:\none for q and one for p, then taking the ratio. In the origianl VMBPO,\nthey use a direct estimator, v, for q/p. Vapnik's principle suggests\nthat learning a direct estimator is typically better than solving\nmore general intermediate steps. Have you compared with this method,\nand why did you choose to learn two models?\n\nPlease also feel free to respond to any of the listed weaknesses.\n\nBottom of page 12, there's a typo: \"p(.|s_t, s_t)\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6236/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698931899924,
        "cdate": 1698931899924,
        "tmdate": 1699636681450,
        "mdate": 1699636681450,
        "license": "CC BY 4.0",
        "version": 2
    }
]