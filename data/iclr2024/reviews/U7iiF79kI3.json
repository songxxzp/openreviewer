[
    {
        "id": "bseZG3n38a",
        "forum": "U7iiF79kI3",
        "replyto": "U7iiF79kI3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3981/Reviewer_ksX5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3981/Reviewer_ksX5"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors try to fill the hole in pre-training for Camera-LiDAR BEV perception. The authors apply contrastive learning on both LiDAR and camera modalities in two stages. The authors develop point-wise positive and negative pairs to balance both region- and scene-level contrasts."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "In sum, I think the proposed solution is simple and straightforward. Thus, I think the proposed approach is easily reproducible. The particular strengths I see are the following:\n1. Point-wise positive and negative pairs to balance both region- and scene-level contrasts.\n2. Strong empirical results across many datasets compared to prior work.\n3. The paper includes ablation experiments on several of the components"
            },
            "weaknesses": {
                "value": "I have some concerns about the proposed method:\n1. How does it identify the semantic-less and semantic-rich points? How does it calculate the 4th dimension of the points? What is the range of 4th dimension of the points?\n2. Is T2 identity transform in Fig. 1? If not, how does it generate the images from the original images? Were any related augmentations applied to the images, when the lidar points were changed?"
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3981/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698491406882,
        "cdate": 1698491406882,
        "tmdate": 1699636359774,
        "mdate": 1699636359774,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GoVN3s1O7i",
        "forum": "U7iiF79kI3",
        "replyto": "U7iiF79kI3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3981/Reviewer_fxyA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3981/Reviewer_fxyA"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a self-supervised learning method of Image and point-cloud input that consists of point-region contrast (PRC) and region-aware distillation (RAD). \nDiffering from the previous works, PRC utilizes both point- and region-level contrastive learning on point cloud. RAD aligns the feature maps of Images and points.\nThe proposed method is evaluated and shows substantial performance improvement on 3D detection and BEV map segmentation tasks on nuScene and Waymo datasets.\nAblation studies and robustness tests are also thorough to demonstrate the effectiveness of the proposed method.\nAfter the author discussion phase, I will adjust or fix my decision."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "[Originality]\n+ Differing from the previous works, the proposed method welly utilizes both point- and region-level contrastive learning on point cloud.\n\n[Quality & Significance]\n+ The proposed method is evaluated and shows substantial performance improvement on 3D detection and BEV map segmentation tasks on nuScene and Waymo datasets.\n+ Ablation studies and robustness tests are also thorough to demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "[Quality]\n- The proposed method adopts the BEVFusion architecture. Lidar backbone is PointPillars, and Image backbone is Swin-T. \nEvaluation of different Lidar/Image backbone models could have a high impact.\n- The necessity of negative samples of P_PLRC and P_RAPC is unclear. The ablation study (w/o negative samples in P_PLRC and P_RAPC) supports the necessity."
            },
            "questions": {
                "value": "Please see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3981/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3981/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3981/Reviewer_fxyA"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3981/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760809140,
        "cdate": 1698760809140,
        "tmdate": 1700594843088,
        "mdate": 1700594843088,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IzdBxliZwf",
        "forum": "U7iiF79kI3",
        "replyto": "U7iiF79kI3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3981/Reviewer_2HK8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3981/Reviewer_2HK8"
        ],
        "content": {
            "summary": {
                "value": "In this paper, a method for LiDAR-Camera BEV fusion is explored via contrastive and self-supervised training. Using currently fashionable machinery - Lift Splat Shoot type camera encoder, Voxelnet LiDAR encoder, Transfusion decoder, etc. - for feature processing in the BEV literature, the paper adapts contrastive learning ideas for the problem. At a high level, delineations are made between between point level and region level contrast, with heuristics (e.g. clustering) being applied to extract more discriminative features. \n\nEvaluations are presented to compare with other contrastive methods on the NuScenes and Waymo datasets. After contrastive (unsupervised/self-supervised) pretraining, the setup is fine tuned for object detection and segmentation tasks with varying amounts of training data to show efficacy of the pretraining step. Furthermore, they also investigate robustness to adversarial attacks (inserting fake objects at some distance from ego vehicle) and corruption of data (as might occur in bad weather, degraded sensors)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Contrastive training is generally less studied in the BEV perception literature. This work adds to the body of work present in the area. \n+ The methods are generalizable, and can be applied to any setup. \n+ Effectiveness is shown across modalities in camera, camera+lidar and lidar. This is convincing. I was particularly impressed with the saliency maps with and without pre-training."
            },
            "weaknesses": {
                "value": "- The paper is entirely empirical, and is as such a purely application based work. One may or may not take this as a weakness, of course. \n- On the same lines as above, the paper is heavy on tables, but I feel that qualitative analysis of where the improvement comes from is light. Some analysis through carefully designed experiments that show improvement with and without various fittings would shed insight. It appears that semantic pooling (feature rich vs feature less regions) plays a part from the figure 5, but I would like more examples of failure cases."
            },
            "questions": {
                "value": "- Could the authors explain how adversarial robustness is relevant in this context? \n- The clustering methods look rather empirical. More experiments on how they work in different cases would be useful. \n- Calibration error analysis: I think it would help to learn if the system can demonstrate robustness against calibration error, a common occurrence in autonomous driving setups. \n- Superfluous lines from possible prior submission (Appendix C, under 'Visualization') \n\n\"As we mentioned in the rebuttal,camera features trained from scratch are not salient to contribute to robustness improvement.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3981/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699115820366,
        "cdate": 1699115820366,
        "tmdate": 1699636359526,
        "mdate": 1699636359526,
        "license": "CC BY 4.0",
        "version": 2
    }
]