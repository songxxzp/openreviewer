[
    {
        "id": "2zhd3H3eQI",
        "forum": "wQCPHxtzGV",
        "replyto": "wQCPHxtzGV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8391/Reviewer_KiF7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8391/Reviewer_KiF7"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the issue of multi-modal policy generation and inference efficiency by proposing a novel offline imitation learning algorithm, RF-POLICY, to achieve a trade-off between policy diversity and model inference efficiency, especially compared to BC and DDIM.\nThe proposed method is based on Rectified Flow, and its training consists of two stages: one is to optimize the Rectified Flow to ensure the straightness so as to reduce inference time, and the other is to optimize the variance prediction network to determine the uncertainty of state so as to generate diverse policy when state's uncertainty or variance is high. While optimizing the Rectified Flow, it is proved that the model reduces to one-step generators, thus improving training and inference efficiency compared to DDIM. \nExperiments on a 2D maze environment and a simulated robot manipulation benchmark suggest that the proposed method can achieve high performance in task success rate, behavioral diversity and inference speed. \nThe main contribution of this paper is to derive an offline IL algorithm to improve the computational efficiency of diffusion-based policy generators while maintaining their ability of multi-modal policy generation."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper addresses an important problem of the application of diffusion-based policy generators in realistic scenarios, such as robot manipulation.\n2. The paper proposes an offline IL method to directly address the issue and provides a proof to explain why the Rectified Flow-based method can improve the training and inference efficiency, and meanwhile, in order to maintain the multi-model policy generation, the paper proposes a training objective by incorporating the state variance estimation into the loss of Rectified Flow optimization.\n3. The paper evaluates the proposed method with a new set of evaluation metrics not including the task success rate, but also the behavioral diversity and computational efficiency, to validate the method's ability. \n4. Empirical results show that the proposed method can achieve high performance on both computational efficiency and multi-modal policy generation."
            },
            "weaknesses": {
                "value": "1. Some claims of the paper are not adequately supported with evidence. For example, the experiments evaluate the method on three dimensions only on two benchmark datasets (the third is in the appendix and performance is comparable), so the last claim in the abstract may be doubted. Another example is that Assumption 1 is too strong and there is no evidence nor data distribution visualization to support it, thus the use of the proposed method with real data and non-expert data (e.g., low-return trajectories) is not convincing based on the limited results shown in this paper. However, the focus of this work is to improve the computational efficiency of diffusion-based policy generators, thus more experiments on other datasets as used in Chi (2023) should be conducted, especially the experiments on real-world robot benchmark if possible.\n2. The proposed method is compared with only BC and Diffusion Policy, though the baselines are representative in either field.  Performance improvements over other diffusion-based polices are missing, such as Diffusion-QL (Wang et al., 2023), Decision Diffuser (Ajay et al., 2023), Difusion BC (Pearce et al., 2023), etc, so the significance of this paper is doubted.\n3. Description of some figures and experimental results is confusing and need further clarity.\n\nReferences:\n[1] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. RSS, 2023.\n[2] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B Tenenbaum, Tommi S Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision making? ICLR, 2023.\n[3] Wang, Zhendong, Jonathan J. Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. ICLR, 2023.\n[4] Tim Pearce, Tabish Rashid, Anssi Kanervisto, David Bignell, Mingfei Sun, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, et al. Imitating human behaviour with diffusion models. ICLR, 2023."
            },
            "questions": {
                "value": "1. Does the textual description corresponds correctly to the picture in Fig.4?\n2. The measurement unit of training and execution time is different as stated in Results in section 5.3, so is it appropriate to exhibit the training and inference efficiency in the same figure as in Fig.5? In addition, the training and execution efficiency of experiments on 2D maze are not quantified clearly.\n3. How are the experimental results calculated in Table 1 and Table 2? Are they average scores across several seeds? And some implementation details are missing, such as epochs.\n4. In Table 1, results on Maze 5 only exhibit SR and Maze 6 only exhibits DS."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8391/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698140838565,
        "cdate": 1698140838565,
        "tmdate": 1699637044924,
        "mdate": 1699637044924,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DmxeMalP0F",
        "forum": "wQCPHxtzGV",
        "replyto": "wQCPHxtzGV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8391/Reviewer_J2ZQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8391/Reviewer_J2ZQ"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an imitation learning algorithm using rectified flow, a recent advancement in flow-based generative modeling combined with probability flow ordinary differential equations. The main idea is to improve the computational efficiency when action mapping from state is deterministic, in which ODE can be solved trivially. The resulting algorithm generates diverse policies yet avoid unnecessary computation whenever mapping from state to action has sufficiently low variance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed algorithm achieves a balance between computational complexity and diversity in generated policies."
            },
            "weaknesses": {
                "value": "1. The paper lacks empirical or mathematical validation for Assumption 1. The authors posit that most demonstration datasets for robotic tasks exhibit deterministic behavior (or uni-modal states), yet they fail to support this claim with experimental evidence.\n2. As delineated by Theorem 1, the RF-Policy loss function (equation 5) optimizes the flow model (ODE model) to generate deterministic (uni-modal) behaviors, evident when the loss function goes to 0 as the variance of action given state reaches 0. This prevents the model to generate multi-modal behaviors. This seems to counter the purpose of using diffusion models.\n3. The study omits a comparative analysis with established offline RL baselines such as CQL and BCQ, as well as other diffusion-based methodologies like Diffuser (Janner et al., 2022) and Diffusion-QL (Wang et al., 2022).\n4. The paper would benefit from a detailed proof of Theorem 1."
            },
            "questions": {
                "value": "1. How do linear flow models (linear ODE models), like RF-Policy, accurately encapsulate complex behaviors? Most existing methods have relied on non-linear SDEs, specifically DDPM, for policy estimation, yet this study utilizes a linear model. What rationale is provided for the superiority of this linear approach over its predecessors? (Related to equation 3)\n2. Figure 1 is intended to demonstrate that, unlike DDIM (an extension of DDPM), RF-Policy generates straight lines in deterministic areas (x < 0). However, I cannot see distinguishable difference between the red (DDIM) and blue (RF-Policy) lines in the figure. It could be considered a potential weakness of the paper.\n3. How does the variance prediction network determine whether a state is uni-modal or multi-modal? It is trained to estimate state variance using an offline dataset, encompassing both epistemic and aleatoric uncertainties. Given that the distinction between uni-modal and multi-modal states pertains to aleatoric uncertainty, how does the model address epistemic uncertainty?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8391/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8391/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8391/Reviewer_J2ZQ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8391/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698663937469,
        "cdate": 1698663937469,
        "tmdate": 1699637044814,
        "mdate": 1699637044814,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Xu4mwrfx8B",
        "forum": "wQCPHxtzGV",
        "replyto": "wQCPHxtzGV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8391/Reviewer_Hbo4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8391/Reviewer_Hbo4"
        ],
        "content": {
            "summary": {
                "value": "Recent papers in offline imitation learning substitute cross-entropy based behavioural cloning with diffusion-based models as a generative model. This paper proposes to use rectified flow instead, a formulation that, still using a mean-squared error objective, forces trajectories of the probability ODE to have no curvature whatsoever, sacrificing some generative accuracy (as it solves a whole family of transport problems) for maximum generation speed with 1 single function evaluation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is well written and straightforward to follow. Besides, it feels clear that combining rectified flow and offline IL should work in practice, based off intuition from the purely supervised RF case."
            },
            "weaknesses": {
                "value": "However, a big weakness in the paper consists in its novelty and magnitude of its contribution. Specifically, its unique contribution (apart from a log-variance additive regularizer) - compared to any standard diffusion-based offline IL approach - is to use rectified flow for acceleration, since the training time of the procedure is directly proportional to the NFE (number of function evaluations) required to perform inference for the diffusion model. It is indeed the case that rectified flow provides some of the best generative performance at 1 NFE amidst the class of extended diffusion-inspired models; but all that is proven here is that the approximation error in rectified flow is very compatible with the approximation error from offline IL. We feel the argument would be materially stronger if it was demonstrated 1. on a variety of more realistic domains than some of the toy domains (maze) treated here, for instance Atari-100k seeded with expert trajectories, which shouldn't require industrial levels of compute; and 2. most importantly, if an ablation study and exhaustive comparison with the performance of diffusion samplers specifically tailored to the low-NFE regime (UniPC [1], Heun and others [2] for pure samplers, even widening scope Consistency Models [3] or TRACT [4]) was performed. To me figure 5 simply means that DDIM 20 steps was used as baseline. This choice of DDIM feels arbitrary and it's not clear how much relative loss DDIM10, DDIM5 or another sampler would incur, thus minimizing any contribution that claims an NFE speedup. It is also not clear that Rectified Flow is a unique or best solution to this problem, as for say Consistency Models is also a class of diffusion-like models that could claim the same in the IL setting.\n\n[1] Zhao et al, UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models.\n[2] Karras et al, Elucidating the Design Space of Diffusion-Based Generative Models.\n[3] Berthelot et al, TRACT: Denoising Diffusion Models with Transitive Closure Time-Distillation.\n[4] Song et al, Consistency Models."
            },
            "questions": {
                "value": "Being cognizant of deadlines and compute constraints, which additional empirical evidence (section 5) could the authors provide in order to bolster their claims ? I would be willing to raise my score if the experiment section were more convincing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8391/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8391/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8391/Reviewer_Hbo4"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8391/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698680154662,
        "cdate": 1698680154662,
        "tmdate": 1699637044694,
        "mdate": 1699637044694,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RkSPsfAY1G",
        "forum": "wQCPHxtzGV",
        "replyto": "wQCPHxtzGV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8391/Reviewer_zCJH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8391/Reviewer_zCJH"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces RF-POLICY, an imitation learning algorithm that leverages Rectified Flow, a recent advancement in flow-based generative modeling. Traditional methods like Behavioral Cloning (BC) struggle with diverse behaviors, and while diffusion-based imitation learning addresses this, it does so at the cost of slower inference. RF-POLICY uniquely employs probability flow ordinary differential equations (ODEs) for policy generation, ensuring rapid inference without compromising on behavioral diversity. The core advantage of RF-POLICY is its adaptability: for uni-modal states, it behaves like a one-step generator, and for multi-modal states, it uses a multi-step approach. Empirical evaluations demonstrate that RF-POLICY offers superior performance across multiple metrics like success rate, behavioral diversity, and inference speed."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. RF-POLICY introduces a novel application of Rectified Flow in imitation learning, highlighting an adaptive mechanism to control generation efficiency based on demonstration variance.\n \n2. RF-POLICY efficiently addresses the trade-off between inference speed and behavioral diversity, which has been a challenge in traditional methods like BC and diffusion-based imitation learning.\n\n3. The paper not only introduces new evaluation metrics for imitation learning but also presents a detailed empirical analysis, demonstrating the algorithm's superior performance across various robotic problems.\n\n4. RF-POLICY is highlighted for its straightforward implementation and rapid training, providing practical advantages in real-world applications."
            },
            "weaknesses": {
                "value": "1. There is a theoretical gap between the objective at eq.~(8) and the implementation at Alg.1. The implementation uses a rectified flow to train the policy function, and uses another neural network to train the variance prediction network. In execution, the variance prediction network is used to determine the update iteration. \n2. Considering that the variance prediction network and the policy are trained separately, the performance gain especially in training is only a contribution of the rectified flow instead of the proposed solution as a whole."
            },
            "questions": {
                "value": "The paper is clearly written with good visualizations. However, the gap between the objective at eq.(8) and the implementation is not explained. \n1. I was wondering if there are any reasons supporting the implementation.\n2. I was wondering if using rectified flow to replace the DDPM in the DDIM models will lead to similar performances in both tasks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8391/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719718849,
        "cdate": 1698719718849,
        "tmdate": 1699637044577,
        "mdate": 1699637044577,
        "license": "CC BY 4.0",
        "version": 2
    }
]