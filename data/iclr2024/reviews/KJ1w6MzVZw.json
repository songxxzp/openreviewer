[
    {
        "id": "FCOjcvm0Bn",
        "forum": "KJ1w6MzVZw",
        "replyto": "KJ1w6MzVZw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6019/Reviewer_e2Vd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6019/Reviewer_e2Vd"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a time series pre-training framework that leverages the concepts of patching and masked token reconstruction, both of which have been extensively studied and utilized in time series modeling. The authors specifically put forth an adaptive patch resampling aimed at better aligning time series patterns across various domains. While this paper is well-structured in general, the core contributions and technical novelty appear to be constrained. Some assertions within the manuscript lack sufficient evidence (refer to my detailed comments below). Additionally, the overall presentation could benefit from further refinement. On the experimental front, related & important baselines are absent and the main experiemntal setting is ill-defined. While time series pre-training holds potential and merits exploration, I believe this work requires substantial improvements before it is fit for publication."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The motivation for time series pre-training is well established; I agree with the authors regarding the overall narrative.\n- The proposed patch resampling is technically feasible, and the ablation studies demonstrate the effectiveness of this design.\n- The overall pre-training & fine-tuning pipeline is well structured, with two important time series analytical tasks (i.e., forecasting and classification) undertaken."
            },
            "weaknesses": {
                "value": "- The overall technical novelty is limited. The primary contribution of this work lies in the concept of patch scoring, as presented in Eq. 2, and the subsequent two paragraphs. The overarching design can be seen as an extension of PatchTST. In the realm of time series pre-training, several recent studies, such as SimMTM, have delved into the concept of masked patch reconstruction.\n\n- The experimental settings are ill-defined. While this work emphasizes cross-domain adaptation, the evaluation datasets (& domains) substantially overlap with the source datasets (& domains) used in pre-training. I do not think this is a valid evaluation protocol for cross-domain adaptation.\n\n- The presentation could benefit from further refinement. For example, Fig.1 offers limited information, and upon examining just this figure and its caption, I have several related questions unsolved. Furthermore, numerous claims and technical assertions are not adequately backed by evidence or in-depth discussion. Please refer to the questions I've enumerated below for further clarity."
            },
            "questions": {
                "value": "**Questions & Detailed comments**\n\n1. I question the validity of the claim, \"... unlike text data, each individual timestamp may not provide enough semantic meaning about local temporal patterns of the time series.\" In natural language processing, doesn't a single token also sometimes fail to convey the full semantic information of a sentence?\n\n2. Regarding the construction of the pre-training set, how is the optimal combination of data samples from different domains determined? I found no discussion on this in the experiment section.\n\n3. How can Eq.2 effectively handle \"out-of-distribution\" samples from domains that were not encountered during pre-training?\n\n4. I'm not sure about what the authors intend by \"how good\" in the paragraph following Eq.2.\n\n5. In section 3.3, what are the fundamental differences between random masking and last token masking in time series pre-training? Is there any deeper analysis or extended discussion available?\n\n6. Fig1 is confusing. After reviewing Fig1 and its caption, I have several related inquiries: Q1: Why choose GRU over a linear projection as the patch embedder? Q2: How do patch scoring and pruning operate? Q3: What do you intend by h(1) to h(R)? Q4: Where is the self-supervised optimization highlighted?\n\n7. Several critical baselines, such as PatchTST, SimMTM, and TimesNet, are absent. Additionally, it would be advantageous to evaluate using datasets from domains not encountered during pre-training."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6019/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698299472903,
        "cdate": 1698299472903,
        "tmdate": 1699636646779,
        "mdate": 1699636646779,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qXU2MkqNa2",
        "forum": "KJ1w6MzVZw",
        "replyto": "KJ1w6MzVZw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6019/Reviewer_mzJa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6019/Reviewer_mzJa"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a cross-domain/dataset self-supervised learning approach to pre-train a time series model. They perform masked reconstruction with a Transformer architecture, introducing a dataset specific segmentation module to transform time series data into intermediate representations which are subsequently fed into the Transformer model. They pre-train the model on 7 datasets from various domains, and evaluate on forecasting and classification tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper tackles the ambitious problem of cross-domain/dataset pre-training for time series to learn a general model for time series tasks. They successfully pre-train such a model across a variety of datasets, and show decent performance across tasks and datasets."
            },
            "weaknesses": {
                "value": "1. Writing can be greatly improved. Abstract should be 1 paragraph. Mathematical notation is not clear -- many variables are not defined.\n2. Empirical comparisons are somewhat lacking. More recent baselines can be included (PatchTST, TimesNet for forecasting, CoST for self-supervised forecasting). More evaluation metrics can be presented (MAE, sMAPE, ...). \n3. The usefulness of the model is diminished with the dataset specific segmentation module. The model is unable to perform zero-shot forecasting or prediction tasks.\n4. Codebase in given link is incomplete. No script for training / predictions. README is empty, without instructions."
            },
            "questions": {
                "value": "-"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6019/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6019/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6019/Reviewer_mzJa"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6019/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698659288110,
        "cdate": 1698659288110,
        "tmdate": 1699636646648,
        "mdate": 1699636646648,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g4BvU6ELYZ",
        "forum": "KJ1w6MzVZw",
        "replyto": "KJ1w6MzVZw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6019/Reviewer_KRyA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6019/Reviewer_KRyA"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel method for pre-training time series models to be used in a wide range of downstream tasks.\nThe method relies on a segmentation of the input time series into possibly overlapping segments that are further encoded using self-attention.\nA segment selection strategy is used to focus on most informative parts of the time series, and automatically select segment lengths."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method relies on a simple yet interesting idea that is to extract important segments of varying lengths from time series such that each segment will be treated the way a token is processed in standard NLP pipelines.\nThe high-level presentation of the method is rather accessible (though the technical details are much harder to grasp due to problems with the notations, see below) and the experiments tend to validate the choices that are made."
            },
            "weaknesses": {
                "value": "There are many mistakes in the notations that make it hard (or even impossible) to fully grasp what is done at places.\nBelow is a list of such issues:\n* I do not understand the rationale behind Eq (4)\n    * Why taking the log of the SSL loss? If the SSL loss tends to 0 (which is probably what one targets at the end of training), then its log will have large gradient values, hence leading to unstable training\n    * What is $g(i, j)$ in Eq. 4? Do you mean $s(i, j)$? If so, why summing $\\log(\\mathcal{L}_{SSL})$ with the sum of scores?\n    * Why does the score rely on $z_i$ (is it $z_i$ or $z^{(i)}$ by the way?) and $z_j$ but not the full sequence of $z$ between indices $i$ and $j$, since recurrent units are known to have hard time catching long-term dependencies (even GRU units, to some extent)\n    * In Sec. 3.2, you write:\n        > The score s(i, j) for a subsequence from time-stamp i to j denotes how good the given segment is for the dataset.\n        * I do not understand this sentence. What does ``how good'' mean in this context?\n        * Also, given that the loss that is optimized operates on the aggregation of all scores, it is not clear how it could enforce large scores for selected segments\n* The use of $h(i)$ in Fig 1 is misleading, since it looks like $h(i)$ is the hidden representation for the $i$-th segment whereas in the text $h(i)$ is said to be the index of the last timestamp for the segment starting at time $i$, and it is said that some of these segments are pruned out, hence indices of the remaining segments should not be adjacent.\n\nAll these notations should be fixed for the reader to be able to understand the technical details of the paper."
            },
            "questions": {
                "value": "Some questions are asked in the \"Weaknesses\" section, below is a list of additional ones:\n* The text refers to \"aggregation\" but not much is said (in Sec 3.2 at least) on which aggregation function is used, why?\n\n* How does your method compare to state-of-the-art methods (ie. ROCKET, COTE variants, etc.) that do not use pre-training on the given classification tasks?\n\n>  While retrieving the optimal S(y(1...t)) is an interesting combinatorial optimization problem, [...]\n\nCould you elaborate a bit more on this interesting combinatorial problem, does it have known solutions? Do you have a way to assess if your approximation is a reasonable one or not?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6019/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698676767956,
        "cdate": 1698676767956,
        "tmdate": 1699636646496,
        "mdate": 1699636646496,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WeoHIWzcAr",
        "forum": "KJ1w6MzVZw",
        "replyto": "KJ1w6MzVZw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6019/Reviewer_jLeR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6019/Reviewer_jLeR"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a model which can be pre-trained on multiple time-series from diverse domains and can perform a wide range of tasks. Their proposed model is trained by masking a proportion of time segments. The authors argue that uniform length segments cannot scale across datasets and tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper attempts to pre-train a model which can solve multiple tasks on multiple time-series from diverse domains. To the best of my knowledge, this is amongst the first few studies which attempts to do this, demonstrating promising performance. \n\nI also appreciate that the authors compare their methods with some domain specific baselines."
            },
            "weaknesses": {
                "value": "I would encourage the authors to address the following to improve their paper: \n\n1. **Reproducibility:** While the code is available, many hyper-parameters important to reproduce the results are not mentioned in the manuscript. (1) As an example, the authors do not mention the proportion of time segments that they mask during self-supervised learning ($\\gamma$). (2) The number, length, ranges, number of channels etc. of time-series use for pre-training and evaluation are not mentioned either. (3) Furthermore, the authors compare \"training time (minutes) till convergence\" but fail to mention the compute infrastructure, what kind of time (wall clock?) are they measuring. \n2. **Clarity:** The paper is unclear and many statements are not rigorously or scientifically define. For e.g., (1) the authors claim in Section 3.2, that their segment score function measures \"how good the given segment is for the dataset\", but do not clarify what the notion of goodness is? The notion of goodness is also not immediately clear as the authors use a hyperbolic Tangent function as the scoring function. (2) The authors invoke $g(i, j)$ for the first time in Equation 4. Consequently, it appears to me it seems that the paper was put together in a hurry, without careful proof-reading. Also see Questions. \n3. **Claims:** The authors claim that variable sized segmentation is a key contribution of their work, but they only compare with time-step level segmentation. While they cite PatchTST, they do not compare with fixed length time-series segmentation, and hence it is unclear whether the contribution leads to significant gains over what seems to work (i.e. uniform time-series segmentation). \n4. **Baselines:** Some state-of-the-art forecasting baselines are missing, e.g., PatchTST and TimesNet from ICLR 2023, along with statistical forecasting methods such as AutoARIMA, AutoTHETA, AutoETS, Naive etc., and non-transformer-abed deep learning methods such as N-HITS and N-BEATS. \n5. **Experimentation:** (1) A pre-trained model should be able to solve tasks without any fine-tuning, especially since all the training parts of the datasets are observed during pre-training. (2) For smaller datasets, a large model trained from scratch is destined to under fit. Since the authors have not mentioned the size of the model, beating LPTM trained from scratch on a small dataset can be attributed to the model being too big for a small dataset. A smaller model might very well learn from scratch. \n\nMinor: \n1. Please fix the capitalization of the datasets. The diseases should be capitalized.\n2. Pease fix the citations using \\citep or \\cite, and \\citet, whichever is appropriate."
            },
            "questions": {
                "value": "1. What is the size of the model? How many layers of transformer? What is the number of heads? What is the size of embeddings? \n2. What is $g(i, j)$ in Equation 4?\n3. What is $\\gamma$? How are the segments sampled? \n4. What are the key differences between this work and \"PEMs: Pre-trained Epidemic Time-Series Models.\"?  \n5. See questions in Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There seems to be a lot of overlap with another paper that I had reviewed for NeurIPS 2023, which is also concurrently submitted to ICLR 2024 -- PEMs: Pre-trained Epidemic Time-Series Models.\n\nAt least one paragraph is completely copied -- please see the paragraphs in Section 3 of this paper and PEMs on Linear-probing and fine-tuning. \n\nI suspect there is a lot more overlap."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6019/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698783458834,
        "cdate": 1698783458834,
        "tmdate": 1699636646386,
        "mdate": 1699636646386,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JwXUQ2QAro",
        "forum": "KJ1w6MzVZw",
        "replyto": "KJ1w6MzVZw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6019/Reviewer_xd4P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6019/Reviewer_xd4P"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces an effective framework for pre-trained time series models and demonstrates strong empirical performance on diverse forecasting and classification tasks. The adaptive segmentation technique is a key contribution enabling learning from heterogeneous time series data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality:\n\nThe idea of pre-training time series models on diverse datasets from multiple domains is highly original and innovative. This enables knowledge transfer and improves efficiency similar to language and vision domains.\n\nThe adaptive segmentation module for handling diverse time series dynamics during pre-training is a creative technique and novel contribution.\n\nClarity:\n\nThe paper is clearly structured and easy to follow. The problem context, proposed method, experiments and results are presented logically.\n\nTechnical details are clearly explained and intuition behind design choices is well-articulated.\n\nTables and graphs effectively summarize key quantitative results.\n\nSignificance:\n\nThis work makes important strides towards general pre-trained models for time series, which might have high impact if the quality is good enough.\n\nThe ideas could inspire more research into techniques for pre-training on diverse time series data."
            },
            "weaknesses": {
                "value": "This paper has some obvious limitations which may lead the reviewer tend to reject it:\n\nThe model architecture used is quite straightforward - just a transformer encoder. Exploring more sophisticated temporal modeling architectures could be beneficial.\n\nMore in-depth analysis into the effect of pre-training like how the adaptive segments evolve could provide useful insights.\n\nAblations only evaluate the removal of components, could also analyze additions like other SSL tasks.\n\nHyperparameter sensitivity analysis is limited - how do factors like segment score thresholds affect performance?\n\nThough diverse, the pre-training datasets are still limited to a few domains. Expanding the data diversity could help.\n\nTheoretical analysis on how pre-training and adaptive segmentation provide benefits is lacking.\n\nComparisons to more sophisticated domain-specific models like those using additional covariates would be informative.\n\nAnalysis of computational requirements for pre-training is needed, especially regarding scaling up.\n\nTesting on a wider range of time series analysis tasks like anomaly detection could help show broad utility.\n\nLack of analysis of any negative societal impacts or limitations of the approach.\n\nLack of baselines: for PEMS-Bays and METR-LA, we have STGNN, StemGNN, GraphWavenet and so on; for ETT dataset, we have PatchTST, FEDformer. Timesnet and so on. The lack of such important baselines makes this paper hard to position.\n\nThe word of \"multi domain\" is overused: the reviewer don't see the specific module for multi domain setting. However, the \"a separate segmentation module for each dataset domains to capture varied sizes of segments that differ across datasets\" in page 3 Section 3.1 limits the ability of generalization on this model."
            },
            "questions": {
                "value": "See Weekness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "-"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6019/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6019/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6019/Reviewer_xd4P"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6019/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824514989,
        "cdate": 1698824514989,
        "tmdate": 1699636646241,
        "mdate": 1699636646241,
        "license": "CC BY 4.0",
        "version": 2
    }
]