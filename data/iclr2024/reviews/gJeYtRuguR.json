[
    {
        "id": "SvxynFCMgv",
        "forum": "gJeYtRuguR",
        "replyto": "gJeYtRuguR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1780/Reviewer_G39w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1780/Reviewer_G39w"
        ],
        "content": {
            "summary": {
                "value": "This paper combines two well-established methods, multi-exit neural networks and vision transformer (ViT) token reduction, to improve the efficiency of ViT. The background is that the prominent ViT token reduction techniques like EViT are based on removing unimportant tokens based on the attention scores that naturally indicate the contribution of the visual tokens to the final ViT prediction. The authors' motivation is that in the previous method, the ViT had no incentive to make sure the attention scores in the shallow layers aligned with the semantic importance of the visual tokens. To motivate the ViT to have such an incentive, the authors propose to exploit the multi-exit training method with ViT exits in the intermediate layers, which requires early class information fusion via the attention scores, thus ensuring the attention scores exhibit semantic importance properties. With the combination of the two techniques, the authors show a noticeable improvement compared to the baseline, especially when a significant number of tokens are removed or with more finetuning epochs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The method is clearly motivated. Token reduction based on the existing attention scores in ViT has been shown to be an effective method in reducing computational costs while maintaining most of the classification accuracy. The authors propose to add the pressure of extracting the classification information in the shallow layers via the multi-exit mechanism, which forces the attention scores in the shallow layers to focus on the important tokens as the scores are directly used as weights to gather the information for classification in the multi-exits. \n- The experiments are extensive and show the effectiveness in improving the classification accuracy over the baselines, especially with a longer training schedule. Experiments also demonstrate the proposed method's effectiveness from different perspectives, including different base models (DeiT/MAE), different model sizes, and different base methods (EViT/DiffRate).\n- The visualization seems to support the claim that adding multi-exit modules to the ViT makes the attention scores in the shallow layers aligned better with human perception (higher scores are allocated to more important tokens on the objects)."
            },
            "weaknesses": {
                "value": "The weaknesses are mostly minor issues, but it is important to address them to make the paper clearer and easier to understand.\n- The phrase \"Reduce Ratio\" is not a good term to indicate the ratio of how many tokens are kept. Please change to another term like \"keep ratio\" to make it clear.\n- Table 1 is not well explained. It took me a while to understand the setting of the experiment. The term \"Off-the-shelf\" is not immediately understandable. It would improve clarity by explicitly explaining the details of the experiments.\n- It would improve the readability of the paper by changing some words/notations to standard ones, e.g., CSE -> CE for cross-entropy.\n- There seems no appendix, but at the end of Section 4 first paragraph, it says \"See appendix for detailed experiment settings.\"\nPlease carefully proofread the whole paper to address these nuanced issues."
            },
            "questions": {
                "value": "- Is the loss $L_{me}$ in Eq (8) also added to the $L_{total}$? It is not clearly mentioned in the paper.\n- Figure 2 can be improved with better illustration and explanation. Specifically, the arrows from the $A^c$ to the [CLS] are somewhat confusing. And why do the patch tokens have fading colors from the bottom up?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1780/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698533519139,
        "cdate": 1698533519139,
        "tmdate": 1699636107554,
        "mdate": 1699636107554,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UxPSqegH7I",
        "forum": "gJeYtRuguR",
        "replyto": "gJeYtRuguR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1780/Reviewer_jWbQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1780/Reviewer_jWbQ"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces METR, a straightforward approach that combines multi-exit architecture and token reduction to decrease the computational burden of vision transformers (ViTs) while maintaining accuracy. The authors discover a discrepancy between the attention score of [CLS] and the actual importance of tokens in early ViT blocks, which negatively affects the performance of token reduction methods relying on this metric. The authors demonstrate that METR can improve existing token reduction techniques and achieve better results than state-of-the-art methods on standard benchmarks, particularly when using high reduction ratios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall, METR is a promising method that can help reduce the computational cost of ViTs while maintaining accuracy.\n\n- The paper is clear and well-motivated.\n- The idea is intriguing and demonstrates significant improvement compared to other baselines.\n- The evaluation is well-designed and highlights the core contribution in the design section."
            },
            "weaknesses": {
                "value": "- The evaluation demonstrates a notable improvement in accuracy compared to the baseline frameworks. It will be helpful to further demonstrate the reduction in latency with fewer FLOPs compared to other baselines.\n- It would be beneficial if the author could offer more insights in the method section, such as explaining how and why this design can enhance performance."
            },
            "questions": {
                "value": "Please see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1780/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723901660,
        "cdate": 1698723901660,
        "tmdate": 1699636107477,
        "mdate": 1699636107477,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u51sD7OTmH",
        "forum": "gJeYtRuguR",
        "replyto": "gJeYtRuguR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1780/Reviewer_Zdao"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1780/Reviewer_Zdao"
        ],
        "content": {
            "summary": {
                "value": "This submission introduces METR, a simple and effective technique for informed token reduction applied in Vision Transformer-based image classification. An analysis presented in the manuscript, demonstrates that the commonly used [CLS] token attention scores, acting as an importance metric for token pruning, are far more effective on deeper blocks in contrast to shallower ones. This is attributed to the long gradient distance from the task loss, traditionally applied at the end of the network. \n\nTo remedy this, the manuscript proposes the introduction of intermediate classifiers at training time, forming a multi-exit transformer model, in which all token reduction blocks are exposed to stronger task supervision. Upon deployment, early-exits are removed eliminating any speed overhead, while extensive experiments demonstrate the effectiveness of the proposed method across different models and in comparison with several baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-The manuscript focuses on the very interesting interplay between multi-exit models and token reduction. \n\n-Sec. 3.2, introduces a simple, yet effective solution to the examined problem. The discussion on the use of attention as an importance metric is insightful and many relevant works can benefit from these findings.\n\n-Experiments are extensive in terms of examined models and baselines, and validate the superiority of the proposed approach to the baselines. \n\n-The manuscript is generally well-written and easy to follow."
            },
            "weaknesses": {
                "value": "-The use of self-distillation loss for the multi-exit training (Eq.9), in place of traditional multi-exit loss of Eq.8, although effective, is not adequately motivated. Self-distillation is typically used to improve the accuracy of the trained exits, which is not a requirement here as these are discarded at inference time. The manuscript would benefit from a more insightful analysis of what motivated this design choice/ why do the authors believe this works better than the traditional approach.\n\n-Row(2) in Tab.3 seems to be the equivalent of row (4) in Tab.2, where multi-exit and token-reduction fine-tuning are jointly applied (instead of the two-stage ablation in Tab2). If this is the case, it can be deduced that token-aware fine-tuning notably reduces the effectiveness of the proposed approach, leading to significantly smaller gains even when aggressive token reduction takes place. This fact is separate from the commented fading of multi-exit effects after separate fine-tuning and needs to be further investigated/discussed in the manuscript.\n\nNote: An appendix is mentioned in the manuscript (Sec.4), but was not accessible to the reviewer."
            },
            "questions": {
                "value": "1. What motivated the use of self-distillation in place of traditional multi-exit training in the proposed setting? What are the authors' insights about the demonstrated effectiveness of this design choice?\n\n2. Is token-reduction aware fine-tuning indeed limiting the effectiveness of the proposed approach? If yes, this should be commented in the manuscript.\n\n3. In Tab.1,2,3 does \u201creduce ratio\u201d refer to number of tokens or GFLOPs? Both should be reported to get the full picture. \n\nMinor comments/ Presentation:\n-Notation in Sec.3 is quite loose. Consider defining the dimensionality of each introduced symbol (X,x,A,...).\n-Sec3.2: Symbol a^{c-p} is confusing.\n-Sec4.1.1: without incorporate -> without incorporating,  Subsequently, We (...) -> Subsequently, we (...)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1780/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1780/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1780/Reviewer_Zdao"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1780/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698753787829,
        "cdate": 1698753787829,
        "tmdate": 1700496411381,
        "mdate": 1700496411381,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6O7Yt850UY",
        "forum": "gJeYtRuguR",
        "replyto": "gJeYtRuguR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1780/Reviewer_9w9u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1780/Reviewer_9w9u"
        ],
        "content": {
            "summary": {
                "value": "This work has proposed a new token-pruning method, by integrating the multi-exit strategy into ViT. This work diagnoses the inconsistency between [CLS] attention and token importance in early ViT block, which degrades the performance of token reduction methods. To tackle this problem, this work introduces multi-exit architecture that allows the [CLS] token to gather information pertinent to the task in the early blocks. It also adopts self-distillation to improve the quality of early supervision. As a results, it achieves state-of-the-art performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "### Good Motivation\nThis work has adeptly identified and proposed solutions for a problem in the literature of token pruning method of ViT.\n\n### Novelty and SOTA performance\nTo address the inconsistency between [CLS] attention and token significance at the early blocks, the proposed method that incorporates multi-exit into ViT) is novel and it shows effectiveness clearly by achieving state-of-the-art performance.\n\n### Nice visualization\nThis work shows well-supportive visualization examples."
            },
            "weaknesses": {
                "value": "No exists."
            },
            "questions": {
                "value": "It would be better to shows the GPU-throughput and compare it with those of SOTA."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1780/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698927394873,
        "cdate": 1698927394873,
        "tmdate": 1699636107302,
        "mdate": 1699636107302,
        "license": "CC BY 4.0",
        "version": 2
    }
]