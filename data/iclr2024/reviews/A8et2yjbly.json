[
    {
        "id": "9OxD7uvqOl",
        "forum": "A8et2yjbly",
        "replyto": "A8et2yjbly",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1907/Reviewer_JDzU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1907/Reviewer_JDzU"
        ],
        "content": {
            "summary": {
                "value": "This paper includes AUTHOR CONTRIBUTIONS and ACKNOWLEDGMENTS sections in the main text. Therefore, this paper violates the double-blind reviewing rules."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "-"
            },
            "weaknesses": {
                "value": "-"
            },
            "questions": {
                "value": "-"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "-"
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1907/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1907/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1907/Reviewer_JDzU"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1907/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698308252084,
        "cdate": 1698308252084,
        "tmdate": 1699636121087,
        "mdate": 1699636121087,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gcKiGlYqyp",
        "forum": "A8et2yjbly",
        "replyto": "A8et2yjbly",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1907/Reviewer_t1sC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1907/Reviewer_t1sC"
        ],
        "content": {
            "summary": {
                "value": "Existing visible-infrared person re-identification(VI-ReID) methods often overlook the disparity between pre-trained data (ImageNet) and VI-ReID data. This paper introduces a novel approach called Cross-Modal Masking Pre-Training (CMMP) for the recognition of visible and infrared humans. Notably, performance enhancements were realized through an initial pre-training stage involving masking and reconstruction, followed by fine-tuning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper introduces a novel approach to the VI-ReID task, focusing on model pre-training for the first time.\n2. This paper has developed a lightweight pre-training method and demonstrated its effectiveness in enhancing VI-ReID."
            },
            "weaknesses": {
                "value": "While the focus of this paper is innovative, the experimental results indicate a weakness in its performance."
            },
            "questions": {
                "value": "1. In this paper, the pre-training data and fine-tuning data are identical. If we conduct pre-training based on ImageNet and then proceed with fine-tuning, does this approach offer any performance advantages compared to the optimal method?\n2.This paper mentions \u201cWhen attempting to predict all three cross-modality images simultaneously, the model\u2019s performance is notably worse, considerably lower than when predicting other targets.\u201d Does this observation suggest that pre-training doesn't necessarily require infrared image data, and instead, we could use visible and generated images from ImageNet datasets or large ReID datasets for the pre-training process?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1907/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698321344307,
        "cdate": 1698321344307,
        "tmdate": 1699636121022,
        "mdate": 1699636121022,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lbTsFpeww0",
        "forum": "A8et2yjbly",
        "replyto": "A8et2yjbly",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1907/Reviewer_HM7x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1907/Reviewer_HM7x"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a mask-based pretraining strategy for visible-infrared person re-identification. Based on the extended images via random channel exchange, the proposed CMMP employs a mask-sharing mechanism to simultaneously mask three types of images. Subsequently, the model is encouraged to reconstruct the masked regions via a lightweight decoder."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Good writing: I can easily catch the main story of this paper, and the organization of the paper is clear."
            },
            "weaknesses": {
                "value": "1.\tLimited novelty: I can hardly catch the significant contribution and inspiring insights for this field. Since both the model architecture and data generation method in the paper are based on existing methods, thus, what is the core value or unique contribution of this paper?\n2.\tLack of citations to recent literature: This paper lacks citations to recent literature, with only one paper from 2021 mentioned in the related work section.\n3.\tOutdated experimental comparisons: The experimental results lack comparisons to the latest work. Since this is a submission to ICLR 2024, it is inappropriate to rely solely on experimental comparisons with those prior to 2021. \n4.\tInaccurate statement about SimMIM: The statement that SimMIM only encodes the unmasked parts is incorrect. SimMIM also encodes the learnable mask features. \n5.\tLack of clarity in explanations: The paper repeatedly mentions \"misalignment inherent in the VI-ReID dataset\" without providing an explanation of the negative impact it causes or whether any existing work has addressed this issue. Additionally, the descriptions of pretraining and finetuning processes are not clear. It is essential to provide clear and detailed explanations to enhance the understanding of the proposed method.\n6.\tIt is not much of a contribution for firstly introducing the mask reconstruction paradigm into VI-ReID. Clarifying the rationality and necessity of introducing it into the task is more important and meaningful.\n7.\tThe and writing and reference format are sometime irregular, such as \u2018Ye et al.Ye et al. (2021)\u2019 in Section 2.2, \u201810 experiments\u2019 in Section 4.1 and \u2018288*144 size\u2019 in Section 4.2.\n8.\tFor the ablation study on the masking strategy, the experimental comparisons with more masking strategies should be provided, rather than only the random mask.\n9.\tThere is a lack of insight into the experimental results, and the view is superficial analysis on results."
            },
            "questions": {
                "value": "1.\tBoth MAE and SimMIM, the most relevant models to this paper, adopt the ViT as the backbones. However, the proposed CMMP model uses a CNN architecture. Could the authors provide the reason for this choice? Did you try to apply CMMP on the ViT?\n2.\tOverall, the encoding strategy of CMMP seems more similar to SimMIM. Have the authors attempted the MAE-like strategy that only encodes the unmasked parts? Given that CMMP has an 80% mask rate, using the MAE-like encoding could significantly improve training efficiency.\n3.\tThe paper mentions the statement multiple times -- \u201cDue to the misalignment inherent in the VI-ReID dataset, CMMP focuses its reconstruction efforts solely on the visible image and the generated images after masking\u201d. Could the authors explain what this misalignment refers to and why reconstruction is not performed on the infrared image?\n4.\tCould the authors provide a more detailed explanation of the finetuning process? Specifically, what are the differences between pretraining and finetuning in this paper?\n5.\tThe encoder processing flow should be explained more thoroughly. Figure 2 shows three input images, while Figure 3 only depicts two input images to the encoder. This discrepancy raises concerns about inconsistency. Please clarify this discrepancy and ensure the consistency of the depiction.\n6.\tIt is recommendable to include more figures in the experimental section. After all, compared to the tables, graphical presentation can provide a more intuitive and direct comparison."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1907/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1907/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1907/Reviewer_HM7x"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1907/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698751194783,
        "cdate": 1698751194783,
        "tmdate": 1699636120943,
        "mdate": 1699636120943,
        "license": "CC BY 4.0",
        "version": 2
    }
]