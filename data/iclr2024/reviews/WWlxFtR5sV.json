[
    {
        "id": "du1EznJn71",
        "forum": "WWlxFtR5sV",
        "replyto": "WWlxFtR5sV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8048/Reviewer_Wj3M"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8048/Reviewer_Wj3M"
        ],
        "content": {
            "summary": {
                "value": "The paper mainly explores how the convergence of training in Physics-Informed Neural Networks (PINNs) is dependent on the condition number of the operators constituting the Partial Differential Equations (PDEs). From the perspective of novelty, the idea posited in this paper is quite interesting. In terms of conclusions, the paper articulately presents the theoretical result that a lower condition number leads to better convergence of PINNs. Moreover, a preconditioning method for linear PINNs is proposed in the paper, significantly reducing the condition number and accelerating the training process. \n\nOverall, I appreciate the theoretical analysis conducted in this paper, which I believe meets the submission standards for ICLR. However, the experimental section is somewhat weak (as noted in the drawbacks), and I believe that more extensive experiments should be conducted to validate the theoretical analysis presented in the paper. Additionally, some related works concerning hard constraints were overlooked, specifically the following two papers:\n\n**Reference**\n1. A Unified Hard-Constraint Framework for Solving Geometrically Complex PDEs (https://arxiv.org/abs/2210.03526)\n2. PFNN: A Penalty-Free Neural Network Method for Solving a Class of Second-Order Boundary-Value Problems on Complex Geometries (https://arxiv.org/abs/2004.06490)"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea presented in this paper is novel, to the best of my knowledge.\n2. The paper's approach of using condition number analysis to study the convergence of PINNs is innovative, providing fundamental theoretical guarantees for PINN convergence.\n3. The writing of most sections is easy to understand. Overall, the theoretical analysis part of this paper is very engaging to read."
            },
            "weaknesses": {
                "value": "1. The experimental section of the paper is quite weak. It solely provides a method for reducing the condition number in linear PDEs using PINNs based on the theoretical analysis presented, which considerably narrows the applicability of the proposed method.\n2. Some parts of the paper are not clearly written, such as the calculation of the preconditioner matrix P, and the discussion regarding complexity appears to be vague.\n3. The logic in the latter experimental and discussion sections is somewhat jumbled. For instance, the last two sections, in my opinion, should be consolidated into one section as related work."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8048/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698568167894,
        "cdate": 1698568167894,
        "tmdate": 1699636994761,
        "mdate": 1699636994761,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EPCHeMs9zT",
        "forum": "WWlxFtR5sV",
        "replyto": "WWlxFtR5sV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8048/Reviewer_E6qc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8048/Reviewer_E6qc"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the training dynamics of gradient descent (GD) for solving partial differential equations (PDE) with physics-informed neural networks (PINN). Theoretically, the authors analyze a linearized form of the GD update. Let $\\mathcal{D}$ denote the differential operator associated with the underlying PDE. The authors show that the convergence rate of GD depends on the conditional number of the Hermitian square operator $\\mathcal{D}^\\ast\\mathcal{D}$, which implies that preconditioning the operator $\\mathcal{D}^\\ast\\mathcal{D}$ helps accelerate the training process. Empirically, the authors propose preconditioning strategies for several PDEs and provide numerical experiments to exhibit the effectiveness of using preconditioning during training."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper includes both theoretical analysis and numerical experiments to support the argument that preconditioning is essential for training PINNs to solve PDEs. For numerical studies, the authors provide plots of both the condition number and loss function to exhibit that preconditioning does improve the training efficiency. Moreover, the authors also explain why preconditioning serves as a general framework for improving training efficiency by showing that several training strategies, such as tuning the regularization parameter, enforcing hard boundary conditions, using high-order optimizers and domain decomposition, can be treated as preconditioning the operator in some way."
            },
            "weaknesses": {
                "value": "(1) The reviewer's first concern is that this paper doesn't discuss its relation to previous work on preconditioning. Specifically, one line of work that researchers within the ML for PDE community have studied is \"Sobolev Training\", where the main idea is to incorporate the Sobolev gradients of the training objective into the loss function. Examples of theoretical and empirical work in this direction include [1] (theory) and [2, 3, 4] (empirical studies). In fact, sobolev training can be interpreted as a form of operator preconditioning, which we will explain it briefly below: Consider the task of solving the PDE $\\mathcal{D}u = f$ over the domain $\\Omega$ with zero boundary condition via PINN, where $\\mathcal{D}$ is some differential operator. Let $E(u) :=\\frac{1}{2} \\|\\mathcal{D}u - f\\|_{L^2(\\Omega)}^2$ be the standard $L^2$ loss function, whose associated gradient flow is $u_t =  -\\frac{\\delta}{\\delta u}E(u) = -\\mathcal{D}^\\ast(\\mathcal{D}u-f)$. \n\nIn contrast, if we consider solving the same problem under the $H^2$ loss $E_{S}(u) := \\frac{1}{2} \\|(I-\\Delta)(\\mathcal{D}u - f)\\|_{L^2(\\Omega)}^2$  (Sobolev training), we have that new gradient flow is given by $u_t =  -\\frac{\\delta}{\\delta u}E_S(u) = -\\mathcal{D}^\\ast\\mathcal{P}^\\ast\\mathcal{P}(\\mathcal{D}u-f)$, where the differential operator $\\mathcal{P} = I - \\Delta$. This implies that training under a loss function that includes Sobolev gradient is equivalent to preconditioning the Hermitian operator $\\mathcal{D}^\\ast\\mathcal{D}$ via the differential operator $\\mathcal{P}$. Another approach for reducing the condition number is to reformulate the problem of minimizing the $L^2$ loss as a min max optimization problem, which was proposed in recent work [5]\n\nMoreover, the idea of using preconditioning/sobolev training to speed up optimization/training of a prespecified model in machine learning and scientific computing is not new either. A few examples of previous work include using preconditioning in neural network training [6, 7], application of Sobolev preconditioned GD in image processing [8, 9, 10] and graphics [11, 12], etc. Hence, the authors are encouraged to have a subsection discussing how this paper is related to previous work on the usage of preconditioning/sobolev training in ML for PDE and related fields. \n\n(2) Secondly, given that previous work [1, 2, 3, 4, 5] have already studied how sobolev training, which is equivalent to preconditioning, accelerates the optimization process of solving PDEs with PINNs, the reviewer thinks that this paper's novelty seems to be a bit limited. \nIt seems to me that sobolev training is a natural and simple method to implement (The main idea is to use integration by parts to kill any term involving the derivatives of the parametrized PDE solution $u_{\\theta}$). However, for the method proposed in this work, it seems hard to find a good preconditioner $P$ to reduce $\\kappa(A)$, especially when the differential operator $\\mathcal{D}$ and the neural network model $u_{\\theta}$ becomes a bit complicated. Consequently, to complement the numerical experiments exhibited in this paper, it would probably be meaningful to test the proposed method's performance on more complicated nonlinear PDEs, such as the Allen-Cahn equation, the Schrodinger equation and the Navier Stokes equation used in [13]. \n\n(3) Thirdly, the presentation of this paper can possibly be improved from a few aspects: Firstly, the review of ML-based/Physics-Informed PDE solvers in the second paragraph of Section 1 seems to be incomplete. A lot of recent work in this field seems to be missing. The authors are encouraged to take a look at the first part of Subsection 1.1 in [1] for a complete review of related work in this field. Secondly, it's probably better to move the subsection of related work from Section 4 to Section 1 (Introduction), as this might help the readers under the main contributions presented at the end of the introduction in a better way. It would also be helpful to include a subsection of the paper's organization and a subsection discussing the mathematical notations used in this paper within Section 1 (Introduction). Moreover, there seem to be some typos in the proofs presented in this paper. A non-exhaustive list of typos is given below:\n\n1. In (2.5), a factor of $\\frac{1}{2}$ is missing in front of the Hessian term $H_k$. Similarly, the $\\frac{1}{2}$ factor is also missing in the expansion of the two gradients $\\nabla_{\\theta}R(\\theta_k)$, $\\nabla_{\\theta}B(\\theta_k)$ and the error term $\\epsilon_k$ in Appendix A.1. \n\n2. Based on the definition of the vector $C$, the sign in front of $C$ in (2.7), (2.8) should be negative, i.e, $C$ should be replaced by $-C$. Also, the expression of $\\theta$ in Theorem 2.3 should be $\\theta = \\theta_0 - A^{-1}C$. Similarly, $\\eta \\epsilon_k$ should be replaced by $-\\eta \\epsilon_k$ in (2.7)\n\n3. In the proof of Lemma 2.1 presented in Appendix A.2, the term $\\epsilon_k$ should be replaced by $\\eta \\epsilon_k$ in the first line (A.2) of the proof. Therefore, the proof and the main result of Lemma 2.1 also need to be slightly modified."
            },
            "questions": {
                "value": "1. Would it be possible for the authors to discuss the main difference between the preconditioning method proposed in this paper and sobolev training? Is there any numerical example that your method outperforms (higher accuracy, lower computational cost, etc) sobolev training when the models used for parametrizing the PDE solution are the same?\n\n2.  The authors mentioned that there are empirical studies showing that first-order method are less effective compared to second order methods for physics-informed ML. Would it be possible for the authors to include related references here? It seems to me that adaptive methods like Adam can also improve the conditioning as it takes use of diagonal rescaling. Hence, it might be meaningful for the authors to provide some numerical examples to explain why preconditioned GD can outperform Adam/SGD. \n\nA full list of references mentioned in this review is given below:\n\n[1] Lu, Y., Blanchet, J., & Ying, L. (2022). Sobolev acceleration and statistical optimality for learning elliptic equations via gradient descent. Advances in Neural Information Processing Systems, 35, 33233-33247.\n\n[2] Yu, J., Lu, L., Meng, X., & Karniadakis, G. E. (2022). Gradient-enhanced physics-informed neural networks for forward and inverse PDE problems. Computer Methods in Applied Mechanics and Engineering, 393, 114823. \n\n[3] Son, H., Jang, J. W., Han, W. J., & Hwang, H. J. (2020). Sobolev training for the neural network solutions of pdes.\n\n[4] Son, H., Jang, J. W., Han, W. J., & Hwang, H. J. (2021). Sobolev training for physics informed neural networks. arXiv preprint arXiv:2101.08932.\n\n[5] Zeng, Q., Kothari, Y., Bryngelson, S. H., & Sch\u00e4fer, F. (2022). Competitive physics informed networks. arXiv preprint arXiv:2204.11144.\n\n[6] Czarnecki, W. M., Osindero, S., Jaderberg, M., Swirszcz, G., & Pascanu, R. (2017). Sobolev training for neural networks. Advances in neural information processing systems, 30.\n\n[7] Amari, S. I., Ba, J., Grosse, R., Li, X., Nitanda, A., Suzuki, T., ... & Xu, J. (2020). When does preconditioning help or hurt generalization?. arXiv preprint arXiv:2006.10732.\n\n[8] Zhu, B., Hu, J., Lou, Y., & Yang, Y. (2021). Implicit regularization effects of the Sobolev norms in image processing. arXiv preprint arXiv:2109.06255.\n\n[9] Calder, J., Mansouri, A., & Yezzi, A. (2010). Image sharpening via Sobolev gradient flows. SIAM Journal on Imaging Sciences, 3(4), 981-1014.\n\n[10] Richardson Jr, W. B. (2008). Sobolev gradient preconditioning for image\u2010processing PDEs. Communications in Numerical Methods in Engineering, 24(6), 493-504.\n\n[11] Soliman, Y., Chern, A., Diamanti, O., Kn\u00f6ppel, F., Pinkall, U., & Schr\u00f6der, P. (2021). Constrained willmore surfaces. ACM Transactions on Graphics (TOG), 40(4), 1-17.\n\n[12] Yu, C., Brakensiek, C., Schumacher, H., & Crane, K. (2021). Repulsive surfaces. arXiv preprint arXiv:2107.01664.\n\n[13] Raissi, M., Perdikaris, P., & Karniadakis, G. E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378, 686-707."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8048/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8048/Reviewer_E6qc",
                    "ICLR.cc/2024/Conference/Submission8048/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8048/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698651691938,
        "cdate": 1698651691938,
        "tmdate": 1700484596934,
        "mdate": 1700484596934,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HGsj6d7WTs",
        "forum": "WWlxFtR5sV",
        "replyto": "WWlxFtR5sV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8048/Reviewer_95TT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8048/Reviewer_95TT"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the problem of training neural networks to solve partial differential equations (PDEs) using a physics-informed neural networks (PINNs) approach. In the introduction, the authors mention empirical studies suggesting that training might be the bottleneck for achieving robust convergence in PINNs, which motivates their approach of studying the behavior of gradient descent algorithms in a PINNs context. Then, in section 2, the authors study a simplification of a gradient step and show that the rate of convergence depends on the condition number of a Gram matrix associated with the PDE. This motivates the authors approach for preconditioning the operator to achieve better convergence rates in practical applications."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-written and focuses on an important problem in PINNs.\n- The authors provide a clear exposition of their approach with both theoretical and empirical results."
            },
            "weaknesses": {
                "value": "- On p.3, the authors mention that their aim is to analyze the convergence of gradient descent. However, the analysis in section 2 only considers a simplified gradient step where they neglect all higher order terms in an error term epsilon_k, which they assume to be small. Is that equivalent to performing a local analysis close to a local minima and if so, how does the current analysis relate to the NTK analysis in the context of PINNs (Wang et al., 2022)? It is not clear whether this explains the poor performance of PINNs in practice as the main issue is to understand the pre-asymptotic behavior of gradient descent.\n- Thm. 2.4 shows that the condition number of the Gram matrix is greater than the condition number of a Hermitian square operator A\\circ TT^*. However, to perform preconditioning, one should instead derive an upper bound on the condition number of the Gram matrix. I am aware that Thm 2.4 gives an equality case but it assumes that the Gram matrix is invertible.\n- The practical section 3 is the main weakness of the paper. It seems that the authors approach might only be useful when one already know a really good approximant to the eigenvectors and eigenvalues of the PDE, when it is linear (e.g. the Poisson equation example in Fig. 1). In this case, why would one use a neural network to solve the PDE instead of a spectral method? The authors states that they employ a linear combination of eigenvectors of the Poisson equation as the machine learning model, but this is exactly a spectral method and we know that the performance of an iterative solver (e.g. conjugate gradient, GMRES) for solving the system is related to the condition number of the matrix. The authors give very little details on how one might use their approach in a real context with a complex neural networks to solve a nonlinear PDE."
            },
            "questions": {
                "value": "- In eq. (2.6), the function space should be defined so that D\\phi_i lies in L^2.\n- I would suggest using \\bm notation for matrices and vectors instead of \\mathbb, which could be confused with the set of complex numbers for \\mathbb{C}.\n- Lemma 2.2, is there a way of controlling the error epsilon_k with respect to m for large m?\n- Typo: Lemma 2.1: \"Let If eta = ...\"\n- Typo: p.7 \"precondiioned Fourier model\"\n- Fig. 3 (left): I don't really see an improvement of the approach as their are still a large number of 0 eigenvalues with the MLP + preconditioned Fourier features approach, suggesting that the system is ill-conditioned.\n- The related work section would be better placed in the introduction."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8048/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698663519595,
        "cdate": 1698663519595,
        "tmdate": 1699636994400,
        "mdate": 1699636994400,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Vl5fuQT2vo",
        "forum": "WWlxFtR5sV",
        "replyto": "WWlxFtR5sV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8048/Reviewer_2Mn2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8048/Reviewer_2Mn2"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates behaviors of gradient descent algorithms when the loss term involves minimizations of the residual connected to partial differential equations (PDEs). The paper simplifies the gradient descent rules via utilizing the Taylor expansion and derives the relationship between the rate of convergence and the differential operator. In particular, the paper provides theoretical analysis showing that the rate of convergence depends on the conditioning of an operator denoted by ``Hermitian square\u2019\u2019, which is derived from the differential operator and its adjoint. The paper proposes a method for preconditioning the differential operator and tests the algorithms on benchmark problems."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-written and theoretically sound (although there are some typos, most notably, there is no $\\lambda$ in Eq. 2.2). \n\n- The paper tackles an important issue in minimizing PDE residual losses, which is not popularized in many scientific machine learning applications."
            },
            "weaknesses": {
                "value": "- It is less convincing if the proposed method can be applicable to a wide variety of neural networks, which is also pointed out in the manuscript, \u201cpreconditioning A for nonlinear models such as neural networks can, in general, be hard\u201d on page 7 and in the limitation paragraph. In the manuscript, preconditioning strategies are showcased only with the simple cases, approximating solutions of Poisson equations via linear and nonlinear models with Fourier feature mapping. It does not seem that the paper provides some guidelines or insights for preconditioning strategies for general cases.\n\n- Computational aspects are weak. In particular, comparisons with other works improving optimizers of PINNs in more general classes of PDEs, which also include the assessment on computational wall times, would be more appreciated and informative for readers/users to decide which method to use. Even if it\u2019s not computational comparisons, it would be better to have some descriptions on differences from other baselines [2,3] (as shown in page 8, where the connections to [1] is presented in the Choice of lambda paragraph.)\n\n[1] Wang, et al, 2022, JCP (the reference in the paper)\n\n[2] Basir and Senocak, Physics and equality constrained artificial neural networks: Application to forward and inverse problems with multi-fidelity data fusion, 2022, JCP\n\n[3] Kim et al, DPM: A novel training method for physics-informed neural networks in extrapolation, 2021, AAAI.\n\n- Although the problem that is investigated in the paper is an important problem and the authors provide good contributions in the theoretical side, it is less convincing that the paper makes significant contributions in ML or DL perspectives."
            },
            "questions": {
                "value": "- could the authors provide more information/insight on how to build preconditioner for more general cases? for complex PDEs where the PINN does not employ Fourier feature transform? \n\n- In the implementation, the gradient descent rule has been reimplemented following Eq. (3.2)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8048/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8048/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8048/Reviewer_2Mn2"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8048/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772741683,
        "cdate": 1698772741683,
        "tmdate": 1700673752026,
        "mdate": 1700673752026,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "t1JuyFCyYD",
        "forum": "WWlxFtR5sV",
        "replyto": "WWlxFtR5sV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8048/Reviewer_gVcB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8048/Reviewer_gVcB"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a preconditioning technique for solving optimisation problems associated with solving ODE, which can be expressed through the differential operator used and some kernel integral operator which is variable and can be chosen to minimize the impact of conditioning number of the optimisation problem. This approach appears to be practically efficient for model of Fourier features."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The proposed preconditioning is elegantly inferred from the approximate form of iteration so that the approach seems to be natural. Theoretical framework allows obtaining improved convergence rates with no dependence on conditioning number of original problem, which is the issue even in finite-dimensional optimisation. Practical efficiency of the precodnitioning operation in its characterising property, decreasing conditioning number, was demonstrated."
            },
            "weaknesses": {
                "value": "No significant weakness that I can notice as a specialist in finite-dimensional optimisation. What was described is quite earnestly and helpful for future readers."
            },
            "questions": {
                "value": "I assume that there are models for which kernel integral operator cannot be chosen manually. What you propose to do if expressing precodnitioned operator explicitly is not possible? What do you think about applying additional optimisation procedure for training this kernel operator?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8048/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8048/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8048/Reviewer_gVcB"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8048/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698780002878,
        "cdate": 1698780002878,
        "tmdate": 1699636994147,
        "mdate": 1699636994147,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cYbJBZe5i6",
        "forum": "WWlxFtR5sV",
        "replyto": "WWlxFtR5sV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8048/Reviewer_Wdfd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8048/Reviewer_Wdfd"
        ],
        "content": {
            "summary": {
                "value": "The paper derives that under certain conditions, the gradient descent of the PINNs can be approximated using a simplified gradient descent algorithm, which further provides a linearized version of the training dynamics. The paper demonstrates that the speed of convergence of this simplified training dynamics depends on the condition number of an operator described using the underlying differential operator and the kernel integral operator. The paper provides a preconditioning approach that can almost achieve the ideal condition number of 1. The paper demonstrates the efficiency of preconditioning the operator approach for simple problems like linear Poisson and Advection Equation."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The paper derives a simplified gradient descent algorithm that linearizes the training dynamics, allowing easier analysis of the convergence speed.\n2. The paper provides a strong theoretical foundation, and shows a novel result that the convergence of PINNs depends on the condition number of a certain operator.\n3. Some of the results shown in the paper demonstrates super fast convergence (less than 50 epochs), which is quite impressive."
            },
            "weaknesses": {
                "value": "1. The paper assumes that the solution can be expressed as a linear combination of basis functions (such as fourier features, which was shown in the experiments). Such an approximation can be quite limiting, and the paper does not show how the preconditioning can be extended for non-linear models. \n2. The evaluation of the proposed preconditioning approach is poor. Firstly, the paper only covers linear PDEs with linear PINN models. Second, only the training loss curves were shown for the two PDEs. It is well-known that the PINNs can learn trivial solutions and achieving very low training losses does not guarantee convergence. See paper [1]. \n\n[1] Daw, Arka, Jie Bu, Sifan Wang, Paris Perdikaris, and Anuj Karpatne. \"Mitigating Propagation Failures in Physics-informed Neural Networks using Retain-Resample-Release (R3) Sampling.\" ICML 2023"
            },
            "questions": {
                "value": "**Questions:**\n1. For a typical Taylor series expansion, the Hessian would be computed at the point $\\theta_0$. Can the authors provide some justification on why the Hessian was computed at an interpolated $\\theta$ value between 0 and k?\n2. Can the authors provide some discussions on the connections and differences of their approach and NTK theory, especially considering that the latter also offers a convergence rate analysis for PINNs?\n3. For the Advection Equation with Fourier Features without preconditioning (Figure 2), it seems that the loss does not converge at all (the value of the loss is $10^3$). Can the authors provide some justification behind this? Is it because the model was chosen to be a simple linear one with Fourier Features?\n\n\n**Minor Comments:**\n1. Typo: Equation 2.2 the $\\lambda$ is missing.\n2. The Taylor expansion in Equation 2.5 should contain higher order terms or the paper should mention that the 3rd order and higher terms are ignored. Similarly, the proof in the appendix A.1 should mention that higher order multiplicative terms by substituting 2.5 in 2.2 are ignored. \n3. The notation: $(\\theta_k \u2212 \\theta_0)^T H_k(\\theta_k \u2212 \\theta_0)$ can be confusing as it seems like the Hessian is computed at $H_k(\\theta_k - \\theta_0)$. The authors can consider using the following: $(\\theta_k \u2212 \\theta_0)^T H_k(x) (\\theta_k \u2212 \\theta_0)$"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8048/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8048/Reviewer_Wdfd",
                    "ICLR.cc/2024/Conference/Submission8048/Senior_Area_Chairs"
                ]
            }
        },
        "number": 6,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8048/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807842885,
        "cdate": 1698807842885,
        "tmdate": 1700520723808,
        "mdate": 1700520723808,
        "license": "CC BY 4.0",
        "version": 2
    }
]