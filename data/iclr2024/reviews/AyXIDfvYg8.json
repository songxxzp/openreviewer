[
    {
        "id": "mTiEmUecpk",
        "forum": "AyXIDfvYg8",
        "replyto": "AyXIDfvYg8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9066/Reviewer_ScWC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9066/Reviewer_ScWC"
        ],
        "content": {
            "summary": {
                "value": "This paper applies RELU activation in the last layer of models in supervised contrastive learning (SCL). With that, SCL can learn representations that converge to the OF geometry irrespective of the level of class imbalance. The author theoretically and empirically verify the effectiveness of this method. Besides, this paper finds that the batch selection is important in representation geometry and they design a batch selection strategy (batch-binding)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The theoretical analysis and empirical results cooperate well. Figure 1, 2, 3 empirically demonstrate the advantages of the additional RELU function.\n2. The motivation is clear and the paper is written well.\n3.  The proposed method is simple and effective."
            },
            "weaknesses": {
                "value": "1. Based on the empirical results in the paper (Table 1), the improvements in the test accuracy on CIFAR-100 are not significant, even when the imbalance ratio is large. It would be better to show the advantages and disadvantages of learned representations in different downstream tasks (e.g., Does SCL+ReLU performs better in fine-tuning tasks?).\n2. It would be better to conduct some additional experiments on large scale datasets (e.g., ImageNet100).\n3. It seems that batch size is an important argument when applying sample selections according to the theoretical analysis. It would be better to add the ablation study on that.\n4. Based on the analysis, it seems that various activation functions can achieve the similar effect as ReLU. Is it possible to add a discussion about that?"
            },
            "questions": {
                "value": "See my comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9066/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698661519201,
        "cdate": 1698661519201,
        "tmdate": 1699637141879,
        "mdate": 1699637141879,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VmYxQe05vC",
        "forum": "AyXIDfvYg8",
        "replyto": "AyXIDfvYg8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9066/Reviewer_q7qd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9066/Reviewer_q7qd"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a variants of SCL loss to restore the symmetrical geometry of the class-mean learned embeddings in the presence of unbalance class samples, just adding a ReLU activation after the projection layer. It theoretically demonstrates that due to the existence of ReLU, the last-layer feature embeddings of samples with the same labels will be finally aligned, while that of samples with different labels will be orthogonal, when we employ the full-batch training.  While implement mini-batch training, it also showcases that mini-batch selection strategy will significantly influence the learned embedding geometry."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed approach stands out for its simplicity and effectiveness. It offers valuable insights into the subject matter, shedding light on the restoration of symmetrical geometry in the presence of unbalanced class samples.\n\n - The theoretical analysis is well-founded, enhancing the credibility of the approach. Experimental results, while limited to relatively simple datasets like MNIST, CIFAR10/CIFAR100, and TinyImageNet, do support the method's efficacy."
            },
            "weaknesses": {
                "value": "The main concerns still lie in the experimental parts.\n\n- The experiments primarily utilize CNN-based architectures. It would be beneficial to explore the applicability of this approach to train other architectures, such as Transformers, to gauge its versatility across neural network types.\n\n-  While the proposed approach demonstrates superiority on simpler datasets, questions remain regarding its performance on more complex and larger datasets. The potential impact of the non-negative constraints imposed by the ReLU activation on representation ability in more intricate tasks needs further investigation."
            },
            "questions": {
                "value": "The minimum of the loss is not  zero and varies with the number of samples in different classes."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9066/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698845796061,
        "cdate": 1698845796061,
        "tmdate": 1699637141778,
        "mdate": 1699637141778,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZtWg9l9MXB",
        "forum": "AyXIDfvYg8",
        "replyto": "AyXIDfvYg8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9066/Reviewer_BEvk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9066/Reviewer_BEvk"
        ],
        "content": {
            "summary": {
                "value": "## Background summary \nThis paper focus on the issue of neural collapse (NC), when hidden features of sample class collapse into one another, and orthogonal framing (OF), when the mean vectors of hidden representation of each class, are orthogonal to one another. For the theoretical framework, the paper chooses the unconstrained features model (UFM), where the loss is minmized over the classification layer and last hidden layers, unconstrained from the other model parameters. \n- *UFM with cross entropy loss:* we have \n$\\min_{w_c,h_i\\in R^d}\\mathcal{L}_{CE}(\\\\{w_c\\\\}, \\\\{h_i\\\\})$ \n- *UFM with supervised contrastive loss (SCL)* for full batch (for mini-batch, it is over $i\\in [n]$):  \n$\\min_{h_i\\in R^d} \\sum_{i\\in B} \\frac{1}{n_{B,y_i} - 1} \\sum_{j\\in B, y_i=y_j, i\\neq j} \\log\\left(1 + \\sum_{\\ell\\neq i, j} \\exp(h_i^\\top h_\\ell - h_i^\\top h_j)\\right)$\n- *Symmetrical solutions under balanced classes.* Intuitively, when classes are balanced (same number of samples per class) both cross entropy & contrastive loss, repel the samples from seperate classes, while absorb the class within one class. This has the effect of the finding the geometry that is symemtric w.r.t. to the class identity, resulting in a equiangular tight frame (ETF). This attraction/repulsion can be seen in the logits $\\sum_{\\ell\\neq i} \\exp(h_i^\\top h_\\ell)$\n- *Non-symmetrical solutions when classes are not balanced*  When the classes are not balanced, the symmetry between class will be broken. For example, if one class completely dominates the training set, both cross-entropy and contrastive loss will lead to solutions, where the majority class is almost anti-parallel to other class means. \n\n## Main contribution: symmetric solutions by passing last hidden layer through ReLU\nThe main contribution of the paper is showing that if the spectral loss is optimised over the space of all-positive hidden layers, namely by passing it through a ReLU activation, denoted as UFM_+, the UFM leads to symmetrical solutions, i.e., NC & OF properties are held again. \n \n- Thm1 proves that for the full batch contrastive loss, if we additionally pass the hidden layers through a ReLU, ie, optimize  last hidden layers over elementwise positive vectors $h_i\\ge 0$, global minima is achieved when for all $i,j \\in [n]$ we have $h_i^\\top h_j = 1(i=j)$. \n- Thm2. proves a similar property for mini-batch contrastive loss, where the collapse and orthogonality property hold within each batch (for samples within each batch $B\\in\\mathcal{B}$ and pair within batch $i,j\\in B$, we have $ h_i^\\top h_j = 1(i = j)$. \n- Cor 2.1: shows a batching propery that if held, the neural collapse and orthgonal frame property hold across the full batch, which can be explained as 3 conditions: \n   1) every batch has a sample from each class \n   2) The batch connectivity graph is connected (nodes $u$ are samples $u\\in [n]$, edges represent $u$ and $v$ appeared in the same batch $u,v\\in B$)\n 3) The sub-graphs corresponding to each class are also connected\n- The paper validates the theories on various ResNet architectures. The experiments show two variations of unbalanced datasts (step and long-tailed) which seem to show that the proposed approach (passing last layer throgh ReLU) does indeed lead to symmetric solutions.\nThe experiments also include a result (Table 1) that shows the proposed approach leads to a higher test accuracy."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The presentation. The presentation of results, both in terms of writing, mathematical notation and formula, and the figures, is exquisite. The authors take many positive steps in ensuring that the concepts are clear and simple to grasp for the reader. \n2. The selection of the research problem and the ideas presented by the author are (to the best of my knowledge) original. The simplicity of the ideas as well as the presentation of the results make the results intriguing to read."
            },
            "weaknesses": {
                "value": "Main issue\n- While the stated objective of the paper, i,e, embeddings that satisfy NC & OF properties, is clear, it not at all clear  that it is a justified goal. For example, consider the simple case of binary classification. The optimal solution without ReLU will map two classes to some anti-parallel vectors $u$ and $v = -u$. This means that the hidden representations for one class will have the maximum degree of separation $\\| u - v\\| = 2$. Now, if we pass them through ReLU, forcing them to be element-wise positive, the solutions will be instead some $u$ and $v$ such that $u^\\top v = 0$. Thus, the their mean-class distance will be $\\| u - v\\| = \\sqrt{2}.$ Without any formalities, what doesn't seem to be a benefit in terms of distance, doesn't seem to be a good approach for generalization either. Even if we make the two classes here very unbalanced, still the anti-parallel geometry seems to be better than the orthogonal approach.  Since the authors do not make any formal/theoretical comments on this topic, the only relevant evidence they provide in this direction is Table 1 test accuracy results. Because of the aforementioned reasons, I consider the results of table 1 to be particularly surprising. Thus, if the the main evidence to support utility of the approach is empirical, I would like much more substantive experiments. Some elaborate explanations or at least speculative comments by the authors will also go a long way to explain the benefits of the orthogonal mean-classes. \n- My second main issue is about the novelty and significance of the contributions. The theory, while stated clean and clearly, is rather thin. The main theorems are not very surprising, and are rather straightforward. As elaborated in the questions section, If I'm not mistaken, similar results can be replicated for the cross-entropy loss in a similar straightforward fashion. On the empirical front, while the results are definitely interesting (notably Table 1), they are not comprehensive to solidify that point. For example, if this approach has any benefits for improving test accuracy, there should be much broader experiments, covering different model architectures, datasets, and most importantly, competing methods, that deal with unbalanced classes. I am not advocating here for state-of-the-art performance. However, it is important to see the empirical results in the context of prior/similar work, so that the reader may assess the empirical benefits for themselves. That said, I am open to reconsidering my views (both on theory and empirical contributions) upon hearing authors responses. \n\nMinor issues: \n- I think there is a slightly overuse of abbreviations in this paper which make it slightly hard to read. For example, I had to go back several times to read what ETF is, or other notations. If possible, please keep the shorthands to a minimum. \n- There is a small discrepancy between eq(1) and eq(2) in the presence of (1+ ... ) within the logarithm and the summation is over $\\ell\\neq i, j$ while another is over $\\ell \\neq i$. Perhaps authors can comment on this? \n- In Figures 2 & 5 axes are not clearly defined, eg. caption can say x-axis ... y-axis ..."
            },
            "questions": {
                "value": "While the authors present the ideas for symmetrising non-balanced the contrastive loss, to me it seems like the cross-entropy loss will benefit from the same approach. For example, consider the CE loss for $c$-class classification for the full batch is given by:\n$$\\mathcal{L} =\\sum_{i=1}^n\\log\\left( \\frac{\\exp(h_i^\\top w_{y_i} ) }{\\sum_k^c \\exp(h_i^\\top w_k)}\\right)$$\nIt is rather trivial to see that if we pass the last hidden layer through ReLU, since that elementwise condition $h_i\\ge 0$ ensures that $w_k$'s are also positive, and the resulting inner products $h_i^\\top w_i$ will be always non-negative, and the global optimum is achieved when neural collapse and orthognal frames are achieved. For example, if the hidden dimension is larger than number of classes $h,w_k\\in R^d, d\\ge c,$ then we can set $w_1,\\dots, w_c$ to the standard basis $w_k := e_k$ and collapse hidden layers onto the corresponding vectors $h_i := e_{y_i}.$ Therefore, my questions are:\n- Do authors agree with my reasoning? Please correct me if my reasoning above is flawed. \n- If yes, why have authors left out cross entropy in their analysis, given that it's leading to the same result as the contrastive loss results?\n- Is there any pros/cons if we compare symmetrized CE loss, and contrastive loss, both by passing hidden layer through ReLU?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9066/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9066/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9066/Reviewer_BEvk"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9066/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698854718700,
        "cdate": 1698854718700,
        "tmdate": 1699637141673,
        "mdate": 1699637141673,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7JPnr6vchT",
        "forum": "AyXIDfvYg8",
        "replyto": "AyXIDfvYg8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9066/Reviewer_PL8u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9066/Reviewer_PL8u"
        ],
        "content": {
            "summary": {
                "value": "This work studies the training of deep neural networks with a supervised contrastive objective. Empirically, it is shown that if there are ReLU activations after the last layer, then the learned features per class collapse, and that the features of different classes are orthogonal to each other. This is supported by an analytic argument that characterizes the minimum of the loss function as a function of the features (instead of the network weights). This feature geometry is independent of the class ratios. Moreover, a batching strategy is derived that guarantees the emergence of this feature geometry."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The finding of this work; that training a deep neural network with a supervised contrastive loss and ReLU activations leads to the same feature geometry irrespective of the class imbalances is very surprising. Prior work showed that this is not the case for the cross entropy loss. Moreover, it is shown that this feature geometry is a *common* minimizer for all batches. This results in a straightforward proof, as there are no 'averaging effects' to account for.\n- The paper is well written\n- Related work (with one exception) is extensively discussed and compared to."
            },
            "weaknesses": {
                "value": "- Significance: The work appears to be written on the premise that achieving a symmetric (orthogonal or simplex) feature geometry is beneficial.  For example, a batching strategy is devised that guarantees such a geometry. While symmetry seems reasonable for class-balanced data, for imbalanced data this is not so clear. I did not see an argument supporting this premise, and thus also not for why using the proposed batching strategy  is a good idea. Moreover, there is few empirical support, as the use of ReLUs does not have any effect on classification accuracy, cf. Table 1.\n\n- The analytical argument requires a very strong assumption: unconstrained features. Moreover, only the loss minimizer is derived, but optimization effects for this non-convex problem are not accounted for. On the other hand, this strong assumption allows for results that are architecture independent.\n\n- The empirical part of this work would be stronger, if the theoretical findings are verified on a large scale imbalanced dataset (compared to the mid-scale datasets CIFAR10, CIFAR100 and TinyImagenet). \n\n-  The very related work [Wang & Isola, Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere, ICML 2020] is not cited. This work studies feature geometries when minimizing the (unsupervised) contrastive loss and should be compared to."
            },
            "questions": {
                "value": "Are there empirical differences when training models with and without ReLU activations after the last layer. If so, can such differences be explained by the targeted feature geometry? Are such differences more pronounced when using the batching strategy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9066/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698928137783,
        "cdate": 1698928137783,
        "tmdate": 1699637141566,
        "mdate": 1699637141566,
        "license": "CC BY 4.0",
        "version": 2
    }
]