[
    {
        "id": "MpAmXyi1OI",
        "forum": "z62Xc88jgF",
        "replyto": "z62Xc88jgF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8021/Reviewer_h1aP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8021/Reviewer_h1aP"
        ],
        "content": {
            "summary": {
                "value": "This paper brings \"functional a posteriori error analysis\" from the mathematics community to machine learning. Similar to Hillebrecht et al and many others, the goal is to obtain error certificates, i.e, some formal statement on the upper bound of the error produced by physics-informed neural networks. In functional a posteriori error analysis, the upper bound of the error depends on approximate solution, data, and additional fields that can potentially tighten the bound. This work builds on this direction and proposes a loss function Astral. Experiments show that in an unsupervised setting, Astral outperforms one baseline (Li et al)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I note that I am not an expert in the related domain of the paper. Hence, I requested area chairs for additional reviewers. My score reflects that I am ignorant in this field and do not want to reject papers.\n\nThere are many papers on certifiable machine learning, where the goal is to analyze the error, traditionally from generalization theory like PAC-Bayes bounds. For physics-informed neural networks, I think there might be meaning in investigating practical techniques to upper bound the error.\n\nWhat this paper provides is perhaps the idea of bringing \"functional a posteriori error analysis\", which can be interesting to certain parts of the community."
            },
            "weaknesses": {
                "value": "On the other hand, I would have preferred if the presentation of the paper was more kind to readers. For example:\n\nThe introduction provides minimum information about astra -- only that it is a loss function with certain benefits like more robustness over residuals and variational losses. In my view, there should be a high-level description of (a) what motivates this specific loss function, and (b) what exactly is this loss function. There should also be reasoning behind it at a high level. It is difficult to grasp the concept and also get interested otherwise. I think the paper can also be more kind to the readers by defining certain terminology before using them, e.g., majorants, posterior error, priori error, etc.\n\nI found section 2.3 to be difficult to parse. The paper states several equations without explaining them in sufficient depth. For example, in equation (9), I did not understand where the definition of astral is from. The paper states \"It is possible to derive\" but at least in the appendix, these derivations should be shown. Equation 12 is also similar. I could not understand the derivation from equation 9 to equation 12. There should also be sufficient reasoning \"why\", for example, equation 12 is a good measure of the predicted solution.\n\nI hope other reviewers can comment more in-depth about technical part of the paper and quality of the experiments."
            },
            "questions": {
                "value": "My questions for clarification are embedded in the comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8021/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8021/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8021/Reviewer_h1aP"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8021/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698435588960,
        "cdate": 1698435588960,
        "tmdate": 1699636989476,
        "mdate": 1699636989476,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HcVDko8zkJ",
        "forum": "z62Xc88jgF",
        "replyto": "z62Xc88jgF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8021/Reviewer_SLC5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8021/Reviewer_SLC5"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new loss function for supervised and physics-informed training of neural networks and operators that incorporates a posteriori error estimate. The trained model can guarantee the approximation error as this is done by the new loss function aiming at minimizing the theoretical posterior error estimate. The later has been established for a number of PDEs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.  It is a novel idea to adopt the theoretical  functional a posteriori error estimates for the learning objective. The functional a posteriori error estimate was initially established in the conventional finite element method analysis for PDE.\n\n2. The entire framework has been clearly described (at least I can follow the main stream of the paper, although I am not the expert in this particular area).\n\n3. The paper is well motivated with the goal to mitigate neural PDE solvers' inability guarantee good accuracy in practice."
            },
            "weaknesses": {
                "value": "Although this is a viable approach in a guaranteed way to produce reliable neural PDE solvers, the application could be limited, e.g., in the case for the problems where there is no theoretical posteriori estimates available."
            },
            "questions": {
                "value": "Not sure why the full GitHub repository is not made available for review."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8021/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8021/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8021/Reviewer_SLC5"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8021/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698534710467,
        "cdate": 1698534710467,
        "tmdate": 1700686523452,
        "mdate": 1700686523452,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "D0YNWWjmRK",
        "forum": "z62Xc88jgF",
        "replyto": "z62Xc88jgF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8021/Reviewer_pN9X"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8021/Reviewer_pN9X"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method for training neural networks that provide solutions to PDEs (including PINNs, Neural Operator networks, and vanilla surrogate models). The idea is to upper bound the error of the predicted solutions and incorporate this upper bound into the training loss of the neural network. Constructing the the upper bound on the solution error is non-trivial and requires expert knowledge, but this provides a way of incorporating domain knowledge into the model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow. Figures are clear and informative.\n- The paper addresses a method for computing error bounds on solutions to PDEs, a challenge of high interest to the ML for physics community.\n- Experiments support the claims."
            },
            "weaknesses": {
                "value": "- It seems like this approach will have limited applicability. The first limitation is that the U function must be specified by the practitioner, but this is addressed in the paper, seems reasonable, and is a way to incorporate domain knowledge. The bigger issue is that in most scenarios, I imagine that predicting the error certificates is at least as difficult (and usually more difficult) than predicting the solution."
            },
            "questions": {
                "value": "- My intuition is that the error certificate w_U can be thought of as an *explanation* of how the solution emerges. For example, the certificate could explain the predictions of a day-ahead weather forecast model by providing the evolution of the weather variables over the intermediate time steps. When a good certificate can be generated, the practitioner can feel confident that the solution is correct (with hard error bounds!), but otherwise the error bounds will be loose and the practitioner will not have much confidence in the solution. This all seems desirable, but I wonder if there exists a large set of problems for which the surrogate model can produce good solutions but not produce good certificates?\n\nThere are some minor typos throughout:\n- Figure 1: should approximate solution u by u^tilde?\n- Section 2.1 \"deep learning is to\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8021/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698870460811,
        "cdate": 1698870460811,
        "tmdate": 1699636989269,
        "mdate": 1699636989269,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Y0ysDtRU5i",
        "forum": "z62Xc88jgF",
        "replyto": "z62Xc88jgF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8021/Reviewer_CFkx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8021/Reviewer_CFkx"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a loss function for training PINNs with theoretically guaranteed error bounds. Experiments are done for verifying their claims."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. By the theory from functional a posteriori error analysis, using Astral loss gives theoretical guarantee that the output of neural network is the exact solution in the sense that their distance is zero in some function space.\n\n2. Astral loss can be computed explicitly for common PDE problems. The authors use elliptic equation as an example.\n\n3. Some experiments are done to compare Astral loss with the commonly used residual loss using different models/equations."
            },
            "weaknesses": {
                "value": "1. Although the use of this type of loss in this setting might be new, this work does not prove any new theoretical results.\n\n2. That being said, experiment is a very important component in this paper, however, I find the evaluation metric of the solution very interesting. More specifically, let $u$ be the output of neural networks and $u^*$ be the exact solution. The test error is usually computed using relative $L^2$ norm (See for example [1][2]), i.e.\n$$|| u - u^*||_2^2 / ||u^*||_2^2 = \\int|u - u^*|^2dx / \\int |u^*|^2 dx.$$\nHowever, in Figure 4, when evaluating solutions, the mean error is computed using equation (15), the energy norm. \n\n(i). why not using the relative $L^2$ norm? How does Astral loss perform if the evaluation is done in $L^2$? \n\n(ii). The a posteriori error bound is in the energy norm, i.e. \n$$L(u, w_L) \\leq |||u-u^*||| \\leq U(u, w_U).$$\nso I would naturally expect Astral loss to achieve fairly small error in this energy norm, but this does not necessarily imply the solution is \"better\". Equations can be solved in different spaces. In fact, I think the space $L^2$ is more commonly used when people study existence and uniqueness of PDE solutions. \n\n(iii). There could be a relation between the energy norm and $L^2$ norm. More explanation is needed for the specific choice of the evaluation metric since it differs from the previous literature. \n\n\n[1] Li et al., Physics-Informed Neural Operator for Learning Partial Differential Equations\n\n[2] Wang et al., An Expert's Guide to Training Physics-informed Neural Networks"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8021/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8021/Reviewer_CFkx",
                    "ICLR.cc/2024/Conference/Submission8021/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8021/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699414137103,
        "cdate": 1699414137103,
        "tmdate": 1700667877697,
        "mdate": 1700667877697,
        "license": "CC BY 4.0",
        "version": 2
    }
]