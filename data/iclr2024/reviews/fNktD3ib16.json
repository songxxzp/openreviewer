[
    {
        "id": "AMTc9vaxXb",
        "forum": "fNktD3ib16",
        "replyto": "fNktD3ib16",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2526/Reviewer_YAyi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2526/Reviewer_YAyi"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the potential pitfalls related to LLMs knowledge editing, including Knowledge Conflict and Knowledge Distortion.\nTo achieve this target, two benchmark datasets and several innovative evaluation metrics are also introduced.\nWith these settings, this paper conducts experiments from two aspects among four common editing approaches on two LLMs, and proposes a Multi-Label Edit (MLE) solution."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper puts forward a valuable research problem, i.e., to discuss the potential risk of editing knowledge encoded in LLM. I believe this could help researchers and practitioners better understand and manipulate the knowledge encoded in LLM, so as to obtain a better model.\n2. Extensive baselines are employed for analyzing the proposed research question.\n3. Several new measures are designed to quantify the degree of knowledge conflicts in experiments."
            },
            "weaknesses": {
                "value": "My major concerns lie in the following three aspects:\n1. The \u201cKnowledge Conflict\u201d proposed in this paper is confusing to me.\n2. The experimental settings of \u201cKnowledge Distortion\u201d are vague and incomplete.\n3. The proposed MLE solution is unclear."
            },
            "questions": {
                "value": "My questions mainly comes from the above three concerns.\n1. The knowledge conflict mentioned in your paper, i.e., \u201ce1: Marie\u2019s husband is Pierre \u2192 Jacques and e2: Jacques\u2019s wife is Marie \u2192 Maurice\u201d, seems to be the \u201cData Collision\u201d rather than \u201cEditing Conflict\u201d.\nIf this type of conflict only exists at the data level, then the evaluation in this paper is meaningless.\nIn addition, I guess you want to emphasize the conflict between different editing operations, as mentioned in Section 2.2 \u201cthere is a possibility that interference occurs between different edits, causing the former edit invalid.\u201d.\nCould you further clarify this problem and provide more suitable examples?\n\n2. The \u201cKnowledge Distortion\u201d is a very promising research question that I pay attention to. However, the evaluation in this aspect is:\na) vague:\nHow many the (s,r) pairs did you evaluate to calculate the results in Table 2 ? (Is there five?)\nHow are the values in Table 2 calculated? Are they arithmetic mean values?\nWhy is the JS divergence chosen? Is the asymmetric KL divergence inappropriate? and why?\n\nb) incomplete:\nWhy only evaluate triples under the same (s, r), and other knowledge is not affected? (e.g, with same (s,.,.) and same (., r, .) or others (., ., .))?\n\n3. The proposed MLE method is unclear. Could you explain what the multi-label is and what is its function?\nHow does this method alleviate the problem of \u201cKnowledge Distortion\u201d?\nAdditionally, is this method effective for the first question (\"Knowledge Conflict\") mentioned in your paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2526/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2526/Reviewer_YAyi",
                    "ICLR.cc/2024/Conference/Submission2526/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2526/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698655045918,
        "cdate": 1698655045918,
        "tmdate": 1700635179836,
        "mdate": 1700635179836,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1nhtaL1BmX",
        "forum": "fNktD3ib16",
        "replyto": "fNktD3ib16",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2526/Reviewer_XF6v"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2526/Reviewer_XF6v"
        ],
        "content": {
            "summary": {
                "value": "This work pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs, and introduces new\nbenchmark datasets to evaluate LLMs after knowledge finetuning with proposed innovative evaluation metrics."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Novel benchmarks and evaluation metrics are developed in the paper\n2. With empirical analysis, the authors develop a simple method, a.k.a Multi-Label Edit, to alleviate Knowledge Distortion in LLMs"
            },
            "weaknesses": {
                "value": "1. The novelty of the developed method is quite low and the real contribution of this paper is the development of new benchmarks equipped with evaluation metrics."
            },
            "questions": {
                "value": "Thanks for your efforts in investigating two pivotal concerns in LLMs, specifically Knowledge Conflict and Knowledge Distortion, which has been widely discussed in NLP community nowadays. However, there remains some issues that I need to discuss with you.\n\n\n1. Fig. 2 illustrates that after the process of Round-Edit, LLMs tend to assign higher probabilities to the knowledge facts stored in recent corpus and gradually forget the knowledge stored in model parameters. Nonetheless, I believe that the demonstration of Knowledge Distortion in Fig. 2 is not a unique issue limited to LLMs, but rather a prevalent concern across all current Deep Learning models. \nAnd I believe that the solution to this issue is to develop more reasonable retrieval-based LLMs, which answers questions based on the knowledge context, rather than finetune-based Knowledge Editing methods mentioned in this article. With retrieval-based LLMs, you just need to modify the knowledge facts stored in the context, then LLMs can directly answer the user question according to the context. In such consideration, I suppose the contribution of this paper is limited, and it will be better to include evaluation of retrieval-based LLMs as baselines.\n\n2. The novelty of devlelopment of Multi-Label Edit is quite low, why not consider memory-based methods or EMA methods to alleviate the forgetting of previous knowledge stored in LLMs when you are certain that these knowledge fact are all accurate.\n\n3. For Knowledge Conflict, it will also be a problem in retrieval-based LLMs, any thoughts to solve this problem according to your expiermental results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2526/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734566625,
        "cdate": 1698734566625,
        "tmdate": 1699636189155,
        "mdate": 1699636189155,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cLPUarr1lX",
        "forum": "fNktD3ib16",
        "replyto": "fNktD3ib16",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2526/Reviewer_ihfd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2526/Reviewer_ihfd"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the potential pitfalls of knowledge editing for Large Language Models (LLMs). It introduces new benchmark datasets and evaluation metrics to investigate the issues of knowledge conflict and knowledge distortion. The results demonstrate that knowledge editing can lead to unintended consequences and inconsistencies in LLMs. The paper also presents potential solutions and challenges for knowledge editing in LLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The information provides insights into different knowledge editing methods and their performance in various setups.\n2. The paper discusses the concept of knowledge distortion and its impact on language models.\n3. The paper introduces the idea of conflict detection technologies to address potential knowledge discrepancies."
            },
            "weaknesses": {
                "value": "1. The information provided is quite technical and may be difficult for non-experts to understand.\n2. Some sentences are poorly structured and difficult to comprehend."
            },
            "questions": {
                "value": "1. What do you think is the fundamental reason for knowledge distortion during knowledge editing for LLMs? How to handle cases of one-to-many knowledge editing?\n2. How do the knowledge editing methods compare to each other in terms of their effectiveness and efficiency, what is the takeaway in method selection?\n3. In Multi-label Edit, how to guarantee the overall conceptual hierarchy among labels?\n\nMinor Issues:\n1. \"Emperically\" should be \"Empirically.\"\n2. \"ROME is effective in both GPT-XL and GPT-J\" should be \"ROME is effective in both GPT2-XL and GPT-J.\"\n3. \"This motivates us to employ conflict detection technologies\" should be \"This motivates the employment of conflict detection technologies.\"\n4. \"Knowledge Conflict has reflected\" should be \"Knowledge Conflict reflects.\"\n5. \"However, it is undesirable for a robust editing method to weaken the preference\" should be \"However, weakening the preference is undesirable for a robust editing method.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2526/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698739097080,
        "cdate": 1698739097080,
        "tmdate": 1699636189043,
        "mdate": 1699636189043,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pGrs5j6YNp",
        "forum": "fNktD3ib16",
        "replyto": "fNktD3ib16",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2526/Reviewer_mRJP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2526/Reviewer_mRJP"
        ],
        "content": {
            "summary": {
                "value": "This paper comprehensively explores the side effects of knowledge editing for large language models (LLMs), highlighting potential risks in real-world use cases. To facilitate a rigorous evaluation, the researchers introduce two innovative datasets specifically crafted to highlight the unintended consequences of knowledge editing. The study offers solutions for knowledge conflicts and introduces the MLE method to mitigate distortion risks. It also discusses implementation challenges and prospects of knowledge editing for LLMs."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The authors assess the risks associated with current knowledge editing methodologies for LLMs, and introduce two datasets for the purposes of finding potential drawbacks of LLMs.\n\nThis paper presents the MLE method as a straightforward solution to mitigate knowledge distortion risks and address potential knowledge conflicts.\n\nThe challenges and prospects of implementing knowledge editing for LLMs are discussed."
            },
            "weaknesses": {
                "value": "The paper's scope is limited to factual knowledge editing. \n\nHowever, the presence or absence of knowledge conflicts or distortions in other types of knowledge editing remains unexplored.\n\nThe authors should supplement this part of the paper to make it more comprehensive."
            },
            "questions": {
                "value": "please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2526/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742065186,
        "cdate": 1698742065186,
        "tmdate": 1699636188969,
        "mdate": 1699636188969,
        "license": "CC BY 4.0",
        "version": 2
    }
]