[
    {
        "id": "FIuO7JwFu2",
        "forum": "ptCIlV24YZ",
        "replyto": "ptCIlV24YZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5846/Reviewer_DGyy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5846/Reviewer_DGyy"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a new image clustering workflow that leverages the powerful feature representation capabilities of large pretrained models (like CLIP) to effectively and efficiently perform image clustering. It first develop a new algorithm to estimate the number of clusters in a given dataset. Through extensive experiments, the paper demonstrate that the workflow performs well on standard datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Novel image clustering pipeline: The paper proposes a novel image clustering pipeline that leverages the powerful feature representation of large pre-trained models such as CLIP and cluster images effectively and efficiently at scale. The paper also develops a new algorithm to estimate the number of clusters in a given dataset, and a simple yet effective self-labeling algorithm that generates meaningful text labels for clusters.\n2. State-of-the-art performance: The paper demonstrates that the proposed pipeline achieves state-of-the-art clustering performance on standard datasets such as CIFAR-10, CIFAR-100, and ImageNet-1k. The paper also shows that the pipeline works well on datasets without predefined labels, such as WikiArt"
            },
            "weaknesses": {
                "value": "1. Dependence on pre-trained models: The paper heavily relies on the pre-trained models such as CLIP to provide the initial feature representation and the text candidates for labeling. The paper does not explore how the choice of pre-trained models affects the clustering performance or the quality of labels. The paper also does not consider the potential biases or limitations of the pre-trained models, such as their data sources, or domains.\n2. Limited evaluation and comparison: The paper also does not report enough ablation studies or sensitivity analysis to show the impact of different components or hyperparameters of the pipeline."
            },
            "questions": {
                "value": "How does the choice of pre-trained models affect the clustering performance or the quality of labels?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5846/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5846/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5846/Reviewer_DGyy"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698557957682,
        "cdate": 1698557957682,
        "tmdate": 1699636618545,
        "mdate": 1699636618545,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5zZJzVxJwM",
        "forum": "ptCIlV24YZ",
        "replyto": "ptCIlV24YZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5846/Reviewer_7qGL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5846/Reviewer_7qGL"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a unified approach for integrating feature learning with clustering processes. Its novel method for selecting the optimal number of clusters enhances the practicality of KNN and similar clustering methods for large-scale data, even when using computationally intensive models like CLIP. Additionally, it achieves state-of-the-art results on various datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1- SOTA results\n2- I find that enhancing KNN methods \u2013 whether by improving their scalability, representation, or explainability \u2013 is valuable. \n3- Good visual analysis Fig 3 and 4."
            },
            "weaknesses": {
                "value": "1- The paper's flow is somewhat challenging to follow.\n2- The acronym MLC is initially mentioned in the contributions section without prior definition, leading to initial confusion, though its relation to previous works becomes clearer later in the paper.\n3- While the method appears to be a practical extension of MLC, its level of novelty and contribution to the field is not distinctly evident."
            },
            "questions": {
                "value": "1- The complexity added by Equation 4 to the optimization process isn't clear. Is it possible for there to be multiple optimal values for K?\n2- Could a simpler method, like the elbow method, be used to determine K?\n3- A brief explanation of what \"more-structured representation\" means in the context of related works would be helpful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5846/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5846/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5846/Reviewer_7qGL"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806404132,
        "cdate": 1698806404132,
        "tmdate": 1701059150086,
        "mdate": 1701059150086,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Tl59uLr7IL",
        "forum": "ptCIlV24YZ",
        "replyto": "ptCIlV24YZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5846/Reviewer_5jgz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5846/Reviewer_5jgz"
        ],
        "content": {
            "summary": {
                "value": "The submission introduces a new image clustering pipeline named CPP, which leverages large pre-trained models like CLIP to efficiently and effectively cluster images, particularly on large-scale datasets. The authors propose to estimate the optimal number of clusters in a dataset and optimize the rate reduction objective using pre-trained features, resulting in a notable improvement in clustering accuracy (e.g., from 57% to 66% on ImageNet-1k). Furthermore, by utilizing CLIP's multimodal capabilities, a simple yet effective self-labeling algorithm is developed to generate meaningful text labels for the clusters. The pipeline demonstrates state-of-the-art performance across various standard datasets including CIFAR-10, CIFAR-100, and ImageNet-1k, and extends its applicability to datasets without predefined labels like LAION-Aesthetics and WikiArts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. **Leveraging Large Pre-trained Models**: The integration of the powerful image encoder from CLIP into the clustering framework MLC significantly enhances the pipeline\u2019s ability to process and analyze images, leading to state-of-the-art clustering performance on various datasets.\n\n2. **Improvement in Clustering Accuracy**: Through the optimization of the rate reduction objective using pre-trained features, the pipeline achieves a noticeable improvement in clustering accuracy, as demonstrated on ImageNet-1k.\n\n3. **Self-Labeling Algorithm**: The pipeline includes a simple yet effective self-labeling algorithm that leverages CLIP\u2019s vision-text capabilities, resulting in semantically meaningful clusters that are comprehensible to humans."
            },
            "weaknesses": {
                "value": "1. **Limited Innovation in Methodology**: The main innovation of the proposed method seems to be centered around utilizing features extracted by CLIP for initialization, but there appears to be a lack of novelty in the algorithmic aspect of the approach.\n2. **Concerns about Stability and Sensitivity**: As depicted in Fig.4, the vicinity of the extreme points in model selection appears quite flat, raising concerns about the algorithm's stability and its sensitivity to perturbations, such as the choice of hyperparameters and network architecture.\n3. **Potential Information Leakage**: Given that CLIP has been trained on vast amounts of data, there is a suspicion of cluster/label information leakage in almost all of the experimental data (CIFAR and ImageNet) presented, which could potentially bias the quantitative results.\n4. **Lack of Adequate Metrics for Text Labeling**: While the automated text annotation aspect of the pipeline is interesting, it seems to be lacking appropriate metrics to adequately evaluate and validate its performance."
            },
            "questions": {
                "value": "Please refer to the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826475293,
        "cdate": 1698826475293,
        "tmdate": 1699636618331,
        "mdate": 1699636618331,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0njcXHDVQk",
        "forum": "ptCIlV24YZ",
        "replyto": "ptCIlV24YZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5846/Reviewer_hovR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5846/Reviewer_hovR"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a novel method that leverages the rate reduction principle to learn to do image clustering using pretrained models. A technique for automatically select the optimal number of clusters is also proposed based on the same principle. Finally, a self-labeling mechism is proposed to label the clusters with semantic labels.\nExperiments show that the proposed method achieves a good performance, as well as give a good estimation of the optimal number of clusters."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. This paper provides an alternative way of performing image clustering, which seems to be performing well and could be of interesting for the community.\n2. The method enables automatic estimation of the optimal number of clusters in a dataset, from the result the method seems to perform pretty well."
            },
            "weaknesses": {
                "value": "1. The main experiments are done on somewhat small datasets like CIFAR, or coarse grained dataset like COCO, the paper would be stronger if it could include finer-grained dataset for clustering like iNaturalist."
            },
            "questions": {
                "value": "1. I would be interesting in how the method perform on fine-grained datasets.\n2. It would be better if the paper could include results of using other variant of CLIP models, such OpenCLIP."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698926104810,
        "cdate": 1698926104810,
        "tmdate": 1699636618230,
        "mdate": 1699636618230,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "funIzBLHkq",
        "forum": "ptCIlV24YZ",
        "replyto": "ptCIlV24YZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5846/Reviewer_2xo4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5846/Reviewer_2xo4"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the image clustering problem in the age of large pre-trained models. Specifically, this paper develops a method to determine the cluster number in a given dataset. Then, this paper validates that the features from large pretrained models, such as CLIP, help achive better custering accuracy than the traditional feature pre-training. Moreover, this paper also develops a self-labeling method to produce text labels for the clusters. Experiments on many image datasets, including ImageNet-1k, demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper achieves state-of-the-art results on many image datasets, including ImageNet-1k."
            },
            "weaknesses": {
                "value": "Three main technique contributions are developed in this paper, including a method to determine the cluster number, a validation that the features from CLIP push the limits of image clustering, and a self-labeling method to annotate the text-labels for the clusters. I have three concerns about these three technique contributions:\n\n- This paper seems did not discuss the existing methods to determine the cluster number and the difference among them. Do all the existing clustering methods not discuss how to determine the cluster number?\n- The proposed clustering method is a simple combination between CLIP features and MLC optimization method. I realize it is meaningful to validate the superiority of CLIP features in image clustering, but the technique contribution itself is kind of subtle.\n- A self-labeling method to annotate the text-labels for the clusters in *Algorithm 2* simply uses a cosine similarity metric to determine which texts are the closest ones given text candidates, which is a very simple solution. It does not meet my expectations that the proposed self-labeling method strongly relies on the pre-define text candidates. What if the text candidates are not given?"
            },
            "questions": {
                "value": "See the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5846/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5846/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5846/Reviewer_2xo4"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699350557804,
        "cdate": 1699350557804,
        "tmdate": 1700653103315,
        "mdate": 1700653103315,
        "license": "CC BY 4.0",
        "version": 2
    }
]