[
    {
        "id": "AsKyhnXRDR",
        "forum": "2TFfLiTGBS",
        "replyto": "2TFfLiTGBS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1376/Reviewer_vW45"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1376/Reviewer_vW45"
        ],
        "content": {
            "summary": {
                "value": "This work aims to improve the adversarial robustness of UDA methods. The authors first provide an upper bound for the adversarial target error, showing that the target adversarial error can be bounded by the source error and the discrepancy. They further approximate the ideal joint loss and propose a practical bound. The algorithm is then implemented based on the error bound. Some experiments show that the proposed method achieves better adversarial robustness than baselines."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Considering the target domain lacks labeled data, it is indeed difficult to gain robustness from other domains with domain shift. So, I admit that the authors focus on a challenging and interesting problem.\n\nBesides, the authors implement a testbed for evaluating the robustness under the UDA setting, which is a vital contribution to this field."
            },
            "weaknesses": {
                "value": "After reading this paper, I have some concerns:\nAfter reading this paper, I have some concerns:\n* My primary concern is that there may be a gap between the theory and the algorithm. Specifically, in order to upper bound and optimize the ideal joint loss, the authors use one of the hypothesis $h\\in \\mathcal{H}$ to replace the optimal hypothesis $h^*$. I don't think this is appropriate. By the definition of the ideal joint loss, it is the minimal value optimized on $\\mathcal{H}$. Hence, given a fixed hypothesis class, this term is fixed. Let's denote the ideal joint loss and its upper bound as $\\gamma$ and $\\gamma^{+}$ respectively. In the algorithm, the learner aims to minimize the $\\gamma^+$. And they explain in Section 3 that it is beneficial to control this term. However, in my opinion, this ideal joint loss is a fixed value and cannot be optimized by minimizing its upper bound. The optimization of this term seems somewhat ridiculous. Although the authors claim in the experiments that optimizing this term is empirically beneficial for robustness, I am inclined to believe that the minimization of the target adversarial error is a heuristic as in prior works, which disproves the authors' claim that their algorithm is theoretically justified.\n* In addition, the algorithm needs to generate pseudo-labels on the target domain, while the theoretical bound involves the adversarial target error, which needs the ground truth labels. The authors don't take the effect of incorrect pseudo-labels into consideration. This is another gap between the theory and the practical algorithm.\n* The optimization process of Eq. (6) in the pseudocode of Algorithm 1 is not clear. In fact, the optimization is complex, involving many steps such as the generation of adversarial examples and the optimization of $f,g$.  It would be better to provide a more clear explanation of the algorithm."
            },
            "questions": {
                "value": "Why does the algorithm achieve the best clean accuracy compared with the baselines in DIGIT datasets? The proposed algorithm involves adversarial training, which is assumed to hurt clean accuracy."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1376/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1376/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1376/Reviewer_vW45"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1376/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698323715906,
        "cdate": 1698323715906,
        "tmdate": 1699636065404,
        "mdate": 1699636065404,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ssGQvGVuQl",
        "forum": "2TFfLiTGBS",
        "replyto": "2TFfLiTGBS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1376/Reviewer_d4Ff"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1376/Reviewer_d4Ff"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors provided an efficient algorithm DART to train a model which is adversarially robust in the target domain. Their method is motivated by an adversarial error bound and does not need to change the framework of DANN. The experiments validate the effectiveness of their method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is well-written.\n2. The adversarial robustness of the model on target domain is largely improved by the proposed method. And DART does not compromise the standard accuracy. This improvement represents a significant advancement in this research field.\n3. The algorithm can be generally applied based on various prior methods. We only need to change a few codes based on the original frameworks."
            },
            "weaknesses": {
                "value": "I am mainly concerned about the experiments:\n1. More expeiments on various neural networks are encouraged. Will the algorithm still perform well on smaller networks such as ResNet-18, or larger networks such as WideResNet?\n2. More expeiments on Office-31 are encouraged, since this is also an important dataset widely used in UDA setting.\n3. The adversarial perturbation $2/255$ is small. What is the performance of DART under larger perturbation, such as $8/255$?\n\nIf the authors can address my questions, I would reconsider the rate."
            },
            "questions": {
                "value": "Please refer to the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1376/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698375886160,
        "cdate": 1698375886160,
        "tmdate": 1699636065327,
        "mdate": 1699636065327,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AfUXBkRQx2",
        "forum": "2TFfLiTGBS",
        "replyto": "2TFfLiTGBS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1376/Reviewer_qNjr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1376/Reviewer_qNjr"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors study the problem of adversarial robustness under distribution shift, i.e., UDA. In order to overcome this problem, they first provide a generalization bound for the adversarial target loss, consisting three terms that can be optimized. Hence, they present a theoretically guaranteed algorithm to improve the adversarial robustness on the target domain. DART significantly improves the robustness on various adaptation tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ Originality: There are some related works that also try to improve adversarial robustness for UDA setting. However, this work is different from the prior works.\n\n+ Quality: This work is theoretically motivated and is more empirically efficient than the baselines.\n\n+ Clarity: The writing is clear generally.\n\n+ Significance: The adversarial robustness is essential in machine learning, especially for the safety-critical applications."
            },
            "weaknesses": {
                "value": "* The theoretical results lack novelty. It seems that Theory 3.1 is similar to the bound in [1]. The authors only need to replace the standard loss function with the adversarial loss function.\n* The perturbation radius is usually set to $\\frac{8}{255}$ or $\\frac{16}{255}$. In this work, the authors set the radius to $\\frac{2}{255}$. This may not be sufficient to illustrate the efficacy of the proposed method. I noticed that the authors conducted experiments with various radii on the task Photo -> Sketch. However, I think more experiments on other datasets are needed.\n\n[1] A theory of learning from different domains. Shai Ben-David, et al."
            },
            "questions": {
                "value": "* In Table 3, the authors compare the proposed algorthm with AT (tgt,cg). What is the difference between these two methods? Why does DART have better robustness compared to AT (tgt,cg) ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I have no ethical concerns."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1376/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698468381785,
        "cdate": 1698468381785,
        "tmdate": 1699636065251,
        "mdate": 1699636065251,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RQCfijg5b0",
        "forum": "2TFfLiTGBS",
        "replyto": "2TFfLiTGBS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1376/Reviewer_ag53"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1376/Reviewer_ag53"
        ],
        "content": {
            "summary": {
                "value": "This paper develops a generalization bound for adversarial target error by incorporating the source loss, worst-case divergence, and the ideal joint loss. This leads to the introduction of a DANN-inspired algorithm aimed at enhancing robustness within the target domain. To validate both the theoretical framework and the effectiveness of the algorithm, a comprehensive set of empirical experiments is conducted."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper provides a novel error bound for the adversarial robustness of unsupervised domain adaptation. Compared to prior works that are heuristic, DART has theoretical support and higher performance."
            },
            "weaknesses": {
                "value": "From a theoretical perspective, the algorithm is more interesting than those heuristic methods. However, when I read the proofs of Theorem 3.1 in Appendix, I find that there are some mistakes which may affect the validity of the theory. At the bottom of page 14, the second equation in the proof is weird. By the definition of $\\mathcal{P}(\\mathcal{D}_T)$, $\\tilde{\\mathcal{D}}_T$ is the distribution of the adversarial examples. But in the third line, it is $(x,y)\\sim \\tilde{\\mathcal{D}}_T$ rather than $(\\tilde{x},y)\\sim \\tilde{\\mathcal{D}}_T$. Actually I cannot find a suitable way to correct this mistake which prevent me reading the rest of the proof.\n\nBesides, the significance of the proposed testbed DomainRobust is limited. As the authors state, DomainRobust is implemented based on the exist framework DomainBed. As far as I can see, they only need to add the implementation of the adversarial part, such as the PGD attack methods."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1376/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698655956321,
        "cdate": 1698655956321,
        "tmdate": 1699636065159,
        "mdate": 1699636065159,
        "license": "CC BY 4.0",
        "version": 2
    }
]