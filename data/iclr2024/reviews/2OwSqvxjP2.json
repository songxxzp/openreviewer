[
    {
        "id": "Q3jS0oZfFz",
        "forum": "2OwSqvxjP2",
        "replyto": "2OwSqvxjP2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1561/Reviewer_iR75"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1561/Reviewer_iR75"
        ],
        "content": {
            "summary": {
                "value": "This study centers on pseudo labeling within the context of semi-supervised learning. To tackle the issue of inaccurate confidence scores and abundant unlabeled examples without data pruning, the author proposes two strategies of variational confidence calibration (VCC) and influence-function-based unlabeled sample elimination (INFUSE). Empirical assessments conducted on widely-adopted benchmark datasets demonstrate the efficacy of these proposed strategies. Notably, VCC yields a remarkable 3.16% reduction in error rates when compared to FixMatch."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed strategies bolster pseudo labeling by addressing two key facets: computing dependable confidence scores and judiciously selecting a subset of the unlabeled dataset. These innovations result in a significant enhancement of generalization performance while also substantially reducing computational overhead in practical applications.\n2. The writing is commendable, making the method easily comprehensible. The author offers ample experimental details, thereby facilitating the reproducibility of the study."
            },
            "weaknesses": {
                "value": "1. The novelty of the method appears somewhat constrained. Several components of the approach, such as Monte-Carlo Dropout, temporal consistency, exponential moving average, variational auto-encoder, and influence functions, are established techniques in the field.\n2. The effectiveness of the Variational Autoencoder (VAE) implementation raises questions. VAE's main advantage lies in introducing randomness, and the efficacy of its calibration may require further substantiation. Additionally, the improvements achieved through VAE, as evidenced in Table 7, seem marginal at best."
            },
            "questions": {
                "value": "1. The author's proposal to generate ground-truth labels using a mixup-based method raises a valid concern about the dataset's stability during training. It's essential to verify whether the constructed labeled dataset remains invariant throughout the training process.\n2. Table 9 in Appendix F.2 highlights the dominance of temporal scores in the experiments. It would be beneficial if the author could provide an explanation for this observation, shedding light on the reasons behind the temporal score's strong performance.\n3. Suggestions for improvement: 1) Placing the table title above the table itself would enhance the document's readability. 2) Updating the template, especially for the page header, would contribute to a better presentation of the work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1561/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1561/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1561/Reviewer_iR75"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1561/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698414014369,
        "cdate": 1698414014369,
        "tmdate": 1699636084519,
        "mdate": 1699636084519,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DqDcu78Gii",
        "forum": "2OwSqvxjP2",
        "replyto": "2OwSqvxjP2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1561/Reviewer_5hwM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1561/Reviewer_5hwM"
        ],
        "content": {
            "summary": {
                "value": "This manuscript proposes two methods, VCC and INFUSE, to improve semi-supervised learning by better utilizing unlabeled data. The effectiveness of these methods is demonstrated through experiments on multiple datasets. Overall, these methods offer promising solutions for improving the efficiency and accuracy of SSL."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality:\n- The manuscript proposes two novel methods, VCC and INFUSE, to improve semi-supervised learning by better utilizing unlabeled data. These methods are designed to address the challenges of leveraging large-scale unlabeled data in SSL, and they offer promising solutions for improving the efficiency and accuracy of SSL.\n\nQuality:\n- The manuscript provides a thorough and well-organized presentation of the proposed methods, including detailed descriptions of the models, algorithms, and experiments. The experiments are conducted on multiple datasets and in various settings, and the results demonstrate the effectiveness of the proposed methods.\n\nSignificance:\n- The proposed methods have the potential to significantly improve the efficiency and accuracy of SSL, which is an important and challenging problem in machine learning. The manuscript discusses the potential for extending the proposed methods to other SSL tasks, suggesting that they have broad applicability and potential impact in real-world scenarios."
            },
            "weaknesses": {
                "value": "1. The manuscript has some issues with the expression of details, making it difficult to follow. For example, the article does not provide an introduction to the first two loss terms in Eq. (11).\n2. The latter part of the method involving INFUSE in the manuscript, and the earlier part on confidence calibration, seem to address two completely different problems, giving the paper a scattered feel and failing to highlight the main focus of the work. This leaves an impression of breadth over depth. \n3. The author mentions that 'INFUSE uses the influence function from Koh & Liang (2017) to compute the importance of each unlabeled example', which implies that the solution to the second issue addressed in the manuscript merely references someone else's strategy. Both the problem itself and the method of solving it lack novelty.\n4. The part on VIEW CONSISTENCY seems somewhat strained. Firstly, obtaining multiple views is difficult, and moreover, the EMA in the manuscript doesn't really have any connection with multiple views. EMA has already been showcased in the TEMPORAL CONSISTENCY section.\n5. There is an issue in the reconstruction loss, where $\\tilde{r}$ is treated as ground-truth; this itself is not accurate enough."
            },
            "questions": {
                "value": "1. I don't quite understand \u201cwe argue that the optimizing function in RETRIEVE only considers the loss on the labeled training set, which may lead to a deviation from the desired results (i.e. minimizing the loss on the validation set)\u201d, could you please explain it in detail?\n2. The author mentioned that \" Although both consider the problem from the perspective of time, our temporal-consistency method is very dissimilar from the time-consistency method proposed by Zhou et al. (2020).\" in Sec 3.2. Please give a detailed explanation and analysis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1561/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698567459726,
        "cdate": 1698567459726,
        "tmdate": 1699636084444,
        "mdate": 1699636084444,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QJsfvqAS97",
        "forum": "2OwSqvxjP2",
        "replyto": "2OwSqvxjP2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1561/Reviewer_o4va"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1561/Reviewer_o4va"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new semi-supervised learning technique which is based on \"variational confidence calibration\" (for calibrating the predictions on unlabeled examples) and \"unlabeled sample elimination\" (for pruning data with the goal to decrease the running time of the method). The main contributions of this paper are as follows:\n\n(i) The authors propose the Variational Confidence Calibration (VCC) method, which aims to obtain well-calibrated scores for pseudo-label selection. The method is based on computing three different scores (ensemble consistency, temporal consistency view consistency), appropriately combining them, and feeding them to a (trainable) variational auto encoder  to get the final calibrated score. The resulting score can be used in combination with other standard/SOTA semi-supervised learning techniques.\n\n(ii) The author propose the INFUSE method, which can dynamically prune unimportant unlabeled examples, in order to speed up  the convergence and reduce the computation costs in training.\n\n(iii) Extensive experimental evaluation showing the competitiveness of the proposed method with respect to other SOTA methods."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "\u2014\u00a0This is a well-written paper that proposes an interesting approach to semi-supervised learning. \n\n\u2014 The use of the VAE in the computation of the calibrated scores is a novel and intriguing idea.\n\n\u2014 Extensive experimental evaluation showing SOTA results in various datasets. In a few cases the performance gains (in terms of test-accuracy) are quite significant, e.g. in the CIFAR-100 dataset with 400 labeled examples VCC reduces the classification error rate of FixMatch from 46.47% to 43.31% (with improvement of 3.16%)."
            },
            "weaknesses": {
                "value": "\u2014 The performance gains of using the method (in terms of test-accuracy) are typically somewhat mild, and often times less than 0.5%.\n\n\u2014\u00a0The method seems to be a bit involved, especially given the typical overall benefit."
            },
            "questions": {
                "value": "Have the authors tried to apply their method in larger datasets like Imagenet? (I know that many of the SOTA semi-supervised learning approach suffer in the case of many classes/ large scale datasets this is why I am asking.)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1561/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698640112654,
        "cdate": 1698640112654,
        "tmdate": 1699636084365,
        "mdate": 1699636084365,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "G7G8gXtwgT",
        "forum": "2OwSqvxjP2",
        "replyto": "2OwSqvxjP2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1561/Reviewer_LFY9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1561/Reviewer_LFY9"
        ],
        "content": {
            "summary": {
                "value": "This paper studies semi-supervised learning (SSL). This paper points out two issues of existing SSL methods, including 1) the incorrect pseudo labels caused by calibration error, 2) the huge computation cost in training. To address the first issue, this paper proposes Variational Confidence Calibration (VCC), a variational method to obtain the calibrated confidence scores for pseudo-label selection. To address the second issue, this paper proposes the INfluence Function-based Unlabeled Sample Elimination (INFUSE) method, which uses the influence function to compute the importance of each unlabeled example. The two methods can be combined together to achieve high prediction accuracy with lower training costs. Experimental results demonstrate the effectiveness of the proposed methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The writing is very clear.\n- The proposed two methods are reasonable. There is an important advantage of VCC, i.e., it can be plugged into existing SSL methods to enhance their performance.\n- Experimental results and ablation studies support the proposed methods."
            },
            "weaknesses": {
                "value": "- The proposed methods seem not novel enough, because they are only adapted from existing techniques, i.e., Variational Auto Encoder and Influence Function. It is intuitive that such a combination method can work well and thus I cannot see any important insights brought by the two methods.\n- I do not think it is a good strategy to address two independent problems of SSL together, which may not increase the contributions of this paper. A good paper is supported by an important finding/contribution. Two independent minor contributions to address different issues may not form a single significant contribution. So I would suggest that the authors should focus on a major problem and try to solve this paper from another novel perspective to dig more deeply.\n- In some tables, only a single result (without using mean$\\pm$std) is provided. I suggest further providing standard deviations."
            },
            "questions": {
                "value": "Please check the above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1561/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698681447463,
        "cdate": 1698681447463,
        "tmdate": 1699636084265,
        "mdate": 1699636084265,
        "license": "CC BY 4.0",
        "version": 2
    }
]