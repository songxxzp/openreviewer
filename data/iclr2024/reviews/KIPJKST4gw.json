[
    {
        "id": "uvtMuxvWvx",
        "forum": "KIPJKST4gw",
        "replyto": "KIPJKST4gw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3194/Reviewer_L6FP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3194/Reviewer_L6FP"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors aim to explore the impact of introducing code at different training stages of large language models (LLMs) and how it affects LLMs\u2019 reasoning capability. The experiments are conducted by introducing code data in the pre-training stage, the instruction tuning stage, and both stages to evaluate LLM through six inference tasks in five domains. They provide deep-in insights via comprehensive experiments and critical analyses. The resources of this paper are released."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper is well-motivated. The impact of code data in LLMs is a hot research question. This paper answers this issue from the reasoning capability aspect.  \n\n- The experiments are comprehensive, and the insights are remarkable. The reasoning capability of LLMs is evaluated via six tasks in five domains. The authors provide critical analyses and significant insights on training LLMs and the reasoning capability of LLMs. \n\n- The idea of dynamic mixed strategy is easy to follow yet effective. It helps LLMs learn reasoning skills progressively during training. \n\n- The authors provide comprehensive open-source resources, demonstrating the reproducibility of the models. These resources are valuable for the LLM community."
            },
            "weaknesses": {
                "value": "- Missing discussion on the applications. Although the authors conduct experiments and provide insights on training LLMs and improving their reasoning capability, this paper does not discuss how to apply the insights to enhance the LLM products in different domains. \n\n- Unclear construction of training corpus. The author should provide more details about data collection, data cleaning, and training data construction. The authors use fuzzy data deduplication, but they have not explained the tools of fuzzy. They should open-source the data for reproducibility. Besides, the detailed model architecture is missing. \n\n- Table 7 is confusing. The results in Table 7 show the code data will lead to a performance drop on four out of five datasets. It indicates that the code data may not help to improve the reasoning capability of LLMs. The authors should provide valid reasons.\n\n- The related work is limited. Recently, there have been various papers discussing the reasoning capability of LLMs. Therefore, the authors should survey more related papers and compare with them. \n\n- Fix the grammar errors and improve the presentation. On page 8, \u201cThe experiment found that\u2026\u2019\u2019 -> \u201cThe experiment showed that\u2019\u2019. \n\n- Missing future work. The authors should provide the potential future work on LLMs based on the experimental results and insights provided in this paper. \n\n[1] Roziere B, Gehring J, Gloeckle F, et al. Code llama: Open foundation models for code[J]. arXiv preprint arXiv:2308.12950, 2023."
            },
            "questions": {
                "value": "Please check in Strengths and Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3194/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3194/Reviewer_L6FP",
                    "ICLR.cc/2024/Conference/Submission3194/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3194/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698508074181,
        "cdate": 1698508074181,
        "tmdate": 1700548327428,
        "mdate": 1700548327428,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dXTv0waMHt",
        "forum": "KIPJKST4gw",
        "replyto": "KIPJKST4gw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3194/Reviewer_PRkj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3194/Reviewer_PRkj"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to answer an important research question: at which training stage does code data help LLMs reasoning? The authors introduce code at the pre-training stage, instruction-tuning stage, and both. The reasoning capability of LLMs is evaluated by six reasoning tasks. Through comprehensive experiments and careful analyses, they provide inspiring conclusions and insights. The authors open-source the code and model parameters."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Valuable research question. The paper raises a meaningful research question: at which training stage introducing code data can really help the reasoning capabilities of LLM? This question is of critical significance for understanding the training and application of LLM. \n \n2. Comprehensive experimental design. This paper provides a comprehensive and fair evaluation of the reasoning capabilities of LLMs on six reasoning tasks covering five domains. This broad experimental scope ensures the generalizability and reliability of the conclusions. Additionally, the authors compare models with different sizes to verify the generalization of the conclusion. \n \n3. In-depth analyses and insights. The paper not only provides experimental results but also performs in-depth analysis, providing insights into mixing code and text data to enhance the general reasoning capabilities and code reasoning capabilities of LLM. Specifically, in the pre-training stage, mixed code data helps LLM improve general reasoning capabilities, and in the SFT stage, mixed code data helps LLM improve specific code reasoning capabilities."
            },
            "weaknesses": {
                "value": "1. Experimental details are insufficient. The paper may not provide enough details on experimental settings and parameter selection in some parts (such as data mixing strategies, decoding strategies, etc.). This might challenge researchers attempting to replicate or extend this work.\n\n2. Recently, various code foundation models, such as CodeLlama [1], have been opened. However, the authors do not conduct any discussions or experiments on them. In my opinion, the reasoning capability of code foundation models is also an essential part of the interest scope of this work. \n\n3. Quality of code data. The quality of code data can have a significant impact on the reasoning capabilities of LLM. How the author ensures the high quality of code data is not discussed in depth in the article.\n\n4. The related work part is weak. Missing important papers, such as [1,2,3].\n[1] Roziere B, Gehring J, Gloeckle F, et al. Code llama: Open foundation models for code[J]. arXiv preprint arXiv:2308.12950, 2023.\n[2] Yang A, Xiao B, Wang B, et al. Baichuan 2: Open large-scale language models[J]. arXiv preprint arXiv:2309.10305, 2023.\n[3] Li P, Sun T, Tang Q, et al. CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors[J]. arXiv preprint arXiv:2305.05711, 2023."
            },
            "questions": {
                "value": "1. Experimental details. In the paper, certain key sections, such as data mixing strategy and decoding strategy, do not seem to give sufficient details of experimental settings and parameter selection. This lack of information can lead to difficulties in reproducing and scaling. How was it set up by the author in the actual experiment?\n\n2. Quality of code data. How were the coding data used by the authors in their experiments collected and screened? The quality of code data can have a significant impact on the reasoning capabilities of LLM. For example, high-quality code data may provide more reasoning impact, while low-quality data may cause the model to learn wrong patterns. How did the authors ensure that the code data used were representative and of high quality?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3194/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3194/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3194/Reviewer_PRkj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3194/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698549288469,
        "cdate": 1698549288469,
        "tmdate": 1699636267360,
        "mdate": 1699636267360,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "87M2Eq4k3D",
        "forum": "KIPJKST4gw",
        "replyto": "KIPJKST4gw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3194/Reviewer_BTot"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3194/Reviewer_BTot"
        ],
        "content": {
            "summary": {
                "value": "Previous work has shown that models trained on code perform better on reasoning tasks. The research question in this paper is at which training stage code data helps reasoning. Specifically, this work looks into two training stages: pre-training and instruction-tuning. The results show that for performance on reasoning tasks, adding code data at the pre-training stage is effective whereas at the instruction-tuning the effect is far less (sometimes leading to lower performance); however, instruction tuning on code can improve model performance on code-related problems."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- **Clear Hypothesis:** The paper asks a clear question and provides a clear setup for testing various hypothesis about that question.\n- **Clear message:** The results show a clear message about the research question, albiet only on smaller-sized models.\n- **Clarity of the writing:** The writing was mostly clear and easy to follow"
            },
            "weaknesses": {
                "value": "- **Datasets:** I was not familiar with some of the datasets used in this work and after looking into some of them, I could not get a sense of how general and challenging they are. The majority of the computation cost for this project seems to be on the training stage, so I believe reporting results on a few more datasets (maybe only for Tables 2 and 3) can strengthen the main arguments of the paper. That could include datasets from other reasoning domains (e.g., math might be an important one that does not appear in the results) or from the same domains but on datasets that are more established.\n- **Mixture experiment:** The experiment on exploring ways to mix code and text data is interesting, but given that adding code at the instruction-tuning stage was already shown to be not that effective, I wonder why it was tested for the instruction-tuning stage. I understand the high computation cost of pre-training, but it seems to me that given the previous set of results, this experiment makes sense mostly at the pre-training stage where we have seen that code data can be effective.\n- **Language Inconsistency:** - For the results in Section 3.3.5, the authors conclude that training with code data *has little negative impact* on the performance of other tasks. But the numbers in Table 7 don't seem to show little negative impact. The large impact on the DuReader dataset has been already pointed out by the authors. Moreover, on CMNLI the performance decreases from 45.07 to 43.49 which is almost equal in magnitude to some of the gains reported in Table 2. I believe the language should be more consistent on the amount of improvement/decrement that can be considered a significant amount for the datasets and the experimental setup of the paper.\n- **Minor suggestions:** 1- In Table 3, there are multiple equal numbers but only one of them is in bold face. 2- In Table 4, I suggest reversing the rows and columns to make it consistent with the other tables."
            },
            "questions": {
                "value": "- For the Logic dataset, I see that the accuracy for many different models is 40.9. This value appears in Table 2, Table 3 and Table 6. Does this hint at a potential problem in this dataset? \n- Some of the datasets don't seem to be available in English (or at least I could not find an English version of them). Could you include some examples from these datasets translated to English, so the reader can get a sense of the nature and the difficulty level?\n- It has been observed that larger LLMs often behave differently than smaller LLMs. I'd be curious to hear the authors' thoughts on how they think their results might transfer to larger models? (to be clear, I understand the computation cost and I'm not suggesting that you run those experiments)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3194/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3194/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3194/Reviewer_BTot"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3194/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698779161278,
        "cdate": 1698779161278,
        "tmdate": 1700514615129,
        "mdate": 1700514615129,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ndP5xZp2gJ",
        "forum": "KIPJKST4gw",
        "replyto": "KIPJKST4gw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3194/Reviewer_viag"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3194/Reviewer_viag"
        ],
        "content": {
            "summary": {
                "value": "The paper explores the effect of including code data during pre-training and instruction tuning over the reasoning capabilities of LLMs. The authors conduct a set of ablation experiments where code is added/removed from both pre-training and fine-tuning and the model performance is measured over a set of reasoning tasks ranging from logical to legal reasoning. The results show that including code data during pre-training is more effective than during instruction tuning. Also, the results show that instruction tuning over the code can end up hurting performance on some tasks. The authors also experimented with a dynamic text-code mixing strategy during instruction tuning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The problem studied is interesting: the relationship between code data and reasoning and the paper aims to somehow tackle this issue.\n* The paper is well-written and the results are well-presented."
            },
            "weaknesses": {
                "value": "* From my understanding (and correct me If I'm wrong), the code model is trained on more overall tokens than the NL model. I would expect a study like that to control for the number of pre-training tokens while changing their nature i.e., text vs. code. If the code model is trained on as many natural text tokens as the baseline model in addition to having code in the pre-training data, then the code model should be expected to perform better because it was trained on more data. No surprise there. \n* It's hard to say whether the results reported have statistical significance. For example, in Table 2, the code model is only 0.13 points better than the NL model on ScienceQA. Are these results significant? And are these enough to conclude that code reasoning? I would expect the authors to run a statistical significance test to support their results. \n* The proposed dynamic mixing strategy produces very marginal improvements (except over logical reasoning) and has a negative effect on the performance over three tasks. The paper does not thoroughly investigate why this is the case. Also, the design of the mixing strategy seems rather arbitrary. \n* The evaluation does not cover mathematical reasoning, although it's one type of reasoning where we should expect great improvements since code data is roughly similar to math data."
            },
            "questions": {
                "value": "* The choice of the baseline model is unclear. Why use PanGu2.6B as your baseline model? \n* Why does your mixing strategy follow the 7:3, then 6:4, then 5:5? Why not, for example, 9:1, 7:3, 5:5, 3:7, 1:9? which would be both increasing and decreasing. And why only 4 phases? What's the intuition behind that design?\n\n\n\n===== POST REBUTTAL =====\n\nI thank the authors for their response. Based on the response and the expectation that the authors will add results from training a model on 150G of natural data, I have decided to increase my score to 5."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3194/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3194/Reviewer_viag",
                    "ICLR.cc/2024/Conference/Submission3194/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3194/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807872340,
        "cdate": 1698807872340,
        "tmdate": 1700854780532,
        "mdate": 1700854780532,
        "license": "CC BY 4.0",
        "version": 2
    }
]