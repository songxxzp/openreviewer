[
    {
        "id": "mlBlzIF8j5",
        "forum": "epFk8e470p",
        "replyto": "epFk8e470p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7339/Reviewer_W6Z2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7339/Reviewer_W6Z2"
        ],
        "content": {
            "summary": {
                "value": "The authors investigate how neural networks label action-recognition video frames and compare the neural network against human behavioral performance. They manipulate the stimuli to separate bodies and background and show that both neural networks and humans perform very well with full stimuli, body only or background only. Humans perform better with the body only compared to background only conditions. The authors propose an architecture that is loosely based on notions of modularity in the brain and this new architecture improves performance and matching to human behavioral data."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Building networks to recognize actions is of high importance to practical applications \n\nComparing how well networks perform to human performance is also of interest in terms of aligning machine and human visual capabilities."
            },
            "weaknesses": {
                "value": "The bottom line is that highly uncontrolled and bad datasets lead to spurious and uninterpretable results. This is the main challenge throughout. \n\nIn Fig. 1 bottom left, the authors claim that baseline models perform similarly well when tested with ORIG, body or BG. This is NOT what the results show. The results do not have error bars, let alone any minimally rigorous statistical analysis. From eyeballing the figure, it seems that ORIG>>BG>>Body. \n\nThe fact that humans can identify actions purely from the background frames with over 0.7 accuracy shows that\n(1) The dataset is way too easy\n(2) Background is a major confounding factor \n(3) Time is not needed in such an easy task \n\nAs far as I understand, the proposed architecture is trained very differently from the baseline architectures. The proposed architecture is trained with Body-only stimuli and does better on Body-only stimuli, and it is trained on BG-only stimuli and does not perform better on BG-only stimuli (again, no error bars, no statistics, this is all from eyeballing). Training yields better performance in general. \n\nIt would be great to present actual results on how well Yolo v8 separates body and background.\n\nThe manuscript only has one main figure and minimal additional information that does not satisfy basic standards in the field. There are no error bars, there are no comparisons across multiple different models, no comparisons with different datasets, no ablations, no description of the effects of key variables like size, etc."
            },
            "questions": {
                "value": "A key aspect of action recognition is likely to be dynamics and time, which is not studied here.\n\nWithin the study of action recognition from frames, it would be useful to use rigorous datasets. If the authors are interested in the effect body and background, it would be important to rigorously control for basic variables like contrast, size, and multiple other confounds."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7339/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698595951774,
        "cdate": 1698595951774,
        "tmdate": 1699636878132,
        "mdate": 1699636878132,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "e0VHuFge4y",
        "forum": "epFk8e470p",
        "replyto": "epFk8e470p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7339/Reviewer_JzJq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7339/Reviewer_JzJq"
        ],
        "content": {
            "summary": {
                "value": "This work begins by examining the similarities and differences between humans and deep neural networks in terms of action recognition. It demonstrates that a deep neural network trained with cross entropy on the entire video cannot perform action recognition when background information is omitted from the training data. In contrast to this, human subjects are capable of identifying activities solely from the body information. This highlights that DNN trained for action recognition incorrectly balances the body and background information present in the video data. In order for the deep neural network to exclusively distinguish actions coming from the body, the authors suggested using two different backbones, one for the body and one for the background. In addition to this, they implemented a loss function that was more complex and yet nevertheless compatible with their category-selective design. As a consequence of this, they demonstrated that a body-background separated backbone may produce an action recognition pattern that is comparable to the pattern seen in human participants, albeit with a significantly lower level of accuracy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The authors try to optimize deep neural works towards reproducing action recognition patterns observed in human subjects."
            },
            "weaknesses": {
                "value": "As the authors already included in the related works, having separate streams for different information in video is not new. For example, an early work on dynamic texture processing used two separate backbones for \u201cappearance\u201d (the scene) and \u201cdynamic\u201d (the optic flow). Their loss function also combines the matching of both appearance and dynamic features. It is possible that the L_{combined} here is new. However, the authors do not include any details on how they define L_{body}, L_{background} or L_{combined}. If the only difference this work has with other work is its usage of L_{combined} (I guess this is the cross entropy between predicted frames vs. the true frames), the novelty is very limited.\u00a0\n\n\n\nTesfaldet et al 2017 Two-Stream Convolutional Networks for Dynamic Texture Synthesis\n\n\n\nThis work needs a much more developed result section to fit as an ICLR manuscript. Its current format has one result. This result is not surprising given previous literature. I would encourage the authors to include a more detailed investigation of the proper loss functions, what predictive features are being used in humans to perform action recognition, etc. These extensions may strengthen this paper."
            },
            "questions": {
                "value": "Does the background contain any information about the actions in the video? If not, I hope the authors illustrate better why the background information should be used at all for action recognition. Would it be desirable that a neural architecture should focus on the \u201caction\u201d component of the video to perform action recognition? \n\nWhich component of the loss function contributes the most when body-only information is being used? Or when background-only information is being used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7339/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784209231,
        "cdate": 1698784209231,
        "tmdate": 1699636877993,
        "mdate": 1699636877993,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8vEhaCghaV",
        "forum": "epFk8e470p",
        "replyto": "epFk8e470p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7339/Reviewer_df1j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7339/Reviewer_df1j"
        ],
        "content": {
            "summary": {
                "value": "This paper takes insights from neuroscience and builds a model that processes body and background in images separately, aiming to improve the performance of action recognition."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper propose novel ideas of incorporating inductive bias from neuroscience in building artificial neural networks and shows that it does improve the performance of action recognition when compared with a baseline network. The paper is well-written and very clear. The human dataset collected in this paper is also valuable and should be perhaps incorporated into action recognition benchmarks."
            },
            "weaknesses": {
                "value": "This paper is a rudimentary effort in showing incorporating certain inductive bias from neuroscience could potentially help with artificial networks in certain tasks. However for the scope of the conference, I think the lack of comparison to state-of-art models as well as insights on how to even combine this inductive bias with state-of-art models makes this paper not suitable for application and making real impact on the task of action recognition. It is also not entirely true to assume that state-of-art model, which is much more complicated than a ResNet50 network does not implicitly extract information from the background and body when recognizing action."
            },
            "questions": {
                "value": "Discussion of how to incorporate this into state-of-art models is recommended."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7339/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802480510,
        "cdate": 1698802480510,
        "tmdate": 1699636877853,
        "mdate": 1699636877853,
        "license": "CC BY 4.0",
        "version": 2
    }
]