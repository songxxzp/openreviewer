[
    {
        "id": "eXuGnWdRsy",
        "forum": "TPAj63ax4Y",
        "replyto": "TPAj63ax4Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5796/Reviewer_VXus"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5796/Reviewer_VXus"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a straightforward pipeline for referring image segmentation through only language sentences. The pipeline is designed to be trained with only images and sentences without masks. To this end, the authors proposed a pipeline of three stages, that is segment, select, and correct, with existing foundation models, such as CLIP, SAM, etc. Experimental results somewhat demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The setting is interesting. The proposed pipeline is straightforward and in some perspective shows the power of the combination of existing foundation models."
            },
            "weaknesses": {
                "value": "This paper is a bit difficult to read. It abusively uses colorful dots to represent almost everything, including results, modules, and stages, which heavily hinders reading. This proposed pipeline is a straightforward combination of existing foundation models, what's the insight beyond the combination? In addition, some important key details about the correction stage are not clear, which requires further explanation."
            },
            "questions": {
                "value": "1. How do you get m^hat and m^c, is the m^hat output of LAVT? In addition, as the authors mentioned in sec3.3, the ZSBootstrap uses LAVT architecture, why the LAVT trained with texts and their pseudo visual masks work better and serve as an error correction model? \n\n2. What does \"grounding\" mean exactly? What's the difference between referring instance segmentation and grounding? I found the authors use them two both in the paper. \n\n3. Since LAVT is already a referring segmentation method, the authors re-trained it. Does that mean stage 1 and 2 are not valuable once the zsbootstrap model is trained?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5796/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5796/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5796/Reviewer_VXus"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5796/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698739992825,
        "cdate": 1698739992825,
        "tmdate": 1699636610180,
        "mdate": 1699636610180,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Pil43JNObd",
        "forum": "TPAj63ax4Y",
        "replyto": "TPAj63ax4Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5796/Reviewer_g3AQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5796/Reviewer_g3AQ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework for weakly supervised referring segmentation. It includes 3 steps, open-vocabulary segmentation, selection based on the CLIP model, and correction with a constrained greedy matching mechanism. The intuition behind the idea is interesting. Very strong performance is also obtained."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The use of CLIP for zero-shot matching is sensible. \n\n+ The overall learning process in step 3 is interesting. It is fully exploiting the dataset information: referring expressions for the same object should lead to the same mask, while  referring expressions for different objects should lead to different mask. \n\n+ The obtained performance is strong."
            },
            "weaknesses": {
                "value": "-  Notations are not well defined, making it really difficult to understand the details some sections. For example,  in {m^c_{i,j,k} }^c, it is really confusing what 'c' means here.  \n\n-  In general, I can appreciate the general idea of the learning mechanism for step 3.  It is really difficult to understand the equation (2) and (3). The reason could be unclear definitions, such as 'c'. Probably there are other undefined notations. \n\n- Fig.1 fails to provide the basic intuition of the third step, which, in my opinion, is the most valuable part of this paper."
            },
            "questions": {
                "value": "- In the ABLATION part, some experiences are based on the training set of the dataset (Table 2 & Table 3). What is the performance on the val and test sets? \n\n- Stage 3 is designed to correct the mistakes of the zero-shot choice with weak information. Why not use this information directly in stage 2 but take an extra large grounding model from LAVT?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5796/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5796/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5796/Reviewer_g3AQ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5796/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698740892220,
        "cdate": 1698740892220,
        "tmdate": 1699636610061,
        "mdate": 1699636610061,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "niyK9hV96u",
        "forum": "TPAj63ax4Y",
        "replyto": "TPAj63ax4Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5796/Reviewer_xVs3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5796/Reviewer_xVs3"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a weakly supervised framework for the task of referring image segmentation.\nIt contains three major components:  a segment component obtaining instance masks for the object mentioned in the referencing instruction, a select component using zero-shot learning to select the correct mask for the given instruction, and a correct component bootstrapping a model to fix the mistakes of zero-shot selection.\nFurther experiments show good performance compared to previous methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper is easy to follow. \n+ The performance is good in zero-shot and weakly supervised setup."
            },
            "weaknesses": {
                "value": "- The paper is a good engineering work, but lacks of enough novelty.\n- The experiment is incomplete and not convincing.\n- It lacks enough ablation studies on the effectiveness of each components used.\n- It lacks enough details on the pre-training bootstrapped model.\n- It lacks enough reference and comparison to previous methods."
            },
            "questions": {
                "value": "What is the major contribution of the proposed framework, since all components are borrowed from existing work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5796/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819658417,
        "cdate": 1698819658417,
        "tmdate": 1699636609927,
        "mdate": 1699636609927,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BayMR7Q6ZC",
        "forum": "TPAj63ax4Y",
        "replyto": "TPAj63ax4Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5796/Reviewer_6s9c"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5796/Reviewer_6s9c"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a weakly supervised method for referring image segmentation. Starting from class-agnostic object proposals, the proposed method first extracts masks corresponding to the object class which is referenced by the given text, using the open-vocabulary segmentation technique. Then, select the actually referenced mask using the existing zero-shot prompting method. Finally, to refine the obtained mask, the authors propose a correction method by using the assumption that they know if the two references indicate the same object or not. The proposed method obtains better performance than the existing methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "+ This paper addresses important problem that can effectively reduces the annotation cost for referring image segmentation.\n\n+ The paper is overall well written."
            },
            "weaknesses": {
                "value": "- My major concern is that the proposed method is significantly overfitted to specific datasets.\n\n-- First, the correction method in Section 3.3 requires a significantly strong assumption. The authors assume that multiple references tend to indicate a single object (mask), and they know which references actually correspond to the same mask. However, this assumption will not work for another dataset, and especially, knowing whether two references point to the same object or not is infeasible for a weakly supervise setting.\n\n-- If I understood correctly, dataset class projection in Section 3.1 is to determine which class among the 80 classes in COCO corresponds to the key noun. The COCO class list is carefully curated by human, and each of the 80 classes is mutually exclusive. In the real open-world setting, assuming the specific set of classes is infeasible.\n\n- My second concern is the novelty. Methods for obtaining object proposals, and matching those proposals with zero-shot prompting, are already well-explored techniques for the same research field.\n\n- More baselines should be included.\n\n-- GroundingDINO already conducted referring object detection. GroundingDION + SAM can be directly used for referring image segmentation. \n\n-- For a fairer comparison with Yu et al, FreeSOLO+Select method without Grounding DINO.\n\n-- SAM Proposals (w/o Grounding DINO) + ReverseBlur prompting"
            },
            "questions": {
                "value": "- Why testB performance is signifcantly lower than that of testA? \n\n- Excepting the correction method, please include the ablation studies on zero-shot validation and test sets for all datasets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5796/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698988939124,
        "cdate": 1698988939124,
        "tmdate": 1699636609816,
        "mdate": 1699636609816,
        "license": "CC BY 4.0",
        "version": 2
    }
]