[
    {
        "id": "xnmxL4FtBP",
        "forum": "4DoSULcfG6",
        "replyto": "4DoSULcfG6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3924/Reviewer_JKci"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3924/Reviewer_JKci"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a data poisoning attack, called Chameleon, to enhance the privacy leakage due to label only membership inference attacks. It first shows that current attacks that aim to enhance privacy leakage via poisoning are not effective in label-only MIA threat model. The attack fails because after poisoning both the IN and OUT models misclassify the target samples. To improve the attack efficacy, Chameleon tailors the number of replicas of poisoning samples for each challenge sample. The paper shows that such poisoning significantly improves label-only MIA accuracy especially at low FPRs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Chameleon idea is elegant and easy to implement\n- Intuition and other aspects of the attack are well explained"
            },
            "weaknesses": {
                "value": "- Chameleon is an expensive attack\n- I am not sure how will such attack be useful in practice due to the computations involved\n- Some parts of the paper need to improve presentation, e.g., theoretical attack and figure 1"
            },
            "questions": {
                "value": "The attack proposed is very elegant in that it is easy to implement and outperforms prior attacks. Also, the explanation of the attack is  clear and easy to understand. The paper also does a fair job in evaluating their proposed attack. Overall I think this is a good paper, but  I have the following concerns:\n\nAttack computation cost and utility:\n- Chameleon is an expensive attack given the number of models one has to train to find the right number of poisoning sample replicas. Can authors discuss the compute cost involved? I didn\u2019t see any discussion in the main paper.\n- Given the high computation cost and the fact that modern ML model architectures are generally huge, I wonder where will this attack be useful? Which type of adversaries can afford it? It will be good to clearly discuss these aspects.\n\nSome concerns about the evaluations\n- For C100, Chameleon adds on average 0.6 replicas of poisoning samples, which means there are 40% data which need no poisoning. This means MIAs without any poisoning should work well. But this is not reflected in Table 1 results. Clarify.\n- Minor: Given that modern ML systems have generally very large and multimodal models, it might be useful to have evaluations on large and/or multimodal models.\n\nClarity of paper:\n- Figure 1 is not readable: I could not understand what it is trying to convey. Please clarify\n- Theoretical attack section currently does not clearly explain what is the attack and why this analysis is needed if the same conclusions can be drawn from empirical analysis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3924/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3924/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3924/Reviewer_JKci"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3924/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698608212121,
        "cdate": 1698608212121,
        "tmdate": 1699636352777,
        "mdate": 1699636352777,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5XRzf8pXrR",
        "forum": "4DoSULcfG6",
        "replyto": "4DoSULcfG6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3924/Reviewer_ECEy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3924/Reviewer_ECEy"
        ],
        "content": {
            "summary": {
                "value": "The key contribution of this paper is to present a poisoning strategy to enhance the success of label-only membership inference attacks. The paper first shows that an existing poisoning regime negatively impacts the label-only attack's success and proposes a new way to calibrate the number of poisoning points to inject. And then, the paper proposes a way to construct shadow models and perform membership inference. In evaluation, the paper demonstrates that poisoning can increase the TPR by an order of magnitude while preserving the model's performance. The paper also analyzes the impact of attack configurations and further tests if (and also shows) DP reduces the attack success."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper presents a new poisoning attack for enhancing label-only MIs.\n2. The paper shows the poisoning can increase the attack success by 18x.\n3. The paper is well-written"
            },
            "weaknesses": {
                "value": "1. The poisoning seems to be a straightforward adaptation of Tramer et al.\n2. The proposed label-only MI seems to be impractical.\n3. (Sec 3.3) The claim about \"theoretical\" attack is unclear.\n\n\nOverall comments:\n\nI agree it is a nice extension of existing work (Tramer et al.) to label-only settings. At the same time, this attack itself and the poisoning strategy are not surprising. So, my impression of this paper is slightly below the acceptance threshold. But if there are surprising factors that I've missed, I am not willing to fight for rejection.\n\n\n**[Straightforward Extension]**\n\nOf course, existing poisoning could not work well against an adversary who only observes hard labels. The adversary cannot \"exploit\" the impacts of poisoning until there is a change in the target's label. If too many are injected, the attacker may not know whether the target is a member. So, in the label-only settings, the key is to calibrate the number of poisoning samples. It is therefore not surprising in Section 3.2 that an \"adaptive\" poisoning strategy is needed.\n\n\n**[Practicality of This Poisoning]**\n\nHowever, I believe that choosing the right threshold $t_p$ is more challenging than shown in this paper. The paper assumes that the adversary can know the \"underlying distribution.\" \n\nBut considering that the label-only attacks are for studying the practicality in the \"true black-box\" settings (e.g., hard-labels), I wonder how well this attack can perform when there's a slight distributional difference between the training data an adversary uses and the victim's. Indirectly, the ablation study shows the proposed label-only attack is a bit sensitive to the choice of a poisoning threshold. \n\nIn practical scenarios, when a practitioner wants to check the risks of \"practical\" label-only membership leakage, the proposed attack may not be a useful one to use.\n\n\n**[Theoretical Attacks (Sec 3.3)]**\n\n(1) In most cases, the theoretical analysis means the best possible attack that an adversary can perform under a specific attack configuration. But I am not sure whether the paper presents the same.\n\n(2) I am a bit unclear on how the paper theoretically analyzes the impact of poisoning samples on the leakage. It depends on many factors, such as the training data and/or the choice of a model and a training algorithm.\n\nI think the section could a bit mislead readers."
            },
            "questions": {
                "value": "My questions are in the detailed comments in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concern about the ethics."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3924/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699154794182,
        "cdate": 1699154794182,
        "tmdate": 1699636352699,
        "mdate": 1699636352699,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cd9KEWGCAU",
        "forum": "4DoSULcfG6",
        "replyto": "4DoSULcfG6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3924/Reviewer_uopX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3924/Reviewer_uopX"
        ],
        "content": {
            "summary": {
                "value": "This paper targets at Membership Inference (MI), in which an attacker seeks to determine whether a particular data sample was included in the training dataset of a model. In contrast to the most of work in this area, the paper considers a less favorable setting: the attacker has access only to the predicted label on a queried sample, instead of the confidence level. I think this is an important problem, which should be interesting to the communities of both DP and privacy attack. To address this challenge, the paper proposes a new\nattack Chameleon that leverages adaptive data poisoning to achieve better accuracy than the previous work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper proposes a new attack Chameleon that leverages adaptive data poisoning to achieve better accuracy than the previous work.\n2. The paper observes an interesting phenomenon: for different challenge point, the sweet spot of the number of samples needed in the data poisoning is different. The paper also proposes a theory to reflect this phenomenon.\n3. Various experiments have shown the advantages of the new method."
            },
            "weaknesses": {
                "value": "Although the attack and the observation is interesting, I think the paper has the following weak points:\n\n1. Time complexity. Clearly from Algorithm 1, to run the adaptive poisoning, the attacker has to run the training model much more times than the baseline algorithms, making the proposed algorithm less practical. However, the paper touches little about this topic, and does not provide any comparison in the experiment section. I think this information is crucial for the readers to better understand and appreciate the proposed algorithm.\n\n2. Multiple challenge points. Usually in practice, the attacker needs to attack multiple challenge points instead of the only one. Although the paper briefly discusses this in the appendix, I think it is far from enough. Specifically, Algorithm 2 is just a simple generalization of Algorithm 1, neglecting many interesting and important problems due to more than one challenge points. For example, the problem of time complexity becomes even worse. Furthermore, due to the correlations of different challenge points, it is not clear how Algorithm 2 performs. Considering an extreme case when there are two challenge points opposing each other, it is possible after k_max iterations, the algorithm can not find meaningful k_i for both points simultaneously.\n\n3. Clarity (minor points). The paper needs to improve the clarity. For example, many definitions are used without being defined, e.g., LIRA, challenge point, in+out model. It is better to provide those definitions in the preliminary to make the paper more self-contained."
            },
            "questions": {
                "value": "Please refer to the section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3924/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3924/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3924/Reviewer_uopX"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3924/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699207947782,
        "cdate": 1699207947782,
        "tmdate": 1699636352625,
        "mdate": 1699636352625,
        "license": "CC BY 4.0",
        "version": 2
    }
]