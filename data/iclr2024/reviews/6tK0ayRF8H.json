[
    {
        "id": "LHilFD7wMs",
        "forum": "6tK0ayRF8H",
        "replyto": "6tK0ayRF8H",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission217/Reviewer_7GFK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission217/Reviewer_7GFK"
        ],
        "content": {
            "summary": {
                "value": "This paper notices that existing text embedding models mainly use cosine function as a part of the objective function, but cosine function has a saturation zone, which may cause gradient vanishing problem and influence the quality of text embeddings. To mitigate this problem, this paper proposes to evaluate the angle difference between two text embeddings for optimization. Experiments on variable lengths of text datasets, including a newly introduced long-text dataset, are conducted to evaluate the performance of the proposed model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper identifies an interesting research question, the gradiant vanishing problem appearing at the saturation zone of cosine function influences the quality of text embeddings.\n\n2. The proposed solution of using angle difference for optimization is orginal and novel.\n\n3. Experiments on semantic textual similarity task are sufficiently conducted."
            },
            "weaknesses": {
                "value": "Despite an appealing motivation and an interesting solution, I still have the following concerns:\n\n1. From my point of view, the only technical contribution of this paper is to design how to evaluate angle difference. This contribution is indeed interesting, but is a bit superficial and insufficient for a long research paper of ICLR standard. I expect authors to propose more __insightful__ designs to better solve the gradient vanishing problem.\n\n2. The explanation of why saturation zone in cosine function influences text embedding learning is not clearly written at the Introduction section. Authors are suggested to explain more about the meaning of saturation zone and why it causes gradient vanishing problems.\n\n3. Usually we encourage authors to conduct the same experiment multiple times and report both mean and standard deviation, in order to verify that the proposed model indeed significantly outperforms baselines. However, I see mean but not standard deviation in the paper."
            },
            "questions": {
                "value": "1. Authors use absolute value at Eq. 6. But absolute function in pytorch or tensorflow is not differentiable, how do authors deal with error backpropagation for absolute function?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission217/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission217/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission217/Reviewer_7GFK"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission217/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698698575148,
        "cdate": 1698698575148,
        "tmdate": 1699635947422,
        "mdate": 1699635947422,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FwhdxIvFmi",
        "forum": "6tK0ayRF8H",
        "replyto": "6tK0ayRF8H",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission217/Reviewer_NPkC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission217/Reviewer_NPkC"
        ],
        "content": {
            "summary": {
                "value": "To overcome the negative impact of vanishing gradients caused by the cosine optimization function, this paper proposed a novel angle-optimized target to improve the quality of text embeddings. Moreover, this paper conducted extensive experiments to prove the effectiveness of the proposed method. Meanwhile, this paper also developed a novel long-text STS dataset to support the community."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThis paper proposed a novel angle-optimized target to enhance the learning ability of contrastive learning-based representation learning models, which tried to alleviate the problem of vanishing gradients. \n2.\tThis paper developed a novel long-text STS dataset to better evaluate the performance of representation learning models. \n3.\tThis paper also explored LLM-based supervised data generation and contrastive learning, which is very interesting."
            },
            "weaknesses": {
                "value": "1.\tFirst of all, the authors argued that gradient vanishing problem is caused by the saturation zones in cosine functions in the optimization target. However, as far as I know, the gradient vanishing problem is mainly due to the deep structure. The saturation zones can be used to prove the high similarity between sentences. Therefore, the motivation of this paper is not so convincing. More explanations are needed. \n2.\tSecond, the authors focused on contrastive learning target, which limits the application range of the proposed method. The authors should provide more evidence to demonstrate the effectiveness of their method since their main contribution is adding an additional target in contrastive loss. \n3.\tThird, the related work in this paper is not sufficient enough. More content should be cited, such as different contrastive loss designs, sentence similarity measurement designs, etc."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission217/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698716370381,
        "cdate": 1698716370381,
        "tmdate": 1699635947353,
        "mdate": 1699635947353,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UoPbGhJmhs",
        "forum": "6tK0ayRF8H",
        "replyto": "6tK0ayRF8H",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission217/Reviewer_3nS6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission217/Reviewer_3nS6"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel angle-optimized text embedding model to improve the semantic textual similarity (STS) tasks, by mitigating the vanishing gradients of cos similarity. Specifically, the authors employ a contrastive learning objective and introduce optimization in a complex space to address the saturation zone in the cosine function. Extensive experiments are conducted to show the effectiveness of the proposed method on various tasks including short-text STS, long-text STS, and domain-specific STS."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method of calculating similarity looks novel to me.\n\n2. The impact of the method has the potential to be significant in many fields."
            },
            "weaknesses": {
                "value": "1. According to the paper, the motivation for introducing a complex space is to deal with the vanishing gradient of cos. In this sense, it would be great if techniques like gradient clipping and gradient normalization could be compared. \n\n2. The writing can be improved. E.g., section 3.4 is a bit confusing to me. See my questions below.\n\n3. I am also worried about the empirical significance. In table 2, the proposed method only improves the performance marginally (<1%) compared to SimCSE-BERT. I appreciate the effort that the p-value is reported and yet the p-value is smaller than 0.05 according to the caption of table 2."
            },
            "questions": {
                "value": "1. In section 3.4, X is decomposed into real part Xre and imaginary part Xim, both of which have dimension 1. However, in the context of contrastive learning / the use of cos similarity, X is often high dimensional. How do you decompose X? If I am not mistaken, this part is missing in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission217/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819190810,
        "cdate": 1698819190810,
        "tmdate": 1699635947263,
        "mdate": 1699635947263,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Xvqsg5vBOi",
        "forum": "6tK0ayRF8H",
        "replyto": "6tK0ayRF8H",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission217/Reviewer_DPjM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission217/Reviewer_DPjM"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new method called AnglE to address the vanishing gradient problem of optimizing cosine similarity in text embedding learning models. AnglE uses an angle-based optimization method to learn text embeddings in a complex space. The method is demonstrated to outperform state-of-the-art models on various semantic textual similarity (STS) tasks, including short-text STS, long-text STS, and domain-specific STS. Additionally, AnglE can be used with limited labeled data and LLM-annotated data, and it achieves competitive performance in these settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper addresses an important issue in optimizing the cosine similarity of learning text embeddings, and the proposed method is interesting and novel.\n* It introduces the GitHub Issues Similarity Dataset as a testbed for evaluating model performance on long-text STS tasks.\n* The proposed method achieves promising results on a wide range of STS tasks."
            },
            "weaknesses": {
                "value": "* Some technical details are not clearly explained. For example, while the angle objective optimizes the text representations in a complex space, it's unclear how these complex vectors are obtained as the representations from language models are real vectors.\n* The paper seems to have missed discussions with a few important related studies. For example, [1] addresses the gradient vanishing issue by incorporating cosine distance in learning text embeddings, [2] designs angular softmax objectives to learn visual representations. The LLM-supervised learning procedure largely follows the prompt-based training data generation paradigm in [3,4,5]. While this part is not the major contribution of the paper, it's better to reference these related works as well.\n\nReferences:  \n- [1] \u201cSpherical Text Embedding.\u201d NeurIPS (2019).\n- [2] \u201cSphereFace: Deep Hypersphere Embedding for Face Recognition.\u201d CVPR (2017).\n- [3] \u201cGenerating Datasets with Pretrained Language Models.\u201d EMNLP (2021).\n- [4] \u201cGenerating Training Data with Language Models: Towards Zero-Shot Language Understanding.\u201d NeurIPS (2022).\n- [5] \u201cZeroGen: Efficient Zero-shot Learning via Dataset Generation.\u201d EMNLP (2022)."
            },
            "questions": {
                "value": "* Could you explain how the complex vectors are obtained exactly from the language models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission217/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698957897398,
        "cdate": 1698957897398,
        "tmdate": 1699635947089,
        "mdate": 1699635947089,
        "license": "CC BY 4.0",
        "version": 2
    }
]