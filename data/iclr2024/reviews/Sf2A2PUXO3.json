[
    {
        "id": "BGVg9HH2dT",
        "forum": "Sf2A2PUXO3",
        "replyto": "Sf2A2PUXO3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2885/Reviewer_BakJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2885/Reviewer_BakJ"
        ],
        "content": {
            "summary": {
                "value": "This paper\u2019s main goal is to explore Rashomon sets of feed-forward neural networks with the help of dropout. Both Gaussian and Bernoulli dropouts are considered, the former involving the addition of noise to the weights of the networks. The approach is used to estimate the Rashomon set, and proof of the consistency of the approach is provided. Empirical results show the effectiveness of the approach on various datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea is intuitive. It leads to impressive computation time saving compared to other approaches from the literature. The article is well-written and clear. The experiments are directly in line with the motivations of the work (ethical concerns)."
            },
            "weaknesses": {
                "value": "**Major**\n\n1.1 \u2013 The biggest weakness of the approach concerns its limitations. As honestly discussed by the authors in Section 6 \u2013 Limitations, the fact that when the hypothesis space to explore is huge (that is when the predictor has many parameters to tune), the exploration that is done by the dropout approach is fairly limited. This is clearly related to Proposition 4, where an important value of $M$ is necessary for layers having hundreds of neurons and an important value of $k$ is necessary with complex neural networks.\n\n1.2 \u2013 Two scenarios could occur: the first one is that the hypothesis space to explore is relatively small. That is, it is fairly explored by the dropout method. But, even though a 30x to 5000x speedup over other approaches is seen, it is never defended that those other approaches are not scalable with small hypothesis space. Even though there is a huge speedup gain, if the other approaches are relatively fast (do not take hours to compute), then why favourising dropout? The second scenario is that the hypothesis space to explore is large. The time gain is then undermined by the limitation in the exploration. Plus, when it comes to large models, it is common to retrain only the classification head of the predictor, or to fix many layers; doing so could really fasten the retraining scheme, thus undermining the potential advantage of the dropout approach.\n\n2 \u2013 It seems to me that both the depiction in Figure 2 and the speedup reported in Table 1 are lacking important details. For example: How many different models are sought? How many reruns were done for retraining VS how many dropouts were computed? What was the total time for each individual method? To my understanding, many reruns are already needed in the first place, no matter the approach, in order to ensure that the reference model is an \u00ab\u00a0empirical minimizer\u00a0\u00bb; was that taken into account when comparing the time for building the empirical Rashomon sets in Table 1? What was the size of the predictor used on these different UCI tasks (this kind of information is necessary in the main article, not the supplementary material)?\n\n3 \u2013 I feel like something is conceptually wrong with the comparison between retraining and the current dropout scheme. Retraining makes it such that the validation loss is the highest possible. Therefore, it makes sense that many runs are needed in order to find models close to the \u00ab\u00a0empirical minimizer\u00bb. With the dropout scheme, an empirical minimizer is found, and then dropout is applied while making sure the training loss does not diminish too much. The two approaches do not have the same objectives.\n\n4.1 - The dropout leads to a scheme where each new model depends on the initial model. All of the models are thus dependent. Therefore, the estimation of the Rashomon metrics is biased. And while \u00ab\u00a0not all estimators of predictive multiplicity metrics carry a theoretical analysis of its statistical properties such as consistency and sample complex\u00a0\u00bb, I feel like it is a property of interest. Indeed, one of the motivations of the work is the need for ethics and, more specifically, fairness. I see the goal in exploring the Rashomon sets to find many predictors giving different predictions to people for different reasons. But having all of the models interconnected makes it such that the reasons for the predictions are all linked and just a few are explored with the dropout scheme.\n\n4.2 \u2013 Proposition 5 aims at proving that the approach is not biased, but relies on the assumption that \u00ab\u00a0 the models around W\u2217 are uniformly distributed in a d-dimensional ball with center $\\mathbf{W}^*$ and radius $\\delta$, i.e., $B(\\mathbf{W}^*, \\delta)$. Accordingly, we may assume that the population means $\\mu$ for a sample can be expressed as [...]\u00a0\u00bb. The method explicitly does that (especially the Gaussian dropout), exploring around the \u00ab\u00a0population mean\u00a0\u00bb, that is, the empirical minimizer. Therefore, assuming the uniform distribution of the Rashomon set around a center trivially leads to the unbiasedness of the dropout scheme, but is unreasonable.\n\n**Minor**\n\n1 \u2013 Typo: \u00ab\u00a0Moreover, as lone as\u00a0\u00bb"
            },
            "questions": {
                "value": "1 \u2013 It is said that \u00ab\u00a0not all estimators of predictive multiplicity metrics carry a theoretical analysis of its statistical properties such as consistency and sample complex\u00a0\u00bb Could you provide some citation supporting this claim?\n\n2 \u2013 What justifies fixing a single Bernoulli or a Gaussian dropout parameter for all layers simultaneously? Shouldn\u2019t the layers be treated independently?\n\n3 \u2013 Concerning the quantification of predictive multiplicity, it is said that \u00ab\u00a0[f]or example, Long et al. [2023], Cooper et al. [2023] and Watson-Daniels et al. [2023] quantify predictive multiplicity by the standard deviation, variance and the largest possible difference of the scores (termed viable prediction range (VPR) therein) respectively\u00a0\u00bb So, what definition between those three is retained in the article?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This might not be of an \"ethical\" concern, but the 9-page limit is exceeded by  1/4th ~ 1/3rd of a page."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2885/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2885/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2885/Reviewer_BakJ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2885/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698328653953,
        "cdate": 1698328653953,
        "tmdate": 1699636232099,
        "mdate": 1699636232099,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jG7KzS4g5I",
        "forum": "Sf2A2PUXO3",
        "replyto": "Sf2A2PUXO3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2885/Reviewer_kvbj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2885/Reviewer_kvbj"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the possibility of using Dropout to explore the Rashomon set. It proves that for a FFNN, we could bound the probability that a Dropout realization is in a certain Rashomon set. In experiments it shows that the proposed the method does not explore the Rashomon set as effectively as AWP (as measured by several predictive multiplicity metrics), but is much faster as it does not retrain any model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper establishes some theoretical bounds (although seemingly loose) on the probability that FFNNs with Dropout are in the corresponding Rashomon Set.\n2. The proposes method is easy to implement."
            },
            "weaknesses": {
                "value": "1. It is unclear what's the practical use of the propose method. It is fast, but it does not explore the Rashomon set well. For this reason, we can only mitigate predictive multiplicity *as estimated by Dropout* but not in general. \n2. Following 1, it seems like additional experiments on whether the mitigation via Dropout also transfers to, say, AWP, is interesting. \n3. The bounds in Proposition 2 and 3 only converge to 1 when $d\\to\\infty$, which does not seem like useful. See Q3 as well.\n4. It is not clear why a concentration bound helps. Notably, in applications, we want the models that are in the Rashomon set but closer to the boundary. In fact, it seems like in practice we need to sample a few weights and empirically verify that they have low loss (?). If so, a concentrated distribution, especially one that's more concentrated when the dim of the model increases, seems like a bad feature. A method that samples very diverse model that potentially has a higher probability of falling outside the Rashomon set seems more desirable."
            },
            "questions": {
                "value": "1. AWP is slower due to re-training, but the models are trained only once. Therefore, doesn't it run *faster* than Dropout (because it uses fewer samples/models to explore the Rashomon set) with a reasonably large test data?\n2. What does \"5 models\" mean in Figure 4b? 5 different base weights, or 5 different architectures?\n3. Is $\\epsilon$ and the $L$ in Eq.(5), (7) and (10) related to the \"sum\" of loss or the \"mean\" of loss? It seems like it's the sum? If so, by changing $\\epsilon$ to some offset on the mean loss, we can probably get a convergence basing on the sample size, which is much more meaningful than dimension of the model's hidden layers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2885/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2885/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2885/Reviewer_kvbj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2885/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698354370056,
        "cdate": 1698354370056,
        "tmdate": 1699636232011,
        "mdate": 1699636232011,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tGsXt2gnyM",
        "forum": "Sf2A2PUXO3",
        "replyto": "Sf2A2PUXO3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2885/Reviewer_pYox"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2885/Reviewer_pYox"
        ],
        "content": {
            "summary": {
                "value": "The paper described a Rashomon set exploration method through Drop-out with probabilistic bound. The paper starts with fairly well-covered literature of Rashomon set research and motivates its proposal by pointing out the computation cost of existing empirical solution (re-training, AWP). The solution is fairly simple by adopting Drop-out where Rashomon set likely rests. Probably the most significant part of this paper (theoretically) would be pointing out the probabilistic bound of Rashomon set under Drop-out. Empirical results show the proposed method is computationally efficient than previous solutions and even showing better divergence metric than retraining."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Predictive multiplicity itself is an interesting topic that worths more investigation. The method proposed in this work is a great complement of existing literature in this field.\n2. The paper is well written and motivated. It comes with sufficient background knowledge to understand the gap in the literature.\n3. Potential application of this approach is covered in Section 5, which is good since I was concerning where people can use this innovation in their work."
            },
            "weaknesses": {
                "value": "1. Model augmented by Dropout could result in a fairly small search space of Rashomon set. I am not very convinced that this is a good idea in practice if our goal is to look for a better model that can address various reliability problem of predictive model. e.g. fairness etc. It maybe inspirational to see the movement of predictive multiplicity measurement, but I am wondering what is the practical meaning of it.\n2. The paper demonstrates the effectiveness of the proposed method on toy datasets that were used for decades. As the paper concerns the efficiency of existing methods, I am wondering if the authors can introduce more realistic tasks to show the effectiveness of the proposed method quantitatively.  While COCO is good example, it is very qualitative without much statistic support. \n3. There is a descriptive gap in section 3.2 where transforming deviation between $L_{SSE}(\\mathbf{w}_D^*)$ and $L_{SSE}(\\mathbf{w})$ suddenly become  deviation between  $L_{SSE}(\\mathbf{w}_D^*)$ and $L_{SSE}(\\mathbf{w}')$. I don't quite see why they are aligned or if the model works correctly under $L_{SSE}(\\mathbf{w}')$ if it is not trained with such dropout rate."
            },
            "questions": {
                "value": "The proposition 1 uses deviation between  $L_{SSE}(\\mathbf{w}_D^*)$ and $L_{SSE}(\\mathbf{w}')$ but not original model parameter $L_{SSE}(\\mathbf{w}')$. How to make the connection ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2885/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769430926,
        "cdate": 1698769430926,
        "tmdate": 1699636231930,
        "mdate": 1699636231930,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Lri9gAkvBT",
        "forum": "Sf2A2PUXO3",
        "replyto": "Sf2A2PUXO3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2885/Reviewer_7265"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2885/Reviewer_7265"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of how to measure and mitigate predictive multiplicity.\nTo achieve them, the authors utilize the dropout technique to explore the models in the Rashomon set.\nRigorous theoretical analysis is provided to connect dropout and Rashomon set.\nNumerical results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method is simple, straightforward, and well-motivated. Utilizing the dropout technique to explore the models in the Rashomon set is interesting.\n2. Rigorous theoretical analysis is provided to connect dropout and Rashomon set.\n3. The paper is well-written and well-organized. The authors first show the implementations on linear models and extend them to feedforward neural networks. \n4. The limitations and potential solutions are also discussed in the paper."
            },
            "weaknesses": {
                "value": "1. In the experiments, the authors mentioned that \"On the other hand, AWP outperforms both dropouts and re-training, since it adversarially searches the models that mostly flip the decisions toward all possible classes for each sample.\" I may miss some details of the method part, how can the proposed method to adversarially search the models since the dropout is random?\n2. As mentioned by the authors, good performance comes at the cost of efficiency."
            },
            "questions": {
                "value": "1. It seems the proposed method is only evaluated on in-distribution scenarios. Can it be applied to out-of-distribution data?\n2. Are the uncertainty scores calibrated? In other words, are the confidence scores reliable?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2885/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2885/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2885/Reviewer_7265"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2885/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698931973849,
        "cdate": 1698931973849,
        "tmdate": 1699636231810,
        "mdate": 1699636231810,
        "license": "CC BY 4.0",
        "version": 2
    }
]