[
    {
        "id": "pHfDbPi8FI",
        "forum": "kxLMnvnZv0",
        "replyto": "kxLMnvnZv0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4351/Reviewer_mQDh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4351/Reviewer_mQDh"
        ],
        "content": {
            "summary": {
                "value": "ComNet is a convolutional image classification network, with inspiration for its operations drawn heavily from biological structure of columnar organization.  In particular, ComNet uses stacked group convs to implement structures that parallel column neurons.  The result is a relatively simple network that performs well, more efficient (in multiple cost measures) than relevant baselines using similar structures for both training and inference.  Measurements were performed on ImageNet."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The system is well-motivated, and simple while achieving good performance.  The parallel with columns is an interesting way to view group convs.  I also like the analogies with biological neuron organization --- while a lot of papers allude to loose parallels or look at specific low-level behaviors, I think this paper makes concrete links for nearly all its components in a way understandable to those (like me) without much neuroscience background."
            },
            "weaknesses": {
                "value": "While there are several interesting ideas, and use of columns in CCM appears to work well, I think the links to group convolutions could be drawn out and studied even more explicitly.  See my comments below on Sec 4.7 for more details.\n\nIn addition, while there are several relevant comparison systems --- especially ResNeXt, which uses group convs, there are no comparisons to many more recent CNNs that may also be relevant.  Searching on paperswithcode.com for CNNs on ImageNet yields many from the past couple years.  (Note not all of these are necessarily relevant comparisons --- but some likely are:  the current leaderboard #1 is RevCol which also uses the term \"column\" extensively, though I think in a different way, while MogaNet uses similar param counts)."
            },
            "questions": {
                "value": "Sec 4.7:\n\n  * \"The idea of IR in ResNeXt is more similar to us however, the major difference is that\nResNeXt has multiple blocks per stage, and each performs replication\":  this could be spelled out more, as this sentence leaves the similarities vague.  ResNeXt uses group convs, but I don't think IR is mentioned in that paper --- however, IR followed by group conv is the same as a single regular convolution ($group_conv(IR(x)) = conv(x)$), so if resnext performs $group_conv(f(conv(x)) = group_conv(f(group_conv(IR(x)))$ where $f$ is normalization+activation, then indeed there is a close link, and stacking more group convs is a major difference.\n\n  * discussion comparing to group convolutions:  This is a good discussion, particularly the second point.  However, I think it could be expanded further, and perhaps referred to even more throughout the paper.  If a \"CCM\" is implemented by group conv + residual, the fact that there are stacked group convs without between-group recombination could be brought more to the forefront and studied explicitly.  What happens when varying $l$ to be smaller, so that there is a 1x1 recombination more frequently, even for $l=1$ (and more blocks); or alternatively, introducing 1x1 recombinations after *every* group conv?  Is this model then the same as ResNeXt?  If so, then this is a critical difference.  Overall it would be good to nail this down more, going incrementally from ResNext-like operations (group conv followed by 1x1 recombination/mapping) to CCM.\n\n  * \"no blocks, only stages\":  It's unclear in this section what constitutes the notion of a \"block\" and why comnet does have it --- since it does have residual connections between each of the $l$ conv applications (which are analogous to \"blocks\").  What is meant by \"no block\" here and why is it advantageous?  Does it have to do with how often there is cross-column combination?\n\n\nSmaller comments on other sections:\n\n* related work:  An older related paper, is \"network-in-network\" (https://arxiv.org/pdf/1312.4400.pdf), which interprets 1x1 convs as applying a small MLP within each conv window, extending plain convs beyond linear combination, in the same vein as this work interprets stacked group convs as columnar operations.\n\n* sec 2, 4.6:  \"exponential param growth\" --- what does this refer to and how exactly is it exponential?  increasing depth is linear param growth, while increasing width is quadratic.\n\n\n* Tables:  A0, B1, C1, etc:  I didn't see the variants described in the text, only listed in an appendix table.\n\n* Table 2:  is this showing the right numbers?  none of the comnet models have a difference in params or flops for plain vs SR, but still a speed increase\n\n\n\n* fig1: abstract mentions parnet next to ref to fig1, but parnet is not in this figure.\n\n* fig 2d:  \"Conv, b=M\":  the parameter \"b\" is not described anywhere; is the number of groups in a group conv?\n\n* eq 1:  $T_{ccm} = CCM(.)$:  I don't see anywhere this is used.  Also, would be good to provide a definition of CCM in this set of equations, expressed in terms of convs or group convs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4351/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698520133083,
        "cdate": 1698520133083,
        "tmdate": 1699636406464,
        "mdate": 1699636406464,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6RYMBdZNID",
        "forum": "kxLMnvnZv0",
        "replyto": "kxLMnvnZv0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4351/Reviewer_Kwm3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4351/Reviewer_Kwm3"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a ConvNet architecture called CoMNet, which is inspired by the structural organization of cortical modules in the biological visual cortex. CoMNet is designed to offer efficiency in multiple dimensions such as network depth, parameters, FLOPs, latency, branching, and memory budget simultaneously. The authors propose a Multi-Dimensional Efficiency (MDE) evaluation protocol to test the model. Through comprehensive evaluations, CoMNet is shown to outperform several representative ConvNet designs including ResNet, ResNeXt, RegNet, RepVGG, and ParNet in the MDE setting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea is inspired from the visual cortex. The authors explore the Columnar Structure, Shared Input or Input Replication, Limited Synaptic Connections, Massive Parallelization and Lateral Connection Inhibition which are refer to the biological visual cortex. Comprehensively , the authors club these valuable cortex properties into one architecture through a systematic study.   \n2. The authors build their template ConvNet step-by-step. They first combine the fundamental design attributes of the cortex design and then translate those attributes into its neural equivalent. Futher, they develop its CNN equivalent and develop the fundamental computational \n CoMNet-unit. This construct process is sequentially and well founded.   \n3. In order to improve the model's efficiency, the authors focus on $5$ crucial dimensions include latency, depth, branching, FLOPs and parameters. Through careful design, the CoMNet is both with powerful representation ability and hardware efficiency."
            },
            "weaknesses": {
                "value": "1. Just as the authors mentioned, most of the ideas inspired from the visual cortex have been used in ConvNets, such as weight sharing, shortcut, groups convolution, etc. For each design point of this article, it is not novel. Generally, the novelty of this paper is weak.  \n2. Inspired from the biological function, there are many frameworks to simulate the structure. The authors and previous works provide different solutions, and the designed modules are based on some hypothesis which are not proved.   \n3. The experiments is insufficient."
            },
            "questions": {
                "value": "1. The experiments only compare with partial works, all works should be compared in different level, such as efficientnet is only exist in R1 and Parnet only in R4.   \n2. One more ablation study, the function of the number of lateral connections or the number of stages. \n3. The results in Table 1 is doubtful, why R0, R1, R2 and R3 have the same latency?  From my experience, it is impossible.   \n4. In Table 2, what is the method that combines CoMNet with Structural Reparameterization?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4351/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698657323345,
        "cdate": 1698657323345,
        "tmdate": 1699636406353,
        "mdate": 1699636406353,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SQ7vVXkmld",
        "forum": "kxLMnvnZv0",
        "replyto": "kxLMnvnZv0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4351/Reviewer_nTg9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4351/Reviewer_nTg9"
        ],
        "content": {
            "summary": {
                "value": "**I am not very familiar with this field, so my evaluation may have limited reference value.**\n\nThis paper presents CoMNet, a ConvNet architecture inspired by the structural organization of the biological visual cortex. CoMNet is described as a simplified yet powerful design that offers efficiency across multiple dimensions such as network depth, parameters, FLOPs, latency, branching, and memory budget."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and presents its ideas clearly.\n2. The proposed model achieves higher performance with fewer computation cost and latency."
            },
            "weaknesses": {
                "value": "1. The article only compares earlier works, with the most recent being from 2021. I believe that comparing CoMNet with some more recent methods could enhance the quality of the paper.\n2. The comparison with Vision Transformer (ViT) could be included in the main body of the paper instead of the appendix. Additionally, some of the latest backbones could be used for ViT.\n3. The paper could also benefit from conducting experiments in more areas, such as segmentation and detection. Such experiments might further enhance the quality of the paper."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4351/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4351/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4351/Reviewer_nTg9"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4351/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698754047805,
        "cdate": 1698754047805,
        "tmdate": 1699636406257,
        "mdate": 1699636406257,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "opoDx9rL6Y",
        "forum": "kxLMnvnZv0",
        "replyto": "kxLMnvnZv0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4351/Reviewer_Y3Qk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4351/Reviewer_Y3Qk"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a new convolutional network block inspired by some properties of biological cortical columns. The architecture is found to perform well in image recognition in multiple dimensions of interest including accuracy, FLOPS, parameter count, etc."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper simultaneously analyses several metrics that are diverse and reasonable. \n\nThe method is compared with strong baselines including widely used convolutional networks and a couple of newer networks and shows competitive outcomes."
            },
            "weaknesses": {
                "value": "The architecture is motivated by certain biological details of the cortex, but these details are largely mischaracterized. Section 3, which reviews the biological visual cortex, has many errors. For example, ocular dominance columns are referred to as \u201cthe modules in shallower layers\u201d whereas they coexist with orientation columns. Mountcastle et al. (1997) is given as a reference for the statement, \u201cA cortical column contains only a few neurons (70\u2212100)\u201d but Mountcastle refers to this unit of organization as a minicolumn, and says that a column has many minicolumns. It is claimed that the small number of neurons in a 70-100 unit \u201ccolumn\u201d limits neuron connectivity, but cortical cells have orders of magnitude more connections than that. The next paragraph claims that receptive fields are small regardless of depth of a neuron within the cortex, but deeper neurons (e.g. in medial superior temporal or inferotemporal cortex) have receptive fields tens of degrees wide. There are a number of other examples as well.  \n\nThere is a further misunderstanding in the statement, \u201cCortical modules can not communicate with each other, except at their output (Tanaka, 1996) via pyramidal neurons.\u201d This is a reasonable statement in the context of a feedforward model, but the authors miss the fact that a strong majority of cortical cells are pyramidal. Instead they associate pyramidal cells with a 1x1 convolution at the output of a multi-layer module in which the majority of model neurons aren\u2019t pyramidal.  \n\nThere are two problems with this general lack of grounding of the architecture in neuroscience. First, as it\u2019s written the paper is misleading with respect to neuroscience. Second, there is no remaining motivation or justification for the architecture. \n\nIn summary, the architecture works well, but it is introduced in a confusing and misleading way. In my opinion, references to biology should be purged, the architecture should be explained in terms of the key innovations relative to prior networks, and these contributions should be analyzed in a revised ablation study."
            },
            "questions": {
                "value": "The ablation study in the appendix shows that skip connections and width improve performance, but otherwise I don\u2019t have a clear sense of why this architecture works well. If I understand correctly the main novelty is that each stage has multiple parallel streams with the same structure. Compared with Inception blocks, they have greater depth and more uniformity. Is that the main innovation in the architecture? Does that organization account for the network\u2019s good performance relative to RepVGG?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4351/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698812763894,
        "cdate": 1698812763894,
        "tmdate": 1699636406160,
        "mdate": 1699636406160,
        "license": "CC BY 4.0",
        "version": 2
    }
]