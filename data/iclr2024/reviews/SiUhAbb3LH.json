[
    {
        "id": "XhjqmmacmT",
        "forum": "SiUhAbb3LH",
        "replyto": "SiUhAbb3LH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4417/Reviewer_QHuf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4417/Reviewer_QHuf"
        ],
        "content": {
            "summary": {
                "value": "The authors propose the CLKGE method to address the incremental update and catastrophic forgetting problems in the representation learning of dynamic knowledge graphs. The proposed method is provided with a physical interpretation and a proof of convergence. Experimental results show that CLKGE can outperform existing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.The authors propose a unified framework to achieve the incremental update of embeddings and alleviating the catastrophic forgetting for dynamic knowledge graph representation learning.\n2.The technical design of the proposed method seems reasonable and the experimental results demonstrate its effectiveness.\n3.The convergence analysis of CLKGE is provided."
            },
            "weaknesses": {
                "value": "1. The effectiveness of g in alleviating the distribution gaps needs more explanations, maybe more mathematical proof.\n2. Eq (3) seems very like a GNN aggregation, more analysis should be provided.\n3. The parameter sensitivity of the proposed model should be studied."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4417/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698647169225,
        "cdate": 1698647169225,
        "tmdate": 1699636415762,
        "mdate": 1699636415762,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UTJzUr7McZ",
        "forum": "SiUhAbb3LH",
        "replyto": "SiUhAbb3LH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4417/Reviewer_RKmJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4417/Reviewer_RKmJ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes CLKGE for the task of dynamic knowledge graphs embeddings, which can allow new knowledge and old knowledge to gain from each other. In particular, to transfer knowledge from the existing to the novel without necessitating the retraining of the entire knowledge graph, this paper employs continual learning for knowledge transfer. To mitigate the problem of catastrophic forgetting when incorporating new knowledge alongside the old, the paper utilizes the brachistochrone curve to model the associations. The extensive experimental results presented in the study demonstrate that CLKGE attains state-of-the-art performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper has a clear structure and strong logical coherence. The language used in the paper is fluent, and the tables and figures in the experimental section are clear and complete. \n2. The authors propose a unified framework to achieve the incremental update of embeddings and alleviating the catastrophic forgetting for dynamic knowledge graph representation learning.\n3. This paper provides comprehensive mathematical proofs for the proposed model."
            },
            "weaknesses": {
                "value": "1.\tThis paper introduces the Energy-based Model, but I am unclear about the necessity of this introduction. I do not understand why the paper did not opt for a purely mathematical optimization approach to address the problem, and instead introduced the Energy-based Model, which is a physical model.\n2.\tAll experiments in this paper are conducted based on FB15K-237. It might be beneficial to include additional datasets to enhance the generalizability and credibility of the conclusions."
            },
            "questions": {
                "value": "1.\tCan you add more datasets?\n2.\tCan you explain the necessity of introducing the Energy-based Model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4417/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4417/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4417/Reviewer_RKmJ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4417/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698670516563,
        "cdate": 1698670516563,
        "tmdate": 1699636415683,
        "mdate": 1699636415683,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EMxxhGE6uS",
        "forum": "SiUhAbb3LH",
        "replyto": "SiUhAbb3LH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4417/Reviewer_7psf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4417/Reviewer_7psf"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the representation of dynamic knowledge graphs and tries to combine continual learning with dynamic graphs. The authors attempt to solve two challenges: 1) transfer the old knowledge to new input, and 2) alleviate the catastrophic forgetting problem. Therefore, they propose a method to conduct continual learning and utilize the energy-based model to align new knowledge and old knowledge such that their energy is minimized. The experiments are conducted on one knowledge graph."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper provides a comprehensive literature review.\n2. Many related works have been compared in the experiments."
            },
            "weaknesses": {
                "value": "1. The definition of dynamic knowledge graphs is vague and inaccurate. The authors restrict that the dynamic knowledge graphs only add new entities, relations, and triples during evolution. However, some old entities, relations, and triples would be removed in dynamic knowledge graphs. In addition, the authors assume that at each snapshot, the added entities, relations, and triples are all new, this scenario is hard to find in the real world in which most cases are that some new entities and old relations are linked and new relations and old entities are linked. This paper focuses on the dynamic knowledge graphs but only considers a very rare scenario. \n\n2. It is hard to find out how embedding transfer learns the representation of new knowledge.\n\n3. The motivation for using EBM is not clarified. The benefits of EBM are not explained.\n\n4. The writing needs further improvement. The methodology part only introduces what are the designs without clear explanations.\n\n5. The setting of continual learning and the setting of dynamic graphs are not the same. The authors should at least discuss the differences."
            },
            "questions": {
                "value": "1. The authors claim that only the new entities and relations are added to the existing knowledge graph, it is not clear why Eq. (3) makes use of the previously learned representation $r_{i-1}$, $h_{i-1}$, and $t_{i-1}$ and how to obtain these representation before seeing the new entities and relations? What will happen if a new entity is linked to another new entity by a new relation?\n\n2. No definition for the function $f$ in Eq. (5).\n\n3. It is not clear why the EBMs are effective in controlling the representational shift and why the energy of new knowledge and old knowledge should be minimized."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4417/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698687939231,
        "cdate": 1698687939231,
        "tmdate": 1699636415605,
        "mdate": 1699636415605,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "o9mPsz4bjQ",
        "forum": "SiUhAbb3LH",
        "replyto": "SiUhAbb3LH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4417/Reviewer_pwUu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4417/Reviewer_pwUu"
        ],
        "content": {
            "summary": {
                "value": "The author tries to address two challenges in dynamic KG: knowledge transfer and knowledge retention. The proposed method included two components - a embedding evolution module based on embedding functions, and a knowledge retention method based on energy-based model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1. The author proposed to leverage EBM method on KG embedding learning. It is a new direction that worth further investigating.\n\nS2. Despite minor typos, the math is sound throughout the paper. It is also good to see the theoretical proof on the convergence of the training. \n\nS3. The experiment is thorough and provides insight into the performance of the CLKGE. It included important baselines on KG embedding methods."
            },
            "weaknesses": {
                "value": "W1. Lack of related works regarding current research on temporal knowledge graphs. The setting is close enough to be included. For example, recently Xu et. al (https://arxiv.org/abs/2305.07912) leverages large language model to tackle the temporal knowledge graph setting. Jung et. al (https://arxiv.org/abs/2012.10595) considers the relative displacement timestamps and uses an attention network to model it. \n\nW2. Section 3.1 is very confusing\u2026 The two steps described are not clear. 1) How does old knowledge embedding update the knowledge representations? 2) How do you learn new knowledge with old entities? And why does that transfer from old to new?\n\nW3. The ablation study result is concerning. I am more interested to see the full results on different dataset. The transfer method might have more negative impact on some of the dataset."
            },
            "questions": {
                "value": "Q1. Typo in definition of CL? why not italicize e_{i-1}?\nQ2. What does sum_{i-1}_{j=1} mean? where does i start?\nQ3. Can you provide more intuition behind your formulation when presenting the section 3.1? \nQ4. Experiments: Can you give definition of FWT and BWT?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4417/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699037493292,
        "cdate": 1699037493292,
        "tmdate": 1699636415510,
        "mdate": 1699636415510,
        "license": "CC BY 4.0",
        "version": 2
    }
]