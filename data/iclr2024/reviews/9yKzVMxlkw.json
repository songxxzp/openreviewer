[
    {
        "id": "DV8qrijyAR",
        "forum": "9yKzVMxlkw",
        "replyto": "9yKzVMxlkw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission371/Reviewer_kyMY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission371/Reviewer_kyMY"
        ],
        "content": {
            "summary": {
                "value": "TiG-BEV proposes an internal geometric learning scheme for the target, which enhances the camera based BEV detector from both depth and BEV features by utilizing LiDAR mode. Firstly, an internal deep supervision module is introduced to learn the low-level relative depth relationships of each target, thereby enabling the camera detector to have a deeper understanding of the target level spatial structure. Secondly, an internal feature BEV distillation module was designed to mimic the high-level semantics of different key points within the foreground target. In order to reduce the domain difference between the two modes, distillation within the channel and between key points was used to model feature similarity."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The research methodology proposed in this paper involves two main components. \n1. An inner-depth supervision module is introduced to learn the low-level relative depth relations within each object. This helps the camera-based detectors gain a deeper understanding of object-level spatial structures. \n2. An inner-feature BEV distillation module is designed to imitate the high-level semantics of different keypoints within foreground targets using inter-channel and interkeypoint distillation."
            },
            "weaknesses": {
                "value": "1. The improvement over other methods is limited, as there have been numerous distillation methods proposed for camera-based detectors such as BEV-LGKD, DistillBEV, BEVSimDet, and X3KD. However, this method lacks clear advantages compared to previous approaches, and no extensive comparison with other methods has been provided.\n2. The comparison is biased. Firstly, BEVDistill is specifically designed for BEVFormer, which only has one distillation module for BEVDepth. Secondly, the experiments conducted on the test set were augmented, which does not provide a fair comparison. Thirdly, I noticed that the baseline model achieved similar scores to BEVDistill in Table 2, but the official code does not provide the checkpoint for that. Therefore, I strongly recommend conducting comparisons using the Res-50 model to demonstrate improvements over previous methods such as BEV-LGKD, DistillBEV, BEVSimDet, and X3KD."
            },
            "questions": {
                "value": "1. The paper should report results based on long-term temporal methods, such as solofusion, to demonstrate the effectiveness of the proposed method.\n2. The paper should provide a more comprehensive and fair comparison with previous methods, specifically in the ResNet-50 (R50) framework, to showcase the improvement of the proposed method over existing approaches such as BEV-LGKD, DistillBEV, BEVSimDet, and X3KD. This will provide a clearer understanding of the performance gains achieved by the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission371/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission371/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission371/Reviewer_kyMY"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission371/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698646861677,
        "cdate": 1698646861677,
        "tmdate": 1699635964113,
        "mdate": 1699635964113,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1pzC6StVQy",
        "forum": "9yKzVMxlkw",
        "replyto": "9yKzVMxlkw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission371/Reviewer_CVGt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission371/Reviewer_CVGt"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a new method for multi-camera 3D object detection, where existing techniques for knowledge distillation from a pre-trained LiDAR-based detector to a camera-based detector are enhanced. The authors propose to extract a reference point in the image space as well as keypoints in bird\u2019s eye view and focus the distillation from LiDAR-based models to camera-based models on these points. Evaluation on the nuScenes dataset shows that the method is able to outperform various previous baseline methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-\tThe introduction and related work provide a clear and concise motivation for the proposed method. Figures 1 and 2 give a nice qualitative insight into the proposed contributions.\n-\tThe method description is clear and easy to follow. The mathematical description is overall quite precise and Figures 2-6 help a lot to better understand the method.\n-\tThe authors show that their method can be combined with various baseline methods.\n-\tThe ablation study verifies the effectiveness of the single method components."
            },
            "weaknesses": {
                "value": "Issues:\n\n1.\tRecently, there have been some relevant related works [a, b], which seem quite related to the method presented in this paper. I think it would be good to compare to these works in the SOTA comparison as well and discuss differences in a bit more detail.\n[a] Klingner et al. \u201cX3KD: Knowledge Distillation Across Modalities, Tasks and Stages for Multi-Camera 3D Object Detection,\u201d CVPR 2023\n[b] Zhou er al. \u201cUniDistill: A Universal Cross-Modality Knowledge Distillation Framework for 3D Object Detection in Bird's-Eye View,\u201d CVPR 2023\n\n2.\tI am wondering how generalizable the method is. It seems that especially the improvement in terms of learning a better depth distribution is quite constraint to methods making use of feature projection via depth maps. However, e.g., DETR-like methods for multi-camera 3DOD do not have this feature. It would be interesting to provide insights how the method could be extended to such methods.\n\n3.\tDue to the two mentioned issues I feel that the contribution and scope of this works appears a bit limited. If they can be addressed in the rebuttal, I am open to reconsider this point.\n\nMinor comments and typos:\n\n4.\tSection 3.1: I think it would be good to have a better differentiation in terms of notation between the number of depth bins D and the depth map D. I also did not completely understand the upper index A in the depth loss.\n\n5.\tSection 3.1: I think it is a bit confusing to talk about 3d and 2d features if they have exactly the same shape. The same goes for 3D and 2D detection heads in Figure 4, which are probably exactly the same just for different modalities as input. Maybe it would help to rather talk about camera and LiDAR features?\n\n6.\tFigure 4: It would be nice to include the math notation also in the figure to be able to better connect the method description and the visualization of the method in the figure\n\n7.\tThe text in Figure 5 and 6 is quite small. It would be nice to increase the font size a bit.\n\n8.\tTable 1 and 2: It would be good to include the latest works on multi-camera 3DOD in the SOTA comparison, e.g., [a,b] or to explain why they are excluded. As far as I saw, they also report on nuScenes, so shouldn\u2019t they be comparable?\n\n9.\tIt would be nice to verify the method on more than one dataset, e.g., the Waymo dataset.\n\n10.\tTable 2: It would be good to also report resolution and backbone in this comparison.\n\n11.\tThere are a few (minor) typos remaining throughout the whole paper. It would be good to resolve them with another round of proofreading."
            },
            "questions": {
                "value": "-\tIt is interesting to see that using only a reference point for depth supervision is superior to using a dense depth map as supervision. I have not completely understood the motivation though. Could the authors provide a bit more insight, maybe also from their experience from qualitative results? Also, I was asking myself what happens if there is no ground truth available at the reference point since supervision usually comes from sparse LiDAR data?\n-\tSimilarly, could the authors maybe explain why it is beneficial to uniformly sample key points from the bounding box in BEV space instead of supervising throughout the whole area of the bounding box?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission371/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791262031,
        "cdate": 1698791262031,
        "tmdate": 1699635964034,
        "mdate": 1699635964034,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JgDReZSboe",
        "forum": "9yKzVMxlkw",
        "replyto": "9yKzVMxlkw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission371/Reviewer_oss3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission371/Reviewer_oss3"
        ],
        "content": {
            "summary": {
                "value": "this paper investigates the problem of BEV 3D object detection from multi-view RGB images. using the LiDAR as teacher supervision, this manuscript introduces a relative depth supervision and relationship matching for knowledge distillation. the effectiveness of the proposed method is verified on nuScenes dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ relatively easy to read\n+ good results\n+ ablation and variant study"
            },
            "weaknesses": {
                "value": "the major issue that the reviewer has with this current manuscript is that it does not introduce significant new knowledge to the readers.\n- the proposed 'inner-depth supervision' is a object-normalized version of the absolute depth from BEVDepth. yes, normalizing the depth within the objects can make it easier to learn, where the absolute depth (normalized to 1) might range from 0.1 to 0.15, and the relative depth can be 0 and 1 correspondingly. it is great that the ablation shows improvement, but this is to be expected and well-recognised, and feels more like a trick. \n- the channel-wise and pixel-wise relationship supervision in Section 3.3 has been investigated by previous works [r1,r2,r3] on multiple tasks, so their capability in the BEV detection task (Table 3 and Table 5) is also somewhat expected. also, please refer to related works on 'relationship supervision', which could greatly benefit readers new to this field.\n\noverall, the reviewer does not feel entirely confident in recommending this manuscript at its current state.\n\n\n\n[r1]. Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. \"Image style transfer using convolutional neural networks.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2414-2423. 2016.\n\n[r2]. Tung, Frederick, and Greg Mori. \"Similarity-preserving knowledge distillation.\" In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1365-1374. 2019.\n\n[r3]. Hou, Yunzhong, and Liang Zheng. \"Visualizing adapted knowledge in domain transfer.\" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 13824-13833. 2021."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission371/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission371/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission371/Reviewer_oss3"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission371/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801698619,
        "cdate": 1698801698619,
        "tmdate": 1699635963932,
        "mdate": 1699635963932,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Uo0B3PKtPG",
        "forum": "9yKzVMxlkw",
        "replyto": "9yKzVMxlkw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission371/Reviewer_CeGw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission371/Reviewer_CeGw"
        ],
        "content": {
            "summary": {
                "value": "Recently, improving camera based 3D object detection by leveraging the LiDAR pure 3D information is get attention in the academia and industry. This paper belongs to this category. Overall, the paper proposed a new multi-view 3D detection via TARGET INNER-GEOMETRY LEARNING. Specifically, the authors proposed to use inner-depth supervision and inner-feature BEV distillation to boost the performance of mutli-view BEV detection. Benchmarked on the nuScenes dataset, it showed the better performance. Meanwhile, the authors conducted the ablation studies to make the work solid."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Good initiatives and motivation to resolve the issues in camera based 3D detection\n+ Good presentation with clear illustration to show the proposed method and great description of the proposed method (although it has some typos)\n+ Comprehensive results and ablation studies on the nuScenes datasets"
            },
            "weaknesses": {
                "value": "I think the novelty and motivation of this paper is pretty clear and I really like the showcase in the Figure 1. However, I have several concerns in the experimental verification parts. \n\nThe first concern is the experimental dataset is pretty limited to nuScenes. I would like to see some experimental analysis on the Waymo One, KiTTI or 3D KiTTI dataset which is also the standard for the 3D detection. \n\nTo follow the first concern, some results in the paper is cherry picked, such as in Figure 7, the author wanted to show the improvement visually, however, in my opinion, it is pretty cherry picked. If the author could illustrate the performance of small and far object detection, it will be better. \n\nAnother big concern is the proposed method is only compared to the BEV4D/BEVDepth and its variances. The author did not list the latest method, such as Cross-Modality Knowledge Distillation Network for Monocular 3D Object Detection etc. Although the proposed method is different, these methods are in the same catrgory as the proposed method, that is L-> C. I would like to see the comparisons in this category."
            },
            "questions": {
                "value": "Please check the weakness part and address the questions there. Overall I hope the author could present clearly and solid in the experimental results to make the paper as a strong submission."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission371/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698865234550,
        "cdate": 1698865234550,
        "tmdate": 1699635963851,
        "mdate": 1699635963851,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nejLTR8C5i",
        "forum": "9yKzVMxlkw",
        "replyto": "9yKzVMxlkw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission371/Reviewer_v25e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission371/Reviewer_v25e"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel approach to 3D object detection, emphasizing the improvement of camera-based detectors. The proposed method introduces \"Inner-depth Supervision\" and \"Inner-feature BEV Distillation\" techniques to enhance the learning of spatial structures and depth perception. The main results show a comprehensive performance evaluation on the nuScenes test set, where the authors compare their method with established benchmarks across several metrics, demonstrating its effectiveness. The ablation study likely delves into the specific contributions of different method components, though the exact details weren't extracted here."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The manuscript presents innovative techniques such as \"Inner-depth Supervision\" and \"Inner-feature BEV Distillation\", indicating a high degree of originality by potentially filling a gap in the camera-based 3D object detection literature. The quality of research seems robust, as evidenced by comprehensive evaluations and methodical ablation studies. Clarity, while harder to fully assess without the complete text, appears to be aided by the use of illustrative figures and a structured presentation of methods and results. The significance of the work is underlined by its potential to enhance the practicality and cost-effectiveness of 3D object detection systems, which could have far-reaching implications for autonomous driving and robotics. This paper could represent a valuable contribution to the field, provided the results hold under peer review and the methods are as scalable and adaptable as implied."
            },
            "weaknesses": {
                "value": "To improve the paper, the authors could expand the methodology section, provide additional experimental results, and include more thorough comparisons with state-of-the-art techniques. Moreover, an in-depth discussion of the limitations and broader implications of the work would add value and show the authors' comprehensive understanding of their method's place in the field."
            },
            "questions": {
                "value": "Can you provide additional details on the mathematical formulation and implementation details of the \"Inner-depth Supervision\" and \"Inner-feature BEV Distillation\" techniques? \nThe paper mentions results on the nuScenes test set. Have you evaluated your method on other datasets or in varied real-world conditions to test its generalizability?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "none."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission371/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699073330183,
        "cdate": 1699073330183,
        "tmdate": 1699635963781,
        "mdate": 1699635963781,
        "license": "CC BY 4.0",
        "version": 2
    }
]