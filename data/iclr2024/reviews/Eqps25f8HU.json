[
    {
        "id": "lmKMzKOpOJ",
        "forum": "Eqps25f8HU",
        "replyto": "Eqps25f8HU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7620/Reviewer_xszJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7620/Reviewer_xszJ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a model-based offline reinforcement learning algorithm with general function approximation. It aims to fill the gap between the theoretical guarantees and the practical implementation. The algorithm proposes a model-based mirror ascent algorithm with general function approximation under partial coverage of offline data. The paper shows the theoretical guarantees of the algorithm, which is supported by numerical studies."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper provides a solid study on the model-based offline reinforcement learning algorithm. The main contribution comes from two side: \n\n1) Comparing to previous model-based offline RL algorithms, this paper provides an practically implementable algorithm and provides an empirical study on it.\n\n2) Comparing with model-free offline RL algorithms, the paper studies a case with weaker assumption and shows a faster suboptimality.\n\nThe theoretical and the empirical study in the paper is solid. Moreover, the results of the paper is presented in a"
            },
            "weaknesses": {
                "value": "My major concern on the paper comes from the contribution of the paper. Algorithm 1 is similar to Uehara and Sun. While Algorithm 2 is a more interesting part, its idea is still similar to previous work, especially on the conservative estimation and the partial coverage. Moreover, I have several concerns on the implementation of the algorithm:\n- My first concern is on the PD algorithm (3) and (4) for finding the estimation of the transition model to construct the conservative estimation of the Q-function. In general, the parameterization of the transition kernel is non-convex, and the loss function is estimated empirically. Thus, it could be hard to find the global optimizer, or find a good estimation of the global minimizer.\n- The above problem also exists in (6) and (7) for estimating $\\beta$."
            },
            "questions": {
                "value": "1. Is there a theoretical guarantee on the PD algorithm in (3) and (4), especially in the case of general function approximation and empirical estimation of the loss function?\n2. The paper presents the comparison with several other offline RL in Section 7. For table 1, is there also a comparison of MoMA and other SOTA algorithms (like in Table 2) on the synthetic dataset? \n3. For table 2, there seems to be a large difference on the scores of the different algorithms. Is there any insight on why such a large difference occurs?\n4. The definition of $\\mathcal{P}$ seems missing.\n5. The error rate in Theorem 1 involves $\\epsilon_{est}$, which, however, depends on $n$ as defined in (8). What is the dependence on $\\epsilon_{est}$ in terms of $n$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7620/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698619923164,
        "cdate": 1698619923164,
        "tmdate": 1699636925374,
        "mdate": 1699636925374,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AhCrpf8RQB",
        "forum": "Eqps25f8HU",
        "replyto": "Eqps25f8HU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7620/Reviewer_cZbn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7620/Reviewer_cZbn"
        ],
        "content": {
            "summary": {
                "value": "This paper:\n\n(1) proposes a model-based mirror descent offline RL algorithm and its corresponding practical version via Lagrangian multiplier and function approximation.\n\n(2) provides theoretical guarantee of the proposed algorithm.\n\n(2) conducts some empirical experiments to validate the performance of the proposed algorithm."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The proofs are rigid.\n\n(2) Apart from theoretical guarantees, there are also numerical results."
            },
            "weaknesses": {
                "value": "Overall I feel this work is not novel or significant enough:\n\n(1) For Algorithm 1, the idea of constructing a confidence set over the model and use pessimism for policy improvement has been applied in the literature of offline RL, such as [1].\n\n(2) For Algorithm 2, there also have been papers [2] utilizing Lagrangian multipliers to get rid of the constraint of confidence set in the theoretical algorithms and get computationally efficient algorithms. The method of function approximation in Algorithm 2 also has been applied in [3].\n\n(3) The empirical performance of the proposed algorithm is not impressive. In Table 2, RAMBO, IQL and CQL all get better performance (averaged over all tasks) and thus I think the proposed algorithm does not have much significance in the empirical studies either.\n\nIn summary, I think this work simply combines the existing techniques and results in the literature and is kind of incremental. \n\n[1] Uehara, Masatoshi, and Wen Sun. \"Pessimistic model-based offline reinforcement learning under partial coverage.\" arXiv preprint arXiv:2107.06226 (2021).\n\n[2] Rigter, Marc, Bruno Lacerda, and Nick Hawes. \"Rambo-rl: Robust adversarial model-based offline reinforcement learning.\" Advances in neural information processing systems 35 (2022): 16082-16097.\n\n[3] Guanghui Lan. Policy optimization over general state and action spaces. arXiv preprint arXiv:2211.16715, 2022."
            },
            "questions": {
                "value": "(1) The paper claims that it does not require Bellman completeness and only needs model realizability. However, the literature have shown that when a model class that contains the true MDP model is given, value-function classes that satisfy a version of Bellman-completeness can be automatically induced from the model class [4]. This implies that model realizability is even stronger than Bellman-completeness.\n\n(2) The paper claims that the size of the function can be arbitrarily large. However, in Theorem 1 (the performance of Algorithm 2), the sample complexity clearly depends on the size of the function class $|\\mathcal{F} _ {t,i}|$. In addition, I believe both the performance of Algorithm 1 and 2 depends on the size of the model class, and model classes can be even larger than function classes.\n\n(3) The paper does not need a parametric policy class. However, in general the computation complexity of the optimization problem Equation 7 will depend on the size of the state space, which can be infinite.\n\n(4) The authors claim that Theorem 1 characterizes the performance of Algorithm 2, but it seems not. Theorem 1 assumes the existence of the confidence set of models and picks the most pessimistic model from such a confidence set while in Algorithm 2 you simply run primal-dual gradient descent-ascent and do not have a confidence set.\n\n[4] Chen, J. and Jiang, N. (2019). Information-theoretic considerations in batch reinforcement learning. In International Conference on Machine Learning, pages 1042\u20131051. PMLR."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7620/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7620/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7620/Reviewer_cZbn"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7620/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698680179692,
        "cdate": 1698680179692,
        "tmdate": 1699636925230,
        "mdate": 1699636925230,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kmi1MRlcCD",
        "forum": "Eqps25f8HU",
        "replyto": "Eqps25f8HU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7620/Reviewer_wmfD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7620/Reviewer_wmfD"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an offline RL algorithm, MOMA,  that offers practical implementability alongside theoretical guarantees.  MoMA uses a model-based mirror ascent approach with general function approximations, operating under partial coverage of offline data. The algorithm iteratively and conservatively estimates value functions and updates policies, moving beyond the conventional use of parametric policy classes. With mild assumptions, MoMA\u2019s effectiveness is theoretically assured by establishing an upper bound on the policy\u2019s suboptimality, and practical utility is confirmed through numerical studies."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper proposes a new offline RL algorithm with pessimism."
            },
            "weaknesses": {
                "value": "1. The contribution of this work seems exaggerated. In particular, the authors claim that the algorithm enjoys theoretical guarantees and practical implementation. But in fact, the algorithm with the theoretical result is **different** from the practically implemented version. In particular, the theoretically analyzed algorithm involves pessimistic model learning, while the practical version uses a regularized form. With significantly different algorithms, the claim does not hold. \n\n2. In terms of the theoretically sound version (Algorithm 1), the theoretical novelty is limited. The analysis is based on pessimistic model learning combined with policy mirror descent. The combination seems quite direct given existing works. In particular, in the proof of Theorem 2, the regret decomposition in (15)--(17) is standard analysis of a pessimism-based algorithm. The first term of (18) is standard analysis of policy mirror descent, and the second term of (18) is an application of simulation lemma. Similar analysis has also appeared in the RL literature, e.g., Provable Benefits of Actor-Critic Methods for Offline Reinforcement Learning. \n\n3. The practically implemented version of algorithm is also similar to some existing algorithms. For example, \"Bayesian Optimistic Optimization: Optimistic Exploration for Model-based Reinforcement Learning\" also studies a regularized version of policy optimization algorithm, but for online RL. In addition, MOPO and MoRel has practical implementations of pessimism-based algorithms. \n\n4. The implemented version of algorithm is only tested on three D4RL tasks. It would be great to have more extensive experiments."
            },
            "questions": {
                "value": "1. How to implement the policy update when there is a normalization factor? \n2. How do you update the multiplier $\\lambda$ in the experiments? \n3. Is it possible to unify these two different versions of algorithms?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7620/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698741393308,
        "cdate": 1698741393308,
        "tmdate": 1699636925114,
        "mdate": 1699636925114,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LtVl8G3Eod",
        "forum": "Eqps25f8HU",
        "replyto": "Eqps25f8HU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7620/Reviewer_CJRs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7620/Reviewer_CJRs"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a conservative and practical model-based offline RL algorithm that alternates between pessimistic value estimation and policy update via mirror ascent. Theoretical guarantees are provided for the algorithm under partial data coverage and some experimental evaluations are provided."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper studies an important problem, which is practical and theoretically-founded model-based offline RL with general function approximation.\n- The proposed algorithm is new, implementable, and enjoys theoretical guarantees."
            },
            "weaknesses": {
                "value": "- One of the main contributions is stated as \u201cIn contrast to model-free RL literature which heavily relies on the assumption of Bellman completeness, we propose novel theoretical techniques for offline model-based policy optimization algorithms that are free of the assumption\u201d  In general, model-based methods (unless additional variables are introduced) do not require Bellman completeness assumption because assuming a realizable model is stronger; see e.g. [1], which shows that model realizability subsumes completeness assumptions.\n- It is stated that \u201cTo our knowledge, this work is the first study that provides a practically implementable model-based offline RL algorithm with theoretical guarantees under general function approximations.\u201d There are prior works that offer implementable model-based offline RL algorithms with optimal statistical rates such the model-based version of the algorithm in [2]. The mentioned algorithm also does not require the difficult step of minimizing within a confidence set of transition models. Another example algorithm is ARMOR [4].\n- Although the work is focused on general function approximation, it requires the concentrability definition with bounded policy visitation and data distribution ratio for every state and action. This is a stronger assumption compared to the Bellman-consistent variant such as in [3].\n- Theory does not seem to be particularly challenging and/or offer new insights or techniques.\n- The experimental section is weak. In particular, the synthetic data experiments only compare MoMA with standard natural policy gradient and model-free FQI, none of which include any form of conservatism/pessimism, and the results are expected. It would be good to see comparison with other pessimistic model-based methods. Additionally, comparison with the work of Uehara & Sun 2021 when combined with the Lagrangian approach of this work would be useful. For the D4RL benchmark, comparison is only provided for a small subset of datasets and only baseline model-free offline RL methods. No comparison is provided with ARMOR and ATAC.\n\n**References:**\n\n[1] Chen, Jinglin, and Nan Jiang. \"Information-theoretic considerations in batch reinforcement learning.\" In\u00a0International Conference on Machine Learning, pp. 1042-1051. PMLR, 2019.\n\n[2] Rashidinejad, Paria, Hanlin Zhu, Kunhe Yang, Stuart Russell, and Jiantao Jiao. \"Optimal Conservative Offline RL with General Function Approximation via Augmented Lagrangian.\" In\u00a0The Eleventh International Conference on Learning Representations. 2022.\n\n[3] Cheng, Ching-An, Tengyang Xie, Nan Jiang, and Alekh Agarwal. \"Adversarially trained actor critic for offline reinforcement learning.\" In\u00a0International Conference on Machine Learning, pp. 3852-3878. PMLR, 2022.\n\n[4] Bhardwaj, Mohak, Tengyang Xie, Byron Boots, Nan Jiang, and Ching-An Cheng. \"Adversarial model for offline reinforcement learning.\"\u00a0arXiv preprint arXiv:2302.11048\u00a0(2023)."
            },
            "questions": {
                "value": "- Comparison with the references above, both in terms of technique and empirical performance.\n- Clarifying challenges and technical contributions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7620/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698884033088,
        "cdate": 1698884033088,
        "tmdate": 1699636925010,
        "mdate": 1699636925010,
        "license": "CC BY 4.0",
        "version": 2
    }
]