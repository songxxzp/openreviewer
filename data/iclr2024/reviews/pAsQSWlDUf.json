[
    {
        "id": "aAfq18jlxA",
        "forum": "pAsQSWlDUf",
        "replyto": "pAsQSWlDUf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4667/Reviewer_e9k3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4667/Reviewer_e9k3"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a new contrastive learning method for time series. Specifically, they propose to remove the hard positive/negative assignment from the original NCE by a soft reweighting incorporating prior information about the temporal closeness or similarity of inputs. The authors evaluate their method on various time series-related tasks showing strong improvement compared to \"hard\" CL methods. They also provide ablation experiments concerning their new objective hyperparameters."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is of high quality with the following strengths: \n\n- Overall very well-written paper with an easy-to-follow structure, in particular:\n    - Related work is structured and extensive (with the exception mentioned below in weaknesses).\n    - Method is clear and figures relevant making the method easy to understand.\n    - Experiments are very extensive and well described both in the manuscript and the supplementary materials.\n\n- I appreciate that despite introducing numerous components and hyperparameters ( temperature, distance metric, weight function, etc..), authors provide ablations to each of these components. \n- The authors discussed the additional computational complexity of their method and in particular DTW known to have a squared complexity. \n- The amount of experiments carried out is very large and diverse"
            },
            "weaknesses": {
                "value": "There are, however, some weaknesses, in particular in terms of related work, which I detail below:\n\n\n**Related work**\nAs mentioned the related work is extensive but misses some seminal works regarding contrastive learning methods for time series tackling the challenges of inter/intra samples dependencies:\n- First, \"Subject-aware contrastive learning for biosignals\" by Cheng et al (2020) proposes to only use negative representations from the same time series to \"promote subject-invariant\" representation, what the authors would refer to as \"temporal CL\".\n- Second, \"CLOCS: Contrastive Learning of Cardiac Signals Across Space, Time, and Patients\" by Kiyasseh et al. (2021) proposes to on the contrary use representations from the same time series as positives, what the authors would refer to as \"instance-wise CL\". This is similar to TNC but with a neighborhood being defined as being from the same time series. \n- Finally, and more importantly, \"Neighborhood Contrastive Learning Applied to Online Patient Monitoring\" by Yeche et al. (2021), introduces the trade-off used by the authors between instance-wise and temporal-wise CL controlled by $\\lambda$. In particular, the objective proposed by Yeche et al., namely NCL, is similar to taking a hard assignment for instance-wise CL $w_I(i,j) = \\mathbb{1}_{[i = j]}$ and a (discontinuous) uniform one over a window for temporal CL. \n\nThus, I think it's really important that this work refers to these three works and in particular NCL from which the SoftCLT is an extension to continuous neighborhood definitions. It would be nice to have a comparison to it as well. \n\n\n**Clarity**\n\n- The authors refer multiple times to instance-wise and temporal CL before defining it properly in the method section. I think pointing the reader to this section or defining the terms in the introduction could improve clarity.\n\n- referring to a \"temperature\" parameter $\\tau_t$ and $\\tau_i$ can be quite misleading in the context of contrastive learning, where this term was coined by Chen et al. (2020), in the simCLR paper. (See my comment below on the choice of assignment function) Given the assignment function is some form of Laplacian kernel, referring to $l =\\frac{1}{\\tau}$ as a lengthscale parameter would be more coherent with literature and avoid confusion with temperature parameters from previous works on CL. \n\n**Method**\n- The authors define their assignment function around a sigmoid function which is defined over $\\mathbb{R}$ whereas its input $D$ lies in $\\mathbb{R}^+$. It seems to overcome this, they tweak around their sigmoid function to obtain a symmetric function $w(D) = \\frac{2}{1+e^{Dt}}$. Why not rely on existing literature instead and typically use a Laplacian kernel $w(D) = e^{-\\frac{D}{l}}$? \n- Exploring further different kernel and their impact on performance would have been a nice addition. In particular, using a generalized Gaussian kernel and looking at the impact of the shape parameter $\\beta$ would be nice as $\\beta=1$ is SoftCLT and  $\\beta=\\infty$ is NCL temporal CL. \n- Exploring further the impact of the trade-off between local (temporal) and global (instance) features learning ruled by $\\alpha$ would be a nice addition to ablations. \n**Conclusion**\n\n\nClarity and Method weakness are easily addressable. Regarding related work, despite the similarities with NCL, I still think the contribution to be significant given the novelty around the neighborhood/assignment function and the extent of the experiments on various tasks, justifying my choice of recommending acceptance. However,  I firmly believe the three works I mentioned should be correctly cited in particular the link to Yeche et al. (2021) work."
            },
            "questions": {
                "value": "I don't have any questions beyond the points raised in the above sections."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4667/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698411847176,
        "cdate": 1698411847176,
        "tmdate": 1699636447727,
        "mdate": 1699636447727,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "y0TM9bHSKr",
        "forum": "pAsQSWlDUf",
        "replyto": "pAsQSWlDUf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4667/Reviewer_mvHV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4667/Reviewer_mvHV"
        ],
        "content": {
            "summary": {
                "value": "This study introduce a new method of performing constrastive learning, a soften version of normal positive-negative strategy. These soft assignments are determined by the distance between time series in the data space for instance-wise contrastive loss and the difference in timestamps for temporal contrastive loss."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Contribution:\n\n- The idea of soft constrastive learning is straight-forward and natural. The underlying functions are widely-adopted and straightforward to implement.\n- The experiments are extensive and cover many time-series tasks (classification, anomaly detection) as well as scenario (self/semi supervised and supervised learning). The comparison with soft-CL techniques from other domains and ablation study make the whole experimental section be quite well-rounded.\n\nRepresentation:\n\n- Intuitive and direct illustrations via Figures (e.g. Fig.1, 2)"
            },
            "weaknesses": {
                "value": "- Contribution:\n    - For instance-wise CL:\n        - the use of DTW might be a potential bottleneck in case of dealing with lengthy time-series. While the authors suggest the use of FastDTW, the complexity regarding the memory might be increased, and also the potential reduce in approximation (in case the warping path between two time series instances is highly nonlinear). In other words, the choices of DTW or FastDTW are hurting the pipeline in some ways.\n        - the calculation of weight based on the distance in the data space. However, this make the weighting process be dependent on the scale of input data. Together with the wrapper of Sigmoid function, it might be saturated upon too large or too small input. This effect might make the weights not representative to use in instance-wise CL. While empirically, it illustrates the effective over in latent space, more effort need to be done to consider on which space one should rely on to calculate distance.\n    - For temporal-wise CL, the current weight assignment implicitly assume the data from neighbors\u2019 timesteps should be weighted heavier than the data from far timesteps. However, that behavior might not always hold true, as illustrated in work of Tonekaboni (2021)."
            },
            "questions": {
                "value": "The authors please address or provide answers to any questions from the weaknesses listed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4667/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4667/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4667/Reviewer_mvHV"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4667/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698449360767,
        "cdate": 1698449360767,
        "tmdate": 1700604064233,
        "mdate": 1700604064233,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MPT6GNPiay",
        "forum": "pAsQSWlDUf",
        "replyto": "pAsQSWlDUf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4667/Reviewer_EHZC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4667/Reviewer_EHZC"
        ],
        "content": {
            "summary": {
                "value": "This study argues that when using time series data in contrastive learning, contrasting ( between positive and negative) instances or values located in proximity can lead to the neglect of their inherent correlation. Therefore, they introduce a continuous (referred as soft) weighting approach as an alternative to binary labeling, serving as a generalization of the standard contrastive loss, with the transformation occurring when replacing soft assignments with hard assignments of zero for negatives and one for positives. For soft assignment, the authors take into account two aspects: the similarity between two time series in data space and the proximity of two time series with respect to their timestamps."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The papers is well written and clear. The figures presented help to clarify the main idea and how it is implemented. The idea is novel for the simplified setup that is considered. The experimental results cover 3 downstream tasks and comprehensively evaluate the assumed setup."
            },
            "weaknesses": {
                "value": "The paper addresses a simplified scenario in which issues related to noise, seasonality, and non-stationarity are not considered, as there is no apparent mechanism in the approach to address these prevalent challenges found in real-world time series data.\nRegarding robustness in the presence of noise and non-stationarity there  is no specific discussion or empirical evaluation.  Regarding seasonality, the authors mentioned \"Our conjecture is that TS in the real world usually do not exhibit the perfect seasonality, as indicated by the ADF test result, such that SoftCLT takes advantage of the non-seasonal portions.\" While perfect seasonality may be absent in some datasets and may vary in intensity across different datasets, I believe completely disregarding it is not a practical approach."
            },
            "questions": {
                "value": "Can this case be elaborated a bit further:\u201d when \u03b1 = 1, we give the assignment of one to the pairs with the distance of zero as well as the pairs of the same TS\u201d What if in the same TS we are experiencing two different patterns, shifts or different distribution\n\nIn equation 3, augmentation for I and i+N, how it  is performed? What if there is only a shift in the pattern in the instances, otherwise there are very similar how you address this in your computation, It would be great to include an illustration for this case to show you approach is robust to shift (or some noise) which is very common in real world applications.\n\n\nHow do you manage non stationary in the time series, where the immediate next point might be the start of a different distribution? How your similarity comparison handles it when the the proximity is assumed to have high similarity which is not necessarily true."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4667/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698658100622,
        "cdate": 1698658100622,
        "tmdate": 1699636447564,
        "mdate": 1699636447564,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eIozRiQg6V",
        "forum": "pAsQSWlDUf",
        "replyto": "pAsQSWlDUf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4667/Reviewer_bKG2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4667/Reviewer_bKG2"
        ],
        "content": {
            "summary": {
                "value": "The authors address challenges in time series data annotation and the limitations of standard contrastive learning (CL) in representing TS data. Key contributions of the paper are:\n\n- Introduction of SoftCLT, a soft contrastive learning strategy tailored for time series data. Their framework can be adapted to other CL frameworks relatively easily.\n- The proposal of soft contrastive losses for both instance and temporal dimensions, addressing the shortcomings of existing CL methods for TS.\n- Comprehensive experimental evidence demonstrating that SoftCLT enhances state-of-the-art performance across multiple TS tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The submission has the following strenghts:\n\n- Ablation study is present and seems to demonstrate the usefulness of the proposed additions.\n- Compared to the selected baselines (emphasis on selected), the model performs well.\n- I appreciate that the authors have chosen to go for a more detailed analysis of the representation learning abilities of their model. By this I mean that rather than considering only task-performance, they also investigate aspects such as robustness to non-stationarity, and also semi-supervised learning. This is usually absent from related papers.\n- The ideas are well explained, the paper is clear."
            },
            "weaknesses": {
                "value": "The submission has the following weaknesses:\n- Problem with the comparisons. Entirely absent from the main paper is any comparison with CoST [1], or any more recent contrastive approach. While this is a single issue it is one I am quite concerned about. Similarly, a comparison to recent approaches in the regression setting of TS2Vec (nothing prevents that comparison, the TS2Vec code works seamlessly for both approaches). My reasoning is that the proposed idea is interesting, but also relatively simple. This is fine in general: simple ideas bring value in research as well. However, coupled with a lack of comparison to recent approaches, it is very difficult to ascertain the value of the contribution. \n\nCurrently this is enough for me to not recommend acceptance, but as noted in the questions section, I am willing to update my score should the authors adress this.\n\nReferences\n[1] CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for Time Series Forecasting"
            },
            "questions": {
                "value": "As noted in the Weaknesses section, I would like a detailed comparison with CoST and an evaluation in the regression setting. If the authors provide this, and the results warrant it, I will raise my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4667/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4667/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4667/Reviewer_bKG2"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4667/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790265767,
        "cdate": 1698790265767,
        "tmdate": 1700614294099,
        "mdate": 1700614294099,
        "license": "CC BY 4.0",
        "version": 2
    }
]