[
    {
        "id": "9vxgxJwixe",
        "forum": "Cnn60wwTe1",
        "replyto": "Cnn60wwTe1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4594/Reviewer_3qgW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4594/Reviewer_3qgW"
        ],
        "content": {
            "summary": {
                "value": "The study examines the performances of both Fed-Avg and decentralized local SGD, known as DFedAvg, in terms of excess risk and generalization, which differs from the conventional convergence rate analysis. Results indicate that centralized methods consistently outperform decentralized ones in terms of generalization. Additionally, the research reveals a requisite network topology condition for DFedAvg, and if not met, the generalization becomes worse."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper provides a theoretical analysis of the performance differences between centralized and decentralized methods, focusing on excess risk and generalization. Its strength is in the conclusion that CFL always generalizes better than DFL. This provides an explanation as to why CFL has better empirical results than DFL in deep learning applications."
            },
            "weaknesses": {
                "value": "Many related works are missing, and a more thorough literature review is necessary.\n\nSeveral related studies are not included, necessitating a more comprehensive literature review.\n\nThe evaluation focuses solely on the DFedAvg method, which is known to have inherent biases. Various improved versions exist, such as local gradient tracking and local exact-diffusion, which might yield different outcomes. For the Fed-Avg method, one might also contemplate incorporating Scaffold.\n\n\nThe presentation of the results is challenging to comprehend and necessitates improved organization and clarity."
            },
            "questions": {
                "value": "Table 1 is presented without an explanation of its significance and implications in the introduction, making it necessary for readers to go through the entire paper to understand the table.\n\n\nThis work seems to lack a thorough review of related literature. For example, attributing the DFedAvg framework solely to Sun et al. (2022) is inaccurate. Multiple studies have examined the scenario of local updates in decentralized training, including:\n\n-\tWang, Jianyu, and Gauri Joshi. \"Cooperative SGD: A unified framework for the design and analysis of local-update SGD algorithms.\" The Journal of Machine Learning Research 22.1 (2021): 9709-9758.\n-\tKoloskova, Anastasia, et al. \"A unified theory of decentralized sgd with changing topology and local updates.\" International Conference on Machine Learning. PMLR, 2020.\n\nSimilarly, numerous studies have explored DSGD, its model consistency, and have proposed corrective methods, including:\n\n-\tYuan, Kun, et al. \"On the influence of bias-correction on distributed stochastic optimization.\" IEEE Transactions on Signal Processing 68 (2020): 4352-4367.\n-\tShi, Wei, et al. \"Extra: An exact first-order algorithm for decentralized consensus optimization.\" SIAM Journal on Optimization 25.2 (2015): 944-966.\n-\tAlghunaim, Sulaiman A., and Kun Yuan. \"A unified and refined convergence analysis for non-convex decentralized learning.\" IEEE Transactions on Signal Processing 70 (2022): 3264-3279.\n-\tMishchenko, Konstantin, et al. \"Proxskip: Yes! local gradient steps provably lead to communication acceleration! finally!.\" International Conference on Machine Learning. PMLR, 2022.\n-\tNguyen, Edward Duc Hien, et al. \"On the performance of gradient tracking with local updates.\" arXiv preprint arXiv:2210.04757 (2022).\n-\tAlghunaim, Sulaiman A. \"Local Exact-Diffusion for Decentralized Optimization and Learning.\" arXiv preprint arXiv:2302.00620 (2023).\n\n\nHow does the generalization analysis presented in this study differ from that of DSGD without local updates as detailed in:\n\n\n-\tTaheri, Hossein, and Christos Thrampoulidis. \"On generalization of decentralized learning with separable data.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.\n-\tBars, Batiste Le, Aur\u00e9lien Bellet, and Marc Tommasi. \"Improved Stability and Generalization Analysis of the Decentralized SGD Algorithm.\" arXiv preprint arXiv:2306.02939 (2023).\n-\tZhu, Tongtian, et al. \"Topology-aware generalization of decentralized sgd.\" International Conference on Machine Learning. PMLR, 2022.\n\nFurthermore, the bounds presented in theorems 1 and 2 demand a comprehensive explanation. A clearer breakdown of the influence of each parameter is necessary. Specifically, the intuition behind equation (9) where the bound seems to deteriorate with increasing values of T and K remains elusive (to readers not quite familiar with generalization analysis).\n\nWhat is small $k$ in theorem 1?\n\nIn the scenario of a fully connected network, DFedAvg reduces to Fed-Avg. However, table 1 indicates disparate generalization errors for both under this condition. Could you provide clarification on this discrepancy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4594/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4594/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4594/Reviewer_3qgW"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4594/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698662147435,
        "cdate": 1698662147435,
        "tmdate": 1699636437709,
        "mdate": 1699636437709,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WljOCOYWvW",
        "forum": "Cnn60wwTe1",
        "replyto": "Cnn60wwTe1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4594/Reviewer_4LFR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4594/Reviewer_4LFR"
        ],
        "content": {
            "summary": {
                "value": "This paper investigated the generalization performance of federated sgd under centralized and decentralized settings. However, this paper missed some important references so that the contribution is not clear compared with those existing works."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. Studying the generalization performance of Fedavg and Dfedavg is interesting. \n\n2. The writing is good. It is easy to follow."
            },
            "weaknesses": {
                "value": "1. There are many existing works studying the generalization of centralized and decentralized federated learning algorithms. However, this paper missed a lot of them. Then, it is not clear what new contributions this paper has. e.g.,is the bound of this paper comparable with existing ones?\n\n[1] https://openreview.net/pdf?id=-EHqoysUYLx\n\n[2] https://arxiv.org/abs/2306.02939\n\n2. This paper does not show how the heterogeneity affects the generalization error. \n\n3. When the communication graph is fully connected, DFedAvg becomes FedAvg. But the generalization error of this paper does not have this relationship."
            },
            "questions": {
                "value": "1. There are many existing works studying the generalization of centralized and decentralized federated learning algorithms. However, this paper missed a lot of them. Then, it is not clear what new contributions this paper has. e.g.,is the bound of this paper comparable with existing ones?\n\n[1] https://openreview.net/pdf?id=-EHqoysUYLx\n\n[2] https://arxiv.org/abs/2306.02939\n\n2. This paper does not show how the heterogeneity affects the generalization error. \n\n3. When the communication graph is fully connected, DFedAvg becomes FedAvg. But the generalization error of this paper does not have this relationship."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4594/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698899586833,
        "cdate": 1698899586833,
        "tmdate": 1699636437636,
        "mdate": 1699636437636,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BbJdhz3Lpo",
        "forum": "Cnn60wwTe1",
        "replyto": "Cnn60wwTe1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4594/Reviewer_9dRK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4594/Reviewer_9dRK"
        ],
        "content": {
            "summary": {
                "value": "This work studied the generalization performances of CFL and DFL in terms of excess risks, under the framework of algorithmic stability. They demonstrate theoretically that CFL has superior generalization capabilities and empirically support this claim with experiments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Thorough study on CFL and DFL\n2. Extensive discussions and experiments."
            },
            "weaknesses": {
                "value": "1. Authors claimed the study focuses on nonconvex functions (e.g., Table 1), but to determine the final excess risk (Corollary 1.2 and 2.2), the additional PL-condition is imposed, so all in all the study and follow-up discussion comparing CFL and DFL in fact focuses on PL functions only, which restricts the scope of the research. Even though I may agree PL is a bit common in nonconvex literature, but all in all it is still pretty strong (similar to strong convexity though), not to mention the mismatch with deep learning models you used in the experiment part. I think the authors should clearly clarify this point in the theory part, and present it at the beginning of the paper.\n2. Follow-up on Corollary 1.2 and 2.2, to derive the final excess risk, the work borrows the work on the optimization literature (Haddadpour & Mahdavi (2019); Zhou et al. (2021)). But as far as I can see, there is a mismatch in the parameter selection between your generalization learning rate and their optimization learning rates (not to mention some assumption differences). Or at least I think authors should have a double check on their text and discussions for a verification. In that sense, I don't think it is correct to simply aggregate the two results together. Could you please clarify this point?\n3. In terms of the theoretical proof, as far as I can see, the proof heavily relies on those in Hardt et al., 2016, with some modification in the later recursion to fit the finite-sum structure. Even though the story of the proof is interesting, the theoretical novelty is restricted a bit regarding the resemblance.\n\nAll in all, I appreciate the authors' efforts in the work, but I still think the significance of the contribution in the work is still unclear to pinpoint, which requires further clarification. Please definitely indicate if I misunderstood any points in the work. Thank you."
            },
            "questions": {
                "value": "1. Missing definition of $U$ in Theorem 1, which is the upper bound of function values.\n2. Several typos with $||f()-f()||$, which should be revised to absolute values for scalars."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4594/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699269365167,
        "cdate": 1699269365167,
        "tmdate": 1699636437564,
        "mdate": 1699636437564,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ICVhntiymA",
        "forum": "Cnn60wwTe1",
        "replyto": "Cnn60wwTe1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4594/Reviewer_Fo8N"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4594/Reviewer_Fo8N"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed the analysis of the uniform stability and excess risk between CFL and DFL. They proved that CFL would give a better generalization performance than DFL. Moreover, they demonstrated that for DFL to serve as an effective compromise, balancing between performance and communication efficiency, its network topology must meet certain minimal criteria. Lastly, the authors provided extensive simulation results to support their theory analysis."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and clearly presented.\n\n2. The authors provide a clear comparison of their findings with existing results such as those of vanilla SGD.\n\n3. The authors provide guidelines on how to tune the optimal active numbers and the optimal topology ratio."
            },
            "weaknesses": {
                "value": "Some notations are not defined in the main body. For example, The matrix U in Theorem 1 is shown before it is formally defined in the Appendix. It would be kind of confusing to readers."
            },
            "questions": {
                "value": "The paper seems to require the sample gradient to be Lipschitz, according to Lemma 6. However, Assumption 1 requires that the full gradient is Lipschitz. Is this a typo?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4594/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4594/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4594/Reviewer_Fo8N"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4594/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699476755086,
        "cdate": 1699476755086,
        "tmdate": 1699636437458,
        "mdate": 1699636437458,
        "license": "CC BY 4.0",
        "version": 2
    }
]