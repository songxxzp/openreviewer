[
    {
        "id": "NndbacTh7J",
        "forum": "w49jlMWDSA",
        "replyto": "w49jlMWDSA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6168/Reviewer_om6n"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6168/Reviewer_om6n"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to enhance VL model ability for few-shot classification. Using the names of interested classes, LLM generates diverse and descriptive explanations of each class. The CLIP model matches each image in the dataset with the generated explanations, resulting in an image-synthetic caption dataset. Using the dataset, the CLIP model can discriminate the fine-grained information better. The proposed method achieves better results than comparable methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The proposed method is simple, straightforward, and sound. \n\n+ The paper is clearly written and easy to understand.\n\n+ The proposed method achieves better results than comparable methods."
            },
            "weaknesses": {
                "value": "- The proposed method requires domain (dataset) specific designs such as prompts for LLM. However, in a few-shot setting, it is hard to determine the designs because of the scarcity of validation and test sets. This makes the proposed method less general to real-world few-shot tasks.\n\n- The paper lacks of comparison with other fine-tuning techniques such as prompt tuning approaches. Please analyze and compare with other families of VL finetuning.\n\n- If I understood correctly, the proposed method requires full images for the target domain. In a real-world setting, it may be hard to obtain such images, especially for medical images. \n\n- Why is a summary is required? We can see the performance improvement by doing this, but I found no proper explanations for why.\n\n- Manual cleaning mentioned in Section 5 seems not fancy. In a specific field, manual cleaning requires an expensive effort of experts such as the medical field. In addition, instead of manually cleaning, we can make labels for more images under the same effort, which is likely to produce much more improvements than manual cleaning."
            },
            "questions": {
                "value": "- Instead of the process of generating captions using LLM and matching with CLIP, how if the trained vision-llm models (e.g., BLIP-2, Flamingo) are used to generate captions?\n\n- The generated explanations are exclusively matched to images? Or the same captions can be matched to multiple images? If so, please statistics how frequently each explanation is matched to the images."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6168/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731286157,
        "cdate": 1698731286157,
        "tmdate": 1699636669932,
        "mdate": 1699636669932,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CgNbobrj43",
        "forum": "w49jlMWDSA",
        "replyto": "w49jlMWDSA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6168/Reviewer_aPA7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6168/Reviewer_aPA7"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method of generating image-specific text by prompting LLM for fine-grained object representations. In particular, the authors provide a three-step workflow, i.e., 1) prompting GPT-3 with domain-specific prompts to generate detailed  candidate text descriptions; 2) using CLIP to match each training each to the candidate text set; 3) summarizing the matched text via LLM to construct image-text pairs for fine-tuning CLIP image encoder. In addition, the learned representation is useful for fine-grained image classification. Besides, the authors provide a new fine-grained image classification dataset, Fitzpatrick40. Experimental results proved the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The motivation of prompting LLM for fine-grained classification is well presented.\n\n(2) The explanations and illustrations of the three-step workflow is well-formulated and mostly clear."
            },
            "weaknesses": {
                "value": "(1) The motivation of prompting LLM for visual classification is not that novel to the community and the authors listed (Maniparambil et al., 2023) as an example.\n\n(2) The contribution is limited since the core idea is the same as typical data labeling workflow that uses LLM. More specifically, during prompting LLM, the user-provided prompt is not coming for free, it is also a kind of human knowledge or preference prior. There is no discussion on this difficulty of prompt preparation as compared to typical easy prompt such as \u201ca photo of class name\u201d. Thus, the workflow of generating image-text pairs has no difference from data labeling workflow that uses LLM.\n\n(3) In the experiments, Table 1, the proposed workflow which uses GPT-3, CLIP and intricate prompts, does not get significant improvement over FLYP which uses CLIP and a typical easy prompt, though the comparison setting is unfair.\n\n(4) The most conflicting method (Maniparambil et al., 2023) is not compared in the experiments."
            },
            "questions": {
                "value": "No."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6168/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6168/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6168/Reviewer_aPA7"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6168/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698735947558,
        "cdate": 1698735947558,
        "tmdate": 1699636669816,
        "mdate": 1699636669816,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "v1Oe5Vuj0z",
        "forum": "w49jlMWDSA",
        "replyto": "w49jlMWDSA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6168/Reviewer_zGRf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6168/Reviewer_zGRf"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new method for generating and matching fine-grained descriptions for images, which is named GIST. The proposed method GIST adopts a large language model to generate fine-grained texts with carefully designed prompts and then uses pre-trained CLIP to match the images with generated texts.\nBased on the generated image-text pairs, this paper adopts a pre-trained CLIP and trains a classifier for downstream tasks. The experiments are conducted in both full-shot and few-shot settings. The experimental results show that the proposed GIST outperforms previous approaches and the baseline CLIP."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper presents a new method named GIST for fine-grained classification.\n2. The proposed method adopts a large language model to generate domain-specific class descriptions and matches the texts and images with a pre-trained vision-language model.\n3. This paper trains a CLIP classifier with the generated and matched descriptions.\n4. The proposed GIST achieves good results on several benchmarks with full/few-shot settings."
            },
            "weaknesses": {
                "value": "1. I'm concerned about the technical contribution of this paper. Using a large language model to generate/augment text for CLIP training has been explored in several works [1,2]. \n2. This paper lacks the ablations about matching texts and images. It's unclear whether the matching based on a pre-trained CLIP will impact the downstream tasks. In addition, I'm concerned about how many captions are matched to one image and whether more captions will help.\n3. This paper lacks studies on the impact of fine-tuning CLIP and whether more data (images) will further improve.\n\n[1] Fan et.al. Improving CLIP Training with Language Rewrites. NeurIPS 2023.\n[2] Maniparambil et.al. Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts. ICCVW 2023."
            },
            "questions": {
                "value": "1. What are text inputs for the proposed GIST, the standard text prompts with labels or the text descriptions?\n2. I'm concerned about the performance of fine-tuning CLIP with both short text descriptions and the original texts, and how about fine-tuning the CLIP with longer texts while fine-tuning classifiers with shorter descriptions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6168/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698854311989,
        "cdate": 1698854311989,
        "tmdate": 1699636669658,
        "mdate": 1699636669658,
        "license": "CC BY 4.0",
        "version": 2
    }
]