[
    {
        "id": "LdL9Wfxq4t",
        "forum": "BRdEBlwUW6",
        "replyto": "BRdEBlwUW6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1844/Reviewer_Wxgz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1844/Reviewer_Wxgz"
        ],
        "content": {
            "summary": {
                "value": "This research addresses the \"robust fairness problem\" in adversarial training, where there's a significant difference in model accuracy between classes. Current methods sacrifice performance on easier classes to improve harder ones. However, the study observes that the model's predictions for the worst class often favor similar classes instead of the easy ones under adversarial attacks. As the distance between classes decreases, robust fairness deteriorates. To mitigate this, the Distance-Aware Fair Adversarial training (DAFA) approach is introduced. It assigns distinct loss weights and adversarial margins to each class and adjusts them to balance robustness among similar classes. Experiments show that DAFA not only maintains average robust accuracy but also significantly enhances fairness, especially for the worst-performing class, compared to existing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is easy to follow\n\n2. The motivation is clear"
            },
            "weaknesses": {
                "value": "1. Experiments on the sensitivity of trade-off hyperparameter $\\beta$ is missing. \n\n2. The experiments only based on TRADES are very limited.\n\n3.  Several baselines are missing. For example, Group-DRO [1]\n\n[1] Sagawa, Shiori, et al. \"Distributionally Robust Neural Networks.\" International Conference on Learning Representations. 2019."
            },
            "questions": {
                "value": "See Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1844/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698072410942,
        "cdate": 1698072410942,
        "tmdate": 1699636114512,
        "mdate": 1699636114512,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "deMYqs1WLi",
        "forum": "BRdEBlwUW6",
        "replyto": "BRdEBlwUW6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1844/Reviewer_73vn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1844/Reviewer_73vn"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to mitigate the class-wise fairness issue in adversarial training (AT) by proposing a DAFA framework, which first theoretically and empirically investigates the impact of the distance between classes on worst-class robustness and then proposes dynamically adjusting the weights and margins deployed in AT for better robust fairness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The motivation of this paper is clear. Figure 1-3 illustrates what similar and dissimilar classes are and how they affect class-wise performance.\n2. The claims of the influence of class-wise distance on their robustness are supported by both theoretical analysis and empirical verification.\n3. This paper further refines the state-of-the-art understanding of class-wise accuracy and robustness. As CFA [4] shows, different classes should use specific training configurations in AT to strengthen class-wise performance, the proposed DAFA further takes the interaction of classes into consideration and highlights the role of similarity between classes."
            },
            "weaknesses": {
                "value": "1. My major concern with this work is the unfair comparison with baselines in terms of experiment settings. Here, I take CFA [4] as an example, and please also check other baselines.\n    - CFA includes a Fairness-Aware Weight-Average (FAWA) strategy to mitigate the fluctuating effect of worst-class robustness. Weight average requires more epochs to converge after learning rate decay, and in the original paper of CFA, the learning rate decays at the 100th and 150th epoch in a 200-epoch training. Therefore, the learning rate schedule in your setting significantly decreases the performance of CFA.\n    \n    I suggest using the original settings of different baselines to ensure fair comparisons. I will change my rating based on how this issue is addressed.\n    \n2. Page 1. The research thread of the robust fairness problem seems to be confusing. Based on my expertise in robust fairness research, the main thread [1-5] on this problem focuses more on how to adjust objective functions and configurations in AT, and the long-tailed classification is a minor perspective. I suggest revising the introduction of this paper by focusing on more related work [1-5].\n3. Page 4. Some of the Theorems seem to be very close to previous work [1, 3]. I suggest clarifying which theorems are a re-deduction of previous results and which are proposed new ones.\n4. There may be a mismatch between the motivation and the proposed method. It seems that the schedule in equation (6)-(9) still pays more attention to easy or hard, though class-wise similarity is involved. I suggest adding theoretical analysis on how these configurations improve class-wise robustness.\n\n### Minor Comments\n\n   \n- Page 2: Typo\n> limiting the learning of these neighboring classes can serve as an effective means to improve robust fairness.\n- Page 2, Section 2: I suggest adding the optimization objective of adversarial training (AT), which is a more primary method than TRADES.\n- Page 3, Section 3: It may benefit from introducing what variance is in this context more detailedly and formally.\n- Page 3, Section 3: Misuse of \"On the other hand\". \"In addition\" is suggested.\n- Page 4. I suggest putting the details on how to calculate class-wise distance in Section 3 rather than the appendix to improve readability.\n- Page 6, Section 4:\n> we employ two strategies: adjusting loss weights and adversarial margins\n   \n    is very similar to the CCM and CCR strategies proposed in [4]. A reference is required here.\n    \n\n[1] To be robust or to be fair: Towards fairness in adversarial training. ICML\n\n[2] Analysis and applications of class-wise robustness in adversarial training.KDD\n\n[3] On the tradeoff between robustness and fairness. NeurIPS\n\n[4] CFA: Class-wise calibrated fair adversarial training. CVPR\n\n[5] WAT: improve the worst-class robustness in adversarial training. AAAI"
            },
            "questions": {
                "value": "1. Page 5, Section 3. Why is class-wise distance defined as the average distance of a class to other classes? In my opinion, for classification tasks, the distance between a class and its closest one plays the most important role in determining the decision boundary, and the influences of classes that are far away are negligible."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1844/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1844/Reviewer_73vn",
                    "ICLR.cc/2024/Conference/Submission1844/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1844/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698634128359,
        "cdate": 1698634128359,
        "tmdate": 1700235628718,
        "mdate": 1700235628718,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hKgyGmMAmX",
        "forum": "BRdEBlwUW6",
        "replyto": "BRdEBlwUW6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1844/Reviewer_n9NT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1844/Reviewer_n9NT"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the robust fairness problem, they empirically find that  the majority of the model\u2019s predictions for samples from the worst class are biased towards classes similar to the worst class. Inspired by this, they show a theoretical result that a class is hard to learn and exhibits lower performance when it is close to other classes. They then propose a novel method, it use class-wise probability to measure the difficulty of the class. Based on this, they computing the weights which is used to reweight the loss and adjust the margin. Empirically, they show the proposed method outperforms other baselines in three different datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors use experiments to show that the misclassified samples from worst class are biased towards classes similar to the worst class, and they provide a theoretical analysis on this.\n2. They propose a novel method, adjusting the weights and margins based on the class-wise probability, which is used to measure the difficulty of the class.\n3. The authors show experiments on three widely used datasets to show the superiority of the proposed method."
            },
            "weaknesses": {
                "value": "1. The update rule for $\\mathcal{W}$ may result in negative values in some cases. For instance, when the performance of a class consistently remains the best, this issue arises. The authors do not address this problem or discuss its implications. If $\\mathcal{W}$ becomes negative, it would lead to negative robust radii ($\\mathcal{W}_y\\epsilon$), which is unreasonable.\n2. The update process for $\\mathcal{W}$ appears to be time-inefficient, particularly when dealing with datasets that contain a large number of classes. It is suggested that the authors provide information on the time costs of the proposed method compared to other baselines."
            },
            "questions": {
                "value": "1. What is the theoretical and empirical range of $\\mathcal{W}$? A clarification of the possible values for $\\mathcal{W}$ is necessary.\n2. Does the proposed method require more time for experimentation compared to other baselines? \n3. What distinguishes the proposed method from other baselines, considering that these methods also adjust weights? Can the authors elaborate on the differences in the weight curves between the proposed method and the baselines?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1844/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1844/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1844/Reviewer_n9NT"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1844/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698670269965,
        "cdate": 1698670269965,
        "tmdate": 1700631411900,
        "mdate": 1700631411900,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mCB1DA64cl",
        "forum": "BRdEBlwUW6",
        "replyto": "BRdEBlwUW6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1844/Reviewer_nK3C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1844/Reviewer_nK3C"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the author introduced a class-distanced method aimed at enhancing fairness in Adversarial Training. To be more specific, the methods designs the robust sample loss weights and adversarial margins based on the class similarities to penalize more for hard similar classes. The author argue that the error of hard samples are mainly caused by falsely predicted as other similar classes, rather than disimilar classes. To verify this, the author first show that for the hard class, the accuracy is much improved dramatically if it trained with classes that not similar to it. Then the author derive theoretical analysis quantitively shows that the robust errors for each class, and predict error between different classes is monotonically decreasingly correlated with the gap between different classes. The large the class gap, the smaller the predictions error. Further more,  in section 4.1, the author present \"an example illustrating that improving\na hard class\u2019s performance is more effectively achieved by moving the decision boundary (DB)\nbetween the hard class and its neighboring class than that between the hard class and a distant class\". Then empirical studies on several datasets verifies the effectiveness of the proposed methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Please refer the summary.\nMoreover, the method proposed by the author is agnostic and is empirically validated by combining it with Trades framework and an PGD to demonstrate its advantages."
            },
            "weaknesses": {
                "value": "The paper is well written. But\n\n 1) It rambling too much until the Page 6. \n\n2) The author only mentioned  once that  it used class prediction probability to measure the class similarities. \"Our proposed method quantifies the similarity between classes in a dataset by utilizing the model output prediction probability. \" When it comes to class similarity, the first idea that came to my mind is embedding similarities between different classes. It is confusing."
            },
            "questions": {
                "value": "1) What's the \"red\" and \"blue\" arrows in Figure 1 represents?\n2) Are all the figures when it comes to measure the class similarities using the average class soft max probabilities you described in section 4.2 when you proposing your methods?\n3) Please proofread the paper and simplify the sentences for better comprehension such as \"Allocating a larger adversarial margin to the hard class than to adjacent classes has the effect of pushing the DB away from the hard class\u2019s center by an amount corresponding to the difference in their adversarial margins.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1844/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699398588586,
        "cdate": 1699398588586,
        "tmdate": 1699636114306,
        "mdate": 1699636114306,
        "license": "CC BY 4.0",
        "version": 2
    }
]