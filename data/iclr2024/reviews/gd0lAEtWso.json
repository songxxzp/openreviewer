[
    {
        "id": "cyDowUPFZr",
        "forum": "gd0lAEtWso",
        "replyto": "gd0lAEtWso",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission195/Reviewer_4D6Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission195/Reviewer_4D6Y"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a generative model for text-conditional human motion synthesis offering detailed control over joint positions in each frame. The authors propose two guidance mechanisms, spatial and realism guidance, that aim to generate human motion which closely adheres to the guidance while maintaining realism. The effectiveness of these designs is established through experiments, supported by comprehensive data and high-quality visualizations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper considers the novel task of guiding individual joints spatially in human motion generation. Experiments conducted on the HumanML3D and KIT-ML datasets show promising results.\n\n2. The experimental setup is robust and the accompanying visualizations are of high quality, reflecting the authors' meticulous efforts in this research."
            },
            "weaknesses": {
                "value": "1. Previous works guiding the position of the pelvis could potentially be intuitively adapted to joint positions with proper coordinate transformations, which undermines the novelty of this work, making it seem more of an incremental step rather than a solution to fundamental problems in the field.\n\n2. Despite the novelty of controlling all joints at any frame, the paper lacks a discussion on efficient collection of the spatial guidance trajectory, particularly considering the fact that the joint positions are no more relative to the pelvis. This aspect is crucial for practical applications in industries such as 3D animation. Moreover, the paper does not discuss the model's tolerance for inherently unnatural guidance (*e.g.*, manually drawn trajectories or those from different datasets).\n\n3. There are some formatting errors (*e.g.*, the misuse of `\\citep` instead of `\\citet` in some citations)."
            },
            "questions": {
                "value": "In most scenarios, the text input and the spatial guidance seem redundant. Can the model effectively comprehend and follow instructions that appear only in one modality? (*e.g.*, providing spatial guidance only on the foot or pelvis, while the hand activity is only described via the text input?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission195/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission195/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission195/Reviewer_4D6Y"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission195/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698323924362,
        "cdate": 1698323924362,
        "tmdate": 1699635945312,
        "mdate": 1699635945312,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cRAkNGGFEO",
        "forum": "gd0lAEtWso",
        "replyto": "gd0lAEtWso",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission195/Reviewer_gCs4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission195/Reviewer_gCs4"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a human motion generation method with the ability of manipulating any joint at any time. The method can take the language prompt as condition and both the spatial guidances and realism guidance as constraints. Compared to previous method, including MDM, based on which the method is developed, the proposed method showcases more flexibility for downstream applications. With a single model, the proposed OmniControl sets a new SOTA to control both the pelvis and other joints in motion generation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The method allows using spatial guidance to constraint the generated motion sequence. The constraint can be put to any joint instead of just pelvis.\n- By combining the realism guidance, the conditionally generated motion sequences can be expected to be more natural under the spatial constraint. The realism guidance is a trainable copy of encoder to enforce the spatial constraints. It is essentially an enforced encoder fusing the information from the language prompt and the spatial constraints. Connected with the main transformer encoder during training, the realism guidance is trained to correct the signal passed in the corresponding layers."
            },
            "weaknesses": {
                "value": "- The paper writing is not fluent enough and needs polishing to be easier to follow.\n- Given the carefully designed modules, the time efficiency for training is important to evaluate the significance of the proposed method. However, this part is missing in the paper.\n- Some important baselines are missing in the experiment sections, such as [1,2]. Adding the full set of published baselines on the benchmarks of HumanML3D and KIT-ML will change the position of the proposed methods highly. Can the authors elaborate more about the comparison with the baselines? Or maybe there is any reason that these baselines are not proper to compare with?\n- Some minor writing issues, such as duplicated typo: input -> \"input\"\n- Referring to Figure 5, which is placed in a very late position in the introduction section makes a bad reading flow. You may want to adjust the position of Figure 5 to make it closer to where it is referred to.\n\nReference:\n\n[1]: \"T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations\u201d CVPR 2023\n\n[2]: \u201cGenerating Diverse and Natural 3D Human Motions from Text\u201d, CVPR 2022"
            },
            "questions": {
                "value": "See my concerns listed above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission195/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission195/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission195/Reviewer_gCs4"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission195/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698582477824,
        "cdate": 1698582477824,
        "tmdate": 1700684550674,
        "mdate": 1700684550674,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "snvxs0XtFT",
        "forum": "gd0lAEtWso",
        "replyto": "gd0lAEtWso",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission195/Reviewer_YHiJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission195/Reviewer_YHiJ"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces OmniControl, a novel method that enhances text-conditioned human motion generation by allowing flexible spatial control across multiple joints, ensuring realistic and coherent movements. The integration of spatial and realism guidance achieves a balance between accuracy and natural motion, demonstrating superior pelvis control and promising outcomes on various joints, marking an advancement in generating constrained, realistic human motions. The commitment to releasing code and model weights further enhances accessibility for future advancements in this field."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper offers a simple yet effective method to integrate spatial control signals into a text-conditioned human motion generation model based on the diffusion process.\nThe introduction of realism guidance to refine all joints for generating more coherent motion is commendable.\nThe evaluation is adequate and comprehensive."
            },
            "weaknesses": {
                "value": "It would be better if the difference and advantage between the global coordinates and local coordinates could be visualized.\nInference time is higher than MDM and GMD.\nThe concept in Fig. 4, such as the input process, requires further clarification for better comprehension. In addition, The components of spatial encoder F and the size of output f_n are not explained.\nThe difference and advantage between the global coordinates and local coordinates were not visually explained."
            },
            "questions": {
                "value": "In Fig. 7, understanding why higher density leads to higher FID and Foot skating ratio while other factors lead to lower FID and Foot skating ratio is required. Traditionally, higher density in certain contexts can lead to better performance due to increased information or more complex interactions. However, in your case, it seems to be causing a lower performance. Additionally, the paper mentions the Avg. error of MDM and PriorMDM being zero due to the inpainting property. Elaborating on the nature of this property would provide clarity. Moreover, why the proposed methods are with zero error when density is low should be addressed.\nIn the supplementary video, it would be better if the video demonstrate \u201cControl other joints\u201d can be visualized compared with GMD.\nQ: Where is the spatial control signal coming from? Is it given by dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission195/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698730820300,
        "cdate": 1698730820300,
        "tmdate": 1699635945126,
        "mdate": 1699635945126,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rCwfkG9n12",
        "forum": "gd0lAEtWso",
        "replyto": "gd0lAEtWso",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission195/Reviewer_EL5t"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission195/Reviewer_EL5t"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a unified approach that can support controlling any joint at any time for text-driven human motion synthesis. The core design is to integrate both spatial and realism guidances to keep the generated motion faithful to the control signals while improving its reality and naturalness. Experiments show that the proposed method outperforms baselines in terms of control accuracy, motion reality, and motion diversity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* (1) The method is the first to control any joint at any time for human motion synthesis, which can improve the flexibility of motion generation tasks and potentially benefit downstream applications such as generating human motion on different terrains.\n\n* (2) The method design is clear and reasonable.\n\n* (3) Experiments demonstrate the effectiveness of the proposed method.\n\n* (4) The analysis for method ablations is solid.\n\n* (5) The paper is well-organized and easy to follow."
            },
            "weaknesses": {
                "value": "* (1) The inference speed for the proposed method is much lower than baselines, which could potentially impede the method to apply to a large amount of data.\n\n* (2) In the third column of Figure 1, the authors show that the method can support a combination of control signals from different joints. However, the paper lacks quantitative analysis to further examine its performance."
            },
            "questions": {
                "value": "I wonder whether the proposed method can support motion editing where after a motion is generated, control signals can be edited and can further adjust the motion to be not only close to the previous one but also faithful to the new control signals."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission195/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission195/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission195/Reviewer_EL5t"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission195/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698739099639,
        "cdate": 1698739099639,
        "tmdate": 1699635945045,
        "mdate": 1699635945045,
        "license": "CC BY 4.0",
        "version": 2
    }
]