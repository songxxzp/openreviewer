[
    {
        "id": "RKlErwpEtX",
        "forum": "b66P1u0k15",
        "replyto": "b66P1u0k15",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission130/Reviewer_wNAW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission130/Reviewer_wNAW"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors mainly tackle the long-tailed learning from the optimization perspective. Specially, this paper first reveals the optimization conflicts among categories in long-tailed learning, and proposes to integrate multi-objective optimization (MOO) with long-tailed learning. Temporal design on MOO, variability collapse loss and sharpness-aware minimization are then employed. Experiments on four long-tailed benchmark datasets are conducted to validate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-\tLeveraging MOO method to enhance long-tailed learning seems reasonable.\n-\tThis paper reveals the phenomena of optimization conflicts among categories, which is an importance topic in the long-tailed learning from the optimization perspective. \n-\tThe proposed method does not introduce additional computational cost.\n-\tThe paper is generally easy to follow."
            },
            "weaknesses": {
                "value": "-\tThe paper introduces some trivial factors, e.g. sharpness-aware minimization, which makes the contribution of this paper ad-hoc. In my humble opinion, sharpness-aware minimization does not show strong connection to the proposed MOO framework, and can be integrated to almost all the long-tailed learning baselines.\n-\tThe technical contribution of this paper seems not strong. This paper mainly adopts CAGRAD with slight modification on the training schedule (temporal design). Besides, sharpness-aware minimization is directly applied and the variability collapse loss is only a regularization term on the standard deviation of the loss.\n-\tThe effectiveness of the proposed PLOT on top of baseline methods shows significant variability, see Table 1-2. However, the authors only conduct experiments on top of cRT + Mixup/MiSLAS on ImageNet-LT, Places-LT, iNaturalist, thus casting doubts on the confidence of the results.\n-\tIn Table 1, MOO underperforms in nearly half the cases when compared with baseline methods (24/54). It cannot support the claims that MOO is beneficial for long-tailed learning. However, in Table 4, the authors present the most favorable performance enhancement with cRT + Mixup, thus making the results less confident.\n-\tAccording to Theorem 4.1, the generalization bound is bounded by the weighted intra-class loss variability, i.e., $w_KM$. It raises the question of whether uniformly constraining the intra-class loss variability is the optimal solution. Besides, in the neural collapse theory, minimizing loss function (e.g. cross-entropy loss) can lead to the intra-class collapse at the optimal case, why there should be an explicit constraint on intra-class variability?\n-\tThis paper lacks background and discussions on the MOO methods, including MGDA, EPO and CAGrad. The reason for choosing CAGrad is solely performance-driven, lacking a connection to its inherent compatibility with the long-tailed learning paradigm.\n-\tIn this paper, multi-classification task is regarded as multiple binary classification tasks. Does the proposed approach yield effective results when applied to imbalanced multi-label classification tasks? The empirical study on the multi-label tasks will benefit a lot to improve the generalizability of the proposed method.\n-\tThis paper lacks disjoint analysis on the classification performance wr.t. different groups (many/medium/few).\n-\tSome results on logit adjustment are missing in Table 2.\n-\tThere is no clue that the proposed method solves the gradient conflicts issue depicted in Figure 1.\n-\tIn figure 8, it's unclear whether the proposed method actually surpass the performance of the baseline method LDAM-DRW in terms of gradient similarities. The gradient similarities of the proposed method exhibit a similar trend in the second stage.\n-\tIt is suggested to provide some qualitative results (e.g. t-SNE) to validate the effectiveness of the proposed method on the embedding space."
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission130/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission130/Reviewer_wNAW"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission130/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698650044274,
        "cdate": 1698650044274,
        "tmdate": 1700633854224,
        "mdate": 1700633854224,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5NhvzCm09T",
        "forum": "b66P1u0k15",
        "replyto": "b66P1u0k15",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission130/Reviewer_KQKE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission130/Reviewer_KQKE"
        ],
        "content": {
            "summary": {
                "value": "This paper considers an interesting and important issue in long-tailed learning, that is the optimization conflicts. To solve this problem, the authors introduce Pareto optimization for long-tailed learning. First, the authors observe that existing widely-used fixed re-balancing strategies can uniformly lead to gradient conflict. Then, they introduce Multi-Objective Optimization (MOO) to enhance existing long-tailed learning methods. Moreover, the authors find that directly integrating MOO-based methods can lead to performance degradation. To solve this, they propose to decouple the MOO-based methods from the temporal rather than structural perspective to enhance the integration. Experimental results demonstrate that the proposed temporal MOO method can boost the baseline method and achieve an improvement by a large gap."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper studies an interesting problem, which is the optimization conflict in long-tailed learning.\n2. The authors propose to utilize MOO to solve the conflict issue, which is reasonable and makes sense.\n3. This paper is clearly written and easy to understand.\n4. The authors conduct multiple empirical studies to demonstrate the effectiveness since the results show that the proposed method can boost the performance of the baseline methods obviously.\n5. The source code is released for reproduction."
            },
            "weaknesses": {
                "value": "1. Figure 1 may cause misunderstandings. How is Figure 1 computed? Is it cosine similarities between the mean of the gradient of different classes? Since the diagonal is red (1.0), it seems that the values represent gradient similarities. And what is the connection between \"gradient conflicts\" and \"gradient similarities\"?\n2. You mainly consider the directions of gradients. Have you considered the impacts of the L2 norms of the gradients?\n3. As you have mentioned MOO is applied during the early stages of representation learning, how to select the applied stage?\n4. The iNaturalist dataset has multiple versions. You should highlight it with \"iNaturalist 2018\"."
            },
            "questions": {
                "value": "(This is not a question, but a suggestion). I found a very similar work that also studies long-tailed learning with multi-objective optimization [1]. Maybe you can publicly release your work considering its timeliness.\n\n[1] Long-Tailed Learning as Multi-Objective Optimization, in https://arxiv.org/abs/2310.20490"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission130/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699027535574,
        "cdate": 1699027535574,
        "tmdate": 1699635938423,
        "mdate": 1699635938423,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2HMUqOcBZs",
        "forum": "b66P1u0k15",
        "replyto": "b66P1u0k15",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission130/Reviewer_yXjC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission130/Reviewer_yXjC"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the problem of Deep Long-Tailed Recognition (DTLR) and highlights the importance of dynamic re-balancing to address optimization conflicts in this domain. The authors empirically demonstrate that existing DTLR methods are dominated by certain categories due to fixed re-balancing strategies, preventing them from effectively handling gradient conflicts. To address this, they introduce an approach based on multi-objective optimization (MOO) to decouple the problem from a temporal perspective, avoiding class-specific feature degradation. Their method, named PLOT (Pareto deep long-tailed recognition), was conducted on several benchmarks for evaluation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The long-tailed recognition problem studied in this paper is a fundamental task that deserves further study.\n- The paper is well organized and easy to follow.\n- The technique contributions of this paper are novel and reasonable, which are tailored for the challenges of long-tailed recognition."
            },
            "weaknesses": {
                "value": "- My main concern with this paper is that the authors primarily demonstrate the effectiveness of their approach by augmenting existing methods with PLOT. However, they lack comprehensive performance comparisons by integrating PLOT with state-of-the-art methods like BBN, SADE, PaCo, etc. This omission makes it challenging to assess the method's effectiveness in comparison to the latest advancements in the field, raising questions about its overall impact and general applicability.\n- Another potential weakness of this paper is the insufficient coverage of long-tailed recognition methods in the references. The paper may not thoroughly discuss some classical methods in the field, which can be essential for providing a comprehensive understanding of the long-tailed recognition landscape.\n- The conclusion of the article should be written in the past tense. Additionally, the conclusions require to be added future work of this paper.\n- The authors are encouraged to carefully proofread the paper."
            },
            "questions": {
                "value": "Please see the paper weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission130/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699081295077,
        "cdate": 1699081295077,
        "tmdate": 1699635938351,
        "mdate": 1699635938351,
        "license": "CC BY 4.0",
        "version": 2
    }
]