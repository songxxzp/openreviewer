[
    {
        "id": "oB6RJNMRrW",
        "forum": "VXak3CZZGC",
        "replyto": "VXak3CZZGC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1657/Reviewer_kMBq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1657/Reviewer_kMBq"
        ],
        "content": {
            "summary": {
                "value": "This paper delves into the challenge of out-of-distribution (OOD) generalization. Building upon previous research, it introduces the HYPO learning algorithm aimed at reducing intra-class variation while increasing inter-class separation. Notably, the paper establishes a connection between the loss function and the von Mises-Fisher (vMF) distribution. Subsequently, it provides a generalization upper bound of variation. These set HYPO apart from an existing work PCL. Extensive experimentation on OOD benchmarks showcases the superior performance of the HYPO algorithm."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper is well-written and well-organized.\n- The problem studied in this paper is interesting and important.\n- The authors have provided a clear discussion of the relation to previous work, PCL."
            },
            "weaknesses": {
                "value": "1. The theoretical result appears to have limitations. \n- Although Theorem 5.1 provides insights into the upper bound of generalization variation, it does not conclusively demonstrate the superiority of the proposed method or loss, since the theorem directly assumes that the variation can be optimized to a small value under the proposed loss, i.e., $\\frac{1}{N}\\sum_j\\mu_{c(j)}^T z_j\\ge 1-\\varepsilon$. If one were to substitute an alternative loss, such as changing the prototype to another sample within the same class (e.g., employing the SupCon loss) or directly using PCL's loss, it would also yield a generalization bound. Consequently, the question arises: How can we establish that the proposed loss is indeed superior, provably?\n- Theorem 5.1 cannot be valid unless we explicitly specify the distribution distance  $\\rho$.\n- Theorem 5.1 does not account for the influence of inter-class separation, a key aspect that this paper seeks to enhance through the second term in loss eq. (5). I notice that in Ye et al (2021)'s Theorem 4.1, function O(.) also depends on additional factors beyond just the variation.\n\n2. Training Loss.\n- Since prototypes $\\mu_i$ are updated in an EMA manner, it's worth noting that the second term in eq. (5) will not generate a gradient for $h$. Consequently, the second term of the loss becomes devoid of meaning.\n\n3. The idea is quite straightforward and shares many similarities with proxy-based contrastive learning methods. Is there any additional insight that I might have overlooked?\n\n4. The empirical improvements appear to be marginal, as indicated by the data in Tables 1 and 2.\n\nOverall, I think the theoretical contribution and empirical enhancements appear to have room for further development and strengthening."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1657/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1657/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1657/Reviewer_kMBq"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1657/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697984992791,
        "cdate": 1697984992791,
        "tmdate": 1700395894611,
        "mdate": 1700395894611,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "scL2oRt9Rc",
        "forum": "VXak3CZZGC",
        "replyto": "VXak3CZZGC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1657/Reviewer_W1ps"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1657/Reviewer_W1ps"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a pracical algorithm for achieving provable out-of-distribution (OOD) generalization. The proposed approach is motivated by recent theoretical work that decomposes OOD generalization into two measurable quantities: intra-class variation and inter-class separation. This paper designs a training objective (and representation space) where these terms can be optimized to achieve low OOD generalization error.\n\nSpecifically, the proposed method learns representations for each data point that lie on a hypersphere. The goal is to encourage data points belonging to the same class to lie close together on the hypersphere (in terms of cosine distance) but to have the centroids of each class lie far apart. This approach itself is not particularly novel, as several learning methods have previously been proposed that utilize hyperspherical embeddings. But this paper is the first to provide a theoretical justification angled at OOD generalization.\n\nThe paper provides a formal theoretical proof that bounds the OOD generalization error via a standard PAC-like learning bound. The proof leans on the prior theoretical results that motivated this work."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper proposed a simple algorithm that is easy to implement. The loss terms can be computed efficiently and are easy to mini-batch for SGD. The authors provide a clear description of the algorithm and even include pseudo-code. It would be easy to reproduce the proposed method.\n\nThe paper is well-written and easy to follow. Motivation is laid out clearly and the paper accurately describes its contributions relative to prior work. I was able to find all of the information that I wanted while reading the paper either within the main text or the appendices.\n\nI see the primary contribution of this work to be the formal theoretical guarantee on the generalization performance of the proposed method. The theoretical results presented in this work are environment agnostic in the sense that they only depend on the environments through the ability to fit the training data effectively and reduce the intra-class variation. This is a valuable contribution.\n\nThe empirical results are relatively thorough and compare HYPO (the proposed method) against a wide range of baseline methods across several tasks. The results show that HYPO performs well consistently, and is on average the best OOD classifier.\n\nI liked the simple theoretical exploration in Appendix J. This was a valuable inclusion that helped to give some intuition for the class separation loss component."
            },
            "weaknesses": {
                "value": "The paper lacks quantitative verification of the theoretical result. I think that this would be a valuable contribution to help give an idea of how tight/vacuous the bound is. I am mostly curious about the $\\epsilon$ term that appears in Theorem 5.1 and can be easily computed in practice.\n\nThe theoretical result shown gives a bound on the intra-class variation. This is a useful component of producing an OOD generalization bound, but it is not sufficient by itself. The results in Ye et al. require some regularity conditions that depend on the distribution over the learned representations --- this is difficult to compute in this case. From my point of view, the theoretical results in this paper provide a strong intuition for the success of the method but have not yet been demonstrated to produce a tractable OOD generalization bound.\n\nSpurious correlations are ignored in this work, though are one of the more challenging aspects of OOD generalization in practice. However, I think that this is a reasonable compromise to make at this stage.\n\nI feel that the novelty is slightly limited here. The proposed learning algorithm is a form of prototypical learning on a hypersphere. The specific loss is, to my knowledge, novel but is made up of fairly standard components. The theoretical results are novel and interesting, but are essentially an instantiation of results from prior work. Indeed, the contribution of the training loss to the generalization error is largely captured in an assumption within the theoretical statement. I do consider the overall novelty of this paper to be sufficient for me to recommend acceptance, but it has affected my overall judgment so I am including this as a weakness."
            },
            "questions": {
                "value": "- I'd appreciate it if the authors could explain the motivation behind Equation 6 a little more.  Is the primary goal to improve on the computational efficiency of computing the average across all training data points? Or is there another benefit to adopting an exponential moving average? This also ties loosely into my next question.\n\n- How strong is the assumption that the samples are aligned? Intuitively, the intra-class variation measures how much the features vary across environments for a single class. The alignment assumption is an assumption over all of the training data in the available environments. Consequently, Theorem 5.1 consists of a term that depends on epsilon, and a generalization term that (intuitively) describes generalization to the unavailable environments. I think it would be more valuable if one could show that the alignment assumption is satisfied by reducing the training loss directly, bringing the result more in line with typical PAC generalization bounds.\n\n- The epsilon factor could potentially make the bound very loose if it is too close to 1. Given that this value is easy to compute, I would be curious to know what epsilon looks like for some of the models trained in the experiments.\n\n- Ye et al. provide a specialized result for linear models (Theorem 4.2 in their work). I see this as a justification that the theoretical framework can be realized by some model. However, in the present work, it is unclear whether the vMF distribution can satisfy the regularity conditions for some choice of environment distribution(s). In other words, how do we know that the OOD generalization bounds can actually be computed for the choice of model used?\n\n\nMinor comments:\n\n- In the introduction, I'd recommend replacing the four lines of citations with a survey paper, for example [1]. The full list of references could be included in the related work, or even as an extended discussion of related work in the supplementary material.\n- [2] is another reference that explores a contrastive metric learning approach for hyperspherical embeddings. The goal here is not to do OOD generalization, but the algorithm is modestly similar.\n- In proof of Theorem 5.1, \"at last $1 - \\delta$\" -> \"at least $1-\\delta$\".\n- It would be nice if Table 1 were sorted by ascending average accuracy.\n\n\n[1]: Domain Generalization: A Survey, Zhou et al.\n[2]: Video Face Clustering with Unknown Number of Clusters, Tapaswi et al. ICCV 2019"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1657/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1657/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1657/Reviewer_W1ps"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1657/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839729003,
        "cdate": 1698839729003,
        "tmdate": 1700477983057,
        "mdate": 1700477983057,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "z6SZLWi3WN",
        "forum": "VXak3CZZGC",
        "replyto": "VXak3CZZGC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1657/Reviewer_HW2k"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1657/Reviewer_HW2k"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new loss function tailored to the out-of-distribution problem, where generalisation of the algorithm is required across multiple (and sometimes unseen) environments. Inspired by prior theoretical work, the authors devise an algorithm that encourages samples with the same label to be learnt by features that are as stable as possible across environments, while at the same time encouraging embeddings to look very dissimilar for data points with different labels. They achieve this by embedding points on the sphere and introducing class centroids per label (shared among environments), where points are encouraged to lie close to their corresponding centroid, and centroids themselves are pushed apart. The authors derive a theoretical guarantee for their algorithm and demonstrate its empirical success on CIFAR10-C, PACS and similar."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is very well-written which made it (mostly) easy to follow as well as a pleasure to read for me. \n2. The suggested loss function is very intuitive and I like the geometric interpretation the authors provide in terms of the Mises-Fisher model. The visualisation in Fig 4 is also very neat. Empirical performance is also very strong across the different explored tasks."
            },
            "weaknesses": {
                "value": "1. I struggle to see how Theorem 5.1 connects back to the proposed loss function. From Theorem 3.1 we know that $\\nu^{\\text{sup}}$ serves as an upper bound to the OOD error, and then Theorem 5.1 in-turn provides an upper bound for $\\nu^{\\text{sup}}$ in terms of the  Rademacher complexity and some additive constants. Which term here is the loss trying to minimise here? The Rademacher complexity is over any $\\sigma_i$, so its sign has nothing to do with the true labels. I don\u2019t see how the developed loss would encourage to minimise this quantity. It\u2019s also a worst-case bound in terms of the hypothesis $h$, so again I don\u2019t see how that could be minimised. I hope the authors can elaborate on this connection.\n2. The CIFAR10-C results look strong but only naive ERM is provided as a baseline. How does the approach fair against more specialised algorithms. I don\u2019t expect this novel approach to be state-of-the-art but it would be nice to know where it stands among more modern algorithms.\n3. While the authors do compare against [1], I think the paper would benefit from a more in-depth comparison of the two losses. I\u2019m also a bit confused as to why the results of [1] are not reported in Table 1 but only in a separate ablation in Table 2. Could the authors clarify this?\n\n\\\n\\\n[1] Yao et al, Pcl: Proxy-based contrastive learning for domain generalization"
            },
            "questions": {
                "value": "1. I think Equation (5) has some typos, shouldn\u2019t the embedding $z_i$ also depend on the environment $e$, i.e. $z_i^e$? If one interprets this equation \u201cliterally\u201d you would be summing over the same $z_i$ over and over again."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1657/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699089379667,
        "cdate": 1699089379667,
        "tmdate": 1699636093419,
        "mdate": 1699636093419,
        "license": "CC BY 4.0",
        "version": 2
    }
]