[
    {
        "id": "122jHxwB4W",
        "forum": "ONPECq0Rk7",
        "replyto": "ONPECq0Rk7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2843/Reviewer_QSPQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2843/Reviewer_QSPQ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a simple method for pre-training LMs with contrastive loss. Instead of evaluating logits for all vocabulary, only embeddings from the batch are used. Such an approach allowed us to pre-train LMs more effectively by reducing memory constraints. Surprisingly, this approach also performed well for autoregressive language modeling. For both autoregressive and masked language modeling, the proposed method showed improved results on downstream tasks, as well as improved training efficiency with respect to training hours."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method is straightforward yet shows promising results. \nPerformance on down-stream tasks is marginally improved with respect to vanilla models\nPre-training is more efficient with respect to training time and the total number of tokens\nThe paper is easy to follow, and the motivation is clear."
            },
            "weaknesses": {
                "value": "I would like to see more insights from the paper. It shows that HLMs provide better efficiency and down-stream performance, while such results are slightly contr-intuitive. More analysis could make the paper better. I.e., it would be beneficial to answer the question, \"what makes HLMs special, and why it outperforms vanilla LMs?\"."
            },
            "questions": {
                "value": "Please, refer to the Weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2843/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698755000459,
        "cdate": 1698755000459,
        "tmdate": 1699636228098,
        "mdate": 1699636228098,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6O61GTdSgH",
        "forum": "ONPECq0Rk7",
        "replyto": "ONPECq0Rk7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2843/Reviewer_CHGU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2843/Reviewer_CHGU"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to replace the cross-entropy minimization by a new pre-training objective in language models: Contrastive Weight Tying (CWT). It consists in contrastively predicting the representation of the missing word. Instead of projecting the representation output by the model into the vocabulary, and using a softmax, the goal is to predict directly this static embedding of the reference word, which the model is trained for by (1) using a contrastive objective with the rest of the batch as negative examples (2) using input static embeddings as reference output embeddings, which is traditionally referred as *weight tying*. After a presentation of previous work, the paper introduces the method, consisting in weight tying, the contrastive objective with no projection, and how to adapt it for text generation, plus computational requirements. Experiments are performed on a Monolingual Encoder (GLUE), Monolingual Decoder (LAMBADA/LM Evaluation Harness), and Multilingual Encoder (XNLI Benchmark), with CWT showing improvements on all settings compared to the model trained classically. Lastly, the paper presents experiments on vocabulary size, as expanding the vocabulary is less costly and may provides some advantages with CWT."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper presents a simple and well motivated method for pre-training a language model, CWT.\n- The paper implements various experiments showing that CWT consistently improves performance, training efficiency, or both, over the classical version of the model."
            },
            "weaknesses": {
                "value": "The main issue I found with this paper is a huge lack of bibliographic work. It mainly keeps the paper from proper contextualization but I believe a proper related work would have greatly helped in making the experiments less shallow. \n- The lack of reference to relevant previous work keeps the paper from being well-positioned within the literature, especially when it comes to comparison with the numerous pre-existing similar methods,\n- It also keeps it from investigating relevant experimental settings - for example, comparison with similar objectives and ablation studies which could shed some light on what makes CWT work better than some previous methods."
            },
            "questions": {
                "value": "- On the missing related work: I will detail what I estimate to be essential. I believe you could find most of these references (and many other) in papers you have yourself cited (the papers describing ELECTRA and ELECTRIC, if only).\n    - You write \"This work stands as the first attempt to train language models using an explicit contrastive loss\".  The first that I am aware of would be [1], but there are many more recent references, especially those making use of Noise Contrastive Estimation; with [2] which, while binary in nature, has been adapted into a loss resembling your own in [3], though I believe the first use of a direct softmax-like objective with negative examples coming from the batch is from [4].\n    - Though I believe input/output embeddings weight sharing was a common approach that was very certainly implemented in most early languages models prior to 2017, your work combines this approach with a contrastive objective in order to get rid of the output vocabulary, which is a very nice idea. On that matter, I will mainly cite [5], a dedicated loss aiming at predicting static embeddings too; another paper [6] studies how to choose the target embeddings (which you use weight tying for).     \n    - Language modelling without an output vocabulary (contrastive objective or not) has been explored quite a lot in the past, with the specific problem you address in subsection 3.3 with fine-tuning having had many proposed solutions. I believe a nearest neighbour approach has been used in [5]\u00a0and [6], but [7] proposes an energy-based model to repurpose a well-trained encoder (outputting *embeddings*) into a generative model.\n    - I believe you will find in this previous work many insights which will help you improve the design of your experiments and directions for improvement.\n\n- On experiments:\n    - Do you have any hypothesis regarding the better performance of CWT than the vanilla model *with the same number of training token* on most GLUE tasks ? \n    - In fine-tuning a causal LM for text generation, is the supplementary amount of compute received by the fine-tuned model compared to the vanilla model significant ? \n\n\n\n\n- [1] Quick Training of Probabilistic Neural Nets by Importance Sampling (Bengio and S\u00e9nacal, 2003)\n- [2] A fast and simple algorithm for training neural probabilistic language models (Mnih and Teh, 2013)\n- [3] Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency (Ma and Collins, 2018)\n- [4] On Using Very Large Target Vocabulary for Neural Machine Translation (Jean et al, 2014)\n- [5] Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs (Kumar and Tsekov, 2019)\n- [6] On Target Representation in Continuous-output Neural Machine Translation (Tokarchuk and Niculae, 2022).\n- [7] Residual Energy-Based Models for Text Generation (Deng et al, 2019)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2843/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2843/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2843/Reviewer_CHGU"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2843/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698795734993,
        "cdate": 1698795734993,
        "tmdate": 1700668248659,
        "mdate": 1700668248659,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Z4T376hwqH",
        "forum": "ONPECq0Rk7",
        "replyto": "ONPECq0Rk7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2843/Reviewer_ccZo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2843/Reviewer_ccZo"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes replacing the standard cross-entropy-based language modeling objective (classification over token vocabulary) with a \"contrastive weight tying\" objective, which is more memory and compute efficient during training. Computing the full cross-entropy loss requires loading and multiplying the final model embeddings by a $d_{model} \\times V$ matrix, where $V$ is the vocabulary size (often $\\geq 30k$), which can be quite expensive (especially for small models, with few layers and $d_{model} \\ll V$). Instead, this paper proposes using a contrastive objective, where the targets are the input token embeddings for the masked tokens (\"weight tying\"), and the negative samples are taken from the other \"masked\" tokens. It shows in experiments that the proposed method is generally more efficient, and attains better performance, than the standard cross-entropy-based training. Lastly, to allow for generating text (using next token prediction) with these \"headless\" models, the paper proposes performing fine-tuning with a LM head for a small number of tokens ($< 2$% of pre-training dataset)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed method can reduce the number of trainable parameters meaningfully for small models with large vocabulary sizes (e.g., by my calculations, reduce # parameters of BERT-base by ~18%).\n- The proposed method generally yields better accuracy on downstream tasks than standard LM training (across mono-lingual encoder experiments with BERT-base, mono-lingual decoder experiments with Pythia-70m, and multi-lingual encoder experiments with distilled multi-lingual BERT).\n- This method decouples the training speed from the number of tokens in the vocabulary, which allows for choosing the vocabulary size that attains best performance."
            },
            "weaknesses": {
                "value": "- The proposed method is not very novel --- straightforward application of contrastive loss function to LM training.\n- The experiments are relatively small-scale, only considering models < 140m parameters. Unclear if any of the benefits of this method hold for larger models.\n- The paper does not discuss that for larger models, the $d_{model} \\times V$ classification matrix corresponds to a tiny percentage of the total number of model parameters (and thus, total memory and computation during training). For example, it corresponds to roughly 2.5% and 0.4% of the Llama2 (7b) and Llama2 (70b) model parameters, respectively.\n- The proposed method adds a step to decoder-only LM training, as the model must first be pre-trained in a \"headless\" way, and then fine-tuned with a LM classification head. This adds complexity to the training process.\n- The paper does not perform careful ablations to help understand why the proposed method attains better model performance than standard cross-entropy (CE) LM training. For example, it could have considered weight-tying with the CE loss, or a contrastive loss without weight tying. This latter option could still be implemented in a compute-efficient way during training (since only a subset of columns of the classification matrix would need to be used). It could have also tried using different numbers of negative samples for the contrastive loss function (assuming # negative samples was decoupled from # masked samples), such that on one extreme (# neg samples = vocab size) the loss would be equivalent to the cross-entropy loss function (modulo the weight tying). Overall, I was confused by why the proposed method would yield better-performing models than cross-entropy.\n\nNote that I was torn on whether to assign this paper \"marginal accept\" or \"marginal reject\", but chose marginal accept given that I see the efficiency benefit of this approach for smaller models, and was intrigued that this approach could improve model quality. The reasons I was considering marginal reject were that I felt the method was not very novel, only leads to meaningful efficiency gains for small models (and no experiments with medium/large models were performed), and because I felt the paper didn't do a good job explaining why the proposed method performs better. Open to being swayed."
            },
            "questions": {
                "value": "- Am I understanding correctly that for the monolingual decoder experiments, the proposed method offers no speed advantages, because  \"negative samples correspond to every input embedding at a different position in the batch\"? Would it make sense to decouple the number of negative samples from the number of \"masked\" samples, to allow for controlling these two hyperparameters separately?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2843/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815541636,
        "cdate": 1698815541636,
        "tmdate": 1699636227835,
        "mdate": 1699636227835,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "X9ytLn66RI",
        "forum": "ONPECq0Rk7",
        "replyto": "ONPECq0Rk7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2843/Reviewer_A6os"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2843/Reviewer_A6os"
        ],
        "content": {
            "summary": {
                "value": "This study introduces an innovative approach to self-supervised pre-training of language models. Instead of predicting probability distributions over token vocabularies, the method focuses on reconstructing input embeddings contrastively using Contrastive Weight Tying (CWT). This approach is applied to pretrain Headless Language Models in both monolingual and multilingual contexts, offering practical advantages. It is significantly more compute-efficient, data-efficient, and performant than classical predictive methods. The paper's contributions include the introduction of a new pretraining objective, the pretraining of encoder and decoder models for English and a multilingual encoder model, demonstrating the benefits of headless training, and exploring the effects of pretraining hyperparameters on downstream performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper maintains a high standard of quality. The methodology is well-structured and explained clearly, making it accessible to readers. The empirical results demonstrate substantial reductions in training computational requirements, improved downstream performance, and increased data efficiency. The study includes a significant increase in GLUE score and LAMBADA accuracy, indicating a quality improvement over classical LMs.\n\n2. The paper is well-written and clear in its explanations. It effectively communicates the core concepts and methodologies involved in the proposed approach. The objectives and benefits of headless training are well-articulated, and the exploration of pretraining hyperparameters adds to the clarity of the study.\n\n3. The paper's significance lies in its departure from traditional language model pre-training by introducing a contrastive objective. Sufficient experiments validate the advantages of the proposed contrastive learning loss over traditional cross-entropy loss, and the experimental results are relatively convincing.\n\nIn summary, it is a valuable contribution to the field of self-supervised language model pre-training. The proposed approach has the potential to impact the efficiency and effectiveness of language models in various applications."
            },
            "weaknesses": {
                "value": "1. The proposed method is not very novel and original. Although it departs from traditional probability prediction, offering a unique perspective on language model pre-training, there is lots of similar work that leverages the similar contrastive learning method. Thus, the use of Contrastive Weight Tying (CWT) in this context almost combines the existing ideas into the pretraining model, and it is not a very novel and creative methodology;\n\n2. For the currently most popular CLMs, the proposed method still requires fine-tuning on a small amount of training data; otherwise, the performance would be significantly worse than the vanilla approach, adding complexity to the training process;\n\n3. With the current trend of scaling up language models, it would be even more valuable to validate the proposed method on larger generative LLMs, such as ChatGLM or LLaMa."
            },
            "questions": {
                "value": "1. I'm curious about the experimental results in Table 2. I wonder why there is a significant improvement on all test sets except the CB test set. Why does the Headless model perform so much worse than the vanilla model on the CB test set? Could the authors provide a detailed analysis and explanation for this? Is there anything special about the CB test set?\n\n2. In Table 3, why is there no output for the Headless model's perplexity (ppl) on the validation set?\n\n3. I believe it's reasonable that the Headless model is more memory-efficient and exhibits higher training and inference efficiency on the XNLI benchmark compared to the Vanilla model. However, why is there such a significant improvement in quality as well? Where do these quality improvements primarily come from? Could the authors provide a more specific explanation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2843/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698835305262,
        "cdate": 1698835305262,
        "tmdate": 1699636227766,
        "mdate": 1699636227766,
        "license": "CC BY 4.0",
        "version": 2
    }
]