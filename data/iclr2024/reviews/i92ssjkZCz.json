[
    {
        "id": "Q2LYDUb1ma",
        "forum": "i92ssjkZCz",
        "replyto": "i92ssjkZCz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission548/Reviewer_Cmb1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission548/Reviewer_Cmb1"
        ],
        "content": {
            "summary": {
                "value": "This paper studies pre-training for 3D perception in the context of autonomous driving. They present UniPAD, a novel self-supervised learning paradigm applying 3D volumetric differentiable rendering. UniPAD implicitly encodes 3D space, facilitating the reconstruction of continuous 3D shape structures and the intricate appearance characteristics of their 2D projections. The flexibility of their method enables seamless integration into both 2D and 3D frameworks, enabling a more holistic comprehension of the scenes. They manifest the feasibility\nand effectiveness of UniPAD by conducting extensive experiments on various downstream 3D tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper is the first paper that leverages volumetric differentiable rendering to resolve the perception pre-training problem.\n\n- Their method unifies the multi-view image representation and the LiDAR point cloud representation into a volumetric space.\n\n- Their method outperforms the existing baseline approaches on the benchmarks, which demonstrates the effectiveness of their proposed method.\n\n- The paper writing is clear and easy to follow."
            },
            "weaknesses": {
                "value": "- The paper title should be **A Universal Pre-training Paradigm for 3D Perception**, instead of **A Universal Pre-training Paradigm for Autonomous Driving**, as this paper mainly focuses on pre-training for perception tasks rather than all driving tasks including prediction and planning.\n\n- Table 8 (a): the detection performance wrt. masking ratio didn't change much when the masking ratio ranged from 0.1 to 0.7. A small masking ratio leads to little information loss, but this pre-training strategy still works well compared to other methods. Hence it is uncertain whether this performance improvement really comes from the masking-and-completion paradigm or other tricks.\n\n- Instead of highlighting the SoTA performances, the authors need to leave more space for the comparison with other pre-training methods (Table 3 and Table 4) and ensure a fair comparison when re-implementing those baselines."
            },
            "questions": {
                "value": "- In depth-ware sampling, LiDAR point clouds are required, so this strategy cannot be applied to only multi-view images. Also, it is not clear the effects of point cloud masking on this ray-sampling strategy."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission548/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission548/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission548/Reviewer_Cmb1"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission548/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698224316194,
        "cdate": 1698224316194,
        "tmdate": 1699635981980,
        "mdate": 1699635981980,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u5Uf2RNxcA",
        "forum": "i92ssjkZCz",
        "replyto": "i92ssjkZCz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission548/Reviewer_2nXX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission548/Reviewer_2nXX"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a 3D representation pre-taining framework that makes use of 3D volumetric differentiable rendering decoder.\nThis technique is not contrastive, and allows for continuous reconstruction.\nIt also is easily adaptable to many modalities, e.g. 3D LiDAR point clouds, and multi-image 2D views.\n\nFor point clouds, the corresponding encoders would be VoxelNet, while images are convolutional.\nThese modality specific features are transformed into a voxel space, followed by a projection layer to enhance voxel features.\nThese voxel features then undergo block-wise masking. That is, region removal for point-clouds and sparse convolution for images.\n\nUniPAD then convert these modalities to a unified 3D volumetric representation to preserve as much of the original information as possible.\nThe model then introduce a novel use of neural rendering to flexibly incorporate geometry or textural cues into the learned voxel representation. \nUniPAD uses Signed Distance Function (SDF) to integrate color and geometry (sampled depth along the ray) features. \n\nTo alleviate computational rendering requirement, the model applies memory-efficient ray sampling (dilated, random, and depth aware from LiDAR information). \nThis optimizes precision of neural rendering by concentrating on most relevant segments within scenes.\n\nOverall the model is trained on color loss and depth loss.\n\nUniPAD is tested on NuScenes against a few STOA techniques.\nIt is able to improve the baseline on 3D object detection and semantic segmentation when compared to different image or point cloud pre-training techniques, transform strategies, and input modalities.\nIn the ablation study, it is shown that the depth-aware sampling is the most effective, and that the projection layer is critical."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The overall approach of using rendering to minimize discrepancy between rendered projection and the input on self-driving is novel."
            },
            "weaknesses": {
                "value": "More grounded discussions on what is (which part of the scenes) actually better represented would help.\nIs it working better on parts of the scenes that are much more intricate, or is it a general overall improvement in accuracy."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission548/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806728680,
        "cdate": 1698806728680,
        "tmdate": 1699635981915,
        "mdate": 1699635981915,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fGAgrQtOmA",
        "forum": "i92ssjkZCz",
        "replyto": "i92ssjkZCz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission548/Reviewer_NRvC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission548/Reviewer_NRvC"
        ],
        "content": {
            "summary": {
                "value": "**Overview:** UniPAD provides a self-supervised training framework for autonomous driving neural networks. It requires a dataset containing camera images, depths, and/or lidar data.\n\n**Method:** Images and/or pointclouds are block-wise masked, then passed through a feature extractor, and then projected/mapped into a unified voxel grid space. A neural network renderer then renders the voxel grid features into rgb and depth images. The renderer works by marching along the ray for each pixel, while trilinearly interpolating the voxel grid features at each step, and passing them to a SDF MLP and an RGB MLP. The SDF is used to decide the opacity value for the rendering equation. Finally, an L1 loss is applied on sampled rays from the rendered RGB and the rendered depth.\n\n**Results:**  The authors show that on nuscenes 3D object detection and 3D semantic segmentation, pre-training with UniPAD noticeably improves mAP and NDS metrics and surpasses state-of-the-art numbers. They also show that UniPAD outperforms several other forms of pretraining, and that it consistently improves results regardless of the view transform, input modalities, and backbone size. They also show in their ablation studies how mask ratio, decoder depth, decoder width, choice of renderer, sampling strategy, projection, and using pre-trained components impacts UniPAD's performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The overall idea of masking the input, as well as having some sort of a \"rendering loss\" in 2D sounds reasonable\n2. Pretraining with UniPAD does seem to show a clear improvement in NDS and mAP numbers as shown in table 1\n3. The existing experiments and ablations (tables 3 through 8) were good, e.g. it was interesting to see decoder depth matters more than width\n4. The paper was overall easy to read"
            },
            "weaknesses": {
                "value": "**Experiments:**\n1. There is no qualitative results/visualizations\n2. Related to above, would be great if the authors share insights from qualitative evaluations: Does the model behave differently, i.e. are the error modes different as a result of pre-training with UniPAD (e.g. having less mis-predicted blobs, flickering less, etc.)? \n3. It seems to me that there are 2 components to UniPAD: Masking (i.e. asking the model to fill in information) and rendering (i.e. a loss that is aware of projection). It would be great if there are some ablations separating these out. One example could be: In table 3, how does masking/not masking applied to 3D detector impact results? This would demonstrate how much of the gain is from masking, and how much masking+3d supervision leaves behind compared to rendered supervision, which I assume is considerably slower\n4. Why are the baseline UVTR and UniPAD NDS/mAP reported in tables 3 and 4 so different compared to table 1?\n\n**Presentation**\n\n5. It's unclear to me what layers are added to the network for pre-training, and later thrown away. A diagram can help with this\n6. It's unclear to me whether pre-training is done just for the 2D backbone/3D pointcloud processor, or also the 3D voxel processor\n7. Figure 3: I think masked inputs need to be shown so that it's reader knows where the model is trying to fill-in information. Also, the rendered depth and GT pointcloud can't be compared by the reader since they're visualized differently.\n8. There's no mention of how much this impacts training speed"
            },
            "questions": {
                "value": "1. In table 8(f), what is meant by \"without projection\"? Does it mean not applying the convolutions post-voxelization? I found the wording confusing\n2. Do you have an intuition on why you would want to train on 2D renderings, if you have 3D labels?\n3. Related to above: Do you expect to not get the same performance if you just supervise on the SDF and avoid the ray marching step?\n4. Is there any reason for not applying the masking after post-projection in voxel grid space (and before the conv layers)? I'm thinking this would impose that the mask is consistent across views"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission548/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699165577194,
        "cdate": 1699165577194,
        "tmdate": 1699635981835,
        "mdate": 1699635981835,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UPABSwYaAA",
        "forum": "i92ssjkZCz",
        "replyto": "i92ssjkZCz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission548/Reviewer_k4Vb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission548/Reviewer_k4Vb"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a self-supervised learning approach for 3D object detection and 3D semantic segmentation on LiDAR point clouds and multi-view images. The main idea is to i) first mask out certain voxelized LiDAR point clouds or multi-view images, and ii) reconstruct the masked regions for representation learning. \n\nThe mask encoder adopts block-wise masking to obtain masked inputs, where the masked points and pixels are removed or ignored during SparseConv processing. To establish a unified representation for both point clouds and images, the authors propose to convert both modalities to the 3D volumetric space.\n\nTo embed the geometrical information in the voxel fields, the authors resort to neural rendering, where a scene is represented as an implicit signed distance function (SDF) symbolizing the 3D distance between a query point and its nearest surface. The major implementations of such an approach were inspired by [R1], while concern about memory consumption was raised since the objective of the above SDF is to render a driving scene of a relatively large range. To reduce memory consumption, three ray sampling strategies are proposed, which conduct dilation, random, and depth-aware samplings, respectively. \n\nThe authors conduct some experiments on the nuScenes dataset for self-supervised with point clouds, multi-view images, and both. The experiments show that the proposed approach can bring certain degrees of improvement over the randomly initialized baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- A self-supervised learning approach that is capable of handling both point clouds and multi-view images. \n- A discussion on the ray sampling for reducing memory consumption.\n- Benchmarking results on two 3D perception tasks under three sensor setups. Consistent improvements over baselines."
            },
            "weaknesses": {
                "value": "- Limited novelty of UniPAD. While many engineering efforts made, most of the ground of the proposed UniPAD approach stems from [R1].\n- Relatively outdated baselines. The chosen baselines (UVTR, FCOS3D, and SpUNet) from three sensor setups are from previous literature; the effectiveness of UniPAD on top of stronger baselines remains unknown.\n- Missing comparisons with current arts. Approaches benchmarked do not include the state of the arts."
            },
            "questions": {
                "value": "- **Q1:** The baselines adopted, including UVTR for LiDAR-based 3D object detection, FCOS3D for multi-camera 3D object detection, and SpUNet for 3D semantic segmentation, are a bit behind the current trend of each subject. Thus, the improvements of UniPAD over these baselines only bring merits of certain degrees. The authors are recommended to include some studies on newer baseline models.\n\n- **Q2:** There are some missing comparisons with current arts in Table 1 and Table 2. For example, the reported BEVFormer [R2] is its *small version* of an NDS of 44.8, while the current open-sourced art BEVFormer V2 [R3] has an NDS of 55.3. Similarly, the authors chose to report the PETR [R4] of an NDS of 44.2, while its best open-sourced model [R5] has an NDS of 50.3. For 3D semantic segmentation, the comparisons with current arts are missing, such as [R6], [R7], and [R8].\n\n- **Q3:** The results shown in Table 8a lacks a detailed analysis. The masking ratios of 0.1 and 0.7 almost brought the same effect during self-supervised learning, which is counter-intuitive to the motivation behind the masking-based UniPAD.\n\n- **Q4:** Table 8e does not include a comparison of the memory usage and computational costs during pretraining. The authors are suggested to check their elaboration and include this study, since the motivation of UniPAD is grounded by such a claim.\n\n- **Q5:** What is the intuition of conducting the backbone and decoder scaling experiments as in Table 7, Table 8b, and Table 8c? The improvements brought by involving more trainable parameters seem perpendicular to the objective self-supervised learning. As mentioned in Q1, the authors are recommended to include some studies on newer baseline models.\n\n- **Q6:** As commonly done in image-based self-supervised methods, the authors are recommended to conduct some experiments to validate i) the performance under the linear proving setting and ii) the out-of-training-distribution robustness of the pretrained models.\n\n**References:**\n- [R1]  P. Wang, et al. \u201cNeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-View Reconstruction,\u201d NeurIPS 2021.\n- [R2] Z. Li, et al. \u201cBEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers,\u201d ECCV 2022.\n- [R3] C. Yang, et al. \u201cBEVFormer v2: Adapting Modern Image Backbones to\nBird\u2019s-Eye-View Recognition via Perspective Supervision,\u201d CVPR 2023.\n- [R4] Y. Liu, et al. \u201cPETR: Position Embedding Transformation for Multi-View 3D Object Detection,\u201d ECCV 2022.\n- [R5] Y. Liu, et al. \u201cPETRv2: A Unified Framework for 3D Perception from Multi-Camera Images,\u201d ICCV 2023.\n- [R6] L. Kong, et al. \u201cRethinking Range View Representation for LiDAR Segmentation,\u201d ICCV 2023.\n- [R7] G. Puy, et al. \u201cUsing a Waffle Iron for Automotive Point Cloud Semantic Segmentation,\u201d ICCV 2023.\n- [R8] Y. Liu, et al. \u201cUniSeg: A Unified Multi-Modal LiDAR Segmentation Network and the OpenPCSeg Codebase,\u201d ICCV 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No or only a minor concern related to the ethics."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission548/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission548/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission548/Reviewer_k4Vb"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission548/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699359207504,
        "cdate": 1699359207504,
        "tmdate": 1699646037923,
        "mdate": 1699646037923,
        "license": "CC BY 4.0",
        "version": 2
    }
]