[
    {
        "id": "i3mh9rqDnw",
        "forum": "gwDuW7Ok5f",
        "replyto": "gwDuW7Ok5f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3133/Reviewer_X7AW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3133/Reviewer_X7AW"
        ],
        "content": {
            "summary": {
                "value": "To address the domain gap between low-quality and high-quality images and improve the performance of face restoration, the paper introduces a novel framework called DAEFR. This framework incorporates LQ (low-quality) image domain information by introducing an auxiliary branch that extracts unique LQ domain-specific features to complement the HQ (high-quality) domain information. To further align the features between the HQ and LQ domains, the paper employs a CLIP-like constraint to enhance the correlation between the two domains. Additionally, to facilitate better feature fusion between these two domains, the framework introduces a multihead cross-attention module. Evaluation results demonstrate the effectiveness of DAEFR."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe paper proposes a framework designed to incorporate distinctive features from low-quality (LQ) images, thereby enhancing the face restoration task.\n2.\tTo mitigate the domain gap between HQ and LQ images, the paper proposes an association strategy during training, and incorporates a multihead cross-attention module for better feature fusion between these two domains.\n3.\tThe experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed framework."
            },
            "weaknesses": {
                "value": "1.\tThe full name of the proposed framework, DAEFR, is missing. It should be mentioned on its first occurrence in the paper.\n2.\tThe proposed method requires training two sets of encoder and decoder for both HQ and LQ images. This will double the training resource requirements.\n3.\tI think it will be better if there is more elaboration on the domain gap issue that the current works exist, i.e., the motivation of the paper. Currently, it is not intuitive from figure 1 and from current discussion.\n4.\tCheck the spellings. For example, \u201crecently\u201d on the beginning of second paragraph in the \u201cVector Quantized Codebook Prior\u201d of the related work.\n5.\tThere are some confusions about the training process of the network. In the first stage (section 3.1), you firstly train the two autoencoders of LQ and HQ using the codebook loss. After the first-stage training is complete, you train the two encoders using both the codebook loss and the association loss. Why not combine the two stages into one, or just apply the association loss in stage 2? Besides, in stage 3, you state in the Training Objectives that the MHCA and transformer module are trained in this stage. However, from figure 2(c), the two encoders seem not to be frozen during stage 3.\n6.\tThe results in Table 1 indicate that the proposed method does not significantly outperform other methods, especially for the synthetic CelebA-Test dataset."
            },
            "questions": {
                "value": "Refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3133/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719560760,
        "cdate": 1698719560760,
        "tmdate": 1699636260210,
        "mdate": 1699636260210,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2RDZ0toPqb",
        "forum": "gwDuW7Ok5f",
        "replyto": "gwDuW7Ok5f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3133/Reviewer_cKwn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3133/Reviewer_cKwn"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a framework, named dual associated encoder for face restoration (DAEFR), for face restoration. Specifically, different from the existing codebook based methods using only one autoencoder for high-resolution images, the authors propose to add another stream for low-resolution images. To fuse and align the features from both low and high resolution images, an association stage is designed. The associated features then will be extracted and utilized for face restoration.\n\nExperimental results have demonstrated the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well written.\n2. The idea is well presented, explained, and demonstrated.\n3. The proposed method may inspire the researchers in this area."
            },
            "weaknesses": {
                "value": "1. The contribution looks marginal to me since all the methods used in different stage are well designed and demonstrated. Adding another stream for low-resolution might not be a major contribution for a top-tier venue like ICLR.\n2. I got some questions for the experimental results which can be seen in the questions part."
            },
            "questions": {
                "value": "In Table 2, it seems like all the alter methods outperform the proposed method in terms of LPIPS. Please give discussions or visualizations to explain why this happens."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3133/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698877241147,
        "cdate": 1698877241147,
        "tmdate": 1699636260118,
        "mdate": 1699636260118,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HeqKDCFvhT",
        "forum": "gwDuW7Ok5f",
        "replyto": "gwDuW7Ok5f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3133/Reviewer_qW23"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3133/Reviewer_qW23"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a dual-branch framework, named DAEFR, designed for the restoration of high-quality (HQ) facial details from low-quality (LQ) images. Within this framework, an auxiliary LQ encoder and an HQ encoder are employed in conjunction with feature association techniques to capture visual characteristics from LQ images. Subsequently, the features extracted from both encoders are combined to enhance their quality. Finally, the HQ decoder is utilized for the reconstruction of high-quality images. The effectiveness of DAEFR is evaluated using both real-world and synthetic datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The notion of incorporating an additional encoder with weight sharing is intriguing.\n\n2. The authors have extensively verified the significance of each component via thorough ablation studies.\n\n3. This approach adeptly addresses various common and severe degradations and maintains a high standard of writing quality."
            },
            "weaknesses": {
                "value": "1. Can you provide a detailed explanation of the primary differentiation between DAEFR and CodeFormer?\n\n2. The paper does not delve into its limitations or potential factors for analysis, which would greatly enrich its discussion.\n\n3. The paper outperforms baseline methods in the downstream face recognition task. Could you provide a comprehensive explanation of these results?\n\n4. The paper does not provide any suggestions or insights into potential avenues for future research or improvements to the proposed method."
            },
            "questions": {
                "value": "Please discuss the concerns in the Weaknesses Section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3133/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698958333067,
        "cdate": 1698958333067,
        "tmdate": 1699636260019,
        "mdate": 1699636260019,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yCTBYYuYoK",
        "forum": "gwDuW7Ok5f",
        "replyto": "gwDuW7Ok5f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3133/Reviewer_huYM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3133/Reviewer_huYM"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new approach called the Dual Associated Encoder for facial restoration. In this method, an auxiliary Low-Quality (LQ) branch is introduced to extract vital information from LQ inputs. Subsequently, it employs a structure similar to CLIP to establish connections between the LQ and High-Quality (HQ) encoders. This connection aims to reduce the domain gap and information loss when restoring HQ images from LQ inputs. The experimental outcomes illustrate the highly promising performance of this novel approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe paper offers a coherent and well-founded justification for the research, with a method design that closely aligns with the research objectives.\n2.\tThe paper effectively communicates the method, ensuring readers can easily comprehend the underlying concepts and techniques.\n3.\tThe experimental results showcase remarkable performance, affirming the method's efficacy in tackling the face restoration challenge."
            },
            "weaknesses": {
                "value": "1.\tAbsence of Future Research Guidance: The paper does not offer any recommendations or insights into potential future research directions or enhancements for the proposed method.\n2.\tOmission of Limitation Discourse: The paper lacks a discussion regarding its limitations and possible factors for analysis."
            },
            "questions": {
                "value": "1.\tWhile the paper predominantly highlights the advantages of the proposed method, could you offer instances where the method encountered shortcomings or limitations?\n2.\tCould you elaborate on the key distinction between DAEFR and CodeFormer?\n3.\tCan you provide further experimental details into the \"Effectiveness of Low-Quality Feature from Auxiliary Branch\" as examined in your ablation studies?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Please see my comments above."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3133/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698968448122,
        "cdate": 1698968448122,
        "tmdate": 1699636259947,
        "mdate": 1699636259947,
        "license": "CC BY 4.0",
        "version": 2
    }
]