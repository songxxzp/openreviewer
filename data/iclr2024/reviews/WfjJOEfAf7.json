[
    {
        "id": "fe0xTw4jIZ",
        "forum": "WfjJOEfAf7",
        "replyto": "WfjJOEfAf7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1221/Reviewer_j36B"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1221/Reviewer_j36B"
        ],
        "content": {
            "summary": {
                "value": "This paper aims at providing better understanding for existing methods (contrastive and feature decorrelation based methods) by leveraging the principles of matrix mutual information and joint entropy. In addition, the paper proposes the \"matrix variational masked auto-encoder\" (M-MAE) method. The paper reports empircal results that show the effectiveness of M-MAE compared with the state-of-the-art methods for representation learning on ImageNet."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper tackles important questions in the field of self-supervised learning"
            },
            "weaknesses": {
                "value": "- The proofs are not always clear/complete (see questions below).\n- There are propositions and theorems in the paper. Then in the appendix, there are only proofs to \"theorems\" and the reader must match correct theorem/proposition with the correct proof.\n- The experimental setup lacks many details."
            },
            "questions": {
                "value": "- Theorems 1 and 2 do not seem to be proved clearly. The proof to theorem 1 in the appendix (the one that includes \"lemma 1\"!) does not seem to proof things by following a clear mathematical reasoning. Can you clarify how the last sentences leads to a valid proof? It it the same for Theorem 2 (the proof on page 15 given the other one seems to refer to Proposition 2).\n- Can you provide additional technical details about the experimental setup. The appendix does not seem to contain any information related to that and only limited information is given in the main paper (what are exactly the training objectives and hyper-parameters such as batch size, etc.).\n- It is not fully clear how Theorems 1 and 2 are not going against each other given Definition 2. Can you provide some information about that?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1221/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1221/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1221/Reviewer_j36B"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1221/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698669834133,
        "cdate": 1698669834133,
        "tmdate": 1699636048564,
        "mdate": 1699636048564,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8nUWxxuQkw",
        "forum": "WfjJOEfAf7",
        "replyto": "WfjJOEfAf7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1221/Reviewer_qQv9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1221/Reviewer_qQv9"
        ],
        "content": {
            "summary": {
                "value": "This paper shows that the optimal point of BarlowTwins and Spectral Contrastive learning objective functions satisfy the maximal matrix mutual information and the maximal matrix joint entropy. Then, this paper proposes a \"matrix variational masked auto-encoder (M-MAE) loss\" that is a combination of the original loss and the total coding rate, defined as $\\log det (\\mu I + ZZ^T)$, where $\\mu$ is a hyperparameter. Experimental results show that compared to MAE and U-MAE, the proposed M-MAE performs better in terms of linear probing and fine-tuning when the hyperparameter $\\mu$ is well-tuned."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper attempts to understand self-supervised learning methods in terms of the mutual information maximization framework, where each random variable is from the online encoder and the target encoder. It could be a somewhat valuable attempt to understand how self-supervised learning methods work."
            },
            "weaknesses": {
                "value": "1. There is no detailed discussion between the introduced matrix entropy (definition 1) and the original Shannon's information entropy. In fact, The Renyi entropy is defined by a special matrix family named density matrix. It is not defined for an arbitrary matrix, but only for a Gram matrix. However, this paper misuses the concept of matrix entropy throughout the whole paper. For example, In page 14, the last paragraph says that \"Take K1 = Z1 and K2 = Z2, the results follow similarly\". However, K1 and K2 should be Gram matrices, while Z1 and Z2 are feature matrices, that are not a Gram matrix. I think this paper should clarify the relationship between the matrix entropy (defined by a Gram matrix of the samples from a probability distribution) and the original Shannon's information entropy (defined by a random variable following a probability density function).\n2. There is no connection between Section 3 and 4. Note that the main difference between MAE and M-MAE is $TCR(Z)$. TCR is used for measuring a joint information quantity in Section 3, but in Section 4, this paper uses TCR for measuring information quantity for a single random variable. As the previous discussions are based on the relationship between Z1 and Z2, the newly introduced regularization term for MAE is irrelevant to the previous results.\n3. The proposed M-MAE is sensitive to the choice of the hyperparameter, as shown in Table 2. The gap between each $\\mu$ varies a lot, and it means that we need to access the original target labels to tune the hyperparemeter. It violates the spirit of self-supervised learning; we should not access the original labels\n4. The results in Section 3 are not generalizable to the generic self-supervised learning methods; these results are only applicable to Barlow twins and spectral contrastive learning. In fact, as mentioned in my previous comment, the proof is wrong for spectral contrastive learning because K1 and K2 should be a Gram matrix. In other words, the proof only works for a special case of self-supervised learning, where the objective function coincides with \"Proposition 1\" and K1 and K2 are Gram matrices of the feature matrices Z1 and Z2.\n5. There is no discussion of why the mutual information maximization (or joint entropy) of Z1, Z2 is a good measure of a good self-supervised learning method. As there is no connection between mutual information maximization and goodness of self-supervised learning methods, the motivation of M-MAE is somewhat weak. What is the benefit of making an MAE model maximize mutual information? (Note that, even more it is actually not about mutual information. See comment 2)\n6. The experimental results only show the comparisons between MAE, U-MAE and M-MAE. There are a lot of self-supervised learning methods. I think this paper needs more comparisons with other self-supervised learning methods (e.g., BalowTwins, MoCo, SimCLR, BYOL, DeepClustering, Swav, Data2Vec, DINO, iBot, SimMIM, ...) in terms of both information quantity and performance. I also think that it would be good for this paper to compare with other MAE variants (or MIM methods, such as SimMIM), but it could depend on the scope of this paper; as I think this paper needs a heavy non-trivial revision, as of now, I don't argue that M-MAE should be compared with other MAE variants, but I think additional comparisons with MAE variants will make the submission stronger. I recommend this survey paper to search more recent MAE variants: \"A Survey on Masked Autoencoder for Self-supervised Learning in Vision and Beyond\""
            },
            "questions": {
                "value": "Please check my previous comment. I think the current version of this paper will need a non-trivial heavy revision, including re-checking the major motivation (W2, W5), the mathematical notations and theoretical results (W1, W2, W4), adding more experiments (W6), fixing the fundamental flaw -- hyperparameter sensitivity -- of the proposed method (W3)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1221/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1221/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1221/Reviewer_qQv9"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1221/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678629882,
        "cdate": 1698678629882,
        "tmdate": 1700735615816,
        "mdate": 1700735615816,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IJXhQcPogI",
        "forum": "WfjJOEfAf7",
        "replyto": "WfjJOEfAf7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1221/Reviewer_cGgm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1221/Reviewer_cGgm"
        ],
        "content": {
            "summary": {
                "value": "The authors consider a theoretical framework to analyze and enhance self-supervised learning (SSL) methodologies utilizing matrix information theory. The work is particularly focused on providing a unified lens for examining both contrastive and feature decorrelation-based SSL paradigms through the application of matrix mutual information and joint entropy.\n\nSSL, a significant branch of unsupervised learning, leverages unlabeled data to learn representations by predicting certain input parts from others. The authors' investigation into the utility of matrix mutual information in SSL is notable. By extending mutual information to the matrix domain, the manuscript aims to elucidate the dependencies among various features or representations within SSL models, shedding light on the information propagation mechanisms within neural networks. Additionally, the manuscript's exploration of joint entropy in the context of SSL is insightful. Assessing how the uncertainty in the input data influences the learning process and the quality of the learned representations can be crucial for enhancing model robustness and efficiency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Theoretical Innovation**: The manuscript presents a novel theoretical framework for analyzing self-supervised learning (SSL) methods through the prism of matrix information theory. The use of matrix mutual information and joint entropy is an innovative approach that could provide new insights into the dependencies between features and the propagation of information within neural networks. This theoretical advancement has the potential to deepen our understanding of SSL mechanisms, making it a significant contribution to the field.\n\n**Unified Analysis for Diverse SSL Approaches**: By offering a unified analytical lens for both contrastive and feature decorrelation-based SSL paradigms, the paper bridges a gap in the current literature. This comprehensive approach allows for a more holistic understanding of SSL and its various implementations, enhancing the ability to compare and improve upon different methods within a common theoretical framework.\n\n**Potential for Enhanced Robustness and Efficiency**: The exploration of joint entropy in SSL models addresses the critical aspect of input data uncertainty. By theoretically examining how this uncertainty affects the learning process, the paper lays the groundwork for developing more robust and efficient SSL algorithms that can better handle real-world data variability."
            },
            "weaknesses": {
                "value": "**Scalability of Information-Theoretic Measures**: A potential weakness could be the lack of a clear discussion on the scalability of the proposed matrix mutual information and joint entropy measures. Calculating these metrics can be computationally intensive, especially for large-scale datasets and high-dimensional feature spaces typical in self-supervised learning. Any insights on the computational overheads is much appreciated"
            },
            "questions": {
                "value": "**Generalization to Diverse Architectures**: Your paper appears to focus on a specific class of self-supervised learning models. How generalizable is your matrix information-theoretic approach to other SSL architectures, such as transformer-based or recurrent neural networks? Can you provide empirical evidence or theoretical justification for the generalizability of your approach?\n\n**Robustness and Sensitivity Analysis**: How robust are your matrix information-theoretic measures to variations in SSL hyperparameters, such as temperature in contrastive learning or weight decay? Could you provide a sensitivity analysis that examines the stability of your proposed metrics under different hyperparameter settings?\n\nThese questions are intended to probe the empirical validation of theoretical insights, the generalizability of the approach to various architectures, and the robustness of the proposed metrics to hyperparameter variations. Addressing these points could significantly strengthen the paper's contributions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1221/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698932677248,
        "cdate": 1698932677248,
        "tmdate": 1699636048397,
        "mdate": 1699636048397,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "o4yV4lvD4F",
        "forum": "WfjJOEfAf7",
        "replyto": "WfjJOEfAf7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1221/Reviewer_znaA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1221/Reviewer_znaA"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses the information flow of three mainstream self-supervised learning methods: contrastive learning; feature de-correlation; and masked auto-encoding. It successfully connects all three methods with matrix information theory and thus offering a unified view. And beyond that it proposes to add an additional term to the original MAE loss. The loss regularizes the latent codes and is shown to be helpful on image classification tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper introduces matrix information theory to understand and connect mainstream methods in self-supervised learning, which is a very meaningful and valuable contribution.\n+ The writing is fairly clear, although I did not delve into the mathematical details, I believe they are sounds.\n+ The initial results on image classification (both the linear-proving and the fine-tuning results) are great."
            },
            "weaknesses": {
                "value": "- While the initial empirical results are great, I do hope to see the final results after having a complete run on MAE. The current version of the paper uses U-MAE's implementation and the hyper-parameters (e.g., batch size) do not follow the settings in MAE. This can cause some discrepancies. MAE's ViT-L, after convergence, can achieve an accuracy of ~85.5 on ImageNet. While the paper's result is promising, it is unclear the trend can still hold. So I would be curious to see. If it is too much of a computation burden, I am fine to see results on CIFAR-100.\n- There are some definitions used before they are defined (e.g., TCR is defined in the appendix). It would be great to at least point to them."
            },
            "questions": {
                "value": "* The TCR loss on the latent codes, how does it contribute to the entropy of the model? Is there a similar plot one can show as the training of M-MAE proceeds to Figure 1/2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1221/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699150464826,
        "cdate": 1699150464826,
        "tmdate": 1699636048314,
        "mdate": 1699636048314,
        "license": "CC BY 4.0",
        "version": 2
    }
]