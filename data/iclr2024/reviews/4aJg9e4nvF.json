[
    {
        "id": "ZONB9wGDLA",
        "forum": "4aJg9e4nvF",
        "replyto": "4aJg9e4nvF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7069/Reviewer_TawR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7069/Reviewer_TawR"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a framework for visualizing the features learned by vision transformers to gain insight into their internal mechanisms. It offers several key findings and conclusions, including the suitability of the linear projections in the feed-forward layers for visualization, the preservation of spatial information by ViTs, their enhanced utilization of background information compared to CNNs, and the observation that ViTs trained using CLIP learn visual concept features rather than object-specific features. The study conducts extensive experiments and visualizations to demonstrate the effectiveness of the proposed framework and to validate these findings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper is among the first few works that visualize the feature learned in vision transformers.\n- A substantial number of experiments have been conducted to offer a comprehensive visual analysis.\n- Some findings such as \u201cViTs  make better use of background information\u201d is interesting."
            },
            "weaknesses": {
                "value": "- The visualization method employed in this study lacks novelty.\n- Section 2.2 and Section 3 suffer from incomplete or missing references, and there is room for improvement in the overall writing quality.\n- The results presented in Table 1 and Section 4 fail to yield significant or novel insights. A noticeable performance disparity exists between \"Natural Accuracy\" and \"Isolating CLS.\" It would be expected that fine-tuning solely the linear classification head could close this performance gap, reinforcing the notion that the CLS token primarily aggregates information in later layers. It's worth mentioning that prior research has also indicated the viability of placing the CLS token in later layers (Touvron et al. 2021), and using global averages for ViTs (Liu et al. 2021). Therefore, these findings lack substantial significance.\n- While the findings in Section 5 and Table 2 are interesting, they are limited to evaluating only the basic ViT architecture.\n- Currently, the presented findings appear somewhat isolated. To enhance the paper's quality, it is advisable to provide a more in-depth analysis and insight into the interrelationship between these findings.\n\n\n1. Touvron, Hugo, et al. \"Going deeper with image transformers.\" ICCV 2021.\n2. Liu, Ze, et al. \"Swin transformer: Hierarchical vision transformer using shifted windows.\" ICCV 2021."
            },
            "questions": {
                "value": "- What does the term \u201cNormalized Top-5 ImageNet Accuracy\u201d mean?\n- The choice of the fifth layer in Section 6 is not well-justified and requires further explanation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7069/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698123215878,
        "cdate": 1698123215878,
        "tmdate": 1699636832768,
        "mdate": 1699636832768,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OOyzZEYZH9",
        "forum": "4aJg9e4nvF",
        "replyto": "4aJg9e4nvF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7069/Reviewer_Y5ru"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7069/Reviewer_Y5ru"
        ],
        "content": {
            "summary": {
                "value": "This paper compares the learning abilities of Vision Transformer vs CNN, explores if the learning abilities changes with the language guided vision transformers such as CLIP. The authors look into optimization based Vision Transformer visualization techniques for their analysis, using gradient steps to maximize feature activations, starting from random noise. The authors experiment to identify the most activating images for each feature and then forward these images to the network. The goal is to understand the presence of spatial information in different layers of the transformer. The authors claim that most of the spatial information in individual channels is lost in the last layer. They also find that ViTs consistently outperform CNNs when information, either foreground or background, is missing"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Finding that ViTs learn to preserve spatial information despite lacking the inductive bias of CNN\n2. Finding that the ViTs spatial information is lost in the last layer\n3. Authors look into text guided ViTs such as CLIP in a different way than existing work which I think is an important contribution that I see will be useful for the community in understanding future vision language models"
            },
            "weaknesses": {
                "value": "1. Section 2.1 last paragraph reference missing 'Related work ?'\n2. Section 3, line 4 reference missing 'augmentation ensembling ?'\n3. The authors claim that ViTs learn to preserve spatial information despite lacking the inductive bias of CNN but this property disappears  from the last layer. The author seems to be not sure why (section 4, page 5). This is a key finding of the paper that needs more theory and/or experiment based proof"
            },
            "questions": {
                "value": "1. Will you be able to show any experimentation and/or theoretical justification on why the last layer of ViTs loses the spatial information?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7069/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7069/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7069/Reviewer_Y5ru"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7069/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698435760945,
        "cdate": 1698435760945,
        "tmdate": 1699636832648,
        "mdate": 1699636832648,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UqxQYfKkZI",
        "forum": "4aJg9e4nvF",
        "replyto": "4aJg9e4nvF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7069/Reviewer_ESvq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7069/Reviewer_ESvq"
        ],
        "content": {
            "summary": {
                "value": "The paper visual the intermediate features learned by ViTs using an optimization-based approach. It then explores the differences between ViTs and CNNs on the learned features and find that ViTs tend to detect background features and depend less on texture information. Further they show that ViTs maintain spatial information in all layers except the last one. The visualization is also conducted on other variants including DeiT, CoaT, Swin and Twin etc."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "-\tVisualization features for ViTs is an important but largely neglected topic. This work presents some solid feature visualization results and may inspire the community on related research."
            },
            "weaknesses": {
                "value": "-\tThe novelty of the visualization method is limited. It mainly borrows the method of Olah et al. 2017 and adapt it on ViTs with some engineering tweaks.\n-\tSome observations of this work are not new. For example, the authors find that ViTs maintain spatial information in all layers except the last one, and the last layer produces very similar patch tokens. This behavior has been pointed out by some existing papers. Check \u201cDeepViT: Towards Deeper Vision Transformer 2021.\u201d It has shown that patch tokens of the last layer are almost the same, producing very similar output patch tokens. So it is not surprising that you can just use any of the patch tokens in the last layer to predict the result and achieve decent accuracy.\n-\tIn table2, multiple ViT models and CNN models are compared to show that the ViTs are better at using background information to predict correct classes. The issue here is that the used ViTs are more powerful than the CNNs with more parameters and more computations. ViTs have consistently better classification accuracies. The comparison is not fair and thus the conclusion of \u201cViTs better at using background information\u201d is not convincing."
            },
            "questions": {
                "value": "There are some missing citations.  Sec. 2.1: Recent work ? has studied. Sec. 3: and augmentation ensembling (?)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7069/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698631709528,
        "cdate": 1698631709528,
        "tmdate": 1699636832242,
        "mdate": 1699636832242,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "H1AIyFP0R1",
        "forum": "4aJg9e4nvF",
        "replyto": "4aJg9e4nvF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7069/Reviewer_f2Nw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7069/Reviewer_f2Nw"
        ],
        "content": {
            "summary": {
                "value": "In this work, authors  address  the visualizations on vision transformers. they observe build up of complex features across the hierarchy across layers, similar to CNNs. They also find that neurons in ViTs trained with language model supervision (e.g., CLIP) are activated by semantic concepts rather than visual features. They also show that transformers detect image background features, just like CNNs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This work addresses an important problem in deep learning which is understanding how visual transformers work, and shed light on these black boxes. This is certainly helpful to, in particular, the vision community.\n\nThe paper is generally well organized and well written. The prior research is also adequately mentioned.\n\nThe findings, although, not all being quite novel, are interesting. In particular, I find the finding that transformers make better use of background and foreground information, compared to CNNs, interesting. \"we find that transformers detect image background features, just like their convolutional counterparts, but their predictions depend far less on high-frequency information.\"\n\n\nThis works applies classic methods for visualizing CNNs to ViTs and is that sense is actually not very different from other approaches and results are somewhat expected and less surprising. The works emphasizes on similarities between CNNs and Transformers, in particular progressive specialization, rather than highlighting main differences deep inside the network, rather what information is being used."
            },
            "weaknesses": {
                "value": "The work still does not get into the meat of what transformers really do! For example, what key, query, and value do? and what makes them more effective. For example, it is shown that they use foreground and background more effectively, but it is not explored why that happens. Another important aspect would be how the key,query,value operations relate to convolution. Some visualization may help get insights regarding this.\n\n\nMinor issues:\nPage 3  Recent work ? \u2014> missing reference"
            },
            "questions": {
                "value": "Q: in light of your results can you tell what really explains higher performance of vision transformers compared to CNNs? I mean in the architecture? can that be visualized?\n\nQ: how much of these also apply to the MLPmixer architecture, or its variants\n\nQ: It seems like authors, are discarding visualization of keys, queries and values! I think visualization of these features might actually gives good insights regarding the designs of ViT. And to understand how exactly these differ from convolution operation! \n\nQ: In page 5, section: you are discussing the receptive field of neurons. It is true that neurons in all layers have full receptive field in the image, but what is the effective receptive field? In other words, can you show how much and where in the image a neuron in getting its input from?\n\nQ: For long researchers believed that CNNs might be a good model of how vision should be done mainly because they parallel well with human visual system and extract features in a hierarchy. Vision transformers seem to match less with CNNs and human visual system in terms of their structure. Can we say we are diverging from coming up with a unified model of how vision should be done?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7069/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698805401906,
        "cdate": 1698805401906,
        "tmdate": 1699636832094,
        "mdate": 1699636832094,
        "license": "CC BY 4.0",
        "version": 2
    }
]