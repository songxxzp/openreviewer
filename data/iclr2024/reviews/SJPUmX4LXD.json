[
    {
        "id": "os7OPVPlee",
        "forum": "SJPUmX4LXD",
        "replyto": "SJPUmX4LXD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission244/Reviewer_WEzt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission244/Reviewer_WEzt"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a framework named BRAIN2MUSIC that maps fMRI data to music waveforms. BRAIN2MUSIC is based on pre-trained models MuLan and MusicLM. MuLan is a text/music embedding model with two encoders. MusicLM is a conditional music generation model. The authors align the fMRI representation with MuLan embeddings and use the embedding as conditions for music generation. The mapping between MuLan representation and fMRI is done by applying linear regression. In addition, the authors utilized a publicly available dataset music genre neuroimaging dataset to verify the performance of the proposed BRAIN2MUSIC."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is well organized with a clear presentation."
            },
            "weaknesses": {
                "value": "* a) The main drawback of this work is that the conclusion drawn is not persuasive by adopting powerful pre-trained models and the limited data size of the dataset utilized in this work. Mulan is a music text embedding model that's pre-trained on a large amount of music/text data. Similarly, MusicLM is a powerful conditional music generation model pre-trained on large amounts of data. Say we randomly sample embeddings from the hidden state space of Mulan and use them as a condition to guide MusicLM; we get a piece of 'meaningful' music. By meaningful, I mean it sounds like music, not noise. Now, linearly mapping fMRI to the embedding space of Mulan to generate music does not necessarily show the correspondence between fMRI and music but the powerful representation ability of Mulan and the generation ability of MusicLM. I also carefully listened to the demos given. This confirmed my thought that the generated or reconstructed music sounds like real music but is not very similar to the stimulus music. This is also verified by the low correlation results demonstrated in Table 1. It would also be interesting to see the correlation result of only using the mean vector or random vectors to reconstruct the music.\n\n* b) No visualization results of the embeddings from fMRI to Mulan are given. Ideally, the embeddings $\\hat{T}$ on the test set should show a clustering effect.\n\n* c) Given the small size of the dataset, a K-fold evaluation should be adopted rather than a fixed test set with only 60 data points.\n\n* d) No comparisons are conducted w.r.t other works. Especially seq2seq approaches, meaning directly predicting music waveform using fMRI. Thus, it is hard to evaluate the solidness of this work."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission244/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698212951241,
        "cdate": 1698212951241,
        "tmdate": 1699635949988,
        "mdate": 1699635949988,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OadXN33UY6",
        "forum": "SJPUmX4LXD",
        "replyto": "SJPUmX4LXD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission244/Reviewer_fWGs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission244/Reviewer_fWGs"
        ],
        "content": {
            "summary": {
                "value": "The authors apply decoding and encoding analyses to fMRI responses to music using recently developed Transformer-based music generation models. \n\nFor decoding analyses, they learn a linear weighting of voxel activity that best maps onto the embeddings from different components of the MusicLM model. They then either select a clip that best matched the predicted embedding from a corpus (FMA) or they use the generation capacity of the model to generate a waveform. They show above chance ability to decode the model features, with the highest identification accuracy for the music embedding of MuLan. They report that they can better recover semantic properties, such as genres, instruments, and moods using the generative approach. \n\nFor the encoding analyses, they learn a linear map from different features of the MusicLM model to voxel data. They report that voxels are similarly well predicted by the w2v-BERT-avg and MuLan. They show that the encoding model predictions are better for the music variant of MuLan compared with the text variant consistent with better decoding. They also perform PCA on the weights from the learned encoding model and plot the stimulus and voxel embeddings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Exploring whether modern transformer-based music models can improve encoding and decoding models in the brain is a good idea. There has been a lot of progress in this area and would be interesting to know whether these models learn representations that mirror the brain to any extent.\n\nLeveraging the generative aspects of these models for the purpose of decoding is also potentially interesting. There could be methodological value and future scientific insights gained from developing improved decoding models for music."
            },
            "weaknesses": {
                "value": "The analyses are fairly preliminary and there are currently no clear neuroscience insights. \n\nThere are no comparisons against standard acoustic models used in the auditory neuroscience literature. For example, it is unclear whether the decoding model performs better at identification compared with the standard spectrotemporal modulation transfer model tested in Zakai. There is also no comparison against other DNN audio models such as wav2vec2.0 or HuBERT which have shown promising prediction accuracy in auditory cortex (the relation between w2v-BERT-avg and these prior models is unclear). \n\nThere are no perceptual experiments done to evaluate the quality of the reconstructions.\n\nThere is no serious investigation of how encoding and decoding results might vary across the auditory hierarchy. \n\nThere is no investigation of how performance might vary across different layers of the network in the case of the encoding models.\n\nFor encoding models there are no attempts to estimate the unique contribution of different models by comparing the performance of individual models against combined models. This is important as the features from different models are highly correlated. Thus a text-based model might predict auditory responses due to correlated features rather than a genuine response to text. As a consequence, the scatter plots showing correlated predictions is not surprising or particularly informative. \n\nThe statistical approach used to compute p-values does not seem appropriate, since it assumes the samples are independent and Gaussian distributed. They could use a permutation test across stimuli as an alternative. \n\nSome of evaluation metrics were unclear to me (see questions below)."
            },
            "questions": {
                "value": "I found some of metrics difficult to understand. For Figure 1A, isn\u2019t the identification accuracy based on the latent embeddings? How is this based on the reconstructions? Can you spell out exactly what was done to compute this figure. \n\nFor Figure 1B and 1C, how were the genre, instrument, and mood labels determined? Please also give the equations for how overlap was computed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission244/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786134733,
        "cdate": 1698786134733,
        "tmdate": 1699635949866,
        "mdate": 1699635949866,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Zp7OfeNLs4",
        "forum": "SJPUmX4LXD",
        "replyto": "SJPUmX4LXD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission244/Reviewer_QVSZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission244/Reviewer_QVSZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a Brain to Music pipeline to reconstruct music from fMRI data. More specifically, this pipeline contains two key components: (1) MuLan, a text/music embedding model, and (2) MusicLM, a conditional music generation model. The pipeline first uses fMRI recordings to predict music embeddings by a regularized linear regression; then, it applies the predicted music embeddings as conditions to MusicLM, where the MusicLM could recover or generate the corresponding music. In the experiments, this paper starts from a decoding task by quantitatively evaluating the music reconstruction; then, it illustrates the difference between text-derived and music-derived embeddings by designing an encoding task to predict fMRI recordings. Finally, this paper explores the generalization ability of the proposed pipeline."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* An interesting and novel topic.\n\n* A clear and detailed writing of the related works and methods.\n\n* A comprehensive experimental section. The authors discuss the proposed pipeline from both the decoding and encoding perspectives, which makes the role of involved components precise.\n\n* A good discussion of the current limitations, e.g., the temporal sampling rate of fMRI may be too slow to collect high-frequency information."
            },
            "weaknesses": {
                "value": "* According to your [demos](https://f2mu.github.io), the presented examples are nearly music clips with a strong rhythm, which may be easy to reconstruct from fMRI. Could you give some instances where the music clips come from a symphony (with a weak rhythm)?"
            },
            "questions": {
                "value": "* As the authors mentioned in section 5, the relatively high TR of fMRI is a limitation. Have you explored the retrieval/reconstruction performance of music clips with different frequencies? \n\n* Are the fMRI recordings able to encode some complex music without a precise rhythm, like a symphony?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission244/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission244/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission244/Reviewer_QVSZ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission244/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698794272735,
        "cdate": 1698794272735,
        "tmdate": 1699635949797,
        "mdate": 1699635949797,
        "license": "CC BY 4.0",
        "version": 2
    }
]