[
    {
        "id": "py8I08xBTo",
        "forum": "9RNfX0ah0K",
        "replyto": "9RNfX0ah0K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5629/Reviewer_s1Kq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5629/Reviewer_s1Kq"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a comprehensive framework using Gaussian processes to analyze the influence of individual and groups of training data points on machine learning model predictions, with a focus on privacy implications. The authors propose Leave-One-Out Distinguishability (LOOD) as a metric to quantify information leakage and demonstrate its applicability in various settings, including Gaussian processes and deep neural networks (NNGPs). They also provide theoretical insights, especially on the impact of activation functions on information leakage, and validate their findings with extensive experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper introduces a novel theoretical framework using Gaussian processes to analyze the influence of individual and groups of data points on machine learning model predictions. The authors creatively combine ideas from influence analysis, memorization, and information leakage, providing a comprehensive perspective on how training data affects model predictions. The application of LOOD (Leave-One-Out Distinguishability) as a measure of information leakage, and the exploration of its relationship with activation functions, add a unique angle to the existing body of work."
            },
            "weaknesses": {
                "value": "1.The authors have considered the fact that LOOD is a non-convex objective when analyzing global optimality using first-order information, acknowledging that analyzing global optima is quite challenging. However, if one solely utilizes the RBF kernel, as a nonlinear equation, many techniques from geometric analysis could be applied to examine the connections between global and local optima (such as considering the local properties of solutions to nonlinear equations using Sard's Lemma, etc.). Relying on experiments to complement the theoretical proof significantly undermines the credibility of the theory.\n\nSimilar issues are present in other sections of the paper as well. As an article that introduces a theoretical framework, the feasibility of this framework is supported by experimental evidence in many places. This approach raises doubts about the theoretical correctness of the framework, as it heavily relies on empirical validation rather than providing rigorous theoretical proofs throughout the paper.\n\n2.The paper extensively analyzes the LOOD under RBF and NNGP kernels. While these are commonly used kernels, the generalizability of the results to other types of kernels or models is not clear. The paper could be strengthened by either extending the analysis to other kernels or by providing a clear justification for focusing on these specific kernels.\n\n3.Given the definition of LOOD, using it to analyze and measure MIAs seems like a natural fit. However, the paper does not discuss whether LOOD can be used to measure the privacy capabilities against other potential types of attacks."
            },
            "questions": {
                "value": "1.Does the approach of modeling the randomness of models using Gaussian processes have general applicability beyond neural networks?\n\n2.Given the definition of LOOD, using it to analyze and measure MIAs seems like a natural fit. However, for other potential attack methods, is there a way for LOOD to measure their privacy capabilities?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5629/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5629/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5629/Reviewer_s1Kq"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5629/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698674104525,
        "cdate": 1698674104525,
        "tmdate": 1699636585084,
        "mdate": 1699636585084,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4qFlhKE020",
        "forum": "9RNfX0ah0K",
        "replyto": "9RNfX0ah0K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5629/Reviewer_Azxn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5629/Reviewer_Azxn"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a notion of algorithmic stability called *leave-one-out distinguishability*. For a learning algorithm that outputs a predictor, a pair of data sets $D$ and $D'$ that differ in a single record, and a specific query point $Q$, LOOD captures: how much does the algorithm's prediction on $Q$ change between $D$ and $D'$? This is a notion of local stability. The paper formalizes \"change\" through either KL divergence or average prediction, depending on the application. We also sometimes allow the data sets to differ in multiple points, or allow $Q$ to be a set of queries.\n\nThe paper aims to both unify existing work on memorization and influence and to answer questions about specific learning procedures. It contains a large number of both theoretical and experimental results. The theory results focus on Gaussian processes, while the experiments supplement these theorems and also show how the results can extend to neural networks.\n\nAfter introducing the definition, we move to Section 3, which addresses the question of what query point maximizes LOOD. We see strong theoretical and experimental evidence pointing to the conclusion that, for GPs, the optimal query is the point that differs between $D$ and $D'$. (Nonconvexity makes establishing global optimality difficult.) We see an experiment with some evidence that the story is similar for neural networks.\n\nSection 4 relates LOOD to existing notions of memorization and privacy. Most interesting to me is Figure 3a, which shows that data points tend to have similar susceptibility to membership inference attacks under NNGPs and NNs. We also see that GPs are vulnerable to data reconstruction attacks.\n\nFinally, Section 5 explores how the activation function affects NNGPs and NNs. We get a theorem about the rank of NNGPs and complementary experiments on both classes of models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This submission is full of ideas and deserves a treatment longer than nine pages. Despite that, the paper is relatively clear. The definition of LOOD closely builds on existing ideas in the literature, but uses them in new ways. The fact that we get clear results on GPs is nice, and I appreciate how we saw the results extended to NNs.\n\nWith so much content, I think it's a paper that will attract a large audience. I hope it gives them food for thought. I vote for acceptance."
            },
            "weaknesses": {
                "value": "The paper's density caused rushed discussions. In particular, I would have appreciated more \"hand-holding\" alongside the theorems and proofs. Similarly, I might prefer fewer experimental results with clearer explanations.\n\nMy largest critique of LOOD is that I feel it lacks a \"killer application\" which cleanly shows off its value. To elevate the paper, I would hope for a stronger answer to \"what can we do now that we could not do before?\" The paper gives answers about GPs, but I find these a bit underwhelming: I do not expect kernel methods to protect privacy. (Of course, people who do not work on privacy may have different expectations.) The results on NNs are interesting, but none of them are explored in enough detail to serve as a headline result."
            },
            "questions": {
                "value": "When having coffee with other researchers, which results from this paper are you most excited to discuss?\n\nWhich results in this paper will have the biggest impact on future research?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5629/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5629/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5629/Reviewer_Azxn"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5629/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698860452593,
        "cdate": 1698860452593,
        "tmdate": 1699636584840,
        "mdate": 1699636584840,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aKR6cSu1Yj",
        "forum": "9RNfX0ah0K",
        "replyto": "9RNfX0ah0K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5629/Reviewer_rJTi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5629/Reviewer_rJTi"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new way to measure or quantify the output distributions of a machine learning model on changing a few points in the input dataset, which they call, \"leave-one-out distinguishability\" or \"LOOD\". This is defined as the statistical distance (in this case, the KL-divergence) between the output distributions of the model on changing a few data points in the dataset. The main applications or advantages of introducing this notion of LOOD is that LOOD could be used to quantify (1) the memorization of data, (2) the leakage of information (via membership inference attacks), and (3) the influence of certain training data points on the model predictions. In this work, the applications of LOOD are illustrated via Gaussian processes, which they use to model the randomness of the machine learning models. They also show the effect of activation functions on LOOD. From their empirical results, they show LOOD as a good measure for all the above phenomena."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Their definition of LOOD captures the influence of data points in the trained machine learning models quite well, which they show for different phenomena, such as information leakage via membership inference attacks, and memorization.\n2. Their experiments cover enough breadth, for example, by considering different kernels (like RBF and NNGP). So, the set of results seems comprehensive enough."
            },
            "weaknesses": {
                "value": "From my limited understanding of the subject, I can't find any significant weaknesses in this work."
            },
            "questions": {
                "value": "1. Have you thought about the connections between LOOD and differential privacy (DP)? As in, DP algorithms guarantee that a few data points cannot influence the output of the algorithm a lot, so will LOOD give any useful information about DP ML algorithms?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5629/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5629/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5629/Reviewer_rJTi"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5629/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699047288174,
        "cdate": 1699047288174,
        "tmdate": 1700505984810,
        "mdate": 1700505984810,
        "license": "CC BY 4.0",
        "version": 2
    }
]