[
    {
        "id": "REEjLHrA48",
        "forum": "XqLcFMMwNb",
        "replyto": "XqLcFMMwNb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission281/Reviewer_n6co"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission281/Reviewer_n6co"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a multi-modal latent diffusion model named SVG for audio and video generation. Both audio and video signals are into latent spaces and then learn joint semantic features via classification and contrastive loss. The resulting semantic features can be used as conditional signals to improve audio-to-video and video-to-audio generation. The experiments were conducted on two sounding video datasets and the quantitative results are better than the baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) Audio-visual cross-modal generation is a challenging task. The authors proposed a promising approach to bridge the gap between audio and video efficiently.\n2) The quality of the generated samples looks good to me.\n3) The paper is easy to understand."
            },
            "weaknesses": {
                "value": "1) The paper is more like a straightforward follow-up on MM-Diffusion. The major difference is changing the diffusion targets from the raw signal domains to the latent spaces. However, based on the literature, it is almost trivial or obvious that transferring to latent space could achieve better results and improve training efficiency in any diffusion generation. From this perspective, the technical novelty of this paper is limited.\n2) The datasets used in the paper are pretty limited. The AIST and Landscape are small-size datasets. The proposed method could overfit the dataset, and indeed, diffusion is really good at overfitting. While the authors mentioned that they have not yet extended the method to open-domain sounding video datasets, I believe that is actually the critical research problem required to solve.\n3) While the quality of the generated samples on the webpage is nice for 1 second, I believe it is quite limited and probably hard to generalize for a longer time.\n4) In addition to quantitative metrics, I believe it is quite important to have a subjective evaluation of the generated samples, especially for audio."
            },
            "questions": {
                "value": "1) Have you tried doing applications on audio-visual continuation? For example, the model is conditioned on the first 1s audio, and then you can generate the next 1s audio and visual together. Based on your approach, it seems like these applications are also feasible.\n2) There exist so many losses and learnable embeddings within the audio-visual autoencoder. How do you search those weights of different losses? While there are ablation studies, it is only in one direction. I am interested in how you eventually could find the best combinations of all terms."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission281/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission281/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission281/Reviewer_n6co"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission281/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698645421138,
        "cdate": 1698645421138,
        "tmdate": 1699635953810,
        "mdate": 1699635953810,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VaP6TRNQ63",
        "forum": "XqLcFMMwNb",
        "replyto": "XqLcFMMwNb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission281/Reviewer_9ERg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission281/Reviewer_9ERg"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the Multi-Modal Latent Diffusion Model (MM-LDM) for Sounding Video Generation (SVG). The main contributions of this paper are two-fold:\n1. MM-LDM establishes audio and video latent spaces for SVG, which significantly reduces computational complexity.\n2. MM-LDM proposes a multi-modal autoencoder to compress video and audio signals from pixel space to a semantically shared latent space.\nThe proposed method achieves state-of-the-art results, demonstrating its effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors attempt to solve a novel and valuable problem and design a reasonable framework for this purpose.\n2. The multimodal VAE designed by the author is interesting, establishing semantic latent spaces for audio and video modalities. Further, the authors use a shared multimodal decoder introduced in cross-modal alignment, which can inspire future multimodal generation.\n3. The experimental results are promising in metrics, demonstrating the effectiveness of the proposed method. In particular, the MM-LDM achieves state-of-the-art results."
            },
            "weaknesses": {
                "value": "1. The designing of a multimodal VAE is innovative, but it may not be as effective as that of two separate VAEs. The authors should compare their multimodal VAE with most direct audio and video VAEs, which would better demonstrate the effectiveness of multimodal VAE.\n2. I have viewed the generated results provided by the author, and only some results from the AIST++ dataset are available (MM-LDM: Multi-Modal Latent Diffusion Model for Sounding Video Generation (anonymouss765.github.io). However, the movements of the video characters are not natural, and due to the similarity of the generated audio, it is not clear whether the two modalities are in temporal alignment. Although the author has demonstrated significant success in metrics for their proposed method, it is necessary to supplement a human evaluation with MM-Diffusion to enhance credibility.\n3. The paper only does experiments on small datasets, and there may be serious overfitting problems for diffusion-based methods. The author should discuss the generalization of their method on larger datasets."
            },
            "questions": {
                "value": "1. In Table 2, parts \u201cMulti-Modal Generative Models on Audio-to-Video Generation\u201d and \u201cMulti-Modal Generative Models on Video-to-Audio Generation\u201d, the results of MM-Diffusion don\u2019t take the other modality as a condition input.\n2. There is a typo in the heading of Table 3. \u201cLatent Average Poolong\u201d should be \u201cLatent Average Pooling\u201d.\n3. On page 5, in the section \u201cSignal Decoding\u201d, the authors state that they initialize their signal decoder with parameters of a pre-trained image diffusion model to reduce training time and enhance the quality of reconstruction. It is confusing to initialize the decoder with a diffusion model as they are for different objectives. I wonder if this initialization has positive benefits."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission281/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698649154166,
        "cdate": 1698649154166,
        "tmdate": 1699635953723,
        "mdate": 1699635953723,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "snL6VDTBbE",
        "forum": "XqLcFMMwNb",
        "replyto": "XqLcFMMwNb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission281/Reviewer_ApFF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission281/Reviewer_ApFF"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the author studies the problem of sounding video generation (SVG) and proposes a novel multi-modality video generation model in latent space named MM-LDM. The idea of incorporating the latent diffusion model and SVG task is interesting and the overall result is promising."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of modeling audio and video in latent space for sounding video generation is interesting and promising.\n2. The writing is good and the results further demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. Similar ideas to the conditional generation section have been proposed in many papers which seems too weak to list as a technical contribution in the paper. I would like the author to claim this point as a \"bonus\" of the proposed model in the paper.\n2. The visual quality of MM-Diffusion results in Fig. 4 seems quite different from their original paper even considering the result has been super-resolved by the SR model. Is there any explanation for that? The visual quality of the results seems to be in low resolution or processed by some simple SR methods like nearest-neighbor."
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission281/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723854259,
        "cdate": 1698723854259,
        "tmdate": 1699635953639,
        "mdate": 1699635953639,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NdzM5l44H3",
        "forum": "XqLcFMMwNb",
        "replyto": "XqLcFMMwNb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission281/Reviewer_1VF7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission281/Reviewer_1VF7"
        ],
        "content": {
            "summary": {
                "value": "The present paper proposes a framework based on the latent diffusion model to address the challenge of audio-visual joint generation. In comparison to the baseline (MM-Diffusion), the generation scheme proposed in this article, which operates in the latent space, offers significantly enhanced accuracy and reduces the computational burden. Moreover, the application of contrastive loss is also more judicious than in previous methods.\nHowever, the author's writing exhibits some instances of ambiguity, which may suggest a lack of thorough understanding of the latent diffusion model. While the article's motivation and approach are commendable, I suggest that the author consider further revision and refinement of the manuscript before submitting it to the next conference."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The application of diffusion in latent space has been demonstrated in the fields of image and video generation, thereby warranting its extension to multi-modal generation. This approach to model generation is highly relevant in the current context of machine learning and artificial intelligence, where multi-modal data is increasingly prevalent. By leveraging the power of diffusion in latent space, multi-modal models can be developed that can generate diverse outputs across various modalities. This approach offers significant benefits in terms of model robustness, scalability, and generalization, making it an attractive choice for businesses and academic researchers alike.\n\n2. The present study's results exhibit substantial enhancements from both qualitative and quantitative perspectives. The research outcomes demonstrate a significant improvement in both the quality and quantity of the study's output.\n\n3. The proposed module in this article has been validated through extensive ablation experiments. The results of these experiments indicate the module's efficacy in addressing the intended objectives."
            },
            "weaknesses": {
                "value": "1. I think the author's understanding of latent diffusion is not deep enough, and there are many unprofessional and unscientific descriptions in the writing process. For details, see Questions 1, 2 and 4.\n\n2. The sign of equation (7) is confusing. According to the paper, (n_a^t,n_v^t) are predicted noise features. But obviously this variable is not a predicted value, but a variable that satisfies the N(0,1) distribution.\n\n3. The Implementation Details only give the training details of the multi-modal autoencoder, but not the training details of the diffusion model. Or is it that the model in this paper does not need to first train an autoencoder and then perform diffusion training, like [1]?\n\n[1] Blattmann, Andreas, et al. \"Align your latents: High-resolution video synthesis with latent diffusion models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "questions": {
                "value": "1. \"we can leverage pre-trained image diffusion models to be our signal decoders\". How is this step implemented? Are the \"signal decoders\" used here the diffusion unet in the image diffusion model? I don't understand how image diffusion model can act as decoder in autoencoder.\n\n2. What is the relationship between the content drawn in Figure 3(b) and T? Why does the multi-modal autoencoder perform a denoising process?\n\n3. In contrastive loss, how are positive and negative samples constructed, and how to define \"matched pairs\" during the implementation process?\n\n4. \"we utilize the \u03f5-prediction to optimize our signal decoder, which involves the noise mean square error loss\". What is the relationship between the process of training autoencoder and the method of noise prediction?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission281/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823005402,
        "cdate": 1698823005402,
        "tmdate": 1699635953565,
        "mdate": 1699635953565,
        "license": "CC BY 4.0",
        "version": 2
    }
]