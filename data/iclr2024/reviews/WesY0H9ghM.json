[
    {
        "id": "erePGJBnR5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3363/Reviewer_BKHb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3363/Reviewer_BKHb"
        ],
        "forum": "WesY0H9ghM",
        "replyto": "WesY0H9ghM",
        "content": {
            "summary": {
                "value": "This work introduces Uni-RLHF, an eco-system for Reinforcement Learning with Human Feedback to facilitate the data collection with human annotators, the sharing of datasets and the RL alignment.\nIn particular, the annotation platform named Enhanced-RLHF supports various feedback types.\nThen, the paper investigates different offline RL strategies, and show that the reward models trained on their crowdsourcing labels lead to better performances than when using synthetic labels, and can approximate the Oracle rewards. This is done on motion control/manipulation tasks such as D4RL or Atari or Smarts. They aim for fair evaluation of RLHF strategies."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The work effectively succesfully presents a comprehensive system to deal with the data collection process with diverse human feedback types.\n- The motivation is clear and interesting: indeed, RLHF appears nowadays as a go-to strategy to ensure the reliability of AI systems. Therefore the proposed eco-system can be of interest to some researchers.\n- Crowdsourced labels are sufficient to approximate Oracle-based reward, showing the flexibility of RLHF even in those D4RL datasets."
            },
            "weaknesses": {
                "value": "- The main weakness is the limitation to control/locomotion tasks. More real world tasks/other modalities (such as text), and also larger architectures are required to make this eco-system more attractive.\n- The benchmark only compares offline RL, thus for example the de facto online strategy for RLHF (PPO) is not ablated.\n- The different query samplers are not ablated.\n- Only the comparative feedback is ablated. It would have been interested to compare the quality of the reward models.\n- While the authors do acknowledge this, providing a data cleaning procedure (or data filters) in the eco-system would be very positive to foster its applicability."
            },
            "questions": {
                "value": "- could you please clarify the differences with RLHF-Blender ? in which case we should use one or the other ?\n- How did you select the hyperparameters?\n- Have you applied any strategy to refine the dataset quality?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3363/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3363/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3363/Reviewer_BKHb"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3363/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697200860695,
        "cdate": 1697200860695,
        "tmdate": 1699636286427,
        "mdate": 1699636286427,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g3ZuKK2HOO",
        "forum": "WesY0H9ghM",
        "replyto": "WesY0H9ghM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3363/Reviewer_FdzJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3363/Reviewer_FdzJ"
        ],
        "content": {
            "summary": {
                "value": "Uni-RLHF is a comprehensive system for reinforcement learning with diverse human feedback. It includes an Enhanced-RLHF platform that supports multiple feedback types, environments, and parallel user annotations, along with a feedback standard encoding format. The system also provides large-scale datasets, offline RLHF baselines, and human feedback datasets for relabeling and human-aligned reward models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Uni-RLHF provides a universal platform for RLHF that supports multiple feedback types, environments, and parallel user annotations. The system includes a feedback standard encoding format that facilitates the integration of diverse feedback types.\n2. Uni-RLHF provides large-scale datasets and offline RLHF baselines for evaluating the performance of RL algorithms with human feedback. The system also includes human feedback datasets for relabeling and human-aligned reward models, which can improve the efficiency and effectiveness of RLHF.\n3. Uni-RLHF can foster progress in the development of practical problems in RLHF by providing a complete workflow from real human feedback."
            },
            "weaknesses": {
                "value": "1. Uni-RLHF's large-scale crowdsourced feedback datasets may contain noise and bias, which can affect the performance of RL algorithms trained on these datasets.\n2.  The system's offline RLHF baselines may not be optimized for specific applications, which can limit their usefulness in practical settings.\n3. The system's reliance on human feedback may introduce additional costs and delays in the RL development process, compared to purely synthetic feedback.\nUni-RLHF is an engineering-focused system that emphasizes incremental improvements rather than groundbreaking innovations."
            },
            "questions": {
                "value": "Please refer to weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3363/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3363/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3363/Reviewer_FdzJ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3363/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698658313667,
        "cdate": 1698658313667,
        "tmdate": 1699636286349,
        "mdate": 1699636286349,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AGzVOBphhB",
        "forum": "WesY0H9ghM",
        "replyto": "WesY0H9ghM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3363/Reviewer_3bfZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3363/Reviewer_3bfZ"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new framework to collect human feedback for RLHF. The framework allows for five different types of feedback to be collected with a convenient user interface, depending on the task at hand. A dataset of feedback of three of these types is collected using crowdsourced human workers. This dataset is then used to evaluate multiple existing offline RLHF approaches, where a trained reward model is used to label trajectories from a dataset, and then an offline RL algorithm is used to produce a policy. The evaluation shows that the collected data for comparative and attribute feedback is of good quality, and allows for the learned policy to perform at a quality comparable to a policy trained from hand-crafted reward models. In some cases the policies trained from human feedback outperform those trained from hand-crafted reward models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "In my view, the main strength of the paper is the open-sourced dataset and the tool for feedback collection. These will bring great value to the community and allow for easy benchmarking of offline RLHF methods. The extensive evaluation on D4RL is another strong point of the paper. It will provide a reasonable baseline for future offline RLHF approaches. Finally, the dataset with collected attribute feedback can be used for multi-objective learning, an area where fewer datasets in general are available."
            },
            "weaknesses": {
                "value": "- The claim for a \"standardized feedback encoding format along with corresponding training methodologies\" in Section 3.2 might be overly ambitious. Most importantly, the training methodologies are only provided in the appendix for two out of the five feedback types. Comparative feedback interface also does not allow the user to indicate the strength of preference, as done e.g. in the Anthropic's dataset for language model alignment [1]. The extension of the methodology in Appendix E.1 to this type seems straightforward (introduce y=(0.75,0.25), for example), and it could be beneficial to mention this.\n- Related to the previous point, the paper provides too few details on the way Atari experiments were performed. Appendix G and Section 4.1.1 imply that comparative feedback was collected, but Figure 5 (d) in the appendix -- that visual feedback was used instead. The highlighting in Table 8 should be explained. My guess is that the best of the ST and CS labels is highlighted.\n- There is no dataset or benchmarking for evaluative feedback, hence it is hard to assess the usefulness of this part of the system. It is unclear whether data for keypoint and visual feedback on the Atari environments was collected. I believe that the datasets for comparative and attributive feedback are already a good enough contribution, and an interface for other feedback types is a nice-to-have extra, so this point does not make me give this paper a negative rating.\n- Star annotations in Table 2 are incomplete. As I understand it, higher is better in the entire table. If that is the case, stars that should mark that the method performs better than the oracle are missing in several places: (hopper-m, CQL, CS-TFM), or (antmaze-u-d, IQL, CS-MLP). There are more missing stars in the table.\n- Blue annotations in Table 2 are also significantly wrong, if I understand them correctly. Blue should mark the methods where crowdsourced labels (CS) show better results than the synthetic labels (ST) for the corresponding method. Then, for example, (walker2d-m-r, IQL, CS-MLP) should not be colored in blue, since (walker2d-m-l, IQL, ST-MLP) shows a better score (109.8 > 109.4). I counted 10 instances of such mislabeling in the IQL group alone. This is especially important since the paper claims that \"the performance of models trained using crowd-sourced labels (CS) tends to slightly outperform those trained with synthetic labels (ST) in most environments, with significant improvements observed in some cases.\" Once the blue labels are revised, it is questionable whether this claim still holds. Other offline RL approaches (CQL and TD3BC) show that CS labels are better more consistently, but these approaches perform worse than IQL, so the results for them are correspondingly less important.\n- Some CS-TFM results are missing from Table 2. I did not find the explanation for this, maybe I missed it? It would be helpful to see these results in the final version, to better assess the claim of the paper that \"CS-TFM exhibits superior stability and average performance compared to CS-MLP\".\n- For the SMARTS experiment in Section 4.1.2, the paper claims that \"the best experimental performance can be obtained simply by crowdsourcing annotations, compared to carefully designed reward functions or scripted teachers.\" I would say that from the three tasks used one cannot conclude that carefully designed rewards perform worse than crowdsourced annotations. In table 3, the former outperforms the latter on two out of three methods. The claim seems to be made based on average success rate, which only differs by 0.01, less than one standard deviation.\n- In Figure 4, speed and torso height are plotted against time. Every 200 steps, the speed attribute value changes (1 to 0.1 and back), so we see the respective changes in speed on the first plot. The relative strength of the \"torso height\" attribute, however, does not change (stays at 0.1), and the torso height parameters do not change much between the changes. It would be more interesting to see the results where the strength of torso height also changes, so that we can see that it influences the plot.\n- Generally speaking, RLHF is most useful in domains where only the humans understand the true reward. This is the case, for example, with \"helpfulness\" of an LLM, or with \"humanness\" of the gait of a walking robot. An important evaluation, then, is to see whether the human evaluators prefer the policies trained with an RLHF approach in terms of these hard-to-define measures. This paper, however, does not present such an evaluation. \"Humanness\" comparisons are collected as shown in Section 4.2, but it is never compared to a policy that is trained without taking humanness into account.\n\nOverall, these weaknesses do not seem to me to be strong enough to motivate a rejection. The dataset and tool contributions are strong, and the problems with baseline evaluations can be clarified in the final version of the paper."
            },
            "questions": {
                "value": "- In table 2, the results are normalized against an \"expert\", as the formula in the beginning of Section 4.1.1 shows. How is the expert trained? It is interesting that some of the methods in the table outperform the expert.\n- I found the attribute training model in Appendix E.2 confusing. The learned log-preferences $\\hat{\\zeta}^\\alpha$ in eq. (7) probably also need the subscript $\\theta$.  The relative strengths of attributes $v^\\alpha_{opt}$ are provided as hyperparameters. These strengths are in $[0,1]$. Why, then, in eq. (7) the attributes are checked for being close to $\\hat{\\zeta}^\\alpha$, which is supposed to become the probability of preference according to the respective attribute only after the softmax operation (5)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3363/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3363/Reviewer_3bfZ",
                    "ICLR.cc/2024/Conference/Submission3363/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3363/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699297065957,
        "cdate": 1699297065957,
        "tmdate": 1700736671217,
        "mdate": 1700736671217,
        "license": "CC BY 4.0",
        "version": 2
    }
]