[
    {
        "id": "JcP4u6j5Yv",
        "forum": "mp8ZgMZ1RG",
        "replyto": "mp8ZgMZ1RG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission136/Reviewer_PQPu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission136/Reviewer_PQPu"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the text-to-sound and text-to-music tasks using an LLM, an audio tokenizer and a vocoder.\nThe proposed method generates audio signal based on a small-scale LLM (flan-t5) that encodes a text, and SoundStream that encodes audio signals into audio tokens.\nThis paper appears well structured and easy to read. But I think important information is not adequatly provided to assess the quality of results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality: Originality of problem definition would be limited because this paper addresses the existing tasks using existing benchmark datasets. But the experiments show that the perofmance is better than existing methods in many evaluation metrics. There would be some originality in some of technical details.\n\nQuality: The proposed method excels in many evaluation metrics.\n\nClarity: The paper structure is clear and it would be easy to readers to find information they want.\n\nSignificance: The task of using pre-trained LLMs for different modalities is a major trend these days, and if the results are really convincing, it will attract many readers."
            },
            "weaknesses": {
                "value": "- While this is a trendy theme and there are many competitors, the originality of this method (that is, the difference from the existing methods), other than the evaluation metrics, has not been explicitly and sufficiently promoted.\n- I am doubtful that the metrics are a legitimate measure of sound quality evaluation. Methods inspired by Frechet inception distance are often used, but in my experience they don't always match my actual feeling. The validity of KL is also not clear to me. Also, in any case, it would be nice if all the Tables have confidence intervals.\n- The authods claim audio samples are available at github but I cannot access it."
            },
            "questions": {
                "value": "- I cannot access the github link and cannot assess the sound qualtiy. \n- At least it would be good to have some spectrograms and some typical prompts (not templates.) More diagrams, equations, examples of prompts and spectrograms of the acoustic data would make this paper more convincing.\n- As noted above, I am suspicious about the validity of the evaluation metrics. And I think confidence intervals (error bars) are also needed.\n- I think the name \"HamonyLM\" is misleading. It sounds like that the method generates a chord sequence of music using an LLM.\n- The figure shows that FLAN-T5 is frozen but the text says \"During the training of language models\" in 3.6.1. I do not see what does it mean."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission136/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission136/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission136/Reviewer_PQPu"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission136/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765643581,
        "cdate": 1698765643581,
        "tmdate": 1699635939060,
        "mdate": 1699635939060,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0w2U87HZlm",
        "forum": "mp8ZgMZ1RG",
        "replyto": "mp8ZgMZ1RG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission136/Reviewer_Vp5Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission136/Reviewer_Vp5Z"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces HarmonyLM, a model both capable for text-to-sound and text-to-music generation tasks using discretized audio representations. It emphasizes model and data scalability and highlights achieving superior quality."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- **Paper Organization:** The authors have well-structured the paper into subsections, enhancing readability and enabling readers to easily locate to desired section.\n- **Comparison with Previous Works:** The authors conducted a comparison with various previous methods on both text-to-sound and text-to-music tasks, providing a context for understanding the performance of proposed approach."
            },
            "weaknesses": {
                "value": "- **Lack of Novelty and Details**\n    - The proposed method lacks novelty. It closely resembles the existing MusicGen [1] system with only minimal differences. The only difference from [1] is that they are using their own version of audio codec model and the training dataset.\n    - The paper introduces their own version of audio codec model, but it lacks essential information for reproducibility, and there's no comparison with previous approaches like SoundStream or Encodec. Therefore, it's unclear if this module adds significant value to the proposed system, and confuses which module part actually contributes to the proposed system\u2019s performance.\n- **Evaluation Methodology**\n    - Most critically, the paper's subjective evaluation methodology raises concerns. The MOS values for baseline systems are exactly the same as [2] and [1] for text-to-audio and text-to-music, respectively. This indicate the authors would have likely conducted the listening test just with their own samples; making the proposed approach\u2019s result hard to justify.\n    - Besides the inappropriate method of the subjective evaluation, it\u2019s hard to know the contribution of this paper is whether on the training dataset or the system. There are open source models of the baseline systems that could be trained with the same training data, yet, the authors likely did not train those systems with their training dataset for a fair comparison.\n- **Missing Information**\n    - The paper references detailed information in the Appendix section, but the provided information are not sufficient for reproducing the proposed work..\n    - The provided link for audio samples was not available at the time of review, limiting the subjective analysis and lacking the confidence for the reviewer.\n\n**minor feedback**\n\n- typo: @Contributions at the end of the Introduction: \u201cWe investigate the model and data scalability **~~if~~\u2192of** large language models for both sound and music generation.\u201d\n\n[1] Copet, Jade, et al. \"Simple and Controllable Music Generation.\"\u00a0*Advances in Neural Information Processing Systems* (NeurIPS 2023).\n\n[2] Huang, Jiawei, et al. \"Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation.\"\u00a0*arXiv preprint arXiv:2305.18474*\u00a0(2023)."
            },
            "questions": {
                "value": "- Will the audio samples be publicly available, ensuring accessibility for future evaluations and comparisons?\n- Will the authors provide implementation code for reproducibility?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission136/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission136/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission136/Reviewer_Vp5Z"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission136/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768251040,
        "cdate": 1698768251040,
        "tmdate": 1699635938994,
        "mdate": 1699635938994,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zaoh6ujTPh",
        "forum": "mp8ZgMZ1RG",
        "replyto": "mp8ZgMZ1RG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission136/Reviewer_GKXd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission136/Reviewer_GKXd"
        ],
        "content": {
            "summary": {
                "value": "The authors describe HarmonyLM, a text-to-audio model that can generate both sound and music. The approach involves training an LLM on audio tokens extracted from large (audio, text) datasets using SoundStream. The authors compare their approach to strong baselines on a variety of text-to-audio benchmarks for sound and music."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The primary strength of this paper is the apparently strong quantitative and qualitative performance. The proposed method outperforms strong baselines on most evaluation metrics across both sound and music generation tasks."
            },
            "weaknesses": {
                "value": "Unfortunately, this paper has numerous issues preventing it from being of publishable quality. Most prominently, this paper is **poorly-written, lacking clarity and omitting important details**, and is **devoid of any salient research contribution**.\n\n**Poorly-written**. This paper has many issues with presentational clarity and often omits key details of the proposed method. The clarity issues are apparent even at the very outset when describing the motivation and the contributions. This work is not the first to propose a unified method for generating both sound and music. In fact, one of their baselines, AudioLDM, is already capable of both tasks using a unified method. Additionally, the purported advantages of the HarmonyLM approach: \u201cmodel scalability\u201d and \u201cdata scalability\u201d, are already features of basically every recent development in the field of audio generation. Moreover, the paper omits numerous crucial details. For example, the \u201cunified prompt template\u201d section appears to be the most salient departure from standard practice, but it is basically incomprehensible (I read it multiple times and still feel no closer to understanding how the approach works). Beyond the lack of high-level clarity and the omission of details, there are countless typos throughout (too many to list) and the paper is littered with handwavey unsubstantiated explanations (e.g., \u201cextensive pre-training enables the models to learn new tasks effectively by leveraging in-context information and mimicking gradient descent through attention weights\u201d).\n\n**Lack of research contribution**. This paper essentially offers no substantive contribution to research. The methodology is more or less standard practice - HarmonyLM maps text tokens into audio tokens which are then converted into listenable audio, the same basic setup as much of the recent work in this area. The aspects of the methodology which are non-standard (e.g., vocoding tokens or the \u201cunified prompt template\u201d) are not described in sufficient detail to allow for reproduction. The paper boasts extremely impressive quantitative metrics, but the authors don\u2019t discuss releasing the model, nor do they even provide any sound examples to listen to. How can anyone in the research community build on this work or even trust the results? Perhaps the list of datasets used to train this model would be useful for reproducing the results, but the identification of a set of text-audio datasets hardly constitutes a research contribution."
            },
            "questions": {
                "value": "- Can the authors provide sound examples for their method?\n- Will the authors release the trained model?\n- Can the authors elaborate on how the unified prompt template approach works and commit to refining the quality of the associated text?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission136/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698803852120,
        "cdate": 1698803852120,
        "tmdate": 1699635938919,
        "mdate": 1699635938919,
        "license": "CC BY 4.0",
        "version": 2
    }
]