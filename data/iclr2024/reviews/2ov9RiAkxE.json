[
    {
        "id": "OFXG6uXvwH",
        "forum": "2ov9RiAkxE",
        "replyto": "2ov9RiAkxE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6840/Reviewer_uAhs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6840/Reviewer_uAhs"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an approach of identifying potential high-risk vulnerabilities in LLM-integrated applications. The identified threats are assessed in applications empowered by OpenAI GPT-3.5 and GPT-4, showing that the threats can bypass the policies of OpenAI. A mitigation is designed and evaluated."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The study focuses on an interesting and important topic, the potential vulnerabilities in LLM-integrated applications.\n+ The service scheme of LLM-integrated applications is clear presented."
            },
            "weaknesses": {
                "value": "- Lack of real-world case analysis\n\nMy first concern is related to threat evaluation. In my opinion, it would be better and necessary to provide a set of real-world cases for this threat evaluation, rather than simply mentioning \"consider an online shopping application whose chatbot uses GPT-3.5 and GPT-4 from OpenAI\". Since there is no detailed information about this shopping application provided, I doubt whether it represents a real-world application. Even if it is, to present the potential threats more effectively, it would be beneficial to involve multiple real-world applications in the evaluation.\n\n- Sending message directly to LLM may break the business model\n\nIn the proposed mitigation, it is mentioned that \"queries from users are also sent to an LLM along with queries refined by the application\". If I understand this correctly, this approach may break the business model of LLM-integrated applications, as illustrated in Figure 1. Additionally, it would be helpful to clarify how directly sending messages to the LLM model can prevent the attacks discussed in the threat model, as transmitting more information may increase the attack surface.\n\n- Not clear what is verified in the proposed Shield\n\nDespite the security concerns that may arise with the proposed Shield, it is not clear what exactly the Shield verifies in the proposed defense. It appears that the Shield only verifies whether the message originates from a user, rather than conducting semantic analysis. As described in the threat model and shown in Figure 4, an attacker can manipulate the output of the LLM by sending a malicious system prompt, rather than altering the information in the user's message. Please clarify how such signature verification can effectively address the potential threats described in Figure 4."
            },
            "questions": {
                "value": "1. How directly sending messages to the LLM model can prevent the attacks discussed in the threat model?\n2. How the proposed signature verification can effectively address the potential threats described in Figure 4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6840/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6840/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6840/Reviewer_uAhs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6840/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698714041932,
        "cdate": 1698714041932,
        "tmdate": 1699636792244,
        "mdate": 1699636792244,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2vcihgWc09",
        "forum": "2ov9RiAkxE",
        "replyto": "2ov9RiAkxE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6840/Reviewer_tqMB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6840/Reviewer_tqMB"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes new attacking surfaces for LLM-integrated applications, which used to refine users\u2019 queries\nwith domain-specific knowledge. two types of threats are defined, one from the inside developed and one from outsiders with control over databases."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper proposes an analysis over vulnerability of LLMs"
            },
            "weaknesses": {
                "value": "1. Assessing the vulnerability of LLMs is an important topic. However, the analysis presented in the paper and the results obtained from those analysis are already  widely known.\n\n2. The paper is poorly written. It is extremely difficult to follow. The problem setting and the proposed attack surfaces are not  well-defined and it is not clear how these attacks are different from the existing attacks proposed for LLMs (e.g., [1]) . At the very end of the paper, it proposes a defense mechanism which is not talked about at all throughout the paper. \n3. It is also not clear how the proposed defense mechanism is different form existing defenses proposed for LLMs.\n\n[1]Wei, Alexander, Nika Haghtalab, and Jacob Steinhardt. \"Jailbroken: How does llm safety training fail?.\" arXiv preprint arXiv:2307.02483 (2023)."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6840/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6840/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6840/Reviewer_tqMB"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6840/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778508756,
        "cdate": 1698778508756,
        "tmdate": 1699636792133,
        "mdate": 1699636792133,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mJairL26iQ",
        "forum": "2ov9RiAkxE",
        "replyto": "2ov9RiAkxE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6840/Reviewer_o6oy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6840/Reviewer_o6oy"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method for identifying and mitigating vulnerabilities in LLM-integrated applications. Specifically, the paper focuses on vulnerabilities that can arise from external adversaries interacting with an LLM application as well as from insider threats. The paper empirically analyses both these types of threats for a chatbot integrated with OpenAI GPT-3.5 and GPT-4. The paper also proposes a defence method to mitigate these security risks  based on four key properties viz. integrity, source identification, attack detectability and utility preservation. The authors claim that the proposed method is able to mitigate the risk for the identified security threats."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Paper discusses a relevant area of research which might become very important in the near future. Because of the recent success of LLMs there is a keen interest in integrating all sorts of applications (including chatbots) with LLMs using APIs. However most people in the industry are still unaware of the potential risks and security threats involved in doing this although they fear that if they are not doing this they might fall behind. This work can help identify some of these risks and the mitigation steps and as such will be very useful for the industry practitioners to read and implement.\n\n2. The contribution of the paper is very well articulated. For example, it is clear that the authors are not focused on the typical risks like hallucination, unwanted content, privacy and bias associated with the LLM response. These risks have been well studied and also the industry is more aware of these kind of risks. The authors here are instead focused on insider and outsider threats associated with LLM integration by which  restrictions and policies imposed by OpenAI can be bypassed to achieve an undesired objective. \n\n3. The paper proposes a simple yet effective method for guarding against upstream and downstream manipulation of user queries using a signing an verification process which ensures that the correct user query is used for prompting and the correct response is received at the user end. Any semantic perturbations of the user query or LLM response are detected by the Shield system. This appears to be a novel contribution and can be easily adopted in the industry."
            },
            "weaknesses": {
                "value": "1. The scientific contribution of this paper is limited except for the defence detection strategy. However this method also does not involve any ML/DL and uses cryptographic techniques (RSA based). Having said that, the overall contribution is valuable as it exposes the weakest of an AI based system and helps in defending against attacks on such systems by malicious users.\n\n2.Some of the contributions of the paper like cost analysis are not mentioned in the paper and is available only in the supplemental information. Not sure if this can be used in the evaluation of the paper as then the paper itself will exceed the content limit. However a lot of questions which I had after reading the paper was actually answered satisfactorily by the supplemental material."
            },
            "questions": {
                "value": "The paper uses a chatbot for an online shopping application and shows that queries can be perturbed to make the user prejudiced towards buying specific items. Can the same method be used for example to evaluate risks in a chatbot for let's say legal queries? Basically my question is - how generic is the method used and how easily can we apply this method of threat defence for other types of applications?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6840/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699187255254,
        "cdate": 1699187255254,
        "tmdate": 1699636792034,
        "mdate": 1699636792034,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uvLYKMSwIF",
        "forum": "2ov9RiAkxE",
        "replyto": "2ov9RiAkxE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6840/Reviewer_E69Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6840/Reviewer_E69Y"
        ],
        "content": {
            "summary": {
                "value": "This paper demonstrates scenarios where insider and outsider threats to LLM-integrated applications can bypass the LLM safeguards and enable malicious behavior such as biased and toxic responses. Four key properties: integrity, source identification, attack detectability, and utility preservation are defined to mitigate these vulnerabilties. A novel API, Shield, is proposed that preserves these properties. Experimental results show that Sheild can successfully detect attacks across risks while preserving utility of the applciation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper provides extensive experimental results on various vulnerabilities in LLM-intergrated applcations. Considering the rapid expansion of such applications, this work focuses on an important problem. These results could be valuable for the community for building more secure applications using LLMs.\n\n- It characterizes key properties required for reducing vulnerabilties in LLM-integrated applications. This characterization could potentially be useful for developing solutions in this domain.\n\n- Experimental results shows that the proposed API,  Sheild, provides effective defense to counter the presented threat models in LLM-integrated applications that use GPT-based models."
            },
            "weaknesses": {
                "value": "- While this work provides extensive empirical results on potential vulnerabilities, the novelty of this work on showing the risks in the query-response protocol with LLM compared to existing works on prompt injection is not clear.\n\n- For attack detection, Shield relies on LLM's capability in detecting maliciousness. It would be interesting to see how this dependency impacts the overall effectiveness of Shield. Results from different LLMs may provide some insights."
            },
            "questions": {
                "value": "1. When an attack is detected, responses from user query is returned instead of \u2018application prompt\u2019 to maintain utility: Is there any degradation in the quality of responses from LLM based on user query compared to the expected responses based the application prompt?\n\n2. As Shield needs an additional prompt per user query, is it correct to assume that this will increase cost per query?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6840/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699232893141,
        "cdate": 1699232893141,
        "tmdate": 1699636791888,
        "mdate": 1699636791888,
        "license": "CC BY 4.0",
        "version": 2
    }
]