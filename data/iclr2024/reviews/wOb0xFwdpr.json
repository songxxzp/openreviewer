[
    {
        "id": "TRMsYMNoNH",
        "forum": "wOb0xFwdpr",
        "replyto": "wOb0xFwdpr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6477/Reviewer_ir58"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6477/Reviewer_ir58"
        ],
        "content": {
            "summary": {
                "value": "The main focus of the paper is to analyze the impact of model size, versions, learning methods, and domain context knowledge on GPT models' ability to detect sarcasm. The authors conducted experiments using different versions of GPT models and tested their performance in sarcasm detection using the SARC 2.0 sarcasm dataset from Reddit. The experiments reveal that larger fine-tuned GPT models achieve higher accuracy and F1-scores, and the latest GPT-4 model performs better than earlier versions in zero-shot detection. The paper also highlights the importance of reassessing model performance after each release as the performance of GPT models can vary. Overall, the findings may provide insights into the detection of sarcasm using GPT models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The work offers a relatively comprehensive investigation into the utilization of GPT-based LLMs for sarcasm detection.\n2. The research showcases experimental outcomes from applying GPT models to the SARC 2.0 sarcasm dataset. Specifically, it demonstrates the proficiency of both fine-tuned and zero-shot GPT variants across multiple model sizes, versions, and context.\n3. The conclusions drawn from the research have potential to provide valuable insights and guidance for the direction of sarcasm detection methodologies."
            },
            "weaknesses": {
                "value": "1. At a holistic level, the manuscript comes across more as an experimental report rather than a meticulously structured technical paper.\n2. The choice of prompts is crucial for achieving optimal task performance. However, this paper seems to overlook a comprehensive exploration and display of various text prompt designs. Furthermore, there is an absence of examination into the potential influence of different prompts on the model's performance. This oversight could lead to inherent biases when drawing conclusions.\n3. The presentation of experimental results is somewhat lackluster. The paper primarily relies on a single table, devoid of additional results or visual representations, which challenges the reader's confidence in certain analytical outcomes. This limited presentation also means missed opportunities for more intricate and engaging analysis.\n4. The experimental scope misses out on comparisons with non-GPT LLMs and also doesn't juxtapose the performance of LLMs against traditional models. Consequently, the findings and conclusions can be perceived as quite constrained.\n5. While the paper attempts to conclude its findings, it does not pave the way forward by suggesting potential future work and directions. Offering such insights would be beneficial for subsequent researchers to build upon this study."
            },
            "questions": {
                "value": "1. You highlight that the most advanced fine-tuned GPT-3 model clocks in with both accuracy and F1-score of 0.81 in sarcasm detection. How does this stack up against prior models? Could you delve deeper and furnish information regarding the performance metrics of preceding models on this identical dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6477/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698486317696,
        "cdate": 1698486317696,
        "tmdate": 1699636725402,
        "mdate": 1699636725402,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wvXryOFmjq",
        "forum": "wOb0xFwdpr",
        "replyto": "wOb0xFwdpr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6477/Reviewer_kn7S"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6477/Reviewer_kn7S"
        ],
        "content": {
            "summary": {
                "value": "Focused on the pol-bal dataset, this study examined the ability of differently-sized and different versions of GPT models to detect sarcasm with and without domain context, fine-tuned and zero-shot.\n\nThe authors designed prompts with and without domain context to feed the data to LLMs for fine-tuning and zero-shot tests.\nThe results led to the answers of the 4 research questions put forward in the introduction, which are:\n\n1. The accuracy and F1-scores of fine-tuned models increases monotonically with model size, and the GPT-3 davinci model achieves the state-of-the-art performance on the pol-bal dataset.\n2. In the pol-bal dataset, only the most sophisticated GPT model (i.e., GPT-4 GPT-4-0613) can detect sarcasm competitively using the zero-shot approach.\n3. In the fine-tuning case, domain context is irrelevant; In the zero-shot case, GPT-4 models may be hindered by the domain context.\n4. The GPT model\u2019s ability to detect sarcasm may decline or improve with new releases."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "### General\n\n- The paper is well-written and easy to follow.\n- The study is the first research of LLMs' performance on sarcasm detection with the pol-bal dataset.\n- Analysis of the results is detailed and rigorous.\n\n### RQ1\n\n- The study achieves state-of-the-art performance on the pol-bal dataset with a fine-tuned GPT-3 model."
            },
            "weaknesses": {
                "value": "### General\n\n- The task of using LLMs to detect sarcasm has been studied before, and has been included in many papers to evaluate the performance of LLMs. As is repeated 4 times in section 2.3, the main distinction from previous works is claimed to be the use of the pol-bal dataset, which don't seem to be a significant contribution.\n- The study is based on GPT models from OpenAI only, which causes the results to be biased and not representative of the whole LLMs family.\n\n### RQ2\n\n- The claim in section 4.2 that GPT-3.5-turbo performs better than ZeroR classifier can't be inferred from the experimental results. The accuracies are just around 50% and F1 scores are terribly low.\n\n### RQ3\n\n- The claim in section 4.3 that domain context reduces the number of missed observations for GPT-3 models (without logit bias) from \u2248 100% to a range between 97.97% and 99.65% is not shown in Table 3. Actually the columns of `w/o domain` in these rows are empty, except for the 0.00% missing rate from ada to curie and the 99.97% missing rate for davinci.\n- The conclusion about the effect of domain context on classification tasks is valid for the single given form **ONLY**, but other forms of domain context might be more effective. This is mentioned in section 4.5, but I recommend adding other tested domain context forms and results in the appendix.\n- The conclusion is insignificant and not decisive.\n\n### RQ4\n\n- Analysis of the performance declination of GPT-3.5-turbo model is doubtful, since the performance is not significantly different from a random classifier.\n- The question is not meaningful, because the paper just presents the results and doesn't provide any technical explanation for the performance variation. No insight can be gained from the conclusion, even if a relation between the performance and the corresponding GPT version were found."
            },
            "questions": {
                "value": "- Is it possible to add more forms of domain context to see how the length, phrasing and other features affect the performance?\n- Why are the accuracies without domain context in Table 3 empty for GPT-3 models without logit bias? And why do the missing rates disagree with the claim in section 4.3 about missing rate reduction due to domain context?\n- Is it possible to try more LLMs to make the conclusions more representative and general?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6477/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6477/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6477/Reviewer_kn7S"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6477/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719342110,
        "cdate": 1698719342110,
        "tmdate": 1699636725272,
        "mdate": 1699636725272,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2Ie7mSFyCO",
        "forum": "wOb0xFwdpr",
        "replyto": "wOb0xFwdpr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6477/Reviewer_5eai"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6477/Reviewer_5eai"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a set of experiments with a large number of GPT\nfamily of models, investigating their ability to detect sarcasm\nwith/without domain information provided as part of the prompt in\nwell as zero-shot setting and through fine-tuning on the task. The\nresults indicate some trends in the GPT models' success on the task -\nthe most straightforward one being \"larger better\". However, the effect of\ndomain information is unclear."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Sarcasm detection, in general, identifying any type of figurative\nlanguage is one of the difficult problems in NLP.\nThe use of context and additional information\n(i.e., the domain of the text) is also interesting,\nas figurative language requires (situational) information beyond the given text.\nThe extensive results presented may help practitioners to decide for\nthe GPT model to use in case the application at hand requires\nsensitivity to sarcasm."
            },
            "weaknesses": {
                "value": "I find the overall information content coming out of the study rather\nweak. Knowing the relative success of the GPT models may help some\nusers of these models. However, the results  are specific to these (rather\nopaque) models, and they do not, otherwise, provide further\ninformation on the problem, or the mechanisms/representations useful\nfor solving it. In this respect, the most interesting research\nquestion is the effect of the domain information. Yet, the experiments\ndo provide any clear knowledge on this question.\nI think the paper would be much more insightful if this question was at the center.\nI will list a few concrete points of criticism of the paper below.\n\n- As noted above, I think a more scientifically interesting question is\n  the effects/mechanisms at play when using (situational) context to\n  detect sarcasm. The results presented are rather unclear on this\n  question. A possible reason could be the limited information\n  provided (only the fact that the conversation is about politics).\n  Presumably the large language models can also infer this from the\n  thread provided (e.g., \"Just finished watching the debate. I love\n  the President\" already has a strong indication that the domain is\n  politics). Focusing on providing information that is not available\n  in the thread, or analyzing the instances where the provided domain\n  information gives additional information may have been interesting\n  here. \n\n- The use of statistical/quantitative comparisons are good, but there\n  are some issues. First, for some of the statistical significance results reported\n  multiple comparisons should be taken into consideration (not all\n  require compensations, but there are some multiple-comparisons that\n  are rather exploratory, and need correction). Second, during\n  reporting, expressions like \"p \u2264 0.021\" are not strictly correct.\n  Here, presenting what the p values equals to (up to some significant\n  digits), or just indicating that it is less than the level chosen\n  would be proper ways of reporting it (I'd personally prefer the\n  former).\n\n- The paper would benefit from some revisions that reduced\n  \"itemize-like\" listing into a more coherent narrative (e.g., in\n  literature review section). It would also be nicer to the reader to\n  use verbal expressions rather than (not-so-standard) notation like\n  \"accuracy \u2248 0.77\". Another minor note: the long footnote (1) should\n  probably be part of the text. There are also some minor typographic/language\n mistakes that could be fixed with a revision (e.g., spurious space around punctuation\nin the first paragraph of Section 2.1, placement of footnote marks before punctuation,\ncase (normalization) issues in bibliography...)"
            },
            "questions": {
                "value": "Please see \"weaknesses above\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6477/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699003719961,
        "cdate": 1699003719961,
        "tmdate": 1699636725142,
        "mdate": 1699636725142,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "K9X0qHCdS9",
        "forum": "wOb0xFwdpr",
        "replyto": "wOb0xFwdpr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6477/Reviewer_XmYa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6477/Reviewer_XmYa"
        ],
        "content": {
            "summary": {
                "value": "Inspired by the effectiveness of OpenAI GPT models over other NLU tasks, this paper studies the effectiveness of OpenAI GPT-based models, including GPT-3, InstructGPT, GPT-3.5, and GPT-4 for sarcasm detection. Authors have considered twelve variants of these models, including all in total for the study, and have considered a balanced SARC (sarcastic Reddit comments) \u201cpol-bal\u201d dataset to validate the effectiveness of the different models. They have fine-tuned and tested the base GPT-3 models and zero-shot tested the GPT-3, InstructGPT, GPT-3.5, and GPT-4 models over the pol-bal dataset with and without domain context prompt, and zero-shot prompt. The study shows that a fine-tuned GPT-3 davinci model with and without domain context achieves state-of-the-art results (accuracy = 0.81 and F 1 = 0.81). Towards zero-shot, GPT-4 GPT-4-0613 reported an accuracy \u2248 0.71 and F 1 \u2248 0.75, which seems a good performance; however, it is unable to beat the prior models and stands second in the list. Including domain context in fine-tuning and zero-shot learning cases seemed mostly irrelevant and could not help the models improve. Although the latest version of GPT-4 performed better than the earlier one, based on the observation that the GPT-3.5-turbo model\u2019s performance has declined from release to release, the GPT model\u2019s ability to detect sarcasm may decline or improve with new releases."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed study leverages the potential of OpenAI GPT models for sarcasm detection, an important NLU problem.\n2. The results obtained over the fine-tuned models are promising; specifically, the fine-tuned GPT-3 davinci model outperforms the prior sarcasm detection models.\n3. Sarcasm is understood between people or groups based on shared knowledge; in that case, the notable performance of the studied models is laudable but also gives the assumption that the respective models might have seen the pol-bal dataset during training."
            },
            "weaknesses": {
                "value": "1. The study is validated over just one balanced dataset; however, in the real world, sarcastic and non-sarcastic conversations rarely occur equally likely over any social media platform.\n2. This study can not be generalized to other datasets. \n3. The prompt with domain context is trivial, and the reported result shows that it is not helping the models towards sarcasm detection.\n4. A detailed study is missing regarding the possible reason behind the model outperforming the prior models in a case.\n5. In many cases, the models are performing even inferior to ZeroR classifier.\n6. Based on the results reported in Table 3, it seems most of the models' performance is inferior to ZeroR classifier in the case of zero-shot."
            },
            "questions": {
                "value": "1. Why have the authors not considered any other LLMs?\n2. Is there no scope for feature engineering in this approach for sarcasm detection?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6477/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6477/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6477/Reviewer_XmYa"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6477/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699539842925,
        "cdate": 1699539842925,
        "tmdate": 1699636724999,
        "mdate": 1699636724999,
        "license": "CC BY 4.0",
        "version": 2
    }
]