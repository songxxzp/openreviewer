[
    {
        "id": "hTq1RPbkNo",
        "forum": "qPloNoDJZn",
        "replyto": "qPloNoDJZn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6827/Reviewer_81X4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6827/Reviewer_81X4"
        ],
        "content": {
            "summary": {
                "value": "This work proposes to find a linear combination of training-free metrics to boost the performance on NAS tasks. Specifically, the authors first train a GP to capture the relationship between weights of training-free metrics and the objective evaluation metric f and obtain a robust estimation metric $M^*$. Then the authors collect the queries during the training procedure of BO as $Q_T$. Finally, the authors utilize the learned $M^*$ as a performance estimator and adopt the greedy search to obtain the best architecture."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe motivation, that using a linear combination of existing training-free metrics to obtain a robust estimation metric $M^*$, makes sense.\n\n2.\tExperiments on NAS benchmarks show the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1.\tThe authors propose to train a BO to capture the relationship between the weight vector and the objective evaluation metric f. However, the queried architecture should be trained from scratch to obtain the objective evaluation during the BO stage, which seems to require large amounts of search costs since a standard BO procedure usually requires tens of queries.\n\nBTW: What does $R_f(A)$ denote in Eq. 1? Does it represent the objective evaluation of an architecture? Since Alg.1 directly uses $f$ to denote the objective evaluation metric, I suggest the authors utilize the same notation.\n\n2.\tI wonder about the effectiveness of the searched robust estimation metric $M^*$. According to Fig. 2, it seems that the optimal architecture has been found in less than 10 queries during the BO procedure. It shows that there is no need to conduct the greedy search through $M^*$, and BO is enough to get the optimal architecture.\n\n3.\tTable 4 shows that RoBoT only requires 0.6 GPU-day to search, does it only count the search cost of the greedy search procedure? I wonder what is the cost of the BO stage, which I am afraid is much larger."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6827/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826391778,
        "cdate": 1698826391778,
        "tmdate": 1699636789688,
        "mdate": 1699636789688,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "G9CeaD9lOg",
        "forum": "qPloNoDJZn",
        "replyto": "qPloNoDJZn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6827/Reviewer_GJPv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6827/Reviewer_GJPv"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to tackle the research gap, the difference between training-free metrics with the final performance. This paper however, propose a weighted linear combination of traditional training free metrics as a estimator, where the weights are obtained automatically via Baysian optimization. Interestingly, this work use partial monitoring theory to prove their method has theoretical performance guarantee. Experiments are conducted on NASBench201."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Propose theory seems interesting"
            },
            "weaknesses": {
                "value": "This paper does not read like an academic paper, where the introduction did not cover the full story. Their related work is quite short to cover the existing literature. I suggest the authors try to read more papers in this field instead of submitting their paper in a rush. Results on NASBench201 shows an incremental improvement without realistic benchmarking their method's performance."
            },
            "questions": {
                "value": "I found the author regularily let the reader \"see later section\" in their introduction, including Section 3 and section 4. I think this is a not a professional way to write the introduction. We should at least grasp the main idea when reading the intro but instead reading the entire paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6827/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699196802078,
        "cdate": 1699196802078,
        "tmdate": 1699636789554,
        "mdate": 1699636789554,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lMPnIwQ3A9",
        "forum": "qPloNoDJZn",
        "replyto": "qPloNoDJZn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6827/Reviewer_3buk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6827/Reviewer_3buk"
        ],
        "content": {
            "summary": {
                "value": "This work introduces RoBoT, an algorithm for robustifying and boosting training-free neural architecture search (NAS). Motivated by the inconsistent performance estimation of existing training-free NAS metrics, this work proposes to explore a linear combination of multiple metrics that is more robust than each single metric, and exploit the robustified metric combination with more search budgets. The overall framework includes two stages. The first exploration stage employs Bayesian optimization (BO) to find the best linear combination weights for the robust metric. Then, in the second exploitation stage, the remaining search budgets are used to investigate the top-scoring architectures given by the robust metric. The proposed algorithm, RoBoT, is supported by both theoretical and empirical results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This work is built on existing training-free NAS methods, and extends them to a robustified ensemble. Therefore, the proposed framework is promising for future extension when better training-free NAS methods are discovered.\n\n- Theoretical analysis is provided to understand the proposed algorithm, RoBoT.\n\n- Extensive and solid experiment results on various datasets and settings are provided to demonstrate the efficacy of RoBoT."
            },
            "weaknesses": {
                "value": "- Missing details regarding robust metric: It seems that some important details about the BO-searched robust estimation metric are missing. What are the base training-free metrics considered in the search? What are the optimized linear combination weights for them? Do they significantly differ on different datasets/tasks?\n\n- Recent NAS methods: It is suggested to include some more recent NAS methods into the comparison, e.g., Shapley-NAS [1], $\\beta$-DARTS [2].\n\nDisclaimer: Although I know BO and NAS literature, I\u2019m not familiar with the theoretical background in this work. Therefore, I cannot provide helpful feedback on the theoretical part. I would like to read how other reviewers think about the theoretical results.\n\n[1] Han Xiao, Ziwei Wang, Zheng Zhu, Jie Zhou, Jiwen Lu. Shapley-NAS: Discovering Operation Contribution for Neural Architecture Search. In CVPR, 2022.\n[2] Peng Ye, Baopu Li, Yikang Li, Tao Chen, Jiayuan Fan, Wanli Ouyang. $\\beta$-DARTS: Beta-Decay Regularization for Differentiable Architecture Search. In CVPR, 2022."
            },
            "questions": {
                "value": "- In Table 3, why are the results on TransNAS-Bench-101 presented as the validation ranking? It seems to be inconsistent with the accuracy/error in the other two datasets (Tables 2 and 4). Also, the search costs are not listed in Table 3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6827/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6827/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6827/Reviewer_3buk"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6827/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699229630159,
        "cdate": 1699229630159,
        "tmdate": 1700529048792,
        "mdate": 1700529048792,
        "license": "CC BY 4.0",
        "version": 2
    }
]