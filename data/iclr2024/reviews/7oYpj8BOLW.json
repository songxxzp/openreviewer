[
    {
        "id": "74EJtmXqv8",
        "forum": "7oYpj8BOLW",
        "replyto": "7oYpj8BOLW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7542/Reviewer_SPdM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7542/Reviewer_SPdM"
        ],
        "content": {
            "summary": {
                "value": "The goal of this paper is to evaluate the resilience of modern vision and multi-modal foundation models against object-to-background context variations. The authors developed a pipeline that can change the background of real images while preserving the foreground semantics. The generated data allows a in-depth analysis of the robustness of vision models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work focuses on an important topic about the robustness of vision and vision-language models to object-to-background context. Such benchmarks allow us to analyze the limitations of existing models.\n2. The method is built on pretrained models, such as SAM, BLIP2 and Stable Diffusion. Making use of these pretrained models for synthetic data generation/perturbation are interesting."
            },
            "weaknesses": {
                "value": "1. The generation pipeline is built on existing methods. The whole method is not entirely new. It seems that the only changes being made are appending some pre-defined descriptions of the background. Figure 3 clearly presents a more diverse backgrounds than other examples in the supplementary results. I believe there\u2019s more to explore how to generate diverse backgrounds other than randomizing seeds.\n2. The title discusses the robustness of vision language models while most experiments focus on classification and detection, and there\u2019s only one real multi-modal task, image captioning.\n3. Latest transformer/CNN models should be considered, such as DeiT and ConvNeXt.\n4. There are limited analyses of the experimental results. A lot of results are presented but what are the conclusions?\n5. References to SAM and BLIP-2 should be added in Section 3.2.\n6. The presentation of the paper can be improved. Some technical details are not very clear in Section 3.2 (see Question 1) and it\u2019s unclear what changes have been made other than directly applying the existing models (see Question 2). Also the presentation of Section 4 is a bit messy to me. A lot of experimental settings and results are presented while there is only one sub-section \u201c4.1 Results\u201d."
            },
            "questions": {
                "value": "1. What \u201cchanges\u201d are made from $\\mathcal{T}$ to $\\mathcal{T}\u2019$? Just appending prompts about the background in Table 6?\n2. In Section 3.2, are there any changes been made besides applying the inpainting diffusion model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698559512002,
        "cdate": 1698559512002,
        "tmdate": 1699636911983,
        "mdate": 1699636911983,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MFd4VManGY",
        "forum": "7oYpj8BOLW",
        "replyto": "7oYpj8BOLW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7542/Reviewer_RPSV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7542/Reviewer_RPSV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an architecture to generate various images to evaluate the resilience of vision models. Specifically, the architecture proposed in this paper properly utilizes modern image-to-text, text-to-image, and image-to-segment models to generate various versions of an image with different backgrounds with same key object. \n\nThe experimental results included in this paper shows robustness  of various vision models against background changes, which show how a vision model understands images similar to humans."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The novelty of this paper comes from its efficiency and effectiveness. Since evaluation sets gathered from wild contains hidden correlation between the object and background inside an image, models trained with images-in-wild tend to rely on this correlation while humans don't. \n\nAs shown in experimental results, evaluating each vision model on various settings effectively tells the robustness of each model. This is where another strength of this paper comes from. While previous works focus on specific settings, the coverage of proposed method is broad."
            },
            "weaknesses": {
                "value": "While this paper shows broad coverage over robustness on object-background, I would say this cannot lead to the conclusion that a vision model is robust like humans. Further suggestion on possible future work will help readers understand the possibility of future development."
            },
            "questions": {
                "value": "As mentioned in weakness section, any suggestion on research direction will further promote future works in this area. \nI also have a small concern on novelty of this work since the architecture proposed in this work can be seen as a mixture of existing methods. More justification on novelty of proposed architecture will strengthen the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698767375568,
        "cdate": 1698767375568,
        "tmdate": 1699636911873,
        "mdate": 1699636911873,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "h3C7D9hdce",
        "forum": "7oYpj8BOLW",
        "replyto": "7oYpj8BOLW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7542/Reviewer_B18W"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7542/Reviewer_B18W"
        ],
        "content": {
            "summary": {
                "value": "This work reports results on examining the resiliency to background change of standard image classification models. The background changes are generated by modifying object images through masked diffusion model inference. An input image is first segmented with the prompt of its corresponding object class. An image captioning model also generates a textual description for it. Then the textual description is altered to another textual prompt that aims to change the scene/background of the image. The diffusion model, trained for inpainting tasks, takes the altered textual prompt, the input image, and the segmentation mask to generate a new image with altered background pixels. The image classification models are tested on these generated images for their classification accuracy. \n\nResults show that tested image classification models, either Transformer or CNN-based, show accuracy drops on the images with altered backgrounds. Object detection and instance segmentation seem to still function properly on these images."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The initiative to use the diffusion model, with its strong image generation capacity, as a tool to further study the background problem in image classification models could be interesting to the ICLR audience. \n\n- The pipeline of image manipulation seems reasonable, judged from the textual description. \n\n- The results can support the conclusions from the previous papers that image classification models are in general overfitted to the background of the object while object detection models suffer less from this problem."
            },
            "weaknesses": {
                "value": "- My primary concern about the study, while already collaborative, is that there seems to be a lack of further analysis. For example, we do not know whether the image alteration process will create additional object information that will confuse the classification. Image classification models are only allowed to make one prediction for each image. Thus, any new visual objects created during the image manipulation could easily confuse them. To rule out this hypothesis, a causal analysis of the classification errors might be necessary. There could be other hypotheses that need to be ruled out, too."
            },
            "questions": {
                "value": "Please see the weakness section for the concern I have. My questions would be:\n\n1) what are the potential \"outside factors\" could mislead us in the experimental results?\n2) the term resiliency to object-to-background seems solely measured by the accuracy drop of classification models on images altered with the proposed method. Is this metric well-calibrated? Will there be another metric that can quantify this effect? Does this metric correlate to the resiliency of models against other background change processes? I am eager to hear the authors' responses to these questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822965223,
        "cdate": 1698822965223,
        "tmdate": 1699636911751,
        "mdate": 1699636911751,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "n6kIoWbLPN",
        "forum": "7oYpj8BOLW",
        "replyto": "7oYpj8BOLW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7542/Reviewer_ATH5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7542/Reviewer_ATH5"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new benchmark for evaluating the resilience of current vision and vision-language models to object-to-background context on real images. The proposed BackBench utilizes the capabilities of image-to-text and image-to-segmentation foundational models to preserve the semantics and appearance of the object while adding diverse background changes in\nreal images through textual guidance of the diffusion model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This article is well-written and easy to understand."
            },
            "weaknesses": {
                "value": "1. This article lacks innovation, and the proposed framework is a combination of existing methods, such as SAM, BLIP-2, and DDIM.\n2. This article does not compare with the current SOTA method of \radversarial and counterfactual manipulations.\n2. An additional SAM model is used, which may not be a fair comparison with the SOTA methods."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699462151927,
        "cdate": 1699462151927,
        "tmdate": 1699636911361,
        "mdate": 1699636911361,
        "license": "CC BY 4.0",
        "version": 2
    }
]