[
    {
        "id": "a7VttbC4kg",
        "forum": "7GkdjhupsV",
        "replyto": "7GkdjhupsV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4826/Reviewer_A3xm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4826/Reviewer_A3xm"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a new method for augmenting the pool of positive samples in self-supervised learning (SSL). In particular, the authors make use of videos, that if two patches p1, p2 in the first frame of a video consistently appear together in later frames, they are considered to be a pair of positive samples. The consistency between p1, p2 is measured by the mutual information between their locations in different frame, which itself can be estimated accurately using classic methods. The newly generated positive samples will eventually be used in addition to conventional positive samples in SSL learning. Different projection heads are used for the old and the new positive samples respectively. Experiments on 3 datasets show that the proposed method improves the performance of 7 SSL methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is overall well written and easy to follow;\n- The method to generate additional positive samples from videos is novel. The idea to treat the whole video as the empirical population of image patches (and to further extract statistical dependence from this population) is also quite interesting;\n- The empirical evaluation covers most mainstream SSL methods (up to seven) and three widely-used datasets (CIFAR-10, CIFAR-100, STL-100). Ablation studies regarding the effect of the newly introduced video-based positive samples, the number of training samples and the number of training epochs are conducted and reported carefully;"
            },
            "weaknesses": {
                "value": "- **On twin pairs of video patches** There is, in my opinion, some room for simplification/improvement in the proposed approach. According to the proposed method, patches that are most statistically dependent will be considered to be a pair. Then why not simply take adjacent patches to form a pair, as adjacent patches are more likely to be dependent. Also adjacent patches are visually more consistent/similar. This is conceptually much simpler than your MI-based approach. In this regards, I am also curious about what are the twin patches found in the method. Are they indeed adjacent patches? If not so, an example or a remark will be very helpful and insightful;\n\n- **Applicability of the method**. While the proposed method consistently brings improvement in diverse SSL methods, it relies on high-quality video datasets to work well. As the authors themselves mention, suitable video dataset may not always be available, and in fact this is the reason behind not testing on larger dataset widely used in other SSL literature e.g. ImageNet. This to some extent makes the method not so general as compared to other methods, despite its novel idea to take additional samples from multiple dataset.\n\n- **Complexity**. Due to the use of additional dataset and video processing model (e.g. TAPIR), the overall complexity of the method is also high compared to some other well-known baselines (e.g. SimCLR, Barlow Twins), which is conceptually simpler. It also gives me a sense that the supreme performance of the method is due to the use of external dataset rather than algorithmic/theoretic innovation."
            },
            "questions": {
                "value": "- See the section `On twin pairs of video patches\u2019 above.\n- How do one typically determine the size of the patches in the video framing processing stage? Is it the same as in the main dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4826/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4826/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4826/Reviewer_A3xm"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4826/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698251697643,
        "cdate": 1698251697643,
        "tmdate": 1699636466224,
        "mdate": 1699636466224,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wwIPqHgwPx",
        "forum": "7GkdjhupsV",
        "replyto": "7GkdjhupsV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4826/Reviewer_xZ59"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4826/Reviewer_xZ59"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces InfoAug, a novel data augmentation technique that identifies positive pairs of patches in a video frame by estimating their mutual information using off-the-shelf tracking models. A dual branch is utilized to handle the mutual-information-guided positive pairs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The concept of identifying positive patches is interesting.\n- Using estimated mutual information to find positive patches seems reasonable.\n- Experiments show the effectiveness of the positive patches selection and the dual branch."
            },
            "weaknesses": {
                "value": "- The experiments are primarily conducted on small datasets and employ small backbones like ResNet-18, which may limit the generalizability of the results.\n- Fairness of Comparisons: The comparison with other methods could be improved. Specifically, the use of a 3-layer head instead of the standard single-layer might lead to skewed results. Additionally, the direct comparison of InfoAug with other methods is not fair due to the employment of 2 branches in InfoAug. The improvements over the \"random twin patch\" are marginal and sometimes worse, as seen in Tables 1 and 2.\n- Training Epochs: The models are trained for only 100 epochs, which might not be sufficient for convergence. When more epochs are used, as shown in Table 5, the improvements seem to diminish. Again, it is not fair to compare them directly."
            },
            "questions": {
                "value": "- Patch Extraction: Could the authors clarify how patches are extracted? Are they overlapping or non-overlapping? Are they selected from specific regions or chosen randomly?\n- Application to Image Datasets: How does InfoAug select positive pairs from video datasets and apply this knowledge to image datasets? Is the model first pre-trained on video datasets and then fine-tuned on image datasets, or are twin patches retrieved from video datasets during training on image datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4826/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4826/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4826/Reviewer_xZ59"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4826/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698804726659,
        "cdate": 1698804726659,
        "tmdate": 1699636466131,
        "mdate": 1699636466131,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HR169GwfzB",
        "forum": "7GkdjhupsV",
        "replyto": "7GkdjhupsV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4826/Reviewer_m3jm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4826/Reviewer_m3jm"
        ],
        "content": {
            "summary": {
                "value": "The rough idea of the proposed method is to construct a video-based dataset that can be applied to unsupervised image representation learning. In specific, for a given patch, mutual information is estimated between patches via applying a pre-trained tracking module. Then, for every patch, the similar patch with the highest mutual information is selected as a twin patch. Then, the assigned twin patch is regarded as a pseudo-positive pair on the contrastive learning scheme. Authors experiment the efficacy of proposed method on the CIFAR-10/100 and STL dataset with integration to existing self-supervised learning algorithm."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The idea of extracting patch-wise information in the video dataset and applying to benefit 2d dataset training is intruiging.\n2. The proposed loss function is simple"
            },
            "weaknesses": {
                "value": "1) My biggest issue in this paper is the significance of the result. The linear probing result on CIFAR-10/100 is strictly underwhelming, given that conventional self-supervised learning result on the given dataset approaches around 92.6% in CIFAR-10 and 70.5% in CIFAR-100 (see [1]). In contrast, BYOL in the author's code shows 60.5% in CIFAR-10 and 30.3% in CIFAR-100.\\\n2) Due to this result, I am puzzled as to why we should look at this \"pre-training on the video dataset\". The method seems like an underwhelming training strategy albeit using additional data.\\\n3) Thereby, I suggest the authors redesign the experiment by pre-training on a much larger dataset and show better performance on such CIFAR-10/100.\\\n4) The paper has some typos (utual information, (z1,z1) before equation (4), etc...).\\\n5) Furthermore, consider incorporating other models (e.g. CLIP) that can be applied in a zero-shot manner. \n\nIn summary, I am severely concerned about the validity of the proposed pre-training approach since the result in Table 1 is very underwhelming and far from the recent numbers. Thereby, I lean to rejection.\n\n\n\n***References***\n[1] Unsupervised Visual Representation Learning via Mutual Information Regularized Assignment, NeurIPS 2022"
            },
            "questions": {
                "value": "See Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4826/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820729989,
        "cdate": 1698820729989,
        "tmdate": 1699636466059,
        "mdate": 1699636466059,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YEKRoC1Htk",
        "forum": "7GkdjhupsV",
        "replyto": "7GkdjhupsV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4826/Reviewer_LYQ4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4826/Reviewer_LYQ4"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes InfoAug, a contrastive learning framework that employs a novel way of selecting positive pairs. Besides positive pairs that are generated by augmentations, InfoAug also selects the \"twin patch\" which maximizes the mutual information of the original patch as another positive patch. Experiments show that when trained on a video dataset and evaluated on CIFAR-10, STL-10, and CIFAR-100, the proposed InfoAug brings 1-2% improvement over existing contrastive learning methods."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper proposes a novel way of selecting positive pairs in contrastive learning, which is to select a twin patch that maximizes mutual information between the two pairs. Such a design proposes a new direction in utilizing video datasets in contrastive learning."
            },
            "weaknesses": {
                "value": "Overall, the major problem of this paper is the weak experimental results. The experiment results are weak in two aspects:\n1. The accuracy of baselines on evaluated datasets is low. For example, when trained and tested on CIFAR-10, SimCLR can achieve >90% accuracy. However, in this paper, when trained on DAVIS20+GMOT40 and tested on CIFAR-10, the performance is only 60%-70%. I would suggest including CIFAR-10 as a pseudo-video dataset in the pre-training stage so that the paper can make a fair comparison with current methods on these datasets.\n2. The improvement of the proposed method is marginal. For example, when the accuracy on CIFAR-10 is 60%-70%, the standard deviation can be large, but the proposed method only improves the performance by 1-2% on each dataset. This makes the improvement shown in the paper not convincing enough.\n3. The paper does not include an analysis of computation overhead over baseline methods. Estimating the mutual information between two patches can introduce some computation overheads, making the framework slower than baseline methods.\n\nThe presentation of the paper is also not very clear and needs further polishing. There are multiple typos and grammar errors, and the font in the figures is too small."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4826/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699117438669,
        "cdate": 1699117438669,
        "tmdate": 1699636465957,
        "mdate": 1699636465957,
        "license": "CC BY 4.0",
        "version": 2
    }
]