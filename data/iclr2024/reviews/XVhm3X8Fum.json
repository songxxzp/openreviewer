[
    {
        "id": "arLLsW5sCB",
        "forum": "XVhm3X8Fum",
        "replyto": "XVhm3X8Fum",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6568/Reviewer_bkRf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6568/Reviewer_bkRf"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the problem of the lack of modeling the hierarchical structure of input sequences in transformers. In response, a new attention mechanism, called stack attention, is proposed. The idea is to frame the sequence modeling problem as a task of running a PDA on the input vectors; that is, each time we take a vector as input, we update the stack of the PDA and produce a new vector as output. In this way, the sequence of input vectors is represented as a sequence of output vectors. The entire process is based on differentiable states and operations. Thus, this attention model operates in the same manner as those in sequence modeling, making it easy to incorporate the model into transformers. However, as a side effect, stack attention introduces recurrence into modeling, thus preventing training from being parallelized across the sequence. The stack attention models are tested on synthetic data generated by context-free grammars. Experimental results show that nondeterministic stack attention models surpass standard transformers and achieve better results than a strong nondeterministic stack RNN baseline. The stack attention models also show promising results on small-scale machine translation and language modeling tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I like this work! Given that language structure is not explicitly modeled in current Transformer models, this work opens a door to a new approach to considering hierarchical patterns in modeling languages. The design of the model is simple and elegant. The experiments support the claims well."
            },
            "weaknesses": {
                "value": "I have no major concerns, but a few comments.\n\nThe state of the stack attention models at a given timestep depends on its past state. This means the models share similar drawbacks and merits with recurrent models like RNNs. Compared to the self-attention used in standard Transformers, stack attention is slower for training because it processes one token at a time, rather than parallelizing the encoding process over the entire sequence. The author states in the appendix that this could be improved using the parallel prefix sum method, but no details are presented.\n\nA related problem is that the experiments here are small-scale. While it's fine to test the models on synthetic data for CFL tasks, the results on language modeling and machine translation aren't comparable to those in other papers. I understand that computational cost is a concern. However, to demonstrate the superiority of stack attention, it's necessary to compare it with previously reported results under the same setup.\n\nI\u2019m not quite satisfied that the models are motivated by handling hierarchical structure behind languages but there is no discussion on what structure is captured. A simple way to examine this is to design probing tasks to see how much syntax is modeled in stack attention and to see how the learned syntax differs from human-annotated syntax. Unsupervised learning of syntactic structures can offer new insights into modeling natural languages.\"\n\nThere have been previous studies on extending standard attention models to hierarchical models, such as hierarchical attention and selective attention. These should be considered baselines for comparison, either in related work or experiments."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6568/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698591466746,
        "cdate": 1698591466746,
        "tmdate": 1699636744474,
        "mdate": 1699636744474,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AXUa23JrlN",
        "forum": "XVhm3X8Fum",
        "replyto": "XVhm3X8Fum",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6568/Reviewer_knG9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6568/Reviewer_knG9"
        ],
        "content": {
            "summary": {
                "value": "The authors present work on stack attention for transformers that attempts to address naturally hierarchically structured problems. They carefully present scaled dot-product attention, a core component of the transformer architecture. Next, they provide background on differentiable stacks, superposition stacks, and a non-deterministic stack -- the differentiable vector push-down automaton. Superposition stacks can be seen as a special case of this. This is used to replace scaled dot-product attention.\n\nThey explore the empirical performance of this approach on a range of tasks, including constructed languages, a small scale language modeling problem, and a small scale machine translation effort. In some settings, the Tf-Nd configuration outperforms a conventional transformer architecture. However, these gains are not huge, the datasets are small, and (in machine translation) as the dimensionality increases, the architecture no longer shows gains."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors carefully present their formalism, including details and relevance connections to prior work.\n\nThe description of the method stands alone reasonably well, though familiarity with related work makes the paper much more accessible.\n\nThe authors evaluate in a range of settings, from constructed language to actual use cases.\n\nThe methodology does not rely on a specific grammar or automaton structure; instead it is latent."
            },
            "weaknesses": {
                "value": "The evaluation settings are quite small by modern standards.\n\nGains (at least in machine translation) seem to disappear as model size increases.\n\nComputational cost at inference seems greater according to the big-O runtimes -- is this a net win? Is it feasible to train at larger scale?"
            },
            "questions": {
                "value": "What is the empirical cost of running these methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6568/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821714363,
        "cdate": 1698821714363,
        "tmdate": 1699636744360,
        "mdate": 1699636744360,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "asOvuWvqk6",
        "forum": "XVhm3X8Fum",
        "replyto": "XVhm3X8Fum",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6568/Reviewer_ecSr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6568/Reviewer_ecSr"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a stack-augmented transformer to help overcome some of the challenges of ordinary self-attention in modeling nested syntactic structure. \n\nThe overall approach is to propose two different stack mechanisms: the \"superposition stack\" which is an extension of Joulin and Mikolov (2015), and a second non-deterministic diferentiable vector pushdown automata (dVPDA). While the details for the dVPDA are not very clear from the methods section (see Weaknesses and questions), both the dVPDA and the superposition stack provide soft-stack vector readouts at each time-step, and can be used to replace standard attention. \n\nFrom results, we find:\n- On formal languages such as Dyck, 5 layer transformers with the stack based attention (coming from the superposition stack) are better than ordinary 5 layer transformers (though both of these are worse than LSTM variants). \n- On language modeling over the penn treebank, we find that transformers + dVPDA obtain lower perplexities than ordinary transformers (though transformers + superposition stack are much worse). \n- On a 5 layer machine translation dataset, we find slight very mixed improvements in BLEU."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The motivation behind this paper is great - there are clear limitations of self-attention w.r.t. modeling syntactic patterns and here, we see a novel approach to use a stack to model such patterns."
            },
            "weaknesses": {
                "value": "- Unfortunately, I think the results are very mixed, experiments are done on very small 5 layer transformers on small datasets.\n- Even for the positive results, there is no analysis of why the stack augmented model may be doing better on natural language - is it discovering good parses / something else?\n\n-  The exposition is very confusing, and i'm a bit lost on various details. The biggest missing detail is how training is done in parallel - from Eq 8, 9 ..., 17 and Figure-1 it seems like there is a stack state that is recurrently updated, but transformer training is fully parallel, so how can state information be passed between different tokens? Is this done by basically reconstructing the previous state of the stack since all previous actions are available to the model at each time step - if so what is the FLOP hit from doing this?"
            },
            "questions": {
                "value": "I'll intersperse questions (Q) with some suggestions for improving writing (S)\n\nIntroduction:\n\nQ1: \"Recent work has shown that transformers have linear rather than hierarchical..\": Is this true? Murty 2023 (\"Grokking\") find that transformers acquire hierarchical bias when trained for long. It would be better to qualify this statement somewhat.\n\nS1: \"on a natural language modeling benchmark\"  => would be better to just say \"Penn TreeBank\". In general, I found the last para to be very vague. It would be better to have concrete numbers and dataset names.\n\nRelated Work:\n\nS2: Missing several keys papers (Ordered Memory, Unsupervised Tree LSTMs, RL-SPINN) and early works on incorporating stack mechanism into transformers (Das, 1993). This is just a quick list, but there is a long history of learning syntax unsupervisedly / augmenting neural models with stacks that is completely missing.\n\nBackground Section 3.2.1:\n\nQ3: This method seems like it would run faster than the method from 3.2.2. Could the authors confirm this?\n\nQ4: In natural language, one might want to do multiple reduce operations after emitting a word. How does this approach allow for multiple reduces at each time step? \n\nBackground Section 3.2.2:\n\nS3: Definition 1 and the next paragraph take too much space and seem like background that could be in an appendix. \n\nQ5: Missing detail: What is the time complexity of Eq. 17 -  Please include pseudocode here.\n\nQ6: \"Each $a_t$ is a flattening of tensor...\": How was the size of the tensor $\\Delta_t$ computed here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6568/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699125678114,
        "cdate": 1699125678114,
        "tmdate": 1699636744142,
        "mdate": 1699636744142,
        "license": "CC BY 4.0",
        "version": 2
    }
]