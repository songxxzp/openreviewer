[
    {
        "id": "cJmU342CNK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6493/Reviewer_cCJK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6493/Reviewer_cCJK"
        ],
        "forum": "aLXRYfIUUd",
        "replyto": "aLXRYfIUUd",
        "content": {
            "summary": {
                "value": "This work introduces a benchmark called SCALE, which consists of seven legal tasks sourced from the Swiss legal system.  On this benchmark, the authors evaluate a wide range of LLMs, including black-box models, open-source models, and the one tuned via in-domain data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. the dataset is unique and definitely interesting to the LLM community and people in related domains.\n2. it includes multiple distinct and challenging tasks\n3. The experiments are extensive and cover many recent models"
            },
            "weaknesses": {
                "value": "1. The title is misleading, as a dataset for a specialized domain, the title should state its scope clearly. I think it is of great importance for a serious research paper to have appropriate title.\n2. Related to the above scope issue, in the abstract (as well as the main body of the paper), the authors state that we need more challenging tasks for LLM, then why legal tasks specifically? To accomplish the goal of proposing more challenging tasks, why not use data from domains like finance, medical, etc?\n3. This work includes extensive experiments, but it could be better to include more analysis, discussion and takeaways"
            },
            "questions": {
                "value": "Citation format in the first sentence of section 5 is incorrect."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6493/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697234486553,
        "cdate": 1697234486553,
        "tmdate": 1699636727991,
        "mdate": 1699636727991,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "t2QKwRZoXL",
        "forum": "aLXRYfIUUd",
        "replyto": "aLXRYfIUUd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6493/Reviewer_pqFy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6493/Reviewer_pqFy"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to introduce a benchmark of model performance across classification, text generation & information retrieval tasks on legal datasets. The datasets consist of long documents that are multilingual in nature. The models benchmarked in the study include LLMs and some (smaller)  models such as mT5 and XLM-R that were fine-tuned on domain-specific data. The results demonstrate the variability in performance that LLMs yield for different tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is clearly written and highlights the variability in performance of LLMs across tasks, especially when applied to domains that they may not have been exposed to during training. It is interesting to see, for example, that smaller fine-tuned models (XLM-R or RoBERTa based models) outperform off-the-shelf LLMs for text classification tasks across all the legal datasets used in this paper.\nAdditionally, the experiments were detailed and clearly explained. \nMoreover, the inclusion of the observant ethics statement is highly commendable."
            },
            "weaknesses": {
                "value": "Although construction of a benchmark for better evaluation of LLMs along specific dimensions is of importance, I was unable to determine what the novelty of this work is, w.r.t. other already existing NLP benchmarks. Benchmarks for LLM evaluation already exist both for the legal domain [e.g. LEXTREME (https://arxiv.org/pdf/2301.13126.pdf), LexGLUE (https://aclanthology.org/2022.acl-long.297.pdf), LegalBench (https://arxiv.org/pdf/2308.11462v1.pdf) etc.] and otherwise [BigBench (https://arxiv.org/pdf/2206.04615.pdf), HELM (https://arxiv.org/pdf/2211.09110.pdf), etc]. The current work expands on these benchmarks in terms of including legal datasets specific to the Swiss legal system, which do not meet the standards of an ICLR paper, in my opinion.\nFurther, if the focus is on evaluating LLMs, it would be important to include more LLMs in the zero-shot & one-shot settings. These could include Falcon, Flan-T5 XXL, Alpaca, Vicuna etc. This would allow for a wider coverage of the behavior of LLMs on the tasks at hand."
            },
            "questions": {
                "value": "It would be great if you could highlight the key, novel contributions of the paper in comparison to the already existing benchmarks for LLMs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The ethics section is very well-written in the paper already."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6493/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698717670519,
        "cdate": 1698717670519,
        "tmdate": 1699636727870,
        "mdate": 1699636727870,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Nma5s6Unu5",
        "forum": "aLXRYfIUUd",
        "replyto": "aLXRYfIUUd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6493/Reviewer_xb8g"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6493/Reviewer_xb8g"
        ],
        "content": {
            "summary": {
                "value": "This study introduces a comprehensive evaluation dataset for large language models (LLMs) focusing on the legal domain. The dataset is sourced from Swiss legal documents and comprises seven multilingual datasets covering four key dimensions: long documents, specificity to the Swiss legal domain, multilinguality, and multitasking. The authors further conduct in-domain pretraining to develop Legal-Swiss-RoBERT and Legal-Swiss-LF models specifically tailored for this domain. The proposed evaluation includes seven tasks (LAP, CP, IR, CVG, JP, CP, and LDS) forming a testbed to assess the performance of existing LLMs in the legal domain."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This study introduces an evaluation dataset specifically designed to assess the performance of large language models (LLMs) in the legal domain. The dataset emphasizes 4 challenging dimensions that pose difficulties for LLMs, thereby providing a comprehensive and rigorous evaluation for LLMs operating within legal fields.\n\n2. The quality of the research is substantiated by rigorous experimental design. The authors conducted experiments to examine and analyzed the performance of existing pre-trained language models (LMs) in legal fields. Furthermore, the authors showcased the significance and worth of their collected dataset by fine-tuning LMs on it. The multilingual nature of the dataset adds an additional layer of complexity to the evaluation of language models."
            },
            "weaknesses": {
                "value": "About the experimental setup: it is advisable to expand the inclusion of additional existing large language models (LLMs) in the experiments. Given the lengthy nature of legal documents, it would be beneficial to evaluate the performance of LLMs specifically pre-trained for handling long contexts. It is recommended to conduct more comparisons with LLMs specifically designed to handle long contexts. Furthermore, it appears inequitable to compare models with input length restrictions imposed by fixed-sized tokens. It is also desirable to extend the proposed method to encompass a wider range of LLMs."
            },
            "questions": {
                "value": "1. Please address the weaknesses above. \n\n2. In text classification tasks, the models are provided with facts and considerations explicitly written by legal professionals such as lawyers or judges. This simplification reduces the evaluation complexity of language models (LMs). What are the underlying reasons for adopting this simplified approach and reducing the complexity in LM evaluation?\n\n3. In Table 3, the majority of models exhibit superior performance on the \"-C\" datasets compared to the \"-F\" datasets. However, BLOOM, Legal-Swiss-RoBERTa_{Large}, and Legal-Swiss-LF_{Base} demonstrate relatively poorer performance. What could be the potential reasons behind the clearly weaker performance of these models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6493/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815041513,
        "cdate": 1698815041513,
        "tmdate": 1699636727757,
        "mdate": 1699636727757,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lr928l9N0W",
        "forum": "aLXRYfIUUd",
        "replyto": "aLXRYfIUUd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6493/Reviewer_x9cb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6493/Reviewer_x9cb"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new multilingual legal NLP benchmark dataset, named SCALE. The characteristics of this dataset are multilingual (German, French, Italian, Romansh, and English), long documents, and multitasking. The origin of SCALE is Swiss legal documents, and the authors arrange the raw data into several text classification and generation tasks. The authors also show the performance of large language models and it shows that the benchmark is still challenging for the NLP community."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The primary strength of this paper lies in proposing a challenging and extensible benchmark dataset. It is helpful for AI and NLP researchers who are interested in the tasks."
            },
            "weaknesses": {
                "value": "The benchmark is quite interesting and sound. But I have some curiosity which I mention in the Questions. I hope to listen to the author's responses."
            },
            "questions": {
                "value": "- For each task, what is human (legal experts and non-experts) performance, especially NLG tasks? Other benchmark tasks show the human performance for the tasks that can be the upper bound or challenge for AI models.\n- How each task helps in the legal domain. For example, if you have an AI model that is good at law area prediction, how would it help?\n- Presentation: it would be better to enlarge the font size of captions. Now it is hard to read."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "In the Judgement documents, it might contain PII."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6493/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699090768018,
        "cdate": 1699090768018,
        "tmdate": 1699636727654,
        "mdate": 1699636727654,
        "license": "CC BY 4.0",
        "version": 2
    }
]