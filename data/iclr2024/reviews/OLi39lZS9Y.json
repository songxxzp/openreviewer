[
    {
        "id": "En8pGEamQd",
        "forum": "OLi39lZS9Y",
        "replyto": "OLi39lZS9Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6225/Reviewer_CT2T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6225/Reviewer_CT2T"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of in-context learning in sequential decision-making settings. The paper finds that it is important for the context to contain full trajectories to cover potential situations at deployment time. The authors provide experiments on MiniHack and Procgen benchmarks, showing the method can generalize to new tasks with just a few expert demonstrations, without weight updates. The work claims to be the first to demonstrate that transformers can generalize to entirely new tasks in these benchmarks using in-context learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well written and easy to follow.\n- The paper shows some nice experiments on the MiniHack and Procgen environments, showing how in-context learning can perform well on unseen tasks.\n- The paper's setting of in-context learning in decision-making problems is an interesting problem to study."
            },
            "weaknesses": {
                "value": "- The paper highlights the ability to perform well on unseen tasks but this actually relies on having a lot of demos from related tasks. Can the authors better clarify the relationship between the data they train on and the unseen tasks they evaluate on? \n- Novelty-wise, the method is very similar to works like Prompt-DT, except actually requires stronger data assumptions (full expert demos). \n- A lot of the insights in the empirical study are not that interesting, e.g. the paper highlights results like showing that in-context learning improves with trajectory burstiness, but it is not surprising that having demos similar to the query inside the context improves the performance. Can the authors give more clarity on what the most surprising, interesting takeaways are from the study?"
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6225/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698716369786,
        "cdate": 1698716369786,
        "tmdate": 1699636679899,
        "mdate": 1699636679899,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FgkO08RjAV",
        "forum": "OLi39lZS9Y",
        "replyto": "OLi39lZS9Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6225/Reviewer_F4ph"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6225/Reviewer_F4ph"
        ],
        "content": {
            "summary": {
                "value": "This work targets the setting of zero-shot and few-shot learning, where the train and test MDPs contain completely separate games (tasks) - in contrast to previous works where the train and test sets contain the same game but with different levels.  The paper proposes to adapt the causal transformer model to this few-shot setting by first training expert agents on each of the training tasks, and then collecting a dataset from the experts\u2019 trajectories to train the transformer model. The authors propose to train the transformer using multi-trajectory sequences rather than single-trajectory sequences and to construct the multi-trajectory training so that the context contains at least one trajectory from the same level as the query. At test time, for the few-shot setting, the transformer is conditioned on 1-7 full expert trajectories, while for the zero-shot setting the transformer is not conditioned on any expert trajectories.\n\nThe authors compare their results to two baselines: BC and hashmap, and performed an extensive ablation study containing the dataset sizes, task diversity, environment stochasticity, and trajectory burstiness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* The paper suggests a new setting that has not been studied before - to test on games withheld during training by utilizing expert policies that were trained on the training set (a separate set of games).  \n\n* The results show a clear advantage to the proposed approach over the baselines and the authors performed an extensive ablation study."
            },
            "weaknesses": {
                "value": "* The proposed approach seems as a small adaptation of pre-existing approaches, i.e. causal transformer with multi-trajectory training, to new benchmarks (MiniHack and Procgen) in the offline setting.\n\n* There is no comparison to other offline methods such as CQL [1]\n\n* It is mentioned in the paper that all the results are produced using 3 seeds. In my opinion, for such noisy benchmarks evaluating on only 3 seeds is not enough to reliably estimate the mean and variance. \n\n* The results are not clear to me - for example, in Figure 4 the episodic return is very low compared to the score reported by [2]."
            },
            "questions": {
                "value": "I would like to ask the author to address the following questions: \n\n1. For the few-shot evaluation (when testing the model): is the expert policy, which creates the few trajectories (1-7) for conditioning the transformer, trained on the test games or the training games? \n2. Are the above few trajectories (1-7) sampled from the same level as the query level? \n3. Why is the return in Figure 4 so low compared to the return reported in [2]?\n4. Is the Procgen dataset evaluated on the easy or hard difficulty mode?\n5. Are the results in Figure 3 normalized?\n\n\nA technical detail: \n* In the first paragraph of the background - /mu the initial state distribution is not defined.\n\n\n\n\n\n[1] Kumar, Aviral, et al. \"Conservative q-learning for offline reinforcement learning.\" Advances in Neural Information Processing Systems (2020): 1179-1191.\n\n[2] Cobbe, Karl, et al. \"Leveraging procedural generation to benchmark reinforcement learning.\" International conference on machine learning. PMLR, 2020"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6225/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6225/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6225/Reviewer_F4ph"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6225/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763113242,
        "cdate": 1698763113242,
        "tmdate": 1699636679770,
        "mdate": 1699636679770,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QaEs9P1mWv",
        "forum": "OLi39lZS9Y",
        "replyto": "OLi39lZS9Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6225/Reviewer_phhR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6225/Reviewer_phhR"
        ],
        "content": {
            "summary": {
                "value": "This work studies the use of transformers for generalization to new tasks from MiniHack and Procgen. Their experiments show that a model pre-trained on different levels and tasks of Procgen can learn a new task from a few demonstrations of the task. The transformer is trained with demonstration contexts that can either be from the same or different levels as the query. There is also additional empirical analysis on different variables, such as whether the context is from the same level, environment stochasticity, and task diversity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This work shows that transformers can perform few-shot imitation learning on new Procgen tasks, which has not been explicitly shown previously.\n\n- The paper is written pretty well at a low-level and tries to be thorough in its experiments through additional experiments to understand failure modes and effect of different environmental and algorithmic factors.\n\n- Given the significance of in-context learning in large language models, it seems timely and appropriate to study it in the context of decision-making.\n\n- The environment stochasticity result is pretty interesting, i.e., that the model can learn copying behavior if the training environments are deterministic."
            },
            "weaknesses": {
                "value": "- Existing papers such as Prompt-DT (Xu et al, 2022) and AdA (Team et al, 2023) have shown similar results (in some cases, with even less presumptive data than full demonstrations) though in different domains. So it's perhaps not too surprising that we see this type of generalization in Procgen as a result. The main result that models trained with demonstrations in the context can perform better than models without the demo context is also expected.\n\n- There are a couple of claims that do not seem sufficiently supported: (1) meta-RL methods \"tend to be difficult to use in practice and require more than a handful of demonstrations or extensive fine-tuning,\" (2) \"in sequential decision-making it is crucial for the context to contain full trajectories (or sequences of predictions) to cover the potentially wide range of states the agent may find itself in at deployment\" (see Questions). \n\n- The concepts of burstiness from Chan et al and trajectory burstiness have a pretty weak relation. In the case of this paper, it seems pretty clear from the get-go that demonstration contexts from the same level as the query would be more relevant than from any other levels."
            },
            "questions": {
                "value": "- What does trajectory burstiness mean for the zero-shot model in Fig. 6(a)?\n\n- \"[Meta-RL methods] tend to be difficult to use in practice and require more than a handful of demonstrations or extensive fine-tuning\" --> Including some of these comparisons in the experiments would be help support this statement.\n\n- \"Our key finding is that in contrast to (self-)supervised learning where the context can simply contain a few different examples (or predictions), in sequential decision-making it is crucial for the context to contain full trajectories (or sequences of predictions)\nto cover the potentially wide range of states the agent may find itself in at deployment.\" --> Full trajectories as opposed to what? The experiments only show comparisons between full demos vs no demos, and didn't study other potential contexts, such as partial demos or non-expert trajectories. I think a study of different potential contexts and what is required for in-context learning would be interesting.\n\n- \"This means that the agent manages to perform well on the new task even without copying actions from its context. This suggests the model is leveraging information stored in its weights during training, also referred to as in-weights learning\" --> Could you elaborate on how not copying the context actions suggests in-weights learning as opposed to ICL? This conclusion seems to equate ICL with the ability to copy context actions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6225/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818527281,
        "cdate": 1698818527281,
        "tmdate": 1699636679654,
        "mdate": 1699636679654,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hwOmf3eVtI",
        "forum": "OLi39lZS9Y",
        "replyto": "OLi39lZS9Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6225/Reviewer_qktf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6225/Reviewer_qktf"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of in-context learning for decision-making. A method of training transformers is proposed where expert demonstrations are generated across many tasks and the model is expected to predict expert behavior from this context. The method is demonstrated on procgen and nethack, two challenging RL settings.  It is shown that in-context learning can be achieved from a handful of demonstrations in order to generalize to new test tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The problem is important and interesting: the in-context abilities of transformers for decision-making problems is comparatively understudied relative to supervised learning problems. This paper contributes to a growing understanding of decision-making with transformers.\n\nThe generalization of the method to entirely new tasks in procgen and nethack is impressive as these are challenging settings and each task is quite different from the others. There are really only a handful of training tasks.\n\nThe analytical studies are thorough and mostly informative, especially the one showing how the performance varies with the number of training tasks and the one on failure modes."
            },
            "weaknesses": {
                "value": "Overall, I think this is a good paper with a thorough analysis, but there are two main weaknesses of the paper: clarity and novelty/significance.\n\nClarity: both the problem setting and the methodology of the training are not very clear and this makes it difficult to understand the significance of the results. \n\n- During testing, the agent is given a handful of expert demonstrations. Are these all demonstrations on the same task and same level? If not, how does this work for the baselines hashmap if they are using demos from different levels? If so, why is the transformers trained with several sequences of demos from different levels? Why not just train with demonstrations from the same level and task always?\n- Related to this, how do I interpret this, which suggests that all demos in the context come from the same level: \u201cwe collect offline data from 11 Procgen tasks and train a transformer on Procgen sequences compromising of five episodes from the same level.\u201d What does burstiness even mean here if the levels are never varied?\n- What does it mean for BC-1 to condition on \u2018one demonstration\u2019? Does this mean you give it full demonstration in the same task and same level? In other words, does the context look like this: [expert demo, history observed so far]. How would this be different from your method if you were just limited to training on just two sequences? \n- What is the maximal achievable reward in each of the environments? This could be helpful to better understand the final results.\n\nIt would further be helpful to distinguish the work from prior methods better. A more thorough comparison would help readers with a better understanding of the present problem setting and method.\n\n- The method appears to be very similar to Prompt-DT [1] perhaps without the return conditioning. There\u2019s already a short discussion in the related but this ought to be carefully dissected, I think.\n- The method is also very similar to DPT [2], which also considered training and conditioning on expert demonstrations to solve new tasks. If there is a difference, both of these papers seem like highly relevant baselines.\n- It is also likely worth distinguishing the method with other in-context RL works like [3] and [4].\n\nBeyond transformers, there\u2019s additional work on meta/few-shot imitation learning that could be helpful to discuss.\n\nAs a result, the overall takeaways are a bit hard to discern. It\u2019s clear now that there are multiple solid contributions in this paper (a method and a thorough analysis), but I think the takeaways could be better communicated.\n\n[1] Xu M et al. Prompting decision transformer for few-shot policy generalization. International conference on machine learning 2022.\n\n[2] Lee JN et al. Supervised Pretraining Can Learn In-Context Reinforcement Learning. arXiv preprint arXiv:2306.14892. 2023.\n\n[3] Lu C et al. Structured state space models for in-context reinforcement learning. arXiv preprint arXiv:2303.03982. 2023.\n\n[4] Laskin M et al. In-context reinforcement learning with algorithm distillation. arXiv preprint arXiv:2210.14215. 2022."
            },
            "questions": {
                "value": "See above section for specific questions. Misc:\n\n- How long are each of these sequences? I.e. what is T?\n- In what settings would you expect this method would work (or these analysis be useful) under the current assumptions, beyond gameplaying?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6225/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699237624820,
        "cdate": 1699237624820,
        "tmdate": 1699636679552,
        "mdate": 1699636679552,
        "license": "CC BY 4.0",
        "version": 2
    }
]