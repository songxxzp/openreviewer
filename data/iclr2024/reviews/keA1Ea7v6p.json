[
    {
        "id": "738Dz1090c",
        "forum": "keA1Ea7v6p",
        "replyto": "keA1Ea7v6p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1145/Reviewer_kS7r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1145/Reviewer_kS7r"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an idea of adding generated data to local client datasets in federated learning to improve local model performances. Coined under the name FedGC but the framework is basically the standard federated learning framework plus data generation for local clients. In data generation, the paper focuses on 4 aspects: budget allocation, prompt design, generation guidance an training strategy. For each aspects, the authors propose 3 simple approaches. \n\nTo show the superiority of FedGC experimentally, they created a dataset with synthesized heretogeneity by merging from a few existing datasets: CIFAR-10, EuroSAT, PACS, VLCS, Sentiment140 from LEAF benchmark and Yahoo! Answers. They tested with a few federated learning frameworks including FedAvg, FedProx and SCAFFOLD and showed that with data generation, the models performed better."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The flow of the paper is clear, straightforward and easy to read. The motivation of the problem is exciting."
            },
            "weaknesses": {
                "value": "There are a few issues in the paper.\n\n1. Other than considerations regarding generating data locally for each client (i.e. the four aspects at generating the data above), there is no significant theoretical contribution. There is no theorem, no proposal. Not a single equation is found in the paper.\n\n2. The paper solely focuses on data generation for local training. However, there is nothing in the communication among the clients that carries any information about data generation from one client to another, other than the number of samples to be generated for each client. In other words, the use of federated learning and data generation appear to be unrelated. I wonder if the whole work could have been better presented in a non-federated setting. \n\n3. In terms of data generation for each client itself, the proposed approaches for each of the 4 aspects above are simple and straightforward. I think this part of the paper is good from a practical point of view. However, it appear to be dominantly engineering contributions, which does not seem suitable for ICLR, a conference about learning representations.\n\n4. The dataset for experimenting was made up by merging from a few existing known datasets which were not designed for federated learning. Heterogeneity in the dataset was synthesized using Dirichlet distribution. This is probably one major weakness of the paper. It is questionable whether the dataset reflects real world. Results in this dataset are therefore not very convincing. I reckon the authors to use real-world datasets, or if that task is not feasible, at least use the same datasets that other federated learning approaches have used, rather than creating one of your own."
            },
            "questions": {
                "value": "FedGC seems to be a straightforward idea of using existing generative models to generate data for learning. Doesn't that mean a related generative model has to exist for a given problem? What if no generative model exists for a given problem?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1145/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1145/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1145/Reviewer_kS7r"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1145/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698061898382,
        "cdate": 1698061898382,
        "tmdate": 1700680502185,
        "mdate": 1700680502185,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7ViRqM6mdx",
        "forum": "keA1Ea7v6p",
        "replyto": "keA1Ea7v6p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1145/Reviewer_c8uL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1145/Reviewer_c8uL"
        ],
        "content": {
            "summary": {
                "value": "The paper explores whether the data heterogeneity issues in federated learning (that limits its performance) could be mitigated by adding synthetic data generated via generative models. The authors first conduct experiments on two image datasets (CIFAR-10 and EuroSAT) and two language datasets (PACS and VLCS) to show that the performance of federated learning significantly improves after combining the private training data with new data that are generated based on the guidance of prompts and private data simultaneously. Interestingly, the authors also observe reduced privacy risk in FL after adding generated data, where the privacy risk means the average success of simple loss-based membership inference attacks over different clients. Algorithmically, the authors conduct extensive experiments and attributed the success of the method to four critical choices: amount of generative contents, the same amount of generative contents for each user, using multiple prompts to guide data generation; and simultaneously use text prompts and real private data for generation. \n\nTo further understand the reason for the performance gain, the authors additionally conduct several interesting ablation studies, where the central conclusions are:\n- The performance improvement exists even when the generated data is not similar to the private data.\n- Adding generative contents reduced data heterogeneity, and under a higher amount of generative contents, the performance of FedAvg becomes better or on par with other algorithms that are designed to tackle data heterogeneity (such as FedProx and SCAFFOLD).\n- Adding generative contents reduces the client drift effect in FL."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Thorough experiments that investigate the algorithmic choices and how federated learning is affected under new generative data.\n- Mitigating the issues due to data heterogeneity in FL is an important question, and exploring how large generative models could alleviate such issues is a timely and vital direction."
            },
            "weaknesses": {
                "value": "- On the one hand, the authors show that increasing the amount of generative data always increases the learning performance (Table 2 and Figure 2). On the other hand, the authors also show that FL on only generative contents (no private data) performs poorly in Table 5. This seems counterintuitive. Could the authors explain why? \n\n- If we allow each user to train a local model on its private data combined with generative contents, would the performance be comparable to FL training on private data combined with newly generated data? If so, there would not be any incentive for clients to perform FL when they have access to additional generative content, thus deeming the problem setting as insignificant."
            },
            "questions": {
                "value": "- See weakness for two questions.\n\n- Additionally, could authors discuss the possible dataset contamination? That is, whether the benchmark dataset is already used in training dataset for the generative model."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1145/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1145/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1145/Reviewer_c8uL"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1145/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698510068519,
        "cdate": 1698510068519,
        "tmdate": 1700722294747,
        "mdate": 1700722294747,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rfngCHBeWU",
        "forum": "keA1Ea7v6p",
        "replyto": "keA1Ea7v6p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1145/Reviewer_kHC9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1145/Reviewer_kHC9"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed FedGC, a synthetic-data-based federated learning system. In high-level speaking, FedGC utilized the foundational model on the local side to generate synthetic data, and mixed the synthetic data with real private data for local training. The experiments with both language and computer vision benchmark datasets show the effectiveness of FedGC."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It is interesting to utilize the power of foundational models to assist federated learning.\n\n2. The experiment and ablation study are detailed."
            },
            "weaknesses": {
                "value": "1. The author does not consider the generation cost in the paper. The Stable Diffusion model needs at least 4.2GB space to deploy locally, and the memory consumption of generation is huge for the IoT or cross-device FL setup. For the black-box foundational models such as ChatGPT, the prompts directly leak the data privacy to the server of ChatGPT. As a result, both methods do not fit the FL setups.\n\n2. The mixed-up training of synthetic and real private data directly increases the computational burden for the local devices. \n\n3. I am concerned about the in-domain generation problem. As the Stable Diffusion is trained with the LAION-5B dataset, we cannot guarantee that the Stable Diffusion does not meet with the test data, such as the CIFAR dataset, during the training. As a result, we could not distinguish whether the performance boost-up is coming from the FedGC or the in-domain generation of the foundational models."
            },
            "questions": {
                "value": "1. How many synthetic data samples does the local client generate for the experiment in Table 1?\n\n2. During local data generation, does the local client only generate the data for the label it holds or generate the data for the whole label space among all participants?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1145/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698739187132,
        "cdate": 1698739187132,
        "tmdate": 1699636040626,
        "mdate": 1699636040626,
        "license": "CC BY 4.0",
        "version": 2
    }
]