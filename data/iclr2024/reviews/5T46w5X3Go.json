[
    {
        "id": "31q4rDZdvi",
        "forum": "5T46w5X3Go",
        "replyto": "5T46w5X3Go",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7587/Reviewer_onS3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7587/Reviewer_onS3"
        ],
        "content": {
            "summary": {
                "value": "The paper studies transfer learning under the noisy linear regression setting. The authors split the features into two distinct sets: a common part shared across tasks and a task-specific part. The paper studies two transfer learning settings, Option A: directly copy the learned common feature to the target task; Option B: use the learned common part as an initial training point. Then, considering different noise levels, the authors claim that, when the total number of features in the source task\u2019s learning model is fixed, it is more advantageous to allocate a greater number of redundant features to the task-specific part rather than the common part."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper studies an important topic. Nowadays, transfer learning is become the new standard paradigm. For example, training a foundation model by self-supervised learning and adapting the model to a specific supervised downstream task. Thus, some important topics include but are not limited to how to select features, how to select the aulixray dataset, how to adapt the models efficiently, and so on. \n\nThe paper\u2019s motivation is clear and the analysis is concise."
            },
            "weaknesses": {
                "value": "- The major concern I have is that the paper can separate the common features and task-specific features explicitly. In other words, in Option A and Option B, we can directly know that $w$ is a common feature, while $q$ is not. In practice, we do not have any idea about which part of the feature should be useful and we should try to figure out the common features and task-specific features rather than assuming we know that. From my perspective, one practical Option C should use all the learned features, e.g., $w$ and $q$  as an initial training point rather than $w$ only in Option B.\n- The problem setup is simple, not practical, and not novel. The linear regression setting seems not surprising and the transfer learning conclusion directly depends mostly on the assumption we made. The paper only considers one source supervised training task, while in practice, we mostly consider multiple source tasks [1,2] or different pretraining objectives, e.g., self-supervised learning [3,4] (no objective gap between source and target). Splitting the data feature space into common features and task-specific features is quite commonly used [4,5]. While in [4,5] they do not know which part is a common feature during pretraining and transfer learning. \n- The claim in Proposition 4 is not significant. From my perspective, it just says that we should only keep the common features in the transferred feature, where the take-home message is not interesting. I cannot get more insights or intuitions. \n- Some minor weaknesses in writing. Assumption 1 should be Definition 1. $\\mathcal{R}$ and $\\mathbb{R}$ are mixed used. \n\n[1] Nilesh Tripuraneni, Michael Jordan, and Chi Jin. On the theory of transfer learning: The importance of task diversity. NeurIPS 2020.\n\n[2] Zhao, Yulai, Jianshu Chen, and Simon Du. Blessing of Class Diversity in Pre-training. AISTATS 2023.\n\n[3] HaoChen, Jeff Z., Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-supervised deep learning with spectral contrastive loss.  NeurIPS 2021.\n\n[4] Shi, Zhenmei, Jiefeng Chen, Kunyang Li, Jayaram Raghuram, Xi Wu, Yingyu Liang, and Somesh Jha. The Trade-off between Universality and Label Efficiency of Representations from Contrastive Learning. ICLR 2023.\n\n[5] Rosenfeld, Elan, Pradeep Kumar Ravikumar, and Andrej Risteski. The Risks of Invariant Risk Minimization. ICLR 2021."
            },
            "questions": {
                "value": "In Remark 1, it is unclear what missing features mean. Providing a toy example will be good."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7587/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698526074079,
        "cdate": 1698526074079,
        "tmdate": 1699636919210,
        "mdate": 1699636919210,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "an0Kmm0OkG",
        "forum": "5T46w5X3Go",
        "replyto": "5T46w5X3Go",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7587/Reviewer_6fkR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7587/Reviewer_6fkR"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a theoretical analysis of transfer learning in the under-parameterized and over-parameterized scenarios. The paper explores two options for transfer: 1) option A, where the common parameters are copied and only the task specific parameters are trained for the target task, 2) option B, where the common parameters are initialized for the target task and both the common and task specific parameters are trained during that task. The paper presents theoretical results, along with some experiments, for the parameter transfer setting, such as when benign overfitting occurs, what are the effects of noise, and the comparison of the common vs. task-specific parameters for both options."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper presents a much needed theoretical analysis of some interesting scenarios in parameter transfer. The contributions seem original and of significance for researchers in transfer learning and related areas. \n- The paper is very well-written and structured. \n- The paper presents the ideas in a clear way for a theoretical paper. The concepts are presented in a structured manner, and some experimental results are provided to support the theoretical claims."
            },
            "weaknesses": {
                "value": "- Understandability of some key parts of the paper could be improved. For example, for \"Option A\" and \"Option B\", considering how relevant these are for the main results of the paper, perhaps more meaningful names could have been used so it is easy to grasp and associate theoretical results with what's going on in these of these \"options\"."
            },
            "questions": {
                "value": "- Is there any connection between your work and well-known theoretical studies in multitask learning? [1] Or is your analysis also applicable to multitask learning scenarios? There may be a strong connection since a lot of approaches in multitask learning also rely on the common/task-specific features paradigm.\n\n[1] Baxter, J. (2000). A model of inductive bias learning. Journal of artificial intelligence research, 12, 149-198."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7587/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698804439736,
        "cdate": 1698804439736,
        "tmdate": 1699636919100,
        "mdate": 1699636919100,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "19TwrRpNmD",
        "forum": "5T46w5X3Go",
        "replyto": "5T46w5X3Go",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7587/Reviewer_hznS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7587/Reviewer_hznS"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the generalization of transfer learning in linear regression models under both underparameterized and overparameterized regimes. This work proposes to partition the feature space into common and task-specific parts and analyze the influence of the number of parameters on the generalization performance. This paper presents the transferring errors for two types of transfer learning: the first one fixes the common features and the second one further trains the model initializes from the parameters corresponding to the common features. This paper provides two insights: First, this paper shows that allocating more features to task-specific parts benefits the target task more than allocating to the common part. Second, This paper finds that under high noise and small parameter regimes, sacrificing certain features in the common part and adding more to the task-specific part can yield better generalization."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This work proposes to partition the feature space into common and task-specific parts and analyze the influence of the number of parameters on the generalization performance. The results provide insights into choosing different transferring methods under linear regression settings."
            },
            "weaknesses": {
                "value": "- It is unclear what the contributions this paper makes to the existing literature. It would be better to specify which existing works are compared to when discussing them in the introduction and the main text. \n- The terms used in this paper need further specification. For example, it would be better to provide formal definitions of generalization performance, transferring errors, benign overfitting, and descent floors."
            },
            "questions": {
                "value": "- What does the second step (\"extract and transfer the learned parameters of the common features\")  of parameter transfer mean? It would be better to elaborate on this step. \n- It would be better to provide a formal definition of the transferring error presented in Theorem 1. \n- Can the insights from the presented theorems be extended to transfer learning with deep neural networks? Although the theorems are presented for linear regression settings, it would be better to discuss the potential implications for deep neural networks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7587/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807162849,
        "cdate": 1698807162849,
        "tmdate": 1699636918989,
        "mdate": 1699636918989,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HAzDIJlmj2",
        "forum": "5T46w5X3Go",
        "replyto": "5T46w5X3Go",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7587/Reviewer_kvri"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7587/Reviewer_kvri"
        ],
        "content": {
            "summary": {
                "value": "The paper explores transfer learning in a linear regression model, wherein certain features are shared between the source and target, while other features are specific to each. Then it analyzes two transfer options:\n A) The learner first learns the source parameter and transfers the parameters corresponding to the shared feature to the target. Afterward, it learns only the parameters corresponding to the target-specific features.\n B) The learner first learns the source parameters and then uses the learned source parameters as an initialization for the target parameters corresponding to the shared features.\n\nThen, by considering both overparameterized and underparameterized settings, it analyzes the behavior of target generalization error as the number of features and parameters varies."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper analyzes the target generalization error as a function of the number of common and task-specific features and parameters as well as the variance of the noise in the model. Furthermore, it characterizes the regimes in which benign overfitting happens. For instance, they show that in some cases it would be more beneficial to allocate more redundant features to the task-specific part rather than the shared one."
            },
            "weaknesses": {
                "value": "The model assumed in the paper is quite restrictive. Firstly, it only considers a linear model. Even more restrictive is the assumption that exact shared features exist between the source and target datasets. Additionally, the paper assumes that the shared features are included in the training examples, and the training examples may contain at most some redundant features. In other words, by eliminating only some of the features, one can recover both the shared features and task-specific features.\n\nCertainly, when the source and target share the same features, they become transferable. However, the reverse is not necessarily true. In a previous study [1], it was demonstrated that in a linear regression model, transferability can still occur even if the datasets lack exact shared features, as long as their ground truth parameters have a small distance according to the Frobenius norm.\n\nFurthermore, the literature on transfer learning contains numerous results where source and target training examples do not initially include shared common features. However, through certain transformations, it is possible to find a subspace in which the source and target datasets share common features.\n\nMoreover, the paper has only derived some formulas for generalization error, and it only explains what would happen if one varies the parameters. It does not provide insight into why varying the parameters in certain regimes results in an increase or decrease in generalization error. The paper does not go beyond explaining the algebraic relationships.\n\n\n[1] SM Mousavi Kalan, Z Fabian, S Avestimehr, M Soltanolkotabi; Minimax Lower Bounds for Transfer Learning with Linear and One-hidden Layer Neural Networks."
            },
            "questions": {
                "value": "I recommend the authors generalize their results to the case where initially the source and target training examples do not contain shared and task-specific features. However, there exists an unknown transformation after which one can discover exact shared features.\nAlternatively, if some of the ground truth parameters between the source and target are close to each other, though not necessarily the same, it would be interesting to investigate and characterize benign overfitting in these more practical scenarios."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7587/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699039794545,
        "cdate": 1699039794545,
        "tmdate": 1699636918870,
        "mdate": 1699636918870,
        "license": "CC BY 4.0",
        "version": 2
    }
]