[
    {
        "id": "vgYXmOA4qN",
        "forum": "ceATjGPTUD",
        "replyto": "ceATjGPTUD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4397/Reviewer_nSVq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4397/Reviewer_nSVq"
        ],
        "content": {
            "summary": {
                "value": "In a previous study, generative error correction (GER) is achieved by learning the mapping from ASR N-best hypotheses to ground-truth transcription through efficient LLM finetuning. This paper extends this idea and focuses on noisy conditions. To avoid the cross-modality gap, the authors propose a novel idea to extract a language-space noise embedding from the N-best list to represent the noise conditions of source speech. Furthermore, in order to enhance its representation ability of audio noise, a knowledge distillation (KD) approach via mutual information estimation (MINE) is employed. The experiments show that the proposed method can significantly outperform the conventional LM rescoring baseline. Several additional experiments are also included in the Appendix which provide more insight into the proposed RobustGER. Overall, the paper is very clear and well-written. It describes the problem and explains the solution well. The experiments done are reflective of the proposed model's performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1)\tNovel idea to apply LLM for noise-robust ASR.\n2)\tExtract language-space noise embedding with knowledge distillation based on mutual information.\n3)\tGood performance improvement.\n4)\tPlenty of experiments and ablation studies.\n5)\tInsightful discussions, such as t-SNE visualization, and the relationship between noisy speech and n-best list diversity."
            },
            "weaknesses": {
                "value": "The way to extract the audio noise embedding is from the ASR encoder (i.e., Whisper Large-V2). This may only make sense for the Whisper ASR, as a recent paper [r1] pointed out that the noise-robustness of Whisper does not come from noise-invariant, but recognizes speech conditioned on the noise type. In summary, the Whisper encoder is a suitable model to extract noise information. On the other hand, other ASR models may not have such ability and they achieve noise-robustness through the noise-invariant encoder. If this is the case, those ASR encoders may not be suitable to extract audio noise embedding. A discussion and simple experiment about this would be great.\n\n\n[r1] Gong, Y., Khurana, S., Karlinsky, L., & Glass, J. (2023). Whisper-at: Noise-robust automatic speech recognizers are also strong general audio event taggers. arXiv preprint arXiv:2307.03183."
            },
            "questions": {
                "value": "1)\tAs pointed out on page 5, the noise embedding is calculated by their diversity \u201csimilar to variance\u201d, however in eq (4) and (6), the sentence embedding differences are simply summed, so I guess an abs or square operation is needed?\n2)\tFollowing the previous question, in the appendix page 16, you mentioned that the dimension of language-space noise embedding E_LN is N(N-1)xD_sbert. Could you explain where N(N-1) comes from? I cannot see this dimension from eq (4) and (6).\n3)\tIn figure 2, and eq (3), why the language-space noise embedding is \u2018subtracted\u2019 from the prompt? I found another related equation in eq (13) of the Appendix and the authors only mention \u201cthe subtraction operation denotes \u201cdenoise\u201d\u201d, more explanation is needed.\n4)\tIn eq(8), I\u0398(X; Y ) should be I\u0398(X; Z)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4397/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697606566081,
        "cdate": 1697606566081,
        "tmdate": 1699636412895,
        "mdate": 1699636412895,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QzpBM6QEwz",
        "forum": "ceATjGPTUD",
        "replyto": "ceATjGPTUD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4397/Reviewer_QKKW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4397/Reviewer_QKKW"
        ],
        "content": {
            "summary": {
                "value": "This work extends an established benchmark of generative error correction with a new \"HyPoradise\" dataset, in order to enable LLMs to perform error correction. The study presents application on noisy-robust speech recognition and claims that it reaches up to 53.9% improvement on word error rate.\n\nThe study itself follows the latest trend of research, where LLM is used to"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The work itself holds certain level of novelty, with good review on earlier literatures on both error correction in ASR and LLM.\n2. The methodology is clearly presented, along with the novelty of the paper.\n3. With some ambiguities in the middle, the paper itself clarifies the idea with experiments subtlely."
            },
            "weaknesses": {
                "value": "1. I think the topic of error correction might be a poor fit to the conference. But perhaps I am wrong on this so correct me if so.\n2. There lacks the practical discussion on additional workload, especially on resources.\n3. The description of building the embedding space is somehow confusing in particular terms. For example, in Section 4.2.1 - what is \"diversity similar to variance\"?\n\nMinor issues:\n1. Section 5.4 - What is Table 14?\n2. I suggest to put the definition of embedding a bit earlier from the beginning of Section 4. Otherwise, Figure 2 looks a bit confusing."
            },
            "questions": {
                "value": "1. I wonder the motivation of using Robust Hyporadise dataset for noisy ASR condition. What kind of noise it exactly contains? Is it replacible with other noisy datasets that are more commonly known to the ASR community, such as Switchboard and VoxCeleb (just two examples, they may not be good fit)?\n2. Do you think your model will be sensitive to sampling frequency? I mentioned Switchboard in the last question, which is an 8KHz dataset.\n3. In section 4.2.2, why you think MINE can enhance the noise representation ability? It looks like MINE is not part of novelty here, so any work backing it up?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper has no ethical concern from my point of view."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4397/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698468872634,
        "cdate": 1698468872634,
        "tmdate": 1699636412806,
        "mdate": 1699636412806,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "T1qsgmrJWC",
        "forum": "ceATjGPTUD",
        "replyto": "ceATjGPTUD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4397/Reviewer_ZWfm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4397/Reviewer_ZWfm"
        ],
        "content": {
            "summary": {
                "value": "This work addresses noisy conditions by proposing a language-space noise embedding derived from ASR hypotheses to aid denoising with large language models (LLMs)-based generative error correction (GER). A knowledge distillation strategy further enhances noise representation. Tests on various LLMs show up to 53.9% improvement in word error rate with limited training data, demonstrating the effectiveness of the proposed noise embedding and denoising ability of LLMs."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "From a generally purposed GER benchmark to a more focused noise-robust problem, it is a suitable extension in depth and kind of milestone using LLM for robust ASR."
            },
            "weaknesses": {
                "value": "The illustrations of (b) GER with audio-space denoising (Zhang et al., 2023b; Fathullah et al., 2023) and (c) GER with language-space denoising (ours) are a little challenging to follow. Is denoised audio directly fed to the LLM adapter, or is there something else you want to express?\n\nWe noticed that the HP database only comes from the n-best of a few models. Is it possible to introduce more diverse system outputs from various models?\n\nAccording to the method in the article, Section 4.3 should be the most important part, relatively speaking. Unfortunately, the space allocated to it in the article is too cramped\u2014too many things to fit into this small section, which is not very reader-friendly."
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4397/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731141888,
        "cdate": 1698731141888,
        "tmdate": 1699636412731,
        "mdate": 1699636412731,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "txipfBRm01",
        "forum": "ceATjGPTUD",
        "replyto": "ceATjGPTUD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4397/Reviewer_BbAN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4397/Reviewer_BbAN"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the noise-robust ASR. The method is based on the LLM-based generative error correction (GER), but contrary to the existing approaches, it extract the noise information from the N-best hypotheses of transcription languages, not directly from the audio. The proposed method significantly outperformed the existing baseline and advanced the area of noise-robust ASR."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Originality:\nWhile it would normally be better to reduce the noise before ASR or to estimate the noise directly from the acoustic data, the idea of doing noise estimation from N-best transcription hypotheses is interesting. It is interesting that this method avoids the difficulty of cross-modal fine-tuning by doing so. It is also compelling because it seems that humans actually perform similar processing in noisy environments.\n\nQuality:\nThe paper is rich in evaluation, and the improvement interval is significant compared with the existing baselines. Also, it provides theoretical hypotheses why the proposed method works better that sound convincing.\n\nClarity:\nThe description appears complete. I have not read all the details, but I get the impression that this paper is very well organized.\n\nSignificance:\nThe task addressed is clearly significant because it has many practical applications. The novel approach presented in this paper is also be interesting, and I think it can potentially be applied in other modalities."
            },
            "weaknesses": {
                "value": "- It was very difficult to find anything to explicitly criticize about the technical content of the paper. There may be flaws and room for improvement, but that is no longer something to do at the peer review stage of this paper.\n- If I had to say something, I was concerned that the notation seemed sometimes inconsistent."
            },
            "questions": {
                "value": "- The notation seems to be mixed up. $\\mathcal{P}$ may be used in the sense of probability density function in equation (2), but this is also used to mean \"prompt\". In the definition of KL, the probability distribution is denoted as $\\mathbb{P}$. \n- I don't feel the need to use too much fancy notation like tensor product $\\mathbb{P}_X \\otimes \\mathbb{P}_Z$ and Radon-Nikodym derivative $\\log \\frac{d\\mathbb{P}}{d\\mathbb{Q}}$.\n- It may be just because I am conservative but $1e^{-2}$ looks like $1/\\exp(2)$.\n- $\\mathbb{E}_p p \\log p$ looks strange in Table 15.\n- $\\textit{i.e.}$ should not be italicized in standard writing convention."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4397/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4397/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4397/Reviewer_BbAN"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4397/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769445659,
        "cdate": 1698769445659,
        "tmdate": 1699636412640,
        "mdate": 1699636412640,
        "license": "CC BY 4.0",
        "version": 2
    }
]