[
    {
        "id": "W9zD0vjbow",
        "forum": "XrunSYwoLr",
        "replyto": "XrunSYwoLr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1774/Reviewer_gaB3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1774/Reviewer_gaB3"
        ],
        "content": {
            "summary": {
                "value": "This research presents a novel method for spiking neural networks from pretrained Transformer models (or models with multi-head attentions). The suggested Universal Group Operators and Temporal-Corrective Self-Attention Layer allow a pretrained Transformer to be converted to a completely event-driven SNN without the need for training, which holds promise for neuromorphic computing. Effectively positive experimental outcomes were obtained."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The ability to convert pretrained Transformer models into spiking neural networks without the need for training is more appealing than training Spiking Transformers directly.\n\n\nFor neuromorphic computing and widespread deployment, the pure implementation on spiking neurons is promising.\n\n\n\nThe universal nature of the conversion method used here makes use of linear models' capacity for global approximation. The inverse function is converted correctly."
            },
            "weaknesses": {
                "value": "In comparison to conventional artificial neural networks, there may be a slight accuracy gap due to the approximation error from Universal Group Operators.\n\nThe proposed method has only been tested on the ViT-B/32 model from CLIP; it is unknown if it can be applied to other models.\n\nThe converted models perform computations at a marginally higher rate than traditional spiking CNNs (which have more synapses and neurons)."
            },
            "questions": {
                "value": "From Figure 3, I can observe severe, uneven quantization. Can you explain how this quantization affects the accuracy of the output?\n\nHow do you prepare the data for pretraining non-linear activations? For GeLU, do you record the actual responses of the ANN and train it on these activation values? For other nonlinearities (inverse, exp, layer norm), how do you pretrain?\n\nAlso, why do these nonlinearities need so many kinds of losses here (Table 3)? Can you explain why Huber loss fits exp, gelu, and inverse? And why does MSE fit the layer norm?\n\nBesides, do you evaluate the actual increment of neurons by using UGO? I want you to give me a table reporting the difference in neuron number and weight according to the models listed in Table 1.\n\nAre you going to release the weight of the pretrained nonlinearity? Do you test the sensitivity of changing $N$ and $T$?\n\nI think there is a typo in equation (22). $V_{th} \\Vert w_2\\Vert_1$ should be $(V_{th} \\Vert w_2\\Vert_1)/T$.\n\nWhy do you mention setting $V_{th}$ using the strategy proposed by Li et al., who proposed to use dynamic thresholds, and demonstrating the quantization gap using the maximum activation as a threshold?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1774/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1774/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1774/Reviewer_gaB3"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1774/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698233430713,
        "cdate": 1698233430713,
        "tmdate": 1700581725924,
        "mdate": 1700581725924,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hyb2Z7hAt1",
        "forum": "XrunSYwoLr",
        "replyto": "XrunSYwoLr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1774/Reviewer_wK6t"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1774/Reviewer_wK6t"
        ],
        "content": {
            "summary": {
                "value": "Summary:\nThe paper proposes a training-free method to convert transformer to SNN platforms. It proposes universal group operators to approximate nonlinear activations and temporal-corrective self-attention layer to approximate spike multiplications. Compared to prior work, it is the first to support pretrained transformers with SNNs without training."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strength:\n\n1.\tThe idea is novel. Use multiple spiking neurons to estimate the nonlinearity functions. The temporal-corrective self-attention can achieve unbiased multiplication between two variable matrices. \n\n2.\tThe convergence and error bound have solid theoretical guarantees and experimental validation.\n\n3.\tCompared to prior work with training/calibration, this work can quickly convert ViT to SNN hardware with high fidelity with a small timesteps T.\n\n4.\tIt also shows the efficiency benefits compared to ANNs, which justifies the advantages of SNN-based transformer."
            },
            "weaknesses": {
                "value": "Weakness:\n\n1.\tThe latency/runtime benefit of SNN-based transformer with different timesteps T needs to be compared to ANN accelerators.\n\n2.\tThe proposed method can have a high fidelity with a small timestep, but there is still 1% gap compared to ANNs, even with a large T. \nCompared to the training/calibration-based method, the training-free one shows a higher accuracy gap. (Table2, SNM, Calib on resnet20 can fully recover the accuracy). Can the authors comment on that? \n\n3.\tIs there any randomness in the spikes-based multiplications given the current data encoding? If there is, the output of the computing result is not deterministic. How robust is it to randomness? To have a deterministic output, the effective resolution will be reduced, thus harming accuracy. Can the authors comment on that?\n\n4.\tHow does the spiking-based multiplication differentiate from the standard multiplication mechanism in stochastic computing?\n\n5.\tThere are other acceleration methods to speed up and reduce energy consumption by a large factor without sacrificing accuracy, e.g., model compression and better architecture design. Moving to a new hardware platform with 30-40% energy reduction seems not very convincing."
            },
            "questions": {
                "value": "Questions are listed in the weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1774/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698615044826,
        "cdate": 1698615044826,
        "tmdate": 1699636106855,
        "mdate": 1699636106855,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NVqhLPpRhN",
        "forum": "XrunSYwoLr",
        "replyto": "XrunSYwoLr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1774/Reviewer_S5Cb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1774/Reviewer_S5Cb"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed Universal Group Operator (UGO) and Spatio-Temporal Approximation (STA) to fit the functions of LayerNorm, GELU layers and optimize the conversion error about self-attention modules."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The theoretical analysis about Temporal Estimation & Correction in Eq.5-Eq.11 is convincing."
            },
            "weaknesses": {
                "value": "1. From Tab.1-2, it seems that the author's approximate fitting methods for nonlinear operations such as LayerNorm require relatively long time-steps ($\\geq 32$) to be effectively implemented, which will result in more significant time latency and energy consumption. In addition, even under 256 time-steps, the author's approximate fitting and error correction methods cannot completely eliminate the conversion error (there is a ~1% accuracy loss).\n\n2. Regarding the fitting calculation of LayerNorm and Softmax layers, as well as the self-attention layer error correction calculation in Eq.9, it seems that the calculation steps and costs involved are still relatively large. I think this may hinder the algorithm's practical application.\n\n3. I noticed that a previous work [1] achieved similar ANN-SNN Conversion performance to this paper when using BatchNorm layers directly (without involving nonlinear operations) and without error correction for attention modules. So I think the value of this approximate fitting and error correction method still needs to be further evaluated.\n\n[1] Ziqing Wang, Yuetong Fang, Jiahang Cao, Qiang Zhang, Zhongrui Wang, Renjing Xu. Masked Spiking Transformer. ICCV 2023."
            },
            "questions": {
                "value": "See Weakness Section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1774/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1774/Reviewer_S5Cb",
                    "ICLR.cc/2024/Conference/Submission1774/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1774/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698683099762,
        "cdate": 1698683099762,
        "tmdate": 1700722815735,
        "mdate": 1700722815735,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ScutC9ckc8",
        "forum": "XrunSYwoLr",
        "replyto": "XrunSYwoLr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1774/Reviewer_jaAV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1774/Reviewer_jaAV"
        ],
        "content": {
            "summary": {
                "value": "This study introduces a training-free method to convert ANN transformers into SNNs, preserving the weights of the original pretrained model to ensure its inference capability remains intact. The resulting SNN Transformer model outperforms its convolutional network counterparts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1.This paper overcomes the differences in computational paradigms between ANN and SNN Transformers, and can accurately approximate ANNs with converted models.\n\n2.The proposed training-free conversion strategy could enable the direct deployment of large-scale pretrained ANN models to low-power neuromorphic hardware."
            },
            "weaknesses": {
                "value": "1.The proposed Universal Group Operators use extensive spiking neurons to model fine-grained ANN operations. This high model complexity reduces power efficiency and incurs large memory usage. \n\n2.The current implementation only handles image Transformers. Longer sequences in language models may introduce more unaddressed issues such as threshold variations similar to that in spiking RNN."
            },
            "questions": {
                "value": "1. Would employing model compression strategies, such as pruning, enhance the efficiency and reduce the size of the Universal Group Operators?\n\n2. The conversion implementation integrates multiple existing ANN-SNN conversion algorithms, including SNM and Burst. Using the same combined conversion for ResNet baselines could enable a more fair comparison.\n\n3. What hurdles might one encounter when adapting this technique to language Transformers? Would the method necessitate modifications?\n\n4. What challenges might arise when adapting this method to larger-scale transformer models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1774/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698759066698,
        "cdate": 1698759066698,
        "tmdate": 1699636106709,
        "mdate": 1699636106709,
        "license": "CC BY 4.0",
        "version": 2
    }
]