[
    {
        "id": "XXrLLNaT0x",
        "forum": "Kg0IDQF3wp",
        "replyto": "Kg0IDQF3wp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4754/Reviewer_x6EA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4754/Reviewer_x6EA"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an efficient approach with a tailored model architecture for massive multilingual neural machine translation. LegoMT2\norganizes 435 languages into 8 language-centric groups and attributes one local encoder-decoder for each group and a global encoder-decoder for all languages. LegoMT2 then trains each local and global encoder-decoder on a group-dedicated set of clients through asynchronous updating of parameters."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- federated learning used in MNMT to solve the parameter interference problem is somewhat novel\n- This paper is well-written, and experiments show their improvements over baselines."
            },
            "weaknesses": {
                "value": "- The authors should present the key features of the traditional federated learning methods in the related works. The authors claim an efficient approach with a tailored model architecture for massive multilingual neural machine translation. What are the key attributes of the tailored model? In other words, what is the key difference between the federated learning used in this paper compared to the traditional federated method? \n- The experimental results are somewhat less convincing. Actually, the model size of the model should be viewed as 10.4B rather than 1.6B. And the final model used in inference is the averaged version of the 8 local models. Therefore, the model should be compared to the same-size finetuned model.\n- Why the model is finetuned from the pre-trained model? Why not training from scratch?"
            },
            "questions": {
                "value": "- See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4754/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4754/Reviewer_x6EA",
                    "ICLR.cc/2024/Conference/Submission4754/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4754/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697880986431,
        "cdate": 1697880986431,
        "tmdate": 1700710863776,
        "mdate": 1700710863776,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lrYxEicplG",
        "forum": "Kg0IDQF3wp",
        "replyto": "Kg0IDQF3wp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4754/Reviewer_s6KB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4754/Reviewer_s6KB"
        ],
        "content": {
            "summary": {
                "value": "To train a single model for massive languages is known for a challenging problem. This paper tackles the problem of how to efficiently train a neural machine translation for massive multilingual languages and proposed LegoMT2 that consists of local encoder-decoder models for language groups and a global encoder-decoder for all languages, where 435 languages are grouped into 8 language-centric category. The experimental results show the training efficiency and translation accuracy improvement, achieving 16.2x faster than the distributed training method for the same-size NLLLB and improving the translation accuracy by 2.2 BLEU on Flores-101 dataset averagely."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The idea of asynchronous model parameter update that are language-group dependent is straightforward. Extensive experiments show that the proposed approach yields improvements in translation accuracy across languages. The proposed approach also helps the multi-way model to get trained faster."
            },
            "weaknesses": {
                "value": "- Extensive experimental results and analyses are not fit in 9 pages. There are some description overlaps in Section 1 and 3 so the authors can move the contents from Appendix to the main pages."
            },
            "questions": {
                "value": "- Reg Section 3.3; how helpful is the parameter initialization with NLLB-200-1.3B? Have you ever looked into this effect, without having the NLLB initialization?\n- Have you ever tried with different language grouping? \n- Why do you think Dec-Flows is better in the low-resource language groups?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4754/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4754/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4754/Reviewer_s6KB"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4754/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814277210,
        "cdate": 1698814277210,
        "tmdate": 1699636457747,
        "mdate": 1699636457747,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AXJzAHHcaO",
        "forum": "Kg0IDQF3wp",
        "replyto": "Kg0IDQF3wp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4754/Reviewer_fThx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4754/Reviewer_fThx"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel approach called LegoMT2 for multilingual neural machine translation. It addresses the challenge of learning a single model for a large number of languages by organizing languages into groups and using a multi-way model that includes multiple encoder-decoders \u2013 each for a certain language group and another global encoder-decoder. LegoMT2 trains these encoder-decoder pairs on dedicated server clients using asynchronous updating of parameters."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed LegoMT2 supports over 400 languages for machine translation with one single encoder-decoder model, doubling the number of NLLB while significantly faster in training."
            },
            "weaknesses": {
                "value": "The paper did not conduct specific verification experiments on parameter interference to demonstrate that the performance improvement of LegoMT2 over finetuned NLLB-200-1.3B indeed stems from the alleviation of parameter interference phenomena."
            },
            "questions": {
                "value": "1. Which of Single-FT or Single-FT + MoE in Table 3 is used for the experiments in Table 1 and Table 2? Have the translation performance of both been evaluated?\n2. Have any other methods for MERGE operation of non-blocking federated learning, apart from simple averaging, been tried and evaluated?\n3. How about LLMs for Multilingual Machine Translation\uff1f"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4754/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698972793073,
        "cdate": 1698972793073,
        "tmdate": 1699636457653,
        "mdate": 1699636457653,
        "license": "CC BY 4.0",
        "version": 2
    }
]