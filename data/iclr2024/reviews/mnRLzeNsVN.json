[
    {
        "id": "cBmv2LijUn",
        "forum": "mnRLzeNsVN",
        "replyto": "mnRLzeNsVN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8240/Reviewer_stG3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8240/Reviewer_stG3"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to adopt graph sparsification techniques in the data preprocessing step for the GNN-based TSP solvers. Experiments demonstrate its effectiveness in both aspects of improving the quality of solutions and the running efficiency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The methods are straight-forward and easy to follow."
            },
            "weaknesses": {
                "value": "- Important baselines are missing. The experiments are more like ablation studies: The authors investigate the performance of GAT/GCN-based solvers with or without the proposed graph sparsification techniques, but does not compare the performance with other solvers mentioned in the related works. To make the contributions strong enough and the results convincing, the paper should compare with the latest methods and outperform them.\n\n- The examples in Figure 1 do not make sense. Message passing in GNN does not propagate the features directly, but with a projection matrix (e.g. GraphSAGE). Furthermore, the problem of over-smoothing of GNN not only exist in complete graphs but also in general graphs [1]. How the proposed graph sparsification technique relieve the problem should be more clearly discussed.\n\n- The covered problems only include the 2D tsp, which limits the the contributions of the proposed techniques.\n\n- It lacks necessary theoretical analysis.\n\n- The proposed 1-tree sparsification method is derivated from LKH which is a very strong TSP solver. Then the use of the technique in data preprocessing indeed brings prior knowledge to the neural solver. It is very hard to say that whether the better performance comes from the graph sparsification, or comes from the prior knowledge for TSP solving. \n\n[1] A SURVEY ON OVERSMOOTHING IN GRAPH NEURAL NETWORKS. https://arxiv.org/pdf/2303.10993.pdf\n\n\u2028Based upon the above points, I believe that the work is still somehow preliminary and the paper does not meet the bar of iclr."
            },
            "questions": {
                "value": "- The size of instances is not given. \n- The others are in the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8240/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697952640967,
        "cdate": 1697952640967,
        "tmdate": 1699637024337,
        "mdate": 1699637024337,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DZriqUIYEP",
        "forum": "mnRLzeNsVN",
        "replyto": "mnRLzeNsVN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8240/Reviewer_CDdM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8240/Reviewer_CDdM"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes two data preprocessing methods for solving the TSP with GNNs, i.e., k-nearest neighbors heuristic and 1-Trees, which make the corresponding TSP instances sparse by deleting unpromising edges. Experiments are carried out to determine the better sparsification method and the relationships between different data distributions/training dataset sizes and sparsification parameter k."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tSparsification (or pruning, candidate selection, etc.) methods are important for solving the TSP as they can substantially reduce the computational complexity, and is commonly used in learning-based algorithms and heuristic algorithms for the TSP.\n2.\tThe paper is overall well written."
            },
            "weaknesses": {
                "value": "1.\tK-nearest neighbors heuristic is already used for sparsification in the input layer (k=20 for TSP100) of GCN by Joshi et al. (2019), Fu et al. (2021) and Xin et al. (2021b) followed this setting. And the proposed \u201c1-Trees\u201d method is similar to the edge candidate set construction process of the LKH algorithm using the 1-tree structure. Thus, the main contribution of this paper seems to be selecting the proper k of k-nn when the problem size is fixed at 100, and transplanting the 1-tree method of LKH as a data preprocessing procedure for learning-based methods. Therefore, the novelty of this paper is not significant enough.\n\n2.\tThe problem size is fixed at 100 in the experiments so that the generalization ability of the proposed method over different problem sizes is unclear. I recommend the authors add the following question in section 4: how does the problem size n (amount of cities in one TSP instance) relate to the sparsification parameter k?\n\n3.\tComparative experiments with state-of-the-art TSP algorithms is not provided. It is uncertain whether the \u201c1-Trees\u201d method or changing the hyperparameter k of k-nn in existing methods like Joshi et al. (2019); Fu et al. (2021); Xin et al. (2021b) can enhance the performance of state-of-the-art learning-based TSP algorithms."
            },
            "questions": {
                "value": "1. Please clarify the novelty of this paper in comparisons with the literature papers.\n\n2. how does the problem size n (amount of cities in one TSP instance) relate to the sparsification parameter k?\n\n3. Comparative experiments with state-of-the-art TSP algorithms is not provided."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8240/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698311731026,
        "cdate": 1698311731026,
        "tmdate": 1699637024206,
        "mdate": 1699637024206,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NWoX36aOaC",
        "forum": "mnRLzeNsVN",
        "replyto": "mnRLzeNsVN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8240/Reviewer_oBRi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8240/Reviewer_oBRi"
        ],
        "content": {
            "summary": {
                "value": "The authors observe that the sparsed TSP graph with KNN and 1-tree could improve the performance of the GNN-based method and reduce training time. The topic of studying the sparsity of TSP graph is interesting. The observation also seems reasonable. However, the sparsity like KNN has been used by previous work on TSP and VRP. The used 1-tree method was borrowed from LKH. Therefore, I think the contributions are quite marginal."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The observations are interesting.\n\nThe experiment design is mostly reasonable."
            },
            "weaknesses": {
                "value": "Quite some related works about neural-based methods for TSP and VRP are missing, especially from TOP AI conferences.\n\nThe sparsity like KNN has been used by previous work on TSP and VRP. The used 1-tree method was borrowed from LKH. Therefore, I think the contribution are quite marginal."
            },
            "questions": {
                "value": "The results of GAT with dense graphs are quite bad, which makes the results less convincing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8240/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698552639358,
        "cdate": 1698552639358,
        "tmdate": 1699637024091,
        "mdate": 1699637024091,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "omaEy6mRnW",
        "forum": "mnRLzeNsVN",
        "replyto": "mnRLzeNsVN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8240/Reviewer_gtua"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8240/Reviewer_gtua"
        ],
        "content": {
            "summary": {
                "value": "This paper use one-tree for edge elimination for GNN. \nThe proposed method achieves an up to \u00d72 performance improvement w.r.t. the optimality gap and a decrease in runtime by 10% during training and validation, when applied to GCNs. For GATs, the improvements in regards of runtime and optimality gap are even bigger when sparsifying the data first."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. one-tree based sparsity saves time.\n2. The introduction and related work sections are well-written."
            },
            "weaknesses": {
                "value": "This paper is very hard to follow.\n\n1. One-tree has been proposed and existed for many year, introducing sparsity to GNN is not a new idea,\nsee https://arxiv.org/abs/2006.07054\n\n2. weak evaluation, on TSP 100 only.\n\n\n\nWe employ GNN for TSP with the aspiration of learning promising edges without the need for human-designed heuristics. However, the use of one-tree heuristics already narrows down the edge set. This means the sparsity is largely dependent on human-designed heuristics rather than data-driven ones.\n\nAlso, this sparsity is only limited to TSP and is not able to generalize any other problem."
            },
            "questions": {
                "value": "1. The table is very confusing, why select different training size? The goal is to investigate how sparsity affect GNN, not training size.\n\n2. How to train your GNN, supervised or reinforcement or even unsupervised? How to get the TSP length? \nMy understanding is that the code is using reinforcement learning framework based on Jin et al.\nBut in Jin et al. The authors report a 0.16\\% on TSP-100. They further study TSP random200, TSP random500 and TSPLIB from 1~1002. \nIf the paper use the same model, they should evaluate on the same dataset with Jin et al.\n\n3. In the paper ```We summarize that for the GCN, smaller k led to the overall best results, whereas for the GAT there is a tendency for bigger k (but not dense graphs!) to lead to the best results.```, this is more confusing, that means graph sparsifying can be different for different GNN models, then how we decide $k$ when we use a different GNN model? \n\n4. We report up to \u00d722 improvements for the optimality gap while reducing the runtime by 50\\%.  Can you reveal more details about the training and evaluation, how to get these results?\n\n\n\nJin et al. Deep reinforced multi-pointer transformer for the traveling salesman problem"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8240/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698698505235,
        "cdate": 1698698505235,
        "tmdate": 1699637023972,
        "mdate": 1699637023972,
        "license": "CC BY 4.0",
        "version": 2
    }
]