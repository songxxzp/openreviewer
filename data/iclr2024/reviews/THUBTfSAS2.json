[
    {
        "id": "H0Lq4NPE5w",
        "forum": "THUBTfSAS2",
        "replyto": "THUBTfSAS2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7173/Reviewer_L6Gr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7173/Reviewer_L6Gr"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new active learning technique based on novel selection strategy of unlabeled examples. Its main contributions are the following:\n\n\u2014\u00a0A new measure of closeness to the decision boundary the authors call the 'least disagree metric' (LDM). This is a metric inspired by the disagree metric of [1],  to play a role akin to \"margin-score\" or \"entropy\" in the standard active-learning algorithms like \"margin\" etc. As the authors mention, conceptually, a sample with a small LDM indicates that its prediction can be easily flip-flopped even by a small perturbation in the predictor. \n\n\u2014\u00a0An estimator of LDM that is provably asymptotically consistent under mild assumptions, and a simple Bayesian-perspective-inspired algorithm to empirically evaluate such an estimator. (LDM is intractable to compute in most cases so the authors propose.)\n\n\u2014\u00a0An LDM-based active learning algorithm (LDM-S) that, besides using LDM as the \"scoring function\" for selecting unlabeled examples\",  it also makes sure that there's \"diversity\" in the selected batch of unlabeled examples to be labeled. In particular, diversity is ensured via a modification of the k-means++ seeding algorithm and  without introducing additional hyperparameters. Finally, The authors compare their algorithm with several SOTA active learning techniques and show that it typically performs better.\n\n[1]  Theory of Disagreement-Based Active Learning. Foundations and Trends\u00ae in Machine Learning, 7(2-3):131\u2013309, 2014, Steve Hanneke."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "\u2014\u00a0Well-written paper\n\n\u2014\u00a0Novel, principled approach to active-learning.\n\n\u2014\u00a0A method of incorporating diversity in the sample selection without adding hyperparameters.\n\n\u2014\u00a0Extensive experimental evaluation."
            },
            "weaknesses": {
                "value": "\u2014\u00a0The proposed approach provides somewhat mild benefits compared to already existing approaches (although it is consistently the best or the second best approach in each scenario, and the best according to metrics that consider the average performance among all datasets.)\n\n\u2014The datasets considered in this paper are somewhat small-scale, and so it's not clear to me whether the proposed approach is suitable for large-scale applications. For example, in Table 2 where the authors present the running time per dataset, Imagenet \u2014 which is the largest dataset considered in this paper, is missing."
            },
            "questions": {
                "value": "Could the authors provide the mean running time of each algorithm considered for the Imagenet dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7173/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637350352,
        "cdate": 1698637350352,
        "tmdate": 1699636850940,
        "mdate": 1699636850940,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zYR9Dbs8AQ",
        "forum": "THUBTfSAS2",
        "replyto": "THUBTfSAS2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7173/Reviewer_ixMJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7173/Reviewer_ixMJ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use LDM estimator as heuristics to query samples for active learning. The estimator is proven to be asymptotically consistent under mild assumptions. Two approaches based on LDM, i.e., naive approach and LDM-S, have been considered for batch active learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The paper maintains a high-quality presentation. The measure, proof of asymptotical consistency, and algorithms are clearly presented.\n(2) Extensive experiments on 3 openml datasets and 6 benchmark image datasets."
            },
            "weaknesses": {
                "value": "(1) Many baseline models are not considered in the paper's experiment, e.g., SAAL, Cluster-Margin, Similar, and [4]. \n\n(2) Computational cost analysis is lacking, including LDM estimation cost and LDM-S. What is the relationship to batch size, ensemble size, M, etc? The computational cost seems to be comparable to BADGE which has squared complexity to batch size. \n\n(3) The authors provide no analysis of the relatedness of LDM to active learner performance in the paper setting.\n\n(4) Careful discussion of limitations is lacking in the paper. There are many deep active learning algorithms coming out each year, each with its own pros and cons. A careful discussion of advantages and limitations will be very helpful to the community. One possible drawback is the need to use ensembles which takes more cost compared to methods that only use one model like BADGE and CoreSet.\n\n(5) The seeding strategy seems detached from the novel estimator. Many strategies, e.g., [1], [2], and [4], have been published to extend to batches. It is better to compare these strategies based on LDM and show if the performance improvement is consistent.\n\nreferences:\n\n[1] Kim, Yoon-Yeong, Youngjae Cho, JoonHo Jang, Byeonghu Na, Yeongmin Kim, Kyungwoo Song, Wanmo Kang, and Il-chul Moon. \"SAAL: Sharpness-Aware Active Learning.\" (2023).\n\n[2] Citovsky, Gui, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand Rajagopalan, Afshin Rostamizadeh, and Sanjiv Kumar. \"Batch active learning at scale.\" Advances in Neural Information Processing Systems 34 (2021): 11933-11944.\n\n[3] Kothawade, Suraj, Nathan Beck, Krishnateja Killamsetty, and Rishabh Iyer. \"Similar: Submodular information measures based active learning in realistic scenarios.\" Advances in Neural Information Processing Systems 34 (2021): 18685-18697.\n\n[4] Kirsch, Andreas, Sebastian Farquhar, Parmida Atighehchian, Andrew Jesson, Fr\u00e9d\u00e9ric Branchaud-Charron, and Yarin Gal. \"Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning.\" Transactions on Machine Learning Research (2023)."
            },
            "questions": {
                "value": "(1) Could the authors provide numerical or illustrative examples to show that LDM-based algorithm can be more effective in deep active learning? The authors provided the asymptotical analysis of the estimator but no analysis is provided for the relatedness of LDM to the final performance. In this case, it is better to provide some illustrative examples to show the effectiveness of LDM-based algorithms. \n\n(2) I am very interested to see the performance of active learning models with MC-dropout. MC-dropout is more efficient compared ensemble method.\n\n(3) Could the authors explain why they do not use more advanced posterior sampling method like [1] and [2]\n\nreferences:\n\n[1] Zhang, Ruqi, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew Gordon Wilson. \"Cyclical stochastic gradient MCMC for Bayesian deep learning.\" arXiv preprint arXiv:1902.03932 (2019).\n\n[2] Chen, Tianqi, Emily Fox, and Carlos Guestrin. \"Stochastic gradient hamiltonian monte carlo.\" In International conference on machine learning, pp. 1683-1691. PMLR, 2014."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7173/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7173/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7173/Reviewer_ixMJ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7173/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698705133510,
        "cdate": 1698705133510,
        "tmdate": 1700612068867,
        "mdate": 1700612068867,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9ie8vBV7QI",
        "forum": "THUBTfSAS2",
        "replyto": "THUBTfSAS2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7173/Reviewer_o58y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7173/Reviewer_o58y"
        ],
        "content": {
            "summary": {
                "value": "Framed in the field of active learning, this paper presents LDM (least disagree metric), a novel concept to quantify the distance between an instance and the decision boundary of a classifier. Along with the theoretical definition of LDM, the authors provide an asimptotically consistent estimator of LDM as well as a practical algorithm to calculate it. Based on this notion of LDM, they define a new acquisition procedure in active learning, by favoring low values of LDMs and enforcing diversity. Empirical evaluation shows that the proposed approach is competitive or superior against other state-of-the-art active learning methods in various datasets and with different architectures."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The quality of the exposition is high in general. The concepts and ideas are presented by combining an specific and accurate definition along with an intuition on their meaning. It is easy to follow the flow of the paper, and sections are well organized in a natural manner. \n\n* The experimental evaluation is a comprehensive one, including a wide range of baselines, datasets and architectures. Results are analyzed in a rigorous way, including statistical tests and popular active learning metrics such as the performance profile (Dolan-More plots). \n\n* The proposed metric LDM is clearly explained, and intuition is provided on how it reflects the distance to the decision boundary. It is well motivated and theoretically sounded. The example provided after Definition 1 is clarifying and helps understanding LDM definition.\n\n* LDM estimator, denoted as $L_{N,M}$, has a theoretical underpinning. Under some assumptions, $L_{N,M}$ is shown to converge in probability to the true value L when M scales logarithmically with respect to N.\n\n* 2D binary classification case is thoroughly investigated. Proposition 1 and Proposition 2 in the appendix theoretically confirm the intuition behind the choices made in this work.\n\n* Additional interesting experiments are included in the appendix. In particular: \n    * An effort to study the hyperparameters effect (number of MC samples, batch size, LDM stop condition and \u03c3 interval) is made.\n    * To favour diversity, the LDM estimator is corrected using a popular weighting strategy\nin active learning. The importance of this correction is addressed in the ablation study."
            },
            "weaknesses": {
                "value": "* I think there exists an important gap between the theoretical description of LDM (its definition in Section 2.1 and its estimator in Section 2.2) and how it is empirically evaluated (Section 2.3). In Section 2.3, the \"motivation\" paragraph includes several sentences to justify the procedure that the authors are going to follow to empirically evaluate LDM, but these sentences are just somewhat \"generic\"/\"loose\", and there is no guarantee that hypothesis in Section 2.2 are satisfied. Taking this into account, I wonder whether Section 2.2 is a core component of the contribution, or could be moved to the appendix, leaving room in the main text for other information that may be more central to the contribution (e.g. a more detailed description of the experiments or further empirical evaluations, which are currently in the appendix). \n\n* Even if Section 2.2 is moved to the appendix, I think that assumptions 1, 2 and 3 should be further motivated. The reader who is unfamiliar with mathematical concepts could get lost: what is a Polish space, and why is it necessary? Why is it necessary that \u03c1 is Lipschitz? How restrictive is assumption 3? There are no references to other works where these assumptions are made. If they are common, these references should be provided. If not, authors should motivate and discuss them.\n\n* I also wonder whether the idea of \"sampling close to the decision boundary\" is the best way to go in active learning. Samples close to the decision boundary may not be informative if all the uncertainty that they present is of _aleatoric_ nature (inherent to the data, i.e. it cannot be reduced by further sampling training data). I think that more subtle distinctions on the types of uncertainty to be considered in active learning should be analyzed. My intuition is that this might be related to the finding that using LDM alone did not work properly, and a (somewhat ad-hoc) procedure to encourage diversity had to be introduced. \n\n* I think that more empirical evaluations should be moved to the main text (specially if theoretical details are moved to appendix, as suggested in the first point above). Right now, the experimental setting is not clearly described in the main text (it is deferred to Appendix C), and several ablation studies that could be interesting are \"lost\" in the appendix. \n\n* In some occasions, the statements made by the authors may lead to confusion. For example, in section 2.3, it is said that \u201c...in Appendix B.2, we show that $E_w[\u03c1_M(h, g)]$ is monotone increasing in $\u03c3^2$.\u201d. One could think that this is shown in general. However, the proof only applies to the 2D binary classification case.\n\n* Related to the first point above, some aspects of theory are disconnected from the implementation:\n    * To implement $L_{N,M}$ the authors propose to sample near the learned hypothesis using standard parameter perturbation techniques. As authors state, this sampling scheme would need to satisfy Assumption 3. However, it is not explained why this specific form\nof sampling assures that the hypothesis spaces $\\mathcal{H}_n$ satisfy Assumption 3.\n    * In the 2D binary classification experiment, the performance of LDM-S, entropy, and random based sampling procedures are investigated. As authors state, true LDM is measurable in the 2D binary classification scenario. Thus, a study could be carried out on the error made when approximating L using $L_{N,M}$. Verification of bounds and rates of\nconvergence could be carried out."
            },
            "questions": {
                "value": "Other questions/comments: \n\n* The effect of each hyperparameter is studied independently. Nothing is said about how they\naffect each other. Does the choice of one hyperparameter (number of MC samples, batch size, LDM stop condition\nand \u03c3 interval) affect each other?\n\n* Which metric/score is used to quantify performance in section 4.2? Accuracy is mentioned in\nthe appendix, but it is not entirely clear to me.\n\n* x should be present in the input in algorithm 1, right?\n\n* Perhaps, Assumption 3 is over-complicated: $\\sup_{\\epsilon\\in(0,1)} \\lim_N \u03b1(N, \\epsilon)= 0$ amounts to saying\nthat $\\lim_N \u03b1(N, \\epsilon)$ exists and is 0 when $\\epsilon\\in (0, 1)$.\n\n* I think $f$ in Theorem 1 should be $\\alpha$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7173/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7173/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7173/Reviewer_o58y"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7173/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773452047,
        "cdate": 1698773452047,
        "tmdate": 1700588642226,
        "mdate": 1700588642226,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qME13xB8Tx",
        "forum": "THUBTfSAS2",
        "replyto": "THUBTfSAS2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7173/Reviewer_2mvn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7173/Reviewer_2mvn"
        ],
        "content": {
            "summary": {
                "value": "This paper presents LDM-S, an active learning method based on selecting samples close to the decision boundary. The paper starts by formulating a Least Disagree Metric (LDM) function, and then proceeds to implement it in a Monte Carlo fashion. Necessary assumptions and theorems are proven as needed. Over 5 repeats, the method is compared with other active learning methods and a t-test is used to check significance."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* At least in some parts of the paper, the flow is very good, and theorems and conclusions naturally lead to the next part.\n* The evidence for diversity in LDM-based active learning, as discussed in Appendix E.3, is extraordinary. Through an intuitive first example and then a real-world example (MNIST), the motivation is very clear. If possible, this should definitely be in the main paper to give readers better intuitions.\n* Ablation studies are presented whenever necessary to justify choices made, for example, for modifying the k-means++ seeding algorithm."
            },
            "weaknesses": {
                "value": "* Minor typo: Page 1, last paragraph: flips-flopped --> flip-flopped\n* Section 2.2 is rather rushed in its presentation, and details are not expanded on. For example, it is unclear why $\\mathcal{H}$ needs to be a Polish space (i.e., why second countability is necessary, for instance). In Theorem 1, $f$ is undefined. In Assumption 3, it seems that the phrase, \"that is monotone decreasing in the first argument\" refers to $\\alpha$, but that is not fully clear.\n* Similarly, some other details are presented with no clear rationale. For example, in (6), why is p(x) squared?\n* In Figure 2a, LDM sometimes samples on or very close to the decision boundary. Please change the color of the LDM crosses so they are more apparent."
            },
            "questions": {
                "value": "* Why does $\\mathcal{H}$ need to be a Polish space as opposed to a more general metric space?\n* In Algorithm 2, when you compute $L_x$ via Algorithm 1, could you clarify the parameters passed? My understanding is that you pass it a hypothesis parameterized by $v$, number of samples $m$, and a \"small\" $s$ (as said in Sec. 2.3), but I'm unclear what $\\{ \\sigma^2_k \\}$ you pass."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7173/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7173/Reviewer_2mvn",
                    "ICLR.cc/2024/Conference/Submission7173/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7173/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796909839,
        "cdate": 1698796909839,
        "tmdate": 1700459986432,
        "mdate": 1700459986432,
        "license": "CC BY 4.0",
        "version": 2
    }
]