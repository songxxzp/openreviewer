[
    {
        "id": "CBw4f9EERd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9008/Reviewer_5tZK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9008/Reviewer_5tZK"
        ],
        "forum": "tQqLV2N0uz",
        "replyto": "tQqLV2N0uz",
        "content": {
            "summary": {
                "value": "The paper introduces Reprompting, an algorithm automatically seeks CoT recipes for a given task. \nReprompting iteratively samples new recipes using previously successful solutions as starting points. Experiments conducted on BBH tasks show that (i) Reprompting can transfer knowledge from GPT-3.5-turbo to text-davinci-003; (ii) Reprompting performs better than previous methods like CoT, AutoCoT, Self-Consistency."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- experimental results on BBH tasks show that the proposed Reprompting performs better than the SOTA CoT+SC and AutoCoT\n- Reprompting can generate the cot receipts without expert knowledge\n- the paper is easy to follow"
            },
            "weaknesses": {
                "value": "- major: the algorithm is designed based on assumptions (1) and (2), which are **not standard** in LLMs.\n- major: in experiments, is message $m$ (i.e., *Let\u2019s think step by step. At the end, show your answer bracketed with <answer> and </answer>. Finally generate END at the end of the solution*) applied to all other baselines? If not, an ablation study is required to justify where the improvement comes from, i.e., the message or the proposed reprompting technique.\n- major: experiments are weak: popular benchmarks (e.g., datasets used in Tables 2 and 3 of Self-Consistency) have not been used in the paper.\n- major: justification for the contribution \"*Reprompting can transfer knowledge from a stronger model to a weaker model\" is not strong; only one model is studied, i.e.,  from GPT-3.5-turbo to text-davinci-003 \n- minor: no ablation study for the rejection probability (i.e., testing accuracy w.r.t. rejection probability)\n- major: ComplexCoT (Complexity-Based Prompting for Multi-Step Reasoning) is also an important baseline but not compared with, and also combining it with Self-Consistency"
            },
            "questions": {
                "value": "- Is the rejection sampling used in Sec 2.2 equivalent to drawing samples according to (5)?\n- \"*Note that we set the rejection probability $p_{rej}$ in a way that allows solutions that lead to incorrect answers to be kept occasionally, as these solutions may still contain useful segments that evolve into good recipes through Reprompting*\", any examples? How many incorrect answers are in the generated CoT receipts?\n- \"*in-context learning is based on the following assumption: ...*\", any credits on this assumption? \n- \"*Formally, in-context learning with CoT is based on the following assumption: ... *\" any references?\n- is reprompting sensitive to the choice of $\\\\{x_i, y_i\\\\}_{i=1}^N$?\n- Will the proposed Reprompting be used in recent verification-based methods? e.g.\n  - Large Language Models are Better Reasoners with Self-Verification\n  - LEVER: Learning to Verify Language-to-Code Generation with Execution\n  - Forward-Backward Reasoning in Large Language Models for Mathematical Verification\n  - Making Language Models Better Reasoners with Step-Aware Verifier\n\n\n## ----- Post-Rebuttal -----\nThanks for the authors' reply, which addressed some concerns about experiments. Thus, I increased my rating to 5.\nThe main reason for rejection is the assumptions required by the proposed algorithm are strong and difficult to check when applying the algorithm in practice\n\n- the authors provided some references for the strong assumptions (1) and (2); however, it is still not convincing.\n- based on the new results provided in tables 3/4 and results provided by https://arxiv.org/abs/2304.09797, Reprompting is worse than ComplexCoT (which also uses \"let's think step by step\") on some tasks by a large margin, I listed some numbers for comparison bellow:\n& GSM8K & Number Theory \nComplex CoT &  82.8 & 33.4  \nReprompting &  79.5 & 28.5  \n\nTherefore, users need to check whether the assumptions are satisfied before applying the proposed algorithm.\n\n> Complex-CoT relies on a pool of human-written CoT recipes to select from while Reprompting does not.\n- this is a valid motivation, but I think writing eight CoT demos is easy."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9008/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9008/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9008/Reviewer_5tZK"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9008/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697428766942,
        "cdate": 1697428766942,
        "tmdate": 1701065704105,
        "mdate": 1701065704105,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IDO9j5LLyw",
        "forum": "tQqLV2N0uz",
        "replyto": "tQqLV2N0uz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9008/Reviewer_78Bd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9008/Reviewer_78Bd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes reprompting, an iterative algorithm that continually improve chain-of-thought (CoT) reasoning from the LLMs' own outputs. The idea is that good in-context learning (ICL) demonstrations should work consistently well for all examples rather than being sensitive to ordering, etc. This problem is formulated as a sampling problem from the joint distribution and the authors propose to sample from it via Gibbs sampling. The authors then demonstrate the effectiveness of the proposed algorithm in ChatGPT model with several Big Bench Hard problems and show outperformance over standard zero-shot, few-shot and CoT prompting baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The approach is fairly novel, well-motivated and theoretically grounded: I think the formulation of finding good in-context learning examples as approximate sampling from an intractable joint probability distribution and using Gibbs sampling, in this case, are both well-motivated. I am convinced that this is a solid point for LLM prompting, a field of study on which the model is often treated as a black box, and the approaches proposed are often largely heuristic.\n- The results seem promising, and given that improving LLM reasoning is an important problem, I think the method presented has clear potential in terms of significance. \n- The paper is well-written overall.\n- I reviewed a previous version of the manuscript; one of my major concerns was the lack of comparison to self-consistency (which improves over zero-shot and few-shot CoT with more computational resources, which makes the baselines more comparable to the proposed method, which is expensive) and baselines like AutoCoT and COSP (the authors added AutoCoT but not COSP; I encourage the authors to include discussions of all relevant work if possible); the authors added discussions and empirical comparisons to these, which significantly strengthened the empirical rigour of the paper. Similarly, I was concerned about the cost of the algorithm, but in this updated version, I can see that the authors have dedicated a section addressing these concerns and included an analysis of the transferability of the discovered in-context examples to justify their design choices. These have again made the paper much more convincing in terms of motivation."
            },
            "weaknesses": {
                "value": "- I am still a bit unconvinced that only five subtasks out of the 23 tasks of BIG-Bench Hard were considered, given that each subtask is quite small and choosing 5 out of 23 leads to doubts on whether the results are cherry-picked. In the previous reviewing round, the authors responded that they chose tasks where 1) CoT prompting is likely effective and 2) diverse, and they endeavoured to include additional tasks. However, I think in a practical setting, it's unlikely that practitioners will know beforehand whether CoT prompting is effective or not, and it is still interesting to see how methods such as Reprompting work *even when CoT prompting does not improve much*, this gives readers stronger faith that in the worst case scenario, the method at least does not deteriorate performance. Secondly, given the time allowance between the review cycles, I'd expect experiments on additional subtasks, but this is not present in the manuscript. To start with, I see this being the largest room for improvement of the current manuscript.\n\n-- **Post-rebuttal** --\n\nI thank the authors for their detailed feedback, which mostly addressed my concerns. I will stick to my original rating recommending acceptance, although some of the concerns raised by other reviewers should be addressed, including the discussions on the validity of the assumptions and potential comparison to additional baselines. Since ICLR allows the authors to revise the paper directly, I also think the authors should directly incorporate some of the responses and promised changes, such as the additional discussions on related works (I also think the additional experiments should be in the main text directly instead of in the appendix). A final remark is that the authors, in multiple responses, quoted the fact that methods like COSP do not outperform manual CoT as a justification; however, these methods are *zero-shot* whereas manual CoT and Reprompting are not, so these arguments should be carefully qualified."
            },
            "questions": {
                "value": "Please address the Weaknesses mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9008/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9008/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9008/Reviewer_78Bd"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9008/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698812092602,
        "cdate": 1698812092602,
        "tmdate": 1700657655973,
        "mdate": 1700657655973,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zbHpJpeapc",
        "forum": "tQqLV2N0uz",
        "replyto": "tQqLV2N0uz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9008/Reviewer_kqkB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9008/Reviewer_kqkB"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes \u201cReprompting\u201d a method to automatically generate chain-of-thought (CoT) prompts for large language models (LLMs). The hope with Reprompting is to alleviate the issue of manually handcrafting chain-of-thought prompts while retaining high performance. In a nutshell, Reprompting is inspired from Gibbs sampling and proceeds as follows:\n\n1. Sample initial CoT $z_j$ and answer $y_j$ from an LLM1, for each query $x_j$ in a given support set.\n2. Update $z_j$ and $y_j$ by sampling them from LLM2 conditioned on the query $x_j$, and the other CoT $z_{i \\neq j}$ and answers $y_{i \\neq j}$ in the support set.\n3. Repeat step 2 until convergence.\n\nThe motivation for using two LLMs (LLM1 and LLM2) is to ensure diversity in the initial set of CoT prompts.\nThe authors demonstrate the effectiveness of Reprompting on 5 reasoning tasks from Big Bench Hard (BBH) and compare against human and automatic baselines for CoT prompting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper has a couple of strong selling points, most notably its effectiveness given how easy it is to implement.\n\n- Reprompting is simple enough to be widely adopted, and I feel confident I could reimplement it without trouble. It also makes minimal assumptions on the infrastructure needed to run the method, and is readily implementable with commercial APIs (the authors use OpenAI\u2019s).\n- Reprompting solves a real-world problem: designing chain-of-thought prompts can be quite costly if human-crafted, especially in a large-scale setting. Here the authors report that Reprompting costed between $80 and $800 USD, which is likely to be significantly less than the cost of hiring and training human annotators, and verifying their solution.\n- Reprompting gets excellent results on the 5 BBH tasks it was tested on (with caveats below)."
            },
            "weaknesses": {
                "value": "Here are some limitations of the work, roughly in order of importance.\n\n- Missing baselines and ablations:\n    - Parameter-efficient fine-tuning: since Reprompting is mainly pitched as a way to find high-performing prompts, it should also be compared against parameter-efficient methods such as soft-prompt tuning and LoRA. I\u2019d be interested in both absolute performance (Table 1) and convergence rate (Figure 2).\n    - Sampling ablation: at the top of page 4, the authors mention that the LLM likelihood is not always accessible so they approximate it by randomly rejecting new samples. First, this approximation needs more justification for why it\u2019s appropriate. Second, I\u2019d like to see quantitative evidence supporting this approximation \u2014\u00a0how much better does the method work when actually computing the sampling probabilities? While they may not be available to many practitioners, scientifically we ought to know how much we\u2019re leaving on the table with the approximation.\n    - Rewriting ablation: line 14 of Algorithm 1 doesn\u2019t condition on the current CoT $z_j$, so the new one is sampled without reading prior guesses. How much better would Reprompting work if it had access to prior CoT guesses? This further breaks away from the original Gibbs sampling motivation but may give substantial improvements, according to Yang et al., 2023 (https://arxiv.org/abs/2309.03409).\n- Limited benchmarks: since this paper rests on experimental evaluations (no theory), I\u2019d expect it to be more thorough. In principle Reprompting could be used on any task and I was expecting results and Hendrycks Math, MMLU, and more.\n- Exposition: the method feels like it\u2019s been shoehorned to fit the Gibbs sampling narrative, with many approximations along the road. I\u2019m not sure why that is the case since the authors don\u2019t use this motivation later on. Similarly, the evolutionary algorithm view is briefly mentioned before Section 3, but why?"
            },
            "questions": {
                "value": "See my weaknesses, but generally:\n\n- How does Reprompting compare to parameter-efficient tuning methods?\n- Can you provide a justification for the approximation at the top of p. 4?\n- How much is left on the table due to this approximation? Is it an issue that generative LLMs are bad at estimating the likelihood of their samples? How does that affect the Gibbs sampling motivation?\n- In terms of wall-clock time, how long did it take to converge in Fig. 2?\n- The motivation for decoupling initialization and sampling is to improve the diversity of the initial prompts. Do you have evidence for this claim? From Table 1 it\u2019s unclear when we should use 1 LLM for both vs decoupling them.\n- Did you see convergence in the type of CoT prompts? Do they all end up following a similar format and is that problematic for some tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9008/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821338040,
        "cdate": 1698821338040,
        "tmdate": 1699637134267,
        "mdate": 1699637134267,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XkopEC5kDN",
        "forum": "tQqLV2N0uz",
        "replyto": "tQqLV2N0uz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9008/Reviewer_BA1Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9008/Reviewer_BA1Z"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a simple chain-of-thought (CoT) prompt optimization strategy that bares similarity to evolutionary algorithms.  They show that their method outperforms baselines such as few-shot prompting, human written CoT prompts, CoT with self-consistency decoding, and an adaption of Auto-CoT to use labeled data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper is well written and easy to follow\n* The algorithm is conceptually simple and easy to grasp and implement\n* The algorithm can be executed without access to model parameters (e.g through a closed source API)"
            },
            "weaknesses": {
                "value": "* This work is performing optimization to learn the CoT prompts, but does not compare against other prompt optimization methods,  prompt tuning, or fine-tuning.  The fact that optimized prompts beat human written CoT prompts is not surprising by default.  To evaluate the merit of the method one would need to compare against other schemes that use the training data to optimize the prompts at hand.\n* There is no ablation study.  It is not clear which elements of the algorithm are most important for it's success.\n\nI do not feel comfortable accepting this paper until there is evaluation against prompt optimization schemes at a similar compute budget.  Currently, by simply comparing against baselines it is too hard to gauge the contribution of this paper."
            },
            "questions": {
                "value": "Table 1: Perhaps add row depicting average performance for each method\n\nConsider using \\eqref to reference equations as it\u2019s helpful to have parenthesis around the number of the equation you\u2019re referencing for readability."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9008/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821356015,
        "cdate": 1698821356015,
        "tmdate": 1699637133769,
        "mdate": 1699637133769,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AwkUR1Jpz9",
        "forum": "tQqLV2N0uz",
        "replyto": "tQqLV2N0uz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9008/Reviewer_hjgQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9008/Reviewer_hjgQ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to automatically figure out the best K-shot chain-of-thoughts (CoT) examples that can be universally used in a task domain. The main idea is to leverage an iterative sampling method that is analogous to Gibbs sampling, where the algorithm starts from the \u2018thoughts\u2019 generated by the zero-shot CoT, and then in each iteration a random set of K-shot CoT is selected as the conditioning part, in order to regenerate the \u2018thoughts\u2019 for a given example. Experiments compared with several baselines show the consistent improvement, and the algorithm seems to be able to converge w.r.t t the training accuracy over the iterations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Automatically construction of the prompt is an important problem to tackle.\n- The idea that is analogous to the Gibbs sampling seems to be novel and interesting.\n- The empirical gain seems to be consistent, and the empirical justification of the algorithmic convergence seems to be convincing."
            },
            "weaknesses": {
                "value": "- Some more justification on the approximation of the equation 4) needs to be provided. Specifically, the eq 4) suggests some form of conditional independence structure in the fully connected graphical model, but how well it approximates (or how the neighborhood size matters) needs to be justified.\n\n- The few-shot experiment setting might need a bit more justification. For example if the authors used 20 examples during CoT generation, then it might be tricky to call it a K-shot learning when K examples are used during the inference stage. \n\n\n- Some latest baselines regarding the automatic prompt engineering need to be discussed or compared if appropriate, for example [1][2]. \n\n\n>**references**:\\\n[1] Better Zero-Shot Reasoning with Self-Adaptive Prompting, Wan et.al, ACL23 \\\n[2] Tempera: Test-time prompt editing via reinforcement learning, Zhang et.al, ICLR23"
            },
            "questions": {
                "value": "I\u2019d like to see the authors\u2019 response regarding my concerns listed above, namely\n- the quality w.r.t. the neighborhood size during Gibbs sampling\n- the experiment setup regarding the k-shot learning\n- the questions regarding the baselines"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9008/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699044437684,
        "cdate": 1699044437684,
        "tmdate": 1699637133361,
        "mdate": 1699637133361,
        "license": "CC BY 4.0",
        "version": 2
    }
]