[
    {
        "id": "9QdOFUJcfO",
        "forum": "PI6yaLXz3C",
        "replyto": "PI6yaLXz3C",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8894/Reviewer_K853"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8894/Reviewer_K853"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the combination of fairness methods and attention-based techniques in machine learning to reduce bias and improve model effectiveness. It proposes innovative approaches to minimize bias in machine learning algorithms, particularly in the context of graph-based data. The paper employs attention mechanisms to guide the model to focus on data that is less likely to introduce bias. It also discusses the role of contrastive learning in bringing similar data points closer together in the feature space, contributing to a more equitable model. The paper provides a thorough technical foundation, making it a valuable guide for those interested in the topic."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is notable for its creative fusion of methods to improve fairness and attention-based techniques to combat bias, bringing a fresh perspective to existing research. It lays a solid technical foundation, serving as a detailed guide for those new to the field as well as seasoned experts. Given the growing emphasis on fairness in machine learning, the relevance of the paper is heightened."
            },
            "weaknesses": {
                "value": "The paper falls short in clearly describing the empirical tests conducted to validate its findings, leaving room for improvement. Questions about the scalability of the proposed methods also remain unanswered, making it uncertain how they would perform on larger datasets or in different domains. In addition, the paper doesn't address the potential trade-offs between fairness and other issues such as accuracy, nor does it explore the ethical considerations associated with using machine learning to reduce bias."
            },
            "questions": {
                "value": "Please see the Strengths and Weaknesses sections."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8894/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698466599208,
        "cdate": 1698466599208,
        "tmdate": 1699637118928,
        "mdate": 1699637118928,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qV9PYQcXiL",
        "forum": "PI6yaLXz3C",
        "replyto": "PI6yaLXz3C",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8894/Reviewer_Nbdh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8894/Reviewer_Nbdh"
        ],
        "content": {
            "summary": {
                "value": "This work is concerned with the problem of fair representation learning, and in particular with how to debias representations in contrastive self-supervised learning. The authors identify limitation with current approaches, in that modelling assumptions about bias attribute are too strong, and they suggest instead a way to condition similarity scores between pairs of positives (or negatives) to a bias attribute via a proposed variant of a \u201cself-attention\u201d mechanism. At the same time, they extend their architectural intervention to a sparsified attention scheme using locality-sensitive hashing which the goal of masking out interactions between pairs which may help with the task. In addition, they propose alternative contrastive learning losses for training with supervision from the bias attribute."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Creating methods for debiasing representations learnt from training datasets containing spurious-correlations, label-imbalances, or sensitive attributes is an important problem.\n\nThe authors use existing literature on the relation between self-attention operator and kernels [1] to derive a similarity score for pairs which is conditioned on bias attribute information. Exploring new formulations of conditional similarity scores can be an interesting avenue.\n\n[1] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019. Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)."
            },
            "weaknesses": {
                "value": "1. The paper is poorly written. There are notation problems (e.g. in section 2 the matrix $U$ of similarity scores is overloaded, in section 3.3 there is no $z_i$ appearing in the loss even if it is sampled), incorrect math statements (like in section 3.1 that $\\phi(g(y))$ is used to estimate $\\mathbb{E}_{y|z} \\phi(g(y))$), missing important citations (like [1] for the derivations in page 5 of attention as a kernel-based similarity), and often times definitions (such as for the dataset used and the \u201cbias removal\u201d evaluation metric) are not self-contained in the paper.\n2. Novelty concerns: the FAREContrast objective function is essentially the same as the one described at [2].\n3. Sparsifying the attention is poorly motivated, and it incurs a considerable implementation cost for the induced performance benefit over the considered baselines.\n4. Empirical evaluation is very limited. The authors consider a variant of ColorMNIST, which is not described in the paper, and measure the top-1 test accuracy and a bias removal evaluation metric, which is not described. The paper needs to consider more benchmarks, such as CelebA (classifying hair color while the sensitive attribute is gender) [see benchmarks, 3], and evaluate according to fair/group-robust classification performance metrics (instead of iid accuracy), such as a group-balanced (or worst-case) test accuracy (depending on the dataset) and/or Equalized Odds [4, see 5 on how it is applied].\n\n[2] Yao-Hung Hubert Tsai, Martin Q Ma, Han Zhao, Kun Zhang, Louis-Philippe Morency, and Ruslan Salakhutdinov. Conditional contrastive learning: Removing undesirable information in self- supervised representations. arXiv preprint arXiv:2106.02866, 2021c.  \n[3] Sagawa, Shiori, et al. \"Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.\" arXiv preprint arXiv:1911.08731 (2019).  \n[4] Hardt, Moritz, Eric Price, and Nati Srebro. \"Equality of opportunity in supervised learning.\" Advances in neural information processing systems 29 (2016).   \n[5] Zhang, Fengda, et al. \"Fairness-aware contrastive learning with partially annotated sensitive attributes.\" The Eleventh International Conference on Learning Representations. 2022.  \n[6] Tsai, Yao-Hung Hubert, et al. \"Conditional contrastive learning with kernel.\" arXiv preprint arXiv:2202.05458 (2022)"
            },
            "questions": {
                "value": "See Weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8894/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8894/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8894/Reviewer_Nbdh"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8894/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698874480688,
        "cdate": 1698874480688,
        "tmdate": 1699637118819,
        "mdate": 1699637118819,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UZxXCRCVUc",
        "forum": "PI6yaLXz3C",
        "replyto": "PI6yaLXz3C",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8894/Reviewer_nzY3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8894/Reviewer_nzY3"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to learn fair feature representations through contrastive learning. \nSimilar to [1], the authors adopt a learning scheme that assigns weights to data p\bairs according to the similarity of sensitive attributes. This is based on the assumption that the samples with similar sensitive attributes will serve as 'bias-reducing samples', which is beneficial \bfor learning fair representations. The proposed method, FARE, for estimating the (conditional) similarity between the anchor and the negative samples utilizes attention-based weights instead of kernel-based weights [1]. The authors also propose an additional method, SparseFARE, that further sparsifies the attention map by discarding \u2018extreme bias-causing\u2019 samples. \bHowever, the experiments section seems incomplete, as the comparison with baselines is carried out solely on a synthetic dataset, and some important details about the experimental set-up are not provided."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* While existing works in fair contrastive learning often assume binary sensitive attribute setting, the two proposed approaches can be applied to settings with high-dimensional and continuous sensitive attributes.\n* When the only available data is the batch of triplets $\\set{(x_{i}, y_{i}, z_{i})}_{i=1}^{b}$, the conditional sampling procedure in the Fair-InfoNCE objective [2] can be addressed through the proposed attention-based approaches. \n* The kernel-based method assumes a pre-defined kernel for calculating similarity, but attention-based methods learn similarity adaptively from the task, which alleviates the need for such an assumption. \n* Attention-based methods can lead to improved computational complexity ($O(b^2)$ or $O(b\\log{b})$) compared to the kernel-based methods ($O(b^3)$)."
            },
            "weaknesses": {
                "value": "* The comparison with baselines is carried out solely on a synthetic dataset, ColorMnist [1]. Since this work is about learning fair representations, it seems necessary to consider experiments on fairness datasets (e.g., COMPAS, Adult), which are commonly used in the fairness literature, and to employ fairness criteria (e.g., Demographic Parity, Equalized Odds) for comprehensive assessment. Plotting a Pareto-frontier curve is an effective way to compare, especially when considering the accuracy-fairness trade-off.\n* Some important details for the proposed method such as the model architecture, batch size, and hyperparameter selection are not provided. For clarity and to ensure the paper is self-contained, it would be better to describe the specific procedures used.\n* Table 1 shows the result for CCLK [1] when using Cosine kernel, but [1] also provides a result for CCLK when Laplacian kernel is applied, showing Top-1 Accuracy of $85.0 \\pm 0.9$ and MSE of $72.8 \\pm 13.2$. Then, I'm not sure whether FARE indeed alleviates a significantly larger amount of bias compared to the baseline methods.\n* Given that the performance gain doesn\u2019t seem to be significant, it is not yet clear to me the benefits of the attention-based approach compared to the kernel-based approach. The kernel-based method relies on choosing an appropriate kernel, whereas the attention-based method focuses on training the model using data. However, it seems that more justification is required for the proposed method. It would be beneficial to include additional intuitive explanations on why attention-based methods are more effective than kernel-based methods for calculating the similarity of sensitive attributes, along with experimental results to support this. For instance, in Adult dataset, if \u2018age\u2019 is selected as the sensitive attribute, one could consider showing experimentally that the attention score tends to be higher when two individuals have similar ages, whereas this may not be the case with kernel-based methods.\n* Minor suggestions\n    * (p.4) \u201c~ Fair-InfoNCE objective in Eqn. 1\u201d \u2192 \u201c~ Fair-InfoNCE objective in Eqn. 1.\u201d\n    * (p.5) \u201cGiven 6 and the kernel density estimators in 7,\u201d \u2192 \u201cGiven Eqn. 6 and the kernel density estimators in Eqn. 7,\u201d\n    * (p.8) \u201cHence we only consider need to consider ~\u201d \u2192 \u201cHence we only consider ~\u201d\n    * (p.14) Consider adding 'Eqn.' for consistency."
            },
            "questions": {
                "value": "* In Table 1, the result for SpareFARE appears to use the adjacent bucket scheme, but it differs from the result in Table 2 of the appendix. Which is correct?\n* In Eqn. (12), does FARE use a feature map associated with the Cosine kernel for $\\phi$?\n\n\n[1]: Tsai et al. Conditional Contrastive Learning with Kernel. ICLR, 2022.\n\n[2]: Tsai et al. Conditional Contrastive Learning for Improving Fairness in Self-Supervised Learning. Arxiv, 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8894/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699363860310,
        "cdate": 1699363860310,
        "tmdate": 1699637118697,
        "mdate": 1699637118697,
        "license": "CC BY 4.0",
        "version": 2
    }
]