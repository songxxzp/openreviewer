[
    {
        "id": "Vkhhu96u5i",
        "forum": "L9NM2CEol3",
        "replyto": "L9NM2CEol3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4150/Reviewer_Yytj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4150/Reviewer_Yytj"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes Dynamic Tiering-based Federated Learning (DTFL), which aims to speed up FL via dynamically allocating training load to heterogeneous resource FL in different tiers. DTFL introduced a dynamic tier scheduler to cluster FL local clients into tiers and then leverage split learning to split different portions of the global model and deploy on local clients based on their tier. Additionally, the paper provides theoretical proof of the convergence properties of DTFL."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. DTFL leverages local-loss-based training and split learning, which enables dynamic offloading training workload for local clients with different resource capacities.\n\n2. DTFL introduced dynamic tier scheduling components to adaptively cluster local clients to different resource tiers and hence speed up training. \n\n3. The paper provides theoretical convergence analysis."
            },
            "weaknesses": {
                "value": "**1.** The experiment looks weak and can not support the arguments proposed in the paper.\n\n**2.** The tier scheduling metrics are unreliable. DTFL uses training time, communication time, and training time of the server-side model to profile the tier. However, using `actually time`  is unreliable, in computational devices (especially edge devices), many factors can affect the execution time for the same program, such as temperature, IO thread, etc. More standard metrics might be considered.\n\nAdditionally, in the experiments, the evaluation metric for training speed is unfair. The authors use total training **time in second** to evaluate the training speed of federated learning. However, simply tracking the training time is hard to avoid hardware and network traffic effects. Instead, more standard evaluation metrics should be used, such as **FLOPs, MACs, GPU Hours, electricity usage, total #trainable parameter**s, etc.\n\n**3.** The experiments is simulated on CPU and GPUs raising further concern on point 2 above.\n\n**4.** Delay on dynamic Tier Scheduling. DTFL uses total training time in previous communication rounds to tiering clients, it may not accurately reflect computational status in the current round.\n\n**5.** No experiments reflect the communication cost of the proposed method."
            },
            "questions": {
                "value": "Please kindly address the concerns I list in the weakness section.\n\nOverall, the experiments are incomprehensive and lack the evidence to support the argument of the paper. I'll change my mind if the authors add further fairness evaluation results.\nFor instance, use more fairness metrics to evaluate the speed, consider more system heterogeneity settings with more diverse resource profiles, etc."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4150/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698185623403,
        "cdate": 1698185623403,
        "tmdate": 1699636380501,
        "mdate": 1699636380501,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eDjYZR9f7K",
        "forum": "L9NM2CEol3",
        "replyto": "L9NM2CEol3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4150/Reviewer_f1S3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4150/Reviewer_f1S3"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new tiered-based split federated learning to handle heterogeneous environments where the resources of clients change over time. Specifically, the authors propose dynamic tier scheduling which operates through tier profiling and tier scheduling. Tier profiling tracks the training times of clients, which change over time, and based on this, the server estimates the current training times for each client in all tiers using EMA. Tier scheduling assigns clients to a tier according to their estimated time for efficient training. The authors provide theoretical convergence bound for both convex and non-convex loss functions. The empirical results show that the proposed method has fast convergence speeds over various baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors propose a new tiered-based split learning which can efficiently train large models depending on clients\u2019 resources in heterogeneous environments.\n- The convergence behavior of the proposed method is theoretically established\n- The proposed method significantly reduces the training time compared to existing works."
            },
            "weaknesses": {
                "value": "- It seems that this work only considers a scenario where computational and communication resources of all clients change over time. However, it\u2019s not clear that this is a reasonable scenario in practice. I think there might be more cases where the resources of only a portion of all clients change in real-world scenarios. To demonstrate the effectiveness of the proposed algorithm in various practical scenarios, it would be beneficial for the authors to conduct additional experiments where they vary the proportion of the devices whose training times change over time.\n- What are some practical applications in which the resources of each client changes over time? It would be helpful to describe some examples of such applications to emphasize the importance of addressing the targeted problem. \n- Ablation studies should be performed to confirm the effect of each component. First, a comparison with local-loss based SFL [1] (not tiered-based) should be considered. Secondly, using local-loss based SFL, a comparison between static tiered methods and the proposed dynamic tiered method seems necessary. Finally, to see the effect of EMA in tier profiling, the author should compare the results of tier profiling with and without EMA. \n- Overall, the technical novelty of the proposed method seems limited. I feel that tier profiling and scheduling are straightforward approaches based on the previous works. \n\n[1] Han et al., \"Accelerating federated learning with split learning on locally generated losses.\" In ICML 2021 Workshop on Federated Learning for User Privacy and Data Confidentiality. ICML Board, 2021."
            },
            "questions": {
                "value": "See Weaknesses and,\n\n- The main results only provide the training time required to achieve the target accuracy. What is the maximum accuracy that each method can achieve? \n- There is a type: cross-solo -> cross-silo"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4150/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4150/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4150/Reviewer_f1S3"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4150/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698383151760,
        "cdate": 1698383151760,
        "tmdate": 1699636380238,
        "mdate": 1699636380238,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WXfkbBWm3c",
        "forum": "L9NM2CEol3",
        "replyto": "L9NM2CEol3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4150/Reviewer_ncgY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4150/Reviewer_ncgY"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an approach to address the challenges posed by heterogeneity in Federated Learning (FL) systems. The proposed Dynamic Tiering-based Federated Learning (DTFL) system leverages the concept of Split Learning to dynamically offload portions of the global model to different tiers of clients, thereby mitigating the straggler problem and reducing the computation and communication demand on resource-constrained devices."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n\n2. The extensive experiments validate the proposed method."
            },
            "weaknesses": {
                "value": "1. The primary question I have regarding this paper pertains to its motivation. While there is a wealth of prior work on heterogeneous computation and communication capacities in the Federated Learning (FL) setting\u2014such as clients training heterogeneous models, clients performing partial training based on their individual abilities, asynchronous updates, and lightweight training with pre-trained models\u2014the proposed method introduces a requirement for the server to update its model with labels, which raises privacy concerns. Therefore, it is crucial to understand the advantages of the proposed method.\n\n2. It is good that the authors have included a convergence analysis. However, the convergence rate presented in Theorem 1 appears to be suboptimal compared to classical FL settings that have been studied previously.\n\n3. The authors are encouraged to provide more results in non-IID settings, similar to the approach demonstrated in Figure 2. Additionally, it would be beneficial if Figure 2 could display the entire coverage process, as it is currently truncated."
            },
            "questions": {
                "value": "Please see the weakness section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4150/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698679435686,
        "cdate": 1698679435686,
        "tmdate": 1699636380146,
        "mdate": 1699636380146,
        "license": "CC BY 4.0",
        "version": 2
    }
]