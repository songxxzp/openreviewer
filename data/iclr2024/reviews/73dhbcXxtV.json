[
    {
        "id": "rS4sgdb7gT",
        "forum": "73dhbcXxtV",
        "replyto": "73dhbcXxtV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2772/Reviewer_JYqN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2772/Reviewer_JYqN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel framework called LOLAMEME including two instantiations, the LoLa language, and MeMe languages. Then it introduces a hybrid architecture T HEX and compares it with transformer-based GPT-2 and convolution-based Hyena based on the LOLAMEME framework. Furthermore, this work conducts comprehensive experiments to demonstrate the effectiveness of T HEX in different tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The new framework LOLAMEME similar to natural language is impressive and interesting.\n2. This work builds multiple datasets with several billion tokens based on the LOLAMEME framework, which would contribute to future research.\n3. This work performs comprehensive experiments over these datasets and a related benchmark dataset to show the effectiveness of the new framework."
            },
            "weaknesses": {
                "value": "1. The motivation for the model design is not clearly discussed in this work. I am confused about the differences among T HEX, GPT-2, and Hyena. \n2. The structure of this paper is not clear enough, which is very hard to follow.\n3. I would suggest that an illustration figure be provided to clearly show the main idea of the LOLAMEME framework, which will make this work easier to understand.\n4. Some sentences should be revised and the format should be unified. For instance, in the Abstract, \"We extend current mechanistic schemes to incorporate Logic, memory, and nuances of Language such as latent structure\", I am curious why the first letter of Logic and Language in this sentence are capitalized."
            },
            "questions": {
                "value": "1. What are the differences among T HEX, GPT-2, and Hyena? \n2. What are the advantages of T HEX? \n3. Why not provide an illustration figure to show the model framework?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2772/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817351097,
        "cdate": 1698817351097,
        "tmdate": 1699636220237,
        "mdate": 1699636220237,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0IXhuVaaLJ",
        "forum": "73dhbcXxtV",
        "replyto": "73dhbcXxtV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2772/Reviewer_5Z2e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2772/Reviewer_5Z2e"
        ],
        "content": {
            "summary": {
                "value": "This paper talks a lot about mechanistic interpretability and evaluation of models with different architectures on synthetic benchmarks to generate more understanding of how they work. I fail to understand completely what insight is gained here."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "."
            },
            "weaknesses": {
                "value": "The only changes done to the transformer architecture is to replace a single layer by a layer from the hyena model. The variations include only replacing a different layer of the transformer with the same hyena layer. Lots of experiments are done to compare the performance of variants and measure the impact on the quality under different input lengths, on some synthetic datasets, etc. But I don't see any insight that could be won from these experiments."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2772/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698964265910,
        "cdate": 1698964265910,
        "tmdate": 1699636220151,
        "mdate": 1699636220151,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "n6PUXYc1HN",
        "forum": "73dhbcXxtV",
        "replyto": "73dhbcXxtV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2772/Reviewer_iHzK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2772/Reviewer_iHzK"
        ],
        "content": {
            "summary": {
                "value": "This article introduces a novel framework called LOLAMEME that expands current mechanistic schemes to incorporate Logic, memory, and nuanced aspects of Language, such as latent structure. By using this framework, the authors compare three generative language model architectures: GPT-2 (transformer-based), Hyena (convolution-based), and proposed hybrid architecture T HEX which are constructed by replacing certain layer of the Hyena model with the GPT-2 layer. To instantiate LOLAMEME, the authors introduce two different manifestations, LoLa and MeMe, and evaluate the performance of the architectures across various aspects of language. The findings demonstrate that T HEX surpasses GPT-2 and Hyena on select tasks as well as a related benchmark dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This work proposes a new hybrid architecture based on transformer-based GPT-2 and convolution-based Hyena. Experiments demonstrate the superiority of this architecture."
            },
            "weaknesses": {
                "value": "\u2022\tThe motivation and problem formulation of this work is unclear. And the novelty and contribution of this paper are somewhat limited. The proposed new architectures are simply constructed by replacing certain layers of the Hyena model with the GPT-2 layer. Although some experiments demonstrate better performance on the proposed two test datasets, there may be a lack of validation experiments on other existing datasets. Additionally, providing some interesting findings or interpretations about the experiments through a deeper analysis of the proposed architectures should be better.\n\u2022\tThe construction procedure of the two datasets should be explained more clearly, and deeper consideration should be given to whether the experimental settings can reliably reflect the behavior of the related models, such as memorization and in-context learning.\n\u2022\tThere are quite a few typo errors, including grammar and table issues, in this paper. For example, in the abstract, it states \"We propose the hybrid architecture T HEX and use LOLAMEME framework is used to compare three architectures.\" There are also typo errors in tables 3, 4, and 5. The grammar, figures, and tables in this paper may require some polishing.\n\u2022\tThe related work on mechanistic interpretability is not comprehensive. In fact, there is a considerable amount of work such as [1], [2], [3] attempting to interpret and understand the mechanisms of LLMs.\n\u2022\tIn section 6.5, it is unclear why TH EX-11 to T HEX-15 showed a loss of 0 after a few epochs but showed an exact match of 0. Further clarification or explanation is needed for this inconsistency.\n[1] A Mathematical Framework for Transformer Circuits\n[2] Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\n[3] Investigating Gender Bias in Language Models Using Causal Mediation Analysis"
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2772/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2772/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2772/Reviewer_iHzK"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2772/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699372626121,
        "cdate": 1699372626121,
        "tmdate": 1699636220090,
        "mdate": 1699636220090,
        "license": "CC BY 4.0",
        "version": 2
    }
]