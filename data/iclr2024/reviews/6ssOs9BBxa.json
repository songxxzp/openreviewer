[
    {
        "id": "3DWtkC4vW2",
        "forum": "6ssOs9BBxa",
        "replyto": "6ssOs9BBxa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6419/Reviewer_Pjfo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6419/Reviewer_Pjfo"
        ],
        "content": {
            "summary": {
                "value": "This paper presents AnonymizedAI, the first Deep Reinforcement Learning (DRL) agent to win the IEEE microRTS competition. AnonymizedAI's training process involved transfer learning to specific maps, which was critical to its winning performance. The paper also discusses the challenges of debugging and fine-tuning a DRL implementation, as well as the potential benefits of combining Imitation Learning and DRL. The contributions of this paper are:\n1. Introducing AnonymizedAI, the first DRL agent to win the IEEE microRTS competition.\n2. Demonstrating the importance of transfer learning to specific maps in achieving competitive performance.\n3. Providing insights into the challenges and potential benefits of using DRL in real-time strategy games."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Quality: The paper provides detailed information on AnonymizedAI's architecture, training process, and performance, as well as insights into the challenges and potential benefits of using DRL in real-time strategy games.\nClarity: The paper is well-organized and clearly written, with sections on Introduction, Related Work, Methodology, Results, and Conclusion."
            },
            "weaknesses": {
                "value": "the contribution lacks novelty, the network architecture mainly references existing algorithm networks. At the same time, a method that can well solve the performance under multiple different maps is not proposed."
            },
            "questions": {
                "value": "Q1: how to ensure the high performance of the model in the new hidden map or an untrained map?\nQ2: the masking significantly reduces the action space per turn and makes training more efficient, and how to determine and obtain the action needs to be masked?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6419/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6419/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6419/Reviewer_Pjfo"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697706489225,
        "cdate": 1697706489225,
        "tmdate": 1699636715744,
        "mdate": 1699636715744,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SjbdSs0LBp",
        "forum": "6ssOs9BBxa",
        "replyto": "6ssOs9BBxa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6419/Reviewer_7eJX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6419/Reviewer_7eJX"
        ],
        "content": {
            "summary": {
                "value": "The authors describe their entry to the IEEE-CoG2023 microRTS competition, where their entry won, thus \nbecoming the first deep RL agent to win the competition. This is a competition to produce the best agent on the microRTS environment. RTS environments are difficult for RL agents because of their complex game structure with varying unit types and strategies, large action space, long episodes and sparse rewards.\n\nThe authors apply a number of implementation tricks to improve on existing methods, including using a value function that predicts three different rewards that vary throughout training and training 7 different networks and selecting based on the map. They then train an agent using behaviour cloning and then fine-tune it with deep RL."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper has a few notable strengths:\n- The technical details of their implementation are very clearly described. \n- Winning the IEEE MicroRTS competition while being the first deep RL agent to do so is clearly a notable achievement."
            },
            "weaknesses": {
                "value": "However, the paper has a few notable weaknesses. In particular, although an impressive feat of engineering, I gained little insight about which parts of their design were particularly important, how exploitable their method was, how to improve the performance on larger maps and the required deeper strategy and other important research questions surrounding designing a good RTS agent. The submission could be significantly improved by focussing more on understanding why and how the system itself works. For example, the paper could include ablations of the different design decisions, or attempt to train a deep RL agent on the largest map. Instead the paper is mostly a grab-bag of previously-known techniques that combine to produce a very good agent."
            },
            "questions": {
                "value": "- You mention that scaling the behaviour cloning loss by the number of units that could take an action was critical to get it to train. Have you investigated why this is?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6419/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6419/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6419/Reviewer_7eJX"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697911486033,
        "cdate": 1697911486033,
        "tmdate": 1699636715605,
        "mdate": 1699636715605,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ciHcLdSkl6",
        "forum": "6ssOs9BBxa",
        "replyto": "6ssOs9BBxa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6419/Reviewer_nGmd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6419/Reviewer_nGmd"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a DRL model named AnonymizedAI, which is the first DRL method to win the IEEE microRTS competition. It defeated two prior competition winners in the IEEE microRTS (\u03bcRTS) competitions hosted at CIG and CoG. Its success largely benefits from iteratively fine-tuning the base policy and transfer learning to specific maps, which can be an economic method for training low-cost and efficient DRL agents."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Originality:**\n\nThis paper presents a novel DRL training paradigm, which fine-tunes the base policy and transfers the policy to new scenarios. The proposed method won the IEEE microRTS competitions.\n\n**Quality:**\n\nTo demonstrate the priority of the proposed method, this paper conducted massive experiments and presents detailed implementation of the novel method. The experimental results in the microRTS scenarios are convincing.\n\n**Clarity:**\n\nOverall, this paper is easy to follow. This article dedicates a considerable amount of text to elaborating on various technical details.\n\n**Significance:**\n\nThis paper investigates an important research problem, i.e., training RTS AI efficiently with DRL. The proposed training method brings insights to the DRL community"
            },
            "weaknesses": {
                "value": "Despite the fact that this paper presents a paper with convincing experimental results, I list some weaknesses:\n\n1. This paper covers a considerable amount of text on technical details, such as policy networks and speeding up inference. However, it is hard to gain insights for training DRL agents on other complex scenarios, such as Mahjong and Stratego. [See the questions below]\n2. The proposed method is a combination of prior methods. Contribution on DRL algorithm is limited.\n3. Discussions on some related works are missing, such as SCC [1], it achieves top human performance defeating GrandMaster players in test matches and top professional players in a live StarCraft II event with order of magnitude less computation.\n\n**Reference:**\n\n[1] Wang et al. SCC: an efficient deep reinforcement learning agent mastering the game of StarCraft II."
            },
            "questions": {
                "value": "1. In Sec. 2.1, why self-play failed in UAS and GridNet?\n2. Why did not you use supervised learning?\n3. Why does AnonymizedAI load 7 different policy networks?\n4. What is Squnet?\n5. What is the key takeaway for readers who want to use this proposed pipeline to train DRL agents on other complex scenarios, such as Stratego, Mahjong, PUBG and Honour of Kings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6419/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6419/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6419/Reviewer_nGmd"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698247428989,
        "cdate": 1698247428989,
        "tmdate": 1699636715486,
        "mdate": 1699636715486,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0KiYaCKtYx",
        "forum": "6ssOs9BBxa",
        "replyto": "6ssOs9BBxa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6419/Reviewer_AVMG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6419/Reviewer_AVMG"
        ],
        "content": {
            "summary": {
                "value": "The paper presents AnonymizedAI, a Deep Reinforcement Learning (DRL) agent, which is the first of its kind to secure a win in the IEEE microRTS competition. AnonymizedAI exploits a combination of carefully fine-tuned base policies and map-specific transfer learning to outperform previous competition winners. The paper describes the implementation of AnonymizedAI, including its 7 trained neural networks and the significant effort required to train and debug the agent, emphasizing the role of iterative fine-tuning and transfer learning in the agent's success. Furthermore, the authors discuss potential improvements in inference time and explore behavior cloning and its potential to bootstrap models with novel behaviors."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The major novelty of this work is the application of iterative fine-tuning and transfer learning to a DRL agent in the microRTS competition. The victory of AnonymizedAI in the competition demonstrates the effectiveness of the DRL approach in complex strategy games.\n\n2. The paper is well-structured and clear in its explanations, making the complex mechanisms behind AnonymizedAI accessible to readers."
            },
            "weaknesses": {
                "value": "1. Although the developed agent provides an innovative solution in the realm of the \u00b5RTS competition, the novelty of the techniques employed\u2014transfer learning and iterative fine-tuning\u2014within the broader context of DRL is somewhat limited as these techniques are already widely adopted.\n2. The authors didn't delve into an analysis of diverse self-play strategies that could potentially improve the agent's performance. Considerations for strategies beyond basic self-play, such as fictitious self-play[1] or more complex schemes[2], would have enriched this study.\n\n- [1] Heinrich, Johannes, Marc Lanctot, and David Silver. \"Fictitious self-play in extensive-form games.\" International conference on machine learning. PMLR, 2015.\n- [2] Lin, Fanqi, et al. \"TiZero: Mastering Multi-Agent Football with Curriculum Learning and Self-Play.\" arXiv preprint arXiv:2302.07515 (2023)."
            },
            "questions": {
                "value": "1. It would be interesting to know whether the authors plan to explore the inclusion of more advanced self-play strategies, and how they believe these could potentially impact AnonymizedAI's performance.\n\n2. The authors have mentioned the possibility of using the imitation learning approach for bootstrapping models with novel behaviors. Could they provide more insights into potential novel behaviors they are considering and how these could improve the agent's performance?\n\n3. I did not find a description of the hardware information for training the agents in the paper, such as the CPU model and number of cores, GPU model and number of cores, the size of training memory usage, etc. I wonder if the author could provide this information."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698653359109,
        "cdate": 1698653359109,
        "tmdate": 1699636715354,
        "mdate": 1699636715354,
        "license": "CC BY 4.0",
        "version": 2
    }
]