[
    {
        "id": "HJwWHNFcWU",
        "forum": "ATQSDgYwqA",
        "replyto": "ATQSDgYwqA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5657/Reviewer_crxL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5657/Reviewer_crxL"
        ],
        "content": {
            "summary": {
                "value": "This paper presents the Diffusion Random Feature Models (DRFMs) as a solution to ongoing challenges in both interpretability and computational complexity of diffusion models. The introduced DRFM yields numerical outcomes comparable to those of a fully connected neural network with an equivalent number of trainable parameters. From a theoretical perspective, the authors expand the random features and establish generalization bounds between the sampled data distribution and the actual distribution, leveraging score-matching properties. The efficacy of the proposed method is validated experimentally using samples from the Fashion MNIST dataset and instrumental audio data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The presentation is overall clear and easy to follow. \n\n- The overall method is clearly stated. The mathematical notations are well-defined, but some are redundant. The theoretical results are sound. The authors have made comprehensive proof and have quantified the error in the estimation, which is good.  \n\n- The authors expand the random features and establish generalization bounds between the sampled data distribution and the actual distribution, leveraging score-matching properties, making contributions in both perspectives."
            },
            "weaknesses": {
                "value": "### Novelty\n\n- The proposed framework is inspired by boththe denoising diffusion probabilistic model introduced by Ho et al. (2020), and the semi-random features introduced by Kawaguchi et al. (2018). It seems the proposed Diffusion Random Feature Model (DRFM) is incremental in integrating the above two works. Specifically, in Kawaguchi et al. (2018), a parameterization of one single-hidden layer is defined in a similar form to Eq (15). The authors employ this parameterization for the noise-prediction network in training DDPM. \n\n- Throughout the paper, it is not straightforward to understand the advantage of the parameterization in Eq (15) in terms of interpretability and computation complexity. Any analysis or experimental results should be provided to address these challenges stated in the introduction.\n\n### Technical quality\n- The experiments validate the efficacy of the proposed DRFM. However, when considering the challenges diffusion models face in terms of interpretability and computation, the experiments seem to omit direct comparisons. Additionally, many of the presented results are not quantified using specific metrics, as observed in Fig 2. This makes it challenging to identify the model's superior performance.\n\n- The paper currently compares the DRFM solely with a fully-connected layer. Given the prominence of both the Unet and transformer architectures in noise prediction networks, it would be beneficial for the authors to discuss the relationship of DRFM to these models. In particular, considering that transformer layers utilize a combination of the attention mechanism and fully-connected MLPs, they align more closely with the scope of the fully-connected layer baseline. It would be valuable to explore this connection further.\n\n- Since the authors have generalized the random features with the properties of score-matching, it remains unknown whether DRFM can improve the performance in the tasks evaluated in Kawaguchi et al. (2018).\n\n### Presentation quality\n- In the abstract, the authors state that diffusion models suffer from a lack of theoretical justification. This assertion is somewhat unclear and could benefit from further specificity. It would be helpful if the authors could elucidate whether this deficiency refers to aspects within statistical, optimization, or other theoretical domains.\n\n- Mathematical notations are redundant and inconsistent. For example, \n$\\tilde \\epsilon_i$ does not seem to be used anywhere in section 2.1 the following sections. \n\n- Since this work is inspired by the DDPM, and the semi-random features. The authors need to introduce the semi-random features liked done in section 2. Without the knowledge of semi-random features, it is hard to understand the importance and contribution of the proposed method."
            },
            "questions": {
                "value": "Please see the point-to-point review in the Weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5657/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698643160568,
        "cdate": 1698643160568,
        "tmdate": 1699636589379,
        "mdate": 1699636589379,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "D6Kum3sXMz",
        "forum": "ATQSDgYwqA",
        "replyto": "ATQSDgYwqA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5657/Reviewer_2YaV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5657/Reviewer_2YaV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to model the score of image distributions with a one-hidden-layer random features network. The first layer weights are frozen to their initialization, while the second layer weights are learned and depend on the noise level (through a factorization to reduce the number of parameters). The model is compared with a classical one-hidden-layer network where the first layer is learned and a random features model where the dependence on the noise level is not trained."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is clearly written (except for section 2 which redundantly exposes both the discrete and continuous formulations of diffusion models).\nThe parameterization of the second-layer weights dependence on the noise level is interesting and to the best of my knowledge novel."
            },
            "weaknesses": {
                "value": "As noted in the final sentence of the conclusion, there is nothing deep about this random features model (despite the name), leading to a very weak expressivity that is plagued by the curse of dimensionality. The paper thus studies a very restricted, toyish setting. This is not a problem for a theoretical paper, but the theoretical analysis is lacking in rigour and novelty. In particular, Theorem 3.3 is a straightforward combination of classical results (from Rahimi and Recht and Chen et al.) and does not state key assumptions (namely, that the true scores of the image distribution are in some function class for all noise levels). It also only tackles score approximation, not optimization and generalization from a finite training set, but this is not stated in the text. Finally, the bound holds in probability, but is stated as if it were almost sure. On the numerical side, there are no quantitative comparisons between the different approaches, and I'm frankly not convinced that the proposed approach really outperforms the one-hidden-layer neural network."
            },
            "questions": {
                "value": "Minor suggestions:\n- choose either discrete or continuous framework to present diffusion models\n- Typo on page 4: \"features [if -> of] the input data\"\n- page 5: \"random and trainable\" is confusing phrasing\n- notation $p_\\theta$ for the network is confusing (usually used for probability density), prefer e.g. $\\varepsilon_\\theta$\n- Put definition of $\\mathcal F_\\omega$ in the main text if it is used in a lemma\n- How can denoising requires 100 steps when it should just be a direct evaluation of the score network? This should be explained in the text."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5657/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5657/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5657/Reviewer_2YaV"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5657/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698698683236,
        "cdate": 1698698683236,
        "tmdate": 1699636589240,
        "mdate": 1699636589240,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qn3Mf1zCfB",
        "forum": "ATQSDgYwqA",
        "replyto": "ATQSDgYwqA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5657/Reviewer_3pnu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5657/Reviewer_3pnu"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors propose a diffusion model-inspired random feature model. This model is interpretable and is able to achieve comparable numerical results to a fully connected neural network (of the same size). The authors then give generalization bounds between the distribution of sampled data and the true distribution for their proposed model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors propose a diffusion RF model. The model they propose is quite simple and interpretable.\n- The model achieves the same performance as a fully connected network on some toy datasets.\n- The generalization bound in Theorem 3.3 is novel and interesting."
            },
            "weaknesses": {
                "value": "- Most of the heavy lifting in the proof of Theorem 3.3 is done in Lemma A.2 which is from Chen et al. (2022).\n\n- The authors only experiment with very simple and small datasets such as fashion MNIST. It is not that clear what happens for more complex datasets. Here, in the diffusion RF model, feature learning is absent because W is fixed at initialization. It is hard to believe that the model can be successful for more complex tasks.\n\n- The main goal of the paper is not clear to me. Are the authors proposing the RF model as a tool to decrease computational complexity (like Rahimi and Recht) or some theoretical tool to analyze a more sophisticated phenomenon (like Mei and Montanari, 2019)? Do the authors expect such models to achieve similar accuracies to state of the art methods? \n\nI agree that there models are analyzable; e.g., Theorem 3.3 is interesting. However, what phenomenon is this model tryin to describe? For example (Mei and Montanari, 2019) and others use RF models to analyze double descent and to show that this phenomenon exists even in very simple models. \n\n- What motivates this particular choice of random features? Why did you choose W to be the random weights and train the other parameters?\n\n- I think in the model that the authors are analyzing, it is more interesting to study the case where the dimension d is proportional to N. Can the authors explain what will happen in that regime to the bound in (19)? In general, I think a more thorough discussion after Theorem 3.3. would benefit the paper a lot. What do we understand from the generalization bound?"
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5657/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5657/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5657/Reviewer_3pnu"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5657/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699020116956,
        "cdate": 1699020116956,
        "tmdate": 1699636589142,
        "mdate": 1699636589142,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SCXP8yj3ZX",
        "forum": "ATQSDgYwqA",
        "replyto": "ATQSDgYwqA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5657/Reviewer_2JVx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5657/Reviewer_2JVx"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to use random features in the training of diffusion models for interpretability and ease of computation. In more details, the authors estimate the score function by a class of functions generated by random features. They demonstrate the method on FASHION-MNIST and Audio data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method replaces deep neural nets, which are commonly used when approximating score functions, with random feature functions and shows that experimentally the new method is competitive."
            },
            "weaknesses": {
                "value": "1. The theorem 3.1 essentially combines error decomposition results from diffusion models and approximation results from random feature functions. Is it possible to have more fine-grained results for the specific feature map (Eq 17) chosen in the paper?\n\n2. How do other choices of random feature maps behave compared to the sin/cos map chosen in the current paper? A sensitivity analysis would improve the validity of the paper."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5657/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699022568089,
        "cdate": 1699022568089,
        "tmdate": 1699636589020,
        "mdate": 1699636589020,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dvq3b2sHDb",
        "forum": "ATQSDgYwqA",
        "replyto": "ATQSDgYwqA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5657/Reviewer_vFu7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5657/Reviewer_vFu7"
        ],
        "content": {
            "summary": {
                "value": "The paper applies the idea of random features model to the diffusion-based generative model. Specifically, the paper proposes using a denoiser of the form Eq. (15), where $W, b$ are fixed and $\\theta^1, \\theta^2$ are trained. Then, the paper presents theoretical results regarding the TV distance between the true distribution and the estimated distribution. Experiments on fashion-MNIST and audio data are provided, and the advantage over other methods is discussed."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper provides a well-written description of diffusion model and related concepts.\n\n- The proposed algorithm is straightforward to understand.\n\n- Combining random features model and diffusion model is a novel approach."
            },
            "weaknesses": {
                "value": "- The random features model was first introduced as an approximation scheme for the kernel method and later used as a toy model for studying neural networks. I personally understand it as a theoretical tool rather than a practical algorithm. Therefore, I think that the idea of using the random features model in generative modeling is not particularly interesting unless it provides a fundamentally new understanding of diffusion-based generative modeling. While the paper provides some theoretical results, they appear to be simple extensions of previously known results.\n\n- To my understanding, the finding of the experiment section can be summarized as \"training only $\\theta^1, \\theta^2$ showed better performance than training all parameters or training only $\\theta^2$\". Can the authors provide results for different $N$'s? The current experiment is using huge $N$, and it is not surprising that restricting the model class can lead to a better generalization."
            },
            "questions": {
                "value": "- Can the authors provide other popular evaluation metrics for generative models such as inception score, Frechet inception distance, etc.?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5657/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5657/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5657/Reviewer_vFu7"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5657/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699289121407,
        "cdate": 1699289121407,
        "tmdate": 1699636588907,
        "mdate": 1699636588907,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tfsArf2EuL",
        "forum": "ATQSDgYwqA",
        "replyto": "ATQSDgYwqA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5657/Reviewer_58mK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5657/Reviewer_58mK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new model, called a diffusion random feature model (DRFM), that incorporates the interpretability of random feature models with the data-generating capability of diffusion models. This hybrid model, DRFM, aims to produce results comparable to fully connected neural networks while maintaining interpretability and using a comparable number of trainable parameters. The model's effectiveness is tested through experiments on the fashion MNIST dataset and instrumental audio data. The results demonstrate that DRFM can learn to generate data from a limited number of training samples (as few as one hundred) and within a limited number of timesteps (one hundred)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper highlights the interpretability and computation efficiency of the DRFM, noting that it allows for the derivation of theoretical upper bounds on the quality of the samples generated. Numerical experiments show that DRFM outperforms both a traditional fully connected network with all layers trainable and a standard random features model where only the last layer is trainable. The result is clear and opens up a theoretical direction for analyzing random feature models with high-dimensional spaces for diffusion models."
            },
            "weaknesses": {
                "value": "1. I did not get the point of focusing on random Fourier-type feature models in Eq. (15). Following, the proof of the paper, it is possible to include more general activation functions. I did not find the motivation why we only consider this specific random feature model defined in Eq. (15) and Eq. (16). The authors may need to provide additional explanation of the choice of the random feature or empirical evidence of the advantage of this specific random feature model.\n\n2. The theory of this paper seems to directly come from Chen et al. (2022) and Ranhimi & Recht (2008b; a). The authors may need to emphasize the difference between the current paper and the references, and the difficulty in the analysis in the current paper. In particular, the authors derived the generalization bounds via the approximation error of the random feature model, which, I think, lacks explanations in the proof in the Appendix.\n\n3. Although there may not be a theory, for completeness, the authors may need to provide additional simulations for deeper neural networks with random features, showing the advantage of this architecture, and comparing it with more commonly used U-Net or other neural network models. The random feature models will reduce the computation complexity but I am not sure the performance is still comparable with conventional diffusion models."
            },
            "questions": {
                "value": "1. It is a standard practice to introduce an abbreviation by providing its full form for the first time. For instance, DDPM appears in the paper first time without explaining the full name.\n\n2. Between (5) and (6), there are typos in $q(x_{k-1}|x_k,x_0)=\\mathcal{N}(\\tilde{\\mu}_t,\\tilde{\\beta}_t)$: you should specify $t$ and insert identity matrix for covariance. And the same issue for (6).\n\n3. Please explain ''U-Nets not only preserve the dimension of input data, they also apply techniques such as downsampling using\nconvolution which helps to learn the features if the input data.'' on page 4.\n\n4. In Algorithm 1, line 4, did you use uniformly sampling from $K$ time points to update the training parameters? For line 7, how do you minimize the loss $L$? Did you assume it will attain some global minimizer after training? Further explanations may be needed.\n\n5. In Lemma 3.1, please explain the notion $\\mathcal{F}_{\\omega}$ and $\\mathbf{\\theta}_j^{(2)}$.\n\n6. In Lemma 3.2, is $\\rho(\\omega)$ the density function of $\\rho$? \n\n7. In Theorem 3.3, you assume a bounded second moment for $q(x_0)$ but Chen et al. (2022) require $(2+\\eta)$-th moment bound for some $\\eta>0$. Is there any extra work to relax this assumption in your proof? Right now, there is a lack of explanation of the proof and how you applied Lemmas 3.1, 3.2, and A.2 to conclude the main theorem. Lemma 3.2 shows the approximation error of the random feature model but in the DRFM, how do you use (18) to finish the proof of Theorem 3.3 and ensure $|\\theta^{(2)}_{ij}|$ are always uniformly bounded by some constant?\n\n8. Any explanation for why neural networks may fail in denoising images in Figure 3 sometimes but DRFM works better and is stable?\n\n9. There should be more details of how you trained the models in the captions of Figures 2-7, e.g. which optimizers are used."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5657/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5657/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5657/Reviewer_58mK"
                ]
            }
        },
        "number": 6,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5657/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699503602742,
        "cdate": 1699503602742,
        "tmdate": 1699636588806,
        "mdate": 1699636588806,
        "license": "CC BY 4.0",
        "version": 2
    }
]