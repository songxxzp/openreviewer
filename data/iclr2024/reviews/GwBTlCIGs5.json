[
    {
        "id": "aiJkjK1x8C",
        "forum": "GwBTlCIGs5",
        "replyto": "GwBTlCIGs5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6318/Reviewer_QPCx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6318/Reviewer_QPCx"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzed two self-supervised contrastive methods including Barlow Twins and VICReg. Based on these loss functions, they reveal that the feature's orthogonality has more impact than the projection head's dimensionality. Next, they find that using more data augmentations for each image can improve the learned representations and reduce the training dataset size while not sacrificing accuracy. Experiments are conducted in small datasets CIFAR-10 and STL-10."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ It provides a new perspective on SSL with non-contrastive approaches. \n+ Self-supervised learning with a strategy that reduces the data needed to learn good representations may benefit the community. \n+ The writing is clear"
            },
            "weaknesses": {
                "value": "There are several concerns that have been raised:\n+ The finding that more data augmentations can help learn better representations is well-known, many works already show that multi-crop can help contrastive learning achieve better performance (DINO [1], SWAV [2], MSF [3], iBOT, etc ...)\n\n+ Experiments are insufficient to demonstrate their effectiveness where only small-scale datasets (in terms of both resolution and dataset size) are conducted. I recommend verifying and evaluating the method on more challenging datasets such as ImageNet 224x224, which is a benchmark in this field.\n\n+ Several Non-contrastive methods such as BYOL and SimSiam or DINO have not involved any negative samples but they are not adequately compared. \n\n+ The experimental setting is also not sufficient since it only trained for 100 epochs on CIFAR-10 and 50 epochs on STL-10, some methods with more augmentations can have a fast convergence, but for longer training that might be diminished. This setting is not practical where most SSL methods are not converged (as shown in the SimSiam paper). Please see [4] for reference where all SSL methods are conducted at least 1000 epochs.\n\n+ Lack of comparisons with other SSL methods would weaken the impact of the paper. \n\n+ The paper claims that more augmentations would help the representation learning quality but some figures show that 8 augmentations perform worse than 4 augmentations, this may be contradicting to that claim.\n\n\n[1] Emerging Properties in Self-Supervised Vision Transformers, ICCV 2021 \\\n[2] Unsupervised Learning of Visual Features by Contrasting Cluster Assignments, NeurIPS 2020 \\\n[3] Mean Shift for Self-Supervised Learning, ICCV 2021 \\\n[4] solo-learn: A Library of Self-supervised Methods for Visual Representation Learning, JMLR 2022"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6318/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6318/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6318/Reviewer_QPCx"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6318/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698225873264,
        "cdate": 1698225873264,
        "tmdate": 1700665456049,
        "mdate": 1700665456049,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5oiNv5s4Ud",
        "forum": "GwBTlCIGs5",
        "replyto": "GwBTlCIGs5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6318/Reviewer_ArGT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6318/Reviewer_ArGT"
        ],
        "content": {
            "summary": {
                "value": "This submission follows a recent line of work and studies self-supervised representation learning from a kernel perspective. It shows that non-contrastive learning methods such as Barlow Twins and VICReg can find the eigenfunctions of the integral operator of the augmentation kernel (positive-pair kernel), and then claims that a low-dimensional representation is sufficient, and using more diverse augmentations can improve pretraining."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The manuscript is easy to read in general."
            },
            "weaknesses": {
                "value": "As someone closely following the literature, I feel that a large part of this submission has already been covered by two prior work: [1, 2], and I don't really find anything particularly new in this submission. ([1] was in ICLR last year and [2] was on arXiv in June.) Moreover, the writing of this submission is quite confusing at times. Especially, the mathematical part is not very rigorous and needs a lot of improvement. Thus, I recommend rejecting this submission. However, the subject matter of this submission is definitely very interesting, and I encourage the authors to dive deeper into this field. I also recommend the authors to read [3, 4], which the authors might have overlooked. In particular, [4] comes from the same group as VICReg, and covers most results about VICReg in this submission.\n\nMy detailed comments are the following:\n### 1. On the nature of non-contrastive learning, and more generally, augmentation-based self-supervised learning\nThis submission shows that non-contrastive learning is essentially approximating the kernel of the augmentation graph (called the positive-pair kernel in [1]). This is a known result. For spectral contrastive learning, this has been shown by [1]; For more general augmentation-based self-supervised learning, this has been shown by [2]. In particular, Appendix C of [2] shows that Barlow Twins and VICReg are minimized when the representation recovers the linear span of the top-d eigenfunctions of $T_k$, the integral operator of the augmentation kernel. This result is stronger than Theorem 3.1 in this submission.\n\nMoreover, the nature of other SSL algorithms such as MAE is not an \"open problem\" (page 9), as it has already been addressed by [2].\n\n### 2. On the mathematical writing of this work\nThe writing of this work is quite confusing at times, especially in the mathematical part:\n- In Theorem 3.1, what does $V(F) \\rightarrow V(G)$ mean exactly? In functional analysis, \"a sequence of subspaces converges\" usually means that the sequence of their projection operators converge under some norm, such as the operator norm or the Hilbert-Schmidt norm. I guess the authors want to mean convergence under the HS norm.\n- The definition of $k^{DAB}$ and $T_M$ seems strange, and I guess that's why the authors cannot prove that V(F) is the linear span of the top-d eigenfunctions. The problem is that $T_Mf(x) = \\int f(x_0) p(x_0|x) dp(x_0)$, so there are two $p(x_0)$ on the numerator. A better definition could be $T_M f(x) = \\int f(x_0) dp(x_0|x)$. I suggest the authors read Section 2.2 of [2], and replace $k^ {DAF}, k^{DAB}$ with the $K_ A, K_ X$ defined there.\n- Sections 3.2 and 3.3 are not \"corollaries\". Maybe \"insight\" is a better term.\n- In Section 3.2 \"Low-dimensional projectors are sufficient\", what is the definition of \"sufficient\"? Does it mean that the ground truth target function could be reconstructed? Or does it mean that the error could be arbitrarily small? The effect of $d$, the representation dimension, has been studied in [2, 3]. Specifically, [2] showed that a larger $d$ leads to a smaller approximation error but a larger estimation error, so there is a trade-off and no $d$ is perfect or \"sufficient\".\n- In Section 3.3 \"Multiple augmentations improve optimization\", what is the definition of \"improve optimization\"? Does it mean faster convergence rate? Or does it mean more stable optimization, or perhaps lower sharpness? I don't think this section is talking about optimization at all.\n- In Section 3.3, the authors wrote \"using only two augmentations per sample yields a noisy estimate of $T_M$\", which seems to suggest that using more augmentations leads to better estimate of $T_M$. The problem is that the definition of $T_M$ depends on $M$, the augmentation. So if there are more augmentations, then $T_M$ would not be the same $T_M$. Thus, I find this statement really confusing.\n- The title of this submission is \"addressing sample inefficiency in ...\", but what is \"sample efficiency\"? Section 4.3 seems to be the only section in the main body addressing sample efficiency, and this section uses an experiment to show that if more, diverse augmentations are used, then fewer samples are required. I guess the authors are trying to express that using more diverse augmentations can lead to lower sample complexity, which is actually a known result. Specifically, [2] showed that stronger augmentations lead to lower sample complexity, and here \"stronger\" includes \"more, diverse\" augmentations.\n\nFinally, in the summary the authors position this submission as providing \"a fresh theoretical analysis\", but (a) most results are known results and thus are not really fresh, and (b) the only theorem in the submission is Theorem 3.1, and I feel that this work is more on the empirical or heuristic side. Regarding the experiments, they are kind of interesting, but I have seen similar experiments in prior work. Foundation models and representation learning are such popular topics these days, so I think the authors really ought to do a much more thorough literature review, before claiming anything to be fresh or new.\n\n\n[1] Johnson et al., Contrastive Learning Can Find an Optimal Basis for Approximately View-invariant Functions, ICLR 2023.  \n[2] Zhai et al., Understanding Augmentation-based Self-supervised Representation Learning via RKHS Approximation and Regression, arXiv:2306.00788.  \n[3] Saunshi et al., Understanding Contrastive Learning Requires Incorporating Inductive Biases, ICML 2022.  \n[4] Cabannes et al., The SSL Interplay: Augmentations, Inductive Bias, and Generalization, ICML 2023."
            },
            "questions": {
                "value": "See my detailed comments above. Overall, I suggest the authors carry out a more thorough literature review."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6318/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698260837726,
        "cdate": 1698260837726,
        "tmdate": 1699636695138,
        "mdate": 1699636695138,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dH4ArUCMKn",
        "forum": "GwBTlCIGs5",
        "replyto": "GwBTlCIGs5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6318/Reviewer_Ajht"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6318/Reviewer_Ajht"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces several enhancements to non-contrastive Self-Supervised Learning (SSL) methods, specifically BarlowTwins and VICReg. The primary claims made are that these methods do not require a high projection dimension, and the utilization of multiple augmentations can enhance performance. The authors provide empirical evidence demonstrating the effectiveness of these improvements on smaller datasets, such as CIFAR10 and STL10."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The assertion that BarlowTwins and VICReg do not necessitate large projection dimensions constitutes a significant improvement. This assertion is further supported by eigenvalue analysis conducted in the study.\n\n2. The paper conducts a valuable comparison of the utility of multiview techniques in the context of BarlowTwins and VICReg. Experiment results demonstrates the effectiveness of using a multiview approach.\n\n3. The authors offer practical recommendations on how to apply these Self-Supervised Learning (SSL) methods, which enhances the paper's utility for potential users."
            },
            "weaknesses": {
                "value": "1. The experimental results presented in the paper are less than convincing and appear to involve an unfair comparison. Figure 4, in particular, showcases curves that have not converged. A fair comparison should ensure that all models are optimized and have reached convergence.\n\n2. The utilization of multiview is not a novel concept, and it appears to be an inferior approach when compared to the multi-crop technique employed in SwAV. SwAV has thoroughly investigated this and found that using full views can be less effective due to increased memory overhead. Surprisingly, this paper does not address memory usage and effective computation, which raises questions about the validity of its claims. If each iteration consumes more computational resources and requires additional memory, it is expected to converge faster, making it a critical factor to consider.\n\n3. There seems to be a lack of a clear connection between the discussion of the graphs and the paper's main idea, which can make the paper's arguments less coherent."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6318/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698526102688,
        "cdate": 1698526102688,
        "tmdate": 1699636694985,
        "mdate": 1699636694985,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "haUiNU3KRL",
        "forum": "GwBTlCIGs5",
        "replyto": "GwBTlCIGs5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6318/Reviewer_tALx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6318/Reviewer_tALx"
        ],
        "content": {
            "summary": {
                "value": "This paper investiages non-contrastive SSL techniques like BarlowTwins and VICReg from a more foundational perspective. The main theoretical result is that the loss formulation of non-contrastive SSL techniques leads to learning the eigenfunctions of the data covariance kernel that results from the data augmentations used to train the non-contrastive SSL setup. This leads to two concrete practical takeaways: stronger orthogonality constraints allow using smaller projection heads, and using more augmentations of each sample can improve the training as the data-augmentation kernel is better approximated."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is clearly written and follows a good structure. The theory is compelling, providing a deeper understanding of something that had previously been made to work by 'engineering tricks'. That the authors are able to provide concrete improvements to two well-known non-contrastive SSL techniques with these insights further rigidifies the value of the theory. \n\nThe paper nicely introduces the important terms and relevant works & basic theoretical components, before providing the main result and the corollaries that lead to practical improvements. I only was able to check the main argumentation of the proof in the appendix, which seemed reasonable, and could not into every detail of every step. The existing experimental results seem convincing."
            },
            "weaknesses": {
                "value": "In page 7, the authors wrote that they train the setup for 2, 4, and 8 augmentations per sample, but Figure 4 only shows results for 2 and 4. \n\nTo me it seems a bit mysterious that the main argument seems to be sample-efficiency. I would expect that if training with more augmentations leads to better training, then training with more augmentations (keeping the dataset size fixed) should lead to better final downstream performance. To me it seems like an obvious set of experiments to run, so unless proven otherwise, its absence suggests that the the changes to the nc-SSL objectives do not improve final downstream performance. It would be good to understand this more."
            },
            "questions": {
                "value": "What happens if you repeat the experiments from Figure 5 with {4, 8} (and even more?) data augmentations but with 100% of the training data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "-"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6318/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6318/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6318/Reviewer_tALx"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6318/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699544582383,
        "cdate": 1699544582383,
        "tmdate": 1699636694823,
        "mdate": 1699636694823,
        "license": "CC BY 4.0",
        "version": 2
    }
]