[
    {
        "id": "i5gX6NjCpM",
        "forum": "hP4iZU8I3Y",
        "replyto": "hP4iZU8I3Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2938/Reviewer_kouL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2938/Reviewer_kouL"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the task of Logical Session Query Answering (LSQA) and presents a solution called the Logical Session Graph Transformer (LSGT) model. The objective of LSQA is to learn logical queries for observed user interaction sessions. This task could help understand the logical intention of user interactions. The LSGT model achieves this by uniformly representing sessions, items, relations, and logical operators as tokens and leveraging a transformer-based sequential model for encoding.\n\nThe paper provides a theoretical analysis that primarily focuses on demonstrating the expressiveness of the proposed LSGT model. Additionally, comprehensive experiments are conducted to validate the superiority of the proposed model compared to existing baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper proposes the task of Logical Session Query Answering (LSQA), providing an novel paradigm for enhancing applications like session-based recommendation and query recommendation by understanding the logical structures of users' latent intents.\n- The paper provides a theoretical analysis on the expressiveness of the proposed Logical Session Graph Transformer (LSGT) model.\n- The paper innovatively build a unified representation model for items, sessions and logical operators using hypergraphs and sequential models."
            },
            "weaknesses": {
                "value": "- Though the proposed task is novel, the proposed technical solution LSGT relies on existing hypergraph structures and transformer architeactures. Such designs have limited differences compared to existing sequential models and graph models. This lower the technical contribution of this paper.\n- The evaluation part could be enhanced with more diverse experiments to conduct a more comprehensive empirical study, such as ablation study, hyperparameter study, case study on the generated queries, and an investigations on the benefits of LSGT brought to downstream tasks like session-based recommendation.\n\nMinor mistake: In the summary for contributions: \"We propose to propose ...\""
            },
            "questions": {
                "value": "My concerns would be alleviated if the authors could provide further clarification on the technical novelty aspect and the comprehensiveness of the experiments. Please refer to the weaknesses part for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2938/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2938/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2938/Reviewer_kouL"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2938/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698552312464,
        "cdate": 1698552312464,
        "tmdate": 1699636237498,
        "mdate": 1699636237498,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HIVQIFtAHz",
        "forum": "hP4iZU8I3Y",
        "replyto": "hP4iZU8I3Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2938/Reviewer_Kyoc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2938/Reviewer_Kyoc"
        ],
        "content": {
            "summary": {
                "value": "The paper formulates an item recommendation task based on the previous session history as a complex logical graph query (named as Logical Session Query Answering). In such a query, items and attributes are nodes, several items can be connected in sessions (hyperedges denoting the order of obtaining the items), relations form projection operators in a query, and other logical operators (intersection, union, negation) combine nodes and projections into a single complex query. Instead of operating on the complete hypergraph of items, sessions, and attributes, the authors decide to operate on the single query level and predict answer entities directly after linearizing the query via the Logical Session Graph Transformer (essentially, a TokenGT from [1]). The authors prove that their transformer is permutation invariant (with respect to intersection and union operators), and run experiments on 3 datasets showing marginal improvements over the baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**S1.** The item recommendation task is framed as a complex logical query. While the task per se is not new (LogiRec [2] originally introduced it with projection and intersection operators), this paper extends it to unions and negations and to hypergraphs.\n\n**S2.** Evaluation includes several baselines (that show, on the other hand, that the proposed approach only marginally outperforms existing models, but more on that in W2)"
            },
            "weaknesses": {
                "value": "Starting from the claimed contributions:\n\n**W1. Task.** The formulated task of Logical Session Query Answering is essentially query answering over hypergraphs. Sessions are n-ary edges, and other relations form 2-ary edges, so the hypergraph has edges of different arity. The temporal aspect of items in session hyperedges (that items follow each other in one session) seems to be of little use as the best-performing models are not using this information anyway. I would recommend the authors to focus the contribution on extending complex query answering to hypergraphs as there is not that much work in that subfield (StarQE is for hyper-relational graphs, and NQE supports both hyper-relational and hypergraphs).\n\n**W2. Encoder + Experimental results.** The proposed logical session graph transformer (LSGT) is just one of the many query linearization strategies, eg, BiQE [3], kgTransformer [4], or SQE [5] that convert the query graph into a sequence with some positional information to be sent jointly into a Transformer. Architecture-wise, LSGT is TokenGT [1] but with a slightly different input format that sends tokens of logical operators. Experimentally, LSGT is very close to SQE [5] (the gap is often <1 MRR point) so it is hard to claim any novelty or effectiveness in this linearization strategy or in a slightly different transformer encoder. \n\n**W3. Theory.** The theoretical study in Section 4.5 is derived from TokenGT and seems to be hardly applicable to the case of logical query answering. TokenGT\u2019s theory of WL expressiveness assumes the graphs are non-relational whereas all logical query graphs studied in this work are relational, i.e., they have labeled edge types. There is a different line of work studying expressiveness of GNNs over relational graphs [6,7] and I would recommend starting from them in order to derive any expressiveness claims. Permutation invariance proofs are rather trivial because the Transformer architecture itself is permutation equivariant.\n\nOverall, I think the paper has more potential if:\n* The authors frame the task as the hypergraph query answering with the full support of first-order logical operators (intersection, union, negation) and demonstrate that several existing Transformer-based models show similar results on 3 benchmarks despite different linearization strategies; \n* Tone down the claims on the _logical session_ QA (it\u2019s a hypergraph), new graph transformer and its expressiveness (TokenGT is not new, theory for non-relational graphs does not apply to relational ones), and state-of-the-art (all Transformer-based models show a very similar performance). \n\nI understand that it would require substantial re-writing of several sections, so I am willing to increase the score if the authors decide to do it during the discussion period. \n\nMinor comments:\n* Too many sentences (especially in Section 3) start with noisy and artificial \u201chowever\u201d and \u201cmeanwhile\u201d. You don\u2019t have to contrast every sentence to each other every time.\n* $p$ and $q$ denote different things in 4.2 (item and session) and 4.3 (just two nodes) and it is confusing.   \n* 4.4 Learning LSGT -> Training LSGT\n\n**References**\n\n[1] Kim et al. Pure transformers are powerful graph learners. NeurIPS 2022.  \n[2] Tang et al. LogicRec: Recommendation with Users' Logical Requirements. SIGIR\u201923.  \n[3] Kotnis et al. Answering complex queries in knowledge graphs with bidirectional sequence encoders. AAAI 2021.  \n[4] Liu et al. Mask and reason: Pre-training knowledge graph transformers for complex logical queries. KDD\u201922.  \n[5] Bai et al. Sequential query encoding for complex query answering on knowledge graphs. TMLR 2023.  \n[6] Barcelo et al. Weisfeiler and Leman Go Relational. LOG 2022.   \n[7] Huang et al. A theory of link prediction via relational Weisfeiler-Leman. NeurIPS 2023."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2938/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698552408151,
        "cdate": 1698552408151,
        "tmdate": 1699636237425,
        "mdate": 1699636237425,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uR9MoVhzuw",
        "forum": "hP4iZU8I3Y",
        "replyto": "hP4iZU8I3Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2938/Reviewer_kt6F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2938/Reviewer_kt6F"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors focus on product and attribute recommendation by modeling complex user intention. They employ the logical session query answering (LSQA) to formulate the task. The proposed logical session graph transformer (LSGT) model runs on a hyper session graph, which uses a standard transformer structure to encode different entities. Experiments on three real-world datasets demonstrate the effectiveness of LSGT for complex session query answering."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The motivation that incorporates logical session query answering into product recommendation to model user intent is novel.\n2. The experimental results demonstrate the effectiveness of the proposed LSGT.\n3. The authors theoretically justify the expressiveness and operator-wise permutation invariance of LSGT."
            },
            "weaknesses": {
                "value": "1. There are some obvious typos. Authors should scrutinize the writing of the paper.\n(1) In the 5th line of section 4.3, the formula after \u201cThe edge feature is denoted as\u201d lacks a proper superscript.\n(2) In Table 5, the first word \u201cPredicti\u201d in explanation of query type 2p should be \u201cPredict\u201d.\n(3) In Table 5, the word \u201cprodict\u201d in explanation of query type ip should be \u201cproduct\u201d.\n(4) In the 2nd line below Figure 5, the word \u201cdescibed\u201d should be \u201cdescribed\u201d.\n2. In Figure 5, the query structure of ip is the same as up and the query structure of 2iS is the same as 2uS. It would be better to distinguish them like [1].\n3. The paper lacks detailed description for figures especially Figure 3, which is hard to understand for readers.\n4. It would be better to evaluate the model\u2019s generalization ability of unseen query structures like [1,2,3].\n\n[1] Jiaxin Bai, Zihao Wang, Hongming Zhang, and Yangqiu Song. 2022. Query2Particles: Knowledge Graph Reasoning with Particle Embeddings. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 2703\u20132714, Seattle, United States. Association for Computational Linguistics.\n[2] Chen, X., Hu, Z., & Sun, Y. (2022). Fuzzy Logic Based Logical Query Answering on Knowledge Graphs. Proceedings of the AAAI Conference on Artificial Intelligence, 36(4), 3939-3948.\n[3] Jiaxin Bai, Tianshi Zheng, and Yangqiu Song. Sequential query encoding for complex query answering on knowledge graphs. Transactions on Machine Learning Research, 2023. ISSN 2835-8856"
            },
            "questions": {
                "value": "1. Why do authors not evaluate the model\u2019s generalization ability of unseen query structures like existing works?\n2. Is there an explanation for the author's choice of 14 query structures? Can some other query structures like 2i, and pni be incorporated?\n3. Is it possible to make an ablation study for hypergraph and logical reasoning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2938/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698759049009,
        "cdate": 1698759049009,
        "tmdate": 1699636237352,
        "mdate": 1699636237352,
        "license": "CC BY 4.0",
        "version": 2
    }
]