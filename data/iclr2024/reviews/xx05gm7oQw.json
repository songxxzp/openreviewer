[
    {
        "id": "4c54n3CTvN",
        "forum": "xx05gm7oQw",
        "replyto": "xx05gm7oQw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6651/Reviewer_nQv5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6651/Reviewer_nQv5"
        ],
        "content": {
            "summary": {
                "value": "Vision-language models (VLMs) have achieved impressive performance on various tasks but have been shown to exhibit biases due to biased training data. In this study, the authors propose a simple debiasing framework, counterfactual vision-language debiasing\n(CVLD), that aims to quantify and mitigate biases in vision-language models. CVLD introduces a causal intervention module to generate counterfactual image-text pairs and use causal fairness metrics to measure the difference in model predictions between original and counterfactual distributions. The authors also propose bias-free adaptation techniques to minimize bias in pre-trained models, achieving promising results in image classification, retrieval, and captioning tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper provides a robust framework that scales to different visual-language downstream tasks like image classification, image retrieval, and image captioning tasks.\n\n2. The proficiency of CVLD is demonstrated in a set of fine-tuning experiments across different tasks using well-established fairness measures.\n\n3. The paper is well-written and details the objectives and results for each of the downstream tasks separately."
            },
            "weaknesses": {
                "value": "1. One of the primary weaknesses of the paper is its novelty in terms of the main framework. In order to infuse fairness, Agarwal et al. [1] introduced a triplet-based objective that maximizes the agreement between the original graph and its counterfactual views. Given that CVLD follows suit and incorporates a similar framework, the novelty is limited.\n\n2. In most cases, the counterfactual image seems noisy and is non-reflective of the counterfactual protected attribute (e.g., in Fig. 3, we don't observe women riding the rowboat). In such cases, are the counterfactual just some noisy version of the original image? How do we attribute the debiasing to a protected attribute if the quality of the counterfactuals is not good?\n\n3. The framework is a data-extensive approach, i.e., for debiasing, it needs a counterfactual version of each image-text pair and expensive fine-tuning of VLMs for debiasing.\n\n\n**References**\n\n1. Agarwal, C., Lakkaraju, H. and Zitnik, M. Towards a unified framework for fair and stable graph representation learning. In UAI, 2021."
            },
            "questions": {
                "value": "Please see the weaknesses for more details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6651/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698517702363,
        "cdate": 1698517702363,
        "tmdate": 1699636760389,
        "mdate": 1699636760389,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xVzteju5Ux",
        "forum": "xx05gm7oQw",
        "replyto": "xx05gm7oQw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6651/Reviewer_KRWy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6651/Reviewer_KRWy"
        ],
        "content": {
            "summary": {
                "value": "This work considers the problem of bias mitigation on vision-language models (VLMs). The authors introduced Counterfactual vision-language debiasing (CVLD), a technique that can be summarized in two main contributions: 1- a data generation pipeline based on off-the-shelf generative models to create counterfactual augmentations from real data; 2- a bias mitigation strategy based on fine-tuning a VLM using the generate counterfactuals. The authors evaluate the proposed approach empirically on 3 tasks: image classification, image-text retrieval, and image captioning. Experiments are carried out using a model from the BLIP family and considering both fairness and task performance metrics. Overall, results show that the proposed yields the best trade-off between improving fairness (i.e. mitigating biases) and attaining good performance in the task."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper tackles a critical problem and very relevant open research question: how to mitigate bias on foundation models;\n\n- The manuscript is overall well-written and most sections are easy to follow; \n\n- The counterfactual augmentation approach is grounded in formal definitions from the counterfactual fairness literature;\n\n- The experimental evaluation is extensive in the number of considered tasks."
            },
            "weaknesses": {
                "value": "- One of the central claims of the work is that the proposed approach is a unified way to mitigate bias in VLMs across multiple tasks, as per the following evidence: \n  - In the title (Debias your VLM with Counterfactuals: A **Unified** Approach, bold text by myself). \n  - Also throughout text: e.g. In Section 2 \"[...] we focus on a task-agnostic fairness framework for VLMs, unifying the study of bias across different tasks and domains.\" ). \n\n   Claiming that the proposed approach is unified and task-agnostic seemed reasonable until Section 4. However, after reading through the details of how fine-tuning with synthetic data should be carried out for the three considered tasks, it seems to me that CVLD practical instantiation takes a very different format from task to task, rendering it a specific and not-unified framework for debiasing.\n\n\n\n- One of the key parts of the introduced approach is the counterfactual data generation. However, the authors did not mention at any part of the manuscript details of the evaluation of the data generation pipeline. Moreover, it is not clear how the quality of generated counterfactuals could affect the performance of CVLD. Moreover, other fine-grained aspects such as *how much synthetic data is needed* and how the number of synthetic samples used at training time affects performance were not addressed in the manuscript, making it difficult to judge to what extent this framework would generalize to other scenarios where it might be difficult to generate high quality counterfactuals.  \n\n- Some parts of the text do not seem to reflect the actual insights that can be extracted from the results. For example, in the introduction, the authors mentioned that CVLD \"demonstrates striking effectiveness for the most studied problems in the bias literature\" while it is not clear from the results that the CVLD demonstrates **striking** effectiveness, neither there are references to support the statement that the considered problems in this work are the most studied ones in the bias literature.\n\n- The experimental results are a bit confusing and hard to parse. The employed metrics in the evaluation are not standard in the literature and it is not clear whether an improvement is observed when a metric increases or decreases. Moreover, it is not clear why some methods were grouped in different parts of the tables (e.g. in Table 1 it is not clear why both CVLDs are in different sections from BLIP-PT and the ResNet-50, aren't they directly comparable?). On a similar note, it is not clear how the different bolded numbers in the tables represent and how they should be compared against each other."
            },
            "questions": {
                "value": "- How can all the three different approaches to fine-tune VLMs with counterfactual data be seen as a unified framework? Also, how would this generalize to other tasks such as, for example, counting and object detection?\n\n- How is CVLD performance affected by counterfactual data generation quality? How did the authors assess the quality of generated data in order to know whether it was \"good enough\" to be employed for bias mitigation?\n\n- How computationally expensive is the data generation approach? How does it compare to techniques that do not rely on data generation for bias mitigation?\n\n- How is the performance of CVLD affected by the choice of the lambda hyperparameter?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6651/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6651/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6651/Reviewer_KRWy"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6651/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758223263,
        "cdate": 1698758223263,
        "tmdate": 1699636760260,
        "mdate": 1699636760260,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "r34K75rPbf",
        "forum": "xx05gm7oQw",
        "replyto": "xx05gm7oQw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6651/Reviewer_Kjn2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6651/Reviewer_Kjn2"
        ],
        "content": {
            "summary": {
                "value": "This paper attempts to unify the study of biases across vision-language problems. It attempts to create counterfactuals to swap the gender in an image-text pair using readily available tools like LLMs and image-editing methodologies. With the help of the generated counterfactuals, it is shown that the model performances improve for multiple tasks like image retrieval, image classification and image captioning."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The use of LLMs and other models like Instruct Pix2Pix is smart to generate counterfactuals.\n2. The authors adopt different ways to incorporate these counterfactuals into multiple downstream tasks as it is not always possible to alter the pretraining itself.\n2. Using existing VLMs on top of the original datasets along with the counterfactuals seem to help reducing the bias while also maintaining model performance."
            },
            "weaknesses": {
                "value": "1. Lack of novelty: The paper simply uses some state-of-the-art LLM to generate counterfactual text, and null text inversion/InstructPix2Pix to generate the counterfactual images.\n2. The paper only covers gender biases - no experiments on other biases like racial/age. Biases may exist even in non-social cases (like the water-land bias in the popular Waterbirds dataset). This has not been explored.\n3. No comparison with other debiasing VLM methods ([1], [2]).\n4. The paper advocates generating counterfactuals for bias mitigation. However, not many sample examples are shown even in the supplementary.\n5. Not all biases (for example, models are seen to learn various spurious correlations like camels can only be present in deserts, airplanes can only be in the sky, etc) are quantifiable like gender. Is generating counterfactuals the solutions for those kinds of biases too?\n\n[1] Zhu et al., Debiased Fine-Tuning for Vision-Language Models by Prompt Regularization, AAAI 2023\n[2] Chuang et al., Debiasing Vision-Language Models via Biased Prompts, arxiv 2023"
            },
            "questions": {
                "value": "1. The method of generating counterfactuals does not generate diverse images, but only modifies the existing images. Generating diverse images can help the models further. Can this be addressed?\n2. What if multiple biases are present at once? Like gender and race together. Can this method of generating counterfactuals handle such scenarios?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6651/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6651/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6651/Reviewer_Kjn2"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6651/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760426093,
        "cdate": 1698760426093,
        "tmdate": 1699636760139,
        "mdate": 1699636760139,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iaRy7VmYee",
        "forum": "xx05gm7oQw",
        "replyto": "xx05gm7oQw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6651/Reviewer_sFrN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6651/Reviewer_sFrN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a simple framework to debias vision-language models. First, one generates a text prompt for the target image that can guide the image editing procedure. Second, one generates a counterfactual image from the text prompt that has been edited by flipping the bias-related word (e.g., boy -> girl). Third, one fine-tunes the target VLM with the generated counterfactual images. The empirical results show that this method can be used to mitigate the gender bias of many VLMs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- **Soundness.** The proposed framework is very reasonably designed; it makes perfect sense that such a method will work, given access to well-performing LLMs and text-based image editors.\n\n- **Novelty.** As far as I know, the method CVLD is novel.\n\n- **Significance of the topic.** VLMs are now one of the core backbones of most machine learning applications, and thus having a safety guarantee on such foundation models is a very important yet understudied topic.\n\n- **Writing.** The paper is clearly written and easy to read, despite having many typos."
            },
            "weaknesses": {
                "value": "- **Limited Empirical Evaluation.** The proposed method has been evaluated almost exclusively on a specific type of bias---the gender bias. This is a very severe limitation for a paper which frames itself as targeting general bias in VLMs; the paper exemplifies racial bias multiple times in the text. If the authors are exclusively targeting the gender bias, a significant portion of this paper should be re-written to clarify this point.\n\n- **Relies on external models, which may be prone to other types of bias.** The debiasing procedure of this paper relies on the generative/editing capabilities of existing models (e.g., prompt-to-prompt editing). This is a vulnerability in terms of a bias, because such edited images may be prone to other types of biases that may be difficult to detect (see, e.g., Bias-to-Text by Kim et al. (2023)). I wonder if authors could demonstrate any \"robustness\" of the proposed paradigm to the potential biases hidden in the LLMs or prompts.\n\n- **(minor) Clarity.** Figure 1 is not very informative and difficult to parse what the figure is trying to say. What the sketch part is trying to say is unclear (perhaps more details in the caption will be better). Also, it took me some time to notice that \"M -> F\" means male -> female. The \"lock\" figures are somewhat difficult to tell whether they are locked or unlocked (maybe use frozen <-> fire analogy, like many other papers, or use additional color cues?)."
            },
            "questions": {
                "value": "Please see the \"weaknesses\" section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6651/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699119066005,
        "cdate": 1699119066005,
        "tmdate": 1699636760018,
        "mdate": 1699636760018,
        "license": "CC BY 4.0",
        "version": 2
    }
]