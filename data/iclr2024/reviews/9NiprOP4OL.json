[
    {
        "id": "tSZnKGUabR",
        "forum": "9NiprOP4OL",
        "replyto": "9NiprOP4OL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission592/Reviewer_VELF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission592/Reviewer_VELF"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a modification to the recent image-editing technique, Asyrp, aimed at enhancing its performance. In the original Asyrp method, intermediate features extracted by diffusion models are manipulated to produce images with specific attributes. These manipulations are guided by a small neural network that predicts the necessary shifts based on the original features. The proposed modification involves changing the input of this predictor from features to an image generated using a text prompt that outlines the desired attribute. This change allows the model to receive more comprehensive visual information about the attribute, resulting in improved editing performance. The experimental results, conducted across various datasets, validate the efficacy of this proposed approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The main idea, which is to change the input of the attribute encoder from features to an image containing the desired attribute, is simple and intuitively reasonable. It would ease the optimization of the attribute encoder for providing accurate feature shifts to generate editted images, because the input itself has visual information of the desired attribute.\n\n- In the experiments, the proposed method outperforms the original Asyrp in some cases, for example wearing glasses shown in Figure 7."
            },
            "weaknesses": {
                "value": "- The novelty of the proposed method is marginal. The major part of the proposed method seems identical to Asyrp.\n\n- The effectiveness of the proposed method is not clear in quantitative evaluations. From Table 1, 2, and 3, we cannot conclude that the proposed method achieves better performance than the other methods.\n\n- Several related studies on image editing with diffusion models are missing in reference.\n  - [R1] \"Imagic: Text-Based Real Image Editing with Diffusion Models,\" CVPR 2023.\n  - [R2] \"UniTune: Text-Driven Image Editing by Fine Tuning a Diffusion Model on a Single Image,\" SIGGRAPH 2023.\n  - [R3] \"Diffusion Visual Counterfactual Explanations,\" NeurIPS 2022.\n  - [R4] \"Zero-shot Image-to-Image Translation,\" SIGGRAPH 2023.\n\n- The name of the proposed method is confusing, because there is no inversion process in the proposed method."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission592/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission592/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission592/Reviewer_VELF"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission592/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698494701330,
        "cdate": 1698494701330,
        "tmdate": 1699635986831,
        "mdate": 1699635986831,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JuWiccoMcF",
        "forum": "9NiprOP4OL",
        "replyto": "9NiprOP4OL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission592/Reviewer_nLmB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission592/Reviewer_nLmB"
        ],
        "content": {
            "summary": {
                "value": "This work delves into the challenges and advancements in image editing using denoising diffusion models (DDMs). Existing methods either use image-guided techniques, which provide visual reference but lack control over semantic coherence, or text-guided methods, which ensure faithfulness to text guidance but may compromise visual quality. To address these issues, the authors introduce the Zero-shot Inversion Process (ZIP), a framework that combines both visual reference and text guidance. ZIP uses a small neural network to encode feature attribute to latent space and demonstrates effectiveness in both in-domain and out-of-domain attribute manipulation on real images. The paper provides experiments on several benchmark datasets, showing that ZIP produces images of equivalent quality while ensuring realistic editing effects."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- ZIP offers an approach by fusing both image-guided and text-guided methods, aiming to harness the strengths of both.\n- The framework only requires a tiny neural network, making it computationally efficient."
            },
            "weaknesses": {
                "value": "- The integration of text guidance and image guidance in a zero-shot setting might introduce complexities in real-world applications.\n- The paper only shows effectiveness on some specilized datasets, which limits the generalization ability of this method. \n- It's hard to claim \"in-domain\" and \"out-of-domain\" for those attributes, since all of these attributes are existing in the datasets, the only difference is whether having explicit labels. \n- The paper does not delve deeply into potential failures or edge cases of the ZIP framework."
            },
            "questions": {
                "value": "- The noise maps and insert/without $\\Delta h$ in Fig 4 are confusing. How do you get noise map? Does both rows have these inserting and removing $\\Delta h$ process? \n- It seems the editing is attribute-wised. Do you need to train the neural network every time when applying a new attribute or every editing?\n- Have you tested your method on models trained with larger datasets, such as Stable Diffusion? Does it still work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission592/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission592/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission592/Reviewer_nLmB"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission592/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698692750568,
        "cdate": 1698692750568,
        "tmdate": 1699635986757,
        "mdate": 1699635986757,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pht7c1zY0b",
        "forum": "9NiprOP4OL",
        "replyto": "9NiprOP4OL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission592/Reviewer_pQWL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission592/Reviewer_pQWL"
        ],
        "content": {
            "summary": {
                "value": "In the paper, the authors present a novel approach known as ZIP (Zero-shot Inversion Process) to effectively tackle the prevalent issue of semantic incoherence in prior methods. Employing a trainable model that maps attributes to vectors, which are then added to the DDPM model, they strive for improved visual quality while maintaining fidelity to the text. The conducted experiments, including comparisons with previous inversion techniques across diverse attributes like makeup editing and glasses editing, thoroughly assess the model's performance."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The presentation and clarity of the content are satisfactory. The information is effectively communicated, and there are no significant issues with how it is presented.\n\n- Figure 4 provides an interesting visualization that offers insights into the addition of the h-vector to the model."
            },
            "weaknesses": {
                "value": "- The paper lacks technical novelty as it relies on ASYRP (https://openreview.net/forum?id=pd1P2eUBVfq). This foundation alone, in my opinion, does not meet the standards for publication in ICLR. The methodology section closely mirrors that of the ASYRP paper, encompassing the training of the h-generator module and the sampling process. The asserted 'Zero Shot Inversion' process appears to be synonymous with the asymmetric reversal process outlined in the ASYRP paper, albeit with the addition of an attribute encoder.\n\n- The experimentation is confined to DDPM, with results excluding recent foundational models such as Stable Diffusion.\n\n- The number of attributes (man, old, smiling, young) utilized in the in-domain evaluation is insufficient. Additionally, only 2 domains (human faces and LSUN dataset) are used.\n\n- I failed to identify any practical advantages of this approach over previous methods concerning time, diversity, and generalizability. Given the inadequacy of the experiments, I remain unconvinced.\n\n- Table 1 illustrates no superiority of the ZIP approach over previous methods in terms of results."
            },
            "questions": {
                "value": "- I am curious if you have applied this approach to latent diffusion models on various datasets. Further elaboration on this aspect would enhance the comprehensiveness of your study."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission592/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission592/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission592/Reviewer_pQWL"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission592/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763006288,
        "cdate": 1698763006288,
        "tmdate": 1699635986686,
        "mdate": 1699635986686,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rIM5PqXD1S",
        "forum": "9NiprOP4OL",
        "replyto": "9NiprOP4OL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission592/Reviewer_L9gz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission592/Reviewer_L9gz"
        ],
        "content": {
            "summary": {
                "value": "In this paper, authors propose a zero-shot inverse process (ZIP) that can inject information from a visual reference as well as a text prompt for image editing tasks. The authors claim that their method offers more realistic and coherent image editing capabilities compared to text-to-image or image-guided methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The ZIP method shows consistent editing capabilities based on the reference image.\n2. Zero-shot method"
            },
            "weaknesses": {
                "value": "1. In general, the paper is not well written. Most of the technical ideas are borrowed from other works and do not offer any new insights into the problem. To point out few issues -\na.  \"Compared to state-of-the-art methods, ZIP produces images of equivalent quality\" Does this sentence mean their method does not offer any advantages in terms of quality?\nb.  \"making it challenging to confirm the exact appearance of the added glasses in advance\" It is hard to comprehend what this sentence is trying to convey.\nc. \"Thus, our method is zero-shot and avoids the bias of manual selection\" What is the bias in the context of image editing?\n\n2. In the text authors explain Figure 2 and mention Visual Generator. I don't see any network named as a \"visual generator\" in Figure 2."
            },
            "questions": {
                "value": "I do not have any questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission592/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission592/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission592/Reviewer_L9gz"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission592/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699272344403,
        "cdate": 1699272344403,
        "tmdate": 1699635986623,
        "mdate": 1699635986623,
        "license": "CC BY 4.0",
        "version": 2
    }
]