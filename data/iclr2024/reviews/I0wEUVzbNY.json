[
    {
        "id": "7hM1OzeAIy",
        "forum": "I0wEUVzbNY",
        "replyto": "I0wEUVzbNY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7451/Reviewer_Yafu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7451/Reviewer_Yafu"
        ],
        "content": {
            "summary": {
                "value": "The paper is a critical study of pre-trained code models, focusing on what kinds of information are or aren\u2019t encoded in the PCMs, to explain why PCMs fail to generalize beyond the datasets they are trained on. The paper creates model graphs by attention analysis and compares them with syntax graphs, dataflow graphs, and non-identifier graphs. Based on the comparison results, the paper argues that while PCMs encode syntactic and data flow relations in the attention layers, but fail to encode relations between syntactic and identifier tokens. The paper further performs a distance prediction task with DirectProbe on hidden representations and finds that hidden representations do not encode enough information to discriminate between different identifier types and syntax structures, which may lead to syntactic errors in the outputs of PCMs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. A critical study of several popular PCMs with careful analysis of their attention maps and hidden representations to reveal the limitations of those PCMs on encoding specific relations.\n2. The paper points out some wrong assumption used by prior works."
            },
            "weaknesses": {
                "value": "1. To my understanding, most of the studied models are primarily token-based models. It is somewhat unexpected by construction that the models will have accurate knowledge of AST and DFG. Also, depending on the tasks it may not matter very accurate knowledge of these code properties. For example, many previous studies have shown that for code summarization tasks, a good choice of variable names is good enough. It is not very clear from the paper what kind of properties primary token-based models are expected to learn and how they have learned it.  \n\n2. Why do authors expect the self-attention score between syntactic and identifier nodes to be high? \n\n3.  Overall, the paper's presentation is poor. It does not have a clear and logical structure, which poses difficulty in reading and understanding. The visualization is not clear. Explanations of the figures and data are not detailed enough, so it is a bit hard to relate them to the conclusions.\n\n4. While the use of the \u201cmotif structure\u201d seems integral to the construction of the code graph, it is not explicitly defined in the work; providing some background or context for this concept may be critical.\n\n5. The paper makes some unsubstantial claims. For example, in the intro, they say one group of papers says LLMs are working for code, while some other group of papers find cases where LLMs fail. The authors say these two trends are contradictory. I think this is how any field progresses ---they are not necessarily contradictory trends."
            },
            "questions": {
                "value": "1. Why do you set all the values above threshold to 1? Why not maintain an individual attention score?\n\n2. What is the intuition behind expecting a higher self-attention score between identifiers and keywords, especially for a token-cased model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7451/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698721006024,
        "cdate": 1698721006024,
        "tmdate": 1699636894892,
        "mdate": 1699636894892,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BvD5SIaaFj",
        "forum": "I0wEUVzbNY",
        "replyto": "I0wEUVzbNY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7451/Reviewer_vS1o"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7451/Reviewer_vS1o"
        ],
        "content": {
            "summary": {
                "value": "This paper presented an analysis of the self-attention mechanism and the hidden representations within PCMs (pre-trained code models). The study reveals that while PCMs do encode syntactic and data flow relations in self-attention, they only encode relations within specific subsets of input tokens and do not encode syntactic-identifier relations between code tokens. The authors believe that this limitation is what leads to syntactic errors in outputs of PCMs. The authors also observe that this problem persists across different model architectures, datasets, and pre-training objectives."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper targets an important problem.\n- Detailed analysis. It is commendable that the authors performed a visualization analysis of the hidden states of pre-trained code models."
            },
            "weaknesses": {
                "value": "This paper targets an important problem of understanding why PCMs work/don\u2019t work. The findings could provide useful insights that could help address current models\u2019 limitations and inspire further research in this area.\n\nCurrently, the analysis was performed on 5 PCMs that were proposed in 2020-2022 and are relatively small. No investigation of more recent models, especially LLMs (such as CodeLLaMa and Starcoder), is performed. It is not clear if the obtained findings are applicable to more recent, large models.\n\nIt seems that this work closely aligns with the research conducted by Wan et al. (2022) and places a greater emphasis on the fine-grained details of code tokens, achieved by categorizing input tokens into syntactic tokens and identifiers. From this standpoint, this work is considered a bit incremental.\n\nThe authors claimed that their findings shed light on why PCMs fail to generalize beyond dataset they are trained on and in real world applications. I do not think this statement can be well supported by the findings shown in this paper. The authors investigate the inability of pre-trained code models to comprehending relations across the syntactical keywords and identifier tokens, however, the connections between the comprehending ability and the generalization ability are non-trivial and need to be explained explicitly.\n\nSeveral statements in this paper are not consistent. For example, the research objectives are inconsistent, the study attempts to explore the extrapolation ability of a model in the introduction, however, the methods mainly focus on evaluating the ability of a model distinguishing syntactical tokens and identifiers.\n\nIn the Introduction, the authors point out that certain prior works often make incorrect assumptions in their experimental settings. Unfortunately, the authors do not specify what kinds of incorrect assumptions these works often make."
            },
            "questions": {
                "value": "- Are the obtained findings applicable to more recent, large models?\n\n- What are the connections between the comprehending ability and the generalization ability?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7451/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698752179346,
        "cdate": 1698752179346,
        "tmdate": 1699636894757,
        "mdate": 1699636894757,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cpCXpe1GkF",
        "forum": "I0wEUVzbNY",
        "replyto": "I0wEUVzbNY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7451/Reviewer_GkFF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7451/Reviewer_GkFF"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a critical study of pre-trained code models (PCMs) to understand what they do and do not learn regarding code relations. By analyzing the self-attention mechanism and hidden representations, the authors reveal that while PCMs encode syntactic and data flow relations among input tokens, they fail to encode relations between syntactic tokens and identifiers. This limitation results in hidden representations not being able to discriminate between different identifier types and syntax structures. The authors show that these learning gaps persist across different model architectures, datasets, and pre-training objectives, providing insights into why PCMs fail to generalize beyond the dataset they are trained on and in real-world applications. The findings encourage further research to address the limitations of current PCMs and develop more robust experimental designs for model interpretability and improvement in training methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper exhibits originality by providing a fine-grained analysis of pre-trained code models, uncovering previously unaddressed limitations. The quality is high due to its critical examination of assumptions in prior work, leading to more accurate conclusions. The paper's clarity is evident in its well-written presentation, making the experiments and findings easily understandable. Its significance lies in inspiring further research to address the identified limitations and develop more robust experimental designs and training methods for PCMs."
            },
            "weaknesses": {
                "value": "1. The scope of the models and datasets analyzed is limited, and the paper could be strengthened by considering a wider range of model architectures, sizes, and training data sources to provide a more comprehensive understanding of the limitations across various PCMs.\n2. The paper focuses primarily on syntax and data flow relations but could extend its analysis to other aspects, such as the influence of natural language on code understanding and the alignment between natural language and programming languages.\n3. The experiments conducted in the paper focus on Python code. Including other programming languages in the analysis would provide insights into whether the observed limitations are language-specific or general across different programming languages."
            },
            "questions": {
                "value": "1. How do the observed limitations in PCMs affect their performance on real-world tasks? It would be helpful to provide examples or case studies to illustrate the practical implications of these limitations.\n2. Are the identified limitations specific to the Transformer-based models studied in the paper, or do they extend to other types of models, such as RNNs or LSTMs, in the context of code understanding?\n3. Have the authors considered exploring the impact of different pre-training objectives or methods that could potentially address the limitations identified in the paper? Some suggestions or proposals to improve the training process would be valuable.\n4. How do the limitations in the PCMs affect their ability to learn from more diverse training data, such as codes from different domains or programming languages? Would incorporating such variety during the training phase help in mitigating the observed limitations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7451/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820762159,
        "cdate": 1698820762159,
        "tmdate": 1699636894643,
        "mdate": 1699636894643,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZRTbfRhWPw",
        "forum": "I0wEUVzbNY",
        "replyto": "I0wEUVzbNY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7451/Reviewer_gkoc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7451/Reviewer_gkoc"
        ],
        "content": {
            "summary": {
                "value": "The paper analyzes the limitations of pre-trained code models (PCMs) by studying what relations they do and do not encode in their internal representations. It focuses on syntactic, data flow, and semantic relations between code tokens.\n\nThe main finding is that while PCMs encode some syntactic and data flow relations, they fail to encode relations between syntactic tokens like keywords and identifiers like variable names.\n\nThe authors perform comprehensive attention analysis and probing of hidden representations across multiple models like CodeBERT, GraphCodeBERT, CodeT5, etc. The limitations are found to persist across models, architectures, objectives and datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. **Comprehensive analysis of multiple relation types.** A key contribution is analyzing syntactic-syntactic, identifier-identifier, and syntactic-identifier relations separately. This allows the authors to uncover that while models encode some relations well, they fail to encode syntactic-identifier relations. The fine-grained categorization and analysis provides valuable insights into exactly where models fall short.\n\n2. **Rigorous experimental methodology.** The paper investigates limitations rigorously through both attention analysis and probing of hidden representations. The authors also avoid common pitfalls by using suitable thresholds, metrics, and probing techniques. The robust experimental design lends credibility to the conclusions drawn.\n\n3. **Analysis across multiple models and settings.** Rather than evaluating a single model, the study analyzes limitations across transformer architectures, training objectives, and datasets. The consistency of observations across these varying settings strongly indicates fundamental, widespread limitations in encoding certain code relations. The cross-model analysis strengthens the paper's central claim regarding limitations of pre-trained code models."
            },
            "weaknesses": {
                "value": "1. The selection of models analyzed is limited. While the paper has covered 5 models that are either encoder-only or encoder-decoder, it has not covered decoder-only PCMs, which seem to be the major choice of pre-trained language models currently.\n2. Lack of real-world evaluation. The paper focuses on analyzing model internals. It does not evaluate how the limitations discovered actually affect model performance on downstream tasks. Testing on real-world code generation and code search benchmarks could better highlight the practical implications.\n3. The paper does not provide architecture-related or data-related explanation for the limitations of current PCMs. Also, it does not propose methods to address the limitations identified. Providing ideas to improve relation encoding could make the work more constructive."
            },
            "questions": {
                "value": "Please see the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7451/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7451/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7451/Reviewer_gkoc"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7451/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699207747714,
        "cdate": 1699207747714,
        "tmdate": 1699636894540,
        "mdate": 1699636894540,
        "license": "CC BY 4.0",
        "version": 2
    }
]