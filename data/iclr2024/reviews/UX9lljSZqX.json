[
    {
        "id": "1L3ZND615P",
        "forum": "UX9lljSZqX",
        "replyto": "UX9lljSZqX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1579/Reviewer_GE2s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1579/Reviewer_GE2s"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles two video grounding problems based on natural and spoken language queries. Inspired by the human biology, the authors propose a novel framework, called UniSDNet, that enables both static and dynamic interactions to facilitate the learning of video grounding. The static interaction is implanted by a series of ResMLP layers, while the dynamic interaction is conducted by graph convolution with Gaussian Radial filters. Experimental results on three benchmarks for each task confirm the effectiveness of the proposed framework."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The manuscript is overall well-organized and easy to follow.\n+ The motivation behind the static and dynamic interactions based on the brain activity is clear and compelling.\n+ The two-stage information aggregation methods are shown to be effective, where each component is appropriately designed.\n+ The experimental results are very strong, clearly outperforming the existing approaches for both grounding tasks.\n+ The collected spoken language grounding datasets will significantly benefit the research community. The authors are encouraged to publish the code and data after the review process."
            },
            "weaknesses": {
                "value": "I did not find major weaknesses in this paper, yet summarize some questions about the method below.\n\n- What is the motivation behind the implementation of Static Semantic Supplement Network? I am wondering how the cross-modal interaction is performed through the MLP layers. To my understanding, the shared weights across different modalities would extract some common features spanning different modalities. Some analytical experiments on this would be beneficial. Also, the architecture design seems similar to that of Transformer blocks except for the self-attention. What happens if we use the conventional Transformer layers?\n- The proposed architecture exploits multiple queries at once, to facilitate the model learning. However, how the number of queries affects the performance is not diagnosed. An ablative study on the number of queries regarding performance and cost would be helpful.\n- In Figure 5, the effectiveness of the proposed filtering GCN is clearly verified. On the other hand, there are some interesting tendency differences between NLVG and SLVG. That is, the graph convolution layer itself is important, yet different layer modeling brings insignificant performance gaps on NLVG. In contrast, on SLVG, the graph modeling brings negligible gains alone, but the proposed filtering mechanism shows substantial improvements. How can one interpret this phenomenon? If you have, please share some insights.\n- The proposed method is well validated in the datasets with one-to-one matching between queries and moments. How would it perform for one-to-many matching datasets, such as QVHighlights [1]?\n\n[1] Lei et al. \"QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries\", Neurips, 2021.\n\n(Minor)\n\nThe manuscript contains some formatting errors due to the excessively small margins between captions and the main text. They should be handled appropriately to raise the quality of the paper."
            },
            "questions": {
                "value": "Please refer to the Weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1579/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1579/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1579/Reviewer_GE2s"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1579/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831720028,
        "cdate": 1698831720028,
        "tmdate": 1699636086630,
        "mdate": 1699636086630,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pi86s7onrw",
        "forum": "UX9lljSZqX",
        "replyto": "UX9lljSZqX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1579/Reviewer_Z5of"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1579/Reviewer_Z5of"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new approach to natural language-based (NLVG) and spoken language-based video grounding (SLVG). It first uses a MLP with residual connection to model the interaction between video feature and queries. Next, it proposes a graph network to model the short-term dynamics. The proposed model achieves great improvements on both NLVG and SLVG benchmarks and runs faster than the multi-query benchmark."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Good performance on both NLVG and SLVG benchmarks.\n\n2. It is nice to see an extension from NLVG to SLVG with a newly proposed benchmark. The proposed method proves effective on both tasks.\n\n3. Detailed implementation details and prediction analysis in the appendix."
            },
            "weaknesses": {
                "value": "1. The inspiration from human visual perception biology is not very motivating. Specifically, it is hard to see why a MLP with residual connection is the way to achieve the \u201cglobal broadcast communication\u201d of the brain. Either bridge the gap or Simply drop the bio-inspiration and go straight into the technical method.\n\n2. When expanding a single gaussian kernel to multi-kernel Gaussian, it seems that only the bias $z_i$ is sweeping? Have you tried different $\\gamma$?\n\n3. Ablation in Fig 5 shows mostly similar results especially on NLVG, indicating that the designs in Dynamic Filter Graph actually do not quite matter."
            },
            "questions": {
                "value": "1. Template\n\n(1) The first page is missing a header.\n\n(2) Please change `\\cite{..}` to `\\citep{..}` for clarity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1579/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699075092855,
        "cdate": 1699075092855,
        "tmdate": 1699636086569,
        "mdate": 1699636086569,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OFRQYAs5FY",
        "forum": "UX9lljSZqX",
        "replyto": "UX9lljSZqX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1579/Reviewer_TT77"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1579/Reviewer_TT77"
        ],
        "content": {
            "summary": {
                "value": "To address the Natural Language Video Grounding (NLVG) and Spoken Language Video Grounding (SLVG) problems, this paper introduces a Unified Static and Dynamic Network (UniSDNet). In which, the Static Network utilizes ResMLP layer to model global context while the Dynamic Network leverages the multi-kernel Temporal Gaussian Filter to build graph, where the gaussian filter leverages the temporal distance and semantic similarity. Extensive experiments demonstrate promising results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Most parts of the paper is well-written, clearly demonstrating the motivation,  methodology and experiments. The methodology part is kind of easy to follow. \n2. The idea is motivated from the human visual perception biology, which formulates an interesting story for this paper.\n3. Extensive experiments successfully demonstrate the effectiveness of each proposed component of this work, which is good.\n4. The visualization and figures are plus to show more intuitions.\n5.  The final results of this paper achieves the state-of-the-art from both efficiency and effectiveness perspectives."
            },
            "weaknesses": {
                "value": "1. The introduction reads like a related work. It will be great to make more comparison between this work and previous work. Answering what is wrong with previous works? and where the efficiency and performance gain come from in this paper?\n2. This paper introduces some new/confusing terminologies with their own definition, which hurts the reading experience. For example, 'static semantic supplement network' and 'activity-silent mechanism' are actually the global context interaction.  \n3. Although the motivation of static and dynamic network is demonstrated, the justification of specific design is not enough. For example, in the static network, transformer architecture or the recent S4[1] architecture can also be used as long-range filter. Some ablation studies regarding either the performance or efficiency would be great to include.\n4. In the dynamic network, not sure why use Gaussian filter on the distance (d_{ij}). Can you provide more insights? why not directly use the distance.\n5. No notation for the 'FNN'. Is this the feedforward network?\n6. In the Figure 5, no notation/description for 'D'.\n\n[1]  Efficiently modeling long sequences with structured state spaces. ICLR 2021"
            },
            "questions": {
                "value": "Is there any chance also leverage the audio signal into this work, formulating a multi-model graph?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1579/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699122467606,
        "cdate": 1699122467606,
        "tmdate": 1699636086496,
        "mdate": 1699636086496,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "q6tRFCz9qI",
        "forum": "UX9lljSZqX",
        "replyto": "UX9lljSZqX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1579/Reviewer_1gED"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1579/Reviewer_1gED"
        ],
        "content": {
            "summary": {
                "value": "This work proposes the Unified Static and Dynamic Network (UniSDNet) for video grounding, which establishes semantic associations between multiple text/audio queries and video content in a cross-modal environment. UniSDNet combines static and dynamic modeling techniques. For static modeling, it employs an MLP within a residual structure (ResMLP) to facilitate comprehensive interactions between video content and multiple queries, enhancing mutual semantic understanding. For dynamic modeling, the paper draws inspiration from human visual perception mechanisms and constructs a diffusive connected video clip graph to represent short-term relationships and employs a multi-kernel Temporal Gaussian Filter for complex visual perception simulation. UniSDNet achieves state-of-the-art performance on various NLVG and SLVG datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed Dynamic Temporal Filter Network captures more fine-grained context correlations between video clips based on a well-desgined graph network.\n2. The proposed method achieves state-of-the-art performance on NLVG and SLVG tasks.\n3. In this work, two new SLVG datasets are collected based on existing NLVG datasets.\n4. Compared with previous multi-queried methods, the proposed UniSDNet has less model parameters and is more efficient according to the average inference time per query."
            },
            "weaknesses": {
                "value": "1. In ResMLP, visual features and multiple query features are concatenated and fed into the network, largely leveraging the information leakage between different queries (because the features incorporate more accurate textual information that describes the video content). If each query is individually input into the network, would this method exhibit a significant performance degradation?\n2. In the ablation study, individually employing the static network and DTFNet yields significant improvements compared to the baseline. However, the combination of both modules does not exhibit a notably large improvement compared to using either single module. Is there a specific explanation for this phenomenon? The authors should provide more details about the baseline models."
            },
            "questions": {
                "value": "I have listed my major concerns and questions in the weaknesses. I hope the authors can provide more details of baseline models in the ablation study and some experimental results about comparing multi-query input and single-query input."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1579/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1579/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1579/Reviewer_1gED"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1579/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699311902072,
        "cdate": 1699311902072,
        "tmdate": 1699636086395,
        "mdate": 1699636086395,
        "license": "CC BY 4.0",
        "version": 2
    }
]