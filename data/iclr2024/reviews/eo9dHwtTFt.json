[
    {
        "id": "NM0XDYPcPu",
        "forum": "eo9dHwtTFt",
        "replyto": "eo9dHwtTFt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1388/Reviewer_qHoq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1388/Reviewer_qHoq"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a planning agent called Skipper that overcomes the limitations of existing reinforcement learning (RL) agents. \n\nRL agents can either rely on intuition or only plan for short-term tasks. Skipper, on the other hand, uses human-like thinking to break down complex tasks into smaller parts called \"proxy\" problems. \n\nThese problems are represented as graphs, with decision points as dots and connections between them as lines. Skipper solves each problem and then focuses on the connections to solve the next part. \n\nThis approach helps Skipper handle new situations better and create useful shortcuts. The paper also shows that Skipper performs better than other methods in terms of adapting to new situations, even with limited training."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Skipper employs both temporal and spatial abstraction. Temporal abstraction breaks down complex tasks into smaller sub-tasks, while spatial abstraction allows the agent to focus on relevant aspects of the environment, improving learning efficiency and generalization.\n\nDecision-time planning enhances the agent's ability to handle novel situations. By formulating a simplified proxy problem and planning within it, the agent can generate plans in real time, adapting to changing environments or goals.\n\nSkipper uses proxy problems, represented as finite graphs, to create a simplified version of the task. The use of checkpoints as vertices in the proxy problems allows for a sparse representation, reducing computational complexity. \n\nThe experiments are conducted in gridworld environments with varying levels of difficulty, providing a realistic and challenging testbed for the agents. The use of MiniGrid-BabyAI framework ensures a well-defined and computationally feasible evaluation environment."
            },
            "weaknesses": {
                "value": "The experiments are conducted in simulated grid world environments, which might not fully represent the complexities of real-world scenarios. The findings might not directly translate to applications in more intricate, dynamic, and unstructured environments.\n\nIn Section 4: the paper assumes fully observable environments, which might not hold in many real-world applications where agents have limited or partial observability. The performance of Skipper in partially observable environments is not explored, limiting its practical applicability.\n\nThe paper introduces different variants of Skipper. While this provides a comprehensive analysis, it also complicates the understanding of the core method. A clearer presentation of the differences and justifications for each variant would enhance the paper's clarity."
            },
            "questions": {
                "value": "What are the potential future directions for research and development in the field of planning agents like Skipper? Are there any specific improvements or enhancements that you plan to explore in your future work? For example, how does this method influence the research of more complex environments?\n\nCan you provide more information about the theoretical guarantees of the quality of the solution to the overall problem? What are the key insights or findings from the theoretical analysis? For example, what are  the limitations when applying Skipper in the continuous control environments.\n\nThe authors mentioned that non-uniform starting state distributions introduce additional difficulties in terms of exploration and therefore globally slow down the learning process. Is this caused by the intrinsic limitations of the Skipper method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns since it focuses on Grid-world studies and RL algorithms only."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1388/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698623348199,
        "cdate": 1698623348199,
        "tmdate": 1699636066572,
        "mdate": 1699636066572,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OyQe1nJiRd",
        "forum": "eo9dHwtTFt",
        "replyto": "eo9dHwtTFt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1388/Reviewer_3j7M"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1388/Reviewer_3j7M"
        ],
        "content": {
            "summary": {
                "value": "This paper presents Skipper, a model-based reinforcement learning approach that leverages both spatial and temporal abstraction to perform well on problems with sparse rewards. The abstraction takes the form of what the authors refer to as a \"proxy problem\": a directed graph in which vertices and edges correspond to states and the ability to reliability traverse between them (respectively). These proxy problems are estimated at decision time from learned models, trained end-to-end via hindsight. The authors show the ability of Skipper to perform well on a navigation-oriented gridworld problem [via the MiniGrid-BabyAI framework] and in particular show how Skipper generalizes quickly given only a handful of training tasks, outperforming both model-based and model-free baselines in its ability to generalize."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This is a well written paper. It is easy to follow despite the challenging subject material that presents a well-scoped approach to tackling model-based RL for gridworld-like problems. The paper is generally good and, between the main body of the paper and the expansive additional experiments and details in the Appendix, the work shows the effectiveness of Skipper. Using hindsight to train Skipper is well-motivated. The baseline algorithms against which Skipper is compared are appropriate, and it is clear that Skipper outperforms them under multiple different circumstances. In particular, the generalization-focused experiments are interesting and convincing, and show the benefits of the decision-time planning approach that Skipper takes to planning."
            },
            "weaknesses": {
                "value": "There are two (related) core weakness of the paper: (1) it seems that Skipper is designed primarily with gridworld environments in mind and seems to be relatively limited in what problems it is suited to solve and (2) the paper is missing a clear \"Weaknesses\" or \"Limitations\" section, which would help the reader understand when Skipper will be appropriate.\n\nThe experiments in the paper focus somewhat narrowly on a single family of gridworld environments. While this is not on its own a problem, there are questions regarding how applicable Skipper is to tasks beyond those shown in the paper. Specifically, Skipper's vertex generation module consists of an attention mechanism over the observation. For the experiments presented in the manuscript, the observations all take the form of overhead views of a map of the environment and so directly correspond to the state space. This means that Skipper is particularly well-suited to work on environments such as these, since vertex proposal works directly on the observation, and (unless I am mistaken in my understanding) arguably Skipper is over-engineered to work on environments such as these.\n\nThe appeal of the other model-based reinforcement learning approaches mentioned in the paper, including MuZero and Dreamer (against which Skipper is compared), is that they work in fairly general observation spaces, both capable of taking as input RGB images and learning a model from those. While the paper on the whole generally does an effective job of communicating what assumptions the system makes and how it works, the construction of Skipper seems to implicitly assume a certain type of state or observation representation that limits the applicability of the work in general. If the authors could clarify this point and make explicit these so-far implicit conditions, it would greatly strengthen the paper.\n\nRelatedly, while the paper spends considerable time discussing the ways in which the proposed Skipper approach is effective, it lacks a clear Weaknesses or Limitations section. Such a section, and comments regarding which problems or domains Skipper is best suited, would greatly aid in understanding when Skipper is appropriate. If possible, I would appreciate hearing the authors' thoughts on the size of map or problem that Skipper would be expected to work; I do not think additional experiments are necessary or appropriate, though I am interested to understand if Skipper would succeed on larger occupancy grids and navigation tasks."
            },
            "questions": {
                "value": "I would like the authors to address what I identify as the two core weaknesses in my comments above. In summary:\n\n1. Could the authors comment on the (implicit) assumptions Skipper makes about the task/environment? What decisions about Skipper's design (e.g., using an attention mechanism over the observations to propose vertices for proxy problem estimation) are core to Skipper's functionality or can be changed without changing the overall approach? On what types of problems is Skipper useful (or not)?\n2. Relatedly, a small summary of Limitations or Weaknesses of the proposed approach would help clarify when Skipper should be used, and thus strengthen the paper. If the answers to my question 1 were in the form of such a section, I think it could be effective and helpful for improving clarity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1388/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765753203,
        "cdate": 1698765753203,
        "tmdate": 1699636066482,
        "mdate": 1699636066482,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sTiATrkaOG",
        "forum": "eo9dHwtTFt",
        "replyto": "eo9dHwtTFt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1388/Reviewer_iydj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1388/Reviewer_iydj"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method for hierarchical planning by learning spatio-temporal abstraction. The approach is tested for zero-shot generalization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper supports the importance of spatial and temporal abstractions together.\n\n- The approach to minimize plans with non-existent checkpoints is interesting."
            },
            "weaknesses": {
                "value": "- In my opinion, the paper lacks an overall description of the algorithm which largely limits the understanding of the paper. \n\n- A proxy problem is estimated using the models by a conditional generative model that randomly samples checkpoints. These checkpoints are only pruned, not improved and thus the approach seems to be heavily dependent on the plan found earlier even if it can not be achieved at the low level. The delusion suppression approach only minimizes the non-existent checkpoints. Are there any guarantees on whether a checkpoint is achievable? \n\n- There is much research done on learning temporal and/or state abstractions automatically for planning and learning [1,2,3,4]. It is not clear how the proposed work is different or novel compared to these different lines of research. The related work needs a thorough justification of differences with different categories of work.\n\n- The proposed approach seems applicable to grid-like environments only as also included in the experiments.\n\nReferences:\n\n[1] Hogg, C., Kuter, U. and Munoz-Avila, H., 2009, July. Learning Hierarchical Task Networks for Nondeterministic Planning Domains. In IJCAI (pp. 1708-1714).\n\n[2] Dadvar, M., Nayyar, R.K. and Srivastava, S., 2023, July. Conditional abstraction trees for sample-efficient reinforcement learning. In Uncertainty in Artificial Intelligence (pp. 485-495). PMLR.\n\n[3] Ravindran, B., AC, I. and IIL, R., Hierarchical Reinforcement Learning using Spatio-Temporal Abstractions and Deep Neural Networks.\n\n[4] McGovern, A. and Barto, A.G., 2001. Automatic discovery of subgoals in reinforcement learning using diverse density."
            },
            "questions": {
                "value": "Included with the limitations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1388/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1388/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1388/Reviewer_iydj"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1388/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813271878,
        "cdate": 1698813271878,
        "tmdate": 1699636066401,
        "mdate": 1699636066401,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GJN5eHsqGe",
        "forum": "eo9dHwtTFt",
        "replyto": "eo9dHwtTFt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1388/Reviewer_muKb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1388/Reviewer_muKb"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new model-based reinforcement learning agent, Skipper, with the explicit aim to generalize its learned \"skills\" to novel situations. Skipper creates a \"proxy problem\" for a given task. Proxy problems are finite graphs created at decision time, whose vertices (\"checkpoints\") are states and whose directed edges are temporally abstract transitions between these states. The vertices are proposed by a generative model (which splits the state into two parts: a context (which is a general description of the environment), and partial description (which is the agent's current state) , while the estimates of the edges (holding time and expected reward) are learned while interacting with the environment. The training is the sum of the losses of the vertex generation and edge estimation.\n\nThe algorithm is evaluated on the MiniGrid-BabyAI framework against the modelfree RL agent that is the basis of the architecture and two relevant Hierarchical Planning methods (LEAP and Director). The evaluation metric is generalization across different instantiations of the environment: subsequent difficulties mean more lava states in the grid. It is demonstrated that Skipper makes use of the context latent representation, as well as the estimated edges to transfer learned policies faster than the baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality: splitting the state to context and partial information the way it is proposed and using it for transfer across slightly altered tasks seems intuitive yet to the best of my knowledge original.\n\nQuality: experiments ran over 20 seeds, which is substantially more than the usual 5 and is generally recommended the absolute minimum for a pilot study. The plots are well annotated, captioned with error bars included. \n\nClarity: the paper is well-written and easily understood in most parts.\n\nSignificance: the results as presented appear to be significantly better than the baselines. The proposed method coupling state (or \"spatial\") and temporal abstraction and using it for transfer across tasks is an interesting avenue for further research."
            },
            "weaknesses": {
                "value": "As mentioned above, the paper has many strengths, however, it is unfortunately not devoid of weaknesses. There are certain parts of the paper that I did not understand, hence I asked several questions in the next section. Furthermore, there is very little information about the way experiments were carried out (with regards to hyperparameter tuning for the proposed algorithm and the baselines on the various environments). These details are required to assess the validity of the claims and to assure reproducibility.\n\nOnce my doubts are cleared, I will consider raising my score accordingly.\n\nI listed the experiments ran with 20 seeds as strength and it is. However, in order to claim significance, it is suggested that after the pilot study with 20 seeds, statistical power analysis should be performed to determine the necessary sample size [1][2]. \n\n[1] How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement Learning Experiments, Cedric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer, 2018\n[2] Empirical Design in Reinforcement Learning, Andrew Pattersion, Samuel Neumann, Martha White, Adam White, 2023\n\nI recommend either adding a statistical power analysis to the paper or softening the claims a little bit.\n\nThe following typos/style issues did not affect the score:\n\nThe abbreviation RL should be defined as reinforcement learning (RL) the first time it is used., even though it is clear for the audience what it stands for, it is good practice.\n\ns\u2299  is defined much later than when first introduced (in Theorem 1) which is confusing.\n\nSection 2, 2nd paragraph, also Section 4 1s paragraph: no need to capitalize the word following a semicolon (;) or end the previous subsentence with a full stop and the capitalize the start of the sentence.\n\nSection 2\n\n1st paragraph: Q: |\\mathcal{S}| \\times |\\mathcal{A}|  (| | missing for S)\n\n2nd paragraph: \"evaluated _on_ unseen tasks, _in which_ there are environment variations, but the core strategies need to finish the task remain [...]\" (not remains)\n\n3rd paragraph: \"Recent work suggests that this approach\" -> the latter approach\n\nFigure 2 should be moved at least one page down, there are concepts in it that are explained much later.\n\n\"The following estimates are learned with using distributional RL\" - with or using\n\n3.3\n\nthe expression \"on top of\" occurs twice and I find it rather ambiguous. Could you use some other expression, e.g. with or after, or anything else that conveys what you are trying to say.\n\n3.3.2\n\n3rd paragraph: \"Similarly to $V_{\\pi}$, we would want to know the cumulative discount leading to the target s\u2299 under $\\pi$.\" - There is no need for this sentence. Just start the next sentence as \"The cumulative discount leading to the target s\u2299 under $\\pi$ is difficult to learn...\"\n\n3.4\n\nsecond paragraph: \"tall order\" - this is slang, consider changing it to something like \"difficult task\"\n\n5\n\n\"Prior to LEAP (Nasiriany et al., 2019), path planning with evolutionary algorithms was investigated by Nair & Finn (2019); Hafner et al. (2022); Mendonca et al. (2021) propose world models to assist temporally abstract background planning.\"\n\nNot clear where the first sentence ends and the second begins. Use a full stop and an 'and' where applicable."
            },
            "questions": {
                "value": "Could you please define spatial abstraction and highlight how it is different from state abstraction? (ideally with a citation) I had a look at all cited papers in the relevant subsection in Section 5, but non of them contained the term \"spatial abstraction\". Machin et al. 2019 used the term \"spatial (and temporal) information\", but it was not defined.\n\n\u201cExisting RL agents either operate solely based on intuition (model-free methods) or are limited to reasoning over mostly shortsighted plans (model-based methods)\u201d could you please back this rather strong statement with a citation (especially the first part).\n\n\"Model-based HRL agents can be prone to blindly optimizing for objectives without understanding the consequences.\" - again, at least a citation would be in order for such a claim.\n\n\u201cFor any set of options defined on an MDP, the decision process that selects only among those options, executing each to termination, is a Semi-MDP (SMDP, Puterman 2014)\u201d Could you please point to the part in the book that defines SMDPs as such? \n\n\u201cTaking inspiration from conscious information processing in brains [...]\u201d - could you please cite the works whose findings inspired the introduced local perceptive field selector?\n\n3.3.2, the update rule for the Cumulative Reward: could you please explains where is the KL-divergence in that update rule? To me it just looks like each $v(s,a)$ is updated with the TD-target $(r(s,a,s') + \\gamma v(s,a))$?\n\n\"The following estimates are learned with using distributional RL, where the output of each estimator takes the form of a histogram over scalar support (Dabney et al., 2018)\"  - I don't understand what this sentence is trying to say, and the linked paper did not help. Could you please elaborate?\n\nHow were hyperparameters tuned for the algorithm (the algorithmic specific ones, such as timeout, pruning threshold, k for k-medoid, etc. as well as the ones basis distributed DDQN) and the various baselines for this task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1388/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830509436,
        "cdate": 1698830509436,
        "tmdate": 1699636066311,
        "mdate": 1699636066311,
        "license": "CC BY 4.0",
        "version": 2
    }
]