[
    {
        "id": "eu8H9nX4ji",
        "forum": "wkbeqr5XhC",
        "replyto": "wkbeqr5XhC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9181/Reviewer_wWh6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9181/Reviewer_wWh6"
        ],
        "content": {
            "summary": {
                "value": "The proposed method of this paper is LUM-ViT, a learnable under-sampling mask vision transformer for bandwidth-limited optical signal acquisition. It is a novel approach that utilizes deep learning and prior information to reduce acquisition volume and optimize for optical calculations. The methodology unfolds in two primary stages: training from pre-trained models in a solely electronic domain using existing datasets, followed by inference to evaluate model performance, and assessing the real-world performance of LUM-ViT with a DMD signal acquisition system. During acquisition, target information undergoes a single instance of DMD optical modulation before capture, and then is funneled into the electronic system for further processing."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(i) The studied problem about under-sampling hyperspectral data acquisition, achieving data reduction from signal collection while preserving model performance. This accelerates the HSI processing in real applications such as remote sensing, object tracking, medical imaging, etc.\n\n(ii) The idea of using a learnable mask refined during training to selectively retain essential points for downstream tasks from the patch embedding outputs, and thereby achieving under-sampling (reducing the required sampling instances) is interesting.\n\n(iii) The performance is good. On the ImageNet-1k classification task, the proposed LUM-ViT maintains accuracy loss within 1.8% at 10% under-sampling and within 5.5% at an extreme 2% under-sampling.\n\n(iv) This work not only conducts experiments on the synthetic data but also sets up hardware (as shown in Figure 6) to evaluate the effectiveness of the proposed method. It is a good and non-trivial exploration. The accuracy loss of LUM-ViT does not exceed 4% compared to the software environment, demonstrating its practical feasibility."
            },
            "weaknesses": {
                "value": "(i) The detailed formulations for DMD are missing, which is confusing. More explanations are required.\n\n(ii) Code and pre-trained weights are not submitted. The reproducibility of this work cannot be checked. \n\n(iii) The multi-stage training pipeline is tedious, which makes the whole technical route unreliable. What's worse, the finetuning details about stage 3 are missing.  Other researchers cannot re-implement this complex approach. \n\n(iv) The writing should be further improved, especially the mathematical notations in Section 3.3. The formula for binary compression is not formal.\n\n(v) The experiments are not sufficient and many critical comparisons are missing. For example, the backbone is ViT variants. However, the ViT is very computationally expensive because its computational complexity is quadratic to the input spatial size. This also embeds the real-time applications of HSI processing. In contrast, MST [1] or MST++ [2] are specially designed for HSI processing. They treat spectral feature maps as a token to capture the interdependencies between spectra with different wavelengths. Most importantly, they are very efficient with linear computational complexity regarding spatial resolution. Thus, it is better to add a comparison with the spectral Transformer in Figure 5.\n\n(vi) The binarization mechanism is out of fashion. BiSCI [3] provides a specially designed binarized convolution unit BiSR-conv block to process HSI data cubes. It is also better to add a comparison with this new technique.\n\n[1] Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction. In CVPR 2022.\n\n[2] MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction. In CVPRW 2022, NTIRE 2022 Winner in Spectral Recovery.\n\n[3] Binarized Spectral Compressive Imaging. In NeurIPS 2023."
            },
            "questions": {
                "value": "The technical route and core idea in this paper is to accelerate the HSI processing, which is similar to Coded Aperture Spectral Snapshot Imaging (CASSI). Could you please analyze the differences, advantages, and disadvantages of these two systems?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9181/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9181/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9181/Reviewer_wWh6"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9181/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698692259477,
        "cdate": 1698692259477,
        "tmdate": 1699637155196,
        "mdate": 1699637155196,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wmebRA4XBY",
        "forum": "wkbeqr5XhC",
        "replyto": "wkbeqr5XhC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9181/Reviewer_vQM3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9181/Reviewer_vQM3"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel approach using pre-acquisition modulation with a deep learning model called LUM-ViT. Specifically, it utilizes ViT as the backbone network and a DMD signal acquisition system for patch-embedding. Moreover, a kernel-level weight binarization technique and a three-stage fine-tuning strategy is proposed for optimizing the optical calculations. With low sampling rates, LUM-ViT maintains high accuracy on ImageNet dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe idea is of the paper is interesting. The proposed method performs calculations of the patch-embedding layer instead of directly sampling the whole images. The proposed LUM-ViT is suited for both dataset and downstream tasks.\n2.\tThe accuracy loss is low with extreme under-sampling"
            },
            "weaknesses": {
                "value": "1.\tThe description of the entire system and methods is not clear and intuitive enough, and the figures are also misleading. The RGB image is used as an example in Figure 1, which does not reflect the characteristics of hyperspectral imaging. \n2.\tLack of experiments on real hyperspectral imaging. The author has emphasized hyperspectral imaging in the introduction section, but in reality, it has not been verified using real hyperspectral images. The performance of this method in real hyperspectral imaging tasks still needs to be discussed"
            },
            "questions": {
                "value": "1.\tThe training phase uses images of 3 color channels while the real-world experiment uses 7 color images. What is the meaning of \u2018reconfigured\u2019? Or the author just fine tuned the LUM-VIT with 7 color samples? Can the pre trained model be used directly for images in different bands without the need for matching data?\n2.\tThis acquisition system seems to have to work together with vit to obtain intermediate features of images. Can it reconstruct a complete hyperspectral image in a real-world environment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9181/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698747496359,
        "cdate": 1698747496359,
        "tmdate": 1699637155086,
        "mdate": 1699637155086,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "88HuKEghJp",
        "forum": "wkbeqr5XhC",
        "replyto": "wkbeqr5XhC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9181/Reviewer_rvtr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9181/Reviewer_rvtr"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a learnable under-sampling mask vision Transformer, which incorporates a learnable undersampling mask tailored for pre-acquisition modulation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper is well-organized and clearly written.\n\n+ The proposed three-stage training strategy for training LUM-ViT is effective."
            },
            "weaknesses": {
                "value": "- Technical details should be clear. How to achieve the learnable under-sampling mask? How is this learnable achieved? Is the learning accurate? Relevant visualization results should be provided.\n\n- The experimental results seem insufficient. The author only conducted validation on the ImageNet-1k classification task, and other tasks should also be further explored.\n\n-----------------------After Rebuttal---------------------------\n\nThank you for your feedback. The rebuttal addressed my concerns well. Considering other reviews, I have decided to increase my score."
            },
            "questions": {
                "value": "See the above Weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9181/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9181/Reviewer_rvtr",
                    "ICLR.cc/2024/Conference/Submission9181/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9181/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821511658,
        "cdate": 1698821511658,
        "tmdate": 1700746364317,
        "mdate": 1700746364317,
        "license": "CC BY 4.0",
        "version": 2
    }
]