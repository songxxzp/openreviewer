[
    {
        "id": "DWPkkdILT0",
        "forum": "g5Iqg4BwsF",
        "replyto": "g5Iqg4BwsF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4849/Reviewer_vsHS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4849/Reviewer_vsHS"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes the Iterative Demonstration Selection method to leverage the merits of both dimensions. With Zero-shot-CoT, IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. The proposed method utilizes the output reasoning path to choose demonstrations. After several iterations, IDS adopts majority voting to obtain the final result. The authors assert that the optimal dimension for selecting demonstration examples is task-specific."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper is written in a clear manner.\n* There is an experiment demonstrating the need to consider diversity and similarity in a task-specific manner.\n* There are several analyses regarding the number of demonstrations, the number of iterations, and model types."
            },
            "weaknesses": {
                "value": "* The experimental results look somewhat marginal. \n* The performed experiments are too limited in scope. They were conducted only on CommonsenseQA, BoolQ, AGNews, DBPedia, SST2, and Amazon, which undermines the robustness of the methodology. Results from a more diverse set of tasks are needed e.g., not the classification tasks.\n* Measuring cosine similarity between the reasoning path R and the training set lacks some persuasiveness as a criterion for selecting few-shot samples. If there are theoretical or empirical reasons why measuring cosine similarity between reasoning path R and training set samples that have somewhat different forms from reasoning path would be helpful, please provide them.\n* There is too much repeated content in the text. e.g.\"the generated answer is accompanied by its\ncorresponding reasoning path \"\n* There is a lack of comparison with other demo selection baselines, not just the internal baseline."
            },
            "questions": {
                "value": "* How long does it take to measure the correlation between reasoning path R and the whole training set for each task?\n* Please provide results using a different encoder model other than Sentence-BERT. It seems necessary to verify if solidity is ensured depending on the encoder.\n* Could you provide full results on all tasks, not the average one?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4849/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698390287269,
        "cdate": 1698390287269,
        "tmdate": 1699636468464,
        "mdate": 1699636468464,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZuyqxyUluj",
        "forum": "g5Iqg4BwsF",
        "replyto": "g5Iqg4BwsF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4849/Reviewer_R6Td"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4849/Reviewer_R6Td"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the challenge of selecting the most suitable few-shot demonstrations for in-context learning (ICL) in large language models (LLMs).\nThe authors argue that the optimal selection dimension, i.e., diversity or similarity, is task-specific and propose an Iterative Demonstration Selection (IDS) method that leverages the merits of both dimensions.\nIDS uses zero-shot chain-of-thought reasoning (Zero-shot-CoT) to iteratively select examples that are diverse but still strongly correlated with the test sample as ICL demonstrations.\nExperiments on various tasks demonstrate the effectiveness of IDS."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The methodology is well-explained, with IDS applying Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is iteratively used to choose demonstrations that are prepended to the test sample for inference. After several iterations, IDS adopts majority voting to obtain the final result.\n\n2. Experiments on various tasks and thorough analysis on hyper-parameters (e.g., number of demonstrations and number of iterations) demonstrate the effectiveness of IDS."
            },
            "weaknesses": {
                "value": "1. Lack of comparison with stronger baselines. Much related work and methods for in-context example selection (e.g., EPR [1], TST[2], CEIL[3], Skill-KNN[4]) are not experimentally compared (or even not mentioned). At least some of them should appear in the experiments part.\n\n2. Actually, this work is not \"the first time consider both the diversity and similarity dimensions of ICL demonstration selection for LLMs\".\nFor instance, [5] use MMR that considers both similarity and diversity. [6] also demonstrates the effectivenss of incorporating both similarity and diversity.\n\n[1] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. \"Learning to retrieve prompts for in-context learning.\"\n\n[2] Gabriel Poesia, et al. \"Synchromesh: Reliable code generation from pre-trained language models.\"\n\n[3] J. Ye, et al. \"Compositional exemplars for in-context learning.\"\n\n[4] S. An, et al. \"Skill-Based Few-Shot Selection for In-Context Learning.\"\n\n[5] X. Ye, et al. \"Complementary explanations for effective in-context learning.\"\n\n[6] S. An, et al. \"How Do In-Context Examples Affect Compositional Generalization?.\""
            },
            "questions": {
                "value": "How would IDS perform without voting, i.e., just use the final answer achieved by the last iteration? I suppose this result better reflect the effectiveness of the designed iteration."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4849/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4849/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4849/Reviewer_R6Td"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4849/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698570802609,
        "cdate": 1698570802609,
        "tmdate": 1700622369851,
        "mdate": 1700622369851,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Epoor03pMl",
        "forum": "g5Iqg4BwsF",
        "replyto": "g5Iqg4BwsF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4849/Reviewer_LAeZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4849/Reviewer_LAeZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Iterative Demonstration Selection (IDS) to conjoin diversity and similarity for tackling the problem of demonstration selection for in-context learning of LLMs. Technically, IDS leverages intermediate reasoning paths provided by LLMs to retrieve relevant training samples. IDS is evaluated on several representative tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well motivated with preliminary experimental results.\n- The literature review is good."
            },
            "weaknesses": {
                "value": "- Overall, the presentation of the paper should be improved, and the technical novelty is limited. In particular, Figure 2 should be carefully polished.\n- The explanation of why IDS can incorporate diversity should be made clear. I notice the argument \"they can be\ndifferent during iterations to ensure diversity because the reasoning paths vary in different iterations\", but how can you ensure such diversity? Purely rely on the randomness in LLM sampling?\n- It seems that the evaluation tasks are relatively simple. Have you tested IDS on GSM8K or MATH?\n- In my opinion, when IDS and the baselines use the same number of reasoning paths or iterations, IDS actually uses one more query to GPT than the baselines (due to the first zero-shot CoT). As a result, in the 1-iteration column of Table 6, IDS is better. If so, I want to know if you give the baselines one more query to GPT, can their performance be further improved?\n- IDS's performance should heavily rely on the metrics used for retrieval. Have you tested other choices except for Bert Distance?"
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4849/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638620086,
        "cdate": 1698638620086,
        "tmdate": 1699636468310,
        "mdate": 1699636468310,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6KCDTyH6ip",
        "forum": "g5Iqg4BwsF",
        "replyto": "g5Iqg4BwsF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4849/Reviewer_MpmC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4849/Reviewer_MpmC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a strategy, Iterative Demonstration Selection (IDS), used for example selection in in-context learning (ICL) setting of large language models (LLMs), focusing on the balance between diversity and similarity. IDS leverages the reasoning path elicited by zero-shot chain-of-thought (CoT) for similarity between the test sample and demonstrations and iterative selection for diversity among ICL examples. They evaluate IDS on several NLP datasets (commonsense reasoning, question answering, topic classification, and sentiment analysis) and show it consistently outperforms existing ICL demonstration selection methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written, and the proposed method IDS is easy to follow.\n\n2. This paper provides a conclusion that both similarity and diversity are important in example selection in ICL scenarios, which can help other researchers who are working on a similar area."
            },
            "weaknesses": {
                "value": "1. Experiments are conducted based on simple tasks such as classification, commonsense reasoning, etc., on which the improvements seem marginal. More complex and difficult generative tasks, including mathematical reasoning, QA, and machine translation, are encouraged to be adapted.\n2. The conclusion that \u201cit is unreasonable to claim that one dimension is consistently better than the other across different tasks\u201d is drawn through only two datasets, AGNews and CommonsenseQA, which is not that solid. \n3. IDS based on voting brings in 4x of the overhead for API requests, while the improvements are a little marginal.\n4. Experiments are conducted on GPT-3.5-turbo. More LLMs, such as Vicuna, Llama, and Alpaca, can be tested."
            },
            "questions": {
                "value": "1. Comparison with other example selection strategies has yet to be explored. Does the balance matter? The exploration for similarity and diversity is only conducted on 2 datasets.\n\n\n[1] Li, Xiaonan, and Xipeng Qiu. \"Finding supporting examples for in-context learning.\" arXiv preprint arXiv:2302.13539 (2023).\n[2] Ma, Huan, et al. \"Fairness-guided Few-shot Prompting for Large Language Models.\" arXiv preprint arXiv:2303.13217 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4849/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763902290,
        "cdate": 1698763902290,
        "tmdate": 1699636468227,
        "mdate": 1699636468227,
        "license": "CC BY 4.0",
        "version": 2
    }
]