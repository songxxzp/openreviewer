[
    {
        "id": "nCgWtcXHkj",
        "forum": "SvEMbCMgb5",
        "replyto": "SvEMbCMgb5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5169/Reviewer_yMkf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5169/Reviewer_yMkf"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a range regularization method for training quantized model. The range regularization extend from $\\ell_\\infty$ regularization to more advanced formulation that can remove the outliers in weight distribution, leading to less quantization error."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper proposes a novel regularization-based method to tackle the outliers in the quantized model. The method is straightforward and effective.\n\nExperiments are conducted under different settings to show the effectiveness of the proposed method\n\nThe paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "There is no clear analysis on under which scenario each of the proposed range regularization formulation will outperform the others. It would be good to have a switching mechanism to decide which formulation to use. \n\nAblation study on the impact of regularization strength for the proposed regularizer would provide more insight on the stability of the proposed method."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5169/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698466789800,
        "cdate": 1698466789800,
        "tmdate": 1699636512698,
        "mdate": 1699636512698,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fobts2d21l",
        "forum": "SvEMbCMgb5",
        "replyto": "SvEMbCMgb5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5169/Reviewer_htAB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5169/Reviewer_htAB"
        ],
        "content": {
            "summary": {
                "value": "This paper posits that outliers in the weight distribution are the primary reason for the decrease in model accuracy after quantization. To address this issue, an R2 regularization method is introduced to constrain the range of model weights, thereby eliminating outliers and improving the accuracy of the quantized model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper has a reasonable structure, simple and easy to understand methods, and is easy to read."
            },
            "weaknesses": {
                "value": "1. Many prior works have argued the impact of outliers on the quantization results, but this paper only references two from 2020 and 2021, and only references KURE from 2020 as the baseline for comparison, which lacks persuasiveness. \n2. It is hard to prove that applying R2 can lead to higher accuracy after quantization than not applying it. First, only four quantization methods are discussed in the experiments, and they can not cover all the quantizers. Secondly, the existing experiments in the paper show that the results  w/o R2 can also achieve higher model accuracy than that with R2 (Table 4). \n3. The work is incomplete or lacking contribution, and it would be better to include the design of a quantization method specifically for this range regularization in the paper. Clearly, EWGS itself already considers outlier handling, so using R2 leads to a decrease in accuracy, indicating the necessity of designing a corresponding quantization method for R2."
            },
            "questions": {
                "value": "1. I am interested in how the experimental results would differ if we applied the clip function instead of R2 regularization in model training. \n2. The author mentions that R2 can be applied to PTQ, but R2 itself is a regularization method used during model training, while PTQ aims to avoid retraining. Do these descriptions conflict with each other?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5169/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698568094421,
        "cdate": 1698568094421,
        "tmdate": 1699636512577,
        "mdate": 1699636512577,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wyXearSkuV",
        "forum": "SvEMbCMgb5",
        "replyto": "SvEMbCMgb5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5169/Reviewer_aeAj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5169/Reviewer_aeAj"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce \u201crange normalization,\u201d a new technique for normalizing model parameters to form weight distributions suitable for quantization.\n\nThis approach acts as a regularization loss during full-precision model training, and shows that pre-training with R^2 improves accuracy in subsequent Post-Training Quantization (PTQ) or Quantization-Aware Training (QAT) steps. This means the possibility of obtaining highly generalized models.\n\nThe authors experimentally verified the applicability of the proposed method to other quantization techniques, such as EWGS or DKM. In particular, this study shows that when the proposed method is applied simultaneously with EWGS, state-of-the-art (SOTA) results can be obtained through 2-bit quantization on MobileNet V1/V2."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This paper introduces a novel model parameter regularization technique called range regularization to shape a weight distribution conducive to compression/quantization.\n* This paper shows that training a model from scratch using $R^2$ can result in a model with better accuracy after applying subsequent post-training quantization (PTQ) or quantization-aware training (QAT). Experimental evidence shows the potential to obtain more highly generalized models with this approach.\n* The proposed method demonstrates its potential applicability to other quantization techniques, such as EWGS or DKM."
            },
            "weaknesses": {
                "value": "* Based solely on the experiments in the paper, it is challenging to discern whether the proposed $R^2$ method is more effective in terms of generalization compared to other regularization methods.\n* While the proposed method demonstrated effectiveness in experiments with CNN-based models, it appears challenging to compare its efficacy in other tasks or architectures, such as Language Models, based solely on the conducted experiments.\n* The paper demonstrates the effectiveness of $R^2$ based on weight distribution, but it does not address activations. Since the experiments in the paper show a significant compression ratio for activations, with activations also sharing the same number of bits as weights, it seems essential to include an analysis or discussion regarding activations."
            },
            "questions": {
                "value": "* The statement, 'In addition, for a large model like ResNet-18, $R^2$ also shows a regularization effect similar to weight decay, therefore, better validation accuracy than the one without $R^2$,' is unclear in its meaning. If $R^2$ exhibits a performance boost due to a regularization effect similar to weight decay, shouldn't $L_2$ regularization be more effective from the outset? A detailed discussion or experimental data addressing this is needed.\n* From the experiments in the paper alone, it's difficult to ascertain the effectiveness of the proposed $R^2$ method in terms of generalization. Are there any experimental data available to assess the Generalization Gap?\n* The paper only presents results for small models like CNN or MobileBERT, primarily focusing on CNN results. Are there any results showcasing the application of the proposed method to larger models such as ResNet-101 or LLM (e.g., transformer architectures like GPT-J)? Is there experimental evidence showing the more general applicability of the proposed approach?\n* Regarding the statement in Section 5, the authors say that 'We establish that range regularization doesn\u2019t affect the performance of the floating-point models as well, and in some overparameterized models, it also has a regularization effect to address overfitting.'. However, i think there is a lack of theoretical or experimental evidence supporting the claim that range regularization is more effective in preventing overfitting than previously proposed regularization techniques. Is there any theoretical basis or experimental contents related to this claim?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5169/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5169/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5169/Reviewer_aeAj"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5169/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762426750,
        "cdate": 1698762426750,
        "tmdate": 1699636512399,
        "mdate": 1699636512399,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UwjYT5igv8",
        "forum": "SvEMbCMgb5",
        "replyto": "SvEMbCMgb5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5169/Reviewer_vWZn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5169/Reviewer_vWZn"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes range regularization to shape the weight distribution for model compression and quantization. Three variants of range regularization are introduced. Experiments with ResNet-18, MobileNet-V1/2 (pretrained on ImageNet) are provided to demonstrate the effectiveness of range regularization on model compression and quantization."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea of shaping weight distribution for model compression and quantization makes sense. It seems the introduced R_inf and R_margin are effective to eliminate outliers from weight distributions. \n\nThe paper is well-written, and easy to follow."
            },
            "weaknesses": {
                "value": "It makes sense to eliminate outliers to improve the performance of weight quantization. However, it seems there is no explanation or justification why elimination of outliers can improve the performance of model compression.\n\nThe performance improvement of the proposed method is more pronounced for very aggressive compression ratio or 2-bit quantization, in which cases the baseline accuracies drop significantly (e.g., mostly halved). This makes the proposed method not very practical as it\u2019s not acceptable in almost all cases to deploy a significantly worse model even though the compression rate is high. Often time, the compressed models should have similar or slightly worse performance to the original uncompressed models.\n\nAll model compression and quantization baseline methods are from 2019, 2020 or 2021. I didn\u2019t follow the research in the area closely. But I believe there should be SOTAs in the past 2 years. So, it\u2019s unclear how effective the proposed method is over the latest baselines. \n\nTypos:\npage 3 bottom: tate-of-the-art \npage 6 middle: prove to be effective here.  -> ineffective"
            },
            "questions": {
                "value": "As indicated above, a justification why weight shaping can help model compression would be helpful.\n\nPlease consider using the SOTAs from 2022 or 2023 as baselines. It would be interesting to see if the gains still hold for aggressive compression or quantization."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5169/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698861689821,
        "cdate": 1698861689821,
        "tmdate": 1699636512226,
        "mdate": 1699636512226,
        "license": "CC BY 4.0",
        "version": 2
    }
]