[
    {
        "id": "il7wWeDi6P",
        "forum": "0NruoU6s5Z",
        "replyto": "0NruoU6s5Z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1626/Reviewer_e76K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1626/Reviewer_e76K"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel diffusion-based model, named CompoDiff, which could merge the multimodal conditional information, for solving composed image retrieval (CIR) task. It also proposes a newly created synthetic dataset, named SynthTriplet18M, of 18 million training triplets (reference image, conditions, and target image). The proposed model and dataset address the poor generalizability of existing CIR methods, due to the small training dataset scale and limited types of conditions. The experimental results show the proposed method achieves better results on four public CIR benchmarks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of leveraging the synthetic data for training of CIR task is pertinent, since the triplet-labeled training data required for CIR task is laborious to collect. \n2. Borrowing the idea from the diffusion of generative task, the authors also explore the possibility of adopting the diffusion mechanism for latent feature extraction in discrimination task, and prove it has the potential to achieve good retrieval accuracy. \n3. It is interesting that the adoption of diffusion enables negative text for CIR task."
            },
            "weaknesses": {
                "value": "1. The section 3.1 needs to be written more clearly, it is preferable to annotate the letters and variables that appear in Eq. (1), (2), (3) in Figure 3, for example, $z_{i,masked}$; what is the relationship between $e_{t}$ and $z_{i}^{t+1}$? It is difficult to understand from Figure 3 and Eq.(1), (2), and (3).\n2. The diffusion part for feature extraction is very opaque. In Figure 3, what is the intuition of forward diffusion (adding noise T times) and denoise diffusion? In the generation task, the output of the diffusion process consists of pixel values, which have a clear and explicit meaning, while in the retrieval task, the output of the diffusion process is latent variables that do not have a clear and explicit meaning.\n3. I think the authors should consider the time and resource consumption carefully. The training stage is very complex, since the framework involves two stages that both require massive data, and the stage 2 requires some tricks such as alternative strategy, which is unstable, and resource-consuming. \nFor the inference stage, it requires 5 diffusion processes for each query sample while each diffusion process still needs multiple steps, which is very time-consuming since the diffusion process is very slow. It is necessary to compare the inference time with previous methods. In section 4, when collecting the synthesized caption, fine-tuning the OPT-6.7B model is very time-consuming and resource-consuming. \n4. I think there is some mistake on the left side of Eq.(4). Besides, in section 4, x_c is used in the fourth and sixth rows, while x_{c_T} is used in the fifth row."
            },
            "questions": {
                "value": "In keyword-based diverse caption generation of Figure 5, according to my knowledge, it is not quite reliable to collect the alternative keywords using the CLIP feature similarity. Firstly, some alternative keywords may share a similar concept with the target keyword, but the synthesized caption may not be reasonable. For example, \u201cplants\u201d, \u201cflora\u201d share similar concept with \u201cstrawberry\u201d, but \u201cplants tart\u201d and \u201cflora tart\u201d is ridiculous. Even if frequency filtering is used and restricting the CLIP similarity within 0.5~0.7, this phenomenon still exists. Moreover, keywords such as \u201cportrait, figure, image\u201d have consistently high similarity with most keywords, but they do not have specific meanings; keywords such as \u201cpainting, drawing, walk, hiking\u201d may have different parts of speech (verb and noun), and some keywords such as \u201clight, chair, season\u201d may involve different meanings in different context. Note that what I'm referring to is not limited to the examples mentioned above, but it's a general issue, and all these problems can lead to the generation of very strange modified captions. I am curious how these problems are considered and solved."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1626/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698671129617,
        "cdate": 1698671129617,
        "tmdate": 1699636091501,
        "mdate": 1699636091501,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2G9rXvMSCb",
        "forum": "0NruoU6s5Z",
        "replyto": "0NruoU6s5Z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1626/Reviewer_wkZo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1626/Reviewer_wkZo"
        ],
        "content": {
            "summary": {
                "value": "The authors introduces CompoDiff, a novel diffusion-based model, for the task of Composed Image Retrieval (CIR) with latent diffusion. It also proposes a new dataset named SynthTriplets18M. Importantly, it supports diverse conditions like negative text and image masks, offers control over query importance, and allows trade-offs between inference speed and performance, improving the overall CIR process."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The concept introduced here is really interesting, although the components used here carry less novelty.\n\n- The writing of introduction, and the overall paper is quite fluid and easy to understand.\n\n- The synopsis of every topic is provided in a self-contained manner.\n\n- Qualitative Figures are well portrayed."
            },
            "weaknesses": {
                "value": "- Although the experiments are extensive, little reasoning is provided as to why the methods perform (low/high) in the way they do. More analytical reasoning would be encouraged.\n\n- The paper could've been written in a more self-contained manner. A basic background of unCLIP, segCLIP and other components could have been provided instead of simply citing the paper, even 2-3 lines would enhance the readability of the paper.\n\n- The training paradigm seems a bit convoluted. Rephrasing of certain sentences could bring about clarity in the understanding, for instance, discussing a small background on diffusion models first, then bringing in text-image composite part.\n\n- Although not intuitive in this respect it makes me wonder what would be the effect if a learnable text prompt is used in the CLIP-text branch?\n\n- Despite having a few competitors, it would have been better to provide a few baselines focussing on variations of design components used for the proposed method."
            },
            "questions": {
                "value": "- Does this retrieval include images containing multiple target objects for retrieval as well?\n- Although not intuitive in this respect it makes me wonder what would be the effect if a learnable text prompt is used in the CLIP-text branch?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1626/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1626/Reviewer_wkZo",
                    "ICLR.cc/2024/Conference/Submission1626/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1626/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698756100281,
        "cdate": 1698756100281,
        "tmdate": 1700884007643,
        "mdate": 1700884007643,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kypDsXX5U1",
        "forum": "0NruoU6s5Z",
        "replyto": "0NruoU6s5Z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1626/Reviewer_FhVu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1626/Reviewer_FhVu"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a composed image retrieval method, called CompoDiff, based on the diffusion model and proposes a large-scale dataset, called SynthTriplets18M, for the composed image retrieval task. In the experiments, the qualitative and quantitative results demonstrated that the performance of the proposed CompoDiff in terms of composed image retrieval exceeds the comparison method. Comparing the results of the model trained with different scales of data, it shows that a large amount of training data can improve the model effect."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper proposes a novel CIR method that can additionally limit the scope of the search image based on the input mask and other conditions.\n2. This method can also control the balance between retrieval accuracy and retrieval efficiency without training, as well as control the impact of each condition on the retrieval results.\n3. This paper proposes a dataset that promotes the development of CIR-related research and illustrates, to a certain extent, the impact of dataset size on methods."
            },
            "weaknesses": {
                "value": "1. The authors raised the problem of requiring triples for training: but it was not solved well, and the authors just proposed a larger data set.\n2. The experimental results show that the effect of the proposed data set and other data sets are similar at the same level, and after the data volume reaches a certain level, continuing to increase the size of the data set may not significantly improve the model performance. It negates the value of the data set to a certain extent.\n3. The data set used by the comparison method is inconsistent with the data set used by the proposed method, which is not quite fair."
            },
            "questions": {
                "value": "1. Can additional comparison experiments be conducted, for example, both the proposed method and the comparison method are trained on SynthTriplets18M to illustrate the effectiveness of the proposed method?\n2. The paper mentions that the size of the data set is very important. Can experiments regarding the size of the data set be conducted in other methods to illustrate the effectiveness of the data set?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1626/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772075004,
        "cdate": 1698772075004,
        "tmdate": 1699636091288,
        "mdate": 1699636091288,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "H7Mc3Fsmd2",
        "forum": "0NruoU6s5Z",
        "replyto": "0NruoU6s5Z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1626/Reviewer_AJF4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1626/Reviewer_AJF4"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the limitation of the small dataset and small categories of conditions on the Composed Image Retrieval (CIR) task through a new diffusion-based model (CompoDiff) and a large-scale CIR dataset (SynthTriplets18M). They show the generalizability of their dataset training CompoDiff on the existing CIR benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Clear presentation of their approach\\\nThis paper tackles the lack of a dataset for this task, which raises the generalizability of this task. It is a clear objective to present a large-scale dataset. To construct on a large scale, they utilize a large generative model such as Stable Diffusion to produce a synthetic dataset. This reviewer can agree with the direction of their approach, and it contributes to this community.\n\n\n- Flexibility of their model\\\nNot only improving the performance on the existing benchmarks of CIR, they also suggest a more flexible manner of CIR including negative texts or masks, which give a large potential for its application."
            },
            "weaknesses": {
                "value": "- A small improvement using better backbone architecture\\\nFor a fair comparison, it is hard to agree that their model (ViT-L) performs better than the previous arts whose backbones (RN50) are much lighter (Table. 2). Therefore, this reviewer recommends comparing them in the backbone with similar capacity (same backbone is the best option) as much as possible.\n- Efficiency comparison with the previous arts\\\nEven though they show inference time varying the diffusion steps, this reviewer suggests the comparison of latency or flops with the previous arts. Their intra-model analysis is also w\n- An insufficient contribution of CompoDiff\\\nAs far as this reviewer\u2019s understanding, COmpoDiff is a minor modified version of Stable Diffusion. This reviewer considers their flexibility for the conditions also stems from the power of Stable Diffusion. Also, this reviewer wonders what the performance of the Stable Diffusion trained on SynthTriplets18M is on the CIR benchmark."
            },
            "questions": {
                "value": "The questions are naturally raised in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1626/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1626/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1626/Reviewer_AJF4"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1626/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699257745214,
        "cdate": 1699257745214,
        "tmdate": 1699636091218,
        "mdate": 1699636091218,
        "license": "CC BY 4.0",
        "version": 2
    }
]