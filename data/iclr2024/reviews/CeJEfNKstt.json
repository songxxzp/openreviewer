[
    {
        "id": "fJauhqDto7",
        "forum": "CeJEfNKstt",
        "replyto": "CeJEfNKstt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3134/Reviewer_LibM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3134/Reviewer_LibM"
        ],
        "content": {
            "summary": {
                "value": "This paper examines a simple probe on the linear separability of binary factuality statements using Llama as a backbone. The analysis seem thorough and techniques all start with the most immediate things at hand, which is a good thing. There is an interesting observation that the PCA plots of true-false statements are so linearly separable. If broadly studied, such pattern could potentially be exploited to improve LLMs during training to make larger impact."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The motivation is good. Interpretability works often suffers from weak generalization and undetermined thresholding on generalization. This paper steps in this problem by observing the conditions of generalization."
            },
            "weaknesses": {
                "value": "The takeaway is unclear. The beautify linear separability could be because of the simplicity of the text in the curated dataset. In reality, this could be a luxury to have. Then, the usefulness of the proposed probe and establishment of the observations need to be re-examed.\n\nAnother thing is, what can people do with this problem is unclear. Can folks use the observation, say, inject a loss to improve LLM's factuality during pretraining or fine-tuning? There are some interesting discussions could happen, but not appeared in this paper.\n\nThe causal interference happens at hidden level instead of word/token level. Roughly saying, most probes can have a hidden interference to revert the binary output. But what's is more direct/interpretable is make it also work on text level."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3134/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791283919,
        "cdate": 1698791283919,
        "tmdate": 1699636260475,
        "mdate": 1699636260475,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HD0KELwY8x",
        "forum": "CeJEfNKstt",
        "replyto": "CeJEfNKstt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3134/Reviewer_hn5S"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3134/Reviewer_hn5S"
        ],
        "content": {
            "summary": {
                "value": "The authors investigated the internal representations of a language model known as LLaMA and identified that the truth values of given sentences are represented in a certain linear direction. The key contributions of the authors include:\n- Development of a dataset consisting of declarative sentences related to world knowledge.\n- Introduction of the mass-mean probing approach, which defines the direction of truth values by connecting the centroids of the TRUE and FALSE classes.\n- Experimental verification that the direction of truth values appears in specific dimensions of specific internal representations of the specific LLM.\n- Validation that the direction of truth values is somewhat shared across multiple sub-datasets.\n- Confirmation that applying perturbations to the direction of truth values can lead to a reversal in the truth value of the given sentence."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Identifying the knowledge held by LLM and discovering the correspondence between its internal representations and world knowledge is crucial for realizing a trustworthy LLM. Particularly, the identification of the truth values of declarative plain sentences related to world knowledge aligns well with fundamental paradigms in semantics within NLP, such as truth-conditional semantics. The theme of the paper is likely to be well-received within the community.\n- Controlled datasets created to measure only specific aspects of meaning will likely be useful for researchers studying the truthfulness of sentences."
            },
            "weaknesses": {
                "value": "The experimental setup appears arbitrary and limited, diminishing the persuasiveness of the authors' general claims.\n\nFor instance, despite the availability of numerous pretrained language models, experiments were conducted solely on LLaMA, making it unclear whether the findings apply generally to LLMs. The authors explicitly mention this as a limitation, which should be acknowledged for its intellectual honesty. However, it is hard to deny that the verification is lacking when considering the subject of interest stated in the main claim (Large Language Models). If the focus was merely on the \"Linear Structure in LLaMA\", the experiments would suffice. Yet, in that case, it might not be deemed impactful enough for acceptance at ICLR.\n\nFurthermore, although there are numerous internal representations available for sentence representations, experimental results are provided only under very specific settings: a specific layer, specific parts of the network (after the residual stream), and right after period characters. Additional settings, such as the utilization of top principal component directions and connecting centroids, also follow specific configurations. Taking into account that multiple options exist for each of these aspects, it becomes somewhat challenging to dispel concerns that the reported results might be cherry-picked."
            },
            "questions": {
                "value": "Experiments related to \"causality\" were conducted using latent representations, and the correspondence with the world of language (where meaning is directly encoded) remains unclear. For example, does the perturbation that converts from TRUE to FALSE correspond to the insertion of a negation word?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3134/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838609915,
        "cdate": 1698838609915,
        "tmdate": 1699636260398,
        "mdate": 1699636260398,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hhsMySnYaZ",
        "forum": "CeJEfNKstt",
        "replyto": "CeJEfNKstt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3134/Reviewer_WvMz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3134/Reviewer_WvMz"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a few new datasets and methods to identify whether LLMs have a \"truth vector\" in their representation spaces: a direction pointing from true to false statements.  They carry this out using one model (LLaMA 13B) and several methods: some data is curated/synthetic, with clear and unambiguous truth labels, while others are more open-ended.  They compare standard probing on these data to \"mass-mean probing\", which does not require training (it defines the vector pointing from the mean of the inputs corresponding to one label to those of the other label and then passes it through a logistic).  They find promising results: aside from the probes finding linear truth information, many of them also transfer from one dataset to another (i.e. trained on one dataset, tested on another), suggesting a general/multi-purpose truth direction.  Similarly, the mass-mean probed vector can be used for causal intervention, to flip a model's judgment from true to false and vice versa."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Detailed analysis of whether an LLM can learn to distinguish true and false data within and across tasks.\n- Good use of synthetic and natural data for this purpose.\n- Mass-mean probing seems to be an effective method for inducing probes _without training_ and could be more broadly applicable.\n- Causal intervention on the model's representations using the probes shows that the truth vectors are \"active\" and not \"inert\".  (This also demonstrates the importance of working with open models, where such interventions can be done.)"
            },
            "weaknesses": {
                "value": "- The paper has so many experiments and results that it could benefit from a richer discussion of how to interpret all of the results.\n- Only tests one model, so it's unclear how generalizable the findings are (the authors, of course, acknowledge this).\n- I would have also appreciated a bit more detail about the datasets and how they were generated to be in the body of the paper instead of appendices."
            },
            "questions": {
                "value": "- Table 2: do you have any intuition for why the False->True direction is so much worse for LR than mass-mean?  This seems especially stark.\n- Page 8, $\\ell = 10$ for the intervention experiments: why that layer (especially since layer 12 was used earlier)?\n- Is there a natural way of generalizing mass-mean probing to multi-class tasks beyond binary ones?\n- Table 2: the $\\alpha$ values seem extremely large here, to the point where the resulting vector is almost going to look just like the truth vector.  How do you think about these large values?  (Does it make sense to do a baseline where the model just sees $\\alpha\\theta$?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3134/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698952188781,
        "cdate": 1698952188781,
        "tmdate": 1699636260318,
        "mdate": 1699636260318,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "y4Lt3MW5CF",
        "forum": "CeJEfNKstt",
        "replyto": "CeJEfNKstt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3134/Reviewer_4DXR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3134/Reviewer_4DXR"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a new probing technique called mass-mean probing that can uncover the truth representations of LLMs. Using this method, the paper found that 1. LLM internally contains a truth representation that is linearly structured. 2. Such representation generalizes to other datasets. 3. Such truth vectors are casually implicated."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. A new alternative to regular logistic regression probing is proposed, which overcame the problem of logistic regression where the truth direction may be interfered with by an independent feature. Mass-mean probes show significant improvements in causal interference. \n2. Clear visualization of the separation of True/False statements.\n3. A new dataset to train such linear probes."
            },
            "weaknesses": {
                "value": "1. The majority of the conclusions and claims are not unique. For example, from Li et al. and Burns et al. (which the paper cites), we already knew that such truth representation is linearly separable and one can apply casual intervention to such representation. \n2. Lack of model variances. The experiments are only conducted on LLaMA-13B. As a result, we don't really know the effects of scale or the effects of the pretraining paradigm on such representations.\n3. Mass-Mean probe doesn't really improve generalization accuracy over some of the other methods that much."
            },
            "questions": {
                "value": "Question:\n1. Does the truth direction generalize to other LLMs? This may be an interesting direction to explore.\n\nStyling:\n1. The figure on page 17 is disproportionate"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3134/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3134/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3134/Reviewer_4DXR"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3134/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699386169996,
        "cdate": 1699386169996,
        "tmdate": 1699636260240,
        "mdate": 1699636260240,
        "license": "CC BY 4.0",
        "version": 2
    }
]