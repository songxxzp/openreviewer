[
    {
        "id": "rwrPQVZCxI",
        "forum": "Je5SHCKpPa",
        "replyto": "Je5SHCKpPa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6228/Reviewer_D1yA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6228/Reviewer_D1yA"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a novel framework, namely MUSE, to handle the missing modalities and labels while learning the multimodal representation of patients. The authors extend the conventional missing modalities problem to the missing labels issue and tackle both challenges concurrently using GNN-based contrastive learning. The authors conduct extensive experiments and analyses to demonstrate the superiority of the proposed MUSE."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The missing label problem widely exists in medical representation learning. However, previous works in this area do not handle it appropriately. This work provides excellent problem formulation to describe this research gap and proposes valuable solutions that can be inspirable to subsequent research.\n2. The paper is well-organized. The research problem is described comprehensively. The necessity of emphasizing modality-agnostic and label-decisive features is clearly illustrated, solidifying the motivation of adopting supervised and unsupervised learning accordingly. \n3. Modeling the modality missing and label missing problem is ingenious. Through contrastive learning, the proposed MUSE appropriately leverages alignments corresponding to these two problems, i.e., alignment between original and augmented graphs and between patients sharing different labels. \n4. Experiments are extensive. Sections 4.4 \u2013 4.7 significantly help readers to understand better how MUSE works, as well as advantages that cannot be simply reflected in major experiments in Sections 4.1 and 4.2."
            },
            "weaknesses": {
                "value": "1. More baselines should have been added for a more comprehensive comparison, e.g., MMIN (Zhao et al., Missing modality imagination network for emotion recognition with uncertain missing modalities.) and Robust Multimodal Transformer (Ma et al., Are multimodal transformers robust to missing modality?).\n2. Other clinical tasks for ICU data can also be explored, such as ARF prediction and shock prediction (Tang et al.,  Democratizing EHR analyses with FIDDLE: a flexible data-driven preprocessing pipeline for structured clinical data.)."
            },
            "questions": {
                "value": "Besides the concerns raised above, I have the following questions regarding this work:\n1. If we regard labels as a subset of features, the whole architecture can be perceived as a pretraining framework combining two contrastive learning loss terms, along with a classification loss serving downstream tasks. I wonder if the authors have conducted experiments to pre-train MUSE with two contrastive learning terms and then fine-tune it using the classification loss term. The two-stage pattern might help the model to take a further step towards better performance by separating pretraining and fine-tuning. \n2. For consistency, it is better to replace the \u201cmodality-general\u201d in the abstract with \u201cmodality-agnostic.\u201d \n3. For readability, it might be better to explicitly introduce how the trio of losses are combined using an individual mathematical formula rather than verbally describing it in Section 3.3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6228/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698716316137,
        "cdate": 1698716316137,
        "tmdate": 1699636680533,
        "mdate": 1699636680533,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FWnmsFnyHa",
        "forum": "Je5SHCKpPa",
        "replyto": "Je5SHCKpPa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6228/Reviewer_eaku"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6228/Reviewer_eaku"
        ],
        "content": {
            "summary": {
                "value": "The paper presents MUSE, a mutual-consistent graph contrastive learning method, which is designed to account for multimodal scenarios where both the label and different modalities are missing. This method is used in the context of clinical decision support and is validated on three different medical datasets (MIMIC-IV, eICU, and ADNI), against existing imputation and direct prediction approaches. Furthermore, the authors perform ablation experiments to better understand the value of each component in the model, examine the similarity of representations across methods, and discuss the run-time efficiency of methods. Overall, MUSE presents performance increases in the presented tasks without exceeding existing methods\u2019 compute times."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper presents a strong argument for needing methods that can account for both missing modalities and missing labels. MUSE, based on the cited literature, is an original idea that has clear implications for the medical community and could be potentially extended to multimodal learning in other scenarios. \n\n\n+ MUSE is validated on three datasets, which is important in demonstrating its generalizability. Furthermore, the additional experiments presented in the paper (missing data, ablation, similarity, and run-time analyses), add interesting insights into the different methods and are valuable in understanding what types of models work well in these multimodal medical settings. \n\n\n+ The model architecture is straightforward and well-described. Representing patient-modality relationships in a graph and using edge dropout as a way to simulate missing modalities is creative yet easy to conceptualize. Incorporating both supervised and unsupervised contrastive learning is a great way to correct the modality collapse problem, as contrastive learning has been shown to be very effective in multimodal scenarios. \n\n\n+ The authors describe the method in great detail and include the exact parameter values they used, which is a great community contribution as this method can be easily reproduced."
            },
            "weaknesses": {
                "value": "+ A weakness of the paper is that some of the architecture, experimental design, and parameter choices are not explained and not justified. For example, there are many ways to compute similarity (such as the normalized dot product used in the well-established InfoNCE loss), so a discussion on the choice of cosine as the similarity function is needed. \n\n+ Similarly, while it is helpful for reproducibility to list the hyperparameters used in the model, this alone is not enough as their choice is not justified. Searching through the main paper as well as the appendix, there is no mention of hyperparameter tuning. Hyperparameter tuning is important both for the baselines and for the proposed MUSE to establish the fairness of comparison, as the wrong choice of parameter could lead to a baseline performing worse than it could. This is especially true as the baseline architectures are vastly different (ranging from auto-encoders to Transformers)\n\n+ Other unexplained choices include using 15% as the edge dropout rate, the use of Siamese GNN over other GNN methods, doing the missing ratio experiment only on the mortality prediction task for MIMIC, and choosing 30% randomly masked modalities in the analysis of the learned representations. \n\n\n+ Even if the MIMIC mortality prediction task is the most interesting, it would be great to include the same experiment for all other datasets and tasks in the appendix. Overall, anytime there is a choice of number, task, or a specific function, there should at least be a sentence explaining that choice, at a minimum backed by logic and intuition and ideally backed by experiments, to justify the omission of alternative choices. \n\n+ The paper mentions the use of bootstrapping 1000 times, however, it is unclear if the reported performance is an average over the random weight initializations.\n\n\n+ While the dataset statistics are mentioned in the Appendix, it is important to point out the class label imbalance present in the datasets in the main paper. This contextualizes the performance of the models; for example, MUSE+ performs with ~92% AUC-ROC on MIMIC mortality prediction, but only 9% of the data has a positive mortality prediction. Meaning, a model that does not learn truly learn the task, can simply guess the majority label and get 91% accuracy on the task. This may not be the case here, but this information is important to better understand performance.\n\nMinor points:\n\n+ The paper will benefit from a more comprehensive Related Works Section, not just on methods pertaining to missing modalities, but on other multimodal methods both supervised and unsupervised, and other contrastive loss functions. This would help the reader understand how the proposed formulas differ from existing formulas and why the author\u2019s choices are better.\n\n+ The authors do not discuss any limitations. The paper would benefit from a Limitations and a Discussion Section to address many of the aforementioned points."
            },
            "questions": {
                "value": "+ In the Missing Modalities and Labels paragraph, it is unclear what the text means by \u201cfail to fully use inter-patient relationships?\u201d \n\n+ Could it be clarified what is meant by storing and reusing similarity calculations? Where is the similarity calculation reused, and why?\n\n+ It is stated in the Training and Inference Section that for MUSE, the authors only feed the original graph through a GNN, but this contradicts the model diagram, which shows both augmented and original graphs going through a GNN. Can this be explained or clarified in the text?\n\n+ The implementation details are unclear. \u201cWe train all models for 100 epochs on the training set, and select the best model by monitoring the performance on the validation set.\u201d What is meant by \u201call models\u201d? Does this include the baselines or only differently initialized MUSE models? Are the authors using the validation performance to determine all hyperparameters, and what specific metric is used? Is there early stopping for epochs, or are models trained for the full 100 epochs? \n\n\n+ The paper cites the Transformer paper (Vaswani et al.) as the backbone encoder for several tabular modalities. The original Transformer paper cited is meant for text and not necessarily tabular data, so were there further adjustments made for this encoder?\n\n\n+ In the Related Works Section, the authors mention MulT, ViLT, and UMSE, as other existing methods. Why were those methods not used as baselines?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6228/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6228/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6228/Reviewer_eaku"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6228/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764235594,
        "cdate": 1698764235594,
        "tmdate": 1700692313541,
        "mdate": 1700692313541,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8PDqTi6hzT",
        "forum": "Je5SHCKpPa",
        "replyto": "Je5SHCKpPa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6228/Reviewer_3igL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6228/Reviewer_3igL"
        ],
        "content": {
            "summary": {
                "value": "This work creates a contrastive learning approach to learning patient representations across data modalities, and augments the learning procedures by creating bipartite patient-modality graphs with various dropout to represent missing data modalities. Through the contrastive objectives, it learns to keep similar representations aligned in the embedded space despite a variety of modality differences. Additionally the authors learn to represent similar and dissimilar representations in relation to downstream labels, in order to provide an ability to represent patients even without said labels. They then demonstrate their model effectiveness over a variety of tasks, compared against a number of baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The approach to learn representations that may allow for some variance in the features collected per person is an important finding for clinical applications\n\n- There is novel application and design of the contrastive objectives, allowing for modality differences as well as using labels to find representations guided by the strongest risk factors\n\n- There are a comprehensive set of baselines and the Muse+ model outperforms on all tasks\n\n- The paper is extremely well written and clear to follow"
            },
            "weaknesses": {
                "value": "Major\n\n - The use of the supervised contrastive loss and the classification loss seems redundant. This is essentially the cross-entropy loss of the representations followed by the cross-entropy loss of a non-linear transformation of the representation. I appreciate ablation study showing that the removal of the supervised contrastive loss results in worse performance. I see that the final loss is the unweighted sum of all the losses. Is it possible to achieve the same results by removing the supervised contrastive loss and adding a higher weight to the classification loss? If it is necessary, could you provide a justification for why we need to classification losses?\n\nMinor\n - While ablation studies were provided that showed an array of dropout, did authors find a rate of missingness that ultimately resulted in the model ignoring certain features? (e.g. really rare lab examinations)\n - Typo in Appendix D.1 first paragraph. \u201cFor clinical nodes\u201d should be \u201cFor clinical notes\u201d I think."
            },
            "questions": {
                "value": "- Is it possible to achieve the same results by removing the supervised contrastive loss and adding a higher weight to the classification loss? If it is necessary, could you provide a justification for why we need to classification losses?\n\n - For the experiment analyzing the learned representation (Section 4.5) are you using the average cosine similarity across all patients? Could you also provide average Euclidean distance between representations since GRAPE minimizes MSE loss for continuous values?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6228/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6228/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6228/Reviewer_3igL"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6228/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778552205,
        "cdate": 1698778552205,
        "tmdate": 1699636680296,
        "mdate": 1699636680296,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "O6mQ4Ux9Du",
        "forum": "Je5SHCKpPa",
        "replyto": "Je5SHCKpPa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6228/Reviewer_se67"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6228/Reviewer_se67"
        ],
        "content": {
            "summary": {
                "value": "The authors tackle the problem of multimodal self-supervised representation learning using clinical data, where some modalities and some labels may be missing. They introduce a new contrastive earning method, which represents the dataset as a bipartite graph between patients and observed modalities. They initialize edge embeddings with observed data, and run a graph neural network over this graph, using a loss that combines supervised and unsupervised contrastive loss with the downstream classification loss. They evaluate their method on three clinical datasets, finding that they outperform the baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method is intuitive and easy to understand.\n- The paper is generally well-written.\n- The authors motivate the gain of jointly modelling missingness in modalities and labels both intuitively and empirically."
            },
            "weaknesses": {
                "value": "1. The datasets studied in the paper are fairly limited from a modality perspective in the classical sense (e.g. MIMIC-IV has time series and text, and eICU has only time series). Though the authors split the time-series into multiple modalities (e.g. labs, vitals, diagnoses), it is unclear what a modality has to encompass in order for the method to work well. In the extreme case, could each individual measurement have been its own modality?\n\n2. The method proposed in the paper tackles a healthcare problem, but could definitely be applied to datasets beyond healthcare. The authors should consider benchmarking their method on datasets from [1], potentially by masking out labels and modalities.\n\n3. It was not clear to me reading through the paper exactly how the method handles completely new patients at test-time. Are they added as new patient nodes and edges in the bipartite graph, and the GNN is run in inference with all of the pre-training patient nodes still in the graph?\n\n4. The authors should add [2] as a baseline which does not make use of any labels during pretraining, which can then be finetuned on the labelled set. The authors should also add in baselines corresponding to pre-training an encoder on each modality separately (e.g. as in [3]), either with contrastive learning or with supervised learning, to show the gain of modelling modalities jointly in the pre-training stage.\n\n5. In healthcare, missingness is often informative, e.g. a patient did not receive a chest X-ray because the physician did not suspect any pulmonary conditions, and such patients are thus less likely to have a pulmonary condition. The authors should consider probing this effect in the experiments. In particular, on the eICU dataset (where the authors currently add missingness to labels randomly), they could instead correlate label missingness with modality missingness and the true label.\n\n[1] MULTIBENCH: Multiscale Benchmarks for Multimodal Representation Learning. NeurIPS 2021.\n\n[2] Understanding Multimodal Contrastive Learning and Incorporating Unpaired Data. AISTATS 2023.\n\n[3] MedFuse: Multi-modal fusion with clinical time-series data and chest X-ray images. MLHC 2022."
            },
            "questions": {
                "value": "Please address the weaknesses above. There is also a minor typo -- the second $\\mathcal{G}$ should say $\\mathcal{G}'$ in Section 3.2 paragraph \"Mutual-Consistent Contrastive Loss\" ."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6228/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6228/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6228/Reviewer_se67"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6228/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699243128251,
        "cdate": 1699243128251,
        "tmdate": 1700692782484,
        "mdate": 1700692782484,
        "license": "CC BY 4.0",
        "version": 2
    }
]