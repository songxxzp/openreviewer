[
    {
        "id": "JXLd2JZndp",
        "forum": "K8Mbkn9c4Q",
        "replyto": "K8Mbkn9c4Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3231/Reviewer_6PPe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3231/Reviewer_6PPe"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to transform tabular data into image formats and utilize pretrained vision models to help the learning of tabular few-shot learning. The experiment results show that the method can be better than a strong LLM-based method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Very simple and effective method.\n- Transforming tabular data into image format is intuitive and novel.\n- The proposed method has good performance even with a small visual encoder, better than an LLM-based method, which is very promising."
            },
            "weaknesses": {
                "value": "- Only two papers are discussed in the related work section, which makes reader difficult to place the paper in an appropriate context.\n- The relationship between the quality of the visual encoder and the few-shot tabular performance is not shown. \n- Missing an important baseline (See questions).\n- Only the domain transformation module is proposed by the authors. Novelty is somewhat lacking."
            },
            "questions": {
                "value": "- Can you discuss more related works in the paper? For example, a brief introduction to the tabular learning literature.\n- Can you give results using a more powerful visual encoder? In Luo et. al [1], it has been shown that better visual encoders can lead to better few-shot learning performance. Perhaps, you can try to use pretrained CLIP [2] or DINO-v2 [3] and report the results.\n- Another straight way of transforming the tabular data into images is to directly visualize the table on an image in its original form. This should be a baseline to illustrate the advantage of your proposed tabular data transformation.\n\n[1] A Closer Look at Few-shot Classification Again. ICML 2023.\n\n[2] Learning Transferable Visual Models From Natural Language Supervision. ICML 2021.\n\n[3] DINOv2: Learning Robust Visual Features without Supervision."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3231/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3231/Reviewer_6PPe",
                    "ICLR.cc/2024/Conference/Submission3231/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3231/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698393794792,
        "cdate": 1698393794792,
        "tmdate": 1700706893452,
        "mdate": 1700706893452,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BAyZrGaKEn",
        "forum": "K8Mbkn9c4Q",
        "replyto": "K8Mbkn9c4Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3231/Reviewer_NMSd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3231/Reviewer_NMSd"
        ],
        "content": {
            "summary": {
                "value": "The authors present a new few-shot learning method for tabular data. By transforming the tabular data into image representations (_tabular images_), they hope to transfer prior knowledge that is readily available in the image domain onto the tabular task to improve results and make up for the scarcity of otherwise shared/prior information in the tabular domain. The argument is that in this way, proven methods from the well-explored image-based few-shot learning area can be leveraged to advance the area of tabular few-shot learning. The authors test their approach using two popular few-shot methods and four vision backbones on a variety of datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "### Originality & Significance:   \nThe paper explores an interesting underlying idea to leverage information from a well-explored area (in this case the image domain) and transfer both prior knowledge and existing/proven algorithmic methods; \n\n### Quality: \n- Data: Authors experiment on different datasets and consider different important aspects: 1) feature diversity (categorical vs. numerical), 2) task diversity (n-way classification), 3) applicability to/relevance for \u2018real-world\u2019 applications, in this case medical data; \n - Architectures: experimentation with 4 different versions to gauge parameter and architectural influences;\n \nI do however see severe weaknesses in most other parts, see the following."
            },
            "weaknesses": {
                "value": "While I do like the general underlying idea, there are several severe weaknesses present in this work \u2013 leading me to lean towards rejection of the manuscript in its current form. The two main areas of concern are briefly listed here, with details explained in the \u2018Questions\u2019 part:\n\n### 1) Lacking quality of the \u201cDomain Transformation\u201d part\nThis is arguably the KEY part of the paper, and needs significant improvement in two points: Underlying intuition/motivation/justification,   as well as technical correctness and clarity.  There are several fundamental points that are unclear to me and require significant improvement and clarification; This applies to both clarity in terms of writing but, more importantly, to the quality of the approach and justifications/underlying motivations. \nPlease see the \u201cQuestions\u201d part for details.\n\n### 2) Lacking detail in experiment description: \nDescription of experimental details would significantly benefit from increased clarity to allow the user to better judge the results, which is very difficult in the manuscript\u2019s current state; See \"Questions\" for further details."
            },
            "questions": {
                "value": "### Main questions regarding Domain Transformation part:\n\nTechnical parts: \n\n-\tCreating the (N,N) feature matrix R via Euclidean distance between N features -> What is the intuition behind this? Euclidean distance is symmetric (as squared), so isn\u2019t the (N,N) matrix symmetric (if unranked) or has double-entries (if sorted/ranked)?\n-\tThe authors then go on to state: \u201cWe also measure the distance and rank between N elements [..] to generate an (N,N) pixel matrix, denoted as Q.\u201d -> What exactly is being compared/contrasted here? What \u2018pixels\u2019 are used here? \n-\tThis is followed by another Euclidean distance between R and Q \u2013 Again, I am missing the intuition/justification behind this. \n-\tThe authors claim that this then results in \u201ca 2-dimensional image of size (Nr x Nc)\u201d. How exactly is this obtained from computing the Euclidean distance between two (N,N) matrices? \n--- \nFurther details & justification: \n\n-\tHow are the ranked features arranged to form a 2D \u2018image\u2019? This should significantly affect the way how ConvNets perform on them! More detail is required here.\n-\tWhy would a ranking of the distances between features and pixels followed by rearrangement in any way resemble information presented in natural images? In natural images, the local relationship between pixels is defined by the occurrence of objects at a spatial location within the image. Why should a network pretrained on such data (in this case miniImageNet) be \u2018useful\u2019 to work on the artificially created tabular images? How do you overcome the (potentially significant) domain gap here? Or at least, what is the intuition behind it? (While the authors provide some insight in Figure 4, a 2D circle in t-SNE is not necessarily representative due to the hyperparameters involved in the projections); I'd invite the authors to further comment on this and their underlying intuitions.\n--- \n-\tAdditionally: Since common CNNs take in RGB images (3 channels) but the authors create only images w/ 1 channel, they simply repeat the same image 3x for each channel \u2013 this seems like unnecessary overhead and simply engineered to fit existing input layers. If the created images are simply grayscale (as they seem to be according to Figure 1), wouldn\u2019t it be more reasonable to pretrain the backbone on grayscale images?\n-\tIn the introduction, the authors state that \u201cfeatures within tabular data have independent distributions and ranges, and missing values may be present.\u201d Neglecting the missing values, how are the authors treating this challenge of different ranges? The Euclidean distance between features can largely vary if ranges differ, so how exactly are these values converted into image pixels which usually are defined within a fixed range of [0, 255] per channel?\n---\n\n### Experiments & Interpretation: \n\nTable 1 aims to demonstrate the benefit of \u201cPrior Knowledge Learnt from the image domain\u201d -> I\u2019d like the authors to further clarify the exact experimental setting that has been performed here: \n- Are the experiments without image-pretraining simply trained on the tabular images?  \n- Or are they using a \u2018randomly initialized\u2019 backbone? \n- Are the image-pretrained methods further fine-tuned on some tabular image data? \n\nAll this information will help the reader to better judge to which extend information is potentially \u2018transferred\u2019, what might be the risk of overfitting, etc.;"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3231/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3231/Reviewer_NMSd",
                    "ICLR.cc/2024/Conference/Submission3231/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3231/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698631595629,
        "cdate": 1698631595629,
        "tmdate": 1700699650895,
        "mdate": 1700699650895,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lnogrsfHaK",
        "forum": "K8Mbkn9c4Q",
        "replyto": "K8Mbkn9c4Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3231/Reviewer_hBJa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3231/Reviewer_hBJa"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors delve into the domain of few-shot tabular representation learning by introducing a novel perspective\u2014treating tabular data as images. They introduce a method called TablEye, which begins by converting tabular data into the image domain and subsequently harnesses image-based representations to enhance performance in few-shot tabular learning tasks. Notably, the experimental results showcase TablEye's efficacy as it outperforms existing methods such as TabLLM and STUNT in these tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "One of the notable strengths of this paper is the novel idea of utilizing image domain priors for few-shot tabular learning. This approach capitalizes on the inherent structure and relationships within image data to address the challenges of tabular learning, demonstrating its effectiveness in transferring knowledge to few-shot scenarios."
            },
            "weaknesses": {
                "value": "While TablEye represents a promising approach, it is not without its limitations. One concern is the potential scalability issues that might arise when dealing with tabular data possessing a substantial number of features. The transformation of tabular data into an image format could lead to image dimensions that are impractically large, which may hinder the method's scalability and efficiency. Additionally, the authors acknowledge that for heterogeneous tabular data, establishing meaningful spatial relationships within the transformed images can be a daunting task. This limitation suggests that the proposed method may not be a universally applicable solution for all tabular learning problems, especially those with highly diverse data structures."
            },
            "questions": {
                "value": "The paper raises intriguing questions regarding the choice of feature extraction techniques. While the primary focus of the paper lies in feature extraction using Convolutional Neural Networks (CNNs), the authors mention the possibility of utilizing pre-trained Vision Transformers (ViT). It prompts further exploration of whether ViT could serve as a viable alternative to CNNs for this specific application. The underlying assumption that inductive bias plays a crucial role in the success of TablEye raises the question of whether ViT, with its distinct characteristics, would be as effective in leveraging this bias.\n\nFurthermore, the paper highlights the potential challenge of handling tabular data with an exceedingly large number of features. It is worth considering how a conventional CNN architecture, or even alternative methods, could adapt to accommodate such datasets while maintaining computational efficiency. This consideration adds an interesting dimension to the discussion about the method's scalability and practicality in real-world applications."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3231/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773005381,
        "cdate": 1698773005381,
        "tmdate": 1699636271355,
        "mdate": 1699636271355,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZInISvzd0e",
        "forum": "K8Mbkn9c4Q",
        "replyto": "K8Mbkn9c4Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3231/Reviewer_TNZH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3231/Reviewer_TNZH"
        ],
        "content": {
            "summary": {
                "value": "The paper presents TablEye, a novel framework for few-shot tabular learning. To overcome the limit of forming prior knowledge for tabular data, TablEye utilizes a two-stage process, transforming tabular data into tabular images and learning prior knowledge from labeled image data. The paper reports improved performance and applicability to medical datasets. Overall, I vote for accepting. TablEye introduces a novel approach to few-shot tabular learning, which is a relatively underexplored area in research. There are only several techniques for this task and they still have some constraints. The paper offers innovative solutions, using prior image knowledge through a few-shot learning method, and demonstrates clear performance improvements."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "TablEye addresses a challenging problem in few-shot tabular learning using a unique approach. Few-shot tabular learning is a relatively new and underserved area, and the paper contributes to this domain. TablEye introduces an innovative approach to few-shot tabular learning by bridging the gap between tabular and image data domains.\n\nThe paper provides evidence of TablEye\u2019s effectiveness through a series of experiments. TablEye consistently outperforms existing methods in multiple scenarios, including 1-shot and 4-shot learning. The use of a significantly smaller model size compared to alternatives and less constraints on datasets are also strong points.\n\nThe paper demonstrates the applicability of TablEye to real medical datasets, which implies its potential value in practical applications, especially in domains where accurate few-shot tabular learning is crucial."
            },
            "weaknesses": {
                "value": "While the paper presents positive results, it lacks detailed discussions regarding the differences in dataset performances. A more in-depth analysis of why certain structures perform better on specific datasets would provide a deeper understanding of TablEye's strengths. It seems that the improvement in performance is partly based on the variety of structures since none of them perform well in most of the experiments. \n\nThe experiments of T-M-C2, T-M-C3, T-M-C4 on comparison with TabLLM and in the context of medical results are lacking.\nLack of Detailed Implementation: The paper offers an overview of the framework but lacks detailed implementation specifics. It would be better if you could present the structure of classifiers and also some equations or pseudo-code for all the parts of the model, especially the domain transformation.\n\nAdditional Context for Few-Shot Learning: Providing a brief introduction to the general few-shot learning problem, its significance, and existing challenges would be beneficial for readers unfamiliar with the field."
            },
            "questions": {
                "value": "Could you please provide more details on the differences in performances across different datasets, particularly explaining why some structures perform better on specific datasets? This would help in understanding TablEye's strengths and limitations better. Could you explain why STUNT performs better in the dataset Karhunen? Similarly, why in some cases Conv2 is better than Conv4, for example, the datasets Optdigits and Karhunen? \n\nMoreover, Could you please provide experiments on T-M-C2, T-M-C3, T-M-C4 on comparison with TabLLM and in the context of medical results? Are there any recommendations or guidelines for selecting the most appropriate structure when using datasets?\n\nCould you offer more detailed implementation specifics? The figures, equations, or pseudo-code can help understand each part of the model better, especially the domain transformation, which was kind of hard to understand at first. \nFor the part about repeating the matrix, have you tried other methods like resizing or padding besides tilling when dealing with the matrix? Is tilling the best solution? \n\nAnd at last, it would be better if you could present briefly the structure of the classifiers you used."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3231/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698892473322,
        "cdate": 1698892473322,
        "tmdate": 1699636271281,
        "mdate": 1699636271281,
        "license": "CC BY 4.0",
        "version": 2
    }
]