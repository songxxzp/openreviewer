[
    {
        "id": "M5WE4JgmGW",
        "forum": "qoYogklIPz",
        "replyto": "qoYogklIPz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2991/Reviewer_viWv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2991/Reviewer_viWv"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a framework (ELM) for interpreting the vectors in domain-embedding spaces using text-only LLMs. The main idea is to project vector representations and textual tokens to a conjoint vector space. This is achieved by training adapter layers that serve as an interface to the language model. This adapter layers, together with the pre-trained model, create a new, extended model that maps both tokens and embeddings (from a arbitrary latent metric space) into a common space. The authors put this framework to the test by attempting to interpret two different semantic embeddings that they generate from the MovieLens dataset: One that models the users by their ratings of movies and one  that models the movies themselves based on their descriptions. The input data for the ELM framework then consists, for example, of text sequences in which the title of a movie has been replaced by the correspondingly generated semantic embedding. In order to verify the results of the framework, the authors rely on the expertise of test persons on the one  hand, and on the other hand, they also design two metrics to test for consistency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I think the strengths of the article lie in the idea of extending a model to accept sequential tokens and embeddings as input data. I agree with the authors that this approach has the potential to provide a tool to study corresponding vector spaces spanned by semantic representations. In addition, this offers interesting possibilities for applications such like recommendation systems. Based on the elaborate and convincing experiments, the authors can very well prove the merits of their approach. Another positive feature is the detailed appendix."
            },
            "weaknesses": {
                "value": "Unfortunately, in my opinion, the actual core of the article is somewhat overshadowed by the detailed experiments. I would also have liked to see a figure (in addition to Figure 2) that shows the structure of the framework at a glance. In my opinion, the surface planes shown in the 3D graphs of Figure 1 do not contribute much to the clarity either, since they only exemplify a distance of points in the vector space. The authors take effords to show the benefits of their framework with two examples (movie ratings - behavioral embeddings and movie descriptions - semantic embeddings), but perhaps one of the vector spaces studied could have come from a completely different domain, perhaps derived from a different dataset. But this probably falls into future work. On the whole, the paper is very well structured and easy to read, but the section on related work seems a bit out of place before the section conclusions. Perhaps it could have been placed further forward in the text."
            },
            "questions": {
                "value": "\u2022\tHas the framework been tested on other embeddings that may contain less semantic information than the domain embeddings used here (similar to the example in Appendix C)?\n\u2022\tHow do the authors assess the potential usefulness of their approach for the interpretation of other dense vector representations, for example graph or image embeddings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2991/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744371657,
        "cdate": 1698744371657,
        "tmdate": 1699636243925,
        "mdate": 1699636243925,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AIuOZzYByI",
        "forum": "qoYogklIPz",
        "replyto": "qoYogklIPz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2991/Reviewer_bEK6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2991/Reviewer_bEK6"
        ],
        "content": {
            "summary": {
                "value": "This paper presents the Embedding Language Model (ELM), a novel language model framework for interpreting domain-embedding spaces. By inserting trainable adapters into existing large language models (LLMs), ELM can accept domain embedding vectors as parts of its textual input sequence to allow the interpretation of continuous domain embeddings using natural language. Abundant experiments demonstrate ELM's proficiency in understanding, navigating, and manipulating complex embedding representations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Interpreting abstract embeddings into human-understandable natural language descriptions is intuitively appealing.\n- The proposed approach is simple and effective by reasonably leveraging the power of large language models (LLMs).\n- The authors have comprehensively assessed the quality of ELM's outputs using a variety of evaluation techniques, including qualitative human evaluations and specific consistency metrics."
            },
            "weaknesses": {
                "value": "Honestly, one of my greatest concerns is the practicality of the proposed framework, given that training an ELM requires manually constructing a batch of tasks, which need to be diverse enough to extract rich semantic information from the target domain $\\mathcal{W}$ to support the interpretation of the embeddings. Admittedly, the experimental part of the paper validates ELM's proficiency in interpreting two forms of embeddings on a movie-related dataset. However, I am not sure if ELM performs equally well in other more specialized domains, considering that the original training corpus of LLMs is likely to contain rich semantic information relevant to movies. In addition, constructing diverse task prompts may be tedious for a realistic user, so are the prompts presented in Appendix D basically applicable to other domains? After all, the most straightforward need for a user is to know what an embedding represents in general.\n\n- Given the prevalence of adapter tuning [1], there is nothing new to me in the method.\n\n1. Parameter-efficient transfer learning for NLP, Neil Houlsby et al."
            },
            "questions": {
                "value": "- What is the detailed form of the loss function $\\mathcal{L}\\_{\\theta}$ used in the experiment? Did the authors use reinforcement learning from AI feedback to optimize $\\mathcal{M}_{ELM}$?\n- Since the output of instances in the training data is generated by LLMs, can ELM reasonably interpret embeddings in specialized domains outside the scope of LLMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethical issues found."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2991/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2991/Reviewer_bEK6",
                    "ICLR.cc/2024/Conference/Submission2991/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2991/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698756910795,
        "cdate": 1698756910795,
        "tmdate": 1700627421657,
        "mdate": 1700627421657,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "poYmJZxiqK",
        "forum": "qoYogklIPz",
        "replyto": "qoYogklIPz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2991/Reviewer_pa1w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2991/Reviewer_pa1w"
        ],
        "content": {
            "summary": {
                "value": "This paper presents ELM, a framework that uses LLM to interpret embeddings. By training an adapter, ELM maps the domain embedding space to the LLM token embedding space, allowing users to use natural language to query and understand the domain embedding space. An evaluation of 24 original tasks with a movie rating dataset shows that ELM provides high semantic and behavioral consistency. Finally, the paper shows promising results of using ELM to query hypothetical embeddings and generalize concept activation vectors."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- [S1] The proposed method ELM is novel and intuitive. The two-stage training (adapter first, full model next) is interesting and makes sense.\n- [S2] I appreciate including human evaluation in section 3. The analysis and discussion on hypothetical embedding vectors and concept activation vectors (section 4) are very interesting.\n- [S3] The paper overall is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "## Major weaknesses\n\n- [W1] The evaluation can be improved. (1) The training data are all synthesized from an LLM. (2) The 24 tasks are all original. (3) ELM is only evaluated on the MovieLens. (4) There is no baseline method. It is unclear how well ELM will perform on real data and tasks compared to other methods.\n\n## Minor weaknesses\n\n- [M1] It would be great to explain $E_A$ in Figure 2 (the definition is on page 4).\n- [M2] The 24 tasks are essential to understand this paper. I recommend adding a table to provide a high-level description of these 24 tasks in the main paper.\n- [M3] The ELM method is quite intuitive, but the writing in section 2 is overly-complex. It would be helpful to have a small glossary to explain all notations.\n- [M4] It is hard to make sense of Table 1. Are these numbers good?"
            },
            "questions": {
                "value": "- [Q1] Have you tried ELM on other datasets and tasks other than movie reviews?\n- [Q2] Figure 1 is a bit hard to understand. For example, how do people generate the embedding for the animated version of Forrest Gump? Interpolating the embedding of Forrest Gump with an animated movie? Is the black text output from an LLM? Who writes the blue text (e.g., users, researchers)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2991/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698767811978,
        "cdate": 1698767811978,
        "tmdate": 1699636243772,
        "mdate": 1699636243772,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "V7AuandHis",
        "forum": "qoYogklIPz",
        "replyto": "qoYogklIPz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2991/Reviewer_BRjZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2991/Reviewer_BRjZ"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a paradigm for demystifying the compressed embedding representations of deep models (such as MF trained on the MovieLens dataset in the paper) with Large Language Models (ELM). ELM first generates training data by prompting PaML 2-L with the movie title and task-specific information. The training procedure is composed of two stages: (1) training an adapter to project the domain embedding; and (2) full-training the full model including the language model and the adapter. Then the authors construct different tasks on the MovieLens dataset to test the performance of demystifying domain embeddings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It is sound to demystify domain embedding with Large Language Models.\n2. The authors introduce their approach step-by-step in Sec 2 & 3, which is clear. \n3. The performance is evaluated by human raters."
            },
            "weaknesses": {
                "value": "1. The training data is generated by PaML 2-L, and ELM uses PaLM 2-XS to interpret the domain embeddings. Their similar architectures and training procedures may make the contribution limited.\n2. The authors argue that ELM is a general framework for different tasks, but the experiments only involve one model (MF) on one dataset (MovieLens). Therefore, the generalization of ELM to other models and datasets is not guaranteed.\n3. Some related works are overlooked. Existing works [1][2] have shown that tuning the adapter can let LLM demystify and reason over the embeddings of images. The authors should provide more explanations and experiments for their differences. \n\nreference:\n* [1] Zhu, Deyao, et al. \"Minigpt-4: Enhancing vision-language understanding with advanced large language models.\" arXiv:2304.10592 (2023).\n* [2] Koh, Jing Yu, et al. \"Generating images with multimodal language models.\" arXiv: 2305.17216."
            },
            "questions": {
                "value": "1. Can ELM perform well with training data generated by other LLMs such as ChatGPT?\n2. Can the training data be real other than generated? Some datasets contain descriptions and revives (such as Amazon datasets). Why not use these kinds of datasets?\n3. Can ELM perform well with other models and other datasets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2991/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699318051494,
        "cdate": 1699318051494,
        "tmdate": 1699636243705,
        "mdate": 1699636243705,
        "license": "CC BY 4.0",
        "version": 2
    }
]