[
    {
        "id": "9WUPA5KyTl",
        "forum": "O1cLOzgi81",
        "replyto": "O1cLOzgi81",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission220/Reviewer_jvA8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission220/Reviewer_jvA8"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on the domain generalization problem of person reID. It proposes a mixture of domain adapters to perform parameter-efficient DG reID. The authors argue that the proposed method is parameter-efficient."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "None"
            },
            "weaknesses": {
                "value": "1. The proposed method lacks novelty. As the author said, the proposed method is an integration of CLIP and adapter. The author just uses some hot techniques in the reid domain. I do not see any insight beyond CLIP and Adapter. I do not think this kind of paper can be accepted in any way.\n2. The parameter-efficient setting may be meaningless in the DG setting; The number of tunable parameters is not a main concern in the DG reid domain. The author argue that, in the DG setting, the number of parameters of classifier heads is very large. As seen in Table 4, the sum of IDs is still very small. I have trained with 100,000 IDs, no problem exists. Also, I am not sure why you focus on this strange aspect. Fewer parameters do not mean a higher training speed and less VRAM used. I do not think the setting and the intuition of this paper is reasonable.\n3. The writing is too poor; The writing of the whole paper is very poor. Also there are some typos: \u201cblcok-aware \u2014> block-aware\u201d. A thorough proofreading is needed. \n4. Not good experiments; in Table 6, your experimental results are much lower than others. I see the tunable parameters are small. But I do not think it is meaningful. Also, have you compared yours with the prompting methods?"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission220/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission220/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission220/Reviewer_jvA8"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission220/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697906807344,
        "cdate": 1697906807344,
        "tmdate": 1699635947920,
        "mdate": 1699635947920,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7f0w3Pjc08",
        "forum": "O1cLOzgi81",
        "replyto": "O1cLOzgi81",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission220/Reviewer_DXS9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission220/Reviewer_DXS9"
        ],
        "content": {
            "summary": {
                "value": "In this work authors present a method that can handle the domain generalizable person re-identification. Inspired by the previous works in this area, authors designed a network in which they leverage the CLIP Re-ID model with a set of domain specific adapters as well as a domain generalizable adapter termed as Mixture of domain adapters (MoDA) that can efficiently scale up to larger backbones models. Another advantage of this approach is that the expert opinions are mixed even at intermediate stages so that the model can leverage finer information mixing."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed approach is reasonable and clearly outlined. Despite sharing similarities with previous works, authors managed to design expert mixing strategy that results in a more parameter efficient architecture when scaling to larger backbone models for variety of source domains. Additionally, the proposed method achieves competitive results to that of state-of-the-art models when tested in equal environment under the same evaluation protocols."
            },
            "weaknesses": {
                "value": "There is no detailed discussion on the obtained results beside merely stating the method achieves better results. It is expected that some insight to be offered as why to the proposed method performs better or worse compared to certain methods. While it is important to know the method has a better accuracy, it is equally as important to know why and how that improvement occurs. Additionally, would be possible for authors to provide some failure cases and some future direction that the designed framework can be enhanced?"
            },
            "questions": {
                "value": "Could authors provide some explanation on why the proposed method performs slightly better than META under protocol one but META performs better by a good margin under protocol 2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission220/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698794341394,
        "cdate": 1698794341394,
        "tmdate": 1699635947832,
        "mdate": 1699635947832,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BQ6CN1G9QI",
        "forum": "O1cLOzgi81",
        "replyto": "O1cLOzgi81",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission220/Reviewer_QbiD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission220/Reviewer_QbiD"
        ],
        "content": {
            "summary": {
                "value": "This study focuses on Domain Generalizable ReID (DG ReID) and introduces a new method called Mixture of Domain Adapters (MoDA) to overcome the problem that the model parameters are too large. The proposed MoDA method incorporates Global-Adapter, Export Adapter, Voting Network, and the CLIP technique for DG ReID, resulting in competitive results compared to state-of-the-art methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1)\tThe authors exploit the Global-Adapter, Export Adapter, and Voting Network for DG ReID task."
            },
            "weaknesses": {
                "value": "1)\tIn Tables 1 and Table 2, the bolded font is misleading, and the performance of the ACL and META methods outperforms the proposed method. In particular, the model parameter of ACL is not much larger than that of the proposed method, but the performance of ACL is much higher than that of this paper. \n2)\tAs shown in Table 3, the Global-Adapter, Export-Adapter, and Voting Network proposed in this paper did not bring significant performance improvement, which makes me doubt the effectiveness of the method proposed in this paper. \n3)\tAbbreviations that appear for the first time need to indicate the full name. For example, in the third line of the abstract, the DG ReID appears for the first time without giving a full description. \n4)\tThe GELU in Figure 2 is not given a full description, as well as in the text, which can confuse the reader. \n5)\tThe use of mathematical symbols is not rigorous enough. The i in Formula 1 represents both the image and the index, which is easy to cause ambiguity. \n6)\tIn Formula 3a, the definition of MA is not stated. \n7)\tIn Formula 5, the definition of d_p and d_a are not stated. \n8)\tThe font size in all the figures is too small. \n9)\tEnglish writing needs improvement. \n10)\tSome grammatical errors. In the top line of section 3.2, \u201cdelineates\u201d -> \u201cdelineate\u201d"
            },
            "questions": {
                "value": "Why does this paper not directly use images as the input to the model, but increase the optimization operation of text tokens? The implication of text tokens needs to give a more detailed analysis and experimental demonstration."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission220/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698890916368,
        "cdate": 1698890916368,
        "tmdate": 1699635947745,
        "mdate": 1699635947745,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qi72aJZ7DW",
        "forum": "O1cLOzgi81",
        "replyto": "O1cLOzgi81",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission220/Reviewer_GQT9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission220/Reviewer_GQT9"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a Mixture of Domain Adapters (MoDA) framework for DG ReID, which imports CLIP and adapter to achieve the parameter efficiency during the scaling up of MOE."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Although CLIP and adapter are not new, the voting network using of the CLIP-trained ID-specific tokens to save the inference time is a reasonable and interesting idea."
            },
            "weaknesses": {
                "value": "1. The improvements in Table.1/2 is slight. The performance is even lower than SOTA. As the advantage of the proposed MoDA is based on the tunable params. I suggest the author to provide a comparison with the same level of tunable params, to see whether the performance can beat other SOTAs.\n2. In Eq.11, I do not get the meaning of the \u0393+agg and \u0393+expert. They are two hardest positive samples, NOT two distances as described, right?\n3. What is the difference between \"Mixture of Expert Adapters\" and \"w/o Global Adapter in Mixture of Experts and Global\". Moreover, in my opinion, the \"Mixture of Expert Adapters\" contains no global adapter, what's the meaning of \"w/o Global Adapter\" in \"Mixture of Expert Adapters\"?\n4. Where is Table.5 in Section4.4?\n5. Many mistakes in the writing, such as \"The expter branch\" in Fig.3 title."
            },
            "questions": {
                "value": "I don't like the writing of this paper, but the idea is reasonable. So I prefer to borderline."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission220/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698906872803,
        "cdate": 1698906872803,
        "tmdate": 1699635947667,
        "mdate": 1699635947667,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QEWmOPIQCy",
        "forum": "O1cLOzgi81",
        "replyto": "O1cLOzgi81",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission220/Reviewer_ZKXT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission220/Reviewer_ZKXT"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a novel MoE-based Domain Generalizable ReID method called \"Mixture of Domain Adapters\" (MoDA) to address the challenges faced by existing DG ReID methods. MoDA leverages Adapter-tuning and CLIP in a parameter-efficient manner to mitigate the need for extensive fine-tuning of backbone, classifier head, and expert parameters. Their approach aims to handle the large number of person IDs typically encountered in DG ReID, allowing for better scalability to larger vision models. Many experiments demonstrate that MoDA achieves competitive or even superior results compared to state-of-the-art methods while requiring significantly fewer tunable parameters."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors of this paper have frozen the weights of CLIP and integrated the \"Mixture of Domain Adapters\" (MoDA) module for fine-tuning. They applied this technique to ReID and achieved promising experimental results. The performance seems good."
            },
            "weaknesses": {
                "value": "This paper encompasses several technologies, including CLIP, PEFT (Adapter), MoE, etc., but none of these were introduced by the authors themselves. Instead, existing technologies were adapted for use in the Re-ID domain. The authors claim, \"To the best of our knowledge, our work is the first one to exploit CLIP to DG Person ReID and also the first one to attempt PEFT methods, such as adapter, for DG Person ReID.\" However, CLIP-ReID [1] and Clip-adapter [2] have already explored these aspects. Furthermore, AdaMix [3] and MixDA [4] have also applied MoE adapter techniques. Utilizing these technologies in the Re-ID field does not necessarily constitute a novel contribution or innovation. It is hoped that the authors can introduce more of their own design elements and emphasize improvements upon prior work.\n\nPerhaps visualizing the relationship between MoE adapters and specific domain or person characteristics could be beneficial. For instance, in Re-ID tasks, conducting a visual analysis to understand when and under what circumstances certain adapters are activated and whether any patterns emerge could provide valuable insights.\n\nIn terms of writing, the paper reads more like a technical report combining elements MoE, CLIP, and Adapter based PEFT without a clear focus or coherence. It is suggested to start with a specific problem within DG Re-ID as the motivation, introduce the proposed solution, and emphasize the uniqueness of this approach. The method should not merely be a simple combination of existing techniques, and authors should highlight the distinctive contributions. \n\nThere is room for improvement in the style of the figures and tables as well.\n\n[1] Li Siyuan, Li Sun, and Qingli Li. CLIP-ReID: exploiting vision-language model for image re-identification without concrete text labels. AAAI. 2023.\n[2] Gao P., Geng S., Zhang R., Ma, T., Fang R., Zhang Y., Li H., and Qiao Y. Clip-adapter: Better vision-language models with feature adapters. arXiv preprint arXiv:2110.04544.\n[3] Wang, Yaqing, et al. Adamix: Mixture-of-adapter for parameter-efficient tuning of large language models. arXiv preprint arXiv:2205.12410.\n[4] Diao, Shizhe, et al. Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models Memories. arXiv preprint arXiv:2306.05406."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission220/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699324805024,
        "cdate": 1699324805024,
        "tmdate": 1699635947600,
        "mdate": 1699635947600,
        "license": "CC BY 4.0",
        "version": 2
    }
]