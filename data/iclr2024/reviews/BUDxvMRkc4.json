[
    {
        "id": "Nryvk99hTD",
        "forum": "BUDxvMRkc4",
        "replyto": "BUDxvMRkc4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1398/Reviewer_pMKo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1398/Reviewer_pMKo"
        ],
        "content": {
            "summary": {
                "value": "This study introduces a framework designed to enhance CLIP in addressing long-tailed visual recognition challenges. This framework integrates a supervised contrastive loss mechanism, grounded on the transport plan, to fortify visual feature extraction. Several evaluations conducted on benchmarks corroborate that this proposed method significantly facilitates discriminative visual feature learning and achieves SOTA performance in long-tailed recognition tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The idea of this paper is clear and easy to follow.\n1. Experimental results show the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. Lack of innovation. The approach in this paper provides a more balanced prototype for visual pre-training models to guide the learning of visual feature extractors and designs supervised comparative learning loss. However a similar approach has appeared in previous long-tail methods [1]. The differences in this paper are: 1. A more robust pre-training model, CLIP, is utilized. 2. A text-based prototype design approach is used to replace the target anchor. These innovations are more limited.\n\n2. Some modules are without good motivation.\nFor example:\n- Why do we need a learnable linear classifier? The purpose of its existence seems to be the matching of visual features with textual features. However, the weights of the classifier will change during training, which does not narrow the gap between template label text and image feature distributions.\n-  In the unsupervised prototype-guided feature learning part, why choose cos similarity as the distance metric instead of other metric methods such as minimum entropy?\n-  For the modules of learnable classifier, unsupervised prototype-guided feature learning, and supervised contrastive loss, it seems to be an incremental improvement and not complementary, so why use them to train models together?\n\n**Reference**\n[1] Li T, Cao P, Yuan Y, et al. Targeted supervised contrastive learning for long-tailed recognition[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 6918-6928."
            },
            "questions": {
                "value": "please refer to Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1398/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772634930,
        "cdate": 1698772634930,
        "tmdate": 1699636067587,
        "mdate": 1699636067587,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zaEftd5y5f",
        "forum": "BUDxvMRkc4",
        "replyto": "BUDxvMRkc4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1398/Reviewer_nBTe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1398/Reviewer_nBTe"
        ],
        "content": {
            "summary": {
                "value": "This study discovers that the fine-tuned CLIP's textual features are more balanced and discriminative compared to its visual counterparts. Building on this, the research proposes utilizing balanced textual features as prototypes to guide the learning of robust representations for biased visual features. The CLIP is further fine-tuned through contrastive learning, followed by the optimization of biased visual representations using linear adapters and the introduction of optimal transport distance to help decouple biased visual features. Additionally, a supervised contrastive learning loss based on the transport plan is designed. Experimental results indicate that the approach excels in leveraging visual-language information for imbalanced visual recognition, achieving state-of-the-art performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Extensive experiments on ImageNet-LT, Places-LT, and iNaturalist 2018 have demonstrated the effectiveness of the proposed method.\n2. Comprehensive visualizations and ablation studies were conducted to validate the impact of the proposed method."
            },
            "weaknesses": {
                "value": "1. The experiment results indicate that the method underperforms for \u201cmany\u201d classes in long-tail data.\n2. The proposed method employs a two-stage training process and fine-tunes the Full-CLIP, which requires significant computational resources and has a prolonged training duration.\n3. The proposed method doesn't seem to have a specific design tailored for long-tail data. The approach of using textual features as guidance for better image features can be applied to situations with limited image feature quality for various reasons, such as long-tail, few-shot, noisy data, generated data, low-resolution data, and so forth."
            },
            "questions": {
                "value": "1. Why does the proposed method underperform in \u201cmany\u201d classes of LT dataset? An analysis of the underlying reasons would be appreciated.\n2. The experimental results show that the proposed method underperforms in \u201cmany\u201d classes of LT dataset. Does this imply that the method is primarily effective for situations with the few-shot scenario (i.e., \u201cmedium\u201d and \u201cfew\u201d classes in long-tail datasets)?\n3. Balanced sampling is a fundamental operation in long-tail methods. Why is random sampling used when fine-tuning the CLIP in the initial stage? Is it to intentionally obtain a CLIP encoder with strong biases caused by imbalance?\n4. Reference [1] also leverages CLIP's text features to enhance the discriminative power of image features. In [1], directly using text features and image features in concurrent training a linear classifier can achieve significant improvements in few-shot tasks. However, compared to the method in this paper, the method in [1] is much simpler, with much faster computation speeds and far less computational overhead.\n[1] https://arxiv.org/abs/2301.06267"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1398/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1398/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1398/Reviewer_nBTe"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1398/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698774062941,
        "cdate": 1698774062941,
        "tmdate": 1699636067513,
        "mdate": 1699636067513,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "coBpu7Tn71",
        "forum": "BUDxvMRkc4",
        "replyto": "BUDxvMRkc4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1398/Reviewer_V37f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1398/Reviewer_V37f"
        ],
        "content": {
            "summary": {
                "value": "After the advent of vision-language pre-training, numerous works have adapted the pre-trained vision-language model to various vision tasks, including long-tailed recognition. This paper first presents empirical evidence that textual features remain balanced even after fine-tuning in the context of long-tailed classification. Based on this, the authors propose a framework that leverages balanced textual features as a guide to obtain more robust visual features."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The empirical finding that, during the fine-tuning of the entire vision-language pre-trained model on long-tailed data, textual features tend to achieve balance is quite intriguing. This paper goes beyond this observation and contributes to the community by proposing a concrete methodology that leverages balanced textual features to rectify imbalanced visual features.\n2. The thorough ablation study conducted on the elements comprising \"Phase B,\" proposed in this work, effectively underscores that the suggested $L_{\\text{OT}}$ and $L_{\\text{SCT}}$ indeed enhance performance."
            },
            "weaknesses": {
                "value": "1. The overall structure of this paper, which deals with challenges in contrastive learning methods due to class imbalance and suggests remedies, evokes thoughts of Suh and Seo (2023). Nevertheless, the current paper does not include any discourse on this topic.\n2. Moreover, while one could mention Kang et al. (2021) as a seminal work on achieving a balanced and discriminative feature space in long-tailed classification scenarios, this is also not discussed.\n\n---\nKang et al., 2021, Exploring Balanced Feature Spaces for Representation Learning.  \nSuh and Seo, 2023, Long-Tailed Recognition by Mutual Information Maximization between Latent Features and Ground-Truth Labels."
            },
            "questions": {
                "value": "1. Could you offer some informed speculation about why there is a tendency for textual features to be balanced?\n2. Since comparing performance between different architectures does not hold much significance, it would be better to provide results for RN50 and ViT-B/16 in separate groups.\n3. Does the proposed approach result in any additional training expenses? For instance, what are the costs associated with setting up an optimal transport plan?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1398/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1398/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1398/Reviewer_V37f"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1398/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823633079,
        "cdate": 1698823633079,
        "tmdate": 1700329326204,
        "mdate": 1700329326204,
        "license": "CC BY 4.0",
        "version": 2
    }
]