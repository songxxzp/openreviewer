[
    {
        "id": "yAfoPoS79f",
        "forum": "ynguffsGfa",
        "replyto": "ynguffsGfa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7282/Reviewer_4AgE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7282/Reviewer_4AgE"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the problem of improving ML performance on ultra small tabular datasets via synthetic data augmentation. The authors introduced a method that (1) generates new data by using the small training set as the context and querying an LLM, and then (2) further filters the synthesized data by looking at the learning dynamics of the synthetic data. The authors compare the proposed method to existing methods of data synthesis and augmentation, and show the superiority of the proposed method in the ultra-low data regime (n < 100)."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The topic on data synthesis for ultra-small dataset is interesting and has far-reaching consequences.\n- The concepts touched upon along the way, specifically data prior and data quality flags, offer a framework to think about this problem.\n- The writing is clear, and the paper is well organized."
            },
            "weaknesses": {
                "value": "- The effect and origin of the LLM\u2019s prior are unclear, so it is difficult to confidently attribute the performance gain to in-context learning.\n- The curation method seems to be model-dependent and may intensify the weaknesses of the ML model.\n- Because the method proposed is quite simple to use, I feel we need to know when the methods will break down in order to understand its pitfalls. There is not really anything on that in the paper.\n\n**On the prior:**\nSection 2.1 suggests that the LLM has a strong prior, as it is able to extrapolate synthesis to regions without any training data. The question that immediately follows is whether the in-context learning is flexible enough to learn the nuances in the ultra-small dataset, or will the LLM\u2019s prior overwhelm those nuances? Note that Table 3 shows that performance does not improve much as n increases for LLM models relative to\u00a0 non-LLM models. This observation is consistent with the prior being too strong. Of course, it is also consistent with the in-context learning being very good. I hope the authors could tease these two scenarios apart.\n\nAnother question relates to the scenario of \u201ctruly\u201d scarce data, i.e., data that is really not in the training data of the LLM. Although the authors mention that 4 of the datasets are unlikely to be in the training of the LLM due to their being closed-source, we will need a way to quantify that likelihood to get at the answer. In fact, Table 3 shows that for n=20, most of the non-LLM\u00a0 Uncur methods have worse utility than D_train, whereas most of the LLM Uncur methods have higher utility than D_train. This observation is consistent with LLM having memory of the data. Of course, this is also consistent with good in-context few-shot learning. It will be important to distinguish between memorization and in-context few-shot learning for the truly scarce scenario.\u00a0\n\nFor both questions above, one idea to get at them is to create datasets that are definitely not in the training of the LLM by manually manipulating the column dependencies and marginal distributions of existing data, then see if the results still hold.\n\n**On the curation:**\nIt seems that the curation depends on the model via the f_e(x) (see equations 1 and 2), therefore, the augmentation is optimized for a model and can make the model\u2019s specific inductive biases more pronounced. Some thoughts related to addressing this include: (1) curating based on a ML model and evaluating on many models, (2) include more metrics such as resemblance. I hope the authors could provide a clear rationale for addressing this issue.\n\nThe selection criteria based on high aleatoric uncertainty makes it similar to active learning. This means that the curation will make the model perform better even from random sampling, as evidenced by curation increasing performance for all the methods. It may be worth it to make this connection.\u00a0\n\nLooking at the two criteria, do they not have a 1-to-1 relationship, as [f_e(x)]_y is the only variable in both quantities? That is, are the two criteria equivalent to selecting a range of confidence or a range of uncertainty? Maybe worth clarifying.\n\n**On pitfalls:**\nAs mentioned, demonstration of pitfalls are extra important because the method proposed is very easy to use. Such demonstration will likely also shed light on the previous questions."
            },
            "questions": {
                "value": "**Questions and comments:**\n- Does the LLM handle continuous variables well?\n- Figure 2: it\u2019s hard to read with solid markers occluding each other.\n- Table 1: what quantity is the performance number?\n- Figure 3: surprisingly smooth, and the dip around 6 samples stands out. What\u2019s the explanation? What is the actual performance after augmentation \u2014 want to check if the gain is substantial?\u00a0\n- Don\u2019t think ICL is ever introduced as an acronym."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The method proposed is very easy to apply, and the paper is framed in terms of using it in fields like medicine. Without a good understanding of the adverse effects of the method, it may unknowingly cause problems."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7282/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7282/Reviewer_4AgE",
                    "ICLR.cc/2024/Conference/Submission7282/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7282/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698253280849,
        "cdate": 1698253280849,
        "tmdate": 1700621480020,
        "mdate": 1700621480020,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JwWB7I4ygP",
        "forum": "ynguffsGfa",
        "replyto": "ynguffsGfa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7282/Reviewer_Nkcr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7282/Reviewer_Nkcr"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the problem of using LLMs to generate tabluar data for medical applications where labeled data are extremely scarse. Specifically, this work considers the \"ultra-low resource\" scenario where the task data is <100 samples. This work proposes to use these samples as example with context information to prompt LLMs to generate synthetic data. The work claims the model is able to leverage its medical and other knowledge to generate usable tabular data. Then, the generated data goes through a data curation process, where the data is examined at multiple checkpoint during the model training process. By throwing out data that is inconsistent to the model or nearby samples, the work shows that the curated synthetic data can help improve model performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The problem being investigated is clearly of interest. Data generation for low-resource scenarios is highly relevant and could be particularly helpful for disadvantaged or marginal groups or rare scenarios. Data generation and curation are also crucial for improving the fairness of ML/AI systems. The potential of foundation models is especially promising.\n\nThe language is very clear. The paper is beautifully formatted. It is a comfortable read. The paper is well-motivated and well-contextualized.\n\nThe research need is valid and important and the proposed approach is relevant and timely. The idea of post-generation curation is smart. Related work is well introduced and comparisons are thorough"
            },
            "weaknesses": {
                "value": "My major concerns are\n\n1. I don't see the technical contribution of this work;\n\n- This essentially comes down to the few-shot generation problem, where there is much more existing work beyond tabular data. If it is a traditional NLP task such as generating data for sentiment analysis, there are already abundant success cases. Whether the data supports training a model depends on what model you are training and what you want to achieve.\n\n- With less than 100 data but 30 attributes, it is impossible for LLMs to effectively infer the pattern of data. And I doubt the generation is trivial. I wonder how the proposed generation process compares to just adding random noise to augment existing samples before curation. D_train in many cases is very high, while the baseline performance is significantly worse. This raises doubt on whether the setup is correct. I suspect using small random noise to augment the dataset would perform better than many baselines.\n\n2. The proposed approach has a significant overlook for the risk of biases and untrustworthiness of LLMs. The proposed methods for medical applications pose major ethical concerns.\n\n- I strongly doubt the validity of this approach in medical applications. Specifically, what is the rationale for the generated data to be considered \"correct\" or incorporate the right knowledge or information? **If the LLM's output suggests a high correlation between certain marginal groups and a high prevalence of sexually transmitted diseases (STDs), how do you tell whether this is based on medical publications or social biases? Factual tracing for LLMs is currently known as a very hard problem. Active research is ongoing and there are not yet effective ways to relate the model's output to its training samples.**  Without due effort investigating this issue, using data generated by black-box models as the foundation for building medical applications is irresponsible and poses ethical concerns. Given that the work targets ultra-low resource scenarios, it is especially alerting to associate the risk with historically disadvantaged or marginal groups.\n\n- Without specific treatments, LLMs are generally quite poor in logical/mathematical reasoning. It is often challenging for these models to identify the simplest patterns in input data, which is consistent with the reported no-context generation scenario. For the generation example provided in the appendix, I found it rather concerning. \n\n- GPT models are trained overwhelmingly on web text, which contains a high level of subjective arguments, biases, and ungrounded claims especially for major social topics such as COVID or COVID patients. It could easily incorporate bias between demographic attributes and medical conditions and outcomes. The most concerning part is I don't see discussions on it at all, which makes me worry that the authors may not be fully aware of the tool they are leveraging. Given the seriousness of medical applications, this level of overlook is worrisome."
            },
            "questions": {
                "value": "- There are existing notions such as \"low-resource\" refers to less than 5k annotated samples and \"strict few-shot\" refers to <=16 labeled samples per class. I'm not aware of the definition of \"ultra-low-resource\". Based on the illustration of this paper, it seems to be < 100 samples. The title suggests ultra-low, but without a definition for it, the abstract and introduction talk about \"low-resources\". **What is ultra-low? Is this an existing notion or is it proposed by this work?** This range of samples seems to be the case that is often considered \"few-shot\".\n\n- To have diversity in the generated samples, I would expect the researchers to look into decoding strategies (such as temperature or sampling). It usually does not work by just \"asking the model to generate diverse samples\".\n\n- Table 3, in \"adult\" row in the first section, the best performer are not marked.\n\n- It is better to provide average performance (with standard deviation) for each method for easy comparison between methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The proposed approach has a significant overlook for the risk of biases and untrustworthiness of LLMs. The proposed methods for medical applications pose major ethical concerns.\n\nI strongly doubt the validity of this approach in medical applications. Specifically, what is the rationale for the generated data to be considered \"correct\" or incorporate the right knowledge or information? **If the LLM's output suggests a high correlation between certain marginal groups and a high prevalence of sexually transmitted diseases (STDs), how do you tell whether this is based on medical publications or social biases? Factual tracing for LLMs is currently known as a very hard problem. Active research is ongoing and there are not yet effective ways to relate the model's output to its training samples.**  Without due effort investigating this issue, using data generated by black-box models as the foundation for building medical applications is irresponsible and poses ethical concerns. Given that the work targets ultra-low resource scenarios, it is especially alerting to associate the risk with historically disadvantaged or marginal groups.\n\nWithout specific treatments, LLMs are generally quite poor in logical/mathematical reasoning. It is often challenging for these models to identify the simplest patterns in input data, which is consistent with the reported no-context generation scenario. For the generation example provided in the appendix, I found it rather concerning. \n\nGPT models are trained overwhelmingly on web text, which contains a high level of subjective arguments, biases, and ungrounded claims especially for major social topics such as COVID or COVID patients. It could easily incorporate bias between demographic attributes and medical conditions and outcomes. The most concerning part is I don't see discussions on it at all, which makes me worry that the authors may not be fully aware of the tool they are leveraging. Given the seriousness of medical applications, this level of overlook is worrisome."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7282/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7282/Reviewer_Nkcr",
                    "ICLR.cc/2024/Conference/Submission7282/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7282/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782273726,
        "cdate": 1698782273726,
        "tmdate": 1700721844782,
        "mdate": 1700721844782,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1MVyRcXPQv",
        "forum": "ynguffsGfa",
        "replyto": "ynguffsGfa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7282/Reviewer_AZSZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7282/Reviewer_AZSZ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to use strong LLMs such as GPT-4 for tabular data augmentation and generation. The paper also proposes a generated data curation technique based on predictive confidence and aleatoric uncertainty metrics. The paper shows that the resulting method CLLM is able to effectively leverage prior knowledge of LLMs and achieves good experimental performance in low-data regimes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strengths:\n* The paper proposes an intuitive and well-motivated approach to leverage background knowledge of GPT-4 for informed tabular data generation\n* The experimental results are promising and cover a representative set of baseline tabular data generation methods\n* The idea of generated data curation is very interesting. Discarding samples with both low predictive confidence and data uncertainty improves performance both of the GPT-4-based data generation and of other tabular data generation model\n* The additional insight into GPT-4\u2019s ability to extrapolate to unseen regions of the manifold is interesting, even though the evidence is somewhat anecdotal and based on a TSNE plots.\n* Importantly, the paper shows that LLM-based tabular data generation is most helpful for generation of underrepresented samples which traditionally is a challenge for other tabular data generation methods. \n* The presentation is excellent."
            },
            "weaknesses": {
                "value": "Weaknesses and Questions:\n* The paper only tests generation with GPT-4. It would be interesting to see if the findings in the paper are consistent across LLMs, for example, including Claude 2 or LLAMAv2 would be helpful.\n* Could you provide an explanation for GPT-4 generated data outperforming D_oracle in the Train-on-synthetic-Test-on-real setting in Table 3?\n* What is your dataset selection logic? It would be helpful to include more datasets from the papers of other tabular data generation baselines.\n* Do the performance gains come solely from the background knowledge of GPT-4 or is GPT-4 also able to build a strong data model? Have you tested CLLM on any datasets where the background knowledge of GPT-4 would not be as useful, for example, on datasets with anonymized features? Such an experiment would help further understand the source of performance gains.\n* Could you explain the utility drop for GPT-4-no-context on 200 samples in Table 2? Why would more data lead to degraded performance? Could this be simply caused by randomness in the results?\n* If the results in Table 2 are indeed volatile, it would be very helpful to include error bars and bold all statistically significant winners. The error bars could be constructed by, for example, running the simulation for multiple seeds or resampling D_train. Right now, for example Recall of 0.89 is bolded for GPT-4 w/context for 40 samples and not bolded for GPT-no-context for 40 samples. This makes the results in Table 2 unconvincing in their current form. Although it is reasonable to hypothesize that including the background information about a dataset is helpful, as I mentioned above, a better validation of that would be helpful.\n* Related, which dataset are the results in Table 2 based on? Are there similar results for other datasets? What about the dataset behind Figure 3 and Table 1?\n* Related work is currently limited and would benefit from including other related papers. While a few examples of tabular data generation methods are included and used as baselines, at least citing other prominent tabular data generation methods such as STaSy[1] would be helpful. Additionally, even though the introduction mentions that the low-data problem is undervalued, a few tabular deep learning works in fact address extreme low-data regimes, some of them also include experiments in the medical domain [2,3,4,5]. For example, [2,3] are tabular transfer learning approaches with experiments in extreme low-data regimes in the medical domain, while [4] and [5] are knowledge-graph-augmented tabular approaches enabling performance improvements in low-data regimes of wide and short datasets. The idea of leveraging a knowledge graph is similar to the idea of using an LLM in that they both rely on prior knowledge. It would definitely be helpful to cite these works, including them in experiments might be tricky because of the knowledge graph construction.\n\n\nReferences:\n\n[1] STaSy: Score-based Tabular data Synthesis, ICLR 2023 (https://openreview.net/forum?id=1mNssCWt_v)\n\n[2] Levin, R., Cherepanova, V., Schwarzschild, A., Bansal, A., Bruss, C.B., Goldstein, T., Wilson, A.G. and Goldblum, M., 2022. Transfer learning with deep tabular models. arXiv preprint arXiv:2206.15306.\n\n[3] Benchmarking Tabular Representation Models in Transfer Learning Settings, NeurIPS 2023 Tabular Representation Learning Workshop (https://openreview.net/forum?id=HtdZSf1ObU)\n\n[4] Margeloiu, A., Simidjievski, N., Lio, P. and Jamnik, M., 2022. Graph-Conditioned MLP for High-Dimensional Tabular Biomedical Data. arXiv preprint arXiv:2211.06302.\n\n[5] Ruiz, C., Ren, H., Huang, K. and Leskovec, J., 2023. Enabling tabular deep learning when $ d\\gg n $ with an auxiliary knowledge graph. arXiv preprint arXiv:2306.04766."
            },
            "questions": {
                "value": "Please, see the weaknesses section for both weaknesses and questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no ethics concerns"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7282/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7282/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7282/Reviewer_AZSZ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7282/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827845928,
        "cdate": 1698827845928,
        "tmdate": 1700524157082,
        "mdate": 1700524157082,
        "license": "CC BY 4.0",
        "version": 2
    }
]