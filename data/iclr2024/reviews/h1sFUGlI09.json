[
    {
        "id": "wGmSu47wTW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission272/Reviewer_KRMX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission272/Reviewer_KRMX"
        ],
        "forum": "h1sFUGlI09",
        "replyto": "h1sFUGlI09",
        "content": {
            "summary": {
                "value": "This paper presents an RGB-D scene understanding framework with RGB-D pertaining weights. Two tasks are considered, including RGB-D semantic segmentation and salient object detection. A global awareness attention module and a local enhancement attention module are designed. RGB-D pre-training is performed on ImageNet-1K with estimated depth data. The proposed model achieves state-of-the-art performance and maintains good efficiency compared to existing works. As shown in Table 3, the benefit of using RGB-D pre-training is significant. Extensive ablation studies and parameter studies are conducted."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work is one of the first works to consider RGB-D pertaining to enhance RGB-D scene understanding. The results show that the benefit of RGB-D pretraining is significant.\n2. The proposed model is highly efficient compared to existing works.\n3. Table 1 presents a nice way of comparing with code-public-available works.\n4. The paper is overall well-written and nicely structured."
            },
            "weaknesses": {
                "value": "1. MultiMAE also uses RGB-D pretraining. However, in this work, a different depth estimation model is used. Would it be nice to provide a more fair comparison by using the same depth estimation model as MultiMAE to produce ImageNet depth data?  \n2. Again regarding fairness, the RGB-D pertaining is based on ImaegNet RGB-D data, and the depth estimation leverages important knowledge learned on other datasets. However, this knowledge is not used by existing RGB-D segmentation works like CMX. This can be discussed.\n3. Will the pretraining weights be released? Would the ImageNet depth data be released? This could be discussed.\n4. In the introduction, it was argued: \"the interactions are densely performed between the RGB branch and depth branch during\nfinetuning, which may destroy the representation distribution\". Do you have any observations to support this argument? E.g., some destroyed distributions or feature maps could be analyzed.\n5. There are still some writing mistakes. E.g., \"we conduct a depth-wise convolution\" should be \"We conduct a depth-wise convolution\". \"Our DFormer perform better segmentation accuracy than the current state-of-the-art\" should be \"Our DFormer produces higher segmentation accuracy than the current state-of-the-art\". \n6. ACNet (ICIP 2019) should be added to Table 1.\n7. How to scale to other modalities like RGB-thermal, RGB-LiDAR, X-Y-Z data, or even more modalities and datasets? This is not well discussed in the future work section. Different from depth data which can be produced by robust depth estimation models, it is harder to have large-scale thermal and LiDAR datasets for pertaining. This can be better discussed.\n8. As the main contribution lies in the study of RGB-D pertaining, more and recent advanced pertaining strategies could be compared and discussed. The main technical design lies in the fusion blocks, but there are no specific pertaining designs. Please discuss this and assess more pertaining choices.\n\nSincerely,"
            },
            "questions": {
                "value": "The proposed model is highly efficient and it has large gains thanks to the RGB-D pertaining.  If the RGB-D pertaining strategy is applied to heavier state-of-the-art RGBD segmentation models like CMX and CMNeXt, how much gain can be achieved? If it is possible, this could be assessed and would help provide a fairer comparison.\n\nFig. 11 shows that the proposed module is sophisticated. Would it be nice to provide more detailed ablations to study other design choices based on this module architecture?\n\nSincerely,"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission272/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission272/Reviewer_KRMX",
                    "ICLR.cc/2024/Conference/Submission272/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission272/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697302401921,
        "cdate": 1697302401921,
        "tmdate": 1700533182752,
        "mdate": 1700533182752,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zmx0FSoyB5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission272/Reviewer_zqyp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission272/Reviewer_zqyp"
        ],
        "forum": "h1sFUGlI09",
        "replyto": "h1sFUGlI09",
        "content": {
            "summary": {
                "value": "The paper introduces a RGB-D pretraining framework for transferable representations in RGB-Depth segmentation tasks. In the proposed methods DFormer, the RGB-depth backbone is pretrained using RGB-D pairs from ImageNet-1K, with the aim of enabling effective encoding of RGB-D information. It incorporates a sequence of RGB-D blocks designed for optimal representation of both RGB and depth data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed RGB-D pretraining framework can be used to solve the representation distribution shift between RGB and the depth information, and to increase the performance of RGB-D representation. \n\nA building block is proposed to perform RGB and depth feature interaction early in the pretraining stage, thus it is possible to reduce the interaction outside the backbone in the fine-tuning stage."
            },
            "weaknesses": {
                "value": "The comparison of using RGB-Depth pretraining on other previous works is missing. The most improvement seems from the join pretraining by using additional depth information as compared to previous methods.\n\nThe analysis of the depth generation is less included. Only one depth estimation method is used to generate the depth image for ImageNet. \n\nThere is generalization limitation in combining two modalities for pre-training. The performance of pre-training or fine-tuning on downstream tasks seems to be highly dependent on the generation or estimation of another modality besides RGB."
            },
            "questions": {
                "value": "What is the effect of using different depth estimation models? How effective is the accuracy of depth estimation for RGBD model pre-training, and will there be accumulation of errors?\n\nHow is the comparison between the fusion building block and the fusion module proposed in previous methods, such as cmx? Also, do the authors try to perform RGB-D pretraining for other methods, so as to perform a more comparable setting? \n\nHow does the DFormer perform if only RGB pretrain + D initialization for finetuning?\n\nHow is the effect and how is improvement from the light hamburger decoder in the proposed model? Whether the authors try to use other decoders? \n\nWhy to perform feature interaction between RGB and depth information in the last two stages?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission272/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission272/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission272/Reviewer_zqyp"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission272/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697382819861,
        "cdate": 1697382819861,
        "tmdate": 1699635952338,
        "mdate": 1699635952338,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zNdu6ofMQ4",
        "forum": "h1sFUGlI09",
        "replyto": "h1sFUGlI09",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission272/Reviewer_HRuY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission272/Reviewer_HRuY"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose an RGB-D pretraining framework for RGB-D semantic segmentation and salient object detection (SOD). First, they use an off-the-shelf depth estimator to generate depth maps for ImageNet-1K. Then, they use the image-depth pairs from ImageNet-1K to pretrain the backbone. Next, they insert an existing head on the backbone and then finetune the model on the RGB-D semantic segmentation and salient object detection datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1.\tTo improve the model performance, the authors pretrained the backbone on ImageNet-1K with image-depth pairs.\n2.\tThe authors conducted experiments on two RGB-D segmentation tasks."
            },
            "weaknesses": {
                "value": "1. The novelty and contributions are too limited. \nFirst, the proposed RGB-D block slightly modifies popular techniques, i.e., self-attention mechanism (Vaswani et al., 2017), depth-wise convolution, and attention weights (Hou et al., 2022), and combines them to fuse RGB and depth features. Second, the design of the RGB-D block follows the widely used idea in SOD, i.e., global and local information fusion. Third, the decoder directly uses the existing head from SegNext (Guo et al., 2022a) without any novel design. Thus, the contribution only comes from the pretraining idea, which is limited.\n\n2.\tThe authors missed some related methods [1-4] for comparison.\n\n[1] Visual Saliency Transformer. ICCV 2021.\n\n[2] 3-d convolutional neural networks for rgb-d salient object detection and beyond. TNNLS 2022.\n\n[3] Bi-Directional Progressive Guidance Network for RGB-D Salient Object Detection. TCSVT 2022.\n\n[4] UCTNet: Uncertainty-aware cross-modal transformer network for indoor RGB-D semantic segmentation. ECCV 2022.\n\n3.\tTo demonstrate the effectiveness of the pretrained backbone, the authors should replace the previous backbone in the compared methods with the proposed one to see whether improvements can be achieved.\n\n4.\tThe authors ignore existing pre-training methods [5, 6] for discussion and comparison.\n\n[5] RGB-based Semantic Segmentation Using Self-Supervised Depth Pre-Training\n\n[6] Self-Supervised Pretraining for RGB-D Salient Object Detection. AAAI 2022.\n\n5.\tSome widely used RGB-D SOD benchmark datasets [7-9] are also ignored.\n\n[7] Depth-induced multi-scale recurrent attention network for saliency detection. ICCV 2019.\n\n[8] Learning selective mutual attention and contrast for rgb-d saliency detection. TPAMI 2021.\n\n[9] Saliency detection on light field. CVPR 2014."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission272/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698656345186,
        "cdate": 1698656345186,
        "tmdate": 1699635952225,
        "mdate": 1699635952225,
        "license": "CC BY 4.0",
        "version": 2
    }
]