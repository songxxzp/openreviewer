[
    {
        "id": "JHsNiX8zsF",
        "forum": "CajWziS3Ia",
        "replyto": "CajWziS3Ia",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2107/Reviewer_WPWU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2107/Reviewer_WPWU"
        ],
        "content": {
            "summary": {
                "value": "This work study the generalizability and efficiency of leveraging foundation multi-modal models for video summarization task. Specifically, a plug-and-play style video language model is proposed, where an LLM uses the text generated from each multi-modal models jointly as prompt to generated a textual output. The design of the framework allow the users to avoid model alignment on feature space."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Explore a new way to leverage foundation multi-modal model without the needs to align the corresponding feature to the same embedding space.\n+ few-shot instruction adaptation strategies to fine-tune an LLM\n+ The evaluation on computational cost, insights discussion, and validation on domain shift scenario present practical insights under specific hardware and data constraints."
            },
            "weaknesses": {
                "value": "- The Ablation of the effect of modalities is incomplete. The modalities are added in a cumulative manner. The design place a strong emphasis on the ASR audio caption, which provide strong information to generate a summary. This is particular \"problematic\" when both dataset are instructional dataset. Hence, it is good to provide results with more variants (e.g., simply remove one modality each time from full model). \n- It is necessary to evaluate the proposed approach on non-instructional dataset."
            },
            "questions": {
                "value": "- For Table 2, can the author provide the METEOR scores for Y -> C? This can give a more intuitive comparison between inter-dataset and intra-dataset transfer."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2107/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698254317810,
        "cdate": 1698254317810,
        "tmdate": 1699636143301,
        "mdate": 1699636143301,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E51FOe3Lu7",
        "forum": "CajWziS3Ia",
        "replyto": "CajWziS3Ia",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2107/Reviewer_PkUQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2107/Reviewer_PkUQ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a plug-and-play video language model by using the texts generated from each input modality in the language mode."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "(1) The paper formulation is good and clear.\n\n(2) The method is straightforward and easy to understand."
            },
            "weaknesses": {
                "value": "(1) The foundation of this work lies in image-to-text generation via BLIP. The resulting text, in conjunction with certain transcripts and metadata, is fed into LLaMa to produce the textual output. The outcome heavily relies on the performance of BLIP.\n\n(2) Exploring ablation studies that focus on specific components could elucidate their importance. Additionally, what if we exclusively use transcripts to generate summaries? This avenue warrants investigation.\n\n(3) While the system generates textual summaries, what about video summaries, such as keyframes? This aspect remains unaddressed.\n\n(4) The results presented in Table 1 suggest that incorporating (O) does not provide any discernible benefit.\n\n(5) To enhance the study's depth, it would be valuable to include more baseline models for comprehensive comparison."
            },
            "questions": {
                "value": "Please see the comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2107/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698464113835,
        "cdate": 1698464113835,
        "tmdate": 1699636143227,
        "mdate": 1699636143227,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XfZ98JjBq2",
        "forum": "CajWziS3Ia",
        "replyto": "CajWziS3Ia",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2107/Reviewer_F78M"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2107/Reviewer_F78M"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a foundation model for video summarization. It does not do alignments between different metadata in the videos, such as audio, images, and captions. Instead, it leverages other existing models, such as SAM, Word2Vec, BLIP2 to convert the information into text. Then, these text modalities are fed into the LLM, i.e., LLaMa to generate the summarization of the videos. In this framework, the authors claim that it achieves good generalization ability without introducing much computational cost."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "[1] Originality: Since videos themselves contain different modalities, it is pretty important to develop a model that can unify different metadata with different modalities to do different video tasks. \n\n[2] Quality: This paper evaluates their method on different datasets, such as YouCook and COIN."
            },
            "weaknesses": {
                "value": "[1] Originality: The most essential concern is originality. This paper proposes linking different methods to convert multimodal signals to text signals. Therefore, these components are not novelties. The author claimed the novelty is the whole framework. However, this framework is very similar to different Multimodal Large Language Models, such as LLAVA, PandaGPT, but even in more complex design. \n\n[2] Originality: The second and the third claimed novelty are experimental results or ablation studies. It can hardly be claimed as the contribution of the paper.\n\n[3] The experimental results are not comprehensive enough. Since the developed method is largely designed for video summarization, it can not be claimed as a foundation model. A foundation model should benefit diverse video tasks, such as action recognition, temporal action proposal, spatiotemporal detection, etc."
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2107/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698544870621,
        "cdate": 1698544870621,
        "tmdate": 1699636143119,
        "mdate": 1699636143119,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tU78i0MD63",
        "forum": "CajWziS3Ia",
        "replyto": "CajWziS3Ia",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2107/Reviewer_zhmP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2107/Reviewer_zhmP"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a framework for video summarization using language models. This framework extracts information from the video using off-the-shelf, modality-specific models and converts it into text. This textual information is then used to populate a predefined template prompt, which the language model utilizes to summarize the video. The authors assess their model in both zero-shot and few-shot settings across two datasets. Additionally, the paper explores the impact of each modality on summarization quality, finding that audio transcripts are the most significant for the evaluated datasets. The authors also investigate how well the prompts tailored for one dataset can be applied to another."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper studies an important problem which is how to perform video summarization using Large Language Models.\n2. The authors propose evaluating the method under both zero-shot and few-shot settings, a reasonable approach considering the prevailing trend of generalist models tackling a broad spectrum of tasks.\n3. The paper examines a greedy selection method to determine the best samples in a few-shot setting."
            },
            "weaknesses": {
                "value": "1. The paper's main weakness is overclaiming contributions that are not well supported.\n    - The second contribution highlights:\n\"what input modalities and model sizes are fitting for a given domain, and expectations of how multi-modal models adapted to one domain will fare in another.\" This claim seems overly broad, especially since proving such a statement would presumably require experiments across multiple tasks, datasets, and domains. However, the paper restricts its experiments to video summarization using just two datasets.\n     - The third contribution suggests a generic method to fine-tune multimodal LLMs. In reality, the paper only offers a tailored approach for video summarization. How can this method be applied to non-local tasks like Action Localization, Video Object Segmentation, Video Action Segmentation, Dense Captioning, and so forth? The paper's claims far exceed its actual content, which essentially presents a method for using LLMs specifically for video summarization.  \n\n2. The method's focus on video summarization is ambiguous. The paper emphasizes that only the center frame is considered. So, is this method merely an advanced image captioning technique? Without the audio modality, the approach essentially becomes an imaging technique. To address this assumption, one would expect the method to perform competently even without audio (as inferred from Table 1).\n    - Related to this point the selection of the dataset is key. Why didn't the authors use other summarization datasets like TvSum? Or even better, other tasks like VQA that require more temporal reasoning.\n    - Since this paper does not reason over temporal sequences, isn't it more like an enhanced image captioner?\n    - There are similar works trying to use LLMs to get video descriptions [G], what is the advantage of the current work over them? \n\n3. Last but not least, the proposed method doesn't introduce a novel approach. Several papers have already explored converting modalities into text and utilizing LLMs for video tasks. For example, Yang et. al. (AAAI-2022) [A] and Tiong, et. al. (ACL-2022) [B] presented a similar framework, wherein frozen LLMs processed the outputs of an image captioner for VQA. With numerous analogous methods [C,D,E,F] introduced recently, the originality of this work is not evident to me.\n\n\n[A] Yang, Z., Gan, Z., Wang, J., Hu, X., Lu, Y., Liu, Z., & Wang, L. (2022, June). An empirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 36, No. 3, pp. 3081-3089).\n\n[B] Tiong, A. M. H., Li, J., Li, B., Savarese, S., & Hoi, S. C. (2022). Plug-and-play vqa: Zero-shot vqa by conjoining large pretrained models with zero training. arXiv preprint arXiv:2210.08773.\n\n[C] Guo, J., Li, J., Li, D., Tiong, A. M. H., Li, B., Tao, D., & Hoi, S. (2023). From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10867-10877).\n\n[D] Li, C., Ge, Y., Mao, J., Li, D., & Shan, Y. (2023). TagGPT: Large Language Models are Zero-shot Multimodal Taggers. arXiv preprint arXiv:2304.03022.\n\n[E] Pan, J., Lin, Z., Ge, Y., Zhu, X., Zhang, R., Wang, Y., ... & Li, H. (2023). Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models. arXiv preprint arXiv:2306.11732.\n\n[F] Chen, J., Zhu, D., Haydarov, K., Li, X., & Elhoseiny, M. (2023). Video chatcaptioner: Towards the enriched spatiotemporal descriptions. arXiv preprint arXiv:2304.04227."
            },
            "questions": {
                "value": "- What are the genuine contributions of this paper? Instead of overstating, I'd recommend presenting only those contributions substantiated by the experiments. It's acceptable if the contributions are task-specific and not generalized.\n- How does this method qualify as a video summarization technique rather than merely an enhanced captioner? (refer to the second weakness)\n    - What would the outcome be if Audio isn't a provided modality, or if the audio lacks speech content? How effective would this method be in summarizing silent videos?\n- What novel insights or methods does this paper introduce? (as mentioned in the third weakness)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2107/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698759980211,
        "cdate": 1698759980211,
        "tmdate": 1699636143038,
        "mdate": 1699636143038,
        "license": "CC BY 4.0",
        "version": 2
    }
]