[
    {
        "id": "DxMnJ48HAr",
        "forum": "KZA5gDk2Jj",
        "replyto": "KZA5gDk2Jj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2520/Reviewer_VRp5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2520/Reviewer_VRp5"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel approach for recovering images on which an auto-encoder was trained. The method assumes access to a set of degraded training images. Specifically, the degrading is done via a noisy *linear* operator. The authors study a particular variation of such an operator which erases the image pixels (i.e., diagonal matrix with $\\\\{0,1\\\\}$ entries). On a high-level, the method consists iof alternating steps of estimating the image $\\hat{\\mathrm{x}}$ and degrading operator $\\hat{\\mathrm{\\boldsymbol{H}}}$ via an ADMM-like algorithm. The authors demonstrate the superiority of  their approach by comparing with DDNM method and iterative application of trained autoencoder (Radhakrishnan et al.), while also validating a variation of their method which has an access to the true pixel mask $\\hat{\\mathrm{\\boldsymbol{H}}}$."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- a novel method for image recovery, that seems to be empirically superior to the existing alternatives\n- the methodology is based on a well-established ADMM method"
            },
            "weaknesses": {
                "value": "- the type of degrading of the images is limited to noisy linear ones\n- the method still seem to assume a particular structure of $\\hat{\\mathrm{\\boldsymbol{H}}}$ (i.e., diagonal for pixel erasure), e.g., the choice of the regularizer $\\phi$ and etc.\n- having access to a degraded training samples for recovery is a bit less practical than one can imagine\n- the comparison and overall experimental evaluation seems a bit lacking"
            },
            "questions": {
                "value": "- do the authors think that their method can potentially treat a general form of unknown $\\hat{\\mathrm{\\boldsymbol{H}}}$? If so, I would be delighted to see some numerical evidence for that, even less rigorous would do\n\n- it would be interesting to see, if the method still performs well under a mismatched scenario, i.e., the degrading process itself is not linear, but one can assume a certain form of $\\hat{\\mathrm{\\boldsymbol{H}}}$ that replicates it close enough\n\n- It would be interesting to see whether the method is able to perform well on an image which is not used during the training but close enough: pick some simple dataset and subsample images of a certain class and than look at the performance of the unused remaining ones"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2520/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2520/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2520/Reviewer_VRp5"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698061781615,
        "cdate": 1698061781615,
        "tmdate": 1699636188488,
        "mdate": 1699636188488,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wMKpiUr9ip",
        "forum": "KZA5gDk2Jj",
        "replyto": "KZA5gDk2Jj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2520/Reviewer_1EzY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2520/Reviewer_1EzY"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed an ADMM algorithm to recover training data from degraded observations. In detail, this paper looks at masking-based linear degradation functions similar to those in linear inverse problems, for AEs that can almost perfectly fit the training data. The proposed method outperforms previous techniques and improves recovery performance. The experiments also show a strong correlation between overfitting and recovery performance of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is clearly written and easy to follow. The formulation follows naturally from the reconstruction task, and all algorithm design follows naturally from the training objective. The decomposition of all parts of the algorithms is also straightforward. \n- It is an interesting application of ADMM to data reconstruction tasks and can be potentially used for the general neural network architectures with the plug-and-play method. There is theory provided to support this change.\n- The experimental results show improvement over a number of baselines, and even under the noisy degradation case."
            },
            "weaknesses": {
                "value": "- The motivation of this paper is unclear. The paper mentions privacy challenges in modern ML at the beginning of the paper, but there lacks connection between the proposed data reconstruction technique and realistic privacy challenges. There should be more discussion (and examples) on how this formulation can represent realistic privacy concerns. For instance, one speculative example can be generating sensitive information with appropriate prompts in an LLM, which seems to be related to masking degradation. Another example can be image copyright issues, which further establishes a connection to membership inference. I hope the authors can draw a connection between the proposed formulation and these practical challenges, and make a clear statement on the attack taxonamy (e.g. black- or white-box, number of queries, etc.). \n- The word \"overparameterized\" is not appropriate in my thoughts. It usually means the middle hidden layer is much wider than input/output dimensions, which is in contrast to AEs where the latent dimension is usually smaller. Overparameterized networks may not always interpolate training data; they usually do under sgd or gd, but not under some other optimizers; and extrapolation may happen together with interpolation, which indicates non-overfitting. Based on your assumption, it is more accurate to use words like interpolating or overfitted AEs. \n- While there is theoretical justification for using plug-and-play to avoid the explicit definition of $s_f$, there isn't convergence analysis on the proposed algorithm. The initialization selection also seems to be heuristic. It is therefore reasonable to doubt how robust the proposed algorithm is, as well as its potential to generalize to a wider range of problems. \n- There isn't a proposed algorithm for noisy degradation that leverages $\\sigma_{\\epsilon}$ assuming it's known, which limits the scope the proposed task. While the experiments for small $\\sigma_{\\epsilon}$ shows the scalability of the proposed method, it is possible to fail for larger $\\sigma_{\\epsilon}$, which is not discussed in the paper. \n- The experiments are only conducted on very small training sets. From my understanding this is to ensure that the model overfits. However, it is very far away from practical scenarios, where most ML models under trustworthy concerns are giant and trained on massive data. It would be more interesting to look at (pretrained) models trained on the full datasets. These models can overfit on parts of the training set and not overfit on others; this can help us better understand the effect of overfitting to reconstruction. \n- There is no discussion on what will happen if you input a degraded sample $\\notin$ training set but close to some $x_i$."
            },
            "questions": {
                "value": "The questions correspond to the weakness mentioned above.\n- Motivation and connection to realistic privacy concerns?\n- Any convergence and initialization analysis? \n- Is there an algorithm for noisy degradation? \n- What is the maximum noise for the proposed method to perform well? \n- Any experiments for standard full training sets? \n- What does the proposed method output for non-training samples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698538840255,
        "cdate": 1698538840255,
        "tmdate": 1699636188416,
        "mdate": 1699636188416,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tnyF4wp5q2",
        "forum": "KZA5gDk2Jj",
        "replyto": "KZA5gDk2Jj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2520/Reviewer_PoRf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2520/Reviewer_PoRf"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on developing methods that can recover degraded training data samples from overparameterized autoencoders (AE).\n\nThey formulate the task as an inverse problem and proposes an iterative optimization method to solve the optimization.\n\nThe proposed method significantly outperformed the baseline method on both FC AE & UNet AE and CIFAR-10 images."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The formulation of the recovery problem is interesting and makes sense to me."
            },
            "weaknesses": {
                "value": "My main concern is on the assumption and experimental setup.\n\nThis work assumes that the autoencoder can overfit to the real data. I'm unsure whether this could happen in practice if we train AE on a real-world dataset. The largest data the AE is trained on is 25,000 images. Wondering that will happen if we apply the proposed method on a AE trained on a  real-world data, e.g., the AE used in Stable DIffusion that has been widely used by tons of work on image generation.\n\nThe current experimental setup is limited to small-scale. Would be great to see results on large-scale setup."
            },
            "questions": {
                "value": "See Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698728005331,
        "cdate": 1698728005331,
        "tmdate": 1699636188323,
        "mdate": 1699636188323,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "c9hUy6pemY",
        "forum": "KZA5gDk2Jj",
        "replyto": "KZA5gDk2Jj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2520/Reviewer_RTXb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2520/Reviewer_RTXb"
        ],
        "content": {
            "summary": {
                "value": "The consider the linear inverse problem of blind inpainting for a particular set of images which are used priory to train an overparameterized autoencoder. The use ADMM and the trained AE in a plug-and-play prior scheme to estimate the degradation operator on the training image, and fully recover the training image."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This problem is of particular interest to both the inverse problem community, and those interested in privacy issues concerning revealing the training data of a trained network. The paper is written clear and was easy to follow."
            },
            "weaknesses": {
                "value": "I find some motivations of the framework not well-suited for wider applications. They consider recovery of training images from degraded measurement of the image. How is this method applicable to inverse problems on data not appeared in the training data? How is the recovery when there is no measurement of the training image? The authors indeed shows (ans to my first question) that their approach cannot be used to solve general inverse problems on a test set. How applicable this method is on inverse problems with other measurement operators (e.g., additive Gaussian noise, Gaussian blurring, random inpainting, motion blur)? The experiments are not comprehensive.\n\n\nThe paper applies plug-and-play approach which is used is solving a general inverse problem using a denoiser on recovery of training images; they replace the denoiser with an autoencoder (the concept is exactly the same). The contribution of the method for inverse problem is limited.\n\nPrior works on AE have shown that AE can recover training images. However, the reported numbers for the baseline is super low.\n\nSee my questions.\n\nMinor comments\n\n- I do not find the usefulness of the Theorem 1 for the general overparameterized, untied autoencoder. In general, (16) has been used and well-motivated by the plug-and-prior literature, so not clear why the authors on discussing usage of a network as a proximal operator for implicit regularization.\n\n-  Please provide appropriate citations for alternating-minimization method (6), (7). This is a well-known procedure in dictionary learning. One example is [1].\n\n- The last two paragraphs in Section 1 statements are very vague (has no citation) and not clear what the exact comparison is. See my question in Q section.\n\n\n[1] Chatterji, N. S., & Bartlett, P. L. (2017). Alternating minimization for dictionary learning: Local convergence guarantees. arXiv preprint arXiv:1711.03634."
            },
            "questions": {
                "value": "1. The paper consider linear inverse problem of blind inpainting. I wonder how the performance of the framework is on other linear inverse problem (additive Gaussian noise, Gaussian blurring, random inpainting) or non-linear inverse problems (motion blur)?\n\n2. Can the authors elaborate how much was the \"small training dataset size\" that was used in prior works?  The authors argued that their method can be applied on large training set images unlike prior work. The main experimental results include 600 images, and 50 images. Could the author elaborate on large training set?\n\n3. Could the author elaborate which prior methods the outperform by citing (above section 2)? Does prior works try to recover training images given some degraded measurement or without having any measurement from it? Please elaborate, as this is crucial for fair comparison between the methods.\n\n4. \"our results also demonstrate the reduction in the recovery ability as the autoencoder is trained to a\nhigher train loss and less overfits its dataset. This, as well as our other results, are useful to understand the privacy risk of training data recovery in autoencoders.\" Is this a new finding? or I find this trivial.\n\n5. The definitions of image regularizer and H regularizer in (5) are not rigorous and is not defined. Please elaborate on the wording \"probable\", and how the regularizers are implemented. Are they smooth? differentiable? ...\n\n6. What is the motivation toward using ADMM as opposed to vanilla gradient on regularized objective? Why the splitting provides benefit in this case?\n\n7. For (18), why not defining H only as a diagonal matrix in the first place? Then (18) is not needed.\n\n8. Can authors elaborate on how they define \"recovery\"?\n\n9. Can the author explain why the \"AE iteration only does not work\"? Providing a visualization on the iterations on AE to find its fixed point can be helpful.\n\n10. Possible to visualize some failed examples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698938632913,
        "cdate": 1698938632913,
        "tmdate": 1699636188261,
        "mdate": 1699636188261,
        "license": "CC BY 4.0",
        "version": 2
    }
]