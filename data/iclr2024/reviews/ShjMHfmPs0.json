[
    {
        "id": "xlBIHlsufl",
        "forum": "ShjMHfmPs0",
        "replyto": "ShjMHfmPs0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4006/Reviewer_uRXW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4006/Reviewer_uRXW"
        ],
        "content": {
            "summary": {
                "value": "The work investigates autophagous generative processes, where generative models train on data that includes samples from AI-synthesized data, and identifies a phenomenon termed Model Autophagy Disorder (MAD). MAD refers to the progressive deterioration of both quality and diversity in synthetic data over generations. The presence of sampling bias, common in generative model training, influences the impact of MAD. This paper shows that with enough fresh real data, the quality and diversity of the generative models do not degrade over generations. Additionally, the paper reveals a phase transition in the admissibility of synthetic data in fresh data loops, with excessive synthetic data potentially leading to MADness.  This work also discusses that autophagous loop behaviors hold across a wide range of generative models and datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) This paper identifies and addresses a novel issue in the field of generative modeling, the phenomenon of Model Autophagy Disorder (MAD) resulting from autophagous generative processes. This issue, which has not been extensively explored before, carries potential consequences for data quality and diversity in AI systems\n\n(2) The paper exhibits a well-structured and easily understandable presentation. It offers clear definitions, explanations, and visualizations to elucidate the concept.\n\n(3) The study conducts extensive experiments to probe autophagous generative processes across diverse scenarios, encompassing fully synthetic loops, synthetic augmentation loops, and fresh data loops. The inclusion of various generative models, approaches, and datasets enhances the robustness of the findings.\n\n(4) The paper's findings hold practical implications for the training of generative models, especially in situations involving synthetic data. They offer valuable insights and guidance for future model training endeavors."
            },
            "weaknesses": {
                "value": "(1) This work would benefit from a dedicated section discussing potential solutions or mitigations for Model Autophagy Disorder (MAD) in the training of generative models. This could enhance the paper's practical utility in the field.\n\n(2) A more in-depth exploration of the broader implications of MAD is expected.  This could involve exploring the impact of MAD in various domains beyond image generation, such as NLP or audio synthesis. Additionally, discussing real-world scenarios and applications where MAD could manifest would make the paper more informative.\n\n(3) To enhance the rigor of the paper, a more comprehensive theoretical framework should be developed to formalize the concept of MAD and its implications. A deeper exploration of the mathematical foundations underlying MAD would contribute to a stronger theoretical basis for the research.\n\n(4) The work lacks a detailed description of the sources or methods used to generate the synthetic data used in this study. Providing specifics on how synthetic data was created for experiments would enhance transparency and reproducibility."
            },
            "questions": {
                "value": "(1) Could the authors provide more insights into potential strategies or mitigations for addressing MAD in generative model training?\n\n(2) How might MAD impact domains beyond image generation, such as NLP or audio synthesis? Can the authors hypothesize on potential challenges and solutions in these areas?\n\n(3) The paper discusses the impact of sampling bias (\u03bb). Could the authors provide a more nuanced analysis of how varying levels of sampling bias affect MAD, particularly in terms of quality and diversity?\n\n(4) Could the authors elaborate more on the mathematical foundations of MAD and its implications?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4006/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697750023302,
        "cdate": 1697750023302,
        "tmdate": 1699636362705,
        "mdate": 1699636362705,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wYu5ELBfUM",
        "forum": "ShjMHfmPs0",
        "replyto": "ShjMHfmPs0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4006/Reviewer_ZeQh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4006/Reviewer_ZeQh"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the effect of repetition of the process that generated data of a generative model are included in the training data of generative models of next generation (self-consuming loop).\nThey categorize possible scenarios to 1. training data are fully generated, 2. training data are partially generated and real data are fixed, and 3. training data are partially generated and real data change.\nTheoretical and empirical analyses reveal that scenarios 1 and 2 lead generative models to collapse after some iterations of the loop."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The topic, self-consuming loop, is relevant to the community, as generative models are more and more common and a plenty of generated data are released to the internet. This paper assumes three possible scenarios of the loop and analyzes each of them, which distinguishes this work.\nIt is also important that the authors discuss how to alleviate the effect of \"MAD\"."
            },
            "weaknesses": {
                "value": "Although the authors define the autophagous mechanisms such that \"each generative model $\\mathcal{G}^t$ is trained on data that includes samples from previous models\" (p3), but to my understand, the theory (around eq 1) and the experiments (described in A.1 and A.2 diffusion model) only consider the case that each model $\\mathcal{G}^t$ is trained on data generated by $\\mathcal{G}^{t-1}$. \nThe discrepancy between the definition of autophagous mechanisms and the theory and experiments is worth noting otherwise it diminishes the soundness."
            },
            "questions": {
                "value": "* To my understand, Huang et al. 2022 (cited in p3) considers \"synthetic data augmentation\", but Hataya et al. 2022 (cited in p3) focuses on the effects of the first loop of synthetic augmentation loops or fully synthetic loops on some downstream tasks, such as classification. Do I misunderstand something?\n* Why the authors choose StyleGAN2 with FFHQ and DDPM with MNIST?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4006/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698720957927,
        "cdate": 1698720957927,
        "tmdate": 1699636362618,
        "mdate": 1699636362618,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "S0nkol1Tv5",
        "forum": "ShjMHfmPs0",
        "replyto": "ShjMHfmPs0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4006/Reviewer_VGNe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4006/Reviewer_VGNe"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates performance degradation of generative models in self-consuming (or autophagous) loops. The paper categorizes three scenarios of self-consuming based on real-world applications. In particular, a new scenario called \"fresh data loop\" is considered for the first time, where new data comes into the self-consuming loop. The empirical investigation on the scenarios reveals that, while the self-conuming loop tends to couse the degradation of the generative models, we can avoid it if the generative models in the loop continues to be exposed to new data. Also, the paper investigates how sampling techniques for better generation affects the degradation in the loop. It is empirically confirmed that these findings hold for different generative models or datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is clearly written and easy to follow. Their problem is appropriately formulated in a mathematically sound way.\n- In Section3, a toy example of gaussian models is analyzed, which provides a theoretical evidence for their heuristic Claim in Section 2.\n- It is predictable but also insightful that the performance degradation in the loop is caused only when using fixed samples over the loop."
            },
            "weaknesses": {
                "value": "- It is unclear what Claim in Section 2 aims for. Although it is proved for a toy model of gaussians, I assume that it is aimed for a hypothesis for general case but the authors seem not to explain such aspects. For example, is random walkness in Claim observed for practical models? If not, the claim would be overclaimed and somewhat doubtful for general case. Finally note that \"Claim\" in a paper is usually considerred as to be proven in the paper, not a hypothesis.\n- Eq. (2) is totally unclear. Is it a hypothesis or theorem? Where is $WD(n_r, n_s, \\lambda)$ defined? If it is just a hypothesis, the author should make it clear in the text.\n- It is unclear why $n_{ini}$ is considered in the first part of Section 5. How does it relate to the main claims in Introduction?\n- The observation of a phase transition is interesting, but it suddenly appears at Section 5. I recommend to briefly discuss the motivation and the results in Introduction."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4006/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4006/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4006/Reviewer_VGNe"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4006/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813346271,
        "cdate": 1698813346271,
        "tmdate": 1699636362532,
        "mdate": 1699636362532,
        "license": "CC BY 4.0",
        "version": 2
    }
]