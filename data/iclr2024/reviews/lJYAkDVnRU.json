[
    {
        "id": "S2kK6aeuY8",
        "forum": "lJYAkDVnRU",
        "replyto": "lJYAkDVnRU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8213/Reviewer_Hb5r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8213/Reviewer_Hb5r"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a novel meta-learning algorithm that allows visual models to learn new concepts during inference, mimicking the capabilities of LLMs such as ChatGPT. The technique utilizes a static pre-trained feature extractor and treats meta-learning similarly to sequence modeling with labeled and unlabeled data points. When evaluated on 11 benchmarks, the proposed method, without any meta-training or fine-tuning, outperformed or matched the leading P>M>F algorithm in 8 of those benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Intriguing Research Question:** This paper delves into a significant question in meta-learning. The authors note that meta-learning traditionally involves pretraining, meta-learning, and fine-tuning. However, their approach seeks to bypass meta-learning and fine-tuning by transforming the learning process into a sequence modeling task and applying the in-context learning objective function.\n\n**Rigorous Numerical Performance:** The paper's data shows the model's performance to be both impressive and robust.\n\n**Novel ELMES Class Encoder:** The class embedding presented here appears to be quite innovative. I've noticed no other usage of this type of class embedding in in-context learning in NLP or VLM literature unless other reviewers bring up some status quo. This is also in alignment with some recent studies on Arxiv that indicate that class embeddings might be unnecessary, as arbitrary words, numbers, or first names can be used to label images (see [2])."
            },
            "weaknesses": {
                "value": "I will provide the paper's weaknesses in the following. \n- The concept of using the ICL objective function to pre-train a transformer model isn't a new one [1]. Unlike [1], which pre-trained a transformer from scratch within a meta-learning framework, this paper adapts the pre-training objective to Clip image embeddings, which doesn't significantly enhance novelty.\n- The primary innovative aspect highlighted in this paper is the ELMES Class Encoder. While this feature is intriguing, it narrows the scope of innovation in the study.\n- Some claims in the paper lack experimental backing. For instance, the assertion that the ELMES Class Encoder upholds label symmetry and is invariant to the permutation of demonstrations isn't convincingly proven with data.\n- The mathematical explanations in Section 4 are challenging to follow. A clearer, more comprehensible presentation of this section would be beneficial. Until then, I'm relying on other reviewers to verify the accuracy of the mathematical derivations presented."
            },
            "questions": {
                "value": "- Could the authors elaborate on what they consider to be the primary innovative contribution of their study?\n- It would be beneficial if the authors could include experiments to demonstrate the label symmetry and permutation invariance capabilities of the demonstrations as claimed.\n\n[1]. General-Purpose In-Context Learning by Meta-Learning Transformers, NeurIPs 2022\n\n[2]. Small Visual Language Models can also be Open-Ended Few-Shot Learners, Arxiv 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8213/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8213/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8213/Reviewer_Hb5r"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8213/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698684405437,
        "cdate": 1698684405437,
        "tmdate": 1700730643572,
        "mdate": 1700730643572,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0EVGJhmreR",
        "forum": "lJYAkDVnRU",
        "replyto": "lJYAkDVnRU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8213/Reviewer_yxC1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8213/Reviewer_yxC1"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a meta-learning algorithm that learns new visual concepts during inference without fine-tuning. The method performs well on several benchmark datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper draws a new perspective for meta-learning: learning to classify a query from a context of support set, imitating the way in LLMs. \n2. The framework is straightforward and clean. The reason (proved theoretically) for using the specific ELMES embedding is presented well.\n3. Extensive experiments and analysis are provided."
            },
            "weaknesses": {
                "value": "Apologies in advance I am not an expert in meta-learning. But I still have the following questions:\n1. What is the unknown class embedding initialization for the ELMES Class Encoder?\n2. As discussed by the authors themselves in section 5.3, the number of classes need to be known in advance and the frozen encoders limit the learning ability. However in my view, the need of number of classes is an inherent problem in few-shot learning. But finetuning more modules could be further discussed.\n3. There is a lack of experimental details (especially training details)."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8213/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698705246881,
        "cdate": 1698705246881,
        "tmdate": 1699637019709,
        "mdate": 1699637019709,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iHOPYtdXLp",
        "forum": "lJYAkDVnRU",
        "replyto": "lJYAkDVnRU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8213/Reviewer_GdKg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8213/Reviewer_GdKg"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new \u201cuniversal meta-learning\u201c setup that \u201cavoids meta-training on the train/validation splits of meta-learning benchmarks or fine-tuning on the support set during inference.\u201d Instead, the paper attempts to recast meta-learning as a sequence modelling problem, where meta-testing on new tasks is analogous to in-context learning in large language models. The proposed approach CAML, context-aware meta-learning, uses CLIP image representations, together with one-hot label encoding dubbed as \u201cEqual Length and Maximally Equiangular Set (ELMES) encoding,\u201d \u00a0to represent each in-context learning example. The base sequence model is a Transformer encoder. It is trained to predict the query class label given in-context examples that are comprised of labelled support examples and a query example. The model is pre-trained on ImageNet-1k, Fungi, MSCOCO, and WikiArt and evaluated on 11 meta-learning benchmark datasets. Empirical results show that the proposed approach outperforms other \u201cuniversal meta-learning\u201c baselines on 15 of 22 evaluation settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper is well-motivated on the need for in-context learning by drawing analogies with large language models. I also liked the analysis in Fig. 2, which illustrates how dynamic in-context examples impact representation learning\n    \n2. Theoretical analysis of the \u201cEqual Length and Maximally Equiangular Set (ELMES) encoding\u201d presents an interesting analysis of label symmetry and permutation invariance in meta-learning.\n    \n3. The paper presents competitive empirical results on various meta-learning baselines."
            },
            "weaknesses": {
                "value": "1. Novelty: the paper does not discuss previous work that also formulated meta-learning as a sequence, or set, modelling problem [1, 2]. The problem formulations in [1, 2] are highly similar to the proposal in this paper, except for architectural differences in implementation. This weakens the novelty of this paper.\n\n2. The dichotomy between \u201cmeta-training\u201c and \u201duniversal meta-learning\u201d: the paper attempts to make the distinction between \u201duniversal meta-learning\u201d and \"meta-training\" in that the proposed CAML approach does not perform \u201cmeta-training\u201c or \u201cfine-tuning on the support set.\u201c Instead, \u201duniversal meta-learning\u201d only performs pre-training. However, I think this dichotomy is not well-defined. Pre-training in the CAML fashion can be understood as learning across many different tasks in the pre-training dataset, i.e., meta-training, and in-context learning can be understood as performing implicit gradient descent based on the in-context examples. This dichotomy also implicates the comparisons in empirical results as CAML was mostly compared with other \u201cuniversal meta-learning\u201c methods, i.e., ProtoNet, MetaOpt, and MetaQDA.\n\nAdditional related work:\n\n[1] Mishra, Nikhil, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. \"A simple neural attentive meta-learner.\"\u00a0*arXiv preprint arXiv:1707.03141*\u00a0(2017).\n\n[2] Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S. &amp; Teh, Y.W.. (2019). Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks. <i>Proceedings of the 36th International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 97:3744-3753 Available from https://proceedings.mlr.press/v97/lee19d.html."
            },
            "questions": {
                "value": "1. Please summarize the novelty of this paper in relation to [1, 2].\n    \n2. Please respond to Weakness 2: The dichotomy between \u201cmeta-training\u201c and \u201duniversal meta-learning.\u201d\n    \n3. Please elaborate on how the transformer encoder is implemented and the rough scale of parameters it has.\n    \n4. Please elaborate on the pre-training dataset of CAML in \u201cwe pre-train CAML\u2019s Transformer encoder on few-shot image classification tasks from ImageNet-1k.\u201d How many examples are included in the pre-training set? Note that one of the benchmarks, miniImageNet, is a subset of ImageNet. Would this pretraining dataset result in task leakage?\n    \n5. Please elaborate on how the ProtoNet baseline is implemented. Is it trained on the same pre-training dataset but with the ProtoNet loss objective?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8213/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698724760684,
        "cdate": 1698724760684,
        "tmdate": 1699637019594,
        "mdate": 1699637019594,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iUAwFrS2Ys",
        "forum": "lJYAkDVnRU",
        "replyto": "lJYAkDVnRU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8213/Reviewer_8Akm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8213/Reviewer_8Akm"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses a gap in the field of visual meta-learning, where models have traditionally struggled to learn new visual concepts during inference without fine-tuning, a capability that Large Language Models (LLMs) like ChatGPT have demonstrated in the textual domain. The authors introduce a novel meta-learning algorithm inspired by the in-context learning of LLMs. This approach treats n-way-k-shot image classification as a sequence modeling over known labeled data points and an unknown test data point."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written.\n2. The problem studied in this paper is interesting and valuable.\n3. The theoretical work of this paper is sufficient, which improves the value of the paper."
            },
            "weaknesses": {
                "value": "1. The authors utilize the CLIP model to encode both images and labels. An area of potential exploration is why they didn't attempt to encode context and images directly, especially using datasets like MSCOCO.\n2. In the experiments, CAML's performance on out-of-domain tasks is notably weak. This might be primarily due to the treatment of unseen categories, which are uniformly encoded as \"Unknown [class] Embedding\".\n3. The study lacks ablation experiments for its various modules, and there's an absence of quantitative analysis for hyperparameters."
            },
            "questions": {
                "value": "Please see the Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8213/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698849443014,
        "cdate": 1698849443014,
        "tmdate": 1699637019449,
        "mdate": 1699637019449,
        "license": "CC BY 4.0",
        "version": 2
    }
]