[
    {
        "id": "Q8jIBJ59lY",
        "forum": "Un0rgm9f04",
        "replyto": "Un0rgm9f04",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission710/Reviewer_mnca"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission710/Reviewer_mnca"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a video diffusion model based on the Transformer architecture, a departure from the commonly used U-Net structure in previous Video Diffusion Models (VDM). The utilization of the Transformer model for this task is a significant contribution, especially given its novelty in the field. Additionally, the incorporation of a mask-based modeling mechanism extends the applicability of the model to a wider range of tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Pioneering exploration of the Transformer architecture for video diffusion models, marking a valuable starting point.\n- Validation of the effectiveness of the proposed architecture and methods across multiple task datasets.\n- Achieving state-of-the-art results on several mainstream benchmarks."
            },
            "weaknesses": {
                "value": "- While the authors conducted validations on various tasks, the ablation experiments in Table 2 are insufficient. Verification in unconditional video generation tasks would enhance the persuasiveness of the study.\n- The method is limited to lower resolutions, diminishing its practical applicability.\n- I believe it would be valuable to compare training and inference times. Understanding the computational costs associated with both training and real-time usage is crucial for assessing the feasibility of the approach in real-world scenarios. It would enhance the completeness of the evaluation and provide readers with a comprehensive understanding of the method's efficiency. Therefore, I recommend including a comparison of training and inference times in the manuscript to strengthen the overall analysis."
            },
            "questions": {
                "value": "- I previously reviewed an earlier version of this manuscript, and there have been notable improvements, particularly in clarifying the significance of the Transformer in the introduction, which I find valuable. \n- I am still curious about why the patch-based method is limited to such low resolutions. Even at 256 resolution, the token length is not excessively long, and it could potentially be experimented with. Is this limitation due to computational constraints?\n- Additionally, it would be beneficial to compare the proposed method with more recent publications. A comparison with the results summarized in [a] could be enlightening.\n- In conclusion, I find the innovation in this paper substantial, and it presents a compelling argument. For now, I am inclined to recommend acceptance.\n\n[a] Xing, Zhen, et al. \"A Survey on Video Diffusion Models.\" arXiv preprint arXiv:2310.10647 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission710/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698406897252,
        "cdate": 1698406897252,
        "tmdate": 1699635998199,
        "mdate": 1699635998199,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Avh9COtk2R",
        "forum": "Un0rgm9f04",
        "replyto": "Un0rgm9f04",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission710/Reviewer_cvvZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission710/Reviewer_cvvZ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Video Diffusion Transformer (VDT), a video generation model with pure transformer architecture.\nThe core idea is to replace the unet structure of the existing video generation model with a pure transformer structure.\nThe authors claimed that it's the first successful model in transformer-based video diffusion.\nThey propose a unified spatial-temporal mask modeling mechanism for VDT, enabling it to unify a diverse array of general-purpose tasks.\nThey show the effectiveness of VDT on several video-related tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Compared with the current U-net-based network with demonic attention structures, VDT is a pure transformer network. If it is indeed the first Transformer-only video diffusion network, it would be a good baseline for this area.\n2. This paper further proposes the spatiotemporal mask mechanism, which can make VDT adapt to various video tasks, including video generation, video prediction, image-to-video generation, video completion, etc.\n3. This paper verifies the effectiveness of VDT on multiple video generation tasks.\n4. The code in this article is open source and will help others replicate this work."
            },
            "weaknesses": {
                "value": "1. The core idea of this paper is simple and reasonable. Replacing the original CNN with transformer is an effective method that has been widely verified in the whole field. In addition, this paper adopts the basic attention mechanism, and the mask strategy proposed is also commonly used in video tasks. So, from this perspective, the contribution is slightly less obvious.\n2. I feel that the work in this paper can be a good benchmark in this field. However, this paper only verifies that it can be effective on multiple video tasks. As a good benchmark, this paper should give more detailed experimental analysis of ablation. For example, why does space-time pay attention to time before space? In this paper, the influence of each module on the video generation effect, the comparison experiment of hyperparameter and so on.\n3. Tab.5, 142.3 (blacken) is not the best one."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission710/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698588711926,
        "cdate": 1698588711926,
        "tmdate": 1699635998096,
        "mdate": 1699635998096,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pfyZZpTxg3",
        "forum": "Un0rgm9f04",
        "replyto": "Un0rgm9f04",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission710/Reviewer_HPRy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission710/Reviewer_HPRy"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a new method called VDT for generating videos. In this approach, transformer architecture replaces U-net as the backbone for the diffusion model. The method employs a unified spatial-temporal mask for diverse tasks and yields state-of-the-art results. The authors also provide an explanation of the mechanism behind VDT for better understanding."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The method is pretty novel and compelling. \n(2) Different tasks are designed to clarify the superiorities.\n(3) The analysis of the training strategy is technically sound."
            },
            "weaknesses": {
                "value": "(1) According to Table 4, the FVD values of the diffusion model pre-trained with U-Net are significantly better than VDT. However, the authors mention GPU resource limitations as a potential issue. I suggest that the authors compare the results of relevant tasks to provide further clarification.\n(2) It is unclear why MCVD-concat performs better than VDT in FVD.\n(3) Due to the resolution, it is difficult to distinguish the differences between Videofusion and VDT in the TaiChi-HD section."
            },
            "questions": {
                "value": "--The presentation was found to be easy to follow by the reviewer. However, there were some typos in the paper that caused misunderstandings. For example, in one instance, the paper mentioned that \"the input can either be pure noise latent features or a concatenation of conditional and noise latent features.\" The word \"conditional\" should have been replaced with \"conditions\". Also, the paper stated that \"the results of convergence speed and sample quality are presented in Figure 3 and Table 2, respectively.\" However, the actual result was shown in Figure 7.\n\n--Although the current quantitative analysis indicates that VDT performs better, the experimental results may not support this fact. Can you provide further analysis and results with improved quality?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission710/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698837757079,
        "cdate": 1698837757079,
        "tmdate": 1699635998010,
        "mdate": 1699635998010,
        "license": "CC BY 4.0",
        "version": 2
    }
]