[
    {
        "id": "pS9WH5HwaI",
        "forum": "iSAgvYhZzg",
        "replyto": "iSAgvYhZzg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2533/Reviewer_BNVX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2533/Reviewer_BNVX"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes Auto-UI, a multimodal solution that directly interacts with the interface, which eliminates the need for environment parsing or reliance on application-specific APIs. The authors introduce a chain-of-action technique that incorporates previous action histories and future action plans to guide the agent's decision-making process. The approach is evaluated using a device-control benchmark called AITW, which consists of 30,000 unique instructions covering tasks like application operation, web searching, and web shopping. Experimental results demonstrate that Auto-UI achieves state-of-the-art performance, with high accuracy in predicting action types (90%) and an overall success rate of 74%. The authors have made the code available for review."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed Auto-UI approach demonstrates a level of originality in addressing the challenges of autonomous user interface agents. By directly interacting with the interface instead of relying on environment parsing or application-specific APIs, it offers a novel solution that bypasses common inefficiencies and risks associated with existing approaches. The introduction of the chain-of-action technique also adds a unique element to the decision-making process of the agent.\n2. The approach is evaluated through experiments with the AITW benchmark. The inclusion of 30,000 unique instructions covering various multi-step tasks provides a comprehensive assessment of the Auto-UI system. Achieving a state-of-the-art performance demonstrates the effectiveness and reliability of the proposed solution.\n3. Overall, the paper is clear and easy to follow. The text provides a clear description of the challenges faced by existing approaches, introduces the Auto-UI solution, and explains the chain-of-action technique. The inclusion of experimental results contribute to a clear understanding of the proposed methodology and its performance.\n4. By addressing the challenges of inference inefficiency and error propagation, Auto-UI offers a more efficient and reliable approach to task automation. The multimodal solution and the elimination of environment parsing and reliance on application-specific APIs provide a significant advancement in the development of autonomous UI agents. Furthermore, the state-of-the-art performance achieved on the AITW benchmark showcases the practical applicability and potential impact of the proposed approach."
            },
            "weaknesses": {
                "value": "1. While the authors highlight the chain-of-action technique as a contribution, it appears to primarily concatenate the output actions, which can be confusing. It would be helpful to provide a more detailed explanation or clarification of how the chain-of-action technique enhances the decision-making process and contributes to the overall effectiveness of the Auto-UI approach.\n\n2. The experiment section lacks an explanation for the rationale behind selecting specific baselines. It would be valuable to include a justification for choosing the particular baselines used in the evaluation. Additionally, providing information on the performance of a GPT4 model, if available, would offer a useful benchmark to compare the performance of the proposed Auto-UI approach."
            },
            "questions": {
                "value": "GPT4 is reported to possess significantly improved agent capabilities compared to existing LLMs. However, it is important to note that the specific performance metrics and details of GPT4 have not been provided in the given context. Therefore, the performance of GPT4 remains unclear and unavailable for direct comparison in this discussion. What is the performance of GPT4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2533/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2533/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2533/Reviewer_BNVX"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2533/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698248307687,
        "cdate": 1698248307687,
        "tmdate": 1699636190100,
        "mdate": 1699636190100,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ydnTigJlJ8",
        "forum": "iSAgvYhZzg",
        "replyto": "iSAgvYhZzg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2533/Reviewer_CMSv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2533/Reviewer_CMSv"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a \u201cchain-of-action\u201d approach to tackle the autonomous web-searching agent problem. Specifically, they propose a multimodal framework that firstly encodes both the language goals and the web-interaction histories, as well as the screen images, into a combined representation, where a decoder will generate a look-ahead future action plan and a formatted immediate next action to perform.\nThe authors conducted experiments on the AITW dataset where an AI agent is tasked to interact with a Web UI following certain goals, where they demonstrate the effectiveness of the proposed models against three major baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed framework is claimed to be much lighter weight than methods that try to take the whole web information into textualized format for agents to comprehend.\n- The formatted action is sound and should be generalizable to other web-search domains.\n- The paper is pretty easy to follow, with illustrations onto the points.\n- The generalization ablation studies are helpful to gauge the capacity of the proposed framework."
            },
            "weaknesses": {
                "value": "- The paper does not describe much about the actual training details, in that sense, to me, the proposed method is still a kind of BC, where the target decoding is optimized towards mimicking the golden action sequences. (Unless some RL or other mechanism is used here, which is not described.) In my opinion, the novelties here mainly lie in the multimodal representations (both modality taken into account) and the format of the action performed.\n- I\u2019m a bit skeptical about the ICL baseline, first of all more details (e.g., how actions are represented, how OCRed results are used) of that baseline need to be described, at least in the appendix. Secondly, it also needs to be evaluated at the action plan level, my guess is that this method should be quite accurate on those but might fail more on the lower-level executions. Thirdly, it is indeed unfair simply because the model is not taking the images into account, which could be the key towards the success of the proposed method in this work. So, at least a multimodal version of it needs to be taken into consideration, or, a better spatial representation of the html syntax is required. (HTML can be many times too coarse to represent a spatial layout.)\n- Similar to above, the third baseline, fine-tuning LLMs, need to have a version with multimodal inputs.\n- An error analysis is required both on the quantitative and qualitative sides, what are the major errors that these models exhibit?"
            },
            "questions": {
                "value": "- I\u2019m a bit surprised that the language decoder is able to predict tokens as precisely as four decimal places, or is the actual precision here not important? I.e., could you not simply split image screens into patches and just use their centers as the coordinate representations? (And the more patches you grid the screen, the more precise it would be.)\n- What are the main types of errors observed by the proposed framework? And, does the framework provide good insights on how to assign these errors to specific modules? I.e., where should the improvements be?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2533/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698362800247,
        "cdate": 1698362800247,
        "tmdate": 1699636190011,
        "mdate": 1699636190011,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kltAhQRXQy",
        "forum": "iSAgvYhZzg",
        "replyto": "iSAgvYhZzg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2533/Reviewer_tvGC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2533/Reviewer_tvGC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an autonomous UI agent called Auto-UI that can interact in a multimodal UI environment without environment parsing or application-dependent API access. Specifically, it proposes a chain-of-action technique to help the agent make decisions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It is novel that the paper pays attention to the limitations in the real-world applications of autonomous agents and seeks to provide an agent that does not need extra intermediate environment parsing or interval application-dependent APIs.\n\n2. The paper proposes a chain-of-action technique which helps the agent to decide step-by-step."
            },
            "weaknesses": {
                "value": "1. The Figure 1 in this paper is somewhat not clear enough, making it difficult to understand the two paradigms in (a) and (b).\n\n2. The author does not provide a specific explanation of the Sandbox Paradigm and the First Principles Thinking Paradigm, which is confused. \n\n3. We find some grammar mistakes in the paper, for example, on page 2, paragraph 2, line 5, do you want to express inefficiency instead of efficiency?\n\n4. The authors don't explain exactly what touch_point, lift_point, etc. mean in the first place, causing some confusion.\n\n5. The authors do not provide a specific example between Auto UI and other baselines in Section 5, which is not clear to understand the effectiveness of the provided Auto UI."
            },
            "questions": {
                "value": "In Section 4.3, why do you use 14% instead of other number to evaluate the correction of a click action, could you provide some references?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2533/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698674099009,
        "cdate": 1698674099009,
        "tmdate": 1699636189925,
        "mdate": 1699636189925,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8rGkFYabUM",
        "forum": "iSAgvYhZzg",
        "replyto": "iSAgvYhZzg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2533/Reviewer_y4jm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2533/Reviewer_y4jm"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a multimodal work for Auto-UI, it proposes to leverage the chain-of-action (including previous history actions and future actions) for model prediction. Their model builds on the top of Llama 2 with an image encoder (for screen image). Empirical experiments on the AITW dataset shows very promising results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work proposes a chain of action operation, leveraging the action history and future actions for current action prediction.\n2. Based on Llama 2, it incorporates a pretrained image encoder into the pretrained LLM for action decision, and shows promising results on AITW dataset."
            },
            "weaknesses": {
                "value": "1. A potential weakness is where is the gain from? It looks PaLM and ChatGPT are pretty low on this dataset, while they only take text input, and BC models and Auto-UI models take image screen as input, and get very high results, it is unclear where is the gain from? image encoder? or a chain of action input?"
            },
            "questions": {
                "value": "I try to understand the setting of the experiments, and why the strong PaLM and ChatGPT baselines are so low. Based on the main Table 2, it looks the most gain is from the image encoder, right? Since PaLM-CoT and ChatGPT-CoT only take text input, and their performance is pretty low, and also similarly for Llama 2. Is this right? Probably needs a baseline/ablation to see the performance of model without image encoder."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2533/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698967002917,
        "cdate": 1698967002917,
        "tmdate": 1699636189782,
        "mdate": 1699636189782,
        "license": "CC BY 4.0",
        "version": 2
    }
]