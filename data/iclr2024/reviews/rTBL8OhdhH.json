[
    {
        "id": "pY3QPRGjze",
        "forum": "rTBL8OhdhH",
        "replyto": "rTBL8OhdhH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission243/Reviewer_VsZC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission243/Reviewer_VsZC"
        ],
        "content": {
            "summary": {
                "value": "The papers proposes a new method to perform dataset distillation under large IPCs. The authors made server observations regarding the trajectory matching process and found that easy patterns are learnt at early states and hard patterns are learnt at later stages. The authors also found that learning labels will help boost the distillation performances. Through gradually increasing the limit of matched expert trajectory epochs, the method achieves SOTA performances on large IPCs. Especially, when IPC increases to 500 or 1000 where all previous methods fail to perform better than random baseline, the proposed method is the first to be able to perform equivalently well as training on the whole dataset or better. The proposed method also achieves SOTA results on cross-architecture evaluations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. the method solves the problem that previous DD methods failed to solve: when IPC becomes large such as 500, their performances start to degrade to similar levels as random selection.\n2. The paper is well written and easy to follow.\n3. The paper introduced the observations gradually and solved the problems through carefully designed strategies. \n4. The comparison to baseline methods are extensive. \n5. The method is the first to scale to large IPCs such as 500, 1000 and achieves lossless performance on the 3 datasets.\n6. The paper also showed lots of insights for the reason community to gain a better understanding of Matching Training Trajectories(MTT) which is one of the recently proposed SOTA methods."
            },
            "weaknesses": {
                "value": "1. In table 1, can the authors indicate which DATM's results are achieved with label learning and which are not? Performance-wise, DATM's performance is very similar to other MTT based methods such as MTT itself with small IPCs, if soft label is used, does that mean DATM will cause MTT's performance to drop under smaller IPCs which is then compensated by soft label learning?\n\n2. Although mentioned briefly in 5.2, can the authors also specify the training resources such as number of GPUs and their type and the training cost in either 4.1 or appendix. This will provide a guidance for other researchers, especially on large IPCs and datasets. This leads to my next question\n\n3. Does this 1.05 times higher cost apply to all settings or it's just for CIFAR-10 (Sec 5.2 mentioned it's for CIFAR-10)? How about the extra training cost on other datasets?\n\n4. Some claims conflicts with the settings adapted by the method, e.g. in section 4.3, the authors first mention that \"We use logits generated by the pre-trained model to initialize soft labels\", then later in the same section, the authors claim again \"This is because using soft labels without optimizing them will enlarge the discrepancy between the training trajectories over the synthetic dataset and the original one, **considering the experts are trained with one-hot labels**\". If the later claim (considering the experts are trained with one-hot labels) is correct, shouldn't initialize the soft labels with a distribution close to one-hot have a smaller gap with expert training trajectories during the matching phase? The decision to use pre-trained model to initialize soft labels can be understood, however I think the explanation given later is confusing\n\n5. Have the authors tried to increase $T^{-}$ as well instead of just increasing the current upper?"
            },
            "questions": {
                "value": "See my questions above inside weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission243/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698615938925,
        "cdate": 1698615938925,
        "tmdate": 1699635949927,
        "mdate": 1699635949927,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "G3enWbgzmM",
        "forum": "rTBL8OhdhH",
        "replyto": "rTBL8OhdhH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission243/Reviewer_G6x3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission243/Reviewer_G6x3"
        ],
        "content": {
            "summary": {
                "value": "This work aims to achieve lossless dataset distillation by synthesizing a small synthetic dataset such that a model trained on this synthetic set will perform equally well as a model trained on the full, real dataset. Concretely, the author identified the problem of performance not being maintained as the size of the synthetic dataset grows. The authors propose a way to process early trajectories (easy patterns) and late ones (hard patterns) separately."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The author identified an important problem from an interesting perspective. The current dataset distillation algorithms cannot handle it well.\n\n2) The model performance on the synthesized dataset is better than that on the dataset synthesized by other dataset distillation methods."
            },
            "weaknesses": {
                "value": "1. The goal of this work is to achieve lossless dataset distillation. This requires a formal definition, what is lossless dataset distillation exactly?  Do models achieve the same performance on the original dataset as on the synthesized one?\n\n2. Why the model performance on the synthesized one is better than the one on the full dataset? Does that mean that the proposed whole dataset distillation works like a regularization method somehow?\n\n3. If a synthesized dataset is indeed lossless, a model with a totally different architecture can also perform well on the synthesized dataset. There lack of such experiments.\n\n4. This work improves previous work when the size of the synthesized dataset becomes larger. The lossless dataset distillation is an overclaim to me."
            },
            "questions": {
                "value": "1) Please explain why the performance across model architecture is so low.\n\n2) Please see my statements in weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission243/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698703594913,
        "cdate": 1698703594913,
        "tmdate": 1699635949824,
        "mdate": 1699635949824,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "D1m64lmZH8",
        "forum": "rTBL8OhdhH",
        "replyto": "rTBL8OhdhH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission243/Reviewer_8eCM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission243/Reviewer_8eCM"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel method of dataset distillation. It introduces the concept of difficulty-aligned trajectory matching, which controls the difficulty of the patterns generated on the synthetic data by matching different training phases of the expert model. \n\nOverall, the paper makes a substantial contribution to the field of dataset distillation, introducing a novel method and providing valuable insights. Despite the need for clarification on certain points and a deeper exploration into the method\u2019s performance across different scenarios, the paper stands out for its clarity, quality, and originality. With sufficient details provided for reproducibility, this work paves the way for further research and development in trajectory matching-based dataset distillation algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper provides a novel and intuitive insight into the effect of matching different training trajectories on the quality of the distilled dataset. The observation in the paper is insightful for the trajectory matching based dataset distillation. The paper is well-written and organized, with clear problem formulation, method description, and experimental setup. \n\nClarity, Quality, Novelty And Reproducibility:\nThe paper is clearly written, and the idea seems novel. The implementation details are provided for reproducing"
            },
            "weaknesses": {
                "value": "Please refer to the Questions."
            },
            "questions": {
                "value": "I have a few questions and concerns about the method:\n\nQ1. Manually setting lower bound and upper bound for the matching range is somehow incremental.\n\nQ2. From Table 2a, the performance of \u201ccomplex\u201d networks is worse than 3-layer convolutional networks. While in table 2, the performance of \u201ccomplex\u201d networks is better than the 3-layer convolutional networks. It would be interesting to see the performance across architectures with the increasing number of IPC.\n\nQ3. How is the synthetic dataset performance on the downstream tasks, such as object detection?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission243/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission243/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission243/Reviewer_8eCM"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission243/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790614500,
        "cdate": 1698790614500,
        "tmdate": 1700498929525,
        "mdate": 1700498929525,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "o24HDaHErG",
        "forum": "rTBL8OhdhH",
        "replyto": "rTBL8OhdhH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission243/Reviewer_5Mev"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission243/Reviewer_5Mev"
        ],
        "content": {
            "summary": {
                "value": "As a compression algorithm, it remains unclear whether dataset distillation can be used to fully recover the full training accuracy of original dataset. The authors in this paper propose to address this issue through specifically aligning the difficult trajectories using part of the data. The experiment result show promising results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The proposed framework is quite well motivated, and the experiment results aligns well with the observation\n+ In the experiments, the authors are the first to demonstrate that distilled synthetic data can outperform real training dataset\n+ The authors perform extensive studies and analysis on the algorithm"
            },
            "weaknesses": {
                "value": "- The algorithm is extending MTT, although important, but there is not much algorithmic novelty\n- In table 4, authors only compared with some simple baselines for cross-architecture generalization. It would be great if the authors can have a more thorough comparison with other dataset distillation algorithms as well."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission243/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797531228,
        "cdate": 1698797531228,
        "tmdate": 1699635949648,
        "mdate": 1699635949648,
        "license": "CC BY 4.0",
        "version": 2
    }
]