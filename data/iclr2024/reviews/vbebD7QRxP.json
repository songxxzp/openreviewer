[
    {
        "id": "Hdaltgi7cm",
        "forum": "vbebD7QRxP",
        "replyto": "vbebD7QRxP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8025/Reviewer_LB1R"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8025/Reviewer_LB1R"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method to modularly train deep generative models to perform causal inference in high dimensional data"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is fairly well written and tackles and interesting problem\n- To the best of my knowledge this is a fairly novel approach on training Neural Nets for causal inference \n- The maths appear to be sound \n- The results and evaluation are acceptable"
            },
            "weaknesses": {
                "value": "- There is a large number of related literature that is missing. There have been too many to enumerate here methods that would be considered related work and should be compared against\n- The Covid chest x-ray dataset is not of high quality , the reviewer would suggest using a more established test dataset.\n- There are multiple typos (trianing -> training) and articles missing \n- The method section could be made a bit better to increase clarity"
            },
            "questions": {
                "value": "- GANs are notoriously messy and hard to train , plus they can only approximate the data distributions in question. Why is this the chosen backbone and not something else like normalizing flows ? \n\n\nOverall i think this is a good paper, that would benefit the community"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8025/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8025/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8025/Reviewer_LB1R"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8025/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698056641898,
        "cdate": 1698056641898,
        "tmdate": 1700412589214,
        "mdate": 1700412589214,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ep3vJmT77i",
        "forum": "vbebD7QRxP",
        "replyto": "vbebD7QRxP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8025/Reviewer_g68Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8025/Reviewer_g68Y"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed a new modular training algorithm to learn deep causal generative models, given a known causal graph and observational dataset. The method proceeds via factorizing the full joint distribution into its different c-components in the form of $\\mathcal{L}_2$ distributions, and then match each components sequentially leveraging rule 2 of do-calculus. In this way, the authors suggest that in certain cases, pre-trained model might be utilized to improve convergence of the learning process. For example, when certain c-component is a high-dimensional node like images, one can simply plugin a pre-trained generative model. Finally, this enables the identification of interventional or counterfactual queries in the presence of high-dimensional variables."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper is built upon well-known results in causality and proposed a modular training method for deep causal models, which, to the best of my knowledge, is a new contribution to the field. This might have potential use cases in learning realistic counterfactual data generation.\n- For certain cases demonstrated in the paper, the proposed method achieved good performance in terms of faster convergence and better identification quality."
            },
            "weaknesses": {
                "value": "My primary reservations regarding this work stem from the possibility that the good performance demonstrated in a few hand-crafted scenarios may not extend to more general cases involving high dimensional variables.\n\n- More specifically, in all the experiments presented in the paper, the underlying causal graph fortuitously satisfies the condition wherein the solitary high-dimensional nodes are isolated c-components. However, in more general cases, such as the example depicted in Figure 2, the method would still need to match $p(Z_1, Z_2,Z_3|X_1)$ and $p(X_1, X_2, Z_1, Z_3)$, which does not significantly simplify the original problem of matching $p(X_1, X_2, Z_1, Z_2, Z_3)$, especially considering the added complexity of the proposed algorithm.\n\n- Furthermore, even in the scenarios where the proposed method was successful (experiments 1 and 2), the authors neglected to provide a comparison with an evident baseline: initializing the neural net architecture of the corresponding image node using a pre-trained marginal distribution, followed by fine-tuning the joint model in an NCM fashion. This baseline could serve as a robust ablation study to justify the necessity of c-component decomposition, a critical aspect that is currently absent from the current paper.\n\n- Lastly, the paper's problem setting requires access to the true causal graph, which is a reasonable assumption. However, in many scenarios where high-dimensional deep causal models are required, the true causal graph may not be known. This limitation could potentially restrict the applicability of the proposed method. In contrast, the joint training approach can easily adapt to unknown graph settings, thereby offering more flexibility."
            },
            "questions": {
                "value": "My questions and concerns are listed above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8025/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698757282811,
        "cdate": 1698757282811,
        "tmdate": 1699636990158,
        "mdate": 1699636990158,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NzIYHBRjoM",
        "forum": "vbebD7QRxP",
        "replyto": "vbebD7QRxP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8025/Reviewer_UCfX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8025/Reviewer_UCfX"
        ],
        "content": {
            "summary": {
                "value": "This paper deals with factorization of joint distributions over variables when there are latent and high dimensional variables exist and how to use pre-trained neural networks to model complex distributions among variables. \n\nThe key is to determine which subgraphs can be modeled by one NN (to learn distribution) and in which order. The results are not surprising given existing results on interventional distribution and the factorization of ADMG, but it is novel in a way to combine with generative models and handle high-dimensional variables."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The method is novel in a way to combine with generative models and handle high-dimensional variables."
            },
            "weaknesses": {
                "value": "Some potential issues or weakness:\n    1. Lack of discussions on assumption 3 and their violation, and some assumptions are hidden in the text besides assumption 1 to 3. \n    2. What is the time efficiency of various approaches?"
            },
            "questions": {
                "value": "Please see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8025/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699116588941,
        "cdate": 1699116588941,
        "tmdate": 1699636990058,
        "mdate": 1699636990058,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1xWPr6CRwc",
        "forum": "vbebD7QRxP",
        "replyto": "vbebD7QRxP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8025/Reviewer_Zypu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8025/Reviewer_Zypu"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an algorithm, WhatIfGAN, for modular learning of generative models for that can then be used for causal inference. WhatIfGAN identifies so-called H-groups of variables from a for which the do-calculus rule-2 holds and allows for separate training of generative models for the different groups. The paper compares the performance of WhatIfGAN to NCMs as well as a GAN without modular training. The results seem to indicate that WhatIfGAN generates superior high-dimensional counterfactuals.\n\n[1] Kevin Xia, Kai-Zhan Lee, Yoshua Bengio, and Elias Bareinboim. The causal-neural connection: Expressiveness, learnability, and inference. Advances in Neural Information Processing Systems, 34:10823\u201310836, 2021."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper tackles the interesting and important problem of high-dimensional causal estimation with unobserved confounding. The authors propose a sound method for modularising the training of deep generative models which allows for efficient estimation of those causal queries. The experiments seem to indicate superior performance to relevant baselines."
            },
            "weaknesses": {
                "value": "The presentation of the paper could be improved including the presentation of the results and choice of experiments. The notation is inconsistent and changes from $Pa(..)$ to $pa(...)$ or from $P(...\\mid do(x=..))$ to $P_x(...)$. The evaluation relies heavily on visual comparison of sampled data rather than quantifiable metrics. I suggest having a look at [1] for ideas of how to quantify counterfactual estimation capabilities for the CovidX dataset. The synthetic MNIST dataset can be validated against ground-truth distributions and samples using the TVD as done, or likelihood measurements or the likes. Also MSEs for ATE differences or CFs could be interesting.\n\n[1] Monteiro, Miguel, et al. \"Measuring axiomatic soundness of counterfactual image models.\" The Eleventh International Conference on Learning Representations. 2022."
            },
            "questions": {
                "value": "- Could this be achieved without modular training but by enforcing the noise to be the same between confounded variables? E.g. training $p(z_3)$ using an invertible function to find the noise to pass into training $p(z_1\\mid z_3)$?\n- It would be helpful to better introduce D and A in the MNIST use-case.\n- I would encourage to also test the method on a more complicated synthetic dataset with more than 3 nodes.\n- The paper would benefit from an introduction of TVD."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8025/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699366214706,
        "cdate": 1699366214706,
        "tmdate": 1699636989914,
        "mdate": 1699636989914,
        "license": "CC BY 4.0",
        "version": 2
    }
]