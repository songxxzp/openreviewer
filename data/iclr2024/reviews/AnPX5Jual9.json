[
    {
        "id": "E2P43idcId",
        "forum": "AnPX5Jual9",
        "replyto": "AnPX5Jual9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission608/Reviewer_5eVE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission608/Reviewer_5eVE"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a rotative factorization machine method, which represents each feature as a polar angle in the complex plane and converts the feature interactions into a series of complex rotations. The authors design a self-attentive rotation function that models the rotation coefficients through a rotation-based attention mechanism and incorporate a modulus amplification network to learn the modulus of the complex features to enhance the representations. The experiments were conducted on five datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method can handle a large number of feature interactions.\n\n2. The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "The experiment is limited on AUC and LogLoss. How about other machine learning tasks?  \nIn recent machine learning tasks, we are often using AUPRC, instead of AUC.\nThe improvement in experiments is marginal."
            },
            "questions": {
                "value": "Please check the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission608/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698646061655,
        "cdate": 1698646061655,
        "tmdate": 1699635988437,
        "mdate": 1699635988437,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sHx8fUtZXm",
        "forum": "AnPX5Jual9",
        "replyto": "AnPX5Jual9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission608/Reviewer_LHqT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission608/Reviewer_LHqT"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a Rotative Factorization Machine (RFM), where feature interactions are converted into a series of complex rotations to facilitate arbitrarily-order feature interaction learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Learning arbitrary order feature interaction with complex rotations is interesting."
            },
            "weaknesses": {
                "value": "- The proposed method is similar to EulerNet with a stack of attention module.\n- The Theorem is not properly defined, used and explained.  \n- There is no detail about dataset train/test split. The paper refers to another paper which refers to another paper where I still could not find clear defined split. Benchmarking is not rigorous and is hard to tell whether the proposed method is better or not in terms of performance, especially there are given prior work for discussion [1,2]. Apart from that, the improvement on ML-1M, ML-Tag, Frappe is marginal, and from my experience the improvement could be a result of randomness. \n\n[1] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, Xiuqiang He. Open Benchmarking for Click-Through Rate Prediction. The 30th ACM International Conference on Information and Knowledge Management (CIKM), 2021.\n\n[2] Jieming Zhu, Quanyu Dai, Liangcai Su, Rong Ma, Jinyang Liu, Guohao Cai, Xi Xiao, Rui Zhang. BARS: Towards Open Benchmarking for Recommender Systems. The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2022."
            },
            "questions": {
                "value": "In addition to the weakness above, I have following questions: \n1) \"To our knowledge, it is the first work that is capable of learning the interactions with arbitrarily large order adaptively from the corresponding interaction contexts.\" Isn't this also done by [1]?\n2) What is the training time for the proposed model? How does it compare to other models?\n3) How is the theorem useful/helpful? Since it gives an asymptotic property of $\\Delta_{RFM}$ to $\\Delta_{R}$, but\n - There is no reason for $\\Delta_{R}$ to be better than $\\Delta_{RFM}$, why we approximate it?\n - If $\\Delta_{R}$ is indeed better, why not directly use $\\Delta_{R}$ to capture feature interaction?\n - Lemma A.1 holds when you assume the embeddings are random vectors. They however are not. \n\n4) Can the proposed method be interpreted as using dual embedding vectors for one feature, with the constraint that they have unit norm in one dimension? \n\n\n[1] Weiyu Cheng, Yanyan Shen, and Linpeng Huang. Adaptive factorization network: Learning adaptive-order feature interactions. In Proceedings of the AAAI Conference on Artificial Intel- ligence, volume 34, pp. 3609\u20133616, 2020.\n\n\n==================================\n\nThanks for the response. I tend to keep my score because:\n\n1. the model still is too similar to EulerNet, despite it replaces the linear block with an attention block. The claims regarding arbitrarily large order and exponential explosion issue are not supported. It could be redundant to separately learn modulus and phase. If exponential explosion issue occurs, I couldn't see how such scheme could help. The modulus could still be too large;\n\n2. the Theorem does not support the model well. A simple MLP would do the same as a universal approximator, without all these loose/inappropriate bounds, under which I think related work could be as powerful as the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission608/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission608/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission608/Reviewer_LHqT"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission608/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698727435803,
        "cdate": 1698727435803,
        "tmdate": 1701045965804,
        "mdate": 1701045965804,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "q1roykKMJq",
        "forum": "AnPX5Jual9",
        "replyto": "AnPX5Jual9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission608/Reviewer_f3tV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission608/Reviewer_f3tV"
        ],
        "content": {
            "summary": {
                "value": "This paper present the RFM  to learn arbitrary orders for the CTR task. Mathematical proofs, extensive experimental results are provided to show the properties and advantages of this work."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This is a novel, complete, well-presented paper. Specifically, it formulates the feature interaction from a new perspective, representing features as a polar angle in the complex plane. Extensive experiments verify the effectiveness of RFM on 5 datasets. Source code is also provided."
            },
            "weaknesses": {
                "value": "1. Some of the concepts are different from the existing CTR papers, will be good to be consistent or make it clearer. For example, the field-aware means each feature has multiple field-aware representations [1]. The term \"relation-aware\" is more like \"feature-aware order\" to me, which indicates the order information a is depends on the indicator of input features. This is something similar to the second-order feature interaction in the existing papers. In other words, the feature interaction information in the existing papers is treated as order information in this paper. Compared with AFN [2], the order information in AFN is not explicitly modled.  \n\n[1] Field-aware Factorization Machines for CTR Prediction, RecSys 16.\n[2] Adaptive Factorization Network: Learning Adaptive-Order Feature Interactions, AAAI 2020\n\n2. The motivation of the projecting features to the complex plane is not clear. Take AFN [2] as an example, it has similar capability to model arbitrary orders. What the diffidence between the existing solutions (e.g., Logarithmic Transformation) and yours should be discussed."
            },
            "questions": {
                "value": "1. Is equation 3 a learning function for 1 feature interaction? Based on the equation and figure 2, it looks like they are an illustration on how to learn a specific feature interaction in the context of feature j. Because only the pairwise relationships between feature j and other features are modeled  (i.e., x_j and x_1, x_j and x_l, and x_j and x_m), which are represented as a_{j,i}, a_{j,l}, and a_{j,m} respectively. \nHow to model the relationship a_{1,m} between x_1 and x_m in this case? How to indicate the number of feature interactions?\nIn my opinion, you might be referring to the Equ 7 in  AFN [1], which is the output of the jth neuron. If yes, how to define the number of neurons (i.e., feature interactions)?\n\n[1] Adaptive Factorization Network: Learning Adaptive-Order Feature Interactions, AAAI 2020.\n\nI would like to increase the rating if the authors can solve my concerns. Thanks for the great efforts, I enjoy reading this paper!"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N.A."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission608/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission608/Reviewer_f3tV",
                    "ICLR.cc/2024/Conference/Submission608/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission608/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698835471960,
        "cdate": 1698835471960,
        "tmdate": 1700741141748,
        "mdate": 1700741141748,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7lAvz4jLDX",
        "forum": "AnPX5Jual9",
        "replyto": "AnPX5Jual9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission608/Reviewer_7fiD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission608/Reviewer_7fiD"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new model, named RFM (Rotative Factorization Machine), that can capture higher order feature interactions. Instead of directly using feature embeddings, RFM applies exponential operator over the embedings, called angular representation of features. With that, the cross of features (say e^x*e^y*e^z) becomes the sum of transformed features (say, e^{x+y+z}). The authors employs transformer-like self-attention structures in RFM. RFM comes with a subnetwork which does modulus amplification for better capture of complex feature interactions. The authors conduct experiments over several datasets and showed RFM beats other state-of-the-art models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* RFM has some good features for capturing feature interaction. It can compute arbitrary high-order feature interactions with help of the angular representation of features. It models feature interaction with rotating vectors in a high dimensional space.  \n* From the empirical evaluation results, the modulus amplification subnetwork does help RFM's overall performance a lot."
            },
            "weaknesses": {
                "value": "* RFM is positioned as a better model for capturing arbitrary high order complex feature interactions. A complex MLP can also do that. One key question is whether the proposed model is good at capturing high-order complex features, and how it achieves that. I don't really see too many discussions or analysis over this direction. In the paper's current form, I doubt readers would be convinced that those hypothetical complex vector rotations are the keys to capture feature interactions better.\n* Section 4 doesn't involve enough evidence that RFM is capturing feature interactions more efficiently, other than showing it has better prediction performance."
            },
            "questions": {
                "value": "* See my comments in the weakness section. I wonder if there could be more supporting discussions/materials that show how RFM's angular representations/rotation-based attention/modulus amplification are helping to better capture complex feature interactions. One thing that might help to convince the readers is to generate some synthetic data to show complex feature interactions are degenerated to simpler ones after these transformations.\n* Question regarding the angular representations. Theoretically, the embedding layers could learn the exponential transformations. Say instead of x_j --> \\theta_j (=E(x_j)) --> {\\tilde e}_j (=e^{i\\theta_j}, there could be a different embedding (\\hat E) such that \\hat E(x_j) = e^{iE(x_j)}. Why does explicitly adding this exponential transformation help? What's the hidden cost of it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission608/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698896905129,
        "cdate": 1698896905129,
        "tmdate": 1699635988209,
        "mdate": 1699635988209,
        "license": "CC BY 4.0",
        "version": 2
    }
]