[
    {
        "id": "8RbgGUE2Y6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3813/Reviewer_p8Zy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3813/Reviewer_p8Zy"
        ],
        "forum": "mhtD74Jgyw",
        "replyto": "mhtD74Jgyw",
        "content": {
            "summary": {
                "value": "The paper introduces an anomaly detection method for the video domain, which combines velocity (via FlowNet), pose (via AlphaPose) and deep features (via CLIP) extracted from objects detected by a pre-trained object detector (Mask R-CNN). The authors perform experiments on three video databases (Avenue, ShanghaiTch, Ped2) to compare the proposed method with several state-of-the-art methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The proposed method obtains very good results.\n- The paper is easy to follow."
            },
            "weaknesses": {
                "value": "- The proposed method seems to be a simple combination of existing (even pre-trained) components. Therefore, in my opinion, the technical novelty is very limited.\n- The pose features can only be extracted for humans. It is not clear what happens if the object is not a human.\n- The authors should relate to existing interpretable video anomaly detection methods, e.g. [1, 2].\n- \"It is surprising that our simple velocity and pose representations achieves state-of-the-art performance on the largest and most complex VAD dataset, with 85.9% AUROC in ShanghaiTech\" => This statement is incorrect. ShanghaiTech is definitely not the largest database, e.g. XD-Violence or UCF-Crime are much larger. Moreover, the simple approach does not seem to work all by itself (it still needs to be coupled with deep features). In the end, it would be interesting to see if the simple approach proposed by the authors would work on these truly large-scale datasets.\n- On Avenue, there are some methods obtaining better results (see [3]). These should be added to Table 1.\n- Some recent works used RBDC/TBDC to estimate localization performance. It would be interesting to see how the proposed method is evaluated against RBDC/TBDC.\n- The proposed method seems to use may heavyweight systems. However, the processing for video anomaly detection is expected to happen in real-time at the standard video FPS rate, e.g. 25/30 FPS. The method seems to be twice as heavier.\n- The interpretability of the method seems rather shallow, i.e. it only indicates if the anomaly is related to pose, velocity or deep features. If deep features are the cause, we are back to square one.\n- Typo fixes:\n  - \"VAD. this result underscores\" should be \"VAD. This result underscores\".\n  - \"It takes my method\" should be \"It takes our method\"\n  \n[1] Doshi, Keval, and Yasin Yilmaz. \"Towards interpretable video anomaly detection.\" In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2655-2664. 2023.\n\n[2] Wang, Yizhou, Dongliang Guo, Sheng Li, and Yun Fu. \"Towards Explainable Visual Anomaly Detection.\" arXiv preprint arXiv:2302.06670 (2023).\n\n[3] https://paperswithcode.com/sota/anomaly-detection-on-chuk-avenue"
            },
            "questions": {
                "value": "- How does the time of the proposed method, which is based on k-NN, scale with the size of the training data? It would be nice to see a graph."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3813/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697456771329,
        "cdate": 1697456771329,
        "tmdate": 1699636338854,
        "mdate": 1699636338854,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "q8xAslIEJl",
        "forum": "mhtD74Jgyw",
        "replyto": "mhtD74Jgyw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3813/Reviewer_W9ta"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3813/Reviewer_W9ta"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a semantic attribute-based approach for improving the accuracy and interpretability of video anomaly detection. The method represents each object in each video frame in terms of its speed and pose, and uses a density-based approach to compute the anomaly score. The method achieves state-of-the-art performance on three publicly available video anomaly detection datasets, while also providing information on why anomalies occur."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1) The paper introduces a simple but effective attribute-based method for video anomaly detection. The use of attribute-based representations allows for interpretability and understanding of the reasoning behind the system's decisions.\n2) The proposed method achieves state-of-the-art performance on three commonly used video anomaly detection datasets, Ped2, Avenue, and ShanghaiTech. This demonstrates the effectiveness of the method in accurately detecting anomalies."
            },
            "weaknesses": {
                "value": "1)The method does not make full use of the temporal information in the video, but only extracts features based on a single frame or two adjacent frames. This may result in ignoring anomalous events that need to take into account long-term dynamic changes, such as the sudden disappearance or appearance of people. The authors should consider using more frames to construct a feature representation of the object, or using some model that can capture timing dependencies, such as recurrent neural networks or transformers.\n2) The method uses a pre-trained CLIP encoder as a deep feature extractor, but no specific experimental details or rationale are given. The authors should explain why CLIP was chosen over other image encoders and how the CLIP encoder was fine-tuned or adapted for the video anomaly detection task. In addition, the authors should have given the parameter settings and performance comparison of the CLIP encoder on each dataset.\n3)The method uses a fixed threshold to determine whether each frame is anomalous or not, but does not explain how this threshold is determined. The authors should give the method of threshold selection and sensitivity analyses, as well as the threshold values taken on different datasets. In addition, the authors should explore the possibility of using adaptive thresholds to accommodate variations in different scenes and objects.\n4)There are some references in the article that are not given in a standardised format. For example, in the last sentence of the third paragraph on the second page, \"In practice, we used AlphaPose (Fang et al., 2017), which we found to work well.\", the specific citation of AlphaPose should be given; in the first sentence of the first paragraph on page 4."
            },
            "questions": {
                "value": "1)Can you explain why you chose the CLIP encoder?\n2)How the CLIP encoder was fine-tuned or adapted on the video anomaly detection task?\n3) Can you give the selection criteria and sensitivity analyses for giving thresholds?\n4)How did you determine the contribution of each feature to the final anomaly score t? Did they all contribute equally?\n5)Can you give the parameter settings and performance comparison of CLIP encoder on various datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3813/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678251602,
        "cdate": 1698678251602,
        "tmdate": 1699636338757,
        "mdate": 1699636338757,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CoOXaFlUKI",
        "forum": "mhtD74Jgyw",
        "replyto": "mhtD74Jgyw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3813/Reviewer_EZHR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3813/Reviewer_EZHR"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a simple yet powerful method for Video Anomaly Detection (VAD) using attribute-based representations. Objects in video frames are represented by their velocity and pose, and anomalies are detected using a density-based approach. This straightforward technique achieves state-of-the-art performance on challenging datasets like ShanghaiTech, with AUROC scores of 99.1%, 93.6%, and 85.9% on Ped2, Avenue, and ShanghaiTech respectively."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Originality: Introduces a novel, simple attribute-based approach to Video Anomaly Detection (VAD), focusing on velocity and pose.\n\n2. Quality: Achieves state-of-the-art results on challenging datasets, backed by a comprehensive ablation study.\n\n3. Clarity: Presents ideas and results in a clear, accessible manner."
            },
            "weaknesses": {
                "value": "1. Dataset Specificity:\nThe proposed method seems tailored specifically to the peculiarities of the three test datasets (Ped2, Avenue, and ShanghaiTech). It effectively distinguishes between certain object types like bicyclists and pedestrians but may not be as effective in more varied scenarios.\n\n2. Limited Attribute Scope:\nWhile the addition of velocity and pose attributes improves performance on these specific datasets, it's questionable how this approach would fare in more complex and diverse datasets, such as UCFCrime.\n\n3. Lack of Generalization:\nThe method demonstrates limited generalizability beyond the tested scenarios. Its effectiveness in other types of anomaly detection tasks, particularly in industrial settings or datasets with different anomaly types, remains unproven.\n4. Narrow Inspirational Scope:\nDue to its dataset-specific nature and limited attribute scope, the method offers minimal inspiration or methodological guidance for broader anomaly detection challenges. The findings might not be readily applicable or extendable to other contexts or more intricate anomaly detection tasks."
            },
            "questions": {
                "value": "See the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3813/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773686477,
        "cdate": 1698773686477,
        "tmdate": 1699636338661,
        "mdate": 1699636338661,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "A1e3r1fuiX",
        "forum": "mhtD74Jgyw",
        "replyto": "mhtD74Jgyw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3813/Reviewer_fxdo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3813/Reviewer_fxdo"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a video anomaly detection method that achieves state-of-the-art performance while also being interpretable. The key contributions are:\n\n- The method represents each object using two semantic attributes: velocity and human pose. These allow automatic interpretation of anomaly decisions based on whether the velocity or pose are unusual. \n\n- Despite the simplicity of the representations, the method achieves SOTA results on the three most commonly used VAD datasets: Ped2, Avenue, and ShanghaiTech. It surpasses prior work substantially on ShanghaiTech, the largest and most complex dataset.\n\n- The velocity and pose representations are complemented with an implicit deep representation to model residual attributes not captured by velocity/pose. This achieves the best of both interpretable attributes and powerful deep features.\n\n- The method is easy to implement, requiring only off-the-shelf components like object detectors and optical flow.\n\n- Comprehensive experiments demonstrate the effectiveness of the approach, including ablation studies and visualization of anomaly decisions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Originality:** The idea of using semantic attributes like velocity and pose as representations for anomaly detection is novel and has not been explored before. Prior work relied on less interpretable features like reconstruction errors or deep embeddings. Representing objects by velocity and pose provides inherent interpretability.\n\n**Quality:** The method achieves impressive results, outperforming prior art substantially on the largest VAD dataset, ShanghaiTech. The ablation studies provide insight into the relative importance of the different components. The experiments are comprehensive and compare favorably to a large number of prior approaches.\n\n**Clarity:** The paper is well-written and easy to follow. The methodology is clearly explained, with helpful diagrams illustrating the components. The results are presented in a clear format with tables comparing to prior work. \n\n**Significance:** The ability to provide interpretable rationales for anomaly decisions is critical for deploying VAD in real-world applications like surveillance. Opaque neural networks can make mistakes that humans cannot understand. This work shows that interpretable semantic attributes can be surprisingly effective for VAD. The significance goes beyond pushing SOTA - it demonstrates that interpretability does not require sacrificing accuracy."
            },
            "weaknesses": {
                "value": "- More analysis could be provided on failure cases to understand limitations of the approach. Where does the method break down? What types of anomalies are missed?\n\n- The motivations for some design choices are unclear. For example, why are velocity and pose specifically chosen as the semantic attributes? How were hyperparameters like number of velocity bins selected?\n\n- The method relies on off-the-shelf components like optical flow and object detection. How robust is it to errors in these modules? Could end-to-end training help?\n\n- More analysis could be provided understanding what concepts are captured by the deep representation versus the attributes. Are they complementary?"
            },
            "questions": {
                "value": "Please see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3813/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834209940,
        "cdate": 1698834209940,
        "tmdate": 1699636338592,
        "mdate": 1699636338592,
        "license": "CC BY 4.0",
        "version": 2
    }
]