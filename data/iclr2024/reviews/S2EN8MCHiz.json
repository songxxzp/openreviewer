[
    {
        "id": "LOA815RomI",
        "forum": "S2EN8MCHiz",
        "replyto": "S2EN8MCHiz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4796/Reviewer_sCrT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4796/Reviewer_sCrT"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an empirical investigation into the intrinsic dimension (ID) of multi-modal models. Traditionally, ID has been analyzed in uni-modal models to measure the utilization of a $D$-dimensional representation space. This study extends such analysis to multi-modal contexts, and analyzes the BLIP visual language model on the MS-COCO dataset. The paper finds that 1) higher IDs are observed in the visual modality compared to the language modality; 2) cross-modal attention struggles to align low-dimensional language representations with their visual counterparts; 3) IDs can be useful indicators of weight importance during model pruning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- This is a pioneering study on intrinsic dimensions within multi-modal models, potentially offering valuable theoretical insights.\n- The findings are interesting, particularly in highlighting the higher intrinsic dimension of the visual modality compared to language, the usefulness of ID in determining layer significance, and the greater sensitivity of the visual modality to pruning."
            },
            "weaknesses": {
                "value": "- The study is limited to one specific vision-language model (BLIP), raising concerns about the generalizability of the conclusions. It remains unclear if these findings hold across diverse multi-modal models with different training objectives, such as discriminative contrastive models like CLIP, generative models with trainable text decoders like LLaVA/OpenFlamingo, or models involving more modalities like ImageBind.\n\n- The paper's presentation is poor regarding writing style and organization. It frequently introduces terms and acronyms without sufficient explanation (e.g., intrinsic dimension, TwoNN algorithm, BLIP, Magnitude/Sensitivity pruning), potentially confusing readers less familiar with the subject. The relevance of certain sections, such as the comparison of intrinsic dimensions between Transformers and CNNs in Section 3.1, is out of scope. Additionally, some sections are purely hypothetical without empirical support, and the overall presentation lacks a cohesive message.\n\n- The implications of the findings on the advancement of multi-modal training techniques are not discussed, lacking a broader context that could enhance the paper's impact."
            },
            "questions": {
                "value": "- The paper often inconsistently uses \\citet and \\citep. \n- In Figure 1, it's unclear which parts represent the vision modality and the language modality.\n- The meaning of \"im\" and \"op\" in Figure 3 is unclear. Could you define these terms?\n- On Page 7, the statement \"ID values have a positive correlation with model performance but not in direct proportion\" requires more detail and further elaboration."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4796/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4796/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4796/Reviewer_sCrT"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4796/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698651354248,
        "cdate": 1698651354248,
        "tmdate": 1699636462313,
        "mdate": 1699636462313,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Q6ZcOKHYr6",
        "forum": "S2EN8MCHiz",
        "replyto": "S2EN8MCHiz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4796/Reviewer_Vq7s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4796/Reviewer_Vq7s"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the intrinsic dimension (ID) of a large-scale vision-language pre-training model and explores the relationships between ID, modality, and prunability. The authors find that the geometric characteristics of visual and language representations differ significantly, resulting in distinct prunability for each modality. They propose an importance metric based on ID for multimodal model pruning, which yields superior performance. The experimental results show that visual representations are more sensitive to pruning, while language representations are more robust."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors propose to investigate the intrinsic dimension (ID) of a large-scale vision-language pre-training model called BLIP and explore the relationships between ID, modality, and prunability.\n2. This paper applies ID to multimodal scenarios(i.e., language and vision) and propose an importance metric based on ID for multimodal model pruning, which yields superior performance.\n3. This article conducts detailed experiments and the experimental results support their claims."
            },
            "weaknesses": {
                "value": "1. In Figure 2 1), the VLP is a transformer-based model, but its hunchback-shaped profiles is not significant, the author should explain the reason.\n2. Is w/o retrain in Figure 4 w/o finetuning? Why not just use w/o finetuning?\n3. This paper argues that the maximum ID is a more critical indicator for performance prediction, which is against the observation that the ID of the last latent layer indicates model performance, but from Figure 4, the ID of the last latent layer can also indicate model performance.\n4. In Figure 5, why the multiplication of IDs lead to the IDs of pure language representations decrease?"
            },
            "questions": {
                "value": "See Weaknesses Part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4796/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698674502100,
        "cdate": 1698674502100,
        "tmdate": 1699636462199,
        "mdate": 1699636462199,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fo4JZFaf6I",
        "forum": "S2EN8MCHiz",
        "replyto": "S2EN8MCHiz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4796/Reviewer_RpzU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4796/Reviewer_RpzU"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the cooperation and the utility of each modality in multimodal representation learning, specifically the intrinsic dimension (ID) of a large-scale vision-language pre-training model BLIP and its implications on layer importance, modality importance, and prunability.  Several new ideas are proposed based on this framework including identifying shortcomings of embedding modalities into the same low-dimensional manifold, studying the contribution of different modalities, predicting model performance, and a new method for multimodal model pruning (for which some experimental results are presented)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem of better understanding multimodal models, particularly vision-language models, is important to this generally empirical field. This paper makes some nice contributions to this study.\n2. The idea of ID is interesting and the implications also have potential for analyzing and improving multimodal models.\n3. There are some experiments on pruning multimodal models which can be quite useful."
            },
            "weaknesses": {
                "value": "1. The biggest issue with this paper is that it tries to do too much and ends up overclaiming on many fronts. Dissecting each of the claims in the abstract:\n\n---- Empirical study of ID: There is a good amount of discussion and experiments for this, which is good. But it is mostly about applying TWONN method for computing ID. Also, how do you know that the IDs computed are accurate? Is there an evaluation metric for the quality of ID?\n\n---- Studying modality contribution: I do not see this experiment at all, only some anecdotal statistics in section 3.\n\n---- Predicting model performance using ID values: I do not see this experiment at all, nor is this mentioned subsequently in the paper.\n\n---- Better pruning: There are experiments for this in tables 1 and 2, but there are no comparisons to established weight pruning methods, only 1 from Sens Zhang et al. (2022). More comparisons are needed to really prove the efficacy of this part.\n\nOverall, the paper would be well-suited from reducing the number of sub-claims/sub-applications and just focus on doing 1 or 2 really well.\n\n2. Section 3 needs work - there are some interesting results but it can be better phrased as well-motivated research questions. Taking '3.3 INTERPRETING CROSS-MODAL ATTENTION VIA IDS' as an example, why is interpreting cross-modal important? Why should I use ID to do it when other people have used attention weights etc.? What insights does using ID to interpret cross-modal tell me, can I use it to better train or debug models? See https://arxiv.org/abs/2207.00056 for an example of setting up what to interpret in multimodal models, and using rigorous human user-studies to validate each of the findings.\n\n3. It would be good to have an overall plot of performance vs parameters, with 1 line being your pruning method and other lines for other pruning baselines, and the line that pareto dominants would be best."
            },
            "questions": {
                "value": "see weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4796/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811355819,
        "cdate": 1698811355819,
        "tmdate": 1699636462078,
        "mdate": 1699636462078,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "m7yCh6RGJk",
        "forum": "S2EN8MCHiz",
        "replyto": "S2EN8MCHiz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4796/Reviewer_r1cw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4796/Reviewer_r1cw"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the intrinsic dimension (ID) of a large-scale vision-language pre-training model BLIP and explore the relationships among intrinsic dimension, modality, and prunability, and show that, the ID geometric characteristics of visual and language representations differ significantly in terms of range and shape, resulting in distinct prunability for each modality."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper first presents the empirical study into the ID of a large-scale multimodal pre-training model.\n2. It explains how visual and language modalities align and change IDs in cross-modal attention mechanisms, and show the visual and language representations do not lie on the same low-dimensional manifold.\n3. This paper alsos shows the correlation between IDs and layer-wise importance for multimodal pruning."
            },
            "weaknesses": {
                "value": "I wonder why BLIP is chosen for this study, instead of more recnet multimodal models? Any explanations on this? Also, I wonder if the observations based on BLIP can be extended to other multimodal models, and how? If not, I'd suggest experiments with more multimodal models to validate the generality of the observations."
            },
            "questions": {
                "value": "please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4796/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817082632,
        "cdate": 1698817082632,
        "tmdate": 1699636461958,
        "mdate": 1699636461958,
        "license": "CC BY 4.0",
        "version": 2
    }
]