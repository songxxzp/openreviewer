[
    {
        "id": "RpeyTLa037",
        "forum": "jYsowwcXV1",
        "replyto": "jYsowwcXV1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6466/Reviewer_AXFR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6466/Reviewer_AXFR"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method to inject new visual concepts into the generation, using few images. The authors propose a novel regularization dataset generation strategy on both the text and image level. The formulated dataset can help to prevent losing text coherence and prompt better identity preservation. The results are established on benchmarks, demonstrating the effects of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed data approach is effective on the chosen benchmarks."
            },
            "weaknesses": {
                "value": "1.\tThe formatted prompt generation is limited to several categories. For example, according to the supp., for live objects, the prompts are all obtained via the subject of animal. How about the prompts for human? I do not think this prompt generation strategy is general enough.\n\n2.\tMoreover, I think the prompts should be generated according to different input images, employing multi-modality models.\n\n3.\tI think the new objects to be inserted into the generation in this paper\u2019s experiments are few. More cases are needed to analyze the effects of the proposed method.\n\n4.\tIn Fig.4, why the performance of w/o format will lead to worse results as the increase of iteration number? Even without the use of formatted prompts, the generation results should be more fitted with the target object along with the training."
            },
            "questions": {
                "value": "1.\tI wonder the performance of the proposed method if there are fewer input examples, like 1-3 examples.\n\n2.\tCan the prompts be generated online with the training? It will save a lot of time.\n\n3.\tFew examples can not reflect the true quality of the generation, is there any subjective evaluation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The generated images may be harmful to the protection of copyright."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6466/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698502079194,
        "cdate": 1698502079194,
        "tmdate": 1699636723339,
        "mdate": 1699636723339,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xI4zpyH1Sx",
        "forum": "jYsowwcXV1",
        "replyto": "jYsowwcXV1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6466/Reviewer_rHiC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6466/Reviewer_rHiC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a data-driven approach to improve personalized generation. The paper first discovers that previous class regularization is ineffective in alleviating overfitting due to a lack of diversity. The paper proposes to first enhance the training prompts associated with the concept images by including more specific class names and background descriptions. Based on the training prompts, the regularization prompts are further enhanced by introducing structural components including shape, color, and texture, which are then amplified with more diverse backgrounds and styles."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The paper attempts to improve personalized T2I generation from a data-centric perspective, which focuses on automatically generating a rich and informative regularization dataset. Despite there exist a few methods that improve T2I generation via better prompting [1, 2] (note that [1] was submitted to arXiv on 25 Oct 2023, hence not in the scope of this work), this paper is the first work to improve personalized generation by diversifying the regularization data. \n\n(2) This paper provides insights into the importance of the quality of the regularization dataset in order to prevent overfitting, and is complementary to existing works that attempt to improve architectures and training schemes. This may inspire future research on further improving diffusion personalization.\n\n(3) The proposed method demonstrates a notable improvement in generating personalized images with higher fidelity and is capable of preventing overfitting especially when facilitating larger training iterations.\n\n\n[1] Segalis, Eyal, et al. \"A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation.\" arXiv preprint arXiv:2310.16656 (2023).\n[2] Wang, Yunlong, Shuyuan Shen, and Brian Y. Lim. \"RePrompt: Automatic Prompt Editing to Refine AI-Generative Art Towards Precise Expressions.\" Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. 2023."
            },
            "weaknesses": {
                "value": "(1) The baselines that are compared in this paper are textual inversion and DreamBooth, which are both pioneering works in diffusion personalization. However, there exist many more improved personalization methods, e.g. Custom Diffusion [3], that are also widely used. Experimenting based on more methods will further emphasize the generalizability and complementarity of the proposed method.\n\n(2) On top of Custom Diffusion [3], it would be also interesting to see whether the data-driven approach can benefit multi-concept learning.\n\n(3) The method requires generating a relatively large regularization dataset (containing ~2000 images), which inevitably leads to much longer training time.\n\nSmall TYPO:\n1. TYPO in section 5, the first bullet in the first paragraph should be \u201c1)\u201d instead of \u201c2)\u201d\n\n[3] Kumari, Nupur, et al. \"Multi-concept customization of text-to-image diffusion.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "questions": {
                "value": "Please see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6466/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769418681,
        "cdate": 1698769418681,
        "tmdate": 1699636723172,
        "mdate": 1699636723172,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zWb2GdoY6t",
        "forum": "jYsowwcXV1",
        "replyto": "jYsowwcXV1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6466/Reviewer_hHDA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6466/Reviewer_hHDA"
        ],
        "content": {
            "summary": {
                "value": "It seems (please refer to Weaknesses for reasons why I use the word \"seem\") that the authors introduced an extension of DreamBooth, which can better preserves the details of the objects of interest. The authors proposed to achieve this goal by generating a larger set of regularization images. Specifically, they seem to be generated using prompts that are (1) generated by a large language model (LLM) or (2) generated following specific templates. Experiments are conducted on DreamBench and metrics show that the proposed method generates higher quality images than existing methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Extensive visual comparison with existing methods are presented in the paper. It is evident that, for the examples provided, the proposed method better preserves the details of the objects of interest\n\n2. The use of English is satisfactory. \n\n3. Ablations are conducted to help readers understand several design choices made by the authors."
            },
            "weaknesses": {
                "value": "1. Poor presentation: I find this paper hard to follow. Several important aspects of the proposed method remain a mystery, e.g., how the 2000 \"regularization images\" are created, why the 2000 images are called regularization images (I am not able to see how they can regularize the model from \"Each training batch contains one example from training set and one example from regularization set.\"). It seems to me that this paper tries to extend DreamBooth [Ruiz, 2023a] and the 2000 regularization images may be generated using Stable Diffusion and the 2000 samples may be used to compute the class-specific prior preservation loss to regularize the model. However, a reader needs to be very familar with DreamBooth in order to make these guesses and they are just guesses.\n\n2. To me, this is a trival extension of DreamBooth. The authors proposed to generate more \"regularization images\" using prompts that are (1) generated by a large language model (LLM) or (2) generated following specific templates. It is hard for me to agree that this paper meets the bar for an ICLR paper.\n\n3. Lack of human evaluation."
            },
            "questions": {
                "value": "1. How are the 2000 regularization images created? Are they generated by a pre-trained diffusion model? If so, why do you use the word \"created\"?\n\n2. Why the 2000 images are called \"regularization images\"? How can they help regularize the model? If you follow DreamBooth [Ruiz, 2023a], please specifically mention this.\n\n3. Would be great to see a comparison with DreamBooth + LoRA and Textual Inversion + LoRA.\n\n4. How does the performance of the proposed method change when more number of samples are available, e.g., 20 samples? How does the performance of the purposed method compare with other methods when more number of samples are available?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6466/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6466/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6466/Reviewer_hHDA"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6466/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820277031,
        "cdate": 1698820277031,
        "tmdate": 1699636723011,
        "mdate": 1699636723011,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "R24MOfGENv",
        "forum": "jYsowwcXV1",
        "replyto": "jYsowwcXV1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6466/Reviewer_vEAP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6466/Reviewer_vEAP"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose to perform prior preservation in personalized text-to-image generation with a regularization set. The authors construct this set by using ancestral sampling with formatted prompts. They tested their newly proposed regularization set on Stable Diffusion based models and achieved improvement compared to baseline."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The authors show qualitative and quantitative improvements compared to baselines in their experiments. In their qualitative examples, we can also observe that the fine-grained details of the objects are preserved better than the baselines."
            },
            "weaknesses": {
                "value": "1. It is very difficult to convince myself that the novelty presented in this paper is significant enough to warrant an acceptance. The main contribution of this paper is to construct a regularization set using a predefined and handcrafted format for prompting and ChatGPT for picking the phrases to fit the format, and the details of the format are not very well justified.\n\n2. Continuing from Weakness 1, it is unclear to me how the authors choose the format described in Section 3 \u201cGenerating Against Training Prompts\u201d and \u201cAmplifying Diversity with Structured Prompts\u201d since there is no related literature or ablation study to justify the effectiveness of each component in the format.\n\n3. The additional time required for generating the regularization set is more substantial (2000 images for this setting v.s. < 1000 images for the original DreamBooth setting).\n\n4. There is no evaluation on the fidelity (e.g. FID score) of the generated image, and fidelity score is a standard metric for this task."
            },
            "questions": {
                "value": "Does the size of the regularization set affect the performance? (e.g. will a smaller regularization set also work? Can the authors provide more ablation studies on this?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6466/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6466/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6466/Reviewer_vEAP"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6466/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699214933121,
        "cdate": 1699214933121,
        "tmdate": 1699636722892,
        "mdate": 1699636722892,
        "license": "CC BY 4.0",
        "version": 2
    }
]