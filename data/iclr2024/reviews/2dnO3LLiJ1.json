[
    {
        "id": "ojGA6WMjKm",
        "forum": "2dnO3LLiJ1",
        "replyto": "2dnO3LLiJ1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3647/Reviewer_mTMB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3647/Reviewer_mTMB"
        ],
        "content": {
            "summary": {
                "value": "This paper identifies an interesting phenomena in large scale transformers where some redundant tokens are repurposed for internal computation. The paper shows how the feature normal can be used to identify such tokens and how such tokens appear to capture global, rather than local, information compared to other tokens. Furthermore, such tokens make the attention maps less interpretable. The paper proposes to augment ViTs with register tokens which similar to CLS tokens are separate from the image patch tokens, but are not used used directly in any loss computations unlike CLS tokens. The proposed augmentation removes the normal outliers, results in small improvements on standard evaluation tasks, while improving the unsupervised object discovery performance of most methods.\n\n**Update (11/20):** I have updated my scores after reading the points made by other reviewers. Specifically, the point regarding the impact of the tokens on dense vs. image-level tasks, specifically weakness 3 raised by reviewer KSLu. It would be great if the authors could engage with the concerns raised by the reviewers."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The analysis of the outlier tokens is very nice and thorough. I found the graphs and explanations very insightful, and the experiments very comprehensive (especially the experiments in Tab 1 and Fig 5). \n- The proposed inclusion of a register token is very simple and elegant and provides more interpretable attention masks.\n- I appreciated the limitations statement at the end of Sec 2.2. \n- The paper was very easy to follow and the visualizations were helpful to provide the reader intuition for what is going on."
            },
            "weaknesses": {
                "value": "- While the paper did a great job at analyzing the behavior of outlier tokens in previous models in Sec 2.1, the paper does not have experiments showing that such behavior is eliminated by adding the register tokens. It would have been interesting to see if the behaviors ascribed to normal/outlier tokens in Fig 5 and table 1 are now transferred to image/register tokens in the proposed model. \n- The discussion around the performance of models on unsupervised object discovery is fairly limited and does not match the resuls. \n    - The paper is strongly motivated by the difference in attention maps compared to DINO and the limited performance of DINOv2 on LOST. While the gains of DINOv2+reg are impressive, it is still very surprising that it doesn't match DINO. It would be great if there was more discussions or some qualitative examples of that to explain why. \n    - The paper states that \"for all models on all datasets, adding registers for training improves the unsupervised object discovery performance.\" However, the results indicate that registers harm the performance on OpenCLIP."
            },
            "questions": {
                "value": "- Do the registers inherit the behavior exhbited by the outlier tokens? Specifically, are they good predictors of global image information as shown in Table 1? \n- Could you clarify on the discrepancy in OpenCLIP performance in Table 3 vs Sec 3.3? I wasn't sure if it's a missed negative result or a typo in the table.\n- Do the image tokens revert back to being more local in nature with the addition of tokens? How do they perform on the tasks exhibited in Table 1 and Figure 5?\n- How does the norm of the CLS token compare to the outliers before and after the addition of register tokens? It would be interesting to see if it matches the outlier tokens across conditions as shown in Figure 4, although simply reporting it on the final model would provide some insight into the internal mechanisms of ViTs. \n- Could you please comment on why you think LOST with DINO still performs better that with DINOv2? Is it the data (ImageNet vs LVD), newer training objective, some other factor? \n- It seems suprising that DINOv2 exhibits this behavior while using a dense mask-image-modeling objective. I was curious if you had any thoughts on why the masked image objective did not discourage such behavior despite requiring the patch features to retain the information that you show is lost in Fig 5b\n- I found the first line in page 9 a bit confusing. As I understand it, Torralba and Efros (2011) were arguing that datasets themselves were biased, not that the specific labels were. Such concerns still apply whether or not the data is labels or the training paradigm used for training as the bias arises from the data source and sampling process. While the samping process would be affected by the target labels, one can still get a biased dataset based solely on the data source (eg, instagram vs. inaturalist). Could you please elaborate on your statement and what you meant? \n- (Suggestion) The paper suggests that this outlier token is exhibited by larger models in Fig 4/Sec 2.2, yet the base model of CLIP/DEIT is used in Sec 3.1. While Fig 7 still shows that the base models of those models exhibit outlier tokens, it would be nice for the authors to add some commentary on this at the end of sec 2.2."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3647/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3647/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3647/Reviewer_mTMB"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3647/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698358568649,
        "cdate": 1698358568649,
        "tmdate": 1700526255433,
        "mdate": 1700526255433,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Zu9LsoB33J",
        "forum": "2dnO3LLiJ1",
        "replyto": "2dnO3LLiJ1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3647/Reviewer_KSLu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3647/Reviewer_KSLu"
        ],
        "content": {
            "summary": {
                "value": "This paper identifies and characterizes artifacts in the feature maps of vision transformer (ViT) models trained with supervision or self-supervision. In particular, the authors observe high-norm \"outlier\" tokens with high redundancy in the output features of several ViT models, and show that they hold less local patch information but more global image information compared to normal tokens. This suggests the model is repurposing redundant patches to store global information. They propose appending dedicated \"register\" tokens to the input sequence, which removes the artifacts and improves performance on downstream dense prediction tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The investigation is quite original; the use of memory/registers in transformers is not necessarily a new idea, but motivating them through removing redundancy and reducing attention artifacts is both novel and interesting.\n\n2. Experiments and analysis are mostly convincing (see questions below).\n\n3. I enjoyed the narrative exposition: the problem setting is clear, the motivation for registers is clear, and their utility is well-demonstrated via experiments."
            },
            "weaknesses": {
                "value": "1. While adding additional token (registers) seems like a simple and efficacious approach, I'm wondering if it's the only possible solution for reducing patch level redundancy. Did the authors observe similar effects across other self-supervised models, like MAE, where nominally the patch-level reconstruction should also alleviate representational redundancy?\n\n2. In demonstrating that the artifacts hold global information, the authors \"choose a single token at random, either high-norm or normal,\" and then \"train a logistic regression classifier to predict the image class from this representation, and measure the accuracy.\" Why choose this token at random? Why not use all the high-norm and normal tokens, or some projected and pooled version over all of them in order to regress to the class? In experiments we have conducted, this almost always outperforms using single tokens (cls or otherwise), and it may be the case that the conclusion that the high-norm tokens outperform the \"normal\" tokens is not so clear when this is done.\n\n2. There seems to be a conflict between the fact that \"high-norm tokens appear on patches that are very similar to their neighbors,\"  \"often appear[ing] in uniform, background areas,\" and the fact that performance on ImageNet classification improves almost monotonically with more registers, but not dense tasks like segmentation or depth estimation. In particular, I would expect that if reappropriating the \"redundant\" local patches helps on object-centric classification, it should help much more substantially on tasks that are even more reliant on good (non-redundant?) local information (i.e. segmentation or depth estimation).  Can the authors comment on this?"
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3647/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3647/Reviewer_KSLu",
                    "ICLR.cc/2024/Conference/Submission3647/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3647/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790266424,
        "cdate": 1698790266424,
        "tmdate": 1700846788676,
        "mdate": 1700846788676,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fhACYgf6aO",
        "forum": "2dnO3LLiJ1",
        "replyto": "2dnO3LLiJ1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3647/Reviewer_GGy7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3647/Reviewer_GGy7"
        ],
        "content": {
            "summary": {
                "value": "The paper identifies the problem of artifactual areas of feature maps in vision transformers. On further analysis, these artifacts correspond to high norm tokens in the ViT coming from background regions in the image, tend to hold more global information and lack spatial information. This leads to the conclusion that tokens from these low-information regions are being repurposed by the model to hold global information for internal computations. Moreover, this issue afflicts most ViTs with DINO v1 being the exception. This problem was previously discussed in Memory transformer (Burtsev et al.) in the context of NLP datasets. Following Memory transformer's recommendation, the paper adds new tokens called registers at the to remediate this issue and show that the register-trained ViTs have better spatial feature maps, thus better downstream performance for object discovery tasks. Various ablations and experiments are also shown to shed light on the behavior of registers and why this problem occurs with DINOv2 models in the first place."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The paper identifies an important problem of heatmaps lacking spatial resolution and accuracy in DINOv2 and other ViTs which leads to suboptimal downstream performance on object discovery and localization tasks. The fact that this is not a problem for DINOv1 is pretty surprising and the experiments done on the changes in token norm across model size will be a useful start to understand this better. The discovery of these high-norm tokens, experiments using the linear models and classifiers to characterize what these tokens contain, and then forming the hypothesis of how these tokens are getting repurposed for holding global information, all presents a coherent story of these misused outlier tokens.\n2. The solution of adding new tokens (memory or registers) is not a new one, but is shown to be very effective in removing these tokens with high norms as well as bringing back spatial interpretability into the feature maps. Furthermore, the downstream performance on image-level tasks stain consistent with ViTs without registers while the performance on object discovery tasks goes up in DINOv2 and DeiT-III after addition of registers. These results indicate that adding these new tokens does resolve the spatial issue with these ViTs. A huge plus of this approach is the simplicity of it."
            },
            "weaknesses": {
                "value": "1. The removal of these artifacts does come at the cost of new tokens, hence additional compute. The paper reports a 2-6% increase when adding 4-16 new register tokens.\n2. One very interesting observation was how the different register tokens end up focussing on the different areas of interest on the object. If there are spatially discrete areas of focus for the registers, does this undermine the argument that we need them for storing global information which was earlier being done using redundant patches?\n3. Not a weakness, but would be nice to see some norm-related metrics and/or visualizations for the outlier tokens across different heads. Do all the heads from these tokens end up getting these tokens repurposed? What does that variance across heads look like?"
            },
            "questions": {
                "value": "1. I'm curious if other simple solutions like penalizing these high norms could work as well and if yes, would they be preferable? Would love to hear from the authors on other ways to address this issue.\n2. Any reason why adding more registers hurts performance for NYU depth dataset?\n3. What happens to downstream performance when registers are added to DINO models which do not seem to need it? Does the nature of what registers learn differ from the case of DINOv2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3647/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3647/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3647/Reviewer_GGy7"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3647/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698812192755,
        "cdate": 1698812192755,
        "tmdate": 1699636320630,
        "mdate": 1699636320630,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gJU95oFfG2",
        "forum": "2dnO3LLiJ1",
        "replyto": "2dnO3LLiJ1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3647/Reviewer_eoPK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3647/Reviewer_eoPK"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses the discovery of artifacts in the feature maps of Vision Transformer (ViT) networks, both supervised and self-supervised. These artifacts appear as high-norm tokens during inference, typically in less informative background areas of images, and are utilized for internal computations by the network. To address this, the authors introduce a novel and straightforward method involving the addition of extra tokens to the ViT's input sequence. This technique effectively resolves the artifact issue for both types of models. It not only sets new performance benchmarks for self-supervised visual models on dense prediction tasks but also enhances object discovery with larger models. Crucially, the approach results in smoother feature and attention maps that benefit subsequent visual processing tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper identifies an interesting phenomenon observed in the popular transformer models (DINO). By removing this artifact, the authors demonstrates the improved models have clear attention maps that could be used for downstream analysis such as object localization. \n- The step-by-step investigation is solid and compelling. \n- The method of providing a junkyard to remove the artifact is novel and effective. \n- The experiments are convincing and comprehensive."
            },
            "weaknesses": {
                "value": "- The norm shows significant reduction for OpenCLIP in Figure 7, yet in Table 3, it doesn\u2019t show significant improvement for object localization which is the main benefit of using register. Further explaination / exploration the reason behind it should be helpful for wide adoptation.  \n- Minor: it should be OpenCLIP instead of CLIP in Figure7."
            },
            "questions": {
                "value": "In table 3, OpenCLIP+reg is not better than OpenCLIP which is contrary to other two models that have significant improvement. Any further explaination could be helpful since it doesn\u2019t support the claim that for all models, adding registers would improves the results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3647/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3647/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3647/Reviewer_eoPK"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3647/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699493665258,
        "cdate": 1699493665258,
        "tmdate": 1699636320548,
        "mdate": 1699636320548,
        "license": "CC BY 4.0",
        "version": 2
    }
]