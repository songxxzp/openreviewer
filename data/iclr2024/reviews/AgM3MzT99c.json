[
    {
        "id": "0gRMLgJelG",
        "forum": "AgM3MzT99c",
        "replyto": "AgM3MzT99c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2863/Reviewer_CZLW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2863/Reviewer_CZLW"
        ],
        "content": {
            "summary": {
                "value": "Paper introduces a new curriculum generation approach for reinforcement learning agents. From the high level, it includes 1) an LP model, which ranks the tasks based off the difficulty of the current learning progress of the agents; 3) an interestingness model (Mol), which ranks the tasks out of the \"interestingness\", a subjective view by human infused to the LLM. The proposed method, OMNI, is evaluated on life-long RL in two domains: crafter and babyAI. Basically, the agent will follow the task proposed by OMNI, an ablative version of OMNI, or a random task selector. The results show that agents learned with the curriculum produced by OMNI have clear advantages."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+The research topic presented here is relevant and important. There have been active explorations of how to turn the powerful LLM into general agents. Building a curricula generator is definitely very promising and should be of interest to a range of audiences from general agents to LLMs.\n\n+The paper is clear and well-written. The proposed method is technically sound -- LLM indeed has some infused human prior that could help determine the \"interestingness\" of the tasks and help build more reasonable curricula for RL.\n\n+The results on crafter and babyAI look quite promising. The advantages over random curriculum generators and ablative versions of OMNI are clear. The additional experiments in the appendix are very thorough."
            },
            "weaknesses": {
                "value": "At this point, I do not have major concerns. But I do hope the authors can help address the following questions:\n\n-What if the LLM does not have the human prior or reasoning capacity about the task \"interestingness\" on the given domain?\n\n-Based on my personal experiences of training agents in Crafter, an agent with a random task generator (Uniform) should not be that bad as it should be able to master not just a limited set of tasks as the curve in Figure 3 suggested. Can the authors clarify this?\n\n-If my understanding is correct, LP seems to be more crucial to the success of mastering more challenging tasks as it takes the learning process into consideration, while \"interestingness\" is about not wasting time on tasks that could not lead to meaningful learning. Therefore, if an unlimited round of learning is allowed, an agent with LP only should ultimately match the results of full OMNI on the average success rate. However, this is not the case in all the experiments. Maybe the LP baseline is not run for enough rounds? I would like to learn more about your thoughts on this.\n\nMinor: some references on building open-ended agents in open-world environments should be cited [1-5] \n\n[1] open-world control: https://arxiv.org/abs/2301.10034\n\n[2] DEPS: https://arxiv.org/abs/2302.01560\n\n[3] STG transformer: https://arxiv.org/abs/2306.12860\n\n[4] Plan4MC: https://arxiv.org/abs/2303.16563\n\n[5] GITM: https://arxiv.org/abs/2305.17144"
            },
            "questions": {
                "value": "See \"weaknesses\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2863/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2863/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2863/Reviewer_CZLW"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2863/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698659140970,
        "cdate": 1698659140970,
        "tmdate": 1700561831886,
        "mdate": 1700561831886,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mAtSZcxqby",
        "forum": "AgM3MzT99c",
        "replyto": "AgM3MzT99c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2863/Reviewer_LSLH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2863/Reviewer_LSLH"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on an interesting and practical scenario that the agent is exploring in an open-ended environment and learning new behaviors forever. Massive unlearned tasks exists in such environment, and how to choose the task to learn becomes a challenge. Beyond conventional choosing criteria, interestingness is an important aspect which will be largely considered by humans when exploring the open-ended environment, but is hard to be measured. This paper proposes to use large models, which already have encoded many human knowledge, as a model of interestingness, providing guidance for the agent to choose the task to learn. The authors further provide a simple implementation for such the above idea. Two environments (Crafter and BabyAI) are tested in experiments, and good results are obtained for both environments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper targets an important problem. Open-endedness needs to be considered in many practical scenarios, where agents need to learn and improve themselves in interactive environments.\n\nThe intuition and idea behind this paper is very reasonable and promising. Large models offer us many potentials, making some previously impossible tasks feasible now, and this paper successfully finds such a task. Interesting tasks are hard to define, and now with the help of large models, one can ask what tasks are interesting and explore environments like a human. Also, finding interesting tasks is very important in open-endedness research.\n\nOverall, I like the idea of this paper, and believe that this principle will have impact on open-endedness research."
            },
            "weaknesses": {
                "value": "The baselines compared in this paper are not strong enough. All baselines are intuitively set, and some previously proposed methods (some are discussed in related work, like curriculum learning for RL) need to be included. \n\nAlthough the authors claim that OMNI is a principle and this paper only presents one instance, practical implementation is essential to verify the proposed principle, and some designs in the current algorithm may need to be improved.\n- It might not be practical to know all candidate tasks in advance, and just let the large model choose one. In the RL setup, the agent needs to explore the environment and finds out all candidate tasks. \n- The downsampling method for boring tasks is quite simple now (multiplying by 0.001). The applicability of this method needs to be discussed and analyzed.\n\nHuman alignment also needs to be discussed. This paper just uses the interestingness of large models as that of human to do exploration. On the other hand, alignment is an unsolved question that targets aligning human and large models and avoiding dangerous actions. Methods like RLHF are proposed here. Considering that human alignment is not perfect now, how the interestingness of large models be used for choosing tasks and some important aspects, like safety, are also ensured at the same time? \n\nThe missing related work (https://arxiv.org/pdf/2302.06692.pdf) uses language model to suggest plausibly useful goals, which shares similar idea with this paper."
            },
            "questions": {
                "value": "Why is the survival component removed in the Crafter environment? This is quite important in a practical open-ended environment. Without considering the survival component, the aspects that need to be considered during choosing the task become less adequate and complex, which goes against the motivation of using LLMs here.\n\nIn experiments, how does the large model know what tasks are interesting tasks, given that interesting tasks are pre-defined by the authors?\n\nIt\u2019s better to show instructions used in determining interesting tasks. How to query the large model and what kind of results are obtained from it? An example here might be helpful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2863/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818471307,
        "cdate": 1698818471307,
        "tmdate": 1699636229779,
        "mdate": 1699636229779,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FicpoHjRkO",
        "forum": "AgM3MzT99c",
        "replyto": "AgM3MzT99c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2863/Reviewer_qXzy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2863/Reviewer_qXzy"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel approach to enhance open-ended learning algorithms, which aim to continually learn new and interesting behaviors in vast environments with innumerable tasks. The core challenge addressed is the existing inability to quantify and prioritize tasks that are not only learnable but also inherently interesting\u2014deemed to be significant due to their worth and novelty. The authors propose the concept of Open-endedness via Models of human Notions of Interestingness (OMNI), leveraging large language models (LLMs) to serve as models of interestingness (MoIs). These LLMs are presumed to embody human perceptions of interestingness, having been trained on extensive human-generated data. The paper demonstrates through experiments that OMNI can direct open-ended learning towards tasks that are both learnable and interesting, showing superiority over baselines that rely solely on uniform task sampling or learning progress."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of leveraging large language models to approximate human notions of interestingness in open-ended learning environments is novel and holds substantial promise.\n-  The paper includes detailed experiments across two domains that showcase the effectiveness of the OMNI framework in focusing learning on tasks that are both interesting and learnable."
            },
            "weaknesses": {
                "value": "## Main concerns\n- The major weakness of this work is the lack of comprehensive comparitive analysis. The authors mention and properly cite a number of curriculum learning papers, and also works attempting to quantify interestingness. Why isn\u2019t any of these methods used for comparison? It is very obvious that OMNI would outperform uniform sampling and LP. The authors should try out LP + other metrics of interestingness, as described in Section 2.2. \n\n- The performance of OMNI and other baselines on BabyAI is extremely bad (success rate < 0.008). Have the authors taken a look at the policies and understood what is happening? I understand this includes hard tasks, but have the authors considered plotting the results for only interesting tasks, similar to Crafter?\n\n- In the experiments, the authors use a pre-defined set of interesting, boring and challenging tasks. However, it would also be interesting to try the methods in a more open-ended setting whereby they use a random task generator that procedurally generates virtually unlimited number of tasks.\n\n- The evaluation of the trained models is performed only on the previously seen tasks. I was wondering if the authors considered evaluating on held-out, and particularly out-of-distribution (OOD) tasks to assess the robustness of the trained methods.\n\n## Content and Writing Style Issues\n- A disproportionate emphasis on potential implications and future work in the paper might obscure the immediate contributions. For example, the Conclusion is > 1.25 page, mostly discussing future work. While this is very interesting, it feels to me that it takes the space from what actually was important in this paper and was moved to the Appendix. In the end, the reader has to go back and forth from main part of the paper to appendix to follow the ideas described in these different sections. It is my recommendation that the authors carefully consider what they want to include in the main part of the paper vs the appendix.\n\n> \"Perhaps, someday, unlimited energy and a cure for cancer can be added to the list\", \"ephemeral fuel ...\", \"eons of human experience\", \"shadows of what human sense could do\", \"intriguing prospect has arisen\", \"vast troves...\", \"to borrow from Newton\",...\n- Additionally, the stylistic choice of using lyrical language is atypical for scientific papers and could compromise the accessibility and clarity for a wider audience. \n\n> \"OMNI has the potential to significantly enhance the ability of AI to intelligently select which tasks to concentrate on next for endless learning and could pave the way for self-improving AI and AI-Generating Algorithms\"\n-  I think there is a giant gap between the what the authors presented in this work, compared to what they claim it has the potential of doing. I would recommend removing/rewarding such statements."
            },
            "questions": {
                "value": "See above.\n\nUPDATE: I have updated the score following the rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2863/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2863/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2863/Reviewer_qXzy"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2863/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699053367341,
        "cdate": 1699053367341,
        "tmdate": 1700565407729,
        "mdate": 1700565407729,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VjCGqGEO1W",
        "forum": "AgM3MzT99c",
        "replyto": "AgM3MzT99c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2863/Reviewer_MxZx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2863/Reviewer_MxZx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use LLMs to measure how interesting tasks are for open-ended algorithms to learn. The problem which they tackle is that for open-ended algorithms, there are an infinite number of tasks which that agent could try to learn. This can be problematic for two reasons: the tasks may be unlearnable, or they may be uninteresting. The paper notes that the notion of \"interestingness\" is hard to measure exactly, but that humans know it when they see it. The paper proposes to combine measures two measures: learning progress (to ignore tasks which are unlearnable, and which has been explored in previous work), and interestingness (which is here measured using an LLM). They construct variants of the Crafter and BabyAI environments to test this approach, and find that it outperforms uniform sampling of tasks and prioritizing tasks by learning progress alone.\n\nOverall, this paper addresses an interesting problem and I think the approach has potential, but I do not think it's ready for publication due to the experiments which are not convincing enough and the presentation which needs work. I think this paper would benefit from another revision cycle giving time to add experiments on more convincing environments and improve the writing, after which it would make a strong submission."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well motivated and the problem is interesting. They correctly identify one of the fundamental issues with open-ended learning algorithms, i.e. that if the agents choose tasks based on novelty, feasibility or information gain alone, they may end up choosing tasks which are completely uninteresting to humans, but which a  _tabular rasa_ agent has no reason to deprioritize."
            },
            "weaknesses": {
                "value": "- Unfortunately, the experimental section of the paper does not live up to the ambitious ideas described in the intro. Only two (fairly simple) environments are considered (Crafter and one MiniGrid task), and the experiments feel contrived. For Crafter, \"Impossible\" tasks are generated by simply assigning the agent 0 reward. \"Boring\" tasks are generated by asking the agent to repeat the same task several times. Because of this, it is possible to solve the Crafter setup with very simple heuristics, and an LLM is not really needed. Granted, LLMs are more general, but it would be a lot more compelling if this method were evaluated on tasks which are not solvable with simple heuristics.\n- The paper's presentation needs improving. The paper is missing a lot of details about the algorithm. While Figure 1 gives a high-level overview of the system, it is still fairly vague and the rest of the description is all in the text and not very precise. It would be helpful to have the entire algorithm spelled out in pseudocode (in the main text), the exact process by which tasks are chosen described in equations or pseudocode, etc."
            },
            "questions": {
                "value": "Here are my questions and suggestions:\n\n- My main suggestion for improving this paper is to test on one or more challenging environments. While artificial settings like the ones used in this paper are nice for proof of concepts or ease of interpretation, they need to be complemented by experiments on a more challenging domain where simple heuristics are insufficient. I would suggest trying this either on Minecraft, which has been used for open-ended research [1], or NetHack, which is another very complex open-ended game where the extrinsic reward is insufficient (it is also fast to run) [2].\n- My second suggestion is improving the presentation (see my comments above). It's also not clear what criteria the agent uses to decide it can do a task \"well\" - this this based on success rate or reward? Please clarify. In addition to this, I think the Conclusion/Discussion/Future Work section is too long at over 1 page. This saved space can be used for extra experiments in this paper. \n\n\n\n[1] Voyager: An Open-Ended Embodied Agent with Large Language Models (Wang et al)\n\n[2] The NetHack Learning Environment (Kuttler et al, NeurIPS 2020)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2863/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699137634235,
        "cdate": 1699137634235,
        "tmdate": 1699636229627,
        "mdate": 1699636229627,
        "license": "CC BY 4.0",
        "version": 2
    }
]