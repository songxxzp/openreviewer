[
    {
        "id": "dOejWOwboX",
        "forum": "M0MF4t3hE9",
        "replyto": "M0MF4t3hE9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4489/Reviewer_t19r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4489/Reviewer_t19r"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel detection task, called IOD, which requires the detector to detect objects related to the human instruction. To unravel the problem, the authors first created a dataset called IOD-Bench, and then designed an Ins-DetCLIP model. The idea is simple and straightforward -- by incorporating an LLM with an open-vocabulary detector. Experiments on the IOD-Bench show that Ins-DetCLIP outperforms the baseline methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed IOD task is novel and has practical values.\n- The method is simple and easy to understand."
            },
            "weaknesses": {
                "value": "- Instruction types 1, 2, and 3 seem to be variants of traditional detection or OV detection. With the help of LLM, it should be easy to convert these instructions to traditional detection or OV detection problems. Intuitively, I believe that using off-the-shelf detectors and LLM will achieve good results on these types of instructions. If not, a discussion of the reason is expected.\n- Instruction type 4 is abstract enough to meet my expectations of IOD. However, the objective of instruction type 4 is to detect related objects, which is vague. For instance, should cutleries be detected if the instruction is \"Prepare a healthy salad for lunch\"?\n- In Phase 2 of Ins-DetCLIP training, the visual encoder is frozen. This suggests that the LLM can only pick objects from the object proposals given by DetCLIP. However, DetCLIP is unaware of the instructions. As the instruction becomes more complex, DetCLIP will be unable to recall the target objects with a fixed number of proposals. \n- There have been several multimodal LLMs that are capable of detecting objects, i.e. Shikra[1], Kosmos[2]. The proposed Ins-DetCLIP should be compared with such methods."
            },
            "questions": {
                "value": "- As for the evaluation metrics, the authors said that they use BERT to compute the similarities between the predicted categories and the GT categories. What if the prediction is completely irrelevant to the GT categories? In this case, the similarity score distribution will likely to be random. Will this cause the metrics to be unstable?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4489/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742641189,
        "cdate": 1698742641189,
        "tmdate": 1699636424951,
        "mdate": 1699636424951,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "27TRnFUq0Q",
        "forum": "M0MF4t3hE9",
        "replyto": "M0MF4t3hE9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4489/Reviewer_Y4Gg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4489/Reviewer_Y4Gg"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce a novel task named Instruction-oriented Object Detection (IOD). In the course of developing an IOD system, an innovative dataset, IOD-Bench, is presented, along with a proposed model named Ins-DetCLIP."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea that the author solves real-world problems through diverse instructions is interesting."
            },
            "weaknesses": {
                "value": "1-I understand the authors' aspiration to address a wide range of real-world scenarios through diverse instructions. However, I find the motivations behind the four tasks to be somewhat unclear. The similarity amongst the first three tasks appears quite pronounced, and it seems that existing datasets already cater to these scenarios to a certain extent.\n\n2-In Section 5.2, the authors described integrating BLIP2 and MiniGPT4 with open-vocabulary detectors (OVD) in a two-stage sequential manner to construct baseline models. I'm curious as to why such a sequential approach was chosen for building the baseline models. Would it not be more straightforward to employ the OVD method for direct open-vocabulary detection? My concern is that a two-stage sequential manner could inherently limit the system's performance to the accuracy of both models, seeming like a suboptimal choice."
            },
            "questions": {
                "value": "see Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4489/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760353808,
        "cdate": 1698760353808,
        "tmdate": 1699636424883,
        "mdate": 1699636424883,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AZHQhRf4kC",
        "forum": "M0MF4t3hE9",
        "replyto": "M0MF4t3hE9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4489/Reviewer_tem2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4489/Reviewer_tem2"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Instruction-oriented Object Detection (IOD), a novel task aimed at improving human-computer interaction by enabling object detectors to interpret user instructions for identifying specific objects. IOD necessitates the understanding of natural-language instructions and contextual reasoning to provide the name and location of the desired objects, posing new challenges to current object detection systems. To address this, the authors develop a dataset called IOD-Bench, consisting of instruction-guided detections and specialized evaluation metrics, and leverage large-scale language models (LLMs) to generate a diverse set of instructions based on existing public object detection datasets. The proposed model, Ins-DetCLIP, utilizes the knowledge within LLMs to enable instruction-following capabilities in the detector. It employs a visual encoder, DetCLIP, to extract object-level features and aligns them with the input instructions through a cross-modal fusion module integrated into a pre-trained LLM. The experimental results on IOD-Bench demonstrate that Ins-DetCLIP consistently outperforms baseline methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Overall, the task presented in this manuscript is captivating and aligns well with the practical applications of an intelligent detection system. The instructions have been well-crafted, bolstering my view on this matter.\n\n2. The clarity of the writing in this submission is commendable, making the content generally straightforward to grasp.\n\n3. The impressive performance of Ins-DetCLIP underscores the effectiveness of the proposed instruction tuning paradigm. Furthermore, the authors have done a good providing an extensive range of experiments to scrutinize all the design choices, which is highly valuable."
            },
            "weaknesses": {
                "value": "1. I would recommend that the authors take another pass at proofreading the manuscript to ensure consistent and correct formatting throughout. For instance, there is a conventional practice of inserting a space before references in both the main body text and tables.\n\n2. With respect to the main results showcased in Table 1, while they are compelling and seem to align with the authors' motivations, I note that the comparison methods, such as BLIP-2 and MiniGPT-4, do not incorporate human instructions interns of object detection. I presume that performing instruction tuning on these models could enhance them and provide insights into the generalizability of the proposed tuning method. I am curious about the feasibility of this approach.\n\n3. In terms of the dense captioning tasks, despite the impressive performance demonstrated, the comparison is made with somewhat outdated methods. There are contemporary methods, building on SAM or other models, capable of performing this task as well. A performance comparison with Ins-DetCLIP would be insightful. While Ins-DetCLIP may not outperform these methods, including such results or providing justification would help to elucidate the performance gap, potentially laying the groundwork for future enhancements."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4489/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4489/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4489/Reviewer_tem2"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4489/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770261518,
        "cdate": 1698770261518,
        "tmdate": 1699636424796,
        "mdate": 1699636424796,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RpDDlcjUSW",
        "forum": "M0MF4t3hE9",
        "replyto": "M0MF4t3hE9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4489/Reviewer_rzsX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4489/Reviewer_rzsX"
        ],
        "content": {
            "summary": {
                "value": "The manuscript presents a new task called Instruction-oriented Object Detection (IOD), which aims for the model to accept human instruction and generate corresponding detection results. For training the model on such a task, the authors first construct a dataset termed IOD-Bench and the corresponding evaluation metrics. Based on DetCLIP, Ins-DetCLIP is presented for making the open-vocabulary object detector able to follow instructions, where an LLM is attached to the frozen visual encoder of pretrained DetCLIP.\n\nEmpirically, Ins-DetCLIP notably outperforms baseline approaches such as BLIP-2, MiniGPT-4 and LLaVA. Additionally, the model shows the capability of performing captioning for the relevant objects."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The manuscript goes beyond image-level captioning/question answering and proposes to use LLM for instructed object detection, which is novel.\n- The approach provides new insights into what a LLM could do when it is connected to an open-vocabulary object detector. \n- Compared to existing approaches such as BLIP-2, MiniGPT-4 and LLaVA, Ins-DetCLIP demonstrates an outstanding capability of performing instruction guided object detection. \n- Increasing the size of LLM could benefit all the tasks, showing good scalability."
            },
            "weaknesses": {
                "value": "- It is unclear how the object bounding boxes and the features are generated in the first place before object-level cross modal fusion.\n- It is not shown to what extent is the approach dependent on the performance/quality of the phase-1 training, which would be an important aspect for understanding the approach that requires two-phase training.\n- The inference speed comparison seems unfair, since the model sizes are different. It is difficult to harness whether the approach is slow or fast.\n- Citation formats are wrong."
            },
            "questions": {
                "value": "- It would be interesting to know how much resources and GPU days are required to train such a model."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4489/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699127251866,
        "cdate": 1699127251866,
        "tmdate": 1699636424726,
        "mdate": 1699636424726,
        "license": "CC BY 4.0",
        "version": 2
    }
]