[
    {
        "id": "lWNIekObYx",
        "forum": "2Oiee202rd",
        "replyto": "2Oiee202rd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5829/Reviewer_VSta"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5829/Reviewer_VSta"
        ],
        "content": {
            "summary": {
                "value": "The authors show that adding text that describes the context of an object in an image can improve the performance of CLIP-based zero-shot classification. In addition, they show that said context can be inferred using CLIP itself."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1- The paper shows some evidence towards towards the fact that CLIP representations do capture the context as well as the foreground objects, making the alignment with text prompts better when the appropriate context is included in the text.\n2- Although the improvements in performance are not large in many of the benchmarks, they come at little cost, probably making it applicable in practice."
            },
            "weaknesses": {
                "value": "I haven\u2019t found any major weakness in this work (although it is not fully within my expertise)."
            },
            "questions": {
                "value": "Some minor issues:\n- For some of the experiments I couldn\u2019t find if they employed ClassAttr or PureAttr.\n- In Algo 1, I assume it should be \u201cSet of classes Y\u201d rather than \u201cclass Y\u201d, and that the sum is over y \\in Y."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5829/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697915060562,
        "cdate": 1697915060562,
        "tmdate": 1699636615212,
        "mdate": 1699636615212,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AXlFUFtCHU",
        "forum": "2Oiee202rd",
        "replyto": "2Oiee202rd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5829/Reviewer_vx4m"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5829/Reviewer_vx4m"
        ],
        "content": {
            "summary": {
                "value": "The paper explores text-prompting in CLIP, to make the inference process more ``human-like\u201d. The authors show that their two-phased prompting process improves (i) performance, and (ii) robustness against specious features (shortcuts)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- In this work, prompting is class-independent; which makes it easily applicable to numerous datasets. \n- Authors systematically evaluate the prowess of CLIP in inferring contextual attributes\n- The performance gain on domain-specific and out-of-distribution ImageNet datasets shows promise in the claims and approach of the authors\n- This work allows for building domain-specific (yet, class independent) augmentations with the possibility of human-in-loop intervention\n- The approach presented is elegant and interpretable"
            },
            "weaknesses": {
                "value": "- If I understand correctly, Figure 3 gives only the score (x100) of the correct class in different scenarios. Is this *completely* informative? I think it can be easily misleading. What if the model provides relatively higher scores to some of the wrong classes as well? Can the authors analyse the score distribution of the wrong classes? Reporting the mean of CLIP_score@topK might be a good start to understanding the false positives.\n- I appreciate the visualisation study provided by GradCAM as a qualitative analysis but I am not confident of the calculation of \u201ccore\u201d versus \u201cspurious\u201d features [1]. \n- Can the authors also report the random accuracy for Tables 4 and 5? It is important to have a random baseline (that is, random string in place of the inferred context having the same token (and not string) length) here to isolate the effect of \u201cregisters\u201d versus actually using the context [1]. \n- The authors have not provided the code for reproducing the paper; implementation details are missing.\n\n**TL;DR**: The authors make strong claims about reducing the reliance on shortcuts, however, the missing baselines and analyses do not make me confident of their approach. However, some analyses seem misrepresented/miscommunicated. If the authors can answer my questions, I\u2019d be open to changing the score.\n\nSome clarity formatting and typographical errors to rectify:\n\n- Overall, the paper is well-written and ideas well-presented\n- The paper, at some points, deviates from standard ICLR formatting:\n  - \u201cinterleaved\u201d figures and tables (minor inconvenience)\n  - Unlabelled table: \u201cConditioning on ground-truth contextual attributes improves classification accuracy\u201d\n- Minor typographical errors:\n  - Section 5.1: \u201cIntuitively, we classify an image using both \u2013 possible classes and the ground-truth contextual attributes\u201d\n  - Remove the asterisk in the author list of \u201cDistributionally robust neural networks\u201c\n\n[1] Darcet, Timoth\u00e9e, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. \"Vision Transformers Need Registers.\" arXiv preprint arXiv:2309.16588 (2023)."
            },
            "questions": {
                "value": "Please refer to weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The authors should consider adding ethical statement (especially since they are using datasets like CelebA with prompts specifically mentioning gender and race)."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5829/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5829/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5829/Reviewer_vx4m"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5829/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698674463710,
        "cdate": 1698674463710,
        "tmdate": 1699636615108,
        "mdate": 1699636615108,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PSX3OLIumI",
        "forum": "2Oiee202rd",
        "replyto": "2Oiee202rd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5829/Reviewer_U13E"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5829/Reviewer_U13E"
        ],
        "content": {
            "summary": {
                "value": "Inspired by the human perception process that the contextual attributes are separated from the foreground objects, this paper proposes a training-free, two-step zero-shot classification method PerceptionCLIP to first infer the contextual attributes (e.g., background) and then performs object clas- sification conditioning on them. A proof-of-concept investigations reveal that conditioning on ground-truth contextual attributes improves CLIP\u2019s zero-shot classification. The proposed PerceptionCLIP demonstrates performance gain and improved interpretability on several datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea of imitating human perception process to improve the generalization and group robustness of the image classification model is insightful for the community.\nThe proposed method is extensively evaluated on 11 datasets."
            },
            "weaknesses": {
                "value": "1. Collecting the contextual attributes requires either pre-knowledge for the test image or a large dataset containing captions, which hinders the generalization ability of the proposed method in the real world. For instance, contextual attributes for the CelebA dataset are manually defined, e.g., gender, age, etc. To collect the contextual attributes for the remote sensing dataset EuroSAT, the authors first retrieve similar images and captions from a large image+text dataset LAION-400M, then ask GPT-4 to summarize the contextual attributes. What if we do not have external datasets to provide captions?\n\n2. The qualitative results in Figure 4 indicate that introducing the contextual attributes reduces reliance on the spurious features and the model focuses more on the core features. It would be fairer to provide a quantitative evaluation, e.g., counting the percentage of model attention on the core features versus on spurious feature on all test set in ImageNet, and compare the ratio of different models.\n\n3. The performance gain seems marginal on most of the datasets. For instance, in Table 4, the performance gain is only around 2% on ImageNet, ImageNetV2, ImageNet-A and ImageNet-Sketch. Besides, since introducing a random attribute or even a wrong attribute can improve the accuracy in Table 2, it would be interesting to include the results of the wrong attribute and random attribute in Table 4 as well.\n\n4. The results in Table 7 are not consistent among different backbones. It is hard to get any conclusion on which method is better."
            },
            "questions": {
                "value": "In Table 7, why lower gap between the Avg and Worst is better?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5829/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698841682818,
        "cdate": 1698841682818,
        "tmdate": 1699636614988,
        "mdate": 1699636614988,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LvPzGT6njs",
        "forum": "2Oiee202rd",
        "replyto": "2Oiee202rd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5829/Reviewer_k2xX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5829/Reviewer_k2xX"
        ],
        "content": {
            "summary": {
                "value": "This paper is inspired by human visual perception, where humans first discern contextual attributes, such as background and orientation, to distinguish objects, and then classify them. Similarly, when CLIP is provided with these contextual attributes, it improves in zero-shot image classification and reduces dependence on irrelevant features. Authors found that CLIP can deduce these attributes from an image itself and based on this fact to propose PerceptionCLIP. PerceptionCLIP first determines contextual attributes from an image and then classifies the object based on these attributes. Experiments are done on CLIP's zero-shot classification settings and show clear improvements over the original CLIP."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well written and easy-to-follow. While the concept of utilizing background information for image classification isn't groundbreaking in literature, its application to CLIP could be innovative.\n- The experiments show clear advantages of using contextual attributes over the traditional 80 templates."
            },
            "weaknesses": {
                "value": "- The authors assert at least twice that PerceptionCLIP mirrors human perception. However, I'm not entirely convinced. Authors gave the preliminary that: \u201chumans first infer contextual attributes (e.g., background and orientation) which help separate the foreground object from the background, and then classify the object based on this information.\u201d Yet, there's no evidence indicating that PerceptionCLIP actively separate foreground from background during classification, or that such separation is utilized the model. It's possible that PerceptionCLIP utilizes background attributes differently.\n- The authors refer to background information as spurious features (e.g. Figure 1). To my knowledge, it is not completely correct. Though they can sometimes overlap, they are not the same. Background information is a broader concept, while spurious features specifically refer to misleading patterns that a model might incorrectly learn as being important. In addition, the GradCAM in Figure 1 primarily emphasizes the foreground, consistent with [a], without highlighting any reduced reliance on spurious features. It's more accurate to state that it offers enhanced focus on foreground objects.\n- When I like the idea of Textual descriptions for contextual attributes Sec 4.1., I could not find how exactly you map Z using the proposed annotation function alpha to attribute text descriptions. Also, why do you say this annotation function model human preferences in captioning? Authors may also want to clarify the p value associated with the textual descriptions.  I imagine that these descriptions can also easily obtained using LLMs (e.g. ChatGPT).\n- The name of the proposed metric can be easily confused with the original CLIP score. How about naming it as Attribute-CLIP."
            },
            "questions": {
                "value": "Post-rebuttal:\n\nI genuinely appreciate your great efforts put into this rebuttal!\n\nI read the (updated) paper one more time, the authors' responses to me thoroughly, and the responses to other reviewers.\nWhile PercentionCLIP is indeed powerful and but answer of \"why does it work\" is not fully addressed via the GradCAM visualization as raised by W2 of reviewer vx4m. \n\nI would like to keep my rating for now and may change later after discussing with other reviewers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5829/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5829/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5829/Reviewer_k2xX"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5829/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698981770397,
        "cdate": 1698981770397,
        "tmdate": 1700775988504,
        "mdate": 1700775988504,
        "license": "CC BY 4.0",
        "version": 2
    }
]