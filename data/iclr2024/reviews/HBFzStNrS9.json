[
    {
        "id": "1QACuTSVXW",
        "forum": "HBFzStNrS9",
        "replyto": "HBFzStNrS9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission111/Reviewer_Ubis"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission111/Reviewer_Ubis"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a trainable module termed Unitention, an abbreviation for universal-individual cross-attention, to improve deep features of a given neural network by attending the features of a data sample to those of the entire dataset. They are inspired by traditional visual encoding methods, such as bag-of-visual-words, to attend to the entire datasets for visual recognition. The paper validates the effectiveness of this new approach through certain experimental evaluations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper proposes an attention mechanism to attend to the entire dataset rather than to individual images for visual recognition,  which is a kind of novel approach.\n2. The paper is well-written with good organization.\n3. The paper justifies its claim through several good experimental settings. For instance, they performed a training-free study on the attention mechanism to justify their design.\n4. They perform detailed experimental evaluation, and the effectiveness of this approach is validated by the improvement of visual recognition performance."
            },
            "weaknesses": {
                "value": "While the methods approach the visual recognition problem through a novel perspective, i.e., the contextual information of the dataset, I suspect this is not very fair for the testing scenario, as testing should be performed via individual images, one by one. If one intends to include the contextual information from datasets, one also should not include the extra data in the dataset during the testing phase, otherwise, it is not fair. \nI only have this concern."
            },
            "questions": {
                "value": "No other questions, but how is including extra data during the testing phase fair for evaluation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission111/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission111/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission111/Reviewer_Ubis"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission111/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698561627924,
        "cdate": 1698561627924,
        "tmdate": 1699635936396,
        "mdate": 1699635936396,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UvRiQW1OXF",
        "forum": "HBFzStNrS9",
        "replyto": "HBFzStNrS9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission111/Reviewer_CKwM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission111/Reviewer_CKwM"
        ],
        "content": {
            "summary": {
                "value": "This research introduces a sample-dataset interaction mechanism through cross-attention block, termed Unitention, which contributes to enhance the deep feature for classification. To be specific, the universal distribution for dataset is characterized by a collection of class-specific prototypes, and accumulated in an EMA manner with label annotation. Unitention module is evaluated on multiple architectures."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(+) Adopting the holistic representation to depict the dataset distribution is an interesting exploration. And it maybe inspire to subsequent works.\n\n(+) The universal information and individual information is interacted with a simple-yet-effective cross-attention block.\n\n(+) The proposed method is evaluated on extensive architectures, and obtains healthy gains."
            },
            "weaknesses": {
                "value": "(-) The relation between Unitention and codebook/k-NN is somehow overclaimed. \n\n(1) Typically, codebook is constructed using unsupervised methods, often employing K-means. However, in this study, But in this work, it seems that the codebook is dependent on labels.\n\n(2) Unlike the selection of k-nearest neighbor in k-NN, the unitention module instead aggregates all feature candidates for cross-attention. In fact, it seems that such unitention module does not allow a partial neighbor, i.e., K less than class number, because each candidate responses a specific class. \n\n\nAlthough Unitention shares some commonalities with coding algorithm, it is essential to clearly highlight their differences to prevent misleading readers.\n\n\n(-) The migration ability for annotated universal bank is limited.\nThe Universal bank essentially serves as a class prototype, requiring label annotations for aggregation. Such mechanism, which relies on annotation information, faces the challenge when migrating to other downstream task with different categories. \nAdditionally, such character should be clearly outlined in section 2.2. Moreover, it is advisable to movie the updating principle for universal banks is recommended to section 2.2.\n\n(-) The algorithm is not clearly demonstrated.\nAs the clarification in paper, the training-free approach is like a variation of k-NN. But how does the classifier come from for test phase? By aggregating training samples?\n\n(-) Lack of some empirical studies.\n\n(1) It is recommended to analysis Unitention training/inference efficiency. In particular, the computational and storage demands for the additional 1000xC class-level feature bank.\n\n(2) Lack of the evaluation on other popular vision tasks, e.g., detection or segmentation."
            },
            "questions": {
                "value": "Other Comments:\n\nQ1: About the hyperparameter analysis on $\\tau$. It seems that tuning $\\tau$ obtains positive gains with momentum 0.8 setting, i.e. row 7, 9, 10 and 11 in Table 4. Why does fix $\\tau$ as 1 finally?\n\nQ2: In Table1, the k-NN accuracy is obtained by the model trained with only k-NN classifier?\n\nQ3: It seems that there is a typo in section 3.2, i.e., BL* in tab:imn -> BL* in tab:1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission111/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698715892912,
        "cdate": 1698715892912,
        "tmdate": 1699635936323,
        "mdate": 1699635936323,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GARAnpV262",
        "forum": "HBFzStNrS9",
        "replyto": "HBFzStNrS9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission111/Reviewer_yV5u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission111/Reviewer_yV5u"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an attention module named Unitention in order to improve the performance of different backbone models with considering the universal information from entire dataset. Unitention takes the output feature vector of a backbone as input and enhances the feature with universal feature from a proposed universal feature bank using cross-attention. The universal feature bank is updated using a momentum mechanism according the classes of input samples. The results show that Unitention demonstrates performance gains on different backbone models and can generalize to multiple modalities."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The proposed method is simple and logical, \n\n(2) Sufficient experiments with multiple backbones and different modalities are provided. \n\n(3) This paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "(1) Parameters, flops and inference speeds are not provided. It shows that an Unitention module contains multiple FC layers and a feature bank. When applying an attention module to a backbone, it will improve its performance but also increase its parameter and flop usually. The authors should provide the parameters, flops and inference speeds for models with and without Unitention module, so that we can make sure whether Unitention can make a good trade-off between performance and cost. For example, the parameters, flops and inference speeds of ResNet50, ResNet101, ResNet152, ViT-Small, Swin-Tiny with and without Unitention.\n\n(2) The authors considered different models and modalities in the experiments. While Unitention was only tested in single-label global-feature-based classification tasks in the experiments. I think the authors could provide some experimental results of Unitention for different tasks such as multi-label classification to show its generalization ability.\n\n(3) From my point of view, Unitention is an enhanced design of classification head focusing on the information of classes, since it uses feature vectors after the global average pooling operation. I think the authors should compare Unitention with other classification head designs, for example, iSQRT-COV[1]. Or the authors could show that Unitention is complementary with them, which can still demonstrate performance gains on the models with other heads. \n\n[1] Wang, Qilong, et al. \"Deep cnns meet global covariance pooling: Better representation and generalization.\" IEEE transactions on pattern analysis and machine intelligence 43.8 (2020): 2582-2597."
            },
            "questions": {
                "value": "(1) In the ablation study, how does Unitention update the feature bank when the number of class centers is less than number of classes?\n\n(2) I'm wondering about the performance of Unitention when the feature bank is replaced by trainable parameters. That is, the parameters in the feature bank is updated according to the gradient but not proposed method.\n\n(3) Will the backbones with Unitention still show performance gains when fine-tuned on the downstream tasks such as detection and segmentation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission111/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698730663171,
        "cdate": 1698730663171,
        "tmdate": 1699635936227,
        "mdate": 1699635936227,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "h4W9DWyUok",
        "forum": "HBFzStNrS9",
        "replyto": "HBFzStNrS9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission111/Reviewer_zCBo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission111/Reviewer_zCBo"
        ],
        "content": {
            "summary": {
                "value": "This paper attempts to improve the performance of a deep neural network (DNN) by modifying its head structure instead of the backbone structure. Specifically, the authors present a trainable fusion module called Unitention. The basic idea of Unitention is to combine individual feature encoding and universal feature encoding. Specifically, individual feature encoding is just the feature output from a given DNN backbone, while universal feature encoding takes the feature output from this given DNN backbone as the input, and uses a cross-attention module following the self-attention concept in popular transforms to capture sample-to-dataset relations. Then, the output of Unitention is just the sum of the outputs from both individual feature encoding and universal feature encoding. Experiments conducted on image classification (with ImageNet-1K dataset), fine-grained classification (with iNaturalist 2018) and one-dimensional signal classification (with three datasets for device/sensor/medical signals) tasks are provided to show the efficacy of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper is well written in most parts.\n\n+ The idea of the proposed method is easy to understand.\n\n+ Comparative experiments are performed on three types of benchmarks with different deep neural network architectures including convnets and vision transformers.\n\n+ The proposed method shows improvement to baselines on different datasets.\n\n+ The limitations of the proposed method are also discussed."
            },
            "weaknesses": {
                "value": "- The motivation, the method and related works.\n\nThe motivation of this paper is to improve the performance of a deep neural network (DNN) by designing a new head structure, or say enhancing feature encoding for the classification head. \n\nThe authors present Unitention, a trainable fusion module that can be inserted after the backbone structure. The basic idea of Unitention is to combine individual feature encoding and universal feature encoding. Specifically, individual feature encoding is just the feature output from a given DNN backbone, while universal feature encoding takes the feature output from this given DNN backbone as the input, and uses a cross-attention module following the self-attention concept in popular transforms to capture sample-to-dataset relations. Then, the output of Unitention is just the sum of the outputs from both individual feature encoding and universal feature encoding. \n\nHowever, the motivation, the idea and the cross-attention design of the proposed Unitention are not new. As a fundamental research topic, there already exist a large number of previous research works that focus on designing a better head structure or enhancing feature encoding for the classification head. Unfortunately, the authors totally ignore this line of research. In what follows, I just list some representative works as well as recent works in this field. \n\n[1] Mircea Cimpoi, et al. Deep Filter Banks for Texture Recognition and Segmentation. CVPR 2015.\n\n[2] Relja Arandjelovic, et al. NetVLAD: CNN architecture for weakly supervised place recognition. CVPR 2016.\n\n[3] Yang Gao, et al. Compact Bilinear Pooling. CVPR 2016.\n\n[4] Feng Zhu, et al. Learning Spatial Regularization with Image-level Supervisions for Multi-label Image Classification. CVPR 2017.\n\n[5] Mengran Gou, et al. MoNet: Moments Embedding Network. CVPR 2018.\n\n[6] Shilong Liu, et al. Query2label: A simple transformer way to multi-label classification. arXiv preprint arXiv:2107.10834, 2021.\n\n[7] Jiangtao Xie, et al. Sot: Delving deeper into classification head for transformer. arXiv preprint arXiv:2104.10935, 2021.\n\n[8] Ke Zhu, et al. Residual Attention: A Simple but Effective Method for Multi-Label Recognition. CVPR 2021.\n\n[9] Chao Li, et al. NOAH: A New Head Structure To Improve Deep Neural Networks For Image Classification. ICLR 2023 Open Review. \n\n- The experiments.\n\nAs I mentioned above, there already exist a large number of previous research works that focus on designing a better head structure or enhancing feature encoding for the classification head. However, the authors totally ignore them throughout the paper, including in the experimental comparison. Comprehensive comparisons of the proposed method to existing works are necessary.\n\nHow about the extra training cost of the proposed method against the baseline?\n\nI would like to the transferring ability of Unitention to downstream tasks as they are more important in real applications."
            },
            "questions": {
                "value": "Please refer to my detailed comments in \"Weaknesses\" for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission111/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840381517,
        "cdate": 1698840381517,
        "tmdate": 1699635936155,
        "mdate": 1699635936155,
        "license": "CC BY 4.0",
        "version": 2
    }
]