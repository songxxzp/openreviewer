[
    {
        "id": "mC8FEcsu8T",
        "forum": "hB2hXtxIPH",
        "replyto": "hB2hXtxIPH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7432/Reviewer_CxDZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7432/Reviewer_CxDZ"
        ],
        "content": {
            "summary": {
                "value": "The authors incorporate sequential execution into a value-decomposition-based MARL algorithm, which can thus be applied to both homogeneous and heterogeneous tasks, while maintaining effective credit assignment. \nTwo approximations are involved: 1) using greedy marginal contribution as the assigned credit for each individual, and 2) using greedy actions (augmented by Monte Carlo samples) as the optimal actions taken by subsequent agents.\nThe experiments show the efficacy of this approach, demonstrating its ability to adapt to dynamically changing partners within homogeneous, heterogeneous, and mixing scenarios."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Originality. The proposed method addresses common problems in MARL algorithms, such as the relative overgeneralization and credit assignment, at the same time. This enhancement expands the scope of potential applications of value-decomposition-based MARL algorithms. \n- Clarity. The paper is basically clear and easy to follow."
            },
            "weaknesses": {
                "value": "- The algorithm is tested exclusively on specially designed tasks. There are no additional experiments conducted on common benchmarks.\n- The ablation study section fails to provide an in-depth discussion of the functions of the key components in the algorithm. I would appreciate a more detailed description of the ablated algorithms and a clearer explanation of the results."
            },
            "questions": {
                "value": "- To generate a joint action $u$, each agent must calculate its action $a$ sequentially, which prevents parallel processing. Therefore, with each agent requiring $t$ time to produce its action, a single environment step requires $n*t$ units of time for $n$ agents. Compared with baseline algorithms like QMIX, does GSE require more wall-clock time for training and evaluation? If so, could you provide a rough estimate of the additional time required?\n- During training, is the execution sequence of all agents fixed, or do you shuffle the sequence?\n- What does 'training with a larger scale of agents' mean in Section 6.3 & Figure 6?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7432/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698745742508,
        "cdate": 1698745742508,
        "tmdate": 1699636892589,
        "mdate": 1699636892589,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wEG0n7U2j1",
        "forum": "hB2hXtxIPH",
        "replyto": "hB2hXtxIPH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7432/Reviewer_GHAQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7432/Reviewer_GHAQ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a way of handling cooperative tasks with multi-agents when the tasks are not all homogeneous.\nThe approach proposed uses sequential execution policies, proposing the Greedy Sequential Execution (GSE), which learns the optimal policy for both cases (homogeneous and heterogeneous tasks). The GSE is evaluated empirically in multiple domains. The GSE  uses a value decomposition method that works for both homogeneous and heterogeneous tasks, and enables the agents to learn utilities that take into account the interactions with other agents. They also propose a credit assignment method that computes the marginal contributions of each agent. The marginal contribution avoids over-generalization, since it represents the optimal value of an action instead of the average value. A couple of theorems are proved in an appendix (not attached to the paper). The experiments use homogeneous scenarios, heterogeneous scenarios, and mixed scenarios. \nMultiple appendices are mentioned but they are not attached to the paper and are not accessible."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is very well written and easy to follow. The work presented addresses a known problem (over-generalization) that takes place when combining homogeneous and heterogeneous tasks and expands the applicability of agent-based approaches to situations where some tasks are homogeneous and some are heterogeneous. Having to deal with a mix of the two types of tasks might not be too common, but when mixes of tasks are used, the approach proposed here, even if greedy, becomes quite useful."
            },
            "weaknesses": {
                "value": "The paper is more appropriate for a journal than for a conference. The numerous appendices are not included in it for space reasons, but are important to understand the method more in depth.\nFigure 2 with the architecture is hard to read and not well explained."
            },
            "questions": {
                "value": "1. The use of Monte Carlo method to estimate the optimal cooperative actions with previous agents to maximize the greedy marginal contribution is mentioned with no other details. How is the number of actions selected?\n2. In the Overcooked scenario, I understand why the size of the map affects the complexity, but there is no indication of how large the maps are. I believe the number of agents is fixed to two. Have you tried a larger number of agents?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7432/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813825418,
        "cdate": 1698813825418,
        "tmdate": 1699636892472,
        "mdate": 1699636892472,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "N1ua7CJlpJ",
        "forum": "hB2hXtxIPH",
        "replyto": "hB2hXtxIPH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7432/Reviewer_ia9o"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7432/Reviewer_ia9o"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a unified framework for learning policies for multiple agents where there are homogeneous and heterogeneous tasks, with the goal of addressing limitations of current methods that work with either homogeneous or heterogeneous tasks. Specifically, the paper proposes greedy sequential execution, with value decomposition including a utility that encodes also the interactions between agents and credit assignment calculated as marginal contribution of Shapley values. Simulation experiments considering different types of tasks are performed and results include comparison with other methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- the paper addresses an open problem in MARL, where a general framework able to learn optimal policies for both homogeneous and heterogeneous tasks is still not fully present.\n\n- the paper presents a technically sound method, with the augmented utility, sampled considering the joint actions of other agents who might cooperate, and with the greedy marginal contribution \n\n- the paper includes a fairly comprehensive evaluation of the proposed method and compares with a good number of state-of-the-art approaches, with corresponding insights from the results, as well as ablation studies.\n\n- the paper has a structure that overall clearly show the gap, with specific examples, and motivates the proposed approach."
            },
            "weaknesses": {
                "value": "- a trend that should be discussed more in detail is in Overcooked, where there is a large standard deviation for both Easy and Medium, with Easy having a decreasing return past 900 episodes. It seems that it didn't converge.\n\nA few minor presentation comments: \n- Section V already introduces comparison methods that are presented in Section VI. It is better instead to introduce the comparison methods in Section V so that the reader doesn't have to guess what methods are they.\n- it is worth including graphically the map for the overcooked environment.\n- the references, when the authors name are not used in the sentence should be all in parentheses, e.g., \" experiences Sunehag et al. (2017); Rashid et al. (2018).\" -> \"experiences (Sunehag et al., 2017); (Rashid et al., 2018).\"\n- please ensure to include the correct venue for papers, instead of just including the arxiv version, e.g., the MAVEN paper was published in NeurIPS 2019."
            },
            "questions": {
                "value": "Please comment on the trends of overcooked as mentioned in the \"Weaknesses\" box."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7432/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7432/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7432/Reviewer_ia9o"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7432/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832147423,
        "cdate": 1698832147423,
        "tmdate": 1699636892371,
        "mdate": 1699636892371,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "97WHERlXOo",
        "forum": "hB2hXtxIPH",
        "replyto": "hB2hXtxIPH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7432/Reviewer_CQwR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7432/Reviewer_CQwR"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a general framework for solving both homogeneous and heterogeneous cooperative tasks, which they call Greedy Sequential Execution (GSE). The key idea behind GSE is to sequentially execute the actions of each agent, while taking into account the dependencies between agents measured by greedy marginal contribution."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The analysis in Sec 3.1 is valuable for readers to gain a deeper understanding of the performance of different policies in homogeneous and heterogeneous scenarios."
            },
            "weaknesses": {
                "value": "The authors claim that existing solutions have not been successful in **addressing both homogeneous and heterogeneous** scenarios simultaneously, and thus GSE is the first to propose for both scenarios, whose key idea is integrating **sequential execution** and **value decomposition** framework for better **credit assignment** and **cooperation**. However, [1] was published in 2022, which also proposed a general framework based on **sequential execution** for **cooperative games** with **both homogeneous and heterogeneous** agents, and leveraged **advantage value decomposition** for **credit assignment**. \n\nThere are too many overlaps in key ideas between these two works, while the main difference seems to be [1] implements these ideas with PPO and sequence model, i.e. transformer, and GSE implements these ideas in a QMIX-like pattern.\n\nHowever, I have not found any comparison, discussion, or citation to [1] in this paper, which should be compared thoroughly, or the contributions might be significantly weakened.\n\n[1] Wen, Muning, et al. \"Multi-agent reinforcement learning is a sequence modeling problem.\" Advances in Neural Information Processing Systems 35 (2022): 16509-16521."
            },
            "questions": {
                "value": "I have no further question."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7432/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7432/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7432/Reviewer_CQwR"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7432/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699287690576,
        "cdate": 1699287690576,
        "tmdate": 1700654586201,
        "mdate": 1700654586201,
        "license": "CC BY 4.0",
        "version": 2
    }
]