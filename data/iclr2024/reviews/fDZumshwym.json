[
    {
        "id": "UYfBIpuDlP",
        "forum": "fDZumshwym",
        "replyto": "fDZumshwym",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4169/Reviewer_UGPZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4169/Reviewer_UGPZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel dataset condensation method following the data parameterization approach. The key idea is to use the newly designed Hierarchical Memory Network (HMN) to learn a three-tier representation of the condensed dataset. Experimental results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea proposed is interesting and novel.\n- The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "- It lacks a discussion and comparison to a very relevant SOTA method, i.e., IDM [1], which also addresses the scalability of data condensation and works in a low GPU memory cost scenario. Thus, it might be incorrect to argue that the proposed method is the \"first method to achieve such good performance with a low GPU memory loss\" and such statements should be revised.\n\n[1] Zhao, G., Li, G., Qin, Y. and Yu, Y., 2023. Improved distribution matching for dataset condensation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7856-7865).\n\n- The technical novelty is a bit thin. Specifically, the second contribution is relatively weak and more similar to a trick. It is interesting to know that over-budget generation and pruning may work, but this is more similar to remedying some of the shortcomings of the main idea rather than an independent contribution."
            },
            "questions": {
                "value": "Please see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4169/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698492993627,
        "cdate": 1698492993627,
        "tmdate": 1699636382896,
        "mdate": 1699636382896,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "R9lF0puoUS",
        "forum": "fDZumshwym",
        "replyto": "fDZumshwym",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4169/Reviewer_RRfc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4169/Reviewer_RRfc"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel data parameterization architecture named Hierarchical Memory Network (HMN), which consists of dataset-level, class-level, and instance-level memory and a feature extractor and a decoder to better leverage the hierarchical nature of the image. Furthermore, the paper proposes instance-level pruning to remove redundant images to improve performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper proposes a novel framework for DC with data parameterization, instead of updating input images, they update the memory (i.e, dataset, class, and instance) and decoder.\n2. Identify the redundancy in condensed images using AUM and suggest to first condense images with over-budget (p%) and then perform post-condensation pruning to remove redundant data."
            },
            "weaknesses": {
                "value": "Although the proposed method (including HMN and condensed over-budged & pruning) is novel, there are several weaknesses:\n\n1. Lack of experiments on higher resolution such as 128x128 (Image Woof/Nette) or 224x224 (ImageNet-10/100, a subset from ImageNet, as done in IDC)\n2. Lack of describing the memory in detail. For example, how does the memory look like?\n3. The evaluation seems to be different from previous works. For example, HMN evaluates condensed images with a cosine annealing learning rate (LR) scheduler, while DSA or IDC uses a multistep LR scheduler. Thus, the comparison may not be fair."
            },
            "questions": {
                "value": "I have several questions:\n1. What does the memory look like? How to initialize these memories, feature extractor, and decoder?\n2. HMN stores parameters instead of images, this will incur extra resources for evaluation (to generate images). Is it correct?\n3. How many generated images per class  (GIPC) used in Table 2? \n4. In Table 2, the result of IDC on CIFAR-10 with 50 IPC is lower than in the paper (74.5 (IDC) vs 71.6 (this paper)). Is it a mistake?  On CIFAR-100, this paper reports the accuracy of IDC with 10 IPC on CIFAR-100 is 44.8 while on other papers such as DREAM, the accuracy is 45.1. Can the author double-check? The results of IDC (at 10 and 50 IPC) in Table 2, Table 3, and Table 6 are different. Can the author explain?\n5. As shown in Figure 4 (right),  with an instance memory size near 100, around 600 images per class are generated. How many images per class (per epoch) are used for training in the evaluation phase? 600? If this is correct, the training time with condensed images is much longer than the conventional method where the total images per lass are set as the IPC. Can the author clarify this?\n\nAlthough the proposed method is novel and seems promising, there are unclear parts regarding the GIPC, the memory, and fairness in evaluation comparison. Additionally, some reported results of IDC of this paper are different from those in the original paper without explanation. Thus, I give a borderline reject."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4169/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4169/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4169/Reviewer_RRfc"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4169/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698754440523,
        "cdate": 1698754440523,
        "tmdate": 1699636382823,
        "mdate": 1699636382823,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zGZ7q97OrK",
        "forum": "fDZumshwym",
        "replyto": "fDZumshwym",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4169/Reviewer_Y8Kv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4169/Reviewer_Y8Kv"
        ],
        "content": {
            "summary": {
                "value": "This paper works on the task of dataset condensation, i.e., producing a data generator whose generated data can be used to train a network that retain similar performance with respect to using a full dataset. The authors proposed a novel hierarchical data generation pipeline, which starts from a dataset level feature, then go through class-specific feature processing layers, and instance-level feature processing layers, and a uniform decoder to decode the features into images. Experiments show the proposed method achieves good performance on 3 benchmarks, on-par or out-performing methods with more expensive computes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The task of dataset condensation is an important and interesting task. This paper provide a good summarization of existing literature, and make solid progresses to this field.\n\n- The proposed methods of hierarchical memory decoding is intuitive and makes a lot of sense to me. The followup pruning technique is also clean.\n\n- Experiments show the proposed method work well on different benchmarks with different data budgets, outperforming existing methods which use more computes during training."
            },
            "weaknesses": {
                "value": "- The authors highlighted that they are using a more efficient training loss \"batch-based\", and claim it is sufficient to outperform a better but more expensive training loss \"trajectory-based\". Can the proposed method also use \"trajectory-based\" loss to further improve the performance? Or is it the proposed method itself is heavier then others, so that it can't be optimized using the better loss? Note experiments are optional in the rebuttal.\n\n- As a researcher not working on this field, I had a hard time understanding what \"IPC\" means in experimental setup. Through searching other papers I get it is \"Images allocated Per Class\". It would be good if the paper can be more self-contained, to introduce this setting in the paper, and provide background knowledge on why this is used as the main setting. A more intuitive setting in my mind (without knowing the literature) is control the number of parameters of the learned data generators. Is this the same as the setup used in the paper?"
            },
            "questions": {
                "value": "Overall this paper proposed a novel and valid method on an important task, with solid results. My questions are mostly for clarification, as I don't work on this field. I am happy to vote for an accept for now, conditioned on the authors clarifying my confusions in paper weaknesses during the rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4169/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814052641,
        "cdate": 1698814052641,
        "tmdate": 1699636382719,
        "mdate": 1699636382719,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Tci11265fN",
        "forum": "fDZumshwym",
        "replyto": "fDZumshwym",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4169/Reviewer_mnZx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4169/Reviewer_mnZx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method called Hierarchical Memory Network (HMN) that stores condensed data in a three-tier structure that reflects the hierarchical nature of image data. The authors exploited that HMN naturally ensures that generated images are independent and proposed a new algorithm to remove redundant images. The authors evaluated the model on four different datasets, and showed that their technique outperformed several SoTA baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1- The paper proposed a new algorithm to store condensed data in a three-tier memory structure: dataset-level, class-level, and instance-level.\n2- The authors proposed a pruning algorithm to prune redundant examples.\n3- The authors demonstrated the effectiveness of their method on 4 different datasets. Their method outperformed several SoTA baselines by convincing margines."
            },
            "weaknesses": {
                "value": "1- It is difficult to follow some of the ideas presented in the paper. For example, the paper didn't mention the abbreviation for IPC. Also, the paper didn't talk about whether the decoder parameters is part of the budget or not.\n\n2- The paper demonstrated the effectiveness of their method on tiny datasets, and they didn't address the scalability of their technique. For example, the generated images seem very pixelated and abstract, how does their method perform in a more complex settings."
            },
            "questions": {
                "value": "Table.1 What is the accuracy drop for randomly pruning 10% ?\n\nAlgorithm 1.  are you computing the accuracy in an online fashion, or you train the model for each subset selection? How this would scale for larger datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4169/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698864788753,
        "cdate": 1698864788753,
        "tmdate": 1699636382653,
        "mdate": 1699636382653,
        "license": "CC BY 4.0",
        "version": 2
    }
]