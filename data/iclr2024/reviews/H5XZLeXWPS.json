[
    {
        "id": "qAXBa8BQf5",
        "forum": "H5XZLeXWPS",
        "replyto": "H5XZLeXWPS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7818/Reviewer_rkxv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7818/Reviewer_rkxv"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes MEMWALKER, a method that enables the LLM to read long-text interactively via iterative LLM prompting of first creating a memory tree and second navigating this tree to answer a query. During the first stage, the long-text is segmented into small chunks that fit within the LLM's context window, to summarize into a textual summary node. These summaries are further summarized, building a hierarchical summary tree structure. MEMWALKER is evaluated on three long context question answering tasks and demonstrate superior performance to existing baselines. MEMWALKER also enhances explainability by highlighting the reasoning steps as it interactively reads the text and can pinpoint the relevant text segments related to the query."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper is clearly written and easy to follow.\n* The paper tackles a real world and important problem of question answering on long input documents that are beyond the context window constraint of LLM-based LLMs."
            },
            "weaknesses": {
                "value": "* One limitation of the method is that each time the underlying long input changes, the memory tree must be constructed again, which requires processing the entire long input multiple times through an expensive LLM. This is more than just processing all the input tokens, as the hierarchical summaries are also created to form the *memory tree*. Therefore it could be useful to include a discussion of this limitation, and perhaps due to this, this method is best suited to applications where the long input does not change, such as customer service question answering for pre-defined product information.\n* It could be useful for the reader if you could provide more empirical results with other LLMs, e.g., GPT3.5, GPT4, Llama 2 etc.\n* It could be helpful for the reader if you provide practical guidelines for setting the maximums number of nodes, and the segment size for a given new task or practical example.\n* No error bars for results. Could you include error bars for the results in Table 2, Table 3, Figure 2, Figure 3, Table 4, Figure 4 and Figure 5. Given there are no error bars, the results seem marginal, particularly for Table 2, and it appears the method is only performing significantly well on the GovReport task. Including error bars would help the reader see which results are statistically significant and not overlapping.\n* For Contriever (the retrieval baseline), it would be more indicative if you only included the top k retrieved responses, where $k=3$ for example or even less, instead of including all segments until they fill the context (as this could bloat the context with irrelevant information---potentially leading to worse performance).\n* Figure 4 is misleading, as the total tokens to process, would be greater than the original example in tokens, as the original example needs to be initially processed into the *summary tree*.\n\n\n\n\n\n\n\nTypos:\n* Page 1. \"consuming large amount of\" -> \"consuming large amounts of\"\n* Page 2. \"find the node that contains relevant\" -> \"find the node that contains a relevant\"\n* Page 7. \"into two bucket of\" -> \"into two buckets of\""
            },
            "questions": {
                "value": "* For future work, it could be helpful to discuss the extension of using the query, or types of expected queries to construct the *memory tree*. As I can imagine knowing the types of queries or the family of possible queries that will be asked, the *memory tree* could be constructed for that particular query family, to give specific summarizations that best address those families of queries, and enable efficient searching.\n* *\" If the LLM fails to generate parsable output three consecutive times, the navigation terminates and returns \u201cno answer\u201d.\"*. How many times does this occur in practice?\n* *\"Further summarizing the working memory as it accumulates would be an alternative approach, which we\nhave not explored in this study.\"*. It could be really interesting to explore this as a further ablation.\n* Can the method handle a larger maximum number of nodes $M_t$? Can you perform an ablation, perhaps showing it handling $M_t =\\\\{ 16, 32, 64, 128\\\\}$?\n* In Table 4, the stray ratio is quite high, is there any way to lower it? Does this indicate that the method is not the most efficient approach?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7818/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7818/Reviewer_rkxv",
                    "ICLR.cc/2024/Conference/Submission7818/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7818/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723621266,
        "cdate": 1698723621266,
        "tmdate": 1700614933751,
        "mdate": 1700614933751,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Lv7d8Vcdef",
        "forum": "H5XZLeXWPS",
        "replyto": "H5XZLeXWPS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7818/Reviewer_T9FN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7818/Reviewer_T9FN"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method for handling long texts using LLMs. The method is based on first building a hierarchical summary tree and then, given a query, traversing the tree in order to find the relevant part of the input document. The authors perform a number of experiments that demonstrate the promise of their method, especially on long sequences."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper \n- Attacks a highly relevant topic of handling extremely long documents with LLMs which might not have long enough context to handle such sequences.\n- Proposes a new approach to the problem.\n- Is clearly written and is a pleasure to read."
            },
            "weaknesses": {
                "value": "Overall, I could see that the authors put a lot of work in their paper, and I appreciate the contribution in many ways, I believe that unfortunately, the paper will require substantial improvement before it (in my opinion) could satisfy the extremely high bar of the ICLR conference. The main weakness of the paper is the quality of its experimental support. Novelty is another concern.\n\n## Experimental support issues.\n\n### Retrieval comparisons\nThe paper, fundamentally, proposes a way to work around context length limitations through a creative use of summarization. It is crucial, therefore, to provide thorough comparisons with other methods of this type. We only have a comparison with Contriver, which is a BERT-based architecture. Given that the underlying model in Contriver is much weaker than what was used in MemWalker, it is impossible to evaluate what proportion of performance gains was due to using MemWalker summarization technique and what proportion is due to a stronger base model.\n\nTo provide a fair comparison, it seems crucial to find a base model that could be used for both MemWalker and retrieval-augmented approaches. Any LLM that can output sentence/paragraph embeddings and is instruction fine-tuned could be a good start.\n\n### Simple summary comparisons\n\nAnother comparison that seems to be missing is a non-tree-based summary search. I understand that tree-based search speeds up query answering, but sometimes we might only have one query per document. It seems important to compare MemWalker with a simple flat summary scanning approach (Figure 5 looks promising, but still only considers tree-structured approaches).\n\n### Long-Context LLM comparisons\n\nOn inputs that fit into LLM context windows, we don't see any advantage of MemWalker over the base model. Truncating the context would, of course, degrade performance.\n\nThe paper states that scaling context lengths is fundamentally limited (\"the attention mechanism may become less effective due to positional biases as the sequence length becomes very long\"). I agree that it might be the case, but to show that MemWalker addresses the issue, we need to have a fair comparison, that is, we need to compare MemWalker with models that can fit the inputs into their context.\n\nOne simple option would be to add GPT-4 (long-context version) and/or Anthopic's Claude. Both models would be able to fit even the longest sequences considered in the paper into their context.\n\nTo make the case clearer (to push context scaling to its limit), it could be reasonable to focus on longer-context documents than what was done in the paper. [NarrativeQA](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00023/43442/The-NarrativeQA-Reading-Comprehension-Challenge), BookSum (https://arxiv.org/abs/2105.08209), and NarrativeXL (https://arxiv.org/abs/2305.13877) could all be a good place to start.\n\n### Experiment scale and result reporting\n\nIn general, the authors used very small data subsets to evaluate model performance. I understand the resource limitations involved in such studies. But, especially when data is so limited, it is crucial to provide a thorough statistical analysis of the obtained results. As presented, it is impossible to estimate which results reflect actual differences in model performance and which are due to noise.\n\n## Novelty concerns\n\nWhile I understand that the specific way in which the model is traversing the memory tree is novel and original, using hierarchical summarization as a way to handle long documents is an idea that has been known for a long time.\n\nSee, e.g. Yang, Christopher C., and Fu Lee Wang. \"Hierarchical summarization of large documents.\" Journal of the American Society for Information Science and Technology 59.6 (2008): 887-902.\n\nWhile this limits the novelty and potential significance of the work, this limitation is not as crucial as the experimental support issues I listed above.\n\n\n## Conclusion\n\nOverall, unfortunately, in its present state, I can not recommend the paper for publication. I do hope to see a revised version of this paper published in the future."
            },
            "questions": {
                "value": "In case I missed it, I was wondering what was the performance of a \"flat summarization\" approach (i.e. no tree, only leaf-level summaries + search over all of them during retrieval). I understand that some relevant data is reported in Figure 3, but it seems that \"flat summarization\" and \"mem-walker w/o memory\" are not exactly equivalent."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7818/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797740575,
        "cdate": 1698797740575,
        "tmdate": 1699636957101,
        "mdate": 1699636957101,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IZNR2twS5e",
        "forum": "H5XZLeXWPS",
        "replyto": "H5XZLeXWPS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7818/Reviewer_yntC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7818/Reviewer_yntC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes MemWalker, a method that allows LLMs to process long context corpora through iterative prompting. It does by first 1) constructing memory tree- text is recursively summarized into a tree structure with summary nodes, going from segments to higher level summaries. 2) Navigating the tree- Given a query, the LLM navigates the tree structure by reasoning about which child node to go to in order to find relevant information, reaching a leaf node containing the full text segment that allows it to answer the query."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper outperforms baselines on 3 datasets and shows good results. The navigation process of the memory tree gives some explanation of the model's reasoning process of the answer. The idea is interesting."
            },
            "weaknesses": {
                "value": "The scalability of MemWalker is a concern. The method doesn\u2019t outperform full context for very short texts. It is unlikely that the model will scale to extremely large scale texts given the computational overhead, limiting the model to medium length texts. As the context windows of models expand, many medium-sized documents will fit in the context length, making the MemWalker not necessary.\n\nThe comparison with two large-context models using a significantly smaller 13B parameter set against MemWalker's 70B model does not seem fair. A  better comparison would be with models of similar size, for example with  llama models fine-tuned for larger contexts using positional interpolation.\n\nDetails on the time and token scaling of MemWalker are absent thus it's unclear if the computational costs are worth the increased quality of medium-sized document. MemWalker bears a resemblance to the tree index method by the llama index and the description of the method in the paper is not sufficient (please see questions).\n\nThe method also relies heavily on summarizations which may suffer from hallucinations, and there is no discussion of this."
            },
            "questions": {
                "value": "How are the nodes grouped together to be summarized ? \n\nIn the case the grouped nodes exceed the maximum context length of the model ? how is this case handled ? \n\nHow does the method scale in terms of wall clock and tokens ? \n\nHow does the model compare to larger context windows with model of similar size ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7818/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7818/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7818/Reviewer_yntC"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7818/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698835386070,
        "cdate": 1698835386070,
        "tmdate": 1699636956998,
        "mdate": 1699636956998,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nX3exNKsPX",
        "forum": "H5XZLeXWPS",
        "replyto": "H5XZLeXWPS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7818/Reviewer_nCny"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7818/Reviewer_nCny"
        ],
        "content": {
            "summary": {
                "value": "In the research article titled \"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading,\" the authors address the challenges associated with long-sequence question-answering tasks using Large Language Models (LLMs). Given that these tasks often require referencing extensive text segments that surpass the standard context-window of an LLM, the study introduces an alternative to extending the LLM's context-window or incorporating recurrence or retrieval-augmented generation. The proposed method involves segmenting and summarizing the text, followed by the assembly of a navigable knowledge tree composed of these summaries. Importantly, the leaf nodes of this tree retain the original text segments, enabling the model to interactively traverse and reference the content, enhancing its question-answering capabilities."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "***Strengths of the Paper:***\n1. Novelty and Originality:\nThe paper introduces MemWalker, an alternative approach to managing long-sequence reading tasks, suggesting a new direction that differs from the existing trend of expanding model context windows.\n\n2. Rigorous Experimental Design:\nThe paper's experimental framework is detailed, employing a variety of datasets and metrics, which supports the validity of the MemWalker system's evaluated performance.\n\n3. Clarity of Presentation:\nThe methodological process, including memory tree construction and interactive navigation, is clearly delineated, facilitating understanding and potential replication of the study.\n\n4. Substantial Findings:\nMemWalker's performance across different datasets and tasks is solid, indicating its usefulness in question-answering and information retrieval within long texts.\n\n5. Detailed Analysis and Discussion:\nA comprehensive analysis is coupled with a discussion that explores the broader implications and areas for future exploration, placing the findings in context with the existing body of research.\n\n6. Quality of Writing:\nThe paper is well-composed, with content organized in a manner that aids in the clear communication of the research to the reader.\n\n\n-----\n\n***Strengths of the Approach:***\n\n1. No Need for Fine-tuning:\nUnlike other methods, MemWalker avoids the cost-intensive process of extensive fine-tuning for longer sequences.\n\n2. Retention of Older Sequence Information:\nMemWalker is designed to retain older sequence information, avoiding the typical compression seen in some recurrent architectures that weakens recall.\n\n3. Effective Navigation with Working Memory:\nThe inclusion of working memory in MemWalker bolsters its performance, demonstrating its essential role in the navigation process.\n\n4. Ability to Recover from Traversal Errors:\nMemWalker exhibits some level of resilience, demonstrative effective recovery from initial navigation errors that lead to improved accuracy.\n\n5. Efficient Content Reading:\nMemWalker's methodology enables it to read and process content efficiently, requiring a smaller portion of the entire text to derive accurate answers.\n\n6. Flexible Memory Tree Construction:\nMemWalker's approach to memory tree construction strikes a balance, providing for some level of information compression without sacrificing content fidelity."
            },
            "weaknesses": {
                "value": "***Shortcomings of the Paper:***\n\n1. The results would be more convincing if they were compared against the SOTA retrieval, recurrence and length-extention tuned models. Currently, all results (except length-exntention fine-tuning) are based on Beluga 2 which is a the main weakness of this paper. \n\n2. It would be good to see how this method compares to recent approaches such as Landmark Attention: https://arxiv.org/abs/2305.16300\n\n3. The lack of a broader impact section weakens this paper.\n\n----\n***Shortcomings of the Approach:***\n1. Scalability with Extremely Long Sequences:\nMemWalker's memory tree generation might struggle with scalability for extremely long sequences. The growth in sequence length could lead to an overwhelming number of nodes, complicating the tree construction process.\n\n2. Dependency on LLM's Reasoning Capability:\nMemWalker's effectiveness is deeply tied to the robust reasoning capabilities of the LLM. The system requires a large (over 70B) and instruction-tuned LLM. A deficiency in this capability could compound errors, jeopardizing the method's success.\n\n3. Limitation of Zero-Shot Prompting:\nMemWalker solely relies on zero-shot prompting without tapping into the potential benefits of fine-tuning. This could constrain its interactive reading capabilities, leaving room for enhancement.\n\n-----\n\nThese are all shortcomings of the approach, highlighted and recognised by the authors."
            },
            "questions": {
                "value": "Will code be open-sourced (including the evaluation code and data splits)?\n\nIs it possible to provide results for benchmark on even longer contexts, e.g. 30k+ tokens?\n\n-----\n\n**Improving clarity and grammatical ammendments:**\n\n***Section 1. Introduction***\n\nOriginal: \"These tasks involve consuming large amount of information,\"\n\nProposed Correction: \"These tasks involve consuming a large amount of information,\"\n\n-------------\n\nOriginal: \"The context window, no matter how long it is extended, assumes a fixed size,\"\n\nProposed Correction: \"Regardless of how it is extended, the context window assumes a fixed size,\"\n\n-------------\n\nOriginal: \"While recurrence can manage infinite-length sequences, it often misses out on retaining information from earlier segments.\"\n\nProposed Correction: \"Although recurrence can handle infinite-length sequences, it frequently fails to retain information from earlier segments.\"\n\n-------------\n\nOriginal: \"Additionally, retrieving segments from the coherent long-text might be ineffective, given that many retrieval systems are tailored to distinguish similar but distinct documents.\"\n\nProposed Correction: \"Furthermore, retrieving segments from coherent, extended texts might be ineffective since many retrieval systems are designed to differentiate between similar yet distinct documents.\"\n\n-------------\n\nOriginal: \"To address these issues, we develop a fundamentally different approach which treats the model with a finite context window as an interactive agent,\"\n\nProposed Correction: \"To address these issues, we introduce an approach that treats the model with a finite context window as an interactive agent,\"\n\n\n\n***Section 2. Related Work***\n\n\nOriginal: \"Another direction is modified self-attention.\"\n\nProposed Correction: \"Another approach involves modifying self-attention.\"\n\n-------------\n\nOriginal: \"to enable models on longer sequences\"\n\nProposed Correction: \"to enable modelling on longer sequences\"\n\n-------------\n\n\nOriginal: \"Despite the recent advances, this approach comes with two natural limitations:\"\n\nProposed Correction: \"Despite recent advancements, this method presents two inherent limitations:\"\n\n-------------"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7818/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698966265546,
        "cdate": 1698966265546,
        "tmdate": 1699636956906,
        "mdate": 1699636956906,
        "license": "CC BY 4.0",
        "version": 2
    }
]