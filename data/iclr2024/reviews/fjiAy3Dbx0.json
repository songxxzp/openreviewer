[
    {
        "id": "LXK4F03X1t",
        "forum": "fjiAy3Dbx0",
        "replyto": "fjiAy3Dbx0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1965/Reviewer_umPG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1965/Reviewer_umPG"
        ],
        "content": {
            "summary": {
                "value": "The focus of the paper is on automating the process of design template generation.\n\nThe main contribution is a design template generation pipeline, consisting of two stages: background generation and layout generation. First, a background image is generated with an extended T2I diffusion model that imposes saliency constraints on the cross-attention activations to preserve space for subsequent layout elements. Then, a layout on the generated background is generated with an autoregressive Transformer, which is then refined together with the background in an alternating fashion for a harmonious composition.\n\nA large-scale banner dataset with rich annotations is constructed for training and testing the method, and an application of the method to multi-page design template generation is demonstrated."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Automatic generation of design templates is an important problem to study.\n\n2. The constructed dataset, if can be publicly released, will be of value to the graphic design synthesis community."
            },
            "weaknesses": {
                "value": "1. The scale of technical novelty is limited. First, the relationship between subjects in the generated image and their attention activations has already been explored in a recent work, Attend-and-Excite. In view of this work, the finding at the beginning of Sec. 4.2 (and in  Fig.3) is unsurprising and the high-level idea of modifying attention values to control the generated images is not new. Second, the proposed spatial control strategy in Sec. 4.2 (including salient attention constraint and attention reduction) is simple and straightforward, which does not bring much novel technical insight. Third, the layout generator is just a previously proposed technique, i.e., LayoutTransformer. More importantly, the proposed iterative inference strategy for background and layout refinement seems to be ad-hoc. It would be desirable to propose a more unified, perhaps learning-based, approach to capture the dependency between the background and layout, e.g., by modeling their joint distribution, which will be of more interest to the ICLR community.\n\n2. The evaluation is insufficient. For background generation, the diversity of generated images is not evaluated. For layout generation, more evaluation metrics such as FID and max IoU as in [Kikuchi et al., 2021] should be used but are missing. Furthermore, background and layout are now evaluated separately, but the overall design template, which is formed by composing the two, is not evaluated. This is unreasonable since the paper is claimed to be aimed for design template generation instead of background or layout generation. Thus, an evaluation on the quality of generated design templates, e.g., at least through some human studies, is needed to support the paper\u2019s main claim, which is missing in the current paper."
            },
            "questions": {
                "value": "How is the attention map A obtained in Eq. (2)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1965/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698662069651,
        "cdate": 1698662069651,
        "tmdate": 1699636127735,
        "mdate": 1699636127735,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oOgUjNlVI0",
        "forum": "fjiAy3Dbx0",
        "replyto": "fjiAy3Dbx0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1965/Reviewer_14nH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1965/Reviewer_14nH"
        ],
        "content": {
            "summary": {
                "value": "The paper generates single-image design templates, e.g., for slides and advertisements, based on a generator trained to do so, given an image prompt layout for which parts of the image should be empty.  The paper notes that the cross-attention values often correspond to saliency, providing a direct way to control saliency."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper solves a novel and useful task. The results seem promising. The cross-attention/saliency observation is interesting."
            },
            "weaknesses": {
                "value": "I'm not sure that this task is necessarily of great interest to the ICLR community; it may be better-suited for another venue.\n\nThe layout-generation component of the work does not seem too novel and is not compared to the state-of-the-art:\n\nPosterLayout: A New Benchmark and Approach for Content-Aware Visual-Textual Presentation Layout\nHsiao Yuan Hsu, Xiangteng He, Yuxin Peng, Hao Kong, Qing Zhang; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 6018-6026\n\n\nFor the image-generation component, much of the same effect can be achieved with existing diffusion models. For example, I tried prompts like `\u201ca background image with empty space for a shoe advertisement\u201d` or `\"a background image of a squirrel with empty space for advertising text\"`, I got reasonable results with empty spaces. This doesn't offer the same level of control as the proposed system, but seems much simpler, and one could run many generations to find reasonable layouts."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1965/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698798359282,
        "cdate": 1698798359282,
        "tmdate": 1699636127666,
        "mdate": 1699636127666,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "W86MkMLVfz",
        "forum": "fjiAy3Dbx0",
        "replyto": "fjiAy3Dbx0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1965/Reviewer_XhSd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1965/Reviewer_XhSd"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to devise a model that could automatically generate a background image with the layout of multiple elements on the image, which could be useful for designing templates for slides, ads, webpages, etc. The desired properties of such templates are: i) background image should leave enough space for overlaying elements (like texts); ii) texts and titles should fit proportionally in the blank space on the image, not occluding each other. To achieve the above goal, the proposed method first generates an initial background with with spatial control using cross-attention on saliency maps, and then a transformer based layout generation model that iteratively refines the background image and the positions and proportions of the elements on it. Qualitative and quantitative results were presented to show that the proposed method generates images that are more suitable as background and also generates layouts with more harmonious elements than baseline methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This work proposes clear and extensive automatic metrics to evaluate the success of layout generation (salient ration, alignment, overlap, occlusion).\n- After defining the goal as generating a background image with enough blank space for overlaying texts, the proposed method is effective in achieving the goal.\n- Ablation study for each proposed component is reported."
            },
            "weaknesses": {
                "value": "- If more discussions can be included on exactly what prompts were experimented for baselines like DALL-E2 and Stable Diffusion, it would really help me understand what existing work is missing that this work provides. Currently the paper reads like baselines and proposed method use the same prompts to generate background images. Is that a fair setting? Baseline models were not trained to generate background only images, so without giving them more explicit prompts specifying that the output should should be a background, it seems natural to me that the output images from baselines are not suitable for background. \n- I wonder why the number of elements in a layout (and their minimal and maximal allowed size) is not an input to the background image generation model? It seems to me that the space to leave blank and its position depends on the potential space the elements would take, so this can be a crucial context for the generation model. While an iterative refinement mechanism is used to adjust the background image and the layout, the iterative nature can be confining the model to only make improvements in a local range of the initial image."
            },
            "questions": {
                "value": "- What was the prompts accompanying each image in the newly collected Web-design dataset, and how were they obtained?\n- In Section 5.1 Implementation Details, it mentions that all training images are resized to 512x512. Does this mean that during inference time the model also generates images of size 512x512? It seems to me that advertisement images can come in a wide range of aspect ratios, would the resizing and squared output size limit the use case of this model?\n- From the qualitative examples, it seems like each layout can have different numbers of elements on it. How are the elements in each layout determined?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1965/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698907040956,
        "cdate": 1698907040956,
        "tmdate": 1699636127605,
        "mdate": 1699636127605,
        "license": "CC BY 4.0",
        "version": 2
    }
]