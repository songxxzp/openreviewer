[
    {
        "id": "m05EnOJIaU",
        "forum": "gqtbL7j2JW",
        "replyto": "gqtbL7j2JW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7280/Reviewer_htir"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7280/Reviewer_htir"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a novel problem for generative models in this paper, called generative model identification. The goal of this problem is to identify an appropriate text-to-image model for a user\u2019s target image. This problem is driven by the fact that model hubs often filter models based on textual descriptions and download counts, which are not sufficient for capturing user requirements. The authors provide a non-trivial solution to this problem based on vision-language models and RKME methods. The experiment shows that the proposed method achieves strong performance compared to the RKME baseline method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe problem proposed in this paper is very interesting and well-motivated by the fact that current model hubs usually filter models via textual descriptions and download counts. As generative AI becomes more and more mainstream, the proposed problem setting will become increasingly important for the generative community.\n2.\tThe solution of this paper is reasonable since modeling the text-to-image mapping is important for capturing the functionality of generative models and then matching the user requirements. The proposed method makes non-trivial contributions compared to the only one related to the RKME baseline method and gives promising performance.\n3.\tThis paper is well-written and easy to follow. The proposed example clearly demonstrates the importance of modeling text-to-image mapping, which is ignored by baseline methods. The method and experiment parts are clear and well-organized."
            },
            "weaknesses": {
                "value": "1.\tFigure 1 shows that users may use more effort in identifying their target models. However, this figure contains too much information and details. Therefore, a detailed textual explanation should be provided for readers to easily understand this illustration.\n2.\tThe authors constructed the model hub and tasks with 16 stable diffusion models. A brief description should be provided for these models.\n3.\tMinor problems: (1) The X ticks of the left two sub-figures of Figure 2 are not aligned with the bars; (2) \u201cshows\u201d in the second sentence of section A.2 should be \u201cshow\u201d."
            },
            "questions": {
                "value": "Please refer to the questions in the weakness and answer the following questions:"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7280/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698720448442,
        "cdate": 1698720448442,
        "tmdate": 1699636869433,
        "mdate": 1699636869433,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pIac1ymyQP",
        "forum": "gqtbL7j2JW",
        "replyto": "gqtbL7j2JW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7280/Reviewer_7QtA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7280/Reviewer_7QtA"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a comprehensive solution consisting of three pivotal modules: a weighted Reduced Kernel Mean Embedding\n(RKME) framework for capturing the generated image distribution and the relationship between images and prompts, a pre-trained vision-language model aimed at addressing dimensionality challenges, and an image interrogator designed to tackle cross-modality issues."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The application is interesting. There are many models online and how to identify the needed model efficiently is very important.\n2. The proposed method is simple and effective.\n3. The paper is overall well-written."
            },
            "weaknesses": {
                "value": "1. The paper is more like a technical report instead of an academic paper. \n2. The technical contributions are limited. It is quite trivial to calculate the distance between the uploaded image/prompt and the existing \nimages/prompts. The MMD distance is a very naive distance metric, and its robustness is questionable."
            },
            "questions": {
                "value": "1. Highlight the technical contributions of this paper.\n2. Discuss the limitation of the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7280/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698730194396,
        "cdate": 1698730194396,
        "tmdate": 1699636869300,
        "mdate": 1699636869300,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JfAvxyprq0",
        "forum": "gqtbL7j2JW",
        "replyto": "gqtbL7j2JW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7280/Reviewer_pBNB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7280/Reviewer_pBNB"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the GMI (Generative Model Identification) task, aiming to match user-provided images to the best generative model. The process has two stages: first, it precomputes the specification for each generative model (map to feature and compute ; second, it compares the query image's specification with these precomputed specifications using weighted RKME to gauge similarity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The problem setting addresses a practical need to identify the most suitable generative model amidst a myriad of options we have nowadays.\n- The application of RKME as a similarity metric is interesting. There's potential relevance to the \"Informative Features for Model Comparison\" work, even though the latter primarily focuses on comparing just two models based on the goodness of fit with image queries.\n- Separating precomputation and actual comparison is not new, but is a useful concept to speed up the process."
            },
            "weaknesses": {
                "value": "- The paper assumes that users will always provide an example image, which might not be universally applicable or intuitive.\n- The work heavily relies on existing methods: RKME, a pre-trained vision-language model (possibly CLIP?), and the image interrogator from a GitHub repository. There's a lack of novelty in the proposed method.\n- Perhaps it's just me, but I found the writing in the technical sections really confusing.  \n-  While the image interrogator is referenced from an existing work, it would still benefit from a brief description within this paper, given that not all readers may be familiar with it.\n- The lack of mention or comparison with the 'Content-Based Search for Deep Generative Models' paper. That work seems to have broader capabilities, being able to process not just image but also text, sketches, or a combination of them. In comparison, the proposed method feels like a less capable variant. Also for the experiment, that paper uses 100+ models whereas this paper uses only 16 models."
            },
            "questions": {
                "value": "- How was the default prompt 'P' determined?\n- The paper suggests that developer/user providing the exact prompt would yield better results. Can this claim be supported with any qualitative or quantitative evaluation? Also, are there any guidelines or recommendations for the type of prompt to be used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7280/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789147465,
        "cdate": 1698789147465,
        "tmdate": 1699636869172,
        "mdate": 1699636869172,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GFZS5Dc8Nm",
        "forum": "gqtbL7j2JW",
        "replyto": "gqtbL7j2JW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7280/Reviewer_BSis"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7280/Reviewer_BSis"
        ],
        "content": {
            "summary": {
                "value": "This paper presents generative model identification that can rank a given set of diffusion based generative models that could potentially produce a user provided image. The idea is to represent each generative model using a small number of images and their generative prompts, which are then used in a reduced kernel mean embedding (RKME) scoring function to rank the proximity of the embeddings of the given user provided image in a suitable RKHS. The paper also explores incorporating the prompt in the RKME scoring function using neura embeddings of the prompts and the images. Experiments are provided on a small scale dataset and show promise."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem setting appears novel and useful. Especially, when there are hundreds of models in a model zoo and one needs to find what model could be used to produce a given image. \n2. The method is straightforward, the exposition is very clear, and easy to read. \n3. Experiments show promising results against existing retrieval baselines."
            },
            "weaknesses": {
                "value": "1. As I understand, RKME scoring would work better if you have image distributions to match; that is, if you have more than one user provided query image. In that sense, it is not clear to me how the paper could claim that a single image is sufficient to identify the model? How do you substantiate this claim? One thought is: to derive a more general scoring function (in Eq. 2, for example) with multiple user query images and the authors did an ablation that shows that using a single image or when using multiple images, the retrieval performance is approximately similar. Further to the above comment, given there is only a single image query, what is the performance if you used a different statistic for the scoring? For example, using \\ell_1 norm or just the \\argmin on the kernel features? \n\n2. As the selection of the reduced image sets is important for efficiency of the method, the paper must provide details of how this is done for generative models. How do you ensure the size is large enough and also discriminative for each model so that overlap between reduced sets across models is minimal while the model representation is sufficient for identification? \n\n3. One of the key innovations in this paper is the inclusion of prompt embeddings in the model specification (Eq. 9). As is clear from Eq. 9, the weighting scheme soft-selects images in the reduced set using the query prompt and compares the kernel embeddings of the respective images with the query image. A natural question that would arise here is: how do you ensure the reduced set prompts are selected so as to cover user provided images or prompts? This part seems less detailed and not evaluated in the experiments. Specifically, how do you produce the (prompt, generated image) pairs for the reduced set? How do you ensure the prompts are different for diverse generated images? How do you account for the domain gaps (if any) between user specified prompts against the submission-time specified prompts? What is the sensitivity of the scoring function against differences in the prompts? It would be useful to  see qualitative results showing images and (varied) prompts."
            },
            "questions": {
                "value": "Other questions.\n1. How does the performance of the method vary when using varied reduced set sizes? It is also important to analyze the scalability of the method when the number of generative models goes higher. \n2. What are the prompts associated with the qualitative results in Figure 4? Can you also include images with user specified prompts? \n3. Why is the performance of RKME-CLIP so close to that of the proposed method? What is the size of the reduced set for the various diffusion models?\n4. What would be the performance of the model if: i) the query image was selected from one of the generative models, but captioned using I(.)? ii) the query image was selected from one of the models and with the respective prompts? iii) you use the prompt weight w to be an indicator function when using i). These experiments will show the upper-bound on performance of the method (when using kernel mean matching).\n5. What is \\gamma in Figure 7? \n6. In Figure 6, Examples 2,3, it is not clear to me why the proposed result is better than other ones. An explanation would help."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethical concerns."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7280/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698847538555,
        "cdate": 1698847538555,
        "tmdate": 1699636869063,
        "mdate": 1699636869063,
        "license": "CC BY 4.0",
        "version": 2
    }
]