[
    {
        "id": "yN1CZ6YnrW",
        "forum": "mvMI3N4AvD",
        "replyto": "mvMI3N4AvD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission66/Reviewer_WuFJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission66/Reviewer_WuFJ"
        ],
        "content": {
            "summary": {
                "value": "This work proposed a zero-shot Text-to-speech model with a prosody large language model (p-LLM). They utilize a multi-reference timbre encoder to extract a timbre from multiple references. Moreover, they introduce an autoregressive duration predictor for prosody modeling. The results show that they can transfer the prosody and timbre respectively."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "They organize the parallel TTS pipeline with autoregressive representation modeling. They only utilize a large language model in prosody modeling so they could enjoy the advantage of in-context learning of the LLM model and may prevent a problem from the autoregressive TTS model which could synthesize a speech with repeating and skipping. It would be better if the authors could explain more cases of robust speech synthesis without repeating and skipping..\n\nIn my opinion, autoregressive duration predictor could significantly improve the prosody transfer performance because duration influences prosody. It would be grate if you could add additional ablation study for autoregressive and non-autoregressive duration predictor.\n\nMeanwhile, an adversarial duration predictor was proposed in VITS2. This could improve the performance of duration modeling in this work."
            },
            "weaknesses": {
                "value": "1.\tI just wonder why the authors do not state ProsoSpeech which has the same structure as P-LLM without word-level prosody modeling. It would be better if the authors add the ablation study for phoneme and word-level prosody modeling. \n\n2.\tThe authors should have conducted more experiments on prosody modeling. There are recently prosody modeling works, Prosody-TTS and CLAPSpeech. Although recently large language model has been investigated, I hope that the author could add an additional experiment with them. All of the papers including this work might be from the same research group but they did not state anything. I hope the authors address this issue.\n\n3.\tMulti-reference Style Transfer methods were already utilized in many speech papers such as Attentron [S. Choi, 2020]. \n\n[S. Choi, 2020] S Choi, \u201cAttentron: Few-Shot Text-to-Speech Utilizing Attention-Based Variable-Length Embedding,\u201d Interspeech 2020"
            },
            "questions": {
                "value": "1.\tI have a question about the Baseline model. This model might be a FastSpeech 2 with GAN training and style encoder of meta-stylespeech. Why do you fine-tune this model? I just wonder about the performance of zero-shot TTS of baseline. \n\n2.\tAre there any failure cases for the auto-regressive duration predictor? In my opinion, there are many components of this work. I think AR duration predictor is one of the important changes and it could be utilized for other TTS model easily so I hope you to analyze this predictor with additional ablation studies. Do not need to train the model additionally. I just suggest to infer the baseline TTS model with the predicted duration of your model. \n\n3.\tDoes the model synthesize the speech robustly with the noisy reference prompt? I would be better if you could add the results on test-other set. \n\nAlthough recently methods introduce codec-based speech synthesis, it is also important to utilize conventional acoustic representation such as Mel-spectrogram so I like the concepts of this paper adopting the language model for prosody modeling. However, I hope the authors include additional experiments\n\n1. Results on conditioning noisy reference prompts\n\n2. Comparisons with other prosody modeling methods such as Prosody-TTS and CLAPSpeech\n\n3. Comparison with word-level Prosody modeling (ProsoSpeech)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission66/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission66/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission66/Reviewer_WuFJ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission66/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698502034433,
        "cdate": 1698502034433,
        "tmdate": 1700386354378,
        "mdate": 1700386354378,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SUFraa9fmP",
        "forum": "mvMI3N4AvD",
        "replyto": "mvMI3N4AvD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission66/Reviewer_azu9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission66/Reviewer_azu9"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel zero-shot TTS framework that can effectively disentangle and control prosody and timbre with extremely long prompts. Specifically, a multi-reference timbre encoder is proposed to extract the timbre information, and a P-LLM is proposed to generate prosody with multiple reference context. A prosody transferring technique is proposed to control the generated prosody with context. Extensive experiments are done to show the superior performance of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The proposed TTS framework can separately control both timbre and prosody. Especially the zero-shot prosody control, which is one of the most challenging topic in TTS area.\n2. The proposed method can scale the in-context learning to very long prompts, like 300s, and the performance does not saturate when prompt is longer than 20s, which is promising.\n3. Although verifying the superiority of a zero-shot TTS system is hard given that most of them are closed-sourced, the authors reimplemented the baseline methods and do the comparison with controlled variables like parameter number, and training datasets."
            },
            "weaknesses": {
                "value": "1. Some of the zero-shot TTS categories are missing from the related works. One of the most related is the attention-base adaptation method. Early in year 2020, Attentron [1] is proposed, which can adapt to unseen speakers with multiple reference utterances, just like the MRTE in MegaTTS. Such strategy is also used in zero-shot voice conversion domain [2]. Later, methods like [3] introduces cross-attention based model and compress the reference into a fixed-length sequence before decoder attention, which is more close to the one in NaturalSpeech2.\n2. Some ablation studies are missing. Since the primary design of the training strategy is using multiple reference instead of only one utterance, it is important to show the performance difference between using long context vs short context during training to verify the necessity of this training strategy.\n\n[1] Choi, Seungwoo, et al. \"Attentron: Few-shot text-to-speech utilizing attention-based variable-length embedding.\" Interspeech 2020.\n\n[2] Lin, Yist Y., et al. \"Fragmentvc: Any-to-any voice conversion by end-to-end extracting and fusing fine-grained voice fragments with attention.\" ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021.\n\n[3] Yin, Dacheng, et al. \"RetrieverTTS: Modeling decomposed factors for text-based speech insertion.\" Interspeech 2022"
            },
            "questions": {
                "value": "1. Is there a justification of the assumption in section 3.1 \"the mutual information between $y^t$ and $\\tilde{y}$ only contains timbre information\"? This assumption is not very obvious, since in the audiobook dataset, some performing skills may change the timbre largely in different sentences. Additionally, the average prosody style information can also be shared by different utterances.\n2. Is it possible to show the baseline methods with 300s context?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission66/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission66/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission66/Reviewer_azu9"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission66/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698688556624,
        "cdate": 1698688556624,
        "tmdate": 1699635931198,
        "mdate": 1699635931198,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pyQ88raur8",
        "forum": "mvMI3N4AvD",
        "replyto": "mvMI3N4AvD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission66/Reviewer_6vRj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission66/Reviewer_6vRj"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Mega-TTS, a zero-shot TTS framework designed to enhance multi-sentence prompts by decomposing them into timbre and prosody information. Mega-TTS utilizes an acoustic auto-encoder to independently encode prosody, content, and timbre information. The model integrates a multi-reference timbre encoder and a prosody latent language model (P-LLM) for efficient extraction of information from multi-sentence prompts. This design facilitates the generation of transferable and controllable prosody by leveraging probabilities derived from P-LLM outputs. The paper demonstrates that the synergy between the multi-reference timbre encoder and the prosody interpolation enabled by P-LLM results in fine-grained and controllable prosody transfer. The proposed outperforms Vall-e and the fine-tuning baseline when speech prompts ranging from 10 seconds to 5 minutes are used."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed method adeptly combines the advantages of non-autoregressive (non-AR) modeling, such as robustness and controllability, with the powerful expressiveness of auto-regressive (AR) modeling, achieving this by decomposing speech into prosody and timbre using an information bottleneck.\n- The proposed model exhibits the capability to independently prompt prosody and timbre within a zero-shot setting.\n- The proposed method empirically shows improved zero-shot performance compared to fine-tuning approaches and outperforms existing state-of-the-art models, and the prosody transfer by using a prosody interpolation technique."
            },
            "weaknesses": {
                "value": "- While we understand the issue of having a limited number of available baselines, the absence of comparisons with NaturalSpeech2 and VoiceBox makes it challenging to ascertain the proposed model's superiority. At the very least, it appears necessary to replicate the experimental conditions described in the baseline papers and evaluate the proposed model, comparing its performance using the metrics reported in each paper, especially for prompts that are 3 seconds long. It raises the question: How does the proposed method perform with 3-second prompts?\n- In the case of datasets like LibriVox, which comprises audiobooks, it is commonly observed that there is not a significant variation in speaking style across different utterances by the same speaker. In this context, it becomes difficult to uphold assumption (1). As a result, the timbre encoder may capture prosody information as well, and the experiment does not conclusively demonstrate the complete separation of these two elements.\n- The description of the information bottleneck, a crucial component of the proposed method, is lacking in detail. Specifically, there is an absence of clear guidelines and processes for setting variables such as $r$, $d$, and the hidden dimensions to ensure an appropriate information bottleneck.\n- From the perspective of a practitioner, the prerequisite of using Montreal Forced Aligner (MFA) to extract alignments beforehand could be seen as a cumbersome step."
            },
            "questions": {
                "value": "- What would be the performance outcome if we generate the prosody latent for the target speaker using only the prosody information from another speaker, instead of using interpolation?\n- It is unclear in which dimension the concatenation is performed in the Multi-Reference Timbre Encoder. While Figure 1 seems to suggest that concatenation occurs along the hidden dimension axis, the description in Section 3.2 of the timbre-to-content attention module implies that it happens along the time axis.\n- In the comparison experiments with VALL-E, why was a different model chosen to measure speaker similarity (SIM) instead of using WavLM the one utilized [1]?\n\n[1] Wang, Chengyi, et al. \"Neural codec language models are zero-shot text to speech synthesizers.\" arXiv preprint arXiv:2301.02111 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission66/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission66/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission66/Reviewer_6vRj"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission66/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698893140367,
        "cdate": 1698893140367,
        "tmdate": 1700632960400,
        "mdate": 1700632960400,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SuIVOUvfT4",
        "forum": "mvMI3N4AvD",
        "replyto": "mvMI3N4AvD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission66/Reviewer_pM8F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission66/Reviewer_pM8F"
        ],
        "content": {
            "summary": {
                "value": "The paper presents Mega-TTS, a novel framework for zero-shot text-to-speech (TTS) systems. The primary aim of Mega-TTS is to synthesize voices with unseen speech prompts, thereby reducing the data and computational requirements associated with voice cloning. The authors address two main challenges faced by existing zero-shot TTS systems: the lack of multi-sentence prompting strategies and the absence of specialized prompting mechanisms for prosodic information. By decomposing speech into content, timbre, and prosody, they propose a system that effectively handles long prompts and offers flexible control over prosodic styles. Experimental results suggest that Mega-TTS outperforms other state-of-the-art models in terms of speaker similarity and speech naturalness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea seems technically solid and well-motivated, and the demo audio examples clearly show the difference.\n\n- The authors introduce a novel approach to decompose speech into content, timbre, and prosody. This method allows for more effective handling of long prompts and provides greater control over prosodic styles. This is an innovative contribution that sets the groundwork for future research in this area.\n\n- Superior Performance: The paper presents experimental results showing that Mega-TTS outperforms other state-of-the-art models regarding speaker similarity and speech naturalness. This is a significant strength as it demonstrates the practical effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "Unclear Performance Across Languages: The experiments presented in the paper only use English datasets. Therefore, it's unclear how well the system performs with different languages or dialects. This limits the generalizability of the findings and may hinder the application of the system in diverse linguistic contexts. While the authors acknowledge some limitations of their approach, a more extensive exploration and testing of these constraints could have provided a more comprehensive understanding of the model. This additional analysis could guide future research addressing these limitations and further refining the Mega-TTS model.\n\nThe authors have not provided any information about the inference times of Mega-TTS compared to other models. This omission makes it difficult to evaluate the model's performance in real-world scenarios where speed may be as important as accuracy."
            },
            "questions": {
                "value": "- How does Mega-TTS handle non-English languages or different dialects? Exploring this could help assess the generalizability of the model across various linguistic contexts.\n- What are the specific computational requirements of Mega-TTS compared to other models? This information is crucial for understanding the trade-offs involved in using Mega-TTS.\n- How does the performance of Mega-TTS scale with the size of the training data? Understanding this can provide insights into how well the model might perform in scenarios with varying amounts of available data.\n- How does Mega-TTS handle non-standard speech patterns such as shouting, laughing, or other forms of emotional expression? This question could illuminate the model's ability to accurately capture and reproduce a wider range of human speech nuances."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission66/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699133382126,
        "cdate": 1699133382126,
        "tmdate": 1699635930897,
        "mdate": 1699635930897,
        "license": "CC BY 4.0",
        "version": 2
    }
]