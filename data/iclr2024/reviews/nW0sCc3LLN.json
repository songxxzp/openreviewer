[
    {
        "id": "XOCAVkRk1O",
        "forum": "nW0sCc3LLN",
        "replyto": "nW0sCc3LLN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4680/Reviewer_zxaM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4680/Reviewer_zxaM"
        ],
        "content": {
            "summary": {
                "value": "In this work authors propose a novel defence against model inversion attacks based on limiting the information flow within the model by restricting the layers which are trained on the sensitive data by using transfer learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The work is structured very well, has numerous figures and has a source code attached. The evaluation is quite extensive showing the advantages of the proposed method."
            },
            "weaknesses": {
                "value": "I do, however, have a number of concerns.\n\nFirstly I do not see anything scientifically new proposed here. It was shown before that A) certain layers can \u2018memorise\u2019 more useful information to aid a MI attacker [1] and B) that reducing the number of layers in the model can reduce the amount of useful information leant and hence limit attacker\u2019s capabilities [2] (which does not universally hold either, but more on that later). It is also important to note that this work claims all of its results and conclusions in a generic MI setting, which is not fully true.\n\nTake gradient-based MI, for instance: Having smaller models (or parts of models) that are exposed to sensitive data, is arguably an easier setting for such an attacker to exploit. Similarly, other works on generative MI have previously shown the opposite results to what you share here [3] - when such mitigation (in that case split learning) is present, it is actually beneficial for the attacker to use a smaller section of the model (due to the ease of reconstruction). I do appreciate that these settings are different to the one described in this work, but the method proposed here does not apply to any generic MI setting, contrary to what authors are trying to suggest.\n\nMoreover, the entire concept of information useful for inversion attacks (as well as how to reduce it) was previously studied in [1], showing that based on the nature of the attack, different layers can contain various amounts of information useful for each specific adversarial method. Thus, I cannot see any novelty in the FI analysis either, as the conclusions are hardly new.\n\nBesides, this method is based on the fact that some public dataset exists, allowing to \u2018outsource\u2019 a lot of the learning to data which is not deemed to be sensitive. This is A) a relatively strong assumption for a generic MI defence and B) has been previously proposed as a potential solution against inference attacks already in [4] and the baselines discussed in that work. Point A) has already been somewhat discussed in work on differentially private learning, showing that yes, it is a valid idea, but it a strong assumption to make (i.e. more public data means there is less private data required and, hence, the privacy budgets can be smaller)."
            },
            "questions": {
                "value": "How can this method be scaled to other types of inversion attacks? Is there any correlation between information that can be used by MI attacker and, for instance, membership or attribute inference attacker?\n\nOverall, this work really lacks scientific novelty and is limited to only a subset of MI attacks, making it severely less applicable to realistic ML settings. In my view, the answer to the question \u2018can transfer learning help against MI?\u2019 has previously been answered in other works. \n\n\n[1] - Mo, Fan, et al. \"Quantifying information leakage from gradients.\" CoRR, abs/2105.13929 (2021).\n[2] - Wang, Yijue, et al. \"Against membership inference attack: Pruning is all you need.\" arXiv preprint arXiv:2008.13578 (2020).\n[3] - Usynin, Dmitrii, et al. \"Zen and the art of model adaptation: Low-utility-cost attack mitigations in collaborative machine learning.\" Proc. Priv. Enhancing Technol. 2022.1 (2022): 274-290.\n[4] - Chourasia, Rishav, et al. \"Knowledge Cross-Distillation for Membership Privacy.\" arXiv preprint arXiv:2111.01363 (2021)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4680/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698061412129,
        "cdate": 1698061412129,
        "tmdate": 1699636449282,
        "mdate": 1699636449282,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mt2znulpRE",
        "forum": "nW0sCc3LLN",
        "replyto": "nW0sCc3LLN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4680/Reviewer_TtAY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4680/Reviewer_TtAY"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new defense strategy against model inversion (MI) attacks by limiting the number of parameters fine-tuned on private data using transfer learning. In specifc, the method involves pre-training on public data, then fine-tuning only the last few layers on private data, freezing the first layers to prevent private data encoding. The paper analyzes layer importance for MI task using Fisher information, suggesting first few layers are important for MI, while last several layers are import for classification task. Experiments show the method achieves state-of-the-art MI defense across various setups and architectures, while being simple to implement."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "originality: This work proposes an innovative approach to defending against MI attacks, which opts for transfer learning over traditional dependency based regularization methods. The idea is not only novel but intuitively appealing, offering a fresh perspective on the problem. What further strengthens the argument is the incorporation of Fisher information, which provides a robust justification for the proposed method. This logical integration of Fisher information bolsters the paper's overall coherence and the credibility of the proposed approach\nquality: This work conducts extensive array of necessary experiments, consistently attains state-of-the-art performance across various model inversion setups and architectural configurations. The comprehensive and rigorous experimental evaluation conducted in this work underscores its significant contributions to the field.\nclarity: The paper is well-written and easy to follow logically.\nsignificance: A new perspective on MI defense."
            },
            "weaknesses": {
                "value": "Potential misleading claim: \n\"In other words, MID and BiDO reduce MI attack accuracy by suppressing likelihood P(y|x).\" it would be beneficial to clarify whether there is experimental evidence supporting the claim that MID and BiDO reduce MI attack accuracy by suppressing the likelihood P(y|x). If such experiments have been conducted, providing references or details about them would strengthen the paper's credibility. If not, it may be advisable to rephrase this statement as a hypothesis or a potential outcome to avoid overclaiming the results.\n\nUnaligned evaluation metrics: \nThe evaluation metrics for 'AttAcc' across different MI attacks are not consistent. For instance, in experiments involving GMI, KEDMI, and VMI, the reported 'AttAcc' values are accompanied by standard deviations, which typically indicate variations in 'AttAcc' across different classes, rather than the results of multiple experiment runs. Conversely, in the case of PPA and MIRROR, only 'AttAcc' is provided, which lacks the same level of rigor. It would be helpful to standardize the reporting of 'AttAcc' to maintain consistency in the evaluation process. Additionally, providing clarity on the interpretation of standard deviations in GMI, KEDMI, and VMI experiments, such as whether they represent variations across classes or multiple experiment runs, would enhance the paper's transparency."
            },
            "questions": {
                "value": "1. For experiments related to VMI, it raises a valid concern that there is no D_pretrain. This absence of a pretraining backbone questions the relevance of these experiments in demonstrating the effectiveness of your proposed method. It would be beneficial to provide a clear rationale for including VMI experiments in the evaluation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4680/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698378754837,
        "cdate": 1698378754837,
        "tmdate": 1699636449200,
        "mdate": 1699636449200,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5S7E0qzROB",
        "forum": "nW0sCc3LLN",
        "replyto": "nW0sCc3LLN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4680/Reviewer_8Fh6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4680/Reviewer_8Fh6"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a defense mechanism against Model Inversion (MI) attacks, which threaten privacy by reconstructing private training. Instead of traditional regularization techniques, the paper employs transfer learning to restrict the encoding of sensitive information to specific layers, hindering MI attacks. An analysis using Fisher Information reveals the importance of the initial model layers for MI attacks, supporting the proposed defense approach. Empirical validation demonstrates that reducing the number of fine-tuned parameters in private datasets significantly reduces MI attack accuracy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Interesting Defense Approach: The paper introduces a new defense strategy using transfer learning.\n- Fisher Information Analysis: The paper conducts a unique analysis of layer importance using Fisher Information, providing valuable insights.\n- Practicality: The proposed defense is straightforward to implement and can be applied to a wide range of model architectures."
            },
            "weaknesses": {
                "value": "- Limited Attack Coverage: The defense's focus on attacks leveraging GANs may not address all model inversion attack types.\n- Unclear Threat Model: The paper lacks clarity in specifying the adversary's knowledge of critical elements, which affects the practicality of the defense.\n- Performance Not Satisfying: The defense's performance is questionable, as it doesn't significantly outperform existing methods."
            },
            "questions": {
                "value": "Three significant concerns merit attention in the evaluation of the paper.\n\nFirstly, while the authors assert the applicability of their defense against general model inversion attacks, it becomes evident from the background section that their primary focus pertains to attacks leveraging extra information from GANs. This orientation does not account for the broader scope of general model inversion, as several attacks rely solely on the model's parameters. It is advisable for the authors to undertake discussions and experimentation that substantiate the efficacy of their defense in mitigating vulnerabilities across a wider spectrum of attack types.\n\nThe foremost concern that arises pertains to the lack of a clear threat model. It remains essential to delineate whether the adversary possesses knowledge of critical elements such as the pretrained model, the pretrained dataset, and the fixed layers. Without such clarification, it becomes difficult to assess the practicality of the proposed defense, particularly, if the adversary has knowledge of the pretrained model and final model, they can perform the model inversion attack as indicated in Salem et al.\u2019s work [1]. \n\nAttack performance is not satisfying. In particular, the results presented in Table 5 demonstrate that, when compared with BiDO, the proposed defense does not exhibit a significant advantage. A more meaningful approach to comparison might involve holding the attack accuracy constant while comparing the original accuracy or vice versa. Notably, the proposed defense leads to a 3% increase in accuracy but also results in a 5% increase in attack accuracy, raising questions about its superiority over previous defense methods. This consideration becomes particularly salient given the heightened complexity of the threat model associated with the proposed defense in comparison to earlier approaches.\n\n[1] Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4680/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4680/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4680/Reviewer_8Fh6"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4680/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698508622131,
        "cdate": 1698508622131,
        "tmdate": 1699636449097,
        "mdate": 1699636449097,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VuKbW1nfBt",
        "forum": "nW0sCc3LLN",
        "replyto": "nW0sCc3LLN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4680/Reviewer_LRKJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4680/Reviewer_LRKJ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to use transfer learning as a possible defense against model inversion attacks. The proposed approach makes use of pre-trained models and only fine-tunes part of the parameters. Using Fisher Information, the importance of the first few layers for successful model inversion attacks is shown, while for the classification task itself the first few layers are not very important. The experimental results show that fine-tuning fewer layers seems to reduce the attack success of model inversion attacks, while at the same time having only moderate impact on the accuracy of the model. The proposed approach is compared to other existing model inversion defense methods (BIDO and MID) and it is shown that the proposed approach achieves lower attack success while at the same time having less reduction in accuracy. The main finding of the paper is that fine-tuning fewer parameters on a private dataset, the model memorizes/leaks less private information about the training data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-structured and easy to follow\n- The topic of the paper is an interesting and important area of research\n- Even though the proposed method is merely an application of an existing technique, using transfer learning for defending against model inversion attacks seems to be novel"
            },
            "weaknesses": {
                "value": "- the different number of parameters are not visible in Fig. 1-IV\n- Table 3: does only show the proposed defense against no defense and the proposed approach is not compared to existing defense methods\n- Tables in the appendix are referenced in Section 4.5. This is not a good practice to reference tables in the appendix as if they were in the main part of the paper\n- from the paper, the whole attack scenario is not quite clear. It is not specified how many classes are attacked and how these classes are chosen.\n- I suspect to measure the Attack Accuracy, a validation model was trained. However, it is not stated how this validation model was trained or what architecture this validation model has.\n- even though the proposed approach is compared to other defenses using the GMI and KEDMI attack, a comparison to other defenses on other attacks like PPA or MIRROR is missing.\n\nMisc:\n- missing \"s\" in introduction \"MI attack is a type of privacy threat that aim[s]\"\n- Page 6: missing plural \"The set of MI reconstructed images [...] for different identit[ies] is used as X\"\n- Page 7: missing plural \"[...] when we combine [the] two approach[es]\"\n- Page 8: missing \"s\" \"Therefore, our approach offer[s] [...]\"\n- Page 9: missing word \"[...], regardless [of] multiple factors [...]\""
            },
            "questions": {
                "value": "- Q1: How often are the experiments repeated?\n- Q2: What are the dots in Fig. 1-IV?\n- Q3: Could you provide a comparison of the approach to MID and BIDO on PPA and MIRROR?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed since the paper only proposes a defense against model inversion attacks."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4680/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765617810,
        "cdate": 1698765617810,
        "tmdate": 1699636448974,
        "mdate": 1699636448974,
        "license": "CC BY 4.0",
        "version": 2
    }
]