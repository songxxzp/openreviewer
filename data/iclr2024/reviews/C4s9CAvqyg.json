[
    {
        "id": "fOO2STIbrW",
        "forum": "C4s9CAvqyg",
        "replyto": "C4s9CAvqyg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7577/Reviewer_HA8U"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7577/Reviewer_HA8U"
        ],
        "content": {
            "summary": {
                "value": "This paper develops a graph transformer model tailored for heterophilic graphs. Specifically, authors first introduce Tree2Token to produce a sequence that captures different-hop neighbor features per node. By concatenating the features with relative positional encodings in each sequence, the proposed model adopts the vanilla Transformer architecture to generate the final node predictions for downstream tasks. Experimental results indicate that the proposed model outperforms several baselines on heterophilic datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Figure 2 is clear to demonstrate the proposed approach.\n- Authors have conducted a comparison of various GNN baselines."
            },
            "weaknesses": {
                "value": "- Limited novelty. The notion of TREE2TOKEN has been introduced in NAGphormer[1]. Besides, applying L2 regularization on weight matrices is also a common way to avoid overfitting and improve model generalization.\n- Improper datasets. The heterophilic datasets used in this paper have serious issues (train-test data leakage), as shown in [2]. Thus, the accuracy improvement on those datasets is not compelling.\n- Missing relevant baselines. There are multiple graph transformers [3-5] that have achieved promising results on heterophilic graphs, which are not compared in this work.\n- Paper writing can be further improved. There are redundant and repeated sentences in the Introduction section. Besides, some less common terms are not clarified or defined (e.g., sequential information jumps). Additionally, there are also multiple typos throughout the paper, especially in Section 3.3.1.\n- Following the previous concern, the hyperparameter sensitivity analysis is confusing. Please refer to my questions below for details.\n\n[1]: Chen et al., \"NAGphormer: A Tokenized Graph Transformer for Node Classification in Large Graphs\", ICLR'23. \\\n[2]: Platonov et al., \"A critical look at the evaluation of GNNs under heterophily: are we really making progress?\", ICLR'23.  \\\n[3]: Zhang et al., \"Hierarchical Graph Transformer with Adaptive Node Sampling\", NeurIPS'22. \\\n[4]: Wu et al., \"DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion\", ICLR'23. \\\n[5]: Kong et al., \"GOAT: A Global Transformer on Large-scale Graphs\", ICML'23."
            },
            "questions": {
                "value": "- For investigating $c$ in Section 3.3.1, authors have set the number of hops to $1$ and $k=128$. What is $k$ here? Is it a typo?\n- What is the hyperparameter $h$ in Section 3.3.1? Where do authors define it?\n- Figure 6 is confusing to me. What does the x axis mean? What does the color bar value represent?\n- Since cosine is not a monotonic function, why do authors claim that the influence of $k$ will decrease when increasing $k$ in Equation (8)?\n- As $d$ is a hyperparameter, why can't we just replace $ln(10000/d)$ with $d$ in Equation (8)?\n- What's the time and space complexity of the proposed model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7577/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7577/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7577/Reviewer_HA8U"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698603874857,
        "cdate": 1698603874857,
        "tmdate": 1699636917546,
        "mdate": 1699636917546,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1VZJ06zE7b",
        "forum": "C4s9CAvqyg",
        "replyto": "C4s9CAvqyg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7577/Reviewer_CiBH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7577/Reviewer_CiBH"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes MPformer to learn heterophilous relationships based on the information aggregation module and the position encoding module called Tree2Token and HeterPos respectively. Experiments demonstrate that MPformer outperforms the baselines on various datasets."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper is easy to follow."
            },
            "weaknesses": {
                "value": "1. [1] points out that there exists train-test data leakage in the squirrel and chameleon datasets used in experiments.\n2. The authors may want to report a statistically significant difference against the second-best result, as Table 2 shows that the results are unstable (the standard deviation is larger than 1% accuracy).\n3. Many notations are confusing.\n\t1. What is the definition of ${X^{(i)}}^{\\top}$ in Theorem 1?\n\t2. What is the difference between $x^{(i)}$ in Theorem 1 and $x_v^k$ in Equations (6) (7) (8)?\n\t3. What is $f$ in Theorem 1? What is the relation between $f$ and the activation function $\\sigma$?\n4. The novelty of the proposed techniques is incremental.\n\t1. (Tree2Token) The second line of Equation (3) was proposed in [2, 3, 4]. Please explain the advantage of the first line of Equation (3). I suggest comparing the generalized gaps with different $A^{(k)}_{norm}$.\n\t2. (HeterPos) The proposed position encoding is similar to [5, 6].\n5. MPformer is difficult to apply to heterophilous graphs with edge features (e.g. knowledge graphs, protein\u2013protein interaction networks, and molecule graphs), which are common in practice.\n6. Please explain why existing graph transformers overlook the possible heterogeneous relationships among interconnected nodes.  In my opinion, the attention matrix learned by existing graph transformers can encode the heterogeneous relationships.\n7. The authors only provide the serializing case without the corresponding heterogenous graph in Figure 1.\n\n\n\n[1] A critical look at the evaluation of GNNs under heterophily: Are we really making progress? ICLR 2023.\n\n[2] Simplifying Graph Convolutional Networks. ICML 2019.\n\n[3] Graph Attention Multi-Layer Perceptron. KDD 2022.\n\n[4] NAGphormer: A Tokenized Graph Transformer for Node Classification in Large Graphs. ICLR 2023.\n\n[5] Self-attention with relative position representations. ACL 2018.\n\n[6] Transformer-xl: Attentive language models beyond a fxed-length context. ACL 2019."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698640861104,
        "cdate": 1698640861104,
        "tmdate": 1699636917420,
        "mdate": 1699636917420,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uP2AOLVAds",
        "forum": "C4s9CAvqyg",
        "replyto": "C4s9CAvqyg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7577/Reviewer_BMT4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7577/Reviewer_BMT4"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a Graph transformer model MPFORMER to deal with the heterophily problem in graph transformer. MPFORMER comprises the information aggregation module called Tree2Token and the position encoding module HeterPos. Experimental results prove the effectiveness of MPFORMER."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1.\tThe paper is well-written and easy to follow.\n2.\tMPformer performs well on the given datasets.\n3.\tAccurate proof of how to improve generalizability."
            },
            "weaknesses": {
                "value": "1. The novelty of the proposed idea is limited. Tree2Token actually selects a k-hop subgraph for each node, which is already a well-studied problem in the literature [1]. HeterPos makes incremental contributions to the existing positional encoding method.\n\n2. The motivation is somehow confusing. This paper aims to solve the problem that Graph Transformer cannot do well on heterophily graphs. However, this paper does not demonstrate the relationship between the two components of MPFORMER and the heterophily problem. Tree2Token is to solve the overlapping problem, while Hetepos is a method of marking neighbors with different hop numbers. The paper does not provide a proof that Hetepos can perform well on heterophily graphs.\n\n3. The Tree2Token method proposed in Section 2.1 is heuristic and straightforward. The training procedure in of MPFORMER seems to be computationally expensive but there is no discussion on the running time and training cost.\n\n4. MPFORMER has many hyperparameters, and hyperparameters need to be carefully selected for each dataset. The paper does not provide the optimal hyperparameters required for each datasets. \n\n5. In the ablation experiment, the effectiveness of Tree2Token was not analyzed.\n\n6. The Introduction Section is not well-organized. There are many paragraphs with a lot of text.\n\n[1] Equivariant Subgraph Aggregation Networks ICLR2022"
            },
            "questions": {
                "value": "1. Since graph transformer take the nodes of the entire graph as input. why graph transformer cannot work well on heterophily graphs. And why MPFORMER can work well on heterophily graphs.\n2. The title of this paper is \u201cMPformer: Advancing Graph Modeling Through Heterophily Relationship-Based Position Encoding\u201d. However, how the proposed position encoding method leverage the heterophily is not clear. More details should be explained.\n3. The efficiency and scalability of MPFORMER need to be analyzed.\n4. How are the hyperparameters of MPFORMER chosen?\n5. The necessity of Tree2Token component needs to be discussed.\n6. In Equation 2, should $\\mathbb {I} (A)$ be $\\mathbb {B}(A)$ ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698666951820,
        "cdate": 1698666951820,
        "tmdate": 1699636917296,
        "mdate": 1699636917296,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9k9u2NXsYF",
        "forum": "C4s9CAvqyg",
        "replyto": "C4s9CAvqyg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7577/Reviewer_o9xM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7577/Reviewer_o9xM"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces MPformer, a novel graph transformer model designed to enhance the modeling of graph-structured data, specifically focusing on addressing the limitations in handling heterophilous relationships in existing models. The authors claim that traditional graph neural networks (GNNs) and previous graph transformer models struggle to incorporate such heterophilous relationships adequately, thus limiting their application in real-world datasets where these relationships are prevalent.\n\nTo overcome these limitations, the authors propose two key components within MPformer:\n\n1. Tree2Token Module: This component transforms the information of a node and its neighbors into token sequences. By treating each node and its adjacent nodes as tokens, and then serializing these sequences, Tree2Token effectively captures the neighborhood information at various hop distances. This method allows the transformer model to recognize and utilize the information from both a node and its nearby nodes, improving the model's understanding of local structures.\n\n2. HeterPos Position Encoding: A novel position encoding technique, HeterPos, is introduced to define the relative positional relationships between nodes based on the shortest path distance. Unlike conventional methods, HeterPos emphasizes the differences in features between neighboring nodes and the central node (ego-node). This focus on heterophilous relationships aids in more accurately incorporating these relationships into the Transformer model.\n\nThe paper asserts that by integrating these two components, MPformer effectively captures both the graph topological information and the heterophilous relationships, thereby advancing the capabilities of graph transformer models. The approach is distinctive in how it generates new tokens from nodes and their neighbors, allowing for a more nuanced aggregation of neighborhood information. The innovative position encoding technique further strengthens the model by integrating shortest path distances and feature distinctions, laying a foundation for future models in handling heterogeneous graphs.\n\nTo substantiate their claims, the authors conduct theoretical analyses and practical experiments. These experiments, performed on various datasets, demonstrate that MPformer outperforms existing graph transformer models and traditional GNN models in modeling heterophilous graphs. This improvement in performance underscores the model's potential in dealing with a broader range of real-world datasets, particularly those characterized by heterophilous relationships."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Originality**:\n1. Innovative Integration of Heterophilous Relationships: The paper introduces a novel approach to integrate heterophilous relationships into the Transformer architecture with the development of MPformer. This model distinctively treats nodes and their neighbors as separate token vectors, which is a creative shift from the typical handling of graph nodes in transformer models.\n\n2. Unique Position Encoding Technique (HeterPos): The introduction of HeterPos, which uses the shortest path distance along with feature distinctions between nodes and neighbors, is an original and significant advancement. This method shows creativity in position encoding, moving beyond traditional approaches and better capturing the complexities of graph-structured data.\n\n**Quality**:\n1. Theoretical and Practical Validation: The paper demonstrates a robust methodology, corroborated by both theoretical analysis and practical experiments. This comprehensive approach ensures that the claims and performance metrics are well-supported and reliable.\n\n2. Effective Combination of Tree2Token and HeterPos Modules: The integration of these modules into the Transformer architecture for handling heterophilous data demonstrates a high level of thought and quality in model design. The model's ability to serialize token sequences from node and neighbor data for better information aggregation is a quality advancement in this field.\n\n**Clarity**:\n1. Well-Structured and Coherent Explanation: The paper articulately explains complex concepts like the Tree2Token aggregation module and HeterPos encoding. The progression from problem identification to solution presentation is logical and easy to follow, which aids in the comprehension of the paper's contributions.\n\n2. Illustrative Examples and Demonstrations: The use of illustrative examples (e.g., Fig.1) to explain the application and benefits of MPformer in classifying nodes within heterogeneous graphs significantly enhances the clarity of the proposed model's functionality.\n\n**Significance**:\n- Addressing Heterophilous Data in Graph Transformers: By focusing on the under-explored area of heterophilous relationships in graph transformer models, this paper tackles a significant and practical challenge in the field. The improvements it introduces have broad implications for enhancing the modeling of complex, real-world graph-structured data.\n\n### Overall Assessment:\nThis paper introduces somewhat innovations in the field of graph transformer models, particularly in addressing heterophilous relationships, a relatively less explored yet crucial aspect of graph-structured data analysis. The originality in model design (MPformer), coupled with a new approach to position encoding (HeterPos), marks some advancement in the field. The quality of research, clarity of presentation, and the effort on both theory and practical applications of graph neural networks make this paper a substantial contribution to the literature."
            },
            "weaknesses": {
                "value": "- Insufficient Benchmarking Against Alternative Methods: Although the paper introduces HeterPos, a position encoding technique, it lacks a comprehensive comparative analysis with other existing positional encoding methods. The authors compared with several position encoding but there are more position encoding methods such as shortest-path distances (Ying et al., 2021) and tree-based encodings (Shiv and Quirk, 2019).  This comparison is crucial for highlighting the strengths and potential limitations of HeterPos in different scenarios.\n\n\n- Ambiguity in Acronym: The paper does not clarify what \"MPformer\" stands for, which can lead to ambiguity and confusion. Providing a full name or a clear expansion of acronyms is crucial for effective communication and for the reader\u2019s understanding, especially in technical fields where specific terms and models are frequently discussed.\n\n\n\n[Ying et al., 2021] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? NeurIPS 2021.\n\n\n[Shiv and Quirk, 2019] Vighnesh Shiv and Chris Quirk. Novel positional encodings to enable tree-based transformers. NeurIPS 2019."
            },
            "questions": {
                "value": "-"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840668264,
        "cdate": 1698840668264,
        "tmdate": 1699636917202,
        "mdate": 1699636917202,
        "license": "CC BY 4.0",
        "version": 2
    }
]