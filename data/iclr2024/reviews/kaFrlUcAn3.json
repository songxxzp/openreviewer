[
    {
        "id": "1d7fw7NtlO",
        "forum": "kaFrlUcAn3",
        "replyto": "kaFrlUcAn3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6103/Reviewer_PBNe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6103/Reviewer_PBNe"
        ],
        "content": {
            "summary": {
                "value": "This paper considers debiasing language models using a combination of energy-based models and ordinary differential equations. It uses the idea of VAE to encode the text into latent space and uses the energy-based model for debiasing. The ODE is for providing sampling and guiding the model to sampling in a lower energy region. The experiments use two language model-based evaluation benchmarks and show it is able to reduce bias. It also conducts an analysis of the latent space representation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is the first to use ODE to do language model debias via its sampling advantages and it is able to mitigate bias in multiple aspects. The idea of using the energy-based model is also new and it shows the effectiveness in two intrinsic tasks as well as latent embedding space."
            },
            "weaknesses": {
                "value": "The paper presents intricate concepts, which, unfortunately, are challenging to decipher due to its structure. Several crucial details seem omitted, making it difficult to follow.\n\nMethodologically, the work seems to merge different methods, lacking a clear, coherent logic.\n\nA significant concern I have lies in the experimental design. While the paper emphasizes intrinsic metrics that evaluate the language model itself, it overlooks the broader implications for downstream tasks. Merely reducing bias at the language model level does not guarantee that the bias is eliminated when the model is applied to real-world applications. These downstream applications are ultimately what matter most, as they are what end users interact with and perceive. Notably, many of the baselines you cite in the paper like INLP, and MABEL all conduct extrinsic evaluations. \n\nMost importantly, the paper's experiment metrics also concern me, as discussed in the paper: https://aclanthology.org/2021.acl-long.81.pdf . The crows-pairs and Stereoset have a range of pitfalls that threaten these benchmarks\u2019 validity as measurement models for stereotyping and bias.  Besides, there are other intrinsic evaluation metrics that need to be tested on: https://proceedings.neurips.cc/paper/2019/hash/201d546992726352471cfea6b0df0a48-Abstract.html, https://ojs.aaai.org/index.php/AAAI/article/view/21453"
            },
            "questions": {
                "value": "1. for the intuition part of the paper, why does removing the word 'Ethiopian' then the bias is reduced? how is the bias defined here?\n2. When using the BERT as encoder, do you use the [cls] vector as the latent space point?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6103/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6103/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6103/Reviewer_PBNe"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6103/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697774448479,
        "cdate": 1697774448479,
        "tmdate": 1700631581546,
        "mdate": 1700631581546,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "K0PCd76kmf",
        "forum": "kaFrlUcAn3",
        "replyto": "kaFrlUcAn3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6103/Reviewer_kTHj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6103/Reviewer_kTHj"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new method for mitigating biases in large language models (LLMs). Their approach involves a multi-step process. First, they employ a Variational Autoencoder (VAE) to transform text into latent variables. Next, a classifier is trained to discern the biased attributes within these latent variables. Finally, they construct an energy-based model (EBM) based on the pretrained classifier. This EBM generates latent variables with consistent density values for various sensitive attributes, enabling the generation of debiased text by decoding these corresponding latent variables. The experiments conducted in the study provide compelling evidence that these methods effectively reduce stereotypes in LLMs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper tackles a crucial concern within the realm of LLMs, one that carries significant societal implications.\n- Empirical findings from the experiments underscore the efficacy of the proposed method in successfully alleviating biases present in LLMs."
            },
            "weaknesses": {
                "value": "- Expensive training is a prerequisite. In the introduction, the authors contend that \"our method can readily adapt to arbitrary biases without costly retraining,\" which, in reality, is not the case. On the contrary, the proposed approach entails training a VAE and a classifier, which can be highly computationally demanding.\n- The writing appears rather unclear to me. Regarding the experimental specifics, there seems to be a substantial lack of information on how to train the VAE and classifier. The performance hinges greatly on the pretrained VAE, but training a VAE, particularly with high-dimensional and discrete data like text, can be quite challenging. It would be immensely beneficial if more elaborate details were provided. Did you employ any pretrained models for initialization? Could you elucidate the precise training procedures, including any strategies or techniques employed?\n- The VAE training process might be detrimental to the overall performance. I believe a more straightforward approach involves training a classifier using the latent representations of existing LLMs, such as BERT or LLAMA. I find myself puzzled about the necessity of the VAE training procedure. Furthermore, fine-tuning LLMs as VAEs could potentially compromise performance if not executed flawlessly. The performance is critically contingent on the quality of the trained VAE, which, in my view, carries inherent risks."
            },
            "questions": {
                "value": "- How is your sampling method connected to diffusion models? In section 3.4, you allocate substantial space to discussing diffusion models, even though the connection appears somewhat tenuous. I am curious about the time-variant density $p_t$ in your context. My understanding is that you merely need to sample from your EBM. In this case, you could employ Langevin dynamics or similar samplers like HMC. Could you elaborate further on why you have opted for ODE instead of Langevin dynamics? Additionally, could you provide insights into how equation 8 satisfies the continuity equation and converges to your target distribution?\n- The algorithm would greatly benefit from a more intuitive explanation. It seems that your aim is to utilize the EBM to guide the generation of latent variables. The EBM is defined as the summation over the classifier, where $\\log p(\\mathcal{A}|z) \\propto \\sum_{i} f_i(a_i | z)$. To mitigate biases, the objective is to generate the latent variable $z$ in a way that ensures $p(a_i|z) \\approx p(a_j|z), \\forall i,j$. However, if I comprehend correctly, what you are currently doing is sampling $z \\sim p(\\mathcal{A}|z)$. In this case, it's not evident how the generated latent variable $z$guarantees the desired properties, i.e., $z \\sim p(\\mathcal{A}|z)$, and subsequently, $p(a_i|z) \\approx p(a_j|z) \\forall i,j$. Could you provide more clarity on this?\n- I am finding it challenging to grasp how the latent variable sampled from EBMs maintains consistency with the latent space of the VAE. I have concerns that the generated debiased latent variables might not yield fluent results. Looking at eq.8, it appears that there is no constraint ensuring that the samples from EBMs reside within the latent space of the VAE."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6103/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6103/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6103/Reviewer_kTHj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6103/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698615179350,
        "cdate": 1698615179350,
        "tmdate": 1700661474749,
        "mdate": 1700661474749,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "02qKr8Jquq",
        "forum": "kaFrlUcAn3",
        "replyto": "kaFrlUcAn3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6103/Reviewer_CA2c"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6103/Reviewer_CA2c"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses a pivotal issue in Large Language Models (LLMs) concerning biases and fairness. The author introduces an EBM-based approach for controlled generation in LLMs, treating the EBM as a classifier that can be learned discriminatively. The author further suggests utilizing ODE sampling over Langevin dynamics to reduce computational overhead. While such techniques are familiar in the realm of large vision models, their application in LLMs remains relatively unexplored."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method seems sound and reasonable.\n2. The EBM formulation is simple and interesting.\n3. The paper is clear to understand."
            },
            "weaknesses": {
                "value": "See questions."
            },
            "questions": {
                "value": "1. I feel the major bottleneck for using latent variable model in LLMs is mode collapse, which has been studied in [1,2]. Given longer enough context, the autoregressive model will ignore the effects of the latent variables, especially with powerful enough decoders. I see you use the cyclic annealing in Bert+GPT2 experiment to address this issue. However, I want to know more results and evaluation for more powerful models such as LLaMA-2. \n\n2. Another way to address mode collapse is to make the prior distribution more meaningful rather than isotropic Gaussian used in VAE. This shares the similar intuition as the cyclic annealing used in this paper. Do you consider using learnable prior in the training rather than in a two-step manner [3]?\n\n3. The energy-based formulation in this paper is similar to latent space energy-based model, where energy function is severed as an exponential tilting of an isotropic Gaussian[4]. Some pioneering EBM work should be cited as well including [5-8].\n\n4. For the ODE sampler, how does the ODE solver compare performance-wise to using Equ.11 directly for Langevin sampling (without noise)? Could you distinguish between your Runge-Kutta method and Langevin sampling in implementation?\n\n[1] Pang, Bo, et al. \"Generative text modeling through short run inference.\" arXiv preprint arXiv:2106.02513 (2021).\n\n[2] Xu, Yan, et al. \"Diverse and Faithful Knowledge-Grounded Dialogue Generation via Sequential Posterior Inference.\" arXiv preprint arXiv:2306.01153 (2023).\n\n[3] Pang, Bo, and Ying Nian Wu. \"Latent space energy-based model of symbol-vector coupling for text generation and classification.\" International Conference on Machine Learning. PMLR, 2021.\n\n[4] Pang, Bo, et al. \"Learning latent space energy-based prior model.\" Advances in Neural Information Processing Systems 33 (2020): 21994-22008.\n\n[5] Xie, Jianwen, et al. \"A theory of generative convnet.\" International Conference on Machine Learning. PMLR, 2016.\n\n[6] Xie, Jianwen, et al. \"Cooperative training of descriptor and generator networks.\" IEEE transactions on pattern analysis and machine intelligence 42.1 (2018): 27-45.\n\n[7] Nijkamp, Erik, et al. \"Learning non-convergent non-persistent short-run MCMC toward energy-based model.\" Advances in Neural Information Processing Systems 32 (2019).\n\n[8] Du, Yilun, and Igor Mordatch. \"Implicit generation and generalization in energy-based models.\" arXiv preprint arXiv:1903.08689 (2019)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6103/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6103/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6103/Reviewer_CA2c"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6103/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698706337977,
        "cdate": 1698706337977,
        "tmdate": 1699636658598,
        "mdate": 1699636658598,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "O5KbL01oNF",
        "forum": "kaFrlUcAn3",
        "replyto": "kaFrlUcAn3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6103/Reviewer_bvt7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6103/Reviewer_bvt7"
        ],
        "content": {
            "summary": {
                "value": "This paper presented Debiasing via Continuous Energy-Based Models (DICE). It utilizes Energy-Based Model (EBM) gradient with Ordinary Differential Equations (ODEs) to reduce biases. The method part was inspired by [1,2], especially [2].\n\nEmpirical results on variant scale language models (GPT2, LLaMA, LLaMA-2) on two datasets (Crows-Pairs, StereoSet) showed effectiveness in debiasing.\n\n[1] Weili Nie, Arash Vahdat, and Anima Anandkumar. Controllable and compositional generation\nwith latent-space energy-based models. Advances in Neural Information Processing Systems,\n34:13497\u201313510, 2021.\n\n[2] Guangyi Liu, Zeyu Feng, Yuan Gao, Zichao Yang, Xiaodan Liang, Junwei Bao, Xiaodong He,\nShuguang Cui, Zhen Li, and Zhiting Hu. Composable text controls in latent space with odes.\narXiv preprint arXiv:2208.00638, 2022."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. Clear presentation with graphic illustrations on the proposed methodology.\n2. Detailed description of the dataset together with extensive empirical results."
            },
            "weaknesses": {
                "value": "1. **Lack of novelty!** Majority of the method are from [1]. DICE can be viewed as [1] with attributes about bias.\n2. Unclear description about Figure 2. Some interpretation was provided but without detail explanation or convincing empirical result.\n3. The presentation about why after the sampling by solving the ODE, the $z_{0}$ can be decoded to less biased context.\n\n[1] Guangyi Liu, Zeyu Feng, Yuan Gao, Zichao Yang, Xiaodan Liang, Junwei Bao, Xiaodong He,\nShuguang Cui, Zhen Li, and Zhiting Hu. Composable text controls in latent space with odes.\narXiv preprint arXiv:2208.00638, 2022."
            },
            "questions": {
                "value": "1. More details about the AtM and PSA are needed.\n2. In [1] the attributed are asserted by assigning value for $a_i$ during the sampling. However, in Appendix A.7 Table 7, the Crows-Pairs data comes with labels $\\\\{more,less\\\\}$ biased. But Sec 2.1 says $a_i$ belongs to Male / Female / Neutral as in Table 9. What are the labels in Algorithm 1 line 9?\n3. If the answer for previous question is the labels in Table 9, then a natural question follows: How to generate debiased context? [1] controls generation by assigning attribute values. Here how to generate debiased context? Are you assigning $a_i$ as well? If so, how to guarentee that $a_i$ is not biased? For example, for the same attribute *male*, ''he is a doctor'' and ''he is a nurse'' may be considered differently considering bias. How $a_i$ was chose for those cases?\n4. Confusing metric scores. Especially icat and ss. In Sec 4.1.2 calculating icat, perfect model has ss of 50. However, in Table 2 ss is lower is better. That leads to GPT2-large Race has best ss, best lms but non-best icat.\n\u00a0\n[1] Guangyi Liu, Zeyu Feng, Yuan Gao, Zichao Yang, Xiaodan Liang, Junwei Bao, Xiaodong He,\nShuguang Cui, Zhen Li, and Zhiting Hu. Composable text controls in latent space with odes.\narXiv preprint arXiv:2208.00638, 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6103/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6103/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6103/Reviewer_bvt7"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6103/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742099753,
        "cdate": 1698742099753,
        "tmdate": 1700698638969,
        "mdate": 1700698638969,
        "license": "CC BY 4.0",
        "version": 2
    }
]