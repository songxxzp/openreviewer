[
    {
        "id": "5Sh79Q2TGk",
        "forum": "WEQS3oUPs3",
        "replyto": "WEQS3oUPs3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8055/Reviewer_yVGF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8055/Reviewer_yVGF"
        ],
        "content": {
            "summary": {
                "value": "This paper argues that LLMs trained with SFT or single-step RL might struggle with tasks that require goal-directed behavior. The authors propose a method to use LLMs to generate useful data for solving such tasks by simulating human-like behaviors. The data are used to train a conversational agent for goal-directed tasks with offline RL, in order to improve over the trained conversations. Their results show that the approach achieves better performance than directly prompting LLMs or training the agents with behaviour cloning, in various goal-directed dialogue tasks. However, there are concerns about how the human annotator evaluates the conversations and why the authors did not choose widely-used task-oriented dialogue benchmark datasets like multiwoz and schema-guided-dialogues (SGD).\nIn addition, although the results show that the learned agents are better than LLMs in information-seeking and generating less overwhelming responses in some specific tasks or domains, I wonder if the smaller agents are able to handle other domains, where the small agents might not have knowledge. Is it still a good alternative to LLMs in this case? Is it still useful?"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper introduces an approach to generate goal-directed conversations with LLMs and then train a smaller agent to improve over these conversations. The human evaluation results show that the learning agents do generate responses that are more helpful in helping the users complete the tasks and generate less overwhelming responses."
            },
            "weaknesses": {
                "value": "1. Why not utilize the widely used task-oriented dialogue benchmarks, MultiWOZ and SGD? There are works that leverage LLMs for task-oriented dialogue by training a small model to generate dialogue actions (plans) with RL, guiding LLMs for improved responses [1]. Have you considered comparing with them?\n2. From the examples comparing GPT-agent and IE+RL agents, GPT's responses didn't seem significantly inferior. How were the responses scored by the evaluators using the four criteria? Was there consensus in their annotations, and what was the level of agreement?\n3. LLMs often produce overwhelming responses, but their strength lies in their capability to converse on a wide range of topics due to their inherent knowledge. While training a smaller model for better goal-oriented conversations for some specific domains and topics might seem more beneficial than directly using LLMs, is such a model able to handle out-of-domain tasks and topics? How does it perform when discussing topics outside its training data? If it can not handle out-of-domain topics, is such a model still practical and useful for real-world applications?\n4. I feel that the paper complicates the data generation section by unnecessarily introducing numerous reinforcement learning concepts. Why introduce these concepts when it appear to be a simple data generation process, making it challenging to comprehend?\n\n[1] Li, Z., Peng, B., He, P., Galley, M., Gao, J., & Yan, X. (2023). Guiding Large Language Models via Directional Stimulus Prompting. arXiv preprint arXiv:2302.11520."
            },
            "questions": {
                "value": "See the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8055/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8055/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8055/Reviewer_yVGF"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8055/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698545852144,
        "cdate": 1698545852144,
        "tmdate": 1699636996110,
        "mdate": 1699636996110,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lbMPfm81Im",
        "forum": "WEQS3oUPs3",
        "replyto": "WEQS3oUPs3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8055/Reviewer_Ntkf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8055/Reviewer_Ntkf"
        ],
        "content": {
            "summary": {
                "value": "The study presents a reinforcement learning (RL) approach for training goal-directed dialogue agents on synthetic dialogues produced by large language models (LLMs). Known as the \"imagination engine,\" this method generates training data from simulated talks instead of large-scale human-generated datasets. This technique yields agents who perform better on goal-oriented activities than typical LLMs, indicating a new direction for conversational AI development\u2014one that can comprehend and accomplish difficult tasks with little to no human oversight."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. It shifts the use of LLMs from direct interaction to data generation for optimization by introducing a zero-shot RL algorithm with a \"imagination engine\" that creatively creates synthetic conversation datasets for training dialogue agents.\n\n2. Compared to traditional approaches, the method optimizes for goal-directed dialogues more effectively since it trains agents on a variety of human-like talks generated by LLMs that are customized for particular dialogue objectives.\n\n3. The usefulness and efficiency of this approach are demonstrated empirically, as agents trained with it outperform state-of-the-art LLMs in interactive tasks."
            },
            "weaknesses": {
                "value": "A shortcoming of the work is its somewhat dependent use of human-generated prompts, suggesting opportunities for further development in automating zero-shot dialogue agents' training to work without task-specific human input."
            },
            "questions": {
                "value": "What are the detailed version specifications and hyper-parameter configurations of GPT-3.5 used in the imagination engine, and how do these parameters affect the generated dialogue quality and diversity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8055/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8055/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8055/Reviewer_Ntkf"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8055/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840651926,
        "cdate": 1698840651926,
        "tmdate": 1699636995973,
        "mdate": 1699636995973,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gsvEjE2ESN",
        "forum": "WEQS3oUPs3",
        "replyto": "WEQS3oUPs3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8055/Reviewer_sDqR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8055/Reviewer_sDqR"
        ],
        "content": {
            "summary": {
                "value": "This paper utilizes LLM to simulate sub-optimal but human-like behavior to produce examples of possible interactions. The algorithm uses the data and offline reinforcement learning to train an interactive conversational agent to learn to perform more optimal interactions. Experiments show that the method achieves the most advanced performance in a variety of goal-oriented conversation tasks.\nWhat contributions does it make: \n1.The paper propose a zero-shot RL algorithm that effectively optimizes for goal-directed dialogue tasks.\n2.The idea of imagination engine (IE) that generates a dataset of diverse, task-relevant, and instructive dialogues makes sense."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.The experimental analysis is detailed and methodical, and the case is clear and intuitive.\n2.The idea of using LLM to imitate human behavior is interesting."
            },
            "weaknesses": {
                "value": "1.Even thought RL can combine parts of behavious seen form behavior policies in the data, it is not convincing that the RL can take all the long-term planing responsibility in the goal-oriented conversation tasks. \n2.The novelty of this paper is limited. The proposed method can be regarded as a pipeline of LLM generation and offline RL training.  \n3.All the evaluation methods are human evaluation, which are highly subjective. \n4.More relevant works should be compared in the experiments."
            },
            "questions": {
                "value": "Need more details about the evaluators in the experiments, such as their education background."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8055/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8055/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8055/Reviewer_sDqR"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8055/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698923816879,
        "cdate": 1698923816879,
        "tmdate": 1700644488464,
        "mdate": 1700644488464,
        "license": "CC BY 4.0",
        "version": 2
    }
]