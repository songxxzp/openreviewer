[
    {
        "id": "XxnYlCmWlx",
        "forum": "RxhOEngX8s",
        "replyto": "RxhOEngX8s",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6028/Reviewer_Jh7c"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6028/Reviewer_Jh7c"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the problem of OOD detection in deployed machine learning systems. The authors analyze existing OOD detectors, identifying a common limitation in their adaptability to diverse distribution shifts. To address this, they propose a new benchmark named BROAD, designed to evaluate OOD detectors across a wide spectrum of distribution shifts. This benchmark examines various scenarios, including novel classes, adversarial perturbations, synthetic images, corruptions, and multi-class inputs. Additionally, the authors introduce an approach that leverages an ensemble of reliable OOD detectors combined with a GMM."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper explains the existing limitations of current OOD detectors, providing a clear and compelling rationale for advancement in this domain. The paper's main strength lies in the introduction of the BROAD benchmark, which establishes a robust evaluation framework. This benchmark not only rigorously assesses OOD detector performance but also offers a critical insight into their ability to extend beyond the scope of novel classes. Through meticulous experimentation and analysis, the authors provide compelling evidence of OOD detector capabilities and potential areas of improvement. This well-constructed evaluation framework significantly contributes to the depth and reliability of the research findings."
            },
            "weaknesses": {
                "value": "The paper introduces an ensemble method for broad OOD detection; however, there are notable weaknesses. Its efficiency and inability to be scaled up raise questions about its practicality in real-world applications. Moreover, the paper lacks a clear roadmap or forward-looking guidance for the broader OOD detection community on how to effectively approach the challenges posed by the BROAD benchmark. While the intent to introduce a method is appreciated, the proposed method is clearly not the ideal solution to this intricate problem, as it does not offer new ideas or new directions to the OOD community. A more comprehensive analysis and discussion on potential future directions and steps for advancing OOD detection would greatly enhance the paper's overall impact and utility to the research community. This would provide valuable insights for researchers looking to build upon this work and make meaningful strides in the field of OOD detection."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6028/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6028/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6028/Reviewer_Jh7c"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6028/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697538871890,
        "cdate": 1697538871890,
        "tmdate": 1699636647945,
        "mdate": 1699636647945,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vyIj3bcxfG",
        "forum": "RxhOEngX8s",
        "replyto": "RxhOEngX8s",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6028/Reviewer_GVGN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6028/Reviewer_GVGN"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a OOD benchmark comprising five different types of distribution shift. The result shows that the performance of OOD detection methods are not consistent over different types of distribution shift. The paper propose a method to ensemble different OOD detection methods to achieve consistent performence over different types of distribution shift."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1). The assessment of a broder OOD detection capabilities is interesting and probablity important for future OOD detection method development.\n\n2). Extensive experiments have been done to benchmark the recent OOD detection methods.\n\n3). Overall, the paper is clear and well-written."
            },
            "weaknesses": {
                "value": "My concerns are mainly about the proposed method.\n\n1). The ensemble of OOD detection methods seems ad-hoc for this benchmark by evaluating and picking some of the methods that perform relatively well on the benchmark.\n\n2). The proposed method of fitting GMM over scores from different OOD detection methods does not make sense to me. For example, in Sec.3, it says \"this approach is adept at identifying atypical realizations of the underlying scores, even in situations where the marginal likelihood of each score is high, but their joint likelihood is low.\" It would be weird if most in-distribution samples can not achieve high likelihood at each score while out-of-distribution samples can, since all these methods aim at measuring if the sample is in-distribution. In other words, what is the advantage of fitting GMM over taking average of different scores?\n\n3). The time complexity or the scalability of the proposed method is still of concern. Though the results shows that the time complexity of the proposed ENS-F is acceptable at 25% of the time for a normal inference. However, when comparing to the methods it ensembles (e.g. MSP takes 1% additional time), the time complexity is extremely high."
            },
            "questions": {
                "value": "1). The proposed method uses a validation set to fit GMM, does it affect the performance? With the use of additional data, is it fair to compare the proposed method with other baseline methods?\n\n2). Does the proposed ensemble method outperform simple ensemble methods such as taking the average of the scores or the largest score? If so, why would it be?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6028/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6028/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6028/Reviewer_GVGN"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6028/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698571150791,
        "cdate": 1698571150791,
        "tmdate": 1699636647808,
        "mdate": 1699636647808,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RcyPD9hAzB",
        "forum": "RxhOEngX8s",
        "replyto": "RxhOEngX8s",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6028/Reviewer_xuax"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6028/Reviewer_xuax"
        ],
        "content": {
            "summary": {
                "value": "The paper propose a new visual OOD detection benchmark consisting of 5 types of distributional shifts (1) novel classes (2) adversarial perturbations (3) synthetic images (4) corruptions (5) images with multiple objects. The paper further evaluates the performance of various OOD detection methods (that do not require training/fine-tuning) and observe that the performance is inconsistent across different types of distribution shifts. Lastly, the authors propose to ensemble various scores with Gaussian mixture models, which demonstrates better performance."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The overall organization of the paper is clear and easy to follow.\n- The proposed ensembling method is straightforward and demonstrate good performance."
            },
            "weaknesses": {
                "value": "- The major weakness of the paper is that most OOD detection scores considered in the paper are proposed to only handle novel classes. Expecting such OOD scores to detect adversarial perturbations and corruptions may be **out-of-scope** and unrealistic.\n\nIn particular, recent work [1] has demonstrated that when OOD samples are not involved during training (the setting considered in this work), it can be **theoretically impossible** to expect common OOD detection methods to work. Despite that detecting adversarial perturbations and corruptions are interesting tasks, directly utilizing post-hoc OOD detection scores is ill-justified and the failure is expected.\n\n- In the multi-label scenario, it seems more reasonable to use object detection models instead of classification models. The failure of OOD detection based on classification models is expected. It would be more interesting to see the performance of OOD detection given bounding boxes."
            },
            "questions": {
                "value": "Method:\n- Can authors justify in theory or principle why OOD detection methods are suitable for detecting adversarial perturbations and corruptions?\n\nExperiments: \n- Can authors provide further OOD detection results with object detection models for the multi-label case?\n\n[1] Fang et al., Is Out-of-Distribution Detection Learnable?, NeurIPS 2022"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6028/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813274474,
        "cdate": 1698813274474,
        "tmdate": 1699636647702,
        "mdate": 1699636647702,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BWD3y6yps6",
        "forum": "RxhOEngX8s",
        "replyto": "RxhOEngX8s",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6028/Reviewer_WvA2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6028/Reviewer_WvA2"
        ],
        "content": {
            "summary": {
                "value": "This paper reviews Out of Distribution (OOD) Detection, in the sense of samples seen in the real world that were not covered in the training data. Two approaches are \n1) to create robust systems, designed to not degrade on OOD data, and \n2) to flag samples uncharacteristic of the training data.\nDistribution shift detection appears more practical, but can be fooled, by the multiple ways OOD can occur for such diverse reasons (in images): as novel classes, adversarial attacks, synthetics, corruptions multiple labels. \n\nThe paper introduces \n - a new OOD benchmark with 12 datasets representing the various OOD reasons\n - benchmarking of a variety of existing methods published in the last decade,\n - And demonstration of a Gaussian Mixture Model of an ensemble of existing methods with significant gains over existing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper offers a comprehensive view of why image recognition models may fail on images that have not been seen during training, and by a comprehensive set of tests demonstrates the relative value of existing detection methods when applied to tasks they were intended for, and for other OOD tasks that are related, but not explicitly targeted by the existing method.   Of note is the interesting comment on how to build OOD detectors with generative models, by use of a function they designate as \"h(x)\" that so to speak \"sees inside\" the generative network, offering an extended feature set for detection.  \n\nThe paper introduces a OOD classifier that is a combination of existing methods by use of a Gaussian mixture model that has better coverage, and achieves an AUC score on average superior to existing methods."
            },
            "weaknesses": {
                "value": "There isn't sufficient detail in the paper to re-construct the Gaussian mixture model (GMMs) proposed by the authors. GMMs are conventionally used to estimate density functions for oddly-shaped distributions, e.g. with multiple modes.  It is intuitive, in fact not unexpected, that creating an ensemble of detectors has better  performance on average than any individual detector, so the novelty of this finding is limited. however the results from the paper are not reproducible from the paper's contents. Given the scores how is the GMM density learnt? In what sense is this an ensemble? How does this generate a classification and thus an AUC? \n\nOne gets the sense that this work would find a better audience in a more engineering-oriented conference where testing comparisons of performance were the primary interest, and algorithmic aspects were not."
            },
            "questions": {
                "value": "If the construction and output of the GMM classifier is actually revealed in the paper and this review overlooked it, please explain."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6028/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699033119670,
        "cdate": 1699033119670,
        "tmdate": 1699636647606,
        "mdate": 1699636647606,
        "license": "CC BY 4.0",
        "version": 2
    }
]