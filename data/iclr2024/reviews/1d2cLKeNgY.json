[
    {
        "id": "aFu9Q3S2RB",
        "forum": "1d2cLKeNgY",
        "replyto": "1d2cLKeNgY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1573/Reviewer_Fz1j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1573/Reviewer_Fz1j"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces ReSimAD, a unified reconstruction-simulation-perception paradigm that addresses the domain shift issue when 3D detectors face cross-domain performance degradation in the field of autonomous driving.\nThe experimental results conducted on multiple 3D detectors and datasets verify that the proposed paradigm can maintain cross-domain performance in the simulated target domain. In addition, it can also assist and accelerate model training optimization, which has a certain application value."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Regarding the phenomenon of cross-domain performance degradation in 3D detectors in the field of autonomous driving, this paper proposes a paradigm of reconstruction-simulation-perception from the perspective of data sources. This paradigm can alleviate or partially solve the problem of cross-data domain discrepancies. The proposed approach, named ReSimAD, has significant research value."
            },
            "weaknesses": {
                "value": "1)\tThe readability of the paper needs improvement, such as the illustrations, explanations, and better formatting.\n2)\tSome necessary elaborations and supporting evidence need to be added."
            },
            "questions": {
                "value": "1)\tFigure 1 and 3 can be made more illustrative of the proposed paradigm.\n2)\tThe formatting of the paper needs adjustment, for example, ensure that figures and corresponding sections are not too far apart.\n3)\tTo eliminate artifacts in Point-to-mesh Implicit Reconstruction, it should be further explained how the authors performed point cloud registration when consolidating all frames from the corresponding sequence.\n4)\tThe interpretation of \"Closed Gap\" in Table 2 and its analysis are not provided in the paper.\n5)\tThe paper does not mention the significance or the specific impact of \"the matching of vehicle traffic flow density\" mentioned in Section 4 (Mesh-to-point Rendering).\n6)\tIn Table 5, what is the reason of the significant gap between using zero-shot and oracle methods? The details regarding the sample quantity and other experimental settings can be supplemented in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1573/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698398625657,
        "cdate": 1698398625657,
        "tmdate": 1699636085751,
        "mdate": 1699636085751,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mk4s7iHXJ8",
        "forum": "1d2cLKeNgY",
        "replyto": "1d2cLKeNgY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1573/Reviewer_bhEL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1573/Reviewer_bhEL"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces 3d domain transfer pipeline, ReSimAD.\nIt focuses on bridging the gap between old autonomous driving datasets and newly collected real-life datasets.\nThe pipeline is threefold, 1) point-to-mesh implicit reconstruction, 2) mesh-to-point rendering, and 3) zero-shot perception process.\nFirst two stages are designed to simulate target domain's LiDAR configuration using source domains. For better granulity compared to using raw sparse LiDAR points, it utilizes implicit SDF representation.\nThe last stage is to use the simulated dataset for 3D detection method and perform zero-shot inference on the real dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper suggests dataset generation/simulation pipeline that may enrich/adjust old annotated dataset to the new target domain."
            },
            "weaknesses": {
                "value": "1. Poor presentation\nOverall, the placement of figures and tables is not aligned with the text, thereby the whole paper is difficult to follow.\n\n2. Technical novelty & Writing\nThe paper mainly focuses on the dataset simulation process using old annotated dataset.\nTherefore, the most of the methodology is restricted to step-by-step instructions, rather than providing theoretical insights or verifications.\nI believe the paper's contribution on introducing new dataset simulation pipeline does not exceeds its lack of technical novelty.\n\n3. Questionable dataset selection\nSince Waymo dataset is more recent and contains more LiDAR sensors all around the vehicle, compared to nescenes or KITTI, wouldn't it be more plausible to simulate Waymo from KITTI, rather than KITTI from Waymo, to be more in coherence with the paper's motivation?\nIt seems like the pipeline only focuses on interchanging sensor configuration, using the richest point cloud information. Please elaborate.\n\nOn the minor note, I believe that this paper is more related to computer vision or robotics field than machine learning."
            },
            "questions": {
                "value": "Addressed in weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1573/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1573/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1573/Reviewer_bhEL"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1573/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770592041,
        "cdate": 1698770592041,
        "tmdate": 1700705241201,
        "mdate": 1700705241201,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tU3ufSpZL4",
        "forum": "1d2cLKeNgY",
        "replyto": "1d2cLKeNgY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1573/Reviewer_kWr7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1573/Reviewer_kWr7"
        ],
        "content": {
            "summary": {
                "value": "This is a system paper. The authors study a weakened domain generalization setting for 3d object detection from lidar point clouds. For example, training only on Waymo dataset and only accessing the target domain's lidar statistics, the setting pursues good performance on nuScenes. Since information about the target domain is not completely unknown, this is a weakened domain generalization setting. The proposal is to reconstruct the mesh using aggregated LIDAR (or RGB? as NeuS is mentioned). Then the background mesh is put into Carla and car assets are placed according to object size matching. The lidar signal is simulated using the composed scene in Carla. Then the authors train detectors using these simulated data and show the results out-perform the UDA baseline."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The idea generally makes sense and the authors benchmark it in a large-scale, showing meaningful margins."
            },
            "weaknesses": {
                "value": "- The biggest issue is a lack of clarify, for both the mesh reconstruction and the sampling part:\n* The reconstruction does not define input, output and losses formally. The only equation is depth rendering. Aggregated lidar piont clouds are used for training? (this is highlighted in bold texts) The authors mention NeuS and I am not sure whether RGB rendering losses are used. \n* Lidar rendering is difficult and there are some sphiscated methods to simulate second-returns [A]. Again the lidar rendering part does not contain any formal mathematical exposition so I cannot understand what the lidar rendering formulation is.\nSince two major algorithmic parts are not understandable, I am rating presentation as poor.\n\n- Let alone the presentation issue about algorithms, I will just assume that these two parts invoke some black-box functions and only consider the system. In this regard, I find the mesh of poor quality. This is understandable as recent papers from my group can only reconstruct meshes from lidar with similar quality. Having that said, I cannot understand why points generated from them are meaningful for detection. The authors should present a systematic evalution for mesh quality (Table.6 does not make sense to me) and rendered point cloud quality. Some comparisons are also needed, e.g., comparing with point clouds rendered from VDBF?\nThis concern makes me rate soundness as fair.\n\nMinor but still confusing issues:\n- Fig.2 is confusing. I cannot understand what the differences are except for the figure color.\n- Table.1 gives literally no additional information since target domains are already mentioned in texts. And Waymo is also target domain\uff1f\n\n[A] Neural LiDAR Fields for Novel View Synthesis"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1573/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1573/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1573/Reviewer_kWr7"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1573/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773982068,
        "cdate": 1698773982068,
        "tmdate": 1700673982063,
        "mdate": 1700673982063,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mgePA656mZ",
        "forum": "1d2cLKeNgY",
        "replyto": "1d2cLKeNgY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1573/Reviewer_7ZCS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1573/Reviewer_7ZCS"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes RESIMAD, which aims to base source 3D reconstruction for target-domain point clouds data generation. Specifically, using implicit neural fields with accumulated point clouds per sequence, a high-quality source-domain 3D mesh could be extracted as background of AD scenes. Foreground synthetic assets like vehicles are added within simulation guided by source GT information. Therefore, target-domain point clouds could be further extracted directly or rectified with domain-specific sensor specs. Experiments compared against UDA baseline and pre-training effectiveness demonstrate the potential usage of RESIMAD."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The overall paper is well motivated and easy to follow. The RESIMAD pipeline of is composed of three key components and clearly discussed with adequate implementation description.\n\n- The experiments on several Waymo to other datasets setup shows the effectiveness of RESIMAD."
            },
            "weaknesses": {
                "value": "1. Effectiveness on larger detection models. One potential benefit of a \u2018reconstruction, simulation and perception\u2019 pipeline is that numerous data could be generated/rendered. As only a typical point RCNN baseline model is chosen for evaluation, it would further highlight the advantages of such methods considering more powerful and data-hungry models (e.g., ViT, DETR like detection models). \n\n2. Only \u2018zero-shot\u2019 performance is given. It would still be valuable to see whether RESIMAD could benefit from increasingly more target-domain information, from sensor specs (almost zero shot), to few-shot set-up or even with more target samples available)\n\nAbove tow points also applies to the pre-training experiment in Table3.\n\n3. Comparison against more recent DA/UDA methods. UDA is a popular topic attracting much attentions, it is expected to compare against more recent SOTA DA/UDA approaches to further support the evaluation.\n\n4. Although an almost \u2018zero-shot\u2019/unsupervised DA could be achieved, such zero-shot performance, I would say, requires the underlying similarity of driving scenarios and a large amount of geometric observations (to form background 3d meshes). I am wondering if the source domain shifts from Waymo to nuScenes or KITTI, will the proposed method still able to work well?\n\n5. Related to last point, one of my main concern is that the acquisition of 3D meshes. It may require a lot of computational costs and multi-lidar sensor specs for pre-training (per-scene optimization of LINR) to get relatively complete and accurate 3d meshs?\n\nEffectiveness and contributions of the proposed methods could be further improved in the above aspects in my opinion. I would like to hear from from authors in the rebuttal phase."
            },
            "questions": {
                "value": "Please see the weaknesses section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1573/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1573/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1573/Reviewer_7ZCS"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1573/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699614978412,
        "cdate": 1699614978412,
        "tmdate": 1700702337742,
        "mdate": 1700702337742,
        "license": "CC BY 4.0",
        "version": 2
    }
]