[
    {
        "id": "4SUIqL0tPr",
        "forum": "Po6lYYsrB4",
        "replyto": "Po6lYYsrB4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2130/Reviewer_e8e8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2130/Reviewer_e8e8"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes Action-Aware Embodied Learning for Perception (ALP) which leverages active exploration and action information for representation learning and finetuning for downstream tasks. In the first stage, ALP learns a task-agnostic visual representation by combining exploration policy and inverse dynamics prediction objectives. During second stage a subset of data from first stage is labelled and used for finetuning the pretrained backbone. Authors claim they propose the novel idea of using a shared backbone for representation learning and exploration policy trained with RL. The paper also shows that the proposed method outperforms baseline methods (especially in OOD scenarios) using SSL, embodied learning methods, and pretrained models trained on ImageNet."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Proposed approach of learning visual representation learning actively using just action aware objectives is interesting and novel\n2. The experiment section is thorough and nicely ablates all important questions required to demonstrated effectiveness of ALP.\n    1. Table 1 and 2 demonstrate effectiveness of ALP compared to other representation learning methods\n    2. Table 3 and 4 ablates importance of representation learning methods by fixing finetuning data and importance of different downstream data collection methods.\n3. Ablations in section 4.3.2 clearly demonstrates importance of combining inverse dynamics and RL objectives for training effective visual representations\n4. Proposed method outperforms baselines trained using static image datasets like ImageNet\n5. Paper is well written and experiments are well organized"
            },
            "weaknesses": {
                "value": "1. The proposed method underperforms compared to SimCLR ImageNet baseline in table 2 on train split and the results for depth estimation are not provided which contradicts the claims made by the authors.\n2. As shown in table 3 the results of finetuning different baselines using same finetuning dataset (collected using RND-ALP) the proposed method is only slightly better ~0.2-0.7% compared to ImageNet baseline. It\u2019d be nice if authors run this experiment with multiple seeds and share mean and variance to show the improvements are consistent across seeds.\n3. Table 4 compares different data collection methods for finetuning where RND-ALP outperforms existing methods. I\u2019d be interested to see a comparison with a baseline that collects data using random sampling strategy and using a heuristic based exploration agent like a frontier exploration agent which is maximizing the coverage in the environment. The frontier agent baseline is similar to ANS but builds a map using depth + camera info which leads to much better maps for exploration. This can be implement by leveraging codebase from [1]. Based on videos attached in supplementary it looks like RND/CRL/RND-ALP are learning simple navigation behavior which might restrict the diversity of the dataset used for training. It\u2019d be nice to compare the performance with such simple heuristic based data collection baselines\n4. In addition to ImageNet pretrained baselines there has been some recent works like MAE, VC-1[2], R3M[3], MVP[4]. Can authors please explain why the comparison with these representation learning baselines are missing? I'd like to see how well ALP does in comparison to these other pretrained representations\n5. For each of the tasks ObjDet, InstSeg, and DepthEst can authors also add comparison with the SOTA baselines for each task finetuned on data collected for finteuning? For example, it is important to compare how well a coco pretrained ObjDet model when finetuned on sim data for the 6 categories being used.\n\n[1] Chaplot, D.S., Gandhi, D., Gupta, A. and Salakhutdinov, R., 2020. Object Goal Navigation using Goal-Oriented Semantic Exploration. In Neural Information Processing Systems (NeurIPS-20)\n[2] A. Majumdar, K Yadav, S. Arnaud, Y. Jason Ma....... {Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence? NeurIPS 2023\n[3] Nair, S., Rajeswaran, A., Kumar, V., Finn, C., and Gupta, A. R3M: A Universal Visual Representation for Robot Manipulation. CoRL, 2022.\n[4] Radosavovic, I., Xiao, T., James, S., Abbeel, P., Malik, J., and Darrell, T. Real world robot learning with masked visual pre-training. In 6th Annual Conference on Robot Learning, 2022"
            },
            "questions": {
                "value": "1. The authors claim idea of learning a shared backbone for representation learning and reinforcement learning for exploration policy is novel but I have seen multiple papers that jointly learn visual backbone with RL for end-to-end tasks[1,2]. Can authors please clarify if they meant learning visual representations using RL+IDM only objectives as novelty here? If yes, can the line in the introduction section rephrased?\n\nMy major concerns are about multiple seeds for training for experiments in section 3, missing results for depth estimation in table 2 and clarification on why RND-ALP does poorly in table 2, missing comparison with other pretrained representations like MAE, VC-1, etc. In addition if authors can provide comparison with listed data collection strategies in weakness point 3 I\u2019d appreciate that. I am willing to update rating if authors address my concerns\n\n[1] Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra. Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames.\n[2]  Joel Ye, Dhruv Batra, Erik Wijmans, and Abhishek Das. Auxiliary tasks speed up learning point goal navigation. In Conference on Robot Learning, pp. 498\u2013516. PMLR, 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2130/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2130/Reviewer_e8e8"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2130/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698253906935,
        "cdate": 1698253906935,
        "tmdate": 1699636145926,
        "mdate": 1699636145926,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Cdva7RBD3T",
        "forum": "Po6lYYsrB4",
        "replyto": "Po6lYYsrB4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2130/Reviewer_DHKz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2130/Reviewer_DHKz"
        ],
        "content": {
            "summary": {
                "value": "THe paper proposes a two stage active learning approach to learn representations. \nIn the first state a agent uses intrinsic motivation as a way to explore the environment\nand learn representations from the scene. That representation learned is used to learn a \ndownstream task on the second part.\nThe reward is directly based on novelty.\nThe paper shows improved downstream tasks results when learning while interacting in the Habitat environment."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Interesting how they are able to use the police training gradient as the representation learning signal.\n* That seems to learn a better representation which enforces better exploration at the same time."
            },
            "weaknesses": {
                "value": "1 In terms of how solid the contribution it seems in my opinion slightly marginal when compared to the related work. The inclusion of police gradient into the representation learning helps the exploration and shows a better learned representation and bigger exploration in habitat. However, I am a bit skeptical with the extent of that as a contribution. The improvement seemed not that convincing and I am honestly worried on how much that can change given variations on the environment and initialization. It is likely that different starting conditions and different scene combinations might have drastically different results. \nIt is expected I think for a few different polices to be trained and be able to measure the variability of the representation learned."
            },
            "questions": {
                "value": "I am curious on potential difficulties found when training with the policy gradient loss ? Was there any sort of instability ? Since it tends to different dynamics changes during the training depending on how much reward is being obtained in a given moment."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2130/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2130/Reviewer_DHKz"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2130/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698845678533,
        "cdate": 1698845678533,
        "tmdate": 1700661912035,
        "mdate": 1700661912035,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mX83DtRfWw",
        "forum": "Po6lYYsrB4",
        "replyto": "Po6lYYsrB4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2130/Reviewer_mMb6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2130/Reviewer_mMb6"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a two stages framework designed to incorporate the actions of an embodied agent to learn representations from actively collected data. In the first stage, the embodied agent is actively exploring the environment driven by intrinsic motivation. The agent utilizes the action information implicitly using a shared backbone for policy and representation learning, and explicitly through inverse dynamics prediction. The second stage involves fine-tuning the learned representations for a wide range of perception downstream tasks (object detection, segmentation, and depth estimation) using random samples of the actively collected data. The authors conduct a series of experiments within simulated environments of 3D scans of real locations (the Gibson environments), demonstrating that their framework outperforms both baseline embodied learning and self-supervised learning methods, as well as pre-trained models from static datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written, offering clarity and ease of understanding.\n2. The framework's generality is a strong point, allowing for integration with various exploration techniques and adaptability to multiple downstream tasks.\n3. Empirical evidence is compelling, especially in demonstrating the benefits of leveraging the action information through using the shared backbone (implicit) and inverse dynamics model objective (explicit).\n4. Fine-tuning performance on diverse downstream perception tasks and robustness in out-of-distribution scenarios are effectively presented.\n5. The exhaustive experimental comparison with other baseline embodied learning and self-supervised learning methods, alongside pre-trained models, solidifies the framework's advantages."
            },
            "weaknesses": {
                "value": "1. The scope of experiments is only limited to simulated environments of 3D scans of real locations (the Gibson environments).\n2. The action space utilized in the experiments is relatively simple, including only discrete actions, no further experiments on more complex action space.\n3. The novelty of this work is somewhat limited, but the empirical findings are still worthwhile.\n    1. The two-stage framework is similar to the one proposed in CRL by Du et al. (2021), but using a different training objective.\n    2. The proposed training objective combining the policy gradient objective and the inverse dynamics model objective is similar to Ye et al. (2021). However, Ye et al. (2021) showed only for downstream navigation tasks.\n\n- Joel Ye, Dhruv Batra, Erik Wijmans, and Abhishek Das. Auxiliary tasks speed up learning point goal navigation. In Conference on Robot Learning, pp. 498\u2013516. PMLR, 2021.\n- Yilun Du, Chuang Gan, and Phillip Isola. Curious representation learning for embodied intelligence. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10408\u201310417, 2021."
            },
            "questions": {
                "value": "1. Concerning the first weakness, has there been any consideration for testing with datasets from Matterport and Replica, as utilized in Chaplot et al. (2020)?\n2. In relation to the second weakness, could you specify which type of embodied agent was employed in the experiments? Based on the Gibson environments, the embodied agent can be humanoid, ant, husky car, drone, minitaur, or Jackrabbot.\n3. Regarding the third weakness, could you provide further clarification, if possible?\n4. Regarding Table 11, which presents the variance across multiple runs, does this imply that the results were derived from a single execution of stage one, subsequently followed by various iterations of stage two? If so, could you clarify how the variance would be accounted for across multiple runs where each run includes running both the first and second stages consecutively?\n5. Is there a specific rationale behind the decision to run the embodied agent for 8M frames?\n\n- Devendra Singh Chaplot, Helen Jiang, Saurabh Gupta, and Abhinav Gupta. Semantic curiosity for active visual learning. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part VI 16, pp. 309\u2013326. Springer, 2020c."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2130/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2130/Reviewer_mMb6"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2130/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699470775751,
        "cdate": 1699470775751,
        "tmdate": 1699636145736,
        "mdate": 1699636145736,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GMBaGyiIbT",
        "forum": "Po6lYYsrB4",
        "replyto": "Po6lYYsrB4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2130/Reviewer_h74j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2130/Reviewer_h74j"
        ],
        "content": {
            "summary": {
                "value": "The authors demonstrate a system that actively learns visual representation and collects data to learn from. Learning signals are RL reward loss and an additional loss from Inverse Dynamics Prediction - which comes from predicting actions given a sequence of frames. It is shown that the system outperforms baseline methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The system convincingly outperforms the baseline methods. \n* The ablation experiment adds to the strength of the method"
            },
            "weaknesses": {
                "value": "* While the results seems strong, the work severely lacks novelty. The two points of novelty as reported are - i. Combined stages for collecting frames and learning from it. and ii. Sharing the backbone for policy and representation learning. In my opinion, these are not significant changes.\n* All the experiments are done on a single environment (Gibson)\n* Not clear why Inverse Dynamics Prediction is a good learning signal."
            },
            "questions": {
                "value": "1. Have you tried some more recent methods for self-sup training like MAE or VideoMAE?\n2. What is the rationale behind IDM? It seems without reason to me, since the RL reward should already be a learning signal to associate visual frames to action."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2130/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2130/Reviewer_h74j"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2130/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699490135378,
        "cdate": 1699490135378,
        "tmdate": 1699636145677,
        "mdate": 1699636145677,
        "license": "CC BY 4.0",
        "version": 2
    }
]