[
    {
        "id": "FPU0Rv9uH9",
        "forum": "xx0ITyHp3u",
        "replyto": "xx0ITyHp3u",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5239/Reviewer_hicK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5239/Reviewer_hicK"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new pruning technique named Sparse Model Soups, which combines the weight averaging methods from [1] with the pruning method described in [2]. They provide empirical evidence to support the idea that this straightforward aggregation enhances the performance of pruned models in image classification tasks.\n\nReferences\n\n[1] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning, 2022.\n\n[2] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems, 2015."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Clarity\n- The paper is well written, ensuring it is understandable for readers.\n- The suggested method is straightforward and effectively explained for easy understanding.\n\nOriginality and Significance\n- This paper introduces a novel pruning method that incorporates model averaging techniques based on the Model Soup methods.\n- They provide empirical evidence demonstrating that the proposed method offers improved performance when compared to baseline approaches."
            },
            "weaknesses": {
                "value": "Method\n- Although they can parallelize the training process, performing $m\\times k$ training epochs still imposes a substantial computational burden. And if the training cost becomes small, the overall performance gain significantly drops for the extreme sparsity cases.\n\nExperiments\n- It would be valuable to conduct empirical analyses to investigate why performance degradation occurs in regions of extreme sparsity.\n- Similarly, it would be beneficial to empirically analyze why SMS performs well in situations of early sparsity, where batch randomness can lead to divergence between averaged weights [3].\n- It would be advantageous to include an ablation study exploring different combinations of averaging coefficients where the $\\lambda_i$ values differ from each other.\n\nRecommend\n- I suggest that the authors consider adding an Ethics Statement and a Reproducibility Statement immediately following the main paper.\n\nReferences\n\n[3] Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning? Advances in neural information processing systems, 2020."
            },
            "questions": {
                "value": "See the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5239/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698487469155,
        "cdate": 1698487469155,
        "tmdate": 1699636522858,
        "mdate": 1699636522858,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SPBYAgdxf5",
        "forum": "xx0ITyHp3u",
        "replyto": "xx0ITyHp3u",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5239/Reviewer_fESa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5239/Reviewer_fESa"
        ],
        "content": {
            "summary": {
                "value": "This paper presents the Sparse Model Soups (SMS) framework, which applies the model soup algorithm to the neural network pruning procedure. Experimental results show that the model soup algorithm, a one of the most well-known weight-averaging methodologies that had demonstrated notable success in training dense neural networks, is also effective in iterative pruning procedures for training sparse neural networks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality and significance: The proposed SMS framework does not bring a substantial level of novelty, as one can consider it as an application of the existing model soup algorithm to an arbitrary iterative pruning process. However, its originality comes from the actual implementation in the domain of neural network pruning, even though the individual elements may already exist separately. Considering the recent success of weight averaging methods for dense neural networks, it is valuable to explore the extension of these techniques to sparse neural networks.\n\nQuality and clarity: Based on observations in the field of transfer learning, where fine-tuned models from the same pre-trained model tend to stay in the nearby region, the hypothesis that a similar phenomenon will occur when re-training the same pruned model is well-founded. The effectiveness of the proposed SMS framework is confirmed through a range of experiments conducted in the domains of image classification, semantic segmentation, and neural machine translation."
            },
            "weaknesses": {
                "value": "While the proposed SMS framework incorporates the model soup algorithm in the context of neural network pruning, it does not provide specific insights into the unique factors that are especially pertinent to sparse network training. The questions remain: What attributes of the model soup algorithm contribute to its effectiveness in the neural network pruning regime? Is it the same reason why weight-averaging methods have succeeded in conventional dense network training?"
            },
            "questions": {
                "value": "1. Alongside model soups, another prominent weight-averaging strategy is Stochastic Weight Averaging (SWA). The fact that SMS requires m training runs at each cycle makes SWA, which performs weight averaging within a single SGD trajectory, somewhat appealing. Are there any baseline results using SWA instead of model soups?\n\n2. I understand that the authors have opted to show exclusively the UniformSoup results for CityScapes and WMT16 since the GreedySoup algorithm utilizes validation data, which are the test split here. However, despite the potential fair comparison issue, presenting additional GreedySoup results might offer valuable insights into the benefits of selectively using soup ingredients at high sparsity levels."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5239/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5239/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5239/Reviewer_fESa"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5239/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698841788227,
        "cdate": 1698841788227,
        "tmdate": 1700690453915,
        "mdate": 1700690453915,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "77qU4jCT2j",
        "forum": "xx0ITyHp3u",
        "replyto": "xx0ITyHp3u",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5239/Reviewer_ja8N"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5239/Reviewer_ja8N"
        ],
        "content": {
            "summary": {
                "value": "The idea is to combine model soups from transfer learning with pruning. The proposal is : in prune-retrain paradigm (be it from scratch or pretrained), the training portion is replaced by model-soup. It improves the overall quality of the final pruned model as is validated extensively in experiments.\n\n[Update]\nIn light of the responses by authors, I am increasing my score to 6."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This being an empirical paper with simple idea proposal, the authors do an excellent job of evaluating their idea in terms of \n(1) extensive experiments on different domains \n(2) covering baselines that are natural competitors to the proposal."
            },
            "weaknesses": {
                "value": "1) I am not sure if this paper contributes new ideas or analysis. The proposal is to replace the training portion of prune-retrain with model soups. I do not have background on transfer learning, but as a general machine learning person, it is not surprising that it improves the accuracy of the model given the backdrop of model soups paper . Since in both the cases it holds that m copies of model start from the same initialization."
            },
            "questions": {
                "value": "1) How is IMP $m \\times$ implemented? Is the pruning rate for each prune step reduced? or is the training portions increased m$\\times$. The latter, I suspect, will not be very useful. \n2) Are there any challenges that are specific to using model soups for training portion of prune-retrain algorithm which differentiate it from applying model soups to finetuning of pretrained models? I felt that there were no new challenges here."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5239/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5239/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5239/Reviewer_ja8N"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5239/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698961774798,
        "cdate": 1698961774798,
        "tmdate": 1700694434391,
        "mdate": 1700694434391,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "m7R3ZgDUvX",
        "forum": "xx0ITyHp3u",
        "replyto": "xx0ITyHp3u",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5239/Reviewer_TvoY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5239/Reviewer_TvoY"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes merging sparse models by initiating each prune-retrain cycle with the averaged model from the previous phase. They show that averaging these models significantly enhances generalization and OOD performance over their individual counterparts. Overall, in summary, it is an extension of model soups for sparse models."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The experimental section of the paper + supplementary is rich illustrating noticeable gain.\n2. OOD experiments are new for sparse model soup showing SMS consistently improves over the baselines."
            },
            "weaknesses": {
                "value": "I have significant novelty concerns with the draft. \n\n1. The idea of sparse model averaging has been widely explored including the model soups (eg. https://arxiv.org/abs/2205.15322 https://arxiv.org/abs/2208.10842 https://arxiv.org/abs/2306.10460  etc). \n2. The authors have failed to detail how their method contrasts with existing sparse model soup papers in their related work section. I feel it is just an incremental work over the existing literature. The benefits of averaging the sparse masks is already known.\n3. Although I appreciate the extensive experiments by authors, I still feel the experiments are limited to small-scale datasets and models (maybe ViT scale or OPT models-based experiments will add value).\n4. I feel auxiliary benefits of model soups like OOD robustness, fairness, etc are good directions to explore."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5239/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5239/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5239/Reviewer_TvoY"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5239/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699143092321,
        "cdate": 1699143092321,
        "tmdate": 1700694294549,
        "mdate": 1700694294549,
        "license": "CC BY 4.0",
        "version": 2
    }
]