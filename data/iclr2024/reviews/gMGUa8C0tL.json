[
    {
        "id": "Yb8DadUOCm",
        "forum": "gMGUa8C0tL",
        "replyto": "gMGUa8C0tL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1059/Reviewer_N62r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1059/Reviewer_N62r"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a new task, Hot-Plugging Upgrades for visual foundation models. The aim is \nto seamlessly integrate superior-performing foundation models into downstream applications without adjusting the downstream modules. To realize this objective, The authors introduce a parameter-efficient and Task-agnostic Compatible Adapter, referred to as TaCA, which promotes compatibility across distinct foundation models while concurrently enhancing performance for the new models. The authors conduct extensive experimental validation of TaCA using different scales of models with up to one billion parameters on various tasks such as video-text retrieval, video recognition, and visual question answering."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors propose a hot-plugging upgrading module, which is interesting.  \n- The experiments have been conducted to illustrate the superiority of the proposed method."
            },
            "weaknesses": {
                "value": "- The authors should validate the flexibility of the proposed TaCA module. How about the performance when the TaCA is aligned with the other LLMs? \n- The qualitative analysis and visualization in experiments are missing."
            },
            "questions": {
                "value": "- The authors should validate the flexibility of the proposed TaCA module. How about the performance when the TaCA is aligned with the other LLMs? \n- The qualitative analysis and visualization in experiments are missing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1059/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1059/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1059/Reviewer_N62r"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1059/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698401680726,
        "cdate": 1698401680726,
        "tmdate": 1699636032251,
        "mdate": 1699636032251,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u63WFj3NUo",
        "forum": "gMGUa8C0tL",
        "replyto": "gMGUa8C0tL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1059/Reviewer_REM4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1059/Reviewer_REM4"
        ],
        "content": {
            "summary": {
                "value": "This paper introducing the task of hot-plugging upgrades for visual foundation models. And it proposes TaCA, which aims at effectively replacing visual foundation model without any downstream adaptation. Extensive experiments on video-related tasks indicate the effectiveness of TaCA."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper introduces the task of hot-plugging upgrades for visual foundation models, which aims at effectively replacing upstream visual fundation model.\n\n2. The experimental results prove TaCA can upgrade the visual foundation models without requiring the training data from downstream video-related tasks."
            },
            "weaknesses": {
                "value": "TaCA forces large-scale visual model to align with relatively small-scale visual model using adapter. It defeats the purpose of changing the visual model in my opinion. This approach restricts the transferability of the large-scale visual model, which may limit its potential benefits. Additionally, according to the results presented in Table 2, TaCA provides marginal improvements on downstream video classification tasks comparing to directly using large-scale visual model.\n\nThis paper evaluates the effectiveness of TaCA on video-related tasks. However, there are a number of works transfer CLIP to image-based tasks, e.g. image-text retrieval, image segmetation, and few-shot image classification. The absence of experiments on image-related tasks in this paper leaves a gap in evaluating TaCA's capability."
            },
            "questions": {
                "value": "What is the training overhead in terms of time?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1059/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698759925333,
        "cdate": 1698759925333,
        "tmdate": 1699636032152,
        "mdate": 1699636032152,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7fcRZtxCPY",
        "forum": "gMGUa8C0tL",
        "replyto": "gMGUa8C0tL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1059/Reviewer_dvkW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1059/Reviewer_dvkW"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Hot-Plugging Upgrades for visual foundation models. The aim is to seamlessly integrate superior-performing foundation models into downstream applications without adjusting the downstream modules. To realize this objective, this paper introduces a parameter-efficient and task-agnostic Compatible Adapter, referred to as TaCA, which promotes compatibility across distinct foundation models while concurrently enhancing performance for the new models. The paper is written well and easy to follow."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. This paper spearheads the exploration into the scenario of upgrading large-scale foundation models and introduces hot-plugging upgrades of visual foundation models in modular frameworks.\n2. This paper introduces a parameter-efficient upgrading strategy using a Task-agnostic Compatible Adapter (TaCA)\n3. The paper is written well and easy to follow."
            },
            "weaknesses": {
                "value": "1. The approach is incremental, and the techniques employed are all verified strategies. Specifically, it utilizes a combination of distillation methods and contrastive learning, forming a hybrid approach.\n\n2. Why not conduct experiments on more basic image classification and retrieval datasets (e.g., MSCOCO and imagenet)? If the effectiveness of this method can be verified on a more basic dataset, I am willing to increase my score\n\n3. In my opinion, TaCA, which utilizes an adapter to align a large-scale visual model with a smaller-scale visual model, undermines the purpose of changing the visual model. This approach hampers the transferability of the large-scale visual model, potentially limiting its advantages. Moreover, based on the results presented in Table 2, TaCA only shows marginal enhancements in downstream video classification tasks compared to directly employing a large-scale visual model.\n\n4. while this paper assesses the effectiveness of TaCA in video-related tasks, it overlooks numerous studies that apply CLIP to image-based tasks such as image-text retrieval, image segmentation, and few-shot image classification. The absence of experiments on image-related tasks in this paper creates a gap in evaluating TaCA.\n\n5. What would happen if Old VFM and New VFM were different (e.g., VIT-B to ResNet-50)? Can we distill and transfer knowledge between VFMs of different architectures to each other? For example, distilling knowledge from miniGPT or LLAMA (decoder only architecture) to CLIP?"
            },
            "questions": {
                "value": "1. The method is incremental, and the methods used are all validated schemes, which is actually distillation method combined with contrastive learning.\n2. Why not conduct experiments on more basic image classification and retrieval datasets"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1059/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1059/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1059/Reviewer_dvkW"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1059/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698775673793,
        "cdate": 1698775673793,
        "tmdate": 1699699431241,
        "mdate": 1699699431241,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ISIyRf9Suf",
        "forum": "gMGUa8C0tL",
        "replyto": "gMGUa8C0tL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1059/Reviewer_TD34"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1059/Reviewer_TD34"
        ],
        "content": {
            "summary": {
                "value": "- This manuscript introduces a new hot-plugging adapter, with which the task-specific model's foundation backbone can be replaced without re-training for both the backbone and the task-specific head. \n- The proposed method is tested on the CLIP series foundation models and evaluated on various vision-language tasks. The results validate the proposed method's effectiveness. Specifically, the performance of downstream tasks is improved when the backbone networks are replaced with more powerful ones."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of hot-plugging adapters is interesting. It could have a good impact on future research and other applications. \n2. The proposed method is technically sound. \n3. The manuscript is well-written and easy to follow. \n4. The comprehensive experiments validate the model's effectiveness on various tasks."
            },
            "weaknesses": {
                "value": "1. While the idea of hot-plugging adapters is intuitively sound at first glance, this paper lacks quantitative evidence to support this motivation. Specifically, the motivation of this paper is: *When replacing the visual backbones, fine-tuning the proposed adapter is better than fine-tuning the downstream task-specific head*. Therefore, a comparison between these two fine-tuning methods should be presented, and such a comparison should be in a fair enough setting because, in my opinion, it should be the most important experiment for the whole manuscript. Specifically, the author should compare 1) the trainable parameter amounts, 2) training FLOPs, 3) The data amounts needed for fine-tuning, and 4) the fine-tuning schedule of these two fine-tuning paradigms. In addition, the results in Table 1 show that the *TACA-BetterModel*'s performance is inferior to directly fine-tuning the task-specific head with a *BetterModel*, i.g., ViT-H, which also shows the necessity of such a comparison. \n2. The symmetric adapter is a very interesting point of this manuscript, as it outperforms the standard adapter. It would be better to include some experiments to study its effectiveness on other tasks, e.g., image generation or some NLP tasks. \n3. The manuscript should include more experiments to show its generalization ability to other non-CLIP models. For example, can the proposed method work on classic vision tasks, like detection or segmentation? \n4. I am also curious if the method can be applied to replacing *different* foundation models. For example, can we use it to replace a DINO ViT with an ImageNet-22k Supervised ViT? \n5. The proposed head-fixing strategy makes me think about the head-inheriting trick that is commonly used in knowledge distillation [a][b]. Therefore, discussing it in the related work section will increase the comprehensiveness of this work.\n\nTo conclude, I like the motivation of this work, and I also acknowledge that the provided experiments do validate its effectiveness to some extent. If the authors can address my concerns, especially the first point, I am happy to raise my rating.\n\n\n[a] Yang, Chenhongyi, et al. \"Prediction-guided distillation for dense object detection.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n[b] Yang, Zhendong, et al. \"Focal and global knowledge distillation for detectors.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1059/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1059/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1059/Reviewer_TD34"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1059/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801197555,
        "cdate": 1698801197555,
        "tmdate": 1699636032007,
        "mdate": 1699636032007,
        "license": "CC BY 4.0",
        "version": 2
    }
]