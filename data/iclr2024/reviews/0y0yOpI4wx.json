[
    {
        "id": "QGZdgzNxmt",
        "forum": "0y0yOpI4wx",
        "replyto": "0y0yOpI4wx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5884/Reviewer_j4eV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5884/Reviewer_j4eV"
        ],
        "content": {
            "summary": {
                "value": "The paper describes a \"General Purpose In Context Learning\" algorithm. It is basically transformed that is trained to predict the label of a specific sample given a context (the train dataset). The authors do experiments in some datasets such as MNIST and SVHN, and compare to some baselines to demonstrate that their method achieves some degree of generalization."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper adresses a very important problem in the community: learning-to-learn, in order to leverage information from previous tasks. \n- The authors invest some effort in demonstrating that the model generalizes."
            },
            "weaknesses": {
                "value": "- Lack of related work discussion: there is a tremendous amount of related work aiming to perform meta-learning or adapting transformers for in-context learning. However, the authors do not discuss any of them. For instance Meta-Transformer [1], OptFormer [2], or PFNs [3]. \n\n- The contribution is limited: the authors propose a very similar approach as to previous work [2][3], while only introducing a data augmentation step.\n\n- The data augmentation step is not well-founded. By performing random projections, it is likely to introduce noise. According to the authors, it allows to achieve generalization, but they do not perform any ablation to test this.\n\n- Experiments are poor in demonstrating the validity and superiority method. They do not use strong baselines or relevant datasets (they limit most of the experiments to MNIST).\n\n\n[1] Zhang, Y., Gong, K., Zhang, K., Li, H., Qiao, Y., Ouyang, W., & Yue, X. (2023). Meta-transformer: A unified framework for multimodal learning.\n\n[2] Li, X., Wu, K., Zhang, X., Wang, H., & Liu, J. (2022). Optformer: Beyond Transformer for Black-box Optimization.\n\n[3] M\u00fcller, S., Hollmann, N., Arango, S. P., Grabocka, J., & Hutter, F. (2021). Transformers can do bayesian inference."
            },
            "questions": {
                "value": "- Are the authors planing to release the code?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5884/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697896445651,
        "cdate": 1697896445651,
        "tmdate": 1699636623560,
        "mdate": 1699636623560,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MJ8azxTGvh",
        "forum": "0y0yOpI4wx",
        "replyto": "0y0yOpI4wx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5884/Reviewer_zaZF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5884/Reviewer_zaZF"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates how transformer-based models can meta-learn general-purpose in-context learning algorithms (that take in training data and produce test-set predictions without any explicit definition of an inference model, training loss, or optimization algorithm) with minimal inductive bias. The authors propose using black-box sequence models like LSTMs and Transformers as meta-learners, since they can learn concepts from demonstrations without an explicit definition of the learning algorithm. \n\nAuthors go to great lengths to introduce and classify different in-context learning algorithms and introduce a Transformer-based model (GPICL) and associated meta-training task distribution. They discuss how they generate different tasks for the meta algorithm to learn by taking existing supervised datasets and randomly projecting the inputs and permuting classes to generate many datasets from a small seed. Based on that they define the General-Purpose In-Context Learner (GPICL) - a transformer model that is fed sequences of input-output data and asked to predict next output based on previous input. During training of GPICL, each iteration uses Adam to optimize the loss on a random batch of training data sampled from a random task. \nAuthors run many ablation studies analysing meta-learning with transformers. Among others, they show results that indicate the transformer starts to learn rather than memorize with enough memory used for training, and that simple data augmentations during meta-training lead to the emergence of learning-to-learn behaviors."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Presents a simple baseline model (GPICL) for meta-learning general purpose learners with minimal inductive bias. Shows competitive performance compared to models with stronger inductive biases.\n\n- Provides interesting insights into the transitions from memorization to task identification to general learning as model size and number of tasks increase during meta-training. Identifies the accessible state/memory size as a key bottleneck for meta-learning capabilities, rather than just model parameter count.\n- Identifies the accessible state/memory size as a key bottleneck for meta-learning capabilities, rather than just model parameter count.\n- Well-written and easy to follow presentation of methods and results."
            },
            "weaknesses": {
                "value": "- Authors use CIFAR10, MNIST, FashionMNIST and SVHN as their datasets. Those are rather simple datasets and it would be good to see if the findings generalizes well to harder and larger datasets. Most importantly it would be interesting to show that the method is performing well due to its inherent ability to learn rather than the datasets being easy.\n- The authors do not make it explicitly clear what elements of their new setup is their contribution and which is already present in other papers. I understand the random projection strategy and the coding for the transformer are the main modeling novelties while the ablation studies have significant impact on the understanding of the field. Still, the presentation would be improved with a clear contributions section that highlights this."
            },
            "questions": {
                "value": "Authors claim that the performance of the model improves not with the number of parameters but with the state size. I am wondering if this is the case because the datasets considered such as MNIST are simple enough that having more parameters is no longer helpful rather than showing a general trend."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5884/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5884/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5884/Reviewer_zaZF"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5884/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698746778838,
        "cdate": 1698746778838,
        "tmdate": 1699636623459,
        "mdate": 1699636623459,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "P6vkWTs2LT",
        "forum": "0y0yOpI4wx",
        "replyto": "0y0yOpI4wx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5884/Reviewer_9s7H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5884/Reviewer_9s7H"
        ],
        "content": {
            "summary": {
                "value": "This submission is a resubmission from another machine learning venue, and the paper has undergone 0 modifications since its previous rejection. While it is permissible to resubmit the work, in this case, the authors have not addressed the points raised in the earlier review process. I believe these points are crucial for the paper's improvement, and it would be counterproductive to overlook the feedback provided in the previous reviews.\n\nIf the Area Chair still deems it appropriate to consider this submission, I recommend using all reviews so far."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "N/A"
            },
            "weaknesses": {
                "value": "N/A"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This submission is a resubmission from another machine learning venue, and the paper has undergone 0 modifications since its previous rejection. While it is permissible to resubmit the work, in this case, the authors have not addressed the points raised in the earlier review process. I believe these points are crucial for the paper's improvement, and it would be counterproductive to overlook the feedback provided in the previous reviews.\n\nIf the Area Chair still deems it appropriate to consider this submission, I recommend using all reviews so far."
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5884/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5884/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5884/Reviewer_9s7H"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5884/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829186938,
        "cdate": 1698829186938,
        "tmdate": 1699636623348,
        "mdate": 1699636623348,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IuHfa38wzk",
        "forum": "0y0yOpI4wx",
        "replyto": "0y0yOpI4wx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5884/Reviewer_ohZB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5884/Reviewer_ohZB"
        ],
        "content": {
            "summary": {
                "value": "This paper demonstrated that transformers can be meta-trained to act as general-purpose in-context learners. This paper also characterizes transitions between algorithms that generalize, algorithms that memorize, and algorithms that fail to meta-train at all, induced by changes in model size, number of tasks, and meta-optimization. This paper proposes practical interventions such as biasing the training distribution to improve the meta-training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper performed experiments on image classification datasets to demonstrate that transformers can be meta-trained to perform in-context learning. \n2. Figure 2 gives convincing evidence of a transition from memorization and generalization induced by model capacity and sample size. \n3. This paper provides practical interventions to improve meta-training."
            },
            "weaknesses": {
                "value": "1. The writing is not completely clear. For example, \"general-purpose in-context learning\" is a vague term without a rigorous mathematical definition. This makes the paper a bit hard to read. \n2. The memory or state in Section 4.2 is quite heuristic without a concrete math definition. Beyond LSTM and transformers, it is not clear how the state is defined. The insight that \"Large state is more crucial than parameter count\" is thus not fully grounded. \n\nThe contributions of this paper were significant one year ago when this paper first came out. However, the main message that \"one can meta-train a transformer to perform ICL\" is well-known nowadays. I am not sure how to evaluate this paper given this situation."
            },
            "questions": {
                "value": "Could the authors address the comments on the weakness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5884/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698951335312,
        "cdate": 1698951335312,
        "tmdate": 1699636623244,
        "mdate": 1699636623244,
        "license": "CC BY 4.0",
        "version": 2
    }
]