[
    {
        "id": "72Dq5FL8Fc",
        "forum": "R7rZUSGOPD",
        "replyto": "R7rZUSGOPD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7222/Reviewer_umQ7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7222/Reviewer_umQ7"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel exploration framework for sparse reward environments, called Planner-Actor-Evaluator (PAE), which teaches RL agents to learn to absorb external knowledge. In particular, PAE adopts a state-knowledge alignment mechanism to enable a Planner to access external knowledge sources and retrieve suitable knowledge that aligns with the current environmental state. Then the actor leverages both of the state information and the provided external knowledge for joint reasoning. Additionally, an Evaluator supplies intrinsic rewards for both the planner and the actor. Experimental results conducted in the BabyAI environments demonstrate the effectiveness of this innovative approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper studies an interesting topic of using natural language as external knowledge for RL agents to improve exploration in reward sparse tasks.\n\n- The writing is clear and easy to follow.\n\n- PAE showed strong performance in the experiments."
            },
            "weaknesses": {
                "value": "- The PAE model comprises three primary components, each with distinct hyper-parameters. Managing these varied parameters can be challenging in real applications.\n\n- Since it depends on special environments and predefined knowledge template, PAE is not generalizable to a broader range of tasks."
            },
            "questions": {
                "value": "- What is the difference in wall-clock running time between PAE and the other baseline models?\n\n- Would it be possible to extract latent knowledge from a collected dataset in a pre-training stage and replace the template-based knowledge in PAE?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7222/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7222/Reviewer_umQ7",
                    "ICLR.cc/2024/Conference/Submission7222/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7222/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698545366439,
        "cdate": 1698545366439,
        "tmdate": 1700592285971,
        "mdate": 1700592285971,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "htInD9GyeQ",
        "forum": "R7rZUSGOPD",
        "replyto": "R7rZUSGOPD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7222/Reviewer_rZvG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7222/Reviewer_rZvG"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces PAE: Planner-Actor-Evaluator, a novel framework for teaching agents to learn to absorb external knowledge. PAE integrates the Planner\u2019s knowledge-state alignment mechanism, the Actor\u2019s mutual information skill control, and the Evaluator\u2019s adaptive intrinsic exploration reward. PAE aims to achieve cross-modal information fusion for the actor\u2019s decision."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.This work teaches agents to leverage external knowledge and approach optimal solutions faster in sparse reward environments.\n2.This work introduces LLM ideas to RL problems, which is very interesting and novel.\n3.It can be applied to the current actor-critic approaches."
            },
            "weaknesses": {
                "value": "1.As authors pointed, the current knowledge relies on the specific template, which lacks naturalness and does not fully use the huge ability of the LLM. It is an obvious weakness for this paper.\n2.The current knowledge needs human labeling following the specific template, which is of labor consumption.\n3.I suggest the definition and description of the alignment loss can be placed in the main text, which I finally find in the supplementary."
            },
            "questions": {
                "value": "1.The authors use the cross-attention as the alignment, how about the current alignment work like CLIP and ImageBind? Do the authors consider to use this alignment?\n2.In Figure 4, I think there is unfair to compare with most of these baselines, since they do not have or use the language guidance. The L-AMIGo is the only one considering language. Are there any other similar work that the authors can compare? The current comparison is not very strong to highlight your work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7222/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698665254376,
        "cdate": 1698665254376,
        "tmdate": 1699636859405,
        "mdate": 1699636859405,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bxBkGzu3dZ",
        "forum": "R7rZUSGOPD",
        "replyto": "R7rZUSGOPD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7222/Reviewer_2pSJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7222/Reviewer_2pSJ"
        ],
        "content": {
            "summary": {
                "value": "The paper presents PAE (Planner-Actor-Evaluator), a novel framework for incorporating external knowledge in reinforcement learning (RL), with the aim of enabling efficient exploration and skill acquisition in sparse reward environments. The framework is built upon three main components: the Planner, the Actor, and the Evaluator. The Planner aligns knowledge in natural language with states in the environment, the Actor integrates external knowledge with internal strategies, and the Evaluator computes intrinsic rewards to guide independent updates of the Planner and Actor. Experiments on six BabyAI environments demonstrated that PAE significantly outperforms existing methods in terms of exploration efficiency and interpretability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Originality: The paper proposes a new framework addressing the issue of knowledge acquisition, integration, and updating in RL. PAE stands out with its ability to use natural language as a knowledge source and to progressively adapt the difficulty of acquired knowledge.\n- Quality: The proposed PAE framework is designed with clear objectives, tackling key challenges that arise when training agents to absorb external knowledge and improve their capabilities.\n- Clarity & Significance: The experiments conducted in multiple challenging environments provide strong evidence of the framework's effectiveness, its generalization across tasks, and superiority compared to existing methods. The paper is well-written and provides meaningful insights into the relationship between knowledge and environment states, making PAE a valuable contribution to the RL community."
            },
            "weaknesses": {
                "value": "I believe that the main limitation of this paper is that the PAE algorithm relies on a pre-defined knowledge set, although the authors have proactively mentioned this in the limitations section. There are already quite a few LLM-based agents that do not require pre-defined external knowledge and are able to complete various tasks well by relying solely on the LLM's inherent commonsense knowledge [1] or by actively gathering, summarizing, or reflecting on dynamic knowledge throughout the task [2]. However, the experimental section does not compare these methods. Although the authors mention that these methods may have higher training/inference costs, I think that given the current popularity of LLM-based agents, this comparison is essential, and higher training/inference costs are not a compelling reason to avoid it. Otherwise, it is difficult to justify the necessity of pre-defined external knowledge and the rationality of the basic settings of this paper.\n\n---\n[1] Carta, Thomas, et al. \"Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning.\" ICML 2023.\n\n[2] Chen, Liting, et al. \"Introspective Tips: Large Language Model for In-Context Decision Making.\" arXiv preprint arXiv:2305.11598 (2023)."
            },
            "questions": {
                "value": "The main questions have been mentioned in the `Weaknesses` section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7222/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7222/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7222/Reviewer_2pSJ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7222/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744518771,
        "cdate": 1698744518771,
        "tmdate": 1700538245138,
        "mdate": 1700538245138,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VedTr4mqkJ",
        "forum": "R7rZUSGOPD",
        "replyto": "R7rZUSGOPD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7222/Reviewer_qGp8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7222/Reviewer_qGp8"
        ],
        "content": {
            "summary": {
                "value": "They propose a novel framework for sparse-reward, long-horizon RL tasks consisting of Planner, Actor and Evaluator. Planner provides guidance/knowledge to the Actor in natural language. Actor is incentivized to explore states aligned with that guidance. Evaluator evaluates if Actor is able to accomplish a sub-task given Planner\u2019s guidance and provides intrinsic rewards to both Planner and Actor. The Actor is given a +1 on completing a sub-task and the Planner is given a reward of +alpha if the Actor completed the sub-task under an adaptive threshold number of steps. Otherwise, Planner gets a -beta. The core idea is that the Planner must generate increasingly harder knowledge/tasks for the Actor based on Actor\u2019s current abilities. They use BabyAI for experiments. Both Planner and Actor also receive external environmental rewards."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThis paper is attempting to bring natural language instruction to curriculum-driven RL by adding in Planner to a modified version of Actor Critic\n2.\tTheir encoders use cross-attention to align instruction knowledge with current environment state embedding to encourage selection of the most relevant knowledge and exploration of relevant states by actor. The actor is also rewarded based on the mutual information between knowledge and states.\n3.\tThe paper is well structured even though the methodology is complex.\n4.\tThe reward structure is intuitive to understand and clearly defined for each component.\n5.\tThey compare against IMPALA, RND, ICM, Amigo, L-Amigo and have consistent improvement as well as good training stability across 6 BabyAI tasks\n6.\tThe ablation studies are thoroughly done and establish the value added by the Planner.\n7.\tThe Planner generated curriculum serves as a window into agent behavior"
            },
            "weaknesses": {
                "value": "1.\tBabyAI action space is quite limited (6 actions) and simplistic, so are the tasks since they are 2D grid world tasks.\n2.\tAs pointed out by authors, current experiments have a knowledge oracle and a very specific template for knowledge peices. Also, planners can observe the whole world, which may be unrealistic for many real-world set up.\n3.\tCurrent experiments require well-defined sub-tasks as well as detection of sub-task completion. Baby AI tasks are about navigating a grid in a particular way and due to task and world structure, it is easy to detect sub-task completion.\n4.\tThis method would need sub-task and knowledge alignment so as to detect if a particular piece of knowledge lead to completion of a particular sub-task. I don\u2019t see how it can work any other way. This is a very hard thing to obtain.\n5.\tThe evaluator currently is highly customized to BabyAI tasks."
            },
            "questions": {
                "value": "1. I would like to hear authors thoughts on weaknesses 3, 4 and 5 to be sure of my understanding of the paper's contributions\n2. How did the authors reach the design for evaluator? What challenges if any, do the authors foresee in making the evaluator more general-purpose?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7222/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7222/Reviewer_qGp8",
                    "ICLR.cc/2024/Conference/Submission7222/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7222/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698777586011,
        "cdate": 1698777586011,
        "tmdate": 1700539221981,
        "mdate": 1700539221981,
        "license": "CC BY 4.0",
        "version": 2
    }
]