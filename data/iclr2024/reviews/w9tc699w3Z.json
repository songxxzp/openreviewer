[
    {
        "id": "IACa3WOShG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6524/Reviewer_J5AT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6524/Reviewer_J5AT"
        ],
        "forum": "w9tc699w3Z",
        "replyto": "w9tc699w3Z",
        "content": {
            "summary": {
                "value": "The authors used internet images to train image encoders. The proposed strategy can efficiently train visual-language models and does not introduce additional annotations. The authors verified the effectiveness of the proposed strategy on multiple subtasks such as zero-shot, open-vocabulary image classification, retrieval, segmentation and visual question\nanswering for satellite images."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The motivation is sound, and the proposed method is theoretically feasible. Experimental results show that the framework achieves good performance."
            },
            "weaknesses": {
                "value": "1. The proposed method is not innovative enough. I'm not an expert in this field, so I'm not sure about it.\n2. The images in the manuscript are of such low resolution that they are difficult to read."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6524/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6524/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6524/Reviewer_J5AT"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6524/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697291387073,
        "cdate": 1697291387073,
        "tmdate": 1700534462115,
        "mdate": 1700534462115,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VV0HxauwZy",
        "forum": "w9tc699w3Z",
        "replyto": "w9tc699w3Z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6524/Reviewer_U5BT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6524/Reviewer_U5BT"
        ],
        "content": {
            "summary": {
                "value": "This paper develops remote sensing vision-language foundation models, named GRAFT, by aligning the embedding of satellite images to internet images in the CLIP space. The obtained models perform well in zero-shot classification, retrieval, segmentation and VQA tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors notice the 1-to-N relationship between satellite image and ground image, and develop image-level and pixel-level models, respectively, by designing two kinds of contrastive losses.\n2. The authors collected nearly ten million images for pretraining the model."
            },
            "weaknesses": {
                "value": "1. In Section 1, the authors claim existing remote sensing image-text datasets have only 10k samples. However, as far as I know, the volume of RS5M has reached 5 million.\n2. According to Section 3.4, the model has been initialized by the CLIP image encoder. So, is it that the high performances in downstream tasks are from CLIP pretraining? It is suggested to train the satellite image encoder $f_S$ from scratch to show the real ability of the proposed method.\n3. The authors claim that they developed a VLM without textual annotations. Alignment is a good idea (and probably a cheap solution) to obtain a remote sensing VLM by leveraging existing ones from other domains. However, experiments show that the proposed model is still served as a visual encoder in existing VLM models. GRAFT cannot independently finish vision-language tasks, especially for text generation (VQA, captioning, etc.). Indeed, since it is only aligned with the CLIP visual encoder, its cross-modality ability is limited by the pre-trained clip text encoder. A discussion of its limitations being applied to other VLMs should be provided. \n4. While the authors have indeed discussed the domain gap between natural images and remote sensing images, discrepancies in how objects are represented and described in these two image types persist. For instance, the size of similar objects (e.g., cars) in natural images may undergo changes due to projection transformation, whereas it remains consistent in remote sensing images. Additionally, the orientation and object placement differ between these two image categories (front view versus top view). Consequently, given that the text descriptions used to train the CLIP model are biased toward natural images, the challenge lies in bridging the gap between describing natural images and remote sensing images, which may limit the usage of the proposed method. \n\n4. Some relevant works are expected to be reviewed, which can offer readers a comprehensive overview of the current advancements in this field:\nSatViT: Pretraining Transformers for Earth Observation, IEEE GRSL, 2022.\nAn Empirical Study of Remote Sensing Pretraining, TGRS, 2023.\nRS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model, arXiv preprint arXiv:2306.11300 (2023).\nAdvancing Plain Vision Transformer Toward Remote Sensing Foundation Model, TGRS, 2023.\nRSGPT: A Remote Sensing Vision Language Model and Benchmark, arXiv preprint arXiv:2307.15266 (2023)."
            },
            "questions": {
                "value": "See the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6524/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6524/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6524/Reviewer_U5BT"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6524/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698631525621,
        "cdate": 1698631525621,
        "tmdate": 1701060122166,
        "mdate": 1701060122166,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NZjrBXPBZU",
        "forum": "w9tc699w3Z",
        "replyto": "w9tc699w3Z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6524/Reviewer_hnot"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6524/Reviewer_hnot"
        ],
        "content": {
            "summary": {
                "value": "This paper is addressing a significant problem, i.e., how to obtain a VLM for remote sensing images. The difficulty lies in that training a CLIP-like model needs a large amount of image-text pairs; however, for remote sensing images (e.g., satellite images), there are few such image-text pairs, even no. This problem blocks the open-vocabulary downstream task in remote sensing.\nThis paper gives a smart solution, which adopts ground image as the intermediate representation. Aligning satellite image representation with ground image representation replaces the directly aligning satellite image representation and text representation when given a good alignment between ground image representation and text representation.\nThis idea is very novel and experimental results are promising."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The idea of aligning satellite image representation and ground image representation is novel and is promising to train a genuine remote sensing CLIP model as a foundation.\n- Experiments are conducted on many downstream tasks and they show superior performances."
            },
            "weaknesses": {
                "value": "The keys to this idea are a well-aligned ground image-language model and the ground-satellite alignment.\nThe first condition seems CLIP can be responsible for it. The main concern is the ground-satellite alignment.\nI am satisfied with everything except satellite-\"alt-text\" alignment.\nI cannot understand why directly aligning satellite-text representations is much worse than the way using intermediate representation (ground). Does your \"alt-text information\" include more errors? thus, this textual supervision is biased causing worse results.\nI am very interested in this part, however, authors has mentioned it in one stroke."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6524/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6524/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6524/Reviewer_hnot"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6524/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809400373,
        "cdate": 1698809400373,
        "tmdate": 1699636733310,
        "mdate": 1699636733310,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1vbv5tuDL4",
        "forum": "w9tc699w3Z",
        "replyto": "w9tc699w3Z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6524/Reviewer_FsSQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6524/Reviewer_FsSQ"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new method to train vision-language models for remote-sensing images without using any textual annotations. Authors use GGB bands of 2 types of satellite imagery: Sentinel-2 (hight resolution, 10m per pixel) and NIAP dataset (1 m per pixel). The model is trained on geotagged annotated photos, obtained from Flickr to train the models. The model itself consists of 2 parts: image-level and pixel level components. The result represent a large vison-language model with few example applications, such as annotation of the imagery and question-answering capability."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper represents an excellent example of the practically useful application, with a large curated dataset and e few useful applications, such as annotation and question-answering capabilities that will contribute well to the VLM literature."
            },
            "weaknesses": {
                "value": "Unfortunately neither the code not the dataset are available for review and the soundness of the paper cannot be well estimated.\n\nOne significant disadvantage of the paper is the fact that it is trained only on the RGB bands of the satellite imagery, significantly reducing applications in agriculture, earth and biological sciences."
            },
            "questions": {
                "value": "1) It's unclear whether there is a connection b/w ground and satellite imagery by timestamp (to monitor things in \"almost real time\" or to account for seasonality)\n2) paragraph 3.2: you mention accurate geo-location. How do you measure/verify that the geolocation is accurate?\n3) how do you account for typical errors in geotagging (~8m)\n4) sec.3.4: do you check how old are the features on open street maps?\n5) Ethics considerations: what measures can you take to avoid revealing personal details about the households/farms/properties of the individuals that can be observed with 1m satellite imagery?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "1m resolution imagery might be potentially too revealing for private property, especially combined with LLMs and CLIP type models. Authors mention that their models should be used responsibly, but there are no measures made by paper authors themselves"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6524/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699111334079,
        "cdate": 1699111334079,
        "tmdate": 1699636733209,
        "mdate": 1699636733209,
        "license": "CC BY 4.0",
        "version": 2
    }
]