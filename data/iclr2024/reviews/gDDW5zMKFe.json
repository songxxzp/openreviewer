[
    {
        "id": "7csHNkYkTz",
        "forum": "gDDW5zMKFe",
        "replyto": "gDDW5zMKFe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8200/Reviewer_Rag8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8200/Reviewer_Rag8"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method called FIne-grained In-Training Embedding Dimension optimization (FIITED) to address the memory-intensive nature of modern Deep Learning Recommender Models (DLRM). These models have large embedding tables that demand a significant amount of memory during both training and inference. The core observation driving the research is that not all embedding vectors carry equal importance. Thus, FIITED dynamically adjusts the dimension of individual embedding vectors during training. It allocates longer dimensions to more critical embeddings and reduces dimensions for less significant ones, making optimal use of available memory. This optimization is based on the computed importance scores of embeddings, considering factors such as access frequency and gradient norms."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1.\tThe authors have shown an earnest effort to address the challenge of model compression, which remains a relevant topic in the field. Their intent to compress embedding dimensions can be useful in certain edge cases or specific scenarios.\n2.\tDespite similarities with existing methods, the introduction of a \"Virtually Hashed Physically Indexed (VHPI) embedding table design\" suggests the authors' endeavor to refine existing approaches."
            },
            "weaknesses": {
                "value": "Novelty and Contributions: The paper's novelty appears to be incremental and may not align with the standards of a top-tier conference. The area of embedding dimension search (EDS) has been previously explored. While the paper distinguishes itself by emphasizing feature value-level search, other works like AutoEmb [1], ESAPN [2], and RULE [3] have already pursued this direction. For the claimed novelty in searching during training, in-training searches using NAS-based optimization have been explored by DNIS [4], AutoEmb [1], and AutoDim [5]. The proposed Virtually Hashed Physically Indexed (VHPI) embedding table design seems reminiscent of what's presented in AdaEmbed. The only noticeable difference is the application of varying selection thresholds for different column groups in an embedding table.\n\nClarity and Motivation: The paper's motivation needs to be more explicitly articulated. The primary concern is the rationale behind the compression of embedding dimensions for standard recommender systems. When general recommendation models typically deploy on powerful cloud servers, the necessity to compromise model accuracy for size reduction seems unclear. The utility of adaptive embedding size search would be more apparent if it could enhance model performance by determining appropriate embedding sizes for different feature values. As it stands, while there's a notable compression rate, there's also a concomitant performance drop. Given the abundant memory and computational capacity of cloud servers, it's debatable whether such compression is essential or practical.\n\nQuality of Figures: It's observed that the jpeg figure in the paper is somewhat blurred. It's recommended that authors use clearer formats like PDF to enhance the visual clarity of the figure.\n\nBaseline Comparisons: The selection of baselines for comparison appears limited. Other potential methods such as DNIS [4], PEP [6], AMTL [7], and AutoDim [5] could have been considered or at least discussed in terms of baseline selection. A clear criterion for selection would lend more weight to the experimental results. Merely mentioning ESAPN as the state-of-the-art may not suffice.\n\nTechnical Details: There are missing technical details that might be crucial for a thorough understanding and replication of the method. The division of the embedding table into m*k embedding blocks is mentioned, but the determination of 'm' and the grouping strategy for different feature values are not elaborated upon. A comprehensive study or discussion on the hyperparameters, especially concerning 'm' and 'k', would be valuable. As of now, only Figure 6 sheds light on the varying training times with 'k'.\n\nReferences:\n[1] Manas R. Joglekar, Cong Li, Mei Chen, Taibai Xu, Xiaoming Wang, Jay K. Adams, Pranav Khaitan, Jiahui Liu, and Quoc V. Le. 2020. Neural Input Search for Large Scale Recommendation Models. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (Virtual Event, CA, USA) (KDD \u201920)\n\n[2] Haochen Liu, Xiangyu Zhao, Chong Wang, Xiaobing Liu, and Jiliang Tang. 2020. Automated Embedding Size Search in Deep Recommender Systems. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR \u201920).\n\n[3] Tong Chen, Hongzhi Yin, Yujia Zheng, Zi Huang, Yang Wang, and Meng Wang. 2021. Learning Elastic Embeddings for Customizing On-Device Recommenders. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (Virtual Event, Singapore) (KDD \u201921)\n\n[4] Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Differentiable neural input search for recommender systems. arXiv preprint arXiv:2006.04466 (2020).\n\n[5] Xiangyu Zhao, Haochen Liu, Hui Liu, Jiliang Tang, Weiwei Guo, Jun Shi, Sida Wang, Huiji Gao, and Bo Long. 2021. AutoDim: Field-Aware Embedding Dimension Searchin Recommender Systems. In Proceedings of the Web Conference 2021 (Ljubljana, Slovenia) (WWW \u201921).\n\n[6] Siyi Liu, Chen Gao, Yihong Chen, Depeng Jin, and Yong Li. 2021. Learnable Embedding Sizes for Recommender Systems. arXiv preprint arXiv:2101.07577 (2021).\n\n[7] Bencheng Yan, Pengjie Wang, Kai Zhang, Wei Lin, Kuang-Chih Lee, Jian Xu, and Bo Zheng. 2021. Learning Effective and Efficient Embedding via an Adaptively-Masked Twins-based Layer. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management (CIKM '21)."
            },
            "questions": {
                "value": "1.\tCould you provide more insights into how the embedding table is divided into m*k blocks? Specifically, how is the value of 'm' determined, and what strategies are employed for grouping different feature values?\n2.\tOn what grounds were the baselines selected for comparison? Why were certain other potential methods excluded from the baseline?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8200/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698662288721,
        "cdate": 1698662288721,
        "tmdate": 1699637017395,
        "mdate": 1699637017395,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lav2dbm4hN",
        "forum": "gDDW5zMKFe",
        "replyto": "gDDW5zMKFe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8200/Reviewer_pdDD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8200/Reviewer_pdDD"
        ],
        "content": {
            "summary": {
                "value": "The research problem addressed in this paper is the challenge of optimizing embedding dimensions during training for recommender systems. The authors note that while embedding pruning methods have been proposed to address this issue, they often result in suboptimal performance due to the loss of information. To overcome this limitation, the authors propose a novel approach called Fine-grained In-Training Embedding Dimension Tuning (FIITED), which adjusts the dimension of each individual embedding vector continuously during training. This approach is designed to achieve significant memory savings without sacrificing model quality.\n\nThe authors note that previous work has explored various methods for embedding pruning, including manual pruning, adaptive pruning, and dynamic pruning. However, these methods have limitations, such as requiring manual tuning, being computationally expensive, or resulting in suboptimal performance. The authors argue that their proposed approach overcomes these limitations by allowing for fine-grained tuning of embedding dimensions during training.\n\nThe authors' contributions in this study include proposing the FIITED approach, which allows for fine-grained in-training embedding dimension tuning, and demonstrating its effectiveness through experiments on industry models and public click-through rate prediction datasets. The authors also propose a new Virtually Hashed Physically Indexed (VHPI) embedding table design adapted from AdaEmbed to address the memory fragmentation issue that arises from fine-grained pruning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors validate the efficacy of FIITED through rigorous experimentation on both proprietary industry models and publicly available click-through rate prediction datasets. Specifically, they demonstrate that FIITED can reduce embedding size during training by more than 65% while maintaining the quality of the trained model. Compared to a state-of-the-art in-training embedding pruning method, AdaEmbed, FIITED is able to achieve higher pruning ratios without affecting model quality. On public datasets, FIITED is able to prune up to 93.75%-99.75% embeddings on three click-through datasets and 77.5%-81.25% on one classification dataset, without significant accuracy loss. \n\n2. In addition to the FIITED methodology, the authors introduce an innovative embedding table design, termed Virtually Hashed Physically Indexed (VHPI), which is a modification of AdaEmbed. This design effectively mitigates the issue of memory fragmentation, a challenge commonly associated with fine-grained pruning. The authors demonstrate that VHPI can support training memory saving in FIITED's chunk-based dimension pruning."
            },
            "weaknesses": {
                "value": "1. The paper could benefit from a more detailed comparison with existing methods for embedding dimension optimization. While the authors briefly mention existing approaches such as EDS, they do not provide a comprehensive comparison with these methods. A more detailed comparison could help readers better understand the strengths and weaknesses of FIITED in relation to existing approaches. \n\n2. The paper could provide more details on the implementation of FIITED and VHPI. Specifically, the authors could provide more information on the computational complexity of their approach and how it scales with the size of the dataset and the number of embedding dimensions. Additionally, the authors could provide more details on the implementation of VHPI, including how it mitigates the issue of memory fragmentation and how it compares to other embedding table designs. \n\n3. The paper could benefit from more extensive experimentation on a wider range of datasets. While the authors demonstrate the efficacy of FIITED on both proprietary industry models and publicly available click-through rate prediction datasets, it would be useful to see how the approach performs on other types of datasets. Additionally, the authors could provide more details on the hyperparameters used in their experiments, such as the learning rate and batch size, and how they were selected."
            },
            "questions": {
                "value": "1. Can you provide more details on the computational complexity of your approach and how it scales with the size of the dataset and the number of embedding dimensions? \n\n2. In Section 3.2.1, you mention that individual pruning ratios for different chunks may be bigger or smaller than the global pruning ratio, and may change over time, depending on chunk utility values computed at runtime. Can you provide more details on how these utility values are computed and how they are used to determine the pruning ratios for each chunk? \n\n3. In Section 3.3, you introduce the Virtually Hashed Physically Indexed (VHPI) embedding table design, which is a modification of AdaEmbed. Can you provide more details on how VHPI mitigates the issue of memory fragmentation and how it compares to other embedding table designs? \n\n4. In Section 4.2, you evaluate the performance of FIITED on four datasets, including three click-through rate prediction datasets and one classification dataset. Can you provide more details on the hyperparameters used in your experiments, such as the learning rate and batch size, and how they were selected? \n\n5. In Section 4.3, you compare the performance of FIITED with that of AdaEmbed, a state-of-the-art in-training embedding pruning method. Can you provide more details on how AdaEmbed works and how it differs from FIITED? \n\n6. In Section 5, you discuss the limitations of your approach, including the trade-offs between memory savings and model quality, and the limitations in terms of scalability and computational complexity. Can you provide more details on how these limitations might impact the applicability of your approach to large-scale datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8200/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698716277480,
        "cdate": 1698716277480,
        "tmdate": 1699637017288,
        "mdate": 1699637017288,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oYMiKZroOy",
        "forum": "gDDW5zMKFe",
        "replyto": "gDDW5zMKFe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8200/Reviewer_CGz8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8200/Reviewer_CGz8"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors have proposed a novel embedding dimension optimization method that adjusts the dimension of each individual embedding vector continuously during training; assigning longer dimension to more important embeddings while adapting to dynamic changes in data."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The authors are solving a very interesting and useful problem. Data storage efficiency is a very industrial problem and this effort is very relevant."
            },
            "weaknesses": {
                "value": "None"
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8200/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823728748,
        "cdate": 1698823728748,
        "tmdate": 1699637017171,
        "mdate": 1699637017171,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3JF3qlXGuN",
        "forum": "gDDW5zMKFe",
        "replyto": "gDDW5zMKFe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8200/Reviewer_1xeD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8200/Reviewer_1xeD"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces FIITED, a novel embedding pruning technique tailored for very large-scale recommendation models. This method is rooted in the understanding that not all embedding dimensions are equally important, thus allowing for selective pruning. FIITED's design philosophy hinges on dimensional chunking, where embeddings are split into chunks by dimensions. The significance of these chunks is gauged through a utility-based approach, taking into account both the access frequency and the gradient values, to avoid prematurely evicting valuable chunks. The paper's design also proposes a system for addressing memory fragmentation issues: the system comprises a hash table for efficient embedding retrieval, an embedding table to store chunks, and a chunk address manager to maintain and manage memory allocation. Through this approach, FIITED aims to reduce the memory footprint of large-scale recommendation models without substantially compromising performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper presents a novel approach to embedding dimensionality reduction, innovatively combining dynamic chunk eviction strategies with utility considerations based on access frequency. The decision to merge small free memory chunks is a thoughtful addition, indicating a holistic approach to the problem.\n2. The system's design, especially its implementation on both open-source and internal frameworks, suggests that FIITED is versatile and can be potentially adapted to various other architectures and platforms.\n3. Tackling embedding dimensionality and memory efficiency is crucial in the domain of deep recommendation systems. The paper's contribution promises potential real-world impacts, particularly for large-scale applications."
            },
            "weaknesses": {
                "value": "1.\tAt the heart of FIITED is the utility-based approach to determine chunk significance. However, basing eviction decisions purely on utility scores might introduce biases. For instance, recent chunks might gain a temporary high utility, leading to potentially premature evictions of other valuable chunks. \n2.\tThis approach does not consider the individual significance of dimensions within a chunk, leading to potential information loss.\n3.\tWhile the chunk address manager maintains a free address stack, this design assumes that the most recently evicted space is optimal for the next allocation. This might not always be the case, especially when considering the locality of data and frequent access patterns.\n4.\tThe system heavily depends on the hash table to fetch and manage embeddings. This approach, while efficient in accessing chunks, might lead to hashing collisions even though the design ensures a low collision rate. Any collision, however rare, can introduce latency in access times or even potential overwrites.\n5.\tThe methodology leans heavily on access frequency to decide on embedding significance. However, frequency doesn't always equate to importance. There could be rarely accessed but critically important embeddings, and the method might be prone to undervaluing them."
            },
            "questions": {
                "value": "1. In the context of FIITED, access frequency influences the utility. When the system frequently accesses certain recent chunks, those chunks might temporarily have a higher utility score compared to older chunks\u2014even if those older chunks are historically more important. Although FIITED utilize the gradients of the chunk to mitigate such biases, The gradient might not always represent the \"big picture\" during the training process. For example, in the context of complex models, significant gradients might lead the optimization towards local minima rather than a more global optimal solution. Especially in the early stages of training, gradients can be noisy. This noise might mislead the system into thinking an embedding chunk is more or less important than it truly is. The authors should discuss more how it handles such potential fluctuations in utility values or if any smoothing mechanisms are in place.\n2. Dimensions in embeddings often have dependencies, where certain dimensions complement or refine the information in others. Chunking might break these dependencies.  This can cause the model to lose subsets of interrelated features, compromising its understanding.Moreover, embedding' dimensions might not have uniform importance. By dividing them into chunks, there's a risk that a chunk might have a mix of crucial and less vital dimensions. If the less important dimensions in a chunk increase its eviction likelihood, it might inadvertently lead to evicting more essential dimensions too.The authors may consider analyzing the dimensional dependency and significance first, as well as the adaptive chunking as a fixed chunk size may not always be optimal.\n3. The address manager simply re-allocates the most recently evicted space (due to the stack-based approach), it might re-allocate space that has high temporal locality. Thus, it could be evicting data that will be frequently accessed shortly after. This isn't efficient, as it could result in a high rate of data swapping in and out.Hence, simply using a stack to manage the evicted spaces could disrupt both forms of locality. If the system doesn't take into account the access patterns and just works on a \"most recently freed\" basis, it could lead to inefficiencies in memory access and potential performance bottlenecks, especially in scenarios where embeddings have strong temporal or spatial access patterns. Instead of purely stack-based address management, the authors may consider using a hybrid approach that takes into account both recent evictions and access frequency.\n4. The system heavily depends on the hash table to fetch and manage embeddings. This approach, while efficient in accessing chunks, might lead to hashing collisions even though the design ensures a low collision rate. Any collision, however rare, can introduce latency in access times or even potential overwrites.The authors may need to employ a more robust hashing mechanism or a secondary probing mechanism to handle collisions more effectively.\n5. Unlike whole embeddings that represent specific entities (like items), dimensions within embeddings capture specific features or facets of the represented entities. Some dimensions might be accessed less frequently because the specific feature they capture is less common in the dataset. Yet, this doesn't necessarily mean that feature is unimportant.For example, in e-commerce recommendation systems, there's often a \"long tail\" of products that aren't as popular as the mainstream ones but cater to specific needs or tastes. The dimensions representing these products might not be accessed as often, but they can be key to offering diverse and personalized recommendations to users looking for something unique. In addition, some users might have niche interests that aren't shared by the broader user base. The dimensions representing these niche interests might be accessed less frequently due to the smaller number of users with these interests. However, for those specific users, these dimensions are essential for generating accurate and satisfying recommendations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8200/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698988857126,
        "cdate": 1698988857126,
        "tmdate": 1699637017047,
        "mdate": 1699637017047,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "74qMekqx0i",
        "forum": "gDDW5zMKFe",
        "replyto": "gDDW5zMKFe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8200/Reviewer_yjuq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8200/Reviewer_yjuq"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel method for optimizing the embedding dimension in to reduce the memory footprint during training. The proposed method  (FIITED), adjusts the dimension of each individual embedding vector continuously during training, assigning longer dimensions to more important embeddings while adapting to dynamic changes in data. The paper also proposes a novel embedding storage system based on virtually-hashed physically-indexed hash tables to efficiently implement the embedding dimension adjustment and effectively enable memory saving. Overall an interesting work with some shortfalls."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper presents a novel approach to optimizing the embedding dimension in DLRMs to reduce the memory footprint during training.\n  he proposed method, FIITED, adjusts the dimension of each individual embedding vector continuously during training, which makes it more flexible and adaptable to dynamic changes in data.\n \nThe paper proposes a novel embedding storage system based on virtually-hashed physically-indexed hash tables to efficiently implement the embedding dimension adjustment and effectively enable memory saving."
            },
            "weaknesses": {
                "value": "The paper could benefit from more detailed explanations of the proposed embedding storage system and how it works, in particular details on the utility value computation or a systematic example.\n\nThe paper does not provide a comparison of their approach with other state-of-the-art methods in the field, in particular hashing and bloom filter-based methods have been shown to provide similar benefits."
            },
            "questions": {
                "value": "How does the methods compare to hashing and bloom methods. \nCould you provide more details on the methodology and perhaps a concrete example?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8200/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699032802773,
        "cdate": 1699032802773,
        "tmdate": 1699637016944,
        "mdate": 1699637016944,
        "license": "CC BY 4.0",
        "version": 2
    }
]