[
    {
        "id": "t4FRkcWb5v",
        "forum": "PAfnMGXief",
        "replyto": "PAfnMGXief",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5046/Reviewer_UEvH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5046/Reviewer_UEvH"
        ],
        "content": {
            "summary": {
                "value": "This paper studies score-based sparse adversarial attacks against deep learning models. The problem is challenging due to the combinatorial optimization problem and black-box setting. The paper introduces a novel attack algorithm by reducing the search space of adversarial attacks and proposes a probabilistic method to optimize the binary matrix. The extensive experiments demonstrate the query efficiency and effectiveness of attacking black-box models, including black-box APIs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The problem formulation is interesting and the method is solid.\n- The experiments demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "- Some technical details are unclear. For example, the authors have stated that they reduce the search space by introducing a synthetic color image. However, it is not clear how this approach can reduce the search space. Although the authors have provided detailed analyses in Appendix, they should provide some intuitions in the main paper to help understand the paper.\n- Learning a distribution of the optimization parameters seems to be a natural way. Natural evolution strategy is commonly adopted for score-based attacks by learning a search distribution, although the previous work adopts Gaussian distribution for $\\ell_2$ and $\\ell_\\infty$ norms. The authors are encourages to discuss with NES. Also, a recent work (\"Black-box Detection of Backdoor Attacks with Limited Information and Data\", ICCV 2021) also proposes a score-based method to optimize sparse perturbations, although it studies a different problem. More discussions and comparisons are needed."
            },
            "questions": {
                "value": "- Can this method be extended to adversarial patch attack, which is a special case of $\\ell_0$ perturbations but is physically realizable.\n- Could you show some sparse adversarial examples? Are there any interesting observations on what pixels are most influential for changing the prediction?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5046/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763034072,
        "cdate": 1698763034072,
        "tmdate": 1699636494414,
        "mdate": 1699636494414,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "z0KEv6aFAe",
        "forum": "PAfnMGXief",
        "replyto": "PAfnMGXief",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5046/Reviewer_tLev"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5046/Reviewer_tLev"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a black-box attack method to generate sparse adversarial perturbations to fool the model. The method focuses on learning the masks representing the perturbed pixels and use Bayesian optimization to achieve it. Extensive experiments and comparison with the baseline demonstrate the effectiveness of the algorithm."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The motivation is well-justified. The observation that interpolation with a real image and a synthetic image can generate sparse adversarial examples is interesting. The Bayesian optimization in this context is novel. The experiments comprehensively demonstrate the effectiveness of the algorithm."
            },
            "weaknesses": {
                "value": "Despite the strengths above, the paper has the following weaknesses:\n\n1. [Presentation] The notation of the paper is a bit confusing. I suggest the authors put a notation table in the appendix to facilitate the reader to better understand every notation in the paper. \n\n2. [Theorem] I did not understand why proof in Appendix G matters. The re-parameterization is straightforward.\n\n3. [Experiments] Regarding the efficiency of the algorithm, using the wall clock time may be a better choice than the number of queries. This is because the proposed algorithm may have a higher complexity per query.\n\n4. [Code] No sample code is provided, there is a reproduction concern."
            },
            "questions": {
                "value": "In addition to the weakness part above, I have the following additional questions:\n\n1. In addition to the $l_2$ and $l_\\infty$ robust models, I suggest the authors try evaluating the attack perturbation for $l_1$ robust models, since $l_1$ norm ball the convex hull of the perturbations bounded by $l_0$ norms. Possible baselines include strong AA-$l_1$ [A] and efficient Fast-EG-$l_1$ [B].\n\n[A] Croce, Francesco, and Matthias Hein. \"Mind the box: $ l_1 $-APGD for sparse adversarial attacks on image classifiers.\" International Conference on Machine Learning. PMLR, 2021.\n\n[B] Jiang, Yulun, et al. \"Towards Stable and Efficient Adversarial Training against $ l_1 $ Bounded Adversarial Attacks.\" International Conference on Machine Learning. PMLR, 2023.\n\nThe current manuscript is a borderline paper, I will conduct another round of evaluation after the discussion with the authors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5046/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699199984224,
        "cdate": 1699199984224,
        "tmdate": 1699636494337,
        "mdate": 1699636494337,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1KjuWzi9Ao",
        "forum": "PAfnMGXief",
        "replyto": "PAfnMGXief",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5046/Reviewer_kfkC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5046/Reviewer_kfkC"
        ],
        "content": {
            "summary": {
                "value": "The authors first highlight the problems related to generating strong sparse l0 norm attacks. The non differential search space makes the problem NP hard. To generate a stronger attack, the authors propose an iterative algorithm which aims to estimate a mask made up of zeros and ones to determine whether to take the pixel from the original image or from a synthetically generated random image. The authors provide an analysis on different sampling distributions considered to generate synthetic images. Once the synthetic image is generated, the authors propose to sample each value of the mask from a categorical distribution parameterized by a training parameter \\theta and conditioned on the difference between the original image and the synthetic image. Then the authors propose to estimate \\theta by initializing it as a Dirichlet prior parameterized by \\alpha_{i} for i \\in {0,k} and updating it by taking an expectation over the posterior distribution of \\theta conditioned on \\alpha_{i} and the mask of last time step. Continuing this iteratively, the authors approximate the mask with zeros and ones within the threat l0 threat model. The authors demonstrate that the proposed approach is faster when compared to [1], scales to larger datasets like Imagenet and achieves improved performance over existing methods.\n\n[1] Croce, Francesco et al. \u201cSparse-RS: a versatile framework for query-efficient sparse black-box adversarial attacks.\u201d AAAI Conference on Artificial Intelligence (2020)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The proposed algorithm seems interesting and innovative.\n* The results demonstrate significant improvements over prior works.\n* The proposed method shows improved performance on standard datasets like CIFAR10 and scales to larger scale datasets like Imagenet."
            },
            "weaknesses": {
                "value": "* I think the presentation of the paper needs improvement. For instance, in the introduction, it seems difficult to understand what authors are trying to say in \u201cOur Proposed Algorithm\u201d part. The authors should try to built a flow in the writing which can help them in explaining their method. Similarly the authors should not use long captions below the figures. This makes it difficult to follow what the authors are trying to convey. The functions GENERATION and INITIALIZATION in algorithm-1 should be defined properly.\n  \n* Since the authors only aim to generate the mask with 0\u2019s and 1\u2019s and thus replace the pixels values corresponding to mask of 1 from a synthetically generated random image, it is likely that the strength of the attack could be enhanced by adjusting the pixel values. Thus the proposed attack though performs better than existing methods, doesn't seem like an optimal attack and shows scope for improvement. It would be nice if the authors could share the effect of first running their proposed algorithm to find the pixels to be replaced and later changing the values of the pixels to make the attack stronger. Is it possible to enhance the strength of the attack by using this second step?\n\n* I think some of the design choices in the proposed algorithm need more discussion. For instance, the algorithm might be sensitive to the initial values of Dirichlet parameters \\alpha_{k}. Could the authors present some analysis on how the convergence time and attack success rate of the proposed method varies with the initial values of \\alpha_{k}. Further, does the algorithm converge to same set of values of \\alpha_{k} for different initial values?"
            },
            "questions": {
                "value": "It is not clear how the authors ensure that the attack follows the threat model. Could the authors clarify this. It would be nice if the authors can answer the concerns and questions raised in the weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5046/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699213899206,
        "cdate": 1699213899206,
        "tmdate": 1699636494266,
        "mdate": 1699636494266,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5y2qxOJbar",
        "forum": "PAfnMGXief",
        "replyto": "PAfnMGXief",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5046/Reviewer_Xni7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5046/Reviewer_Xni7"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new score-based blackbox adversarial attack under the sparse constraint (l_0 norm). Specifically, it first initial a random image to get the perturbation strength on every pixel. It then mix the random image with the original image using a learnable mask. By limiting the number of non-zero element in the mask under the l_0 norm constraint, it uses bayesian optimization to find the optimal position to generate an adversarial example on the sparsity constraint. The experiments show the proposed method could achieve a better attack success rate under different models, datasets and defensive models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow. \n2. The paper provides some interesting findings regarding sparse adversarial attack."
            },
            "weaknesses": {
                "value": "1. The proposed optimization is only conducted on finding the optimal position given a random initialized image x'. In other words, the proposed method is heavily based on the quality of the initialized x' and it heavily affects the optimality condition of the proposed method. I believe a successful sparse attack should not only find the best position but also find the optimal perturbation for those spaces. \n2. The comparison with PGD_0 is not fair to me. The PGD_0 needs to find the minimum number of pixels to be perturbed where the proposed method only finds the best solution under certain threshold. I believe a fair comparison should let the proposed method also find the minimum number of pixels along with its optimal perturbation strength.\n3. The baseline included in the experiments are pretty weak. For example, some baselines like iteratively conducting one-pixel adversarial attack should be considered rather than just comparing with a decision-based attack. Also, some dense attack could easily be changed to handle the sparse attack. For example, selecting the largest perturbation in the l-2 norm attack or using a relaxed l-1 attack would also be viable baselines.\n4. Although the proposed method claims to largely improve the query efficiency, the number of query is still around in the same level with decision based attack. However, the score based attack should be much more query-efficient than decision-based attack in dense attack, where makes the score-based attack not attractive and impractical."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5046/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699290481634,
        "cdate": 1699290481634,
        "tmdate": 1699636494195,
        "mdate": 1699636494195,
        "license": "CC BY 4.0",
        "version": 2
    }
]