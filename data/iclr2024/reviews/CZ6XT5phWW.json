[
    {
        "id": "Dsarncg8NW",
        "forum": "CZ6XT5phWW",
        "replyto": "CZ6XT5phWW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8350/Reviewer_bonY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8350/Reviewer_bonY"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the quailty of prompts and how to improve them for better capabilities of zero-shot and few-shot in-context learning. To ahieve this, authors first define the characteristics that good prompts should have, and then proposes a method to rewrite prompts to improve their quality based on this. Effectiveness is evaluated on mathematical reasoning, code generations and those tasks in BigBench."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I think the tageted issue, i.e., the quality and improvements of prompts, is currently needed by both the industrial and academic communities. Improving the zero-shot or few-shot in-context learning ability of large-scale models by improving the quality of prompts still has extremely high research value in the short term. This work could be viewed as a start point for this aspect.\n\nHowever, we have to realize that the research space in prompt engineering also reflects the shortcomings of the current large models. The improvement of large models in the near future will be reflected in their stronger robustness to prompts. I suggest researchers to look at this issue with a more long-term developmental perspective, instead of being satisfied with the immediate results."
            },
            "weaknesses": {
                "value": "I very much agree with the starting point of this article, but at the same time, I regret that this research work lacks the necessary depth. From the definition of the quality of prompts to the method of improving the quality of prompts, most of the content is confined to quantitative analysis, lacking more in-depth and specific method design. There is also no larger scale quantitative evaluation on more general downstream tasks. For readers, it is somewhat difficult to catch the technical insights and contribution so that the current version seems premature."
            },
            "questions": {
                "value": "Could you further clarify how you rewrite the prompts and clearly hightlight the insights in them?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8350/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830244304,
        "cdate": 1698830244304,
        "tmdate": 1699637038714,
        "mdate": 1699637038714,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "S4dLMQ1Ig1",
        "forum": "CZ6XT5phWW",
        "replyto": "CZ6XT5phWW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8350/Reviewer_xGQ6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8350/Reviewer_xGQ6"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses a significant challenge in the application of large language models (LLMs) to zero-shot tasks \u2013 the design of task prompts that are sufficiently informative and unambiguous to guide the model to the correct solution without task-specific annotations. The authors propose PROMPTD, an innovative approach that generates customized prompts for each test instance by designing some pre-defined prompts, enhancing the LLM's ability to handle tasks across various domains including arithmetics, logical reasoning, and code generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of dynamically rewriting prompts for individual instances is a novel and interesting approach that represents a significant departure from the more static strategies employed in prior works.\n\n2. The reported results indicate that PROMPTD provides a substantial boost in performance, achieving an improvement of 10% on the MATH dataset and 5% on the code generation tasks in HumanEval, which is impressive and suggests that the approach has practical value.\n\n3. By applying PROMPTD across eight different datasets, the authors demonstrate the method's general applicability, an essential characteristic for real-world deployments.\n\n4. The paper presents an additional benefit of using PROMPTD \u2013 the rewritten prompts not only aid in task resolution but also enhance the interpretability of the LLM's decision-making process, which could be crucial for trust and reliability in AI systems."
            },
            "weaknesses": {
                "value": "1. The paper does not sufficiently discuss the computational overhead of the PROMPTD method. Since the approach involves generating custom prompts for each instance, there may be a significant increase in the computational cost that could limit its scalability. More importantly, the PROMPTD is quite long; can the authors make some ablation studies about it?\n\n2: The efficacy of PROMPTD is likely highly dependent on the initial quality of the prompts it is based upon. The paper could better address how the system performs with suboptimal base prompts and the robustness of the method to variations in prompt quality.\n\n3: While the performance improvements are impressive, the evaluation might benefit from a deeper analysis of where and why the approach fails. Understanding the limitations of PROMPTD is as important as understanding its strengths.\n\n4: There have been so many zero-shot prompting methods recently. The paper would be strengthened by including a more comprehensive comparison with the recent state-of-the-art methods for zero-shot learning.\n\n\n5. The evaluation of PROMPTD on a single new task type (sentiment classification) is a significant limitation. Given the length and complexity of the original PromptD, it is unclear how well the method would generalize to other task types. The reviewer's expectation of a more general prompt applicable to a wide array of task types is unmet. This is a critical aspect, as the creation of highly specialized prompts may not be feasible in many real-world applications."
            },
            "questions": {
                "value": "Same as before"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8350/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698995967130,
        "cdate": 1698995967130,
        "tmdate": 1699637038613,
        "mdate": 1699637038613,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IwRD7HJ9gW",
        "forum": "CZ6XT5phWW",
        "replyto": "CZ6XT5phWW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8350/Reviewer_vep9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8350/Reviewer_vep9"
        ],
        "content": {
            "summary": {
                "value": "This paper propose to rewrite a specific prompt for each test point by prompting GPT4 with demonstrations that show how to rephrase a bad prompt into a better one with a rationale and task type. With the refined prompt, we can achieve better zero-shot performance on several benchmark datasets than other relevant baselines including zero-shot Chain-Of-Thought. Moreover, such prompt rewriting method generalizes to refine tasks that are not included the demonstrations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed method is simple and effective. It rewrite a prompt into clear, specific, complete and more structure prompts, which leads to improved performance.\n\n- For rewriting a prompt, we need 10 demonstrations, which is really practical for real-world applications.\n\n- The authors performed human evaluation to verify that the quality of the rewritten prompts becomes better.\n\n- The authors performed ablation study to show that task type and reasons are crucial component for prompt rewriting."
            },
            "weaknesses": {
                "value": "-  Compared to zero-shot prompt models, it requires extra forward pass of LLMs to rewrite a prompt. It would be better to show how much more computational cost is required than other baselines.\n\n- It is not clear such GPT4 written prompt would be transferred to other LLMs such as Llama."
            },
            "questions": {
                "value": "Please see weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8350/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8350/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8350/Reviewer_vep9"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8350/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699450477930,
        "cdate": 1699450477930,
        "tmdate": 1700702362903,
        "mdate": 1700702362903,
        "license": "CC BY 4.0",
        "version": 2
    }
]