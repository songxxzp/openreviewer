[
    {
        "id": "DIKeBKfiLe",
        "forum": "8oUF3uGIVo",
        "replyto": "8oUF3uGIVo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7254/Reviewer_eGUc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7254/Reviewer_eGUc"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new graph representation learning method called HOtrans, which builds upon the Transformer architecture. HOtrans introduces a three-step message-passing approach to capture high-order correlations and long-range dependent information in graphs. Specifically, it first constructs communities, and then involves message passing between node to virtual node, virtual node to virtual node, and virtual node to a node. Experiments results showed the effectiveness on Homophilic and Heterophilic graphs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.  The proposed HOtrans can utilize high order information, and serves as a general framework for many existing graph transformers.\n\n2. HOtrans achieves a competitive performance.\n\n3. This paper is clearly presented and easy to follow."
            },
            "weaknesses": {
                "value": "1. The key components of HOtrans are basically built upon existing concepts, e.g., virtual node, community sampling etc, which makes this paper less disruptive.\n\n2 . High order information is important and existing solutions usually adopt hypergraph method. The proposed HOtrans resembles hypergraph convolutional networks, and there exist many hypergraph solutions, but the authors didn\u2019t discuss the differences, without making compasions in the experiments. \n\n3. I think the community sampling method will have a big impact on the model performance and should be datasets dependent. However, this paper is mainly based on existing random walk and sepctal clustering.\n\nminor: \n\n1. I cannot find the corresponding text for table 3. What is this table for?\n\n2. Typos: A.1, gapformer should be HOtrans?  Also, I suggest the authors to have a table summarizing the complexity compasions with existing models."
            },
            "questions": {
                "value": "1. How the proposed method compared with hypergraph approaches?\n\n2. Can the proposed HOtrans be applied to graph level tasks and how about the performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7254/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698398424538,
        "cdate": 1698398424538,
        "tmdate": 1699636864764,
        "mdate": 1699636864764,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1Y3upKFVRE",
        "forum": "8oUF3uGIVo",
        "replyto": "8oUF3uGIVo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7254/Reviewer_7DnL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7254/Reviewer_7DnL"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a Transformer architecture called HOtrans, which employs a higher-order message passing strategy. The model initially extracts communities, each centered around a virtual node, from the entire graph. Subsequently, the proposed attention mechanism comprises three steps: first, information is aggregated towards higher-order virtual nodes; second, information is exchanged among representations of these higher-order virtual nodes; and finally, the updated information is relayed back to the original graph nodes. In the experimental study, the authors demonstrate the effectiveness of HOtrans in node classification tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method not only delivers competitive performance on homophilic datasets but also surpasses the state-of-the-art methods on heterophilic datasets.\n\n2. The proposed HOtrans is scalable to large graphs."
            },
            "weaknesses": {
                "value": "1. The novelty of the paper appears somewhat limited. Both community sampling techniques and the virtual node design have been previously introduced in other studies. This work seems to primarily amalgamate these existing strategies.\n\n2. The proposed method may not be competitive to some linear graph transformers. For example, NodeFormer [1], with less computational resource and fewer training samples, gets better results on full attention graphs, e.g., on Cora with 88.80% accuracy versus 88.11% obtained by the proposed method.\n\n3. It is claimed that the proposed method can capture long-range dependency in graphs. Results on some long-range graph datasets are expected, e.g., PascalVOC-SP from LRGB [2]. \n\n4. It would be interesting to try learning-based community sampling methods to assess its effect to the proposed model.\n\n[1] Wu Q, Zhao W, Li Z, Wipf D.P, Yan J. Nodeformer: A scalable graph structure learning transformer for node classification. Advances in Neural Information Processing Systems. 2022 Dec 6;35:27387-401.\n\n[2] Dwivedi V.P, Ramp\u00e1\u0161ek L, Galkin M, Parviz A, Wolf G, Luu A.T, Beaini D. Long range graph benchmark. Advances in Neural Information Processing Systems. 2022 Dec 6;35:22326-40.\n\nTypo: In appendix A.1, it should be \u201ccomplexity of HOtrans\u201d, instead of \u201ccomplexity of Gapformer\u201d.\n\nMinor problem: The citations for Gapformer in Tables 1 and 2 are missing."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7254/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698653818897,
        "cdate": 1698653818897,
        "tmdate": 1699636864653,
        "mdate": 1699636864653,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lO4UV2QJNn",
        "forum": "8oUF3uGIVo",
        "replyto": "8oUF3uGIVo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7254/Reviewer_e8WT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7254/Reviewer_e8WT"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors propose a new architecture for graph learning, HOtrans, specializing in transductive node classification tasks. The approach involves three steps: (a) community detection, (b) self-attention between community representations, and (c) update of the node representations from community representations. The authors compare their model on various node classification datasets, including both homophilic and heterophilic graphs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Strengths:**\n- The paper addresses an important problem.\n- The proposed model compares well against most of the presented baselines\n- The empirical study is thorough, and the paper presents ablation studies to gain further understanding of the essential components of the proposed architecture."
            },
            "weaknesses": {
                "value": "**Weaknesses** (in the order of importance):\n- The most important issue with this work is that the empirical results of the proposed model are not very convincing. Overall, the presented improvements of HOtrans are well within the standard deviation of prior works such as Graphormer [1]. The only exception to this are Texas and Wisconsin, where HOtrans is significantly better than previous models. On Actor, the authors also improve significantly over Gapformer but are not significantly better than a standard transformer with Laplacian encodings [2]. Considering that HOtrans is overall very close in performance to Gapformer and the fact that Gapformer also explores applying self-attention to pooled representations of sets of nodes, it is not convincing that HOtrans adds much value over Gapformer.\n\n- The motivation of HOtrans is quite weak. The authors mention that HOtrans was developed to capture long-range dependencies and to aggregate the information of sets of nodes instead of single nodes. However, such approaches have already been explored, e.g., with the Graph-ViT [3]. Further, the ability of graph transformers to capture long-range dependencies has already been demonstrated in e.g., GraphGPS [4] and even in the context of transductive node classification [2]. It would be helpful if the authors could empirically compare their approach to these prior works.\n\n- The datasets used for heterophilic node classification, in particular Cornell, Texas, and Wisconsin, have recently been shown to have substantial weaknesses, such as their very small size and imbalanced classes [5]. The claim that HOtrans performs favorably to graph transformers on heterophilic datasets could be further substantiated by additional results on more suitable datasets such as the ones in [5]. On Actor, which is the only heterophilic dataset that does not suffer from the aforementioned issues, HOtrans performs no better than a standard transformer with Laplacian encodings [2] (as already mentioned above).\n\nIn summary, while the proposed model, HOtrans, outperforms most presented baselines, it only outperforms the best prior method on each dataset on 2/10 datasets. Most crucially, the empirical results are very close to those of Gapformer on nearly all datasets and it is not made clear how the proposed method is otherwise favorable to Gapformer.\n\nReferences:\n[1]: Gapformer: Graph Transformer with Graph Pooling for Node Classification, Liu et al. 2023\n[2]: Attending to Graph Transformers, M\u00fcller et al. 2023\n[3]: A Generalization of ViT/MLP-Mixer to Graphs, He et al. 2022\n[4]: Recipe for a General, Powerful, Scalable Graph Transformer, Rampasek et al. 2022\n[5]: A Critical Look at the Evaluation of GNNs under Heterophily: Are We Really Making Progress, Platanov et al., 2023"
            },
            "questions": {
                "value": "What added value does HOTrans provide compared to Gapformer considering that both models perform nearly the same across all datasets?\n\nHow does HOtrans perform to prior graph transformers that aggregate sets of nodes (e.g., Graph-ViT) or have demonstrated to capture long-range dependencies (e.g., GraphGPS)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7254/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7254/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7254/Reviewer_e8WT"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7254/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769195983,
        "cdate": 1698769195983,
        "tmdate": 1700733555789,
        "mdate": 1700733555789,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HyTDy7sXH0",
        "forum": "8oUF3uGIVo",
        "replyto": "8oUF3uGIVo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7254/Reviewer_akHZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7254/Reviewer_akHZ"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a virtual node based graph transformer to capture the high-order structure information of the graph. The designed approach is by (1) node clustering by ClusterGCN or random walk-based GraphSaint, (2) attentions at three levels including node-to-virtual-node, virtual-node-to-virtual-node, and virtual-node-to-node. This work is evaluated by node classification tasks on both homophily and heterophily graphs, which show better or competitive performance over the state-of-the-art baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This work introduces an interesting approach to leveraging virtual nodes to improve the capability of graph transformers to capture graph high-order information, for node-level tasks. Prior works leverage virtual nodes to solve graph-level tasks (e.g., graph classification).\n\n2. The designed approach is to some extent generic and can be specified into different types of GNNs according to how communities/clusters are defined.\n\n3. The proposed method indeed performs better than the state-of-the-art on most datasets, except ogbn-arxiv dataset. Ablation studies are done to show the effectiveness of different components."
            },
            "weaknesses": {
                "value": "1. It is not clear how theorem 4.1 can demonstrate the power of the designed method from the theoretical perspective. In other words, given that the proof of Theorem 4.1 is based on top of Proposition 4.1 while there's no approximation error provided, it is vague to define `arbitrarily well' mentioned in Theorem 4.1. \n\n2. The explanations of why positional encoding hurts the performance are a bit insufficient. I agree with the authors that Laplacian PE can hurt the performance on heterophily datasets since heterophily graph usually requires non-low-pass filters for aggregations. However, it's not clear why random walk based encoding also performs worse. Besides, both encoding methods hurt the performance on both homophily and heterophily datasets.\n\n3. Eq. 3 seems to be incorrect. \\bar{x}_i is defined as a zero vector, as described in the paper, so \\bar{q}_i is essentially just a zero vector and thus carries no information at all."
            },
            "questions": {
                "value": "1. How do you define 'arbitrarily well' in Theorem 4.1 mathematically? \n2. Can you elaborate more about the reasons why positional encodings perform generally worse than not using positional encodings on both homophily and heterophily datasets?\n3. Why is \\bar{x}_i defined as a zero vector?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7254/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698792889802,
        "cdate": 1698792889802,
        "tmdate": 1699636864404,
        "mdate": 1699636864404,
        "license": "CC BY 4.0",
        "version": 2
    }
]