[
    {
        "id": "7dKyuBgDFg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3804/Reviewer_s1Ur"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3804/Reviewer_s1Ur"
        ],
        "forum": "W0fmImkpHp",
        "replyto": "W0fmImkpHp",
        "content": {
            "summary": {
                "value": "This paper proposes using interpretability tools in a logical, sound, and insightful manner to understand how deep learning models work. This is then used to improve the predictive performance of deep learning models. Convincing and sufficient validation is presented for the proposed approach."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is technically sound, impactful, meaningful, and opens up brilliant new directions using tools typically reserved for interpretability and explainability. The reviewer believes that this work has significant insight and realizes that these tools can be used for interpreting behavior of neural networks, and in a reinforcement learning loop to continue to improve the performance and predictive power of deep learning models."
            },
            "weaknesses": {
                "value": "This paper is not well aligned with recent works and overall research direction in the interpretability, explainability, ethics, and the usage of such techniques in a humanist, and ethical manner.\n\nFinalize Doshi-Velez, and Timnit Gebru have written extensively in academic, and non-academic environments about the growing problem with AI models being overly large, and overly powerful. This pattern and trend, and the ownership of these models by private corporations can lead to significant harm."
            },
            "questions": {
                "value": "I have no questions for the authors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I find this paper uncomfortable to digest and accept given recent outcry in the AI communities regarding the growing power of AI models. I think research along this direction and accepting research in this direction puts ICLR, reviewers, PCs, and SPCs in a significant moral quandry."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3804/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697356193988,
        "cdate": 1697356193988,
        "tmdate": 1699636337777,
        "mdate": 1699636337777,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9BukMxsLgD",
        "forum": "W0fmImkpHp",
        "replyto": "W0fmImkpHp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3804/Reviewer_feHw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3804/Reviewer_feHw"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on exploring neuron activation patterns to improve performance of deep learning classifiers. The authors argue that the entropy of the neuron activation pattern is related to model performance. The authors utilize the neurons\u2019 activation probabilities to help boosting the predicted values of base model. The experimental results prove the validity of the theoretical hypothesis and the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThis paper discusses the definition of neuronal activation patterns in detail. Assumptions and theories are available in the paper, and the narrative logic is strong. \n2.\tIn this paper, we hope to directly constrain the activation pattern of neurons to enhance the final classification performance."
            },
            "weaknesses": {
                "value": "1.\tThe auxiliary model has some neurons, and the classification tasks and datasets are simple. I suspect that part of the performance gain is due to the extra parameters in this part. \n2.\tIt seems that an auxiliary model is not needed, and the probability of neuronal activation may serve as a new loss function. \n3.\tIn the author's hypothesis, specific neurons are expected to be activated frequently for a particular class. However, in the proposed process of calculating the activation probability of neurons, neurons may be activated by multiple classes, and serve the calculation of activation probability of multiple classes. \n4.\tThe notation in Figure 1 should be consistent with the description in Section 5.2 to avoid confusion to the reader. In particular, the input and output of the auxiliary model should be clearer."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3804/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698657910506,
        "cdate": 1698657910506,
        "tmdate": 1699636337693,
        "mdate": 1699636337693,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rEtooCD2E2",
        "forum": "W0fmImkpHp",
        "replyto": "W0fmImkpHp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3804/Reviewer_YNcH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3804/Reviewer_YNcH"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes neuron activation patterns in deep learning models to see if model performance can be improved. The authors find activation entropy is negatively correlated with model accuracy. They propose a novel approach leveraging neuron activation probabilities to boost classification performance of trained models, demonstrating notable accuracy improvements on benchmark datasets. The proposed techniques help explain neuron behavior and use activation probabilities to enhance common deep learning models. Key findings show activation entropy relates to model performance, and the proposed auxiliary model improves accuracy across many datasets. The research provides new methods to explain and improve deep learning through information-theoretic analysis of neuron activations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper works on an important and timely topic.\n2. The proposed methods provide novel ways to explain neuron behavior in deep learning models by analyzing activation patterns. More importantly, it leverages neuron activation probabilities to improve classification accuracy of trained deep learning models across several benchmark datasets.\n3. Experimental results show that the proposed method improves accuracy across many common deep learning models. This research provides new techniques to explain and enhance deep learning through information-theoretic analysis of neuron activations."
            },
            "weaknesses": {
                "value": "Refer to the Questions section"
            },
            "questions": {
                "value": "1. Table 2 shows that the improvement is higher during the earlier stage of model training, and gradually decreases along with the model training. It is interesting to know whether the Te and PTe performance could converge as the Iter continues to increase, e.g., when Iter equals 40.\n2. This work is based on the strong hypothesis that: H1: The entropy of the activation pattern is related to a deep learning model\u2019s performance. H2: The activation probabilities of the representative neurons in a model can be leveraged to improve its performance. It would be great if the authors could give some hints when these two hypotheses hold and when these two hypotheses do not hold."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3804/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698961532594,
        "cdate": 1698961532594,
        "tmdate": 1699636337630,
        "mdate": 1699636337630,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rHhh1PieyP",
        "forum": "W0fmImkpHp",
        "replyto": "W0fmImkpHp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3804/Reviewer_zAME"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3804/Reviewer_zAME"
        ],
        "content": {
            "summary": {
                "value": "The paper explores probabilities of activations as a means to improve accuracies on a test set. In Section 5.2 they use a class-probability reweighted prediction, and input this prediction together with the original prediction into a 3 layer neural network to arrive at the final prediction."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "They perform experiments with an MLP head on top of neural network outputs."
            },
            "weaknesses": {
                "value": "Certain issues:\n\nissue 1: \n\nSection 5.2  makes no use of the entropy from section 5.1. The whole entropy discussion is not used for the prediction.\n\nequation 16 computes as modified prediction, the prediction for a class weighted by the probabilities for this class that neurons get activated. Effectively it is upweighted by probabilities of neurons being activated. \nThis is very likely not improving test time prediction on a wide range of networks.\n \nLooking into the code, it becomes clear that the effect comes from putting an MLP head on top of the networks. This has nothing to do with probabilities of activations or entropies. \n\nThe effect comes from the MLP head. Putting some reweighted outputs into an additional MLP has very little novelty.\n\nissue 2: The paper has no explaining aspect in it.\n\nissue 3: Aside from that there are lots of mistakes in the intro:\n\neq 4 is wrong: every concatenation references the input s. \nThey meant to concat the layers.\n\\sigma_{M} there is either undefined, or references the sigma in the line below, in which case it would be not correct\n\n\nIn a deep learning model for\nith iteration the parameters \u03b1(i) are calculated using \u03b1(t) for 0 \u2264 t \u2264 i \u2212 1.\n\n- this sentence contains no information!\n\neq 5 is not an accuracy because it outputs a set, not even a count. There is little point defining trivialities like accuracy.\n\n\neq 7 is a boolean statement which evaluates to true or false. \n\n\"In the following discussions, on(beta) and of f (beta)\nare the subset of neurons that are active and inactive, respectively.\"\n\nTherefore eq.7 is a tautology, as it always evaluates to True .\n\nSomething is formally wrong in eq 9 when they want to describe an activation pattern.\nIt says True implies a that a neuron is an affine mapping with a non-linearity. This is an assumption and has nothing to do with mathematical induction.\n\n\nequation 13 states the same natural assumption again\n\n\n\"The discussion shows that neurons within the activation pattern are a linear combination of the input\nand each activation adds linear constraint to the input predicate.\"\n\nThe first is a trivial assumption. The second is unclear or trivial.\n\nSo, there exist input properties that\nare responsible for prediction P for any S.\n\nA trivially true statement.\n\nIt also implies that for S \u2208 T of the same class, the\nactivation pattern will be similar\n\nIn this generality a true statement.\n\n5.2: U_X is positive whenever P_{nc} is positive. Therefore U_X^* in eq 16 adds no information."
            },
            "questions": {
                "value": "In a deep learning classifier, assume there are c \u2208 C classes with Ic members in each class c \u2208 C, - how is this computed for test data ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3804/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699186133332,
        "cdate": 1699186133332,
        "tmdate": 1699636337569,
        "mdate": 1699636337569,
        "license": "CC BY 4.0",
        "version": 2
    }
]