[
    {
        "id": "98vLge5bZ8",
        "forum": "HoY24hOeVP",
        "replyto": "HoY24hOeVP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1577/Reviewer_uaaP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1577/Reviewer_uaaP"
        ],
        "content": {
            "summary": {
                "value": "In order to leverage a combined textual prompt and alleviate the computational demands of working in a high-dimensional embedding space, this paper introduces the BaTex method for acquiring versatile embeddings within a low-dimensional textual subspace for personalized text-to-image generation. The experimental results affirm the effectiveness of this approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is excellently structured and offers a clear, comprehensible narrative.\n\n2. This paper presents a novel approach, introducing textual subspace learning to eliminate the need for time-consuming training in high-dimensional embedding spaces.\n\n3. The experiments provide compelling evidence of the efficiency and robustness of the proposed BaTex method."
            },
            "weaknesses": {
                "value": "1. Regarding the use of subspace learning:\n\nSince the vocabulary V corresponds to a set of pre-trained vectors {v_i}, it might seem logical to directly select the top vector. So, this arises an important problem: why do we need to use the subspace learning method? Firstly, one important motivation is the combination with different textual prompts. I guess that this combination is implemented in a single subspace (and if my understanding is incorrect, kindly correct me). In Figure 2, it is not clear which textual prompts are amalgamated in this single subspace. Additionally, as depicted in Figure 3, the model considers multiple embeddings. Is the model simultaneously exploring multiple subspaces? Secondly, when we compare Figure 6 with Figure 8, we notice that both scenarios using the top vector manage to fulfill the textual descriptions. However, why does Figure 8 fall short in delivering the desired results?\n\n2. Addressing missing objects in generated images:\n\nFigure 2 highlights that the generated images miss some objects mentioned using the TI method. For instance, when examining the images from top to bottom, we notice the absence of objects such as a lady, a student, the beach, and a table. It's important to note that this work primarily focuses on learning a new embedding. How does this learned embedding enable the generation of missing objects? What is the mechanism for achieving this?\n\n3. Clarification on optimization problem and loss:\n\nIn Equation (5), the optimization problem is to optimize the variable v. However, in Algorithm 2, the L\\_res loss is calculated concerning the variable w. Could you please provide clarification on this?\n\n4. Discussion of limitations:\n\nIt appears that the paper lacks a discussion on its limitations. It would be valuable to address and discuss any limitations of this work. What potential constraints or drawbacks should readers be aware of when considering the findings and applications of this research?"
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831660599,
        "cdate": 1698831660599,
        "tmdate": 1699636086523,
        "mdate": 1699636086523,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ArEl7xUVig",
        "forum": "HoY24hOeVP",
        "replyto": "HoY24hOeVP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1577/Reviewer_GqUH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1577/Reviewer_GqUH"
        ],
        "content": {
            "summary": {
                "value": "This work primarily focuses on personalizing text-to-image models. The authors introduce Batex, a method that leverages multiple text embeddings with high similarity and updates their weights. This approach offers the advantages of reduced training time and improved text-image alignment. The paper provides both qualitative and quantitative results to support the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The organization and writing of the paper are commendable, resulting in a clear and easily understandable presentation.\n2. The proposed method is both simple and effective, as exemplified by the compelling results presented in Table 2."
            },
            "weaknesses": {
                "value": "1. The authors say that their method offers advantages in terms of reduced training time and improved text-image alignment. However, the significance of improving training time may be less important considering the already inexpensive nature of personalization (text inversion). Additionally, the reasoning behind achieving higher text-image alignment compared to traditional text-to-image (TI) methods is not adequately clarified. Why TI cannot learn an embedding to align text and images?\n2. It is hard to tell if Batex outperforms TI in Figure 2. It would be beneficial for the authors to provide further explanations regarding the superiority of Batex in Figure 2 to help readers better understand the comparative strengths of the proposed method."
            },
            "questions": {
                "value": "1. Pls see weakness.\n2. How is the results of other baselines in fig 3?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1577/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1577/Reviewer_GqUH",
                    "ICLR.cc/2024/Conference/Submission1577/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836607007,
        "cdate": 1698836607007,
        "tmdate": 1701071344059,
        "mdate": 1701071344059,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mxXX1PFXb1",
        "forum": "HoY24hOeVP",
        "replyto": "HoY24hOeVP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1577/Reviewer_pRpB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1577/Reviewer_pRpB"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to address the issues of prompt editing degradation and time-consuming training process in personalized text-to-image generation.\nTo this end, it introduces an efficient method to explore the target embedding in a textual subspace with higher text similarity.\nSpecifically, it proposes a selection strategy to determine the basis vecors of textual subspace.\nExperiments demonstrate that the learned embedding can both reconstruct input image and improve its alignment with editing prompts."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "See summary."
            },
            "weaknesses": {
                "value": "1. The optimization of target embedding is performed in an explainable textual subspace. So could the authors provide visualizations of both learned weights and corresponding basis vectors (i.e., words)?\n2. Too few methods are compared in this paper. The authors are encouraged to add more baselines including optimization-based [1] and encoder-based[2,3] in revision.\n3. The expressiveness of words combination may be limited, especically in reconstructing image detalis, e.g., human faces.\n4. The selection stratgy of textual subspace chooses M basis embeddings that are most similar to initialization embedding u. Is there too much redundancy among these basis embeddings?\n\n[1] Ligong Han, et al. SVDiff: Compact Parameter Space for Diffusion Fine-Tuning. ICCV 2023.\n\n[2] Yuxiang Wei, et al. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. ICCV 2023.\n\n[3] Rinon Gal, et al. Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models. SIGGRAPH 2023."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1577/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1577/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1577/Reviewer_pRpB"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698985963863,
        "cdate": 1698985963863,
        "tmdate": 1700658080890,
        "mdate": 1700658080890,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IYLoNebwII",
        "forum": "HoY24hOeVP",
        "replyto": "HoY24hOeVP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1577/Reviewer_s24C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1577/Reviewer_s24C"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a method for learning embeddings in a low-dimensional textual subspace to achieve improved time efficiency and better preservation of text similarity in the learned embeddings for personalized text-to-image generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The writing is clear and easy to follow.\n\n2. The authors propose using a reduced number of vectors to represent the original embedding, enabling the proposed model to undergo fewer training steps without significantly compromising performance.\n\n3. The experiments presented in the paper support the authors' assertions. They require fewer training steps while achieving competitive performance compared to other methods."
            },
            "weaknesses": {
                "value": "1. I remain unconvinced regarding the claimed time efficiency of the proposed method. First, the paper only provides the number of training steps, but it lacks information on the actual time required for each step of the proposed method. Furthermore, the method's need for a loop to search for the proper number M, as shown in Algorithm 1, can be time-consuming. Consequently, the quantitative comparison in Table 2 does not demonstrate a significant advantage of the proposed method.\n\n2. I also have reservations about the qualitative performance of the proposed method. The paper showcases only a limited number of results with restricted diversity, such as a limited variety of style images.\n\n3. I am unclear as to why the authors assert that previous methods solely focus on image reconstruction, thereby degrading their ability to combine the learned embeddings with different textual prompts. The primary objective of personalized text-to-image generation is to create new images based on input images. Additionally, Textual Inversion can also combine various textual prompts. More comprehensive details and discussion are needed to support this claim."
            },
            "questions": {
                "value": "Please see above weaknesses. I am willing to change my rating if authors could address my concerns."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699024305598,
        "cdate": 1699024305598,
        "tmdate": 1699636086311,
        "mdate": 1699636086311,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eg6L3cvhZk",
        "forum": "HoY24hOeVP",
        "replyto": "HoY24hOeVP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1577/Reviewer_qu14"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1577/Reviewer_qu14"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a  method named as  BaTex for learning arbitrary embedding in a low-dimentional textual subspace. This paper also proposes an efficient selection strategy for determining the basis vectors of the textual subspace. The proposed methods achieve good performances on the public datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper proposes a method named as BaTex for learning arbitrary embedding in a low-dimentional textual subspace, which is time-efficient and better preserves the text similarity of the learned embedding. The learned embeddings can not only faithfully reconstruct the input image, but also significantly improve its alignment with different textual prompt."
            },
            "weaknesses": {
                "value": "The novelty is limited. Although this paper proposes a method to extract the specific textual subspace for personalized text-to-image generation, the novelty of te proposed method is limited. The textual subspace vector is widely adopted for the conditional diffusion models, e.g. [Medical diffusion on a budget: textual inversion for medical image generation], [LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On], etc. What's more, the pritority and advantages of the proposed method comparing with the traditional textual inversion is not obvious. The difference of the training step need to be clarified more clearly."
            },
            "questions": {
                "value": "1. Please highlight the priority and novelty of the proposed method from the whole diffusion procedures. \n2. Please add more analysis of the training details of the proposed methods comparing with the textual inversion.\n3. Please add more experimental results of more conditions of the text-to-image tasks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699191610759,
        "cdate": 1699191610759,
        "tmdate": 1699636086209,
        "mdate": 1699636086209,
        "license": "CC BY 4.0",
        "version": 2
    }
]