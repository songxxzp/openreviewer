[
    {
        "id": "xultIfGEzY",
        "forum": "NiefAhgJqH",
        "replyto": "NiefAhgJqH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7595/Reviewer_Z7MF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7595/Reviewer_Z7MF"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel model-free method for learning Bayes-optimal policies. It is found through theoretical analysis that existing model-free methods fails to address the epistemic uncertainty in the future or optimizes over a set of contextual policies. The proposed Bayesian exploration network addresses these gaps by simultaneously modeling both epistemic and aleatoric uncertainties through the use of normalizing flows. The method has potential to learn the Bayes-optimal in the limit of complete optimization."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper introduces a novel method that models epistemic and aleatoric uncertainties using normalizing flows. This could be a valuable contribution to the field of reinforcement learning."
            },
            "weaknesses": {
                "value": "1.  **Mathematical Rigor:** Although the paper provides extensive mathematical analysis, there's a discernible lack of mathematical rigor.\n    \n*   _Theorem 1:_ The theorem hinges on Lemma 1. However, Lemma 1's proof is questionable as its final equality doesn't hold true. The MDP parameter, $\\phi$, doesn't impact future decisions made by the contextual policy, which depend solely on history.\n*   _Theorem 2:_ The theorem states that the posterior of a mis-specified deterministic model concentrate on the maximum likelihood estimation (MLE). However, it is obvious that given few data, there could be multiple models fit the data perfectly. All of them are MLE but not all MLE has non-zero posterior probability. Besides, if no model can fit the data perfectly, then the likelihood of data under any model are always $0$, implying that marginal likelihood of the data is also $0$. In this case, the posterior probability is the probability of the model conditioned on an impossible event, which is undefined.\n*   _Theorem 3:_ The third theorem is based on the presumption that $b_t$ is a sufficient statistic. However, this is not the case if no further assumptions are made. We can easily construct examples where the pair $(s_{t+1},r_t)$ cannot be differentiated by just looking at $b_t=r_t+\\gamma \\sup_{a\u2019}Q^*(h_{h+1},a\u2019)$.\n\n2.  **Exploration-Exploitation Dilemma:** The goal of this paper is to learn a Bayes-optimal policy that strikes a balance between exploration and exploitation. But the process of learning this policy through environmental interactions has its own exploration-exploitation quandary, which isn't adequately tackled.\n\n3.  **Structure and Presentation:** The paper's layout is unwieldy. While the bulk of main text reiterates well-established and well-recognized results, critical details about the new method are relegated to the appendix. Consequently, understanding the new approach demands a thorough read of the appendix.\n\n4.  **Related Work:** The paper overlooks model-free meta-reinforcement learning methods, such as those in [1], which align closely with its theme. Additionally, RL methods for POMDPs, e.g., [2], are not mentioned. It is not clear how the proposed method performs compared to methods that directly solve the history-MDP.\n\n\n**Minor Points:**\n\n*   Notations are introduced without prior definitions, such as $P_Q$ in Definition 1 and $P_{al}$ in page 19.\n*   Definition 1: Replace \"... over and a model \u2026\" with \"... over a model \u2026\"\n*   Definition 1 & Last paragraph of Sec 3: The term $P_Q$ is ambiguous. Typically, a Q-function wouldn't yield a distribution.\n*   First paragraph in Sec 4.1: Change $\\pi(\\cdot,\\theta)$ to $\\pi(\\cdot,\\phi)$\n*   Theorem 3: The phrase \"be a measurable mapping $\\mathcal{S}\\times\\mathbb{R}\\to\\mathbb{R}$\" is initially perplexing and warrants elaboration.\n\n\n[1] Beck, J., Vuorio, R., Liu, E.Z., Xiong, Z., Zintgraf, L., Finn, C. and Whiteson, S., 2023. A survey of meta-reinforcement learning. arXiv preprint arXiv:2301.08028.\n[2] Hausknecht, M. and Stone, P., 2015, September. Deep recurrent q-learning for partially observable mdps. In 2015 aaai fall symposium series."
            },
            "questions": {
                "value": "How come the variational posterior isn't history-dependent?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7595/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7595/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7595/Reviewer_Z7MF"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698596743276,
        "cdate": 1698596743276,
        "tmdate": 1699636920369,
        "mdate": 1699636920369,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8P0lvpvY3o",
        "forum": "NiefAhgJqH",
        "replyto": "NiefAhgJqH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7595/Reviewer_Hvyz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7595/Reviewer_Hvyz"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a model-free Bayesian RL method.\n\nAs opposed to maintaining and using a posterior over the MDP (transition/reward) model, this approach attempts to directly learn the (history-based) Q-function that is optimized with a Bayesian Bellman update.\nWhile this update is defined as an expectation of the posterior over the MDP, the method avoids computing this posterior by instead learning the posterior over the Bellman update as a sufficient statistic.\n\nThis posterior is rather complex and, instead, they learn an approximation through variational inference with normalizing flow networks.\nNow given this approximate posterior over the Bellman update, a typical RNN is used to learn the Bayesian Q-function."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Bayesian RL is an important line of work that provides a solution to the exploration but suffers from computational complexity.\nAny progress on this front, as a result, should be relevant to a significant portion of ICLR's community.\n\nAdditionally, as described in the paper, while model-based methods have proliferated, model-free approaches have seen less attention.\nHence, this work is an important contribution.\n\nAn important aspect of this is the rigorous theory to support the rather novel perspective taken in the paper which is well supported (in the appendix).\nLastly, it is interesting to see that despite it being a model-free method, model-based prior knowledge is still an important aspect (which is positive because generally, those priors are much more practical to define)."
            },
            "weaknesses": {
                "value": "Two key rooms for improvement are the clarity (presentation) and experimental section.\n\nFirst, while the theoretical support in the appendix is certainly substantial, I found it rather difficult to follow key parts of the method description.\nI believe this is, first, because the (general/theoretical) learning objective and its concrete (practical, normalizing flow network approximation) implementation are presented simultaneously.\nPotential more important is the fact that the method description did not start until page 7 with background plus related work taking up much of the space which is a nice addition, but in this case in my opinion wrong priorities.\n\nIn terms of evaluation, the experiments were relatively bare (in terms of baselines an domains), limited in scope, and assumed a lot of prior knowledge.\nFor instance, the complete prior and posterior that a model-based approach would have used in the tiger problem is a distribution over two elements (the tiger is behind door 1 or door 2, with a uniform prior).\nFurthermore, all the experiments are one-shot tasks. \nMaking it hard to estimate how good the method would do with less prior knowledge and in a more typical reinforcement learning setting.\n\nAs a result, in general, I found the paper more difficult to comprehend (than I believe necessary), and overall the empirical evaluation was lacking.\nI do not know whether this is because some scaling difficulties stop the proposed solution from being tested on problems with less prior knowledge."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7595/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7595/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7595/Reviewer_Hvyz"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698997691561,
        "cdate": 1698997691561,
        "tmdate": 1699636920245,
        "mdate": 1699636920245,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5KOsl17ZbS",
        "forum": "NiefAhgJqH",
        "replyto": "NiefAhgJqH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7595/Reviewer_MKkw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7595/Reviewer_MKkw"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a new model-free framework, Bayesian exploration networks, for performing Bayesian reinforcement learning. The first contribution is to show the shortcomings of existing model-free BRL methods. They show that these existing approaches don't learn Bayes-optimal policies because they don't properly propagate uncertainty through the MDP and only solve an approximation to the true Bayesian objective. Their second contribution is to propose a solution for these issue in the form of Bayesian exploration networks. The BEN framework simplifies the objective by using a Q-function approximator to reduce the dimensionality of the input, which is then passed to a Bayesian network. This results in a fewer number of parameters over which inference must be performed. Beyond theoretical results, they demonstrate the practical performance of BENs in a search and rescue problem."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Clear communication. The authors present prior work and their own work in a clear and concise manner. The logical flow of the paper is very nice.\n- The authors are very clear about the shortcomings of prior model-free BRL methods and how exactly their proposed approach addresses these shortcomings.\n- The structure of BENs is not overly complicated. They use well-known building blocks, such as Q-function approximating functions and normalizing flows, to address the need to model uncertainty in certain parts of the method."
            },
            "weaknesses": {
                "value": "- While the authors do a nice job of reviewing prior literature, the magnitude of the contribution presented here is not clear. I am inclined to say that the importance of the authors' contributions is relatively low, although they are novel. The theoretical results showing the shortcomings of other model-free BRL approaches is arguably their most important contribution, but it's not clear that that's a sufficient contribution in isolation. I view their formulation of BENs as less impactful.\n- The solution to circumventing costly nested optimization in Algorithm 1 is questionable. I would want to see more results that this is not harmful.\n- Lack of comparison to methods. I would like to see further empirical evaluation of their approach, both in terms of environments tested and methods compared."
            },
            "questions": {
                "value": "In which practical situations would you genuinely seek to avoid existing model-free BRL methods? Do the theoretical shortcomings of existing approaches translate into material empirical shortcomings in common situations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7595/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7595/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7595/Reviewer_MKkw"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699267697418,
        "cdate": 1699267697418,
        "tmdate": 1699636920119,
        "mdate": 1699636920119,
        "license": "CC BY 4.0",
        "version": 2
    }
]