[
    {
        "id": "YYxu33YlRv",
        "forum": "8w6FzR68DS",
        "replyto": "8w6FzR68DS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1467/Reviewer_pfyb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1467/Reviewer_pfyb"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces \"PriViT,\" an algorithm designed to make Vision Transformers (ViTs) suitable for private inference using secure multi-party computation (MPC) protocols. Traditional ViTs are not ideal for MPC due to their complex non-polynomial operations. PriViT aims to reduce the nonlinearities in ViTs and preserve their prediction accuracy. It demonstrates a balance in latency and accuracy superior to existing methods, as confirmed through experiments on image classification tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is easy to follow. The paper tackles the significant challenge of making Vision Transformers (ViTs) compatible with private inference using secure multi-party computation (MPC) protocols."
            },
            "weaknesses": {
                "value": "A predominant weakness of the paper is its limited novelty, as it appears to draw heavily on core concepts previously established in SNL."
            },
            "questions": {
                "value": "1.\tWhile the paper introduces an interesting approach, its novelty appears constrained. The core concept of utilizing binary gates to manage the number of nonlinear operations through $\\ell_0$ sparsity has been proposed introduced in SNL (Cho et al., 2022b). The proposed method seems like an adaptation of SNL for ViTs. This adaptation is intriguing, but a deeper exploration into its unique contributions compared to SNL would provide more clarity on its novelty.\n\n2.\tThe paper's organization raises some concerns. While critical figures, and experimental results are put in the supplementary material, the main body of the paper exhibits noticeable empty spaces. It would enhance the paper's comprehensibility and impact if these essential elements were incorporated directly into the main content, making the narrative more coherent for readers without necessitating frequent references to supplementary sections.\n\n3.\tIn Figure 1, the computational cost is quantified using the metric of # AND gates. However, the methodology or criteria for determining the number of AND gates associated with a nonlinear operation remains ambiguous in the paper. Clarifying this calculation or providing a concise explanation within the main content would enhance the reader's understanding.\n\n4.\tThe experimental validation is primarily focused on small-scale datasets, which could limit the generalizability of the findings. For a comprehensive evaluation, it would be beneficial for the authors to include results from large-scale datasets like ImageNet. Such experiments would provide a more holistic view of the method's efficacy and scalability.\n\n5.\tBased on Figure 2, both MPCViT and MPCViT+ seem to outperform the proposed method in achieving a superior Pareto balance between accuracy and latency. This observation raises the question: what distinctive advantages or contributions does the proposed method offer? Is it perhaps more streamlined in terms of training efficiency or some other metric? A clearer delineation of the method's unique strengths would be beneficial for readers.\n\n6.\tObservations from Table 4 present some anomalies. Specifically, for PriViT-R, there is a counterintuitive trend where increased latency corresponds to a decline in performance. This is perplexing as one would typically expect a trade-off where longer computation times would yield better results. An explanation or insight into this apparent contradiction would enhance the clarity of the findings.\n\n7.\tThe clarity of notational conventions could be enhanced. For instance, the terms PriViT-R and PriViT-G appear in the text without explicit definitions or context. Providing a clear explanation or description of what these notations signify would aid in a more comprehensive understanding of the content for readers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1467/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698586064397,
        "cdate": 1698586064397,
        "tmdate": 1699636075647,
        "mdate": 1699636075647,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FP8iVmDuIm",
        "forum": "8w6FzR68DS",
        "replyto": "8w6FzR68DS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1467/Reviewer_BazG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1467/Reviewer_BazG"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces PriViT as a solution for implementing Vision Transformers~(ViTs) in private inference (PI) applications. To achieve this, PriViT tailors the nonlinear operations (e.g., GELU and row-wise softmax) of a pre-trained Transformer in order to make them compatible with secure multi-party computation (MPC) protocols. This adaptation involves the introduction of learnable switching variables to gradually replace the original nonlinear operations. The experimental results on CIFAR-10/100 and TinyImageNet demonstrate a significant improvement in both speed and performance compared to prior approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper is well-motivated as the deployment of ViTs in private scenarios is becoming increasingly important and current approaches are not tailored for Transformer architecture.\n* The proposed method is quite simple yet effective comparing with SOTA approaches."
            },
            "weaknesses": {
                "value": "[Major]\n\n1. **Experiments:** The authors have conducted sufficient comparative experiments conducted on 3 datasets (CIFAR-10/100 and TinyImageNet). However, the image resolutions are no more than $64\\times 64$, which is rather small compared with commonly-used datasets like ImageNet-1k and Caltech-101/256. It will be interesting to see ImageNet results and compare with SENet if possible.\n2. **Experiments:** The authors' exclusive use of ViT-Tiny for comparison is insufficient to establish the method's universality. It is strongly recommended that the authors broaden their evaluation to include a variety of ViT architectures, such as DeiT, and models with diverse parameters, including ViT-Small and ViT-Base. Such an expanded assessment would be of significant interest to the community, particularly in light of the flourishing development of large Transformer-based vision models.\n3. **Experiments:** In Table 3, the authors have presented PriViT and MPCViT in distinct hyper-parameter configurations, giving rise to concerns regarding the fairness of the comparisons.\n4. **Experiments:** Furthermore, I strongly encourage the authors to furnish additional results to showcase the superiority of their proposed method. This could encompass metrics like GPU memory usage and training time in comparison to other existing methods.\n\n[Minor]\n1. This article is poorly written, with issues including imprecise mathematical formulas, non-standard captions, typographical errors, and punctuation. Here, I have listed some errors and hope that the authors will carefully revise and proofread their manuscript and the appendices.\n    * Eq (1): $o=\\frac{\\mathrm{Softmax}(XW_qW_kX)}{\\sqrt{d}}W_vX$ -> $o=\\frac{\\mathrm{Softmax}(XW_qW_k^\\top X^\\top)}{\\sqrt{d}}XW_v$. Eq (4) has the same problem.\n    * Page 5, last paragraph: Once the model satisfies the required budgets,,we... -> Once the model satisfies the required budgets, we...\n    * Page 6, first paragraph: Architecture and data set. -> Architecture and dataset.\n    * Page 6, first paragraph: ViT Tiny -> ViT-Tiny\n    * Page 6, first paragraph: 224x224 -> $224\\times 224$\n    * The caption of tables should be on the top of the table: Table 3 and 5.\n    * Page 7: Pareto analysis of PriViT over Tiny Imagenet, and Cifar10/100 -> Pareto analysis of PriViT over Tiny Imagenet, and Cifar10/100.\n    * Page 7: table 4 -> Table 4, Fig 2 -> Figure 2, fig 13 -> Figure 13, figure 3 -> Figure 3, Fig. 13 -> Figure 13.\n    * Page 8: The latency is calculated as per 4.1 -> The latency is calculated as per Section 4.1.\n2. Missing dataset details in Table 1.\n3. The authors do not discuss the limitations of their method.\n4. The authors do not provide codes for reproducibility check."
            },
            "questions": {
                "value": "My questions are listed in the \"Weaknesses\" section. I am looking forward to the authors' relply."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1467/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1467/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1467/Reviewer_BazG"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1467/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744987194,
        "cdate": 1698744987194,
        "tmdate": 1699636075571,
        "mdate": 1699636075571,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CD6lSDa2Dc",
        "forum": "8w6FzR68DS",
        "replyto": "8w6FzR68DS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1467/Reviewer_HTX8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1467/Reviewer_HTX8"
        ],
        "content": {
            "summary": {
                "value": "They propose a new vision transformer architecture which is a MPC-friendly ViT. The proposed architecture optimizes the attention part with the SquaredAttn and replace the GELU function to reduce the nonlinear operations in the model. The proposed method achieve state-of-the-art performance on several image classification benchmarks and speed up the model compared with current MPCViT."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The method analysis is clear and latency breakdown is helpful.\n2. The experiments on serval image classification benchmarks are solid and comprehensive.\n3. The proposed method is speed up than previous SOTA model and achieve competitive performance."
            },
            "weaknesses": {
                "value": "1. Need more detailed about the knowledge distillation part.\n2. More discussion about non-linearity distribution."
            },
            "questions": {
                "value": "1. I am curious about the teacher model size and can it boost more performance if we use a larger teacher?\n2. As you states that later layers have a larger number of linearized units, can we first try to linearized the later layers then try to linearized the earlier layers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1467/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827670836,
        "cdate": 1698827670836,
        "tmdate": 1699636075491,
        "mdate": 1699636075491,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wkCIgNQAlh",
        "forum": "8w6FzR68DS",
        "replyto": "8w6FzR68DS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1467/Reviewer_y916"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1467/Reviewer_y916"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a new algorithm for constructing MPC-friendly vision transformers. Accuracy and latency performance were compared on several datasets with the previous result, MPCViT."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "By adjusting GELU and softmax through training using a switched method, they found a different optimal method for each layer."
            },
            "weaknesses": {
                "value": "Compared to the prior technology, MPCViT, it shows better results in the TinyImagenet but worse latency in the CIFAR-100. In terms of accuracy, it has superior performance in any case. The paper said that DELPHI is focused as the subject of comparison. \"In this paper, our focus is exclusively on the DELPHI protocol (Mishra et al., 2020a) for private inference. We choose DELPHI as a matter of convenience;\" However, the actual results do not show any performance comparison with DELPHI."
            },
            "questions": {
                "value": "It is necessary to explain in what aspects DELPHI was considered."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I have no concern"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1467/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1467/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1467/Reviewer_y916"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1467/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699089649208,
        "cdate": 1699089649208,
        "tmdate": 1699636075409,
        "mdate": 1699636075409,
        "license": "CC BY 4.0",
        "version": 2
    }
]