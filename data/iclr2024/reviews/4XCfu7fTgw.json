[
    {
        "id": "RNkGFAbW7W",
        "forum": "4XCfu7fTgw",
        "replyto": "4XCfu7fTgw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3117/Reviewer_xKM4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3117/Reviewer_xKM4"
        ],
        "content": {
            "summary": {
                "value": "This paper tackle the problem of generalization for regression tasks. It proposes a method based on metric learning assumption that the distance between features and labels should be proportional. defined as a mapping function. The proposed loss function aims at minimizing the error of the mapping function for the proportion and stabilizing its fluctuating behavior by smoothing out its variations. To enable out-of-distribution generalization, it also proposes to align the maximum singular value of the feature matrices across different domains. The paper conducts experiments on both in-distribution generalization and out-of-distribution robustness and shows that the proposed method can achieve superior performance in most cases."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The method is quite novel, and the empirical results are promising."
            },
            "weaknesses": {
                "value": "My main concern is that some related works / baselines are missing in this paper. It is not as the authors claimed that regression generalization remains relatively underexplored. Also, there are already many papers try to align the order of feature distances with the order of label distances, and they also evaluated OOD/zero-shot generalization, such as:\n\n[1] Yang et al. Delving into Deep Imbalanced Regression. ICML 2021.\n\n[2] Gong et al. RankSim: Ranking Similarity Regularization for Deep Imbalanced Regression. ICML 2022.\n\n[3] Zha et al. Rank-N-Contrast: Learning Continuous Representations for Regression. NeurIPS 2023.\n\nI think the authors should avoid claiming this paper introduces the contrastive interdependence between features and labels, and discuss about and compare with the above papers. \n\nMinor: It would be better to give your method a name, instead of  FT+L_std+L_svd."
            },
            "questions": {
                "value": "1. Does FT refer to fine-tuning in the experiments? It's better to explain it in texts. \n\n2. In the experiments, FT+L_std+L_svd seldom gets the best results over all FT methods. Is there an explanation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3117/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698440022093,
        "cdate": 1698440022093,
        "tmdate": 1699636258535,
        "mdate": 1699636258535,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eiOOQaM7wX",
        "forum": "4XCfu7fTgw",
        "replyto": "4XCfu7fTgw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3117/Reviewer_xHdj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3117/Reviewer_xHdj"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an innovative approach for generalizing regression tasks by leveraging the metric learning assumption that emphasizes the proportional relationship between features and labels. The method incorporates a std. loss and spectral loss to address two key aspects: ensuring the distance proportionality between features and labels and enabling OOD generalization. The effectiveness of the proposed method is demonstrated through experiments conducted on multiple datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This work addresses an emerging issue in regression tasks, namely the challenge of handling OOD data. As the author notes, while OOD generalization has been studied in classification tasks, it has not been explored in depth for regression tasks.\n\n2. The proposed method involves measuring the feature-label distance proportion using a mapping function and aligning real and synthesized distributions by minimizing the difference between the spectral norms of their feature representations."
            },
            "weaknesses": {
                "value": "1. The organization and statements in this paper can be unclear at times, as the authors attempt to cover a lot of ground on the topic. For example, the abstract section contains too many details that may not be necessary. In essence, the paper proposes an OOD generalization method for regression tasks, which involves two penalties to address feature-label distance proportion and distribution gap issues. However, some of the irrelevant expressions can make it difficult to grasp the main topic at first.\n\n2. The title of the paper is also unclear and does not directly convey the main theme, similar to the abstract. It lacks a clear focus and fails to capture the essence of the research."
            },
            "questions": {
                "value": "1. I am confused about why the title is \"Spectral Contrastive Regression.\" On one hand, the title does not explicitly mention OOD generalization, which is the main focus of the paper. On the other hand, the term \"spectral\" does not seem to directly relate to the contrastive loss used in the paper. While the paper introduces concepts of spectral and contrastive learning in the context of a regression task, the title may give the impression of avoiding the core content and introducing the concept of contrastive learning.\n\n2. Throughout the entire paper, the concept of contrastive learning is not emphasized enough. The term \"contrastive\" appears only five times in the main text and is not even mentioned in the abstract. While this expression may not be crucial for the technical contributions of the paper, the overall writing style feels somewhat disjointed. Unlike traditional contrastive learning, the concept is not reinforced, and even after reading about the std. loss, it is surprising to see the section titled \"Relational Contrastive Learning.\" The paper gives the impression of being written in a fragmented manner. This is just my personal perception and may not necessarily be correct.\n\n3. Building upon the previous point, I understand that the authors utilize the relationship between feature and label distances, adopting a contrasting perspective to examine this proportion and control its fluctuation by proposing a loss based on standard deviation. However, I still question why this loss and the keyword \"contrastive\" are not aligned, and instead, the paper introduces the concepts of standard deviation and the corresponding expressions in the abstract. Overall, it might be my personal bias, but I feel that the writing in this paper lacks cohesion.\n\n4. Regarding the issue with the loss function, in Equation 4, both the first and third terms measure the difference between two distributions. The former considers the MSE between the individual in-distribution of the two distributions, while the latter measures the difference between the two distributions themselves. However, it is unclear which distribution the third term specifically refers to. Does it pertain only to the real distribution or both distributions? Equation 2 appears to be a general constraint without specifying the source of i and j from each distribution.\n\n5. The related work section appears to be somewhat perfunctory, as there is not much informative content provided in the three paragraphs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3117/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3117/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3117/Reviewer_xHdj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3117/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698543323488,
        "cdate": 1698543323488,
        "tmdate": 1699636258448,
        "mdate": 1699636258448,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MnZZCO2Mk3",
        "forum": "4XCfu7fTgw",
        "replyto": "4XCfu7fTgw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3117/Reviewer_Rgn5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3117/Reviewer_Rgn5"
        ],
        "content": {
            "summary": {
                "value": "To improve the generalization of deep regression problems, the authors present a new objective composed of several ideas including relational contrastive learning, spectral alignment, and augmented sample pairs. The experiments are extensively conducted on multiple benchmarks and show improvement over baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The manuscript is clear and easy to follow.\n- The experiments show good results and are conducted on multiple different datasets.\n- The idea is technically sound and the authors present a neat combination of several different ideas to improve the generalization of deep regression problems."
            },
            "weaknesses": {
                "value": "I'm concerned about the technical novelty. Though the experiments show good improvement over baselines, in the current state of the manuscript, the proposed objective is the combination of several different terms that are similar to some existing work. Please see point 1 and point 2 in the next section of Questions. The authors may consider including more ablation studies to further solidify the technical contribution."
            },
            "questions": {
                "value": "- More ablation studies on Eq.4. The authors have conducted ablation studies on $\\alpha$ and $\\beta$. \n\n  In Eq.4, does $\\mathcal{L}_{std}$ include augmented samples? \n\n   Since $\\mathcal{L}_{mse}$ includes the augmented samples, I suggest the authors also conduct an ablation study on how much improvement is introduced by using augmented samples in mse loss term.\n\n- Missing related work. One of the core ideas in the proposed objective $\\mathcal{L}_{std}$ is that the distance between features and labels should be proportional. A similar idea can be found in deep regression problems [1], which showed similar patterns in the feature space with t-SNE visualization. The authors should properly discuss and compare the differences and similarities.\n\n- In Fig.2, $\\beta$ introduces little to no effect on the metrics for varying its values in the entire range. Do the authors have any speculation or analysis on this pattern? Because from Tables 1 and 2, single svd loss term can provide significant improvement and sometimes has the best performance. But when combined with std, it does not show a significant effect.\n\n- It can provide a full picture of how the proposed objective works if the authors can have t-SNE visualization for only svd loss term, and the sum of svd + std loss terms.\n\n\n\n\n- Minor points: it might be better for the audience if the abbreviated 'FT' can be explained as 'fine-tuning' before using it. \n\n\n\n[1] Gong et al., RankSim: Ranking Similarity Regularization for Deep Imbalanced Regression. ICML 2022"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3117/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698627936125,
        "cdate": 1698627936125,
        "tmdate": 1699636258313,
        "mdate": 1699636258313,
        "license": "CC BY 4.0",
        "version": 2
    }
]