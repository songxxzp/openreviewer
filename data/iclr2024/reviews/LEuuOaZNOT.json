[
    {
        "id": "p3PXjHpctl",
        "forum": "LEuuOaZNOT",
        "replyto": "LEuuOaZNOT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8722/Reviewer_LdZ7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8722/Reviewer_LdZ7"
        ],
        "content": {
            "summary": {
                "value": "This paper examines the learning of Boolean functions using two-layer neural networks, focusing on training only the last layer. In this particular configuration, the author initializes the first layer with a random binary vector and evaluates both mean-squared error and hinge loss as the loss functions. The authors assert that they have enhanced sample complexity while decreasing the number of iterations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The author conducted a thorough analysis of network training for Boolean functions."
            },
            "weaknesses": {
                "value": "1. The organization of the paper is lacking, necessitating substantial restructuring.\n- The problem setup demands a more detailed explanation. For instance, the fact that only the last layer is trained should be explicitly mentioned.\n- Instead of delving into the proofs of Facts 3.2 and 3.3, it would be beneficial to provide a broader understanding of the main theorem, specifically Example 4.9, 4.10, and Theorem 4.11.\n\n2. The notations used are ambiguous and inconsistent.\n- The definition of $\\sigma_1$ on page 3 is unclear.\n- The same symbol, $u$, has been used for both weight and the vectorized function $u(f(T))$ on page 7.\n- $W^k$ has not been clearly defined in Fact 3.2-3.\n- The influence of the majority function is inadequately described in Fact 3.3.\n- $f_i$ and $a_i$ are ambiguously defined on page 5.\n- The symbol $\\mathcal{J}$ is redundantly defined multiple times.\n- There is no clear definition of $\\mathcal{H}$.\n- The best degree $k$ approximation needs a more meticulous definition.\n- Neither definition 1.5 nor Theorem 1.6 is provided on page 7.\n\n3. The paper would benefit from a more comprehensive discussion and comparison with the work of Barak et al. (2022) to highlight the - unique contributions of the present study.\n\n4. The usage of contractions like \"don't\" and \"won't\" is not suitable for formal writing. It is recommended to avoid such informal language."
            },
            "questions": {
                "value": "Please check Weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8722/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8722/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8722/Reviewer_LdZ7"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8722/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698628124594,
        "cdate": 1698628124594,
        "tmdate": 1699637093858,
        "mdate": 1699637093858,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MpNtFf0c7y",
        "forum": "LEuuOaZNOT",
        "replyto": "LEuuOaZNOT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8722/Reviewer_cvKh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8722/Reviewer_cvKh"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of learning Boolean functions and particularly parities with neural networks. More specifically, they consider a random features model trained with $\\ell_2$ or hinge loss and they show that parities in dimension $n$ with degree $k$ can be learned with error $\\epsilon$ assuming that the model has $n^{O(k)}poly(\\frac{1}{\\epsilon})$ samples and features and $n^{O(k)}\\log(\\frac{1}{\\epsilon})$ iterations. \n\nThe paper further shows that in the provided training setting, lower degree monomials are learned faster than higher degree monomials."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The result showing that lower degree features are learned faster than higher degree features is interesting, although the proof is limited to the proposed training setting.\n- The learning results are applicable to many activation functions."
            },
            "weaknesses": {
                "value": "- In the abstract, the paper says that previous analyses of GD use the convexity of the problem while most loss functions are highly non-convex. Indeed, in this paper the loss functions considered are $\\ell_2$ and hinge that are both convex. Further, the model is random features which makes the training linear and convex. (The paper also shows the convergence by almost proving a descent lemma.)\n- Similarly, the paper mentions that the current theoretical results for Boolean functions are not close to the practice. While, in this paper, the training is on random features model with a large batch size. I personally think this method is even further from practice (compared to the works that employ layer-wise training for example). Note that in previous years, there have been many results showing the superiority of feature learning regimes over training in the kernel regime (including random features model)."
            },
            "questions": {
                "value": "- Q1. Can the asymptomatics be made more precise? For example, the current version of the paper presents them as $n^{O(k)}$, but can it be made more exact, e.g., $O(n^{2k})$?\n- Q2. How does the asymptotics (e.g., number of samples/features/iterations) compare to other results for kernel methods and other recent works such as [1]? For example, [1] discusses how one can compensate for a low number of iterations/samples/parameters by keeping the two others high. \n- Q3. In the last paragraph of page 13, one has to prove $\\frac{\\zeta_k^2}{\\zeta_l^2}= \\Omega(n^{l-k})$. From fact 3.2, we have a lower bound of $\\zeta_k \\geq n^{-O(k)}$, but what is the upper bound for $\\zeta_l$?\n\n\n[1] Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck -- https://arxiv.org/pdf/2309.03800.pdf"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8722/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8722/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8722/Reviewer_cvKh"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8722/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840264720,
        "cdate": 1698840264720,
        "tmdate": 1699637093739,
        "mdate": 1699637093739,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "K8EONMYTqc",
        "forum": "LEuuOaZNOT",
        "replyto": "LEuuOaZNOT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8722/Reviewer_gUqw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8722/Reviewer_gUqw"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes the learnability of Boolean functions using neural nets with a single hidden layer, and Relu activations when trained with gradient/hinge loss using gradient descent. They show that it performs fairly well learning parities of size $k$ in time close to the SQ lower bound of $n^\\Omega(k)$. For arbitrary functions, they can prove learnability as good as the best low degree approximation, assuming certain spectral properties of  the transfer function."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. I think the problem they study is interesting and important. Boolean functions are a natural setting where one can hope to rigorously analyze the kind of behavior that is empirically seen to hold for neural nests (like simple features are composed to get more complex ones).\n\n2. The results seem technically quite solid. While I have not verified the details, it seems like the push the envelope of Boolean Fourier analysis by developing tools to analyze transfer functions like Relu. This might turn out to be important in the long run, encouraging more experts in this area to think about DNN-related problems. \n\n3. The results themselves are interesting and solid  if not too  surprising."
            },
            "weaknesses": {
                "value": "The main weakness of the paper is its presentation, which I found very hard to penetrate (and I have worked in this area before). The notation is excessively heavy, the flow of results is not clear, a lot of heavy math is typeset inline in definitions and theorem statements. . It appears to me that the paper was written in a hurry. In its current form, it is inaccessible even to experts, leave alone non-experts.I feel the results are important enough that the paper ought to be substantially rewritten, and resubmitted (to a top venue). I cannot recommend acceptance in the current form."
            },
            "questions": {
                "value": "I will highlight just a couple of examples:\n\n1. In defining $h^{\\leq k}_{even}$ you say the best approximation with even degree and degree 1 support. Can you state more precisely what this means? Presumably if $k$ is even, this is just the level $k$ truncation? By even degree, I assume you mean that the highest degree term is even? In which case, why do we need to specify \"Degree 1 support\" separately? Or do you mean projecting the spectrum onto the even weight subsets, while also keeping the degree one terms?\n\n2. It might be worth highlighting that $J(g)$ is a convolution operator. Isn't Proposition 4.1 just a consequence of the standard fact that convolution in the time domain is pointwise multiplication in the Fourier domain? \n\n3. Remark 4.8 claims that Def 4.7 is \"equivalent to: there is some $u$ so that $|\\hat{\\sigma}_u(S)| \\geq \\alpha $ or any $s \\in S$. This statement does not parse. Perhaps you meant \"for any $s \\in S$ and not \"or\"? That statement does parse, but it is not equivalent to Def 4.7 which requires the condition to hold for all $u$. \n\n4. There are too many theorem statements in section 4, and their organization could perhaps be better. The one line proofs for Theorem 4.3, 4.4 etc are not very helpful to the reader to understand what is going on. If the main results are Theorem 4.11 and 4.12 then one might state those first, give some intuiton for their proofs, and derive the other results as corollaries of these results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "none"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8722/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8722/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8722/Reviewer_gUqw"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8722/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699391988114,
        "cdate": 1699391988114,
        "tmdate": 1699637093621,
        "mdate": 1699637093621,
        "license": "CC BY 4.0",
        "version": 2
    }
]