[
    {
        "id": "hPd1d0jaom",
        "forum": "vE8Vn6DM0y",
        "replyto": "vE8Vn6DM0y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6647/Reviewer_CbtZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6647/Reviewer_CbtZ"
        ],
        "content": {
            "summary": {
                "value": "In this work the researchers aim to understand human language processing by building encoding models predicting neural responses based on LLM contextual embeddings. Studying shared stimuli interpretations among subjects, the study investigates common information spaces using LLM-based encoding models and compares them with original brain responses. The research uses neural responses from eight subjects listening to a podcast, employing a Shared Response Model (SRM) to aggregate data and identify shared stimulus features. Results show significantly improved encoding performance using the SRM-calculated shared space compared to individual brain data. This shared space is also used to denoise individual responses, leading to substantial performance enhancements. The study demonstrates generalizability to new subjects and highlights the effectiveness of shared space in isolating stimulus-related features, particularly in brain areas specialized for language comprehension, offering deeper insights into human language processing."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The combination of utilizing Large Language Models (LLMs) and the Shared Response Model (SRM) to explore common information spaces in human brain processing seems to be a novel approach. The significant improvements in encoding performance and the ability to denoise individual responses using shared space, are noteworthy."
            },
            "weaknesses": {
                "value": "The paper lacks clarity regarding whether it introduces new methods or replicates Goldstein et al.'s (2022) approach. This ambiguity hampers a clear understanding of the paper's contributions.\n\nThe paper misses an opportunity to explore the impact of scale by not conducting the study on smaller auto-regressive models. With the vast parameter difference between GPT-2 XL and GPT-4, assessing the performance of smaller models would provide valuable insights into scalability and generalizability.\n\nThe impact of findings or what they entail in this field of study is not clearly described. For example other works have been carried out studying shared response for fMRI."
            },
            "questions": {
                "value": "Is the paper introducing any new methods, or is it simply replicating the approach outlined in Goldstein et al. (2022)? The methodology section lacks clarity, making it unclear whether the same setup as Goldstein et al. is being utilized. It is crucial to differentiate your work from previous research and provide appropriate citations for the foundational work you are building upon.\n\nIt would have been valuable to observe the study conducted on smaller auto-regressive models to assess the impact of scale (i.e. using large models). Btw, GPT-2 XL, with 1.5 billion parameters, in comparison to GPT-4's 1.76 trillion parameters, raises questions about whether it can still be classified as a Language Learning Model (LLM) today.\n\nWhy do you need to apply dimensionality reduction to the extracted representation? Does that not reduce the representation power of the model? There has been work showing that applying PCA to the repsentations achieve sub-optimal performance. Also some of the relevant knowledge may not be present at the final layer. Did you look at the representations from the earlier layers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6647/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697716674319,
        "cdate": 1697716674319,
        "tmdate": 1699636759566,
        "mdate": 1699636759566,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "l4l76kxjw7",
        "forum": "vE8Vn6DM0y",
        "replyto": "vE8Vn6DM0y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6647/Reviewer_MBTm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6647/Reviewer_MBTm"
        ],
        "content": {
            "summary": {
                "value": "This paper offers new insights into how our brains understand language. This work examines the neural responses of eight subjects as they listened to a podcast. Using a method called the Shared Response Model (SRM) on each subject's neural data, a common shared information space is learned across subjects. This shared information space, along with the contextual embeddings for each word in the podcast, is then used to build an encoding model. Experimental results indicate that the shared space derived from the SRM provides better encoding performance than the original brain responses."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Clear description of background knowledge and motivations needed to understand the study.\n2. Clear exposition of the proposed model.\n3. The paper demonstrates that aligning brain responses across subjects into a shared space significantly enhances the encoding performance.\n4. Experimental demonstrates that the SRM model is generalizable to new subjects suggesting the model's robustness and adaptability."
            },
            "weaknesses": {
                "value": "1. The paper lacks implementation details under the Shared Response Model (SRM). How were S and W jointly estimated? What method was used to estimate W? Was it solved under the orthogonal Procrustes problem?\n2. Could the experimental results be compared against other baselines?"
            },
            "questions": {
                "value": "1. So what happens in the case where you don't have initial parallel data between the two spaces under the Shared Response Model (SRM) could the linear transformation (W) be learned in an unsupervised through optimal transport?\n2. How does the Shared Response Model (SRM)  scale to a larger subject size?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6647/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6647/Reviewer_MBTm",
                    "ICLR.cc/2024/Conference/Submission6647/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6647/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742672072,
        "cdate": 1698742672072,
        "tmdate": 1700638511232,
        "mdate": 1700638511232,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OBzzjqg6jv",
        "forum": "vE8Vn6DM0y",
        "replyto": "vE8Vn6DM0y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6647/Reviewer_6UCh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6647/Reviewer_6UCh"
        ],
        "content": {
            "summary": {
                "value": "This study aims to investigate whether the shared information space of subjects' brain data can be more accurately predicted than the original brain data using contextual embeddings derived from large language models like GPT-2. The primary contributions can be summarized as follows: a language encoding model was trained to predict original neural story responses and also predict shared neural responses across subjects using a shared response model derived from story representations. This resulted in a higher predictive accuracy for shared responses compared to original responses.\nSubsequently, the language encoding model was employed to attempt the prediction of denoised individual brain responses by projecting them back into the neural space. The accuracy of predictions improved for denoised responses compared to the original responses. A detailed analysis of language ROI (Region of Interest) revealed that the superior temporal gyrus (STG) and inferior frontal gyrus (IFG) showed improvements over other regions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe idea of shared response model is a well-known technique and authors demonstrate that aligning brains into a shared space significantly improves the encoding performance.\n2.\tAdditionally, the authors' examination of their encoding model's performance on ECoG recordings is intriguing, considering that prior studies predominantly concentrated on fMRI."
            },
            "weaknesses": {
                "value": "1.\tThe novelty in the paper is somewhat limited as it primarily delves into a comparison between shared responses, original responses, and denoised responses. Notably, the shared response model itself isn't a new concept.\n2.\tThe implications of this paper are unclear to me. Given the authors' emphasis on shared responses rather than original neural responses, the role of contextual word representations remains ambiguous. If the authors intended to present findings related to Large Language Models (LLMs), they could have explored a comparison of different word embeddings such as GloVe or Word2Vec alongside word representations from LLMs to assess their performance. However, the authors solely utilized word representations from LLMs and concluded that LLMs performed better.\n3.\tSimilarly, authors can explain what the percentage of shared information across subjects across regions is. Also, the comparison of estimated noise ceiling vs. shared responses is more interesting.\n4.\tThe authors concentrate solely on contextual word representations from LLMs, but it's worth noting that there exists a rich language-hierarchy across layers, and layer-wise representations contain a wealth of information (such as POS tags, NER, and dependency tags) that may also be pertinent to neural brain responses. These findings provide substantial evidence across various types of information and shared responses among individual subjects.\n5.\tthe clarity can be improved:\n6.\tFigure 6 result is difficult to parse because it contain a lot of information. Also, check the typo in Figure 6: Relaive -> Relative, Predictibility -> Predictability"
            },
            "questions": {
                "value": "1. What the percentage of shared information across subjects across regions is? Is it possible to differentiate good vs. bad subjects?\n2. The comparison of estimated noise ceiling vs. shared responses is more interesting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6647/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698932465921,
        "cdate": 1698932465921,
        "tmdate": 1699636759333,
        "mdate": 1699636759333,
        "license": "CC BY 4.0",
        "version": 2
    }
]