[
    {
        "id": "tqwsNiohJI",
        "forum": "QNW42cjkym",
        "replyto": "QNW42cjkym",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2907/Reviewer_4VCU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2907/Reviewer_4VCU"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an empirical study on active learning on medical image datasets to address the cold-start issue. By pre-trained an encoder with contrastive learning, unlabeled data is projected into the embedding space and furthest point sampling (FPS) is applied to select a subset of samples for manual annotation. These annotated samples are then used to train a classifier for the target task. In experiments, 4 medical image datasets are adopted to evaluate the performance of the active learning pipeline."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "(1) This work addresses a very challenging problem in medical imaging examination. Since annotation in the medical domain may be expensive, the cold-start problem is more severe than natural image analysis. \n\n(2) The paper is well-written and easy to follow.\n\n(3) It is nice to have multiple medial datasets from different imaging modalities."
            },
            "weaknesses": {
                "value": "My biggest concern for this paper is the need for more innovation. All building blocks in the active learning cycle, including contrastive learning for encoder pertaining, sample selection metric for annotation, and discriminator training, are well studied in prior works. The contribution is about the empirical study and analysis. However, neither the study nor its discussion is comprehensive.\n\nMore explanations on selecting various building blocks in the active learning paradigm are required. For instance, is there a particular reason for selecting contrastive learning to pre-train the endear? Aside from contrastive learning, there are many self-supervised learning methods, such as reconstruction and masked auto-encoder, why does the study focus on evaluating contrastive learning methods here? In addition to FPS, many sample selection metrics like entropy, marginal entropy, etc., are proposed in prior arts. Why FPS is selected? What are their advantages compared to other methods?"
            },
            "questions": {
                "value": "Please refer to the Weakness section for my questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2907/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698424409607,
        "cdate": 1698424409607,
        "tmdate": 1699636234292,
        "mdate": 1699636234292,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZtA8doXDRJ",
        "forum": "QNW42cjkym",
        "replyto": "QNW42cjkym",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2907/Reviewer_eBNR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2907/Reviewer_eBNR"
        ],
        "content": {
            "summary": {
                "value": "This work studies the cold start problem in challenging real-world biomedical datasets. This work investigates three self-supervised learning for pre-training to embed the unlabeled data in a meaningful latent space. Then, they explore four sampling methods to select the most informative initial data points for labeling. Lastly, they use model soups to alleviate the reliance on the validation set. The pipeline is evaluated on three biomedical datasets."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The pipeline is clearly presented from pre-training to data selection for labeling for the task. This work studies three different types of self-supervised learning for pre-training. Algorithm 1 and Algorithm 2 are easy to follow and understand."
            },
            "weaknesses": {
                "value": "1. The biggest concern I have is about the technical novelty. All of the modules have been established in the prior works. \n2. Self-supervised learning should be pre-trained on a large-scale dataset to learn diverse representations. However, the datasets in this work seem very small. I have doubt about the power of SSL in this setting. On the other hand, how about using a large-scale pre-trained SSL on biomedical images instead? It is more interesting to study the gap between the pre-trained SSL and the public weights of SSL. Morever, what is the advantage/effect of SSL on cold-start problem? Can we directly use semi-supervised learning as one-stage pipeline instead of multi-stage pipeline proposed in this work? \n3. The motivation is not convincing too me. If the aim is to reduce labeling effort, could we also try active learning, few-shot learning for this work? What is the benefit of the multi-stage pipeline proposed here?"
            },
            "questions": {
                "value": "1. Typo in the first paragraph: \u201dHow many labels should be \u201cHow many labels...\n2. Figure 8 is hard to understand \n3. Figure 10 is also hard to understand for the efficiency of model soups on the validation set"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2907/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2907/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2907/Reviewer_eBNR"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2907/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698565777203,
        "cdate": 1698565777203,
        "tmdate": 1700459271761,
        "mdate": 1700459271761,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "abFDI1qBqA",
        "forum": "QNW42cjkym",
        "replyto": "QNW42cjkym",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2907/Reviewer_GNyG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2907/Reviewer_GNyG"
        ],
        "content": {
            "summary": {
                "value": "Addressing the problem of lack of sufficient annotations for biomedical contexts ( the cold start problem).\n\nProposed framework is mentioned as containing three key components: (i) Pretraining an encoder using self-supervised learning to construct a meaningful data representation of unlabeled data, (ii) sampling the most informative data points for annotation, and (iii) initializing a model ensemble to overcome the lack of validation data in such contexts"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper claims to be the first to identify and address the cold start problem. The remaining strengths of the paper appears to be primarily in terms of the application and adaptation of some existing self-supervised learning techniques, often without substantial modifications, for representation learning, followed by furthest point sampling for identification of most representative data points. The methodological novelty is unclear, although the ablation studies are good and the application chosen is socially important."
            },
            "weaknesses": {
                "value": "Unclear methodological novelty, with the primary USP of the paper being in the application of a set of existing techniques to biomedical data sets. The descriptions of the adaptations used for the sampling processes used, and also the other technical methodologies, is unclear and the adaptations made for the unique challenges in the datasets used (with respect to issues like artefacts, spurious labels etc) has not been sufficiently mentioned. Overall, this paper may be a better fit as a longer form journal paper with better descriptions of techniques used and novelties pursued."
            },
            "questions": {
                "value": "1. methodological novelty beyond the extant techniques in literature?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2907/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698776504586,
        "cdate": 1698776504586,
        "tmdate": 1699636234140,
        "mdate": 1699636234140,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RmnwbhKFJp",
        "forum": "QNW42cjkym",
        "replyto": "QNW42cjkym",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2907/Reviewer_yiXC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2907/Reviewer_yiXC"
        ],
        "content": {
            "summary": {
                "value": "The paper tackles the challenge of acquiring quality annotated images for effective deep learning classifiers in biomedicine. The authors propose a solution for the \"cold start\" problem by pretraining an encoder with self-supervised learning, selecting informative data for annotation, and using a model ensemble. Their approach outperformed existing methods in all tested datasets, notably achieving a 7% improvement in leukemia blood cell classification, and offers an efficient solution to biomedical deep learning challenges."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The study is the first to address the cold start problem in challenging real-world biomedical datasets.\n\nThe research quantitatively compares three state-of-the-art self-supervised learning methods and identifies SimCLR as the most suitable technique for biomedical data representation.\n\nThe study conducts a comprehensive ablation study to evaluate four sampling strategies, highlighting furthest point sampling (FPS) as the most effective method for identifying representative biomedical data points.\n\nThe introduction of the model soups technique offers a novel solution to the challenges of dealing with an unreliable validation set and class distribution information."
            },
            "weaknesses": {
                "value": "Elaborate on Significance: It would be helpful to briefly explain the significance of addressing the \"cold start\" problem in the biomedical domain. Why is this problem important, and how does your proposed solution contribute to solving it?\n\nTable 1: While furthest point sampling (FPS) outperformed other sampling methods, it exhibited relatively low performance compared to using the full dataset. Providing a discussion or explanation for this performance gap would be valuable."
            },
            "questions": {
                "value": "Figure 3: The imbalanced distribution of cell types makes it challenging to visualize all cell types.\n\nTable 2: Regarding the Retinopathy dataset, all sampling methods produced identical results, which were indistinguishable from those obtained through random sampling. Could you explain this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2907/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698963001981,
        "cdate": 1698963001981,
        "tmdate": 1699636234058,
        "mdate": 1699636234058,
        "license": "CC BY 4.0",
        "version": 2
    }
]