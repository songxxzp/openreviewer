[
    {
        "id": "rb5e9HeBdR",
        "forum": "gU6OqJfO0G",
        "replyto": "gU6OqJfO0G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2965/Reviewer_vspK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2965/Reviewer_vspK"
        ],
        "content": {
            "summary": {
                "value": "The authors study the problem of graph incremental learning, where (a batch of) nodes arrive at each time step. We hope to update our model efficiently in this setting as in the standard incremental or online learning setting. The authors claim that this problem is not ``learnable`` when the structural shift is not controlled. They propose a replay-based method to mitigate the effect of structural shift."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-\tThe studied problem is important.\n-\tThe extensive experiments show that the proposed method outperforms prior works."
            },
            "weaknesses": {
                "value": "-\tThe clarity, especially for the theoretical results, can be improved.\n-\tThe Theorem 2.3 seems problematic. See the counter-example below.\n-\tIt is not clear why the proposed method can mitigate the issue of ``structural shift``.\n\nI really like to convey ideas and it is great to see the authors attempt to provide some analysis to understand the problem of Node-wise Graph Incremental Learning (NGIL). However, I find it hard to understand the impossibility result of NGIL and why the proposed method can solve the issue. For instance, what do the authors mean by a **good** classifier $f$ in Theorem 2.3? What is exactly the setting of available training data information we can use at each time step considered in Theorem 2.3? Clearly, it has to rule out the case of retraining from scratch (denoted as Joint Training by the authors) otherwise Theorem 2.3 makes no sense. Unfortunately, I cannot see where and how Theorem 2.3 rules out this method. Also, the authors assume that we can sample the $k$-hop ego-subgraph for all nodes in $\\mathcal{V}_i$ at each time step $i$. Notably, the statement of Theorem 2.3 is independent of the choice of $k$. As a result, if we set $k$ sufficiently large, then getting $g_v$ is equivalent to getting the entire graph. Thus, retraining from scratch is included in this scenario. I feel there must be some other assumption in order to make Theorem 2.3 reasonable, and I hope the authors can state them clearly. \n\nOn the other hand, even if the conclusion of Theorem 2.3 is correct the main issue is the structural shift being uncontrolled. The authors claim that their method can mitigate this issue, which sounds weird to me. Note that the structural shift is defined by how a new node (or batch of nodes) is added to the current graph, which changes the graph topology. This is definitely not controllable in practice, as this process depends on the nature of the data but not the algorithm design. I am confused as to why the authors can claim that their method mitigates this issue.\n\nFinally, I wonder how the problem of NGIL is related to graph unlearning [1,2]. If we do not care about the privacy issue in the graph unlearning, essentially NGIL is the reverse direction of graph unlearning. Then for a simple problem and model, the technique in [1,2] seem also provably applicable. I wonder if Theorem 2.3 contradicts the finding of the machine unlearning literature.\n\n## References\n[1] Efficient Model Updates for Approximate Unlearning of Graph-Structured Data, Chien et al. ICLR 2023.\n\n[2] Efficiently Forgetting What You Have Learned in Graph Representation Learning via Projection, Cong et al. AISTATS 2023."
            },
            "questions": {
                "value": "Please check my comments in the weaknesses section. In summary, my questions are:\n\n1.\tWhat does ``good $f$`` mean in Theorem 2.3?\n\n2.\tWhy Theorem 2.3 is correct? Isn\u2019t retraining from scratch a counter-example?\n\n3.\tSince Theorem 2.3 does not depend on the number of hop $k$. If we choose $k$ sufficiently large (i.e., larger than the diameter of the graph), then essentially getting $g_v$ means we get the entire graph, and thus retraining from scratch must be included. Do the authors miss some assumptions for Theorem 2.3 to hold?\n\n4.\tWhy the proposed method can mitigate the structural shift? Isn\u2019t this dependent on the data nature that we have no way to control?\n\n5. Does Theorem 2.3 contradict with graph unlearning literature [1,2]?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2965/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698522050892,
        "cdate": 1698522050892,
        "tmdate": 1699636240423,
        "mdate": 1699636240423,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "p71BoU4r4n",
        "forum": "gU6OqJfO0G",
        "replyto": "gU6OqJfO0G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2965/Reviewer_MQqi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2965/Reviewer_MQqi"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a theoretical examination of the learnability of Node-wise Graph Incremental Learning (NGIL) in dynamical settings. Specifically, the paper presents that NGIL is not always learnable under uncontrolled structural changes. \nBased on this analysis, the paper presents a technique, the Structure-Evolution-Aware Experience Replay (SEA-ER) \nto control structural shifts with a sample selection that uses topological information of the GNN with importance re-weighting. \nIn the experiments, three real-world datasets and synthetic data are used to evaluate the proposed method (SEA-ER) and compare it to existing experience replay NGIL frameworks. It evaluates the impact of structural shift (dynamics of graph structure) and that the distortion rate is small for the datasets. Finally, the paper also presents a meta-analysis of the model with the corresponding ablation study on the size of the experience replay buffer."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper has several strong points. The NGIL learnability problem presents an interesting re-interpretation of the effect of cyclic probabilistic dependencies of nodes and node attributes in an evolving graph. Creating a formulation that is relatively agnostic to the mechanics of how those probabilities were produced is interesting. Theorem 2.3 is the main theoretical contribution of the paper and has some intuitive components to it, particularly concerning the fact that the impossibility of learning unconstrained dynamics.  However, this is in itself a subtle line, since the probabilistic causes for this are not fully described in the paper. Other strengths of the paper include its organization (although the clarity could be improved) and the meta-analysis."
            },
            "weaknesses": {
                "value": "The main weakness of this paper is that central to the argument is the content of Theorem 2.3. I have my reservations about this result not because it may not be true (I think is true) but because this result could be traced back to the cyclic dependencies of the node probabilities that are ultimately the contributors to the structural shift. However, this is not indicated in the paper. The paper also has several imprecisions in the descriptions for example, the metric $r_{i,j}$ is only briefly indicated to be accuracy in the \"Evaluation Metric\" subsection between parentheses with a \"e.g.\" before stating it. Thus, it reads like an example and not a definitive fact. However, this is not fully confirmed later in the paper. The choice of accuracy as a metric, itself could be considered a little problematic for the problem at hand. A graph is a sparse topological mathematical entity and accuracy alone may not be the most appropriate for the task. Finally, since Theorem 2.3 is the core of the contribution the proof cannot be relegated to the appendix.\n\nMinor comment. The caption for the Figure in page 8 is missing."
            },
            "questions": {
                "value": "I would appreciate if you could clarify the points I raised in the weakness section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2965/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698646963811,
        "cdate": 1698646963811,
        "tmdate": 1699636240350,
        "mdate": 1699636240350,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Shaz2uBedU",
        "forum": "gU6OqJfO0G",
        "replyto": "gU6OqJfO0G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2965/Reviewer_Fnb6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2965/Reviewer_Fnb6"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on the challenges posed by Graph Incremental Learning (GIL), particularly within the context of Node-wise Graph Incremental Learning (NGIL). Traditional Graph Neural Networks (GNNs) are typically modeled for static graphs. However, many real-life networks, such as citation and financial networks, are dynamic, evolving over time. This dynamic nature results in challenges like catastrophic forgetting, where newly acquired knowledge supersedes prior learning. The paper delves deeply into the learnability of NGIL, where tasks are sequential, and the graph structure changes with each new task, giving rise to what is termed a \"structural shift.\" Experimental results from various datasets showcase the efficacy of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1. The study is well-motivated, with a comprehensive review of related work.\nS2. The paper's content is easy to understand and follow.\nS3. Experiments conducted on real-world datasets demonstrate the effectiveness of the methods proposed."
            },
            "weaknesses": {
                "value": "W1. The problem setting is not novel. My first concern is the novelty of the problem setting. Node-wise Graph Incremental Learning has been extensively studied by previous works\n\nW2. The technical contributions, while sound, seem limited. The method presented integrates an experience buffer and importance re-weighting to tackle challenges such as catastrophic forgetting and structural shifts. However, using an experience buffer for stream data isn't a new concept, and many works have already explored it. The reviewer finds the technical contributions of this section somewhat limited.\n\nW3. The theoretical innovation appears to be minimal. Once the input graph is broken down into a series of ego-graphs, the definitions, formulations, and theoretical underpinnings seem like straightforward adaptations from their IID data counterparts.\n\nW4. The writing requires refinement:\nThe use of \"bf\" before \"Node-wise Graph Incremental Learning\" appears to be a formatting mistake.\nThe definition of distortion, as presented in Definition ??, is incomplete or missing.\n\nW5. Lack of   time complexity analysis. it would be beneficial to compare the overall time complexity of the entire framework to that of the baselines and to provide insights into the runtime of the proposed method."
            },
            "questions": {
                "value": "See the above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2965/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2965/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2965/Reviewer_Fnb6"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2965/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698787058739,
        "cdate": 1698787058739,
        "tmdate": 1699636240270,
        "mdate": 1699636240270,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gFhZPXMsoX",
        "forum": "gU6OqJfO0G",
        "replyto": "gU6OqJfO0G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2965/Reviewer_tgji"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2965/Reviewer_tgji"
        ],
        "content": {
            "summary": {
                "value": "The paper is about incremental learning of graphs where increment happens in terms of nodes. They propose a plan to solve catastrophic forgetting in incremental learning. The idea is to subsample from historic evidence and reuse them as replay. The paper also provides theoretical analysis of the method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Learning from graph in incremental setup is an important problem. \n2. The idea of reusing older samples as revision seems ok.\n3. The authors have provided some theoretical analysis too."
            },
            "weaknesses": {
                "value": "1. The paper is written with unnecessary formalism in some cases which makes it harder to read. \n2. The main idea and the rational could have been presented in a more straightforward manner.\n\n3. However the main concern is using some samples again and again. Although it may appear to be replay or revision but it has to be critically analysed how revisiting some samples is justified. \n4. It is not clear if there is some unlearning and relearning effect is there or not. \n5. It is also not clear how such replays deviate the overall learning objective. \n6. How does the objective or the learning path change with respect to the order of increments of the graphs ? \n7. How does the final solution change if all increments are available at the same time ?"
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2965/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2965/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2965/Reviewer_tgji"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2965/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698860857691,
        "cdate": 1698860857691,
        "tmdate": 1699636240204,
        "mdate": 1699636240204,
        "license": "CC BY 4.0",
        "version": 2
    }
]