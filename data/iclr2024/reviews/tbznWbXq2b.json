[
    {
        "id": "7BjKEhX2iQ",
        "forum": "tbznWbXq2b",
        "replyto": "tbznWbXq2b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission745/Reviewer_iSGe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission745/Reviewer_iSGe"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a federated learning framework, which utilizes pre-trained generative models to synthesize data for server model training and then finetuning client models via local data. The proposed framework is validated on three datasets and shows higher performance than the compared methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- How to use synthetic data in FL is a relevant and important topic.\n- The paper is well-organized and easy to follow.\n- The authors analyzed the method from many aspects."
            },
            "weaknesses": {
                "value": "- The technical contribution is incremental; the framework mainly applies existing generative models.\n- The methodology is agnostic to the FL framework. The generative-model-based pretraining can be done without FL. The specific relation with FL needs to be justified.\n- It is not clear how to evaluate the quality of data generated by this framework.\n- The motivation for why clients choose to finetune models in a federated way rather than locally is not justified. Better add the performance of performing local finetuning based on the GPT-pretrained server model.\n- The comparison may not be proper. Some FL methods are agnostic to the pretraining. A better way would be to combine the GPT-FL and these methods to show the relative improvements.\n- Fig.2 may not be fair. These methods start from scratch, and it is intuitive that these methods would require more communication to converge.\n- The fourth finding that \u2018downstream model generated by synthetic data controls gradient diversity during FL training\u2019 is not new. This has been discussed by Nguyen et al.[1]\n\n[1] Nguyen, John, Jianyu Wang, Kshitiz Malik, Maziar Sanjabi, and Michael Rabbat. \"Where to begin? on the impact of pretraining and initialization in federated learning.\" ICLR, 2022."
            },
            "questions": {
                "value": "- Why does the MOON method involve the use of public data? The main idea of MOON is contrastive learning; the claim may not be correct and needs to be further justified.\n- Why does FedOPT underperform 3x synthetic on image data but outperform it on audio data?\n- Why change the model to ResNets in Table 4? Here needs to be further clarified.\n- Using synthetic data seems promising. It would be interesting to further explore the effects of training larger models such as transformer-based methods.\n- Does this method still work for more finer tasks? E.g., detection or segmentation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission745/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission745/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission745/Reviewer_iSGe"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission745/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698593729809,
        "cdate": 1698593729809,
        "tmdate": 1699636001570,
        "mdate": 1699636001570,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YQHw2QGtev",
        "forum": "tbznWbXq2b",
        "replyto": "tbznWbXq2b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission745/Reviewer_4UJh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission745/Reviewer_4UJh"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new approach for Federated Learning involving the use of synthetic data to assist model training. In particular, the proposed method uses the GPT model to generate image/audio prompts given label description, and then passes the prompt to text-to-image/text-to-audio generative models to create synthetic data. This dataset is then used to pre-train a downstream model that acts as an initializer to the FL training process. The paper shows empirical evidence that this improves FL performances."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I think this is a very interesting and novel approach to generally augment predictive models (not just in FL context).\n\nThe paper is clearly written and is very well motivated. Experiments are well thought out, thought provoking and yield promising results."
            },
            "weaknesses": {
                "value": "Regarding soundness, the motivation and the proposed method are very straight forward. I do not have any problem with the technical approach.\n\nHowever, I think there should be some extra empirical study to better understand when the proposed method will and will not work. Below are several suggestions, which are not necessarily the weaknesses of this paper (although would be interesting if addressed/investigated).\n\nGPT-FL seems to do very well on standard vision benchmark. How do we know that the generative model was not trained on these benchmark previously? Is there a chance that the StableDiffusion model has ingested enough test data and thus will provide an advantage to the downstream model? \n\nI would suggest measuring the Fr\u00e9chet Inception Distance to the training/test images (or some similar generative model evaluation metrics) to get a better understanding. I am not sure how to do that with audio data, but my hypothesis is that the closer the synthetic data to the training/test distribution, the better GPT-FL will perform -- which could explain Table 3.\n\nAnother ablation study that the authors can perform is to conduct FL training on tasks that are more domain specific (e.g., medical images) and hence less likely to overlap with the training data of the respective generative models.\n\nThe authors have conducted ablation studies to demonstrate two key points: (1) Centralized training with synthetic data alone cannot completely replace FL, which benefits from private siloed data (Table 3, downstream model vs. standard FL); (2) Synthetic data improves FL performance (Table 4, standard FL vs. FL initialized by downstream model). Those are important points, but I think it would also be interesting to investigate if each client can just fine-tune the downstream model in isolation and achieve the same level of performance with GPT-FL (i.e., local learning initialized by downstream model vs. FL initialized by downstream model). If this is the case, then there is no need to frame this contribution in the FL setting. It would instead be a new approach to synthesize a pre-trained model without data.\n\nI find this paper which has a similar idea \"PS-FedGAN: An Efficient Federated Learning Framework Based on Partially Shared Generative Adversarial Networks For Data Privacy\" (Wijesinghe et al., 2023). The main difference is that they do not assume having a generative model at their disposal, and instead try to learn one. Do you foresee this would work better in the case where the FL task data are OOD of the pretrained generative model?"
            },
            "questions": {
                "value": "I have put all my concerns above in question form."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission745/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698694800257,
        "cdate": 1698694800257,
        "tmdate": 1699636001506,
        "mdate": 1699636001506,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PGE7QVOD6W",
        "forum": "tbznWbXq2b",
        "replyto": "tbznWbXq2b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission745/Reviewer_p1Ep"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission745/Reviewer_p1Ep"
        ],
        "content": {
            "summary": {
                "value": "The present paper proposes a learning algorithm using generative model to enable the proposed algorithm to cope with heterogenous data distribution among clients. Specifically, the paper proposes that a central server employs a generative model to generate synthetic data samples. Then the synthetic data samples are used to train a global model at the server. Then the server sends the global model to clients and clients fine-tune the global model using their local data to obtain their personalized local models. The paper conducts extensive experiments on various tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper conducts extensive experiments to evaluate the performance of the proposed algorithm."
            },
            "weaknesses": {
                "value": "I do not agree with authors that the proposed framework is a federated learning method. To train the global model at the server, the server does not need the collaboration of clients. In fact, participation of a client does not provide any benefit for others. In this case, federation is not needed. Therefore, clients themselves can use a generative model to generate synthetic data and then train a model using the synthetic data locally. After that client can fine-tune the model. The only reason that the server can be used for this purpose is that the server might have superior ability in training. To sum up, I do not think that the proposed framework makes a significant contribution. Also, the proposed algorithm do no need to train models iteratively and just one iteration should be enough.\n\nSecond, unlike the claim in the paper, I do not believe that the proposed framework can deal with client drift effectively. If the distribution of synthetic data is significantly different from those of clients, then fine-tuning the global model might be the same as training a model from scratch locally by the clients."
            },
            "questions": {
                "value": "No question."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission745/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742789689,
        "cdate": 1698742789689,
        "tmdate": 1699636001435,
        "mdate": 1699636001435,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CKtcQeMw4s",
        "forum": "tbznWbXq2b",
        "replyto": "tbznWbXq2b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission745/Reviewer_5fiA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission745/Reviewer_5fiA"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes GPT-FL, which leverages generative pretrained models to generate synthetic data for training a downstream model for FL. The experimental results show the remarkable performance of GPT-FL when compared with state-of-the-art FL methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The writing is mostly clear. The proposed method significantly outperforms state-of-the-art FL methods."
            },
            "weaknesses": {
                "value": "My main concern is that GPT-FL may significantly increase the burden of the central server. Notice that in the standard FL, the server only needs to aggregate parameters received from clients. However, GPT-FL needs to run pre-trained models on the central server, which requires additional computational cost."
            },
            "questions": {
                "value": "1. Which pretained models are used in the experiments?\n2. I think the experiments may compare the time and memory overhead used by GPT-FL and standard FL methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission745/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699027029750,
        "cdate": 1699027029750,
        "tmdate": 1699636001369,
        "mdate": 1699636001369,
        "license": "CC BY 4.0",
        "version": 2
    }
]