[
    {
        "id": "TRhw5JAGBw",
        "forum": "MY8SBpUece",
        "replyto": "MY8SBpUece",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8250/Reviewer_nmYk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8250/Reviewer_nmYk"
        ],
        "content": {
            "summary": {
                "value": "This paper studies how neural networks learn the features with different learning rates. Specifically, this paper considers a setup of two-layer neural network under one-step GD with the step size $\\eta=n^\\alpha$, where $n$ is the number of training samples. \nThis paper provides a theoretical characterization of how a specific choice of $\\alpha$ influences the neural network's ability to learn various types of features."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper goes beyond the NTK region and characterizes the performance of the neural networks with a relatively large learning rate. Instead of doing lazy training, this paper shows the ability of neural networks to learn features and characterize the relationship between the learning rate and the learned features by the weights."
            },
            "weaknesses": {
                "value": "1. Assuming that the data follows a zero-mean Gaussian distribution is strong, and whether real-world data satisfies this assumption can vary. Specifically, when considering Gaussian data, it is assumed to exhibit symmetric properties, which are necessary for the proofs. However, it's important to recognize that not all real-world data inherently possesses such symmetrical properties.\n\n2. The assumptions for the activation function are unclear. I had a hard time understanding Condition 2.3 and Condition 2.4. Could the authors directly tell us which activation functions satisfy these conditions? \n\n3. More discussions are needed for the magnitude of $c_\\star$. For example, for some common activation functions, like ReLU and Sigmoid,  where $M$ can go to infinity, I would like to learn about the dependence of the sample complexity.\n\n4. I noticed that the theoretical results regarding the training loss are only provided for the cases where $\\ell$ equals 1 and 2. While the authors mentioned that results for the general case of $\\ell$ can be found in the Appendix, I believe it is essential to include and discuss these results in the main content.\n\n5. Show the decrease in training loss may not be surprising.  Instead, the focus should be on generalization and test error.\n\n6. Emphasizing the technical challenges involved in deriving the proofs would make it easier to appreciate the technical novelty. Currently, it's challenging to discern these technical aspects when looking at the LONG proofs in the Appendix.\n\n7. We need some experiments of higher-order feature learning to justify the theoretical findings in Theorem 4.1. \n\n8. We need some numerical experiments on real data and deep neural networks to justify the theoretical findings. Currently, due to the strong assumptions on the input data and activation functions, it's challenging to envision practical applications."
            },
            "questions": {
                "value": "1. The setup of this paper and Zhenmei et al.2022 seems significantly different from my POV, but both are counted as feature learnings. It would be better to clarify the field of feature learning and how they are connected. \n\n2. The values of training error and test error are quite confusing. It would be helpful if the authors included some baseline measures to assess the algorithm's performance. Specifically, it's challenging to see the significance of studying an algorithm with a large test error.  While it may be caused by the scaling issue, the authors should consider making adjustments to avoid any misunderstanding.\n\n3. How does Figure 2 change as the number of iterations increases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8250/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8250/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8250/Reviewer_nmYk"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698708791870,
        "cdate": 1698708791870,
        "tmdate": 1699637026166,
        "mdate": 1699637026166,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9UoPwUnDuw",
        "forum": "MY8SBpUece",
        "replyto": "MY8SBpUece",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8250/Reviewer_boim"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8250/Reviewer_boim"
        ],
        "content": {
            "summary": {
                "value": "The paper delves into the feature learning capabilities of two-layer fully-connected neural networks. It builds on the understanding that with a constant gradient descent step size, only linear components of the target function can be learned. The research introduces a varying learning rate that grows with sample size, which results in the emergence of multiple rank-one components in the feature matrix, each representing a specific polynomial feature. Through spectral analysis, it's shown that the feature matrix's spectrum undergoes phase transitions based on the learning rate, leading to the addition of separated singular values or \"spikes\". These spikes, as the paper demonstrates, are aligned with polynomial features of varying degrees, influencing the neural network's ability to learn non-linear components. The study establishes that the training and test errors of the updated networks are determined by the initial feature matrix and these spikes, with specific cases illustrating the network's capacity to learn quadratic components of the target function."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strength:\n1. Paper is well organized\n2. I think this paper has a good contribution to understanding the learning dynamics of non-linear features by networks, with concrete improvements over Ba et al. 2022."
            },
            "weaknesses": {
                "value": "Weakness:\n1. Based on my understanding, the core advantage of the proposed analysis is from the Hermite expansion of the activation layer, which can characterize higher-order nonlinearity and explain more non-linear behaviors than the orthogonal decomposition used in Ba et al. 2022. Please clarify this.\n2. The required condition on the learning rate (scaling with the number of samples) is not scalable. I never see a step size grows with the sample size in practice, which will lead to unreasonably large learning rate when learning on large-scale dataset. I understand the authors need a way to precisely characterize the benefit of large learning rates, but this condition is not realistic itself."
            },
            "questions": {
                "value": "Question:\nIn Figure 3 left/middle: is the total number of training steps fixed? I.e. more iterations for small LR, and fewer iterations for large LR? This is important for a fair comparison between small and large learning rates."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816516875,
        "cdate": 1698816516875,
        "tmdate": 1699637026045,
        "mdate": 1699637026045,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EYOY3XmNT0",
        "forum": "MY8SBpUece",
        "replyto": "MY8SBpUece",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8250/Reviewer_w1FP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8250/Reviewer_w1FP"
        ],
        "content": {
            "summary": {
                "value": "The authors studied the effect of feature learning in a two-layer neural network, where the first-layer weight matrix receives one gradient update with large learning rate, and the target function is a single-index model. The main contribution is a spike decomposition of the feature matrix, where the corresponding singular vectors contain polynomial features of different degrees\ndepending on the scaling of step size $\\eta$. This allows the authors to compute the asymptotic training error under a Gaussian equivalence conjecture and quantify the improvement in the loss due to feature learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This submission generalizes the result in (Ba et al. 2022) to step sizes that scales with $\\eta\\asymp n^\\alpha$ for $\\alpha\\in (0,1/2)$, and provides a precise description of the nonlinear feature learning after one gradient update. Moreover, the authors identified a sequence of phase transitions with respect to the learning rate scaling, where a degree-$\\ell$ spike appears when the exponent of the step size exceeds $\\alpha^2>1-\\frac{1}{\\ell}$. This finding may motivate random matrix theory research on similar nonlinear spiked matrix models."
            },
            "weaknesses": {
                "value": "My main concern is that unlike (Ba et al. 2022), the theoretical results in the current submission does not translate to learning guarantees for the studied single-index teacher. As a result, it is unclear if a larger learning rate provides any statistical benefits, so the claim that *\"for large enough step sizes, the model can learn non-linear components of the teacher function\"* is not supported.  \nIn Figure 3 the authors plotted the test error which exhibits improvement due to the learning of the quadratic component, but such improvement is not proved, and the experimental setting is only for target function with information exponent $s=1$. In fact, by inspecting the formulae for $\\Delta$, it appears that the test error cannot improve for $s>1$.   \nThis limitation needs to be explicitly mentioned in the main text."
            },
            "questions": {
                "value": "I have the following questions regarding the figures in the main text. \n\n1. In Figure 2, do the crosses represent the theoretical predictions of the spike location? If so, how are these values obtained? \n\n2. In Figure 3, do the solid lines correspond to the analytic predictions based on Theorems 4.4 and 4.5? If so, why do we observe fluctuations in the curve?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699428140688,
        "cdate": 1699428140688,
        "tmdate": 1699637025901,
        "mdate": 1699637025901,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0TAzEvsybP",
        "forum": "MY8SBpUece",
        "replyto": "MY8SBpUece",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8250/Reviewer_EiLn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8250/Reviewer_EiLn"
        ],
        "content": {
            "summary": {
                "value": "This paper studied the spikes in the feature map matrix of a two-layer neural network with large step gradient descent (GD) on mean square loss. When learning rate $\\eta=n^\\alpha$ with $\\frac{\\ell-1}{2\\ell}<\\alpha<\\frac{\\ell}{2\\ell+\\ell}$, there will be $\\ell$ large spikes in the feature map matrix (or the Conjugate Kernel matrix) and these spikes are correlated to the degree $\\ell$ Hermite components of the target function. The asymptotic training errors for ridge regression of this trained feature map matrix has been presented in the proportional limit. This paper fills the gap in the learning rates, when $1\\ll\\eta\\ll\\sqrt{n}$, in Ba et al. (2022)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Despite its technical nature, the paper is very well written. The particular setting the authors study is novel and interesting for both the random matrix theory community and deep learning theory. The detailed analysis of the scaling of learning provides us with a more comprehensive understanding of the features learned in GD training processes, although the authors only consider one step of GD. The result is precise and clean, showing the asymptotic improvement in the loss for potential feature learning."
            },
            "weaknesses": {
                "value": "1. A limitations section is missing. In the conclusion section, the authors should state the limitations of the assumptions and the results. The authors only proved the improvement of training loss in this two-stage training process for neural networks (NNs). There is a lack of analysis for generalization errors, although I understand there may be some difficulty with this kind of theoretical result. There should be some remark or discussion on this, or providing some conjectures related to the generalization error.    \n\n2. Additional simulations are needed. In Section 5, there are only cases for linear and quadratic target functions. It would be better to provide more simulations for training and testing errors with more complicated target functions to show the feature learning when $\\eta$ is sufficiently large. There is no empirical simulation for the staircase phenomenon in Figure 3 (Right).\n\n3. Theorems 4.4 and 4.5 rely on Conjecture 4.3. However, this conjecture is not well stated in the main text. It would be also better to explain the difficulty of the proof and why this conjecture cannot be proved by previous results like Hu&Lu, (2023) and Ba et al. (2022)."
            },
            "questions": {
                "value": "1. In Section 2.1, the scaling of the neural network is different from Ba et al. (2022). In Ba et al. (2022), they used a mean-field regime with learning rate $\\eta\\sqrt{N}$ and there is an extra $1/\\sqrt{N}$ for the second layer $\\mathbf{a}$. Is this regime the same as the setting of this paper?\n\n2. The initialization of $\\mathbf{W}_0$ is sampled from a uniform distribution on the unit sphere, which is different from the Gaussian initialization of Ba et al. (2022). I guess this initialization will make the analysis simpler, e.g. Lemma B.1 can be applied directly. This should be mentioned somewhere in the paper and explain why you use this initialization.\n\n3. Condition 2.3 assumes that $\\sigma$ has bounded first three derivatives but this won't be true if you consider the general polynomial activation function, which is set in Theorem 3.3. For Theorems 3.4, 4.1-4.2, and 4.4-4.5, do you only consider $\\sigma$ as a polynomial or center ReLU function? I am confused why Theorem 3.3 needs polynomial activation functions which may contradict with Condition 2.3.\n\n4. For Figure 2, are the locations of the spikes and the alignments empirically simulated or can you predict them from your theory? From Theorem 3.4, these alignments should converge to one, right?\n\n5. In Theorem 3.3, how about the case when $\\alpha=(\\ell-1)/(2\\ell)$? Any observations in this critical regime?\n\n6. [1] and [2] also studied the initial feature matrix $\\\\mathbf{F}_{0}$. And [1] also presented the limit of training error for random feature ridge regression but with a slightly different definition than yours.\n\n7. Above Theorem 3.4, vector $\\mathbf{w}_i$ is not defined.\n\n8. Below (4), why does $c_{>1}$ also include $c_1$?\n\n9. For Theorems 4.4 and 4.3, can you say something about some extreme cases? For instance, $n\\gg N,d$ or $N\\gg n,d$. \n\n10. In Section 4.2, why not present the theory of training loss for general $\\ell$ like Appendix L? Can Appendix L directly cover Theorems 4.4 and 4.3? Besides, using Appendix L, can you plot Figure 3 (Right) and show that the training loss is always decreasing?\n\n11. I cannot see how Figure 3 (Left and Middle) matches Theorems 4.4 and 4.3 for training loss with $\\log\\eta/\\log n<1/4$ or $1/4<\\log\\eta/\\log n<1/3$. You may need to point out the threshold in the figures. Besides, in the middle figure, why is the testing error increasing for setting 1 with a large learning rate? Can you explain this phenomenon here? It seems like in this case, we do not have improvement for feature learning. Besides, there should be a benchmark, the prediction risk for the best linear model in this figure to compare with the feature learning.\n\n12. In Appendix A, a typo for the definition of $\\mathbf{R}_{0}$.\n\n13. Lemma B.1, do you need to require $\\\\|a\\\\|=\\\\|b\\\\|=1$?\n\n14. In the proof of proposition 3.1, how do you use Lemma J.1 to derive the limit of $\\boldsymbol{\\beta}^\\top\\boldsymbol{\\beta}_{\\*}$?\n\n15. In the proof of Theorem 3.4, how do you show the rank of the sum of the spikes is exactly $\\ell$? Is it easy to see that $(\\tilde{\\mathbf{X}}\\boldsymbol{\\beta})^{\\odot k}$ are linearly independent for different $k$?\n\n16. In the final result of Appendix L, why is $c_{\\*,0}$ also included in the asymptotic difference of the training errors? In $\\ell=1,2$, there is no $c_{\\*,0}$; see (5) and (6). And What is $M$ in the summation? There should be some discussion about this result.\n\n\n\n==================================================================================================\n\n[1] Louart, et al. \"A random matrix approach to neural networks.\"  \n\n[2] Fan and Wang. \"Spectra of the conjugate kernel and neural tangent kernel for linear-width neural networks.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8250/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8250/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8250/Reviewer_EiLn"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699602440557,
        "cdate": 1699602440557,
        "tmdate": 1699637025778,
        "mdate": 1699637025778,
        "license": "CC BY 4.0",
        "version": 2
    }
]