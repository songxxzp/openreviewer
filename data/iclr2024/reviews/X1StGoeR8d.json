[
    {
        "id": "pw10N5xNav",
        "forum": "X1StGoeR8d",
        "replyto": "X1StGoeR8d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission567/Reviewer_wPps"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission567/Reviewer_wPps"
        ],
        "content": {
            "summary": {
                "value": "This paper deals with the few-show image classification problem. It proposes an attention-based module to adaptively weight the distributions for metric learning. The experiments show that the proposed method can generate feature distributions for computing maximum mean discrepancy, with higher probability mass on the discriminative local features."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The motivation is sound, which is weighting the local features to generate distributions for few-shot learning.\n\n2. Visualization results show that the method assigns large weights for more discriminative local features.\n\n3. The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. My biggest concern is that the performance of the proposed method is not SOTA based on numerical results. The most compared methods in this paper are published in AAAI2023 and PAMI, while the method in PAMI was first proposed in CVPR2020, which all seem a little bit outdated. The authors should consider comparing their methods with recent methods [1,2] published in CVPR2023 and ICCV2023. As for the numerical results, the accuracy under miniImageNet and tieredImageNet is 71.97\u00b10.65, 87.06\u00b10.38, 76.93\u00b10.70, 90.12\u00b10.45 while the number in this paper is 71.31\u00b10.45 86.07\u00b10.29 77.35\u00b10.48 89.49\u00b10.31. Furthermore, the authors use the Swin-Tiny backbone, which should achieve higher performance than ViT-S used by [2] as stated by the authors. Also, in paper [3], the reported accuracy under LR-DB-ventral in their Table 1 is 79.48 \u00b1 0.26 and 91.03 \u00b1 0.41 compared with 77.35\u00b10.48 and 89.49\u00b10.31 in this paper.\n\n2. The contribution is not enough. As for me, the pipeline follows the same pipeline in the few-shot image classification field. The novelty lies in a re-weighting module for metric learning, which is based on attention. The techniques are not very novel and I think the contribution is not enough for the ICLR conference.\n\n3. Experiments using WRN28 as the backbone should be carried as many papers including [3,4] in few-shot classification using this backbone.\n\n4. (Minor) The citation of the paper \"SpatialFormer: Semantic and Target Aware Attentions for Few-Shot Learning\" is wrong. The authors are wrong.\n\n5. (Minor) Authors should consider open-sourcing their codes.\n\n\n[1] Transductive Few-shot Learning with Prototype-based Label Propagation by Iterative Graph Refinement.\n\n[2] Class-Aware Patch Embedding Adaptation for Few-Shot Image Classification.\n\n[3] Exploring Tuning Characteristics of Ventral Stream\u2019s Neurons for Few-Shot Image Classifcation.\n\n[4] Adaptive Distribution Calibration for Few-Shot Learning with Hierarchical Optimal Transport."
            },
            "questions": {
                "value": "1. What methods are using the reported accuracy in their papers and what methods are re-implemented by the authors in Table 1?\n\n2. How can we select the most effective patch number in Table 4? Do the authors use validation sets to select this hyper-parameter?\n\n3. As for me, the selected most discriminative parts are the objects and the unselected parts are the backgrounds. Can the authors explain why using the attentive module can only select the foreground objects?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission567/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission567/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission567/Reviewer_wPps"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission567/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698486980131,
        "cdate": 1698486980131,
        "tmdate": 1699635984193,
        "mdate": 1699635984193,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XumR8XZScn",
        "forum": "X1StGoeR8d",
        "replyto": "X1StGoeR8d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission567/Reviewer_qKBe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission567/Reviewer_qKBe"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces MMD distance as an alternative metric to tackle few-shot learning problems. The MMD distance is calculated over query and support features of all patches of images. To give different weights for different patches, an attention module is then applied to produce weights dynamically, leading to AMMD. The experiments show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- Introducing the MMD distance to few-shot learning is interesting, and the attentive module makes sense for few-shot learning.\n- The writing is clear and easy to understand."
            },
            "weaknesses": {
                "value": "- The motivation for using MMD as a metric is not stated in the paper. It is not clear why using MMD is better than other metrics like BDC [1] and EMD [2] distance, since MMD distance does not considers correspondence between support and query patches.\n- As the method uses both supervised and self-supervised pretraining, the comparisons are unfair (self-supervised pretraining has proven to be extremely helpful for few-shot learning, as evidenced by [3]). In addition, cutting images into patches can lead to much better performance for every method [4], so in order to achieve a fair comparison, the authors should do this for every compared method at test time (for non-patch-based methods, cut into patches and average their features).\n- The performance of pretrain-only baseline is missing.\n- As shown in [3,5], a strong backbone can be much better than any pure meta-learning method. It is not clear from the paper whether the proposed method can scale to large datasets. The authors should apply their method to more pretrained models (like CLIP and DINO-v2) trained on large datasets (just like what is done in [5], first pretrain and then tune with meta-learning), and see if the performance improves more than other methods using the same pretrained models. Only by doing this, we can see some values of this paper.\n- The classes of images in Figure 5 are all from the training set of miniImageNet. Thus it is not surprising for the visualization results. The authors should give visualization results from the unseen test set.\n\n\n[1] Joint Distribution Matters: Deep Brownian Distance Covariance for Few-Shot Classification. CVPR 2022.\n\n[2] DeepEMD: Few-Shot Image Classification with Differentiable Earth Mover\u2019s Distance and Structured Classifiers. CVPR 2020.\n\n[3] A Closer Look at Few-shot Classification Again. ICMl 2023.\n\n[4] Rectifying the shortcut learning of background for few-shot learning. NeurIPS 2021.\n\n[5] Pushing the Limits of Simple Pipelines for Few-Shot Learning: External Data and Fine-Tuning Make a Difference. CVPR 2022."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission567/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission567/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission567/Reviewer_qKBe"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission567/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698647440772,
        "cdate": 1698647440772,
        "tmdate": 1699635984112,
        "mdate": 1699635984112,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lEFI9y3NAZ",
        "forum": "X1StGoeR8d",
        "replyto": "X1StGoeR8d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission567/Reviewer_sp9L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission567/Reviewer_sp9L"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel approach called Attentive Maximum Mean Discrepancy (AMMD) for few-shot classification. In traditional empirical Maximum Mean Discrepancy (MMD) methods, the mean of patch-level features is used to approximate expectations.\nInstead, the proposed AMMD adaptively estimates distributions of patch-level features by an attentive distribution generation module (ADGM) and then computes the expectations in MMD based on the estimated distributions.\nIn a word, AMMD integrates the attention-based module ADGM and the distance metric MMD.\nThe proposed AMMD obtains good performance in few-shot classification."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The writing is easy to follow.\n2. Applying Maximum Mean Discrepancy (MMD) in few-shot learning is interesting.\n3. The performance is good."
            },
            "weaknesses": {
                "value": "1. The novelty of ADGM is incremental. CTX and STANet also performed cross-attention between support and query.\n2. Please further explain the meaning of eq.(7)\n3. Please further explain the meaning of \u03b1 and \u03b2.\n4. According to eq.(9), \u03b1 is the similarity map of query features with global support features. According to eq.(10), \u03b2 is the similarity map of support features with global query features. Then, in Fig.1, What is the meaning of using MMD to measure the similarity between \u03b1 and \u03b2?\n5. Add more ablation studies of MMD, such as comparing to ADGM+cosine distance.\n6. add more visualization comparisons with other attention-based methods, such as DeepEMD, CTX, and STANet.\n7. Show the visualization comparison with the baseline without ADGM.\n8. (1) The authors' information of STANet is incorrect. (2) The citation of MixFSL is incorrect. It should be Mixture-based feature space learning for few-shot image classification."
            },
            "questions": {
                "value": "see the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission567/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744237121,
        "cdate": 1698744237121,
        "tmdate": 1699635984026,
        "mdate": 1699635984026,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3mRbvxsGwu",
        "forum": "X1StGoeR8d",
        "replyto": "X1StGoeR8d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission567/Reviewer_H34j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission567/Reviewer_H34j"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel approach called Attentive Maximum Mean Discrepancy (AMMD) aimed at enhancing the Maximum Mean Discrepancy (MMD) method. The proposal involves adaptively estimating distributions using an attentive distribution generation module to assist MMD. The focus is on applying AMMD to few-shot learning, treating it as an AMMD metric learning problem. To implement this, the authors use part-based feature representation to model the AMMD between images.\n\nThe method incorporates a meta-learning technique to train the attentive distribution generation module of AMMD. This module generates feature distributions for computing MMD between images, giving more weight to the more discriminative features. During the meta-test phase, each query image is labeled based on the support class that exhibits the minimum AMMD to the query image. \n\nExperiments are done on typical few-shot image classification datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Illustrative figures 2 and 3 are clear. \n\n- The proposed method shows performance improvement on few-shot image classification."
            },
            "weaknesses": {
                "value": "1. The novelty is limited. The patch-level feature extractor is not a new idea. As for the Attention-based Distribution Generation Module (ADGM), it estimates the improtance of patch-level feature. What is the difference if you directly use multi-head attention to obtain updated patch-level features? In this way, the patch-level features of query samples are transformed along with the patch-level features of support samples. Then, one can apply MMD to calculate distance. Why bother to design ADGM? The authors are also suggested to provide empirical evidence. \n\n2. The proposed AMMD contains many parameters. Will that be costly to train? Any comparison on time consumption? \n\n3. The benefits of using patch-based features can be explained in more length. Why is it better than learning with sample-level features? \nThe authors may add a comparison with a related baseline [1] about this question. \n\n[1] Distribution Consistency Based Covariance Metric Networks for Few-Shot Learning, AAAI-19\n\n\n4. The paper needs significant proofreading. \n\n- Some sentences are hard to follow, such as \"each query image is labeled as the support class with minimal AMMD to the query image\" in the abstract. \n\n- Wrong method names, such as FewTrue and FewTURE. Even for the proposed method, the authors use a wrong name. E.g., AWGM in Table 3."
            },
            "questions": {
                "value": "Please see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission567/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698835915410,
        "cdate": 1698835915410,
        "tmdate": 1699635983875,
        "mdate": 1699635983875,
        "license": "CC BY 4.0",
        "version": 2
    }
]