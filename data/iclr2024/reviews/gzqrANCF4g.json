[
    {
        "id": "pVbccFjRLl",
        "forum": "gzqrANCF4g",
        "replyto": "gzqrANCF4g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6714/Reviewer_k12R"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6714/Reviewer_k12R"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel visual tokenizer designed to enhance large language models in producing high-quality images and videos. Experimental results show that, when integrated with the proposed tokenizer, LLM surpasses diffusion models in standard benchmarks such as ImageNet, UCF-101, and Kinetics-600. Additionally, the paper presents promising results in video compression and representation learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1.\tThis paper presents the first evidence of large language models surpassing diffusion models on the ImageNet benchmark.\n2.\tThe paper proposes a novel lookup-free quantization approach, providing a promising direction for expanding vocabulary size in LLM-based visual generation.\n3.\tThe motivation is clear, and the overall presentation is coherent and easy to follow.\n4.\tGood results on visual generation video compression, and video representation learning."
            },
            "weaknesses": {
                "value": "1.\tWhile the presented method is tailored for masked LM, many of the prevailing and powerful LLMs, such as LLaMA [A], employ an autoregressive approach. Incorporating results from AR-LM would greatly enhance the paper's relevance to the community.\n2.\tIn Table 4, despite the good action recognition performance showcased by the proposed method, it doesn't conclusively establish its efficacy as a viable self-supervised pre-training target. Notably, some pivotal baselines, like pixel colors and the image descriptor from MaskFeat [B], are missing.\n\n[A] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A., Lacroix, T., ... & Lample, G. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\n\n[B] Wei, C., Fan, H., Xie, S., Wu, C. Y., Yuille, A., & Feichtenhofer, C. Masked feature prediction for self-supervised visual pre-training. In CVPR 2022."
            },
            "questions": {
                "value": "1.\tIn Figure 1, it's highlighted that the VQ generation FID sees a pivotal change at a vocabulary size of 2^14, while the LFG generation FID consistently improves. I'm curious to understand how the LFG generation FID would respond to even larger vocabulary sizes.\n2.\tRegarding Table 3, what could be the reason behind the proposed method's PSNR and MS-SSIM values being inferior to those of the standard video codec?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6714/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6714/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6714/Reviewer_k12R"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6714/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638358535,
        "cdate": 1698638358535,
        "tmdate": 1699636771481,
        "mdate": 1699636771481,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XeMEGPcs1v",
        "forum": "gzqrANCF4g",
        "replyto": "gzqrANCF4g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6714/Reviewer_sfJe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6714/Reviewer_sfJe"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on learning a video / image tokenizer to discretize video / images so that they can be modeled using a Language Model. They specifically introduce one innovation in this setting: A lookup free quantizer. They show that in this limit of using a large vocabulary and no lookup, the tokenizer reconstruction and LM generation quality both increase with vocabulary size. The authors also show that the learned tokenizer performs very well as a compression scheme."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper does a good job of motivating the core thesis of the paper: How to design a tokenizer for image / video? The authors also do a good job of presenting this idea to the uninitiated. It is also pretty clear that the work is significant to academia and industry given that it helps unify image generation with image understanding and natural language generation and understanding techniques. The ideas in the paper are well explained. The authors also do a thorough job of running experiments to substantiate many claims including numerous ablations. Some of the important technical insights like the lookup free quantizer (and in general lower dimensional code words) helping in generation quality are substantiated by experimental results"
            },
            "weaknesses": {
                "value": "My main concern is the completeness of the exposition in the paper. The authors assume that the reader is familiar with the state of the art in video tokenization and details do get rather buried in the many \u201cdeltas\u201d relative to the baseline. I do understand the space limitations but it might be helpful if the authors try to make the core system / model design more explicit. Lot of the ideas like factorization of the output space in the decoder (and associated weight tying) for example are just mentioned in passing."
            },
            "questions": {
                "value": "* The question \u201cWhy masked LM and not AR LM for image / video generation?\u201d for evaluating the tokenizer was not clearly answered.\n* No explicit definition of the objective for training the tokenizer (loss function)\n* No mention of decoder in VQ-VAE and VQ-VAE loss used when we use no lookup\n* More motivation needed on why the authors choose to use a causal encoder for tokenizer when doing masked LM for image / video generation\n* It\u2019s not clear why the authors tackle video generation if the aim was to understand the fundamentals of tokenization. It may be desirable for them to clearly motivate why they study videos and not images alone?\n* It may be interesting for the reader to understand the computational complexity of both the tokenizer (encoder) and the detokenizer (decoder) and how they compare with video or audio codecs"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6714/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6714/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6714/Reviewer_sfJe"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6714/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698783954186,
        "cdate": 1698783954186,
        "tmdate": 1699636771350,
        "mdate": 1699636771350,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6Q12UZEOwW",
        "forum": "gzqrANCF4g",
        "replyto": "gzqrANCF4g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6714/Reviewer_rVyC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6714/Reviewer_rVyC"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel visual tokenizer based on lookup-free quantization (LFQ). With the growth of the vocabulary size LFQ consistently improve both reconstruction and generation quality, which is in stark contrast with Vector Quantization (VQ) where an increased vocabulary size reduces reconstruction error but hurts generation results. The tokenizer can be integrated with MAGVIT and achieves state-of-the-art performance on video generation. The tokenizer can also improve video compression and video recognition."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. Starting from an interesting finding that enlarging the vocabulary improves reconstruction quality but hurts generation results, the paper proposes solutions (LFQ) to tame both reconstruction and generation simultaneously.\n\n2. A detailed study of architecture modifications that improves up MAGVIT supported by extensive ablations. \n\n3. The tokenizer is proved to benefit video generation, compression and recognition. It will potentially have a huge impact on the general audience of video understanding."
            },
            "weaknesses": {
                "value": "1. For video compression results, it would be better if there is a PSNR/LPIPS/MS-SSIM-bpp curve comparing the performance across different bpps.\n\n2. In the video recognition setup, it seems unnecessary to detokenize the visual tokens back to pixels since BEVT and BEIT can work with tokenized input. I understand one of the main reasons is that the underlying recognition model is the ViViT which takes raw pixels as input (as stated in the draft). However, you may also have a comparison with BEVT."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6714/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699078850701,
        "cdate": 1699078850701,
        "tmdate": 1699636771173,
        "mdate": 1699636771173,
        "license": "CC BY 4.0",
        "version": 2
    }
]