[
    {
        "id": "IPKolYOBx2",
        "forum": "W3T9rql5eo",
        "replyto": "W3T9rql5eo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3545/Reviewer_AzbY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3545/Reviewer_AzbY"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new indicator to measure the uniformity of Pareto objectives on the Pareto front and introduces a new adaptive weight adjustment method that utilizes a neural model to represent the Pareto objective distribution, enabling the generation of uniformly distributed solutions on the Pareto front. The proposed adaptive weight adjustment method is integrated into MOEA/D and the generalization error bound of the proposed neural model is analyzed."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tA new indicator is proposed for measuring the uniformity of Pareto objectives for multi-objective optimization.\n2.\tA neural model is proposed to learn the relationship between preference angles and aggregated objective functions.\n3.\tThe error bound of the proposed neural model is studied theoretically."
            },
            "weaknesses": {
                "value": "1.\tThe work related to MOEA/D with adaptive preference adjustment methods has not been adequately investigated. The most recent paper mentioned in this paper was published in 2014, which does not correspond to the extensive research MOEA/D has received over the years.\n2.\tThe effectiveness of the proposed method needs more evaluation by considering test problems with more complicated Pareto fronts, e.g., the WFG and UF test suite, and more state-of-the-art algorithms published within the last eight years.\n3.\tMore details of the proposed method need to be provided, e.g., when the method uses the real objective evaluation and the model-based estimation.\n4.\tThe conclusion that MOEA/D fails to achieve uniform objectives shown in Section 4 is not rigorous, given that many MOEA/D variants have been proposed. Specific descriptions or references that hold for the conclusion should be provided."
            },
            "questions": {
                "value": "1.\tWhat is the scope of the proposed adaptive weight adjustment method? Is it suitable only for decomposition-based multi-objective evolutionary algorithms? If not, how would it be used in other frameworks, e.g., dominance relation-based, indicator-based frameworks?\n2.\tHow were the test problems in the experiments chosen? For example, for the DTLZ test suite, why were only DTLZ1-2 used, but not the more complex other problems?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3545/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698670402926,
        "cdate": 1698670402926,
        "tmdate": 1699636308641,
        "mdate": 1699636308641,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HZ4PR0ZC3C",
        "forum": "W3T9rql5eo",
        "replyto": "W3T9rql5eo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3545/Reviewer_1zQZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3545/Reviewer_1zQZ"
        ],
        "content": {
            "summary": {
                "value": "Unlike directly predicting Pareto solutions from the preference vector using a neural model in previous work, MOEA/D-UAWA uses a neural model as a surrogate to estimate the final vector of objective functions from the preference vector, and adaptively adjusts the corresponding preference vectors using gradient-based optimization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is easy to follow. \n\nThe idea of using a neural model as a differential surrogate to optimize the uniformity objective is novel and interesting."
            },
            "weaknesses": {
                "value": "The motivation of this paper assumes that the optimization problem is a black box and thus proposes a neural model as a differential surrogate. My main concern is that the above motivation is improper for neural network optimization, in which you can use a gradient descent optimizer. And the eq. (5) can also be optimized without the proposed neural surrogate model.\n\nOne possible solution is to show some additional results on large-scale neural network optimization but with a \"small\" neural surrogate model, which can improve computational efficiency significantly.\n\nMoreover, the baselines used in the experiment are weak and old. A lot of work in the field of evolutionary multi-objective optimization discussed the adaptive reference/preference vectors."
            },
            "questions": {
                "value": "Some comments:\n\n1. figure 1 is unclear, please improve it.\n\n2. the claim in sec. 3.3 \"whereas the proposed method aims to achieve global optimal MOO solutions\" seems to be improper.\n\n3. the main body of this paper misses an ablation study section.\n\n---post-rebuttal comment---\n\nAccording to the author's responses and the current version of the manuscript, I decided to raise my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3545/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3545/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3545/Reviewer_1zQZ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3545/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698718102537,
        "cdate": 1698718102537,
        "tmdate": 1700729751622,
        "mdate": 1700729751622,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fS3j0MbwAY",
        "forum": "W3T9rql5eo",
        "replyto": "W3T9rql5eo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3545/Reviewer_gHJ7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3545/Reviewer_gHJ7"
        ],
        "content": {
            "summary": {
                "value": "An approach aimed at presenting a uniformly distributed pareto front\nin MOO by combining pareto front learning with uniform pareto front\nselection."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The manuscript highlights and formalizes limitations of some of the\nexisting solutions, and provides arguments for the potential of the\nproposed approach in overcoming these limitations."
            },
            "weaknesses": {
                "value": "Pareto Front Learning is introduced in the Related work section. This\nis confusing, because the method has not been presented yet.  In the\ngeneral, the presentation is rather confused, with concepts being\nintroduced in a non clearly defined order so that one has to jump back\nand forth to connect the dots and figure out the big picture. \n\nFigure 3, which provides the overview of the framework, is not clearly\nexplained. The authors refer to the appendix for most details, but a\nhigh level description should be provided, possibly including some\npreliminaries earlier on (e.g., on preference angles and MOEA/D),\notherwise the paper is not self-contained.\n\nFor lemma 1, it's unclear why f shouldn't have weakly Pareto\nsolutions. The implications of this requirement should be better\nexplained.\n\nTheorem 1 is badly presented, it's unclear from the content of the\ntheorem what are the constraints on h that make the pareto front\nuniform.\n\nAlso, the fact that sampling uniformly from the preference vector does\nnot imply a uniform pareto front generation was already observed in\nLiu et al, 2021 (the SVGD paper). \n\nI am not sure pareto set learning can be dismissed by just saying that\nf has many local optima. E.g. the SVGD method claims theoretical\nguarantees of convergence to the paret front, and report competitive\nperformance on the ZDT problem set. The advantage of the proposed\nsolution over PSL methods should be assessed, both formally and\nexperimentally.\n\nEnglish is not entirely satisfactory (e.g. \"Previous methods (Deb et\nal., 2019; Blank et al., 2020) focusing on generating well-spaced\n(uniform) preferences\", \"We first give the condition of such function\nh is well defined\")"
            },
            "questions": {
                "value": "Please explain how you plan to address the weaknesses I described."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3545/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698753122033,
        "cdate": 1698753122033,
        "tmdate": 1699636308448,
        "mdate": 1699636308448,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4Gi1mR9Z6q",
        "forum": "W3T9rql5eo",
        "replyto": "W3T9rql5eo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3545/Reviewer_EYB8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3545/Reviewer_EYB8"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of profiling Pareto front in multi-objective optimization. In this paper, the authors first show that traditional methods with uniformly distributed preferences does not necessarily induces uniformity in the Pareto objective space. To resolve the issue, the MMS problem is formulated to explicitly impose the iterates to be uniformly distributed in the objective space, which is then optimized by replacing the preference-to-objective mapping by a surrogate NN model. Theoretical analysis shows the asympotic uniformity property and the generalization error of the proposed method. Experiments on various numerical MOO tasks verify the effectiveness of the proposed method compared to classic evolutionary methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of directly modeling the preference-to-PF mapping is interesting, which might inspire future research on Pareto front profiling.\n\n2. This paper is technically sound with solid theoretical analysis."
            },
            "weaknesses": {
                "value": "1. The relevance to previous works is not clear enough. It seems that the technique of replacing the preference-to-objective mapping by a neural network as the surrogate model is developed from (Borodachov et al., 2019), and the generalization error analysis is adapted from prior works; hence, it would be helpful to clarify the technical difficulty or novelty compared to these works. \n\n2. This paper has briefly reviewed gradient-based methods for Pareto front profiling (e.g., MOO-SVGD), but the comparison seems insufficient. As I understand, the example to indicate \"gradient-based methods struggle to produce globally optimal solutions\" is merely concerned with the gradient aggregration method, not the MOO-SVGD or EPO methods as discussed in the main paper. The comparison should be made more comprehensively, say, comparing the performance and efficiency in experiments."
            },
            "questions": {
                "value": "1. It is interesting to model the preference-to-objective mapping to characterize the PF in a more direct way, but I wonder how can we generate certain Pareto solution given a specific preference from the learned Pareto front. It seems that the proposed model does not explicitly involve the solutions in the decision space."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3545/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698912350817,
        "cdate": 1698912350817,
        "tmdate": 1699636308375,
        "mdate": 1699636308375,
        "license": "CC BY 4.0",
        "version": 2
    }
]