[
    {
        "id": "eCJIPxZI2w",
        "forum": "14rn7HpKVk",
        "replyto": "14rn7HpKVk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5325/Reviewer_bK7i"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5325/Reviewer_bK7i"
        ],
        "content": {
            "summary": {
                "value": "The authors propose to fuse both speech encoder (whisper) and general audio encoder (beats) as inputs, connect with LLM via a Q-Former and fine-tune with LoRA. Besides pre-training and instruction tuning stages, the authors also propose an activation tuning stage, which is to prevent from overfitting to short captions and is able to generate long and diverse stories. For some of the instruction tuning dataset, the authors leverage LLM for curation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The combination of speech and audio encoders are interesting ideas and the results of speech audio co-reasoning provide new capabilities for audio understanding."
            },
            "weaknesses": {
                "value": "- According to the results in table 3, it seems that the proposed method only works significantly better on level 3 tasks of Story and SAC, which the evaluation metrics are specifically designed and there is no other reference value from other models provided. It would be better to provide more information on the 2 tasks and providing other baseline performance on these two tasks.\n- For the Story task, it is worth including accuracy FR along with diversity FR. This can provide a more holistic understanding of the tradeoffs.\n- For some tasks in level 1 and 2, the performance of proposed method is significant worse, e.g. PR, OSR. and SQQA for level 2. It might worth providing some in depth discussion and analysis. For example, can it be that adding both whisper and beats features introduce more confusion to the model? How are the Q-former attending to the concatenated features to make predictions?"
            },
            "questions": {
                "value": "- The process of activation tuning stage is not clear described, according to the last paragraph of 4.2, if the data is generated by SALMONN model, what do they look like? It would be helpful to provide an example. Also for teacher-forcing training, is it just a standard cross-entropy loss on generated text? How do you control the diversity and length of generated examples?\n- How are the instruction prompted for evaluation tasks? How do you instruct the model for evaluating these tasks?\n- Is ChatGPT also leveraged to generate text data for some evaluation tasks, especially for SAC and Story. If so, how are they leveraged to curate answers?\n- How are the prompted QA for training generated by ChatGPT verified?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5325/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5325/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5325/Reviewer_bK7i"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5325/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698504321237,
        "cdate": 1698504321237,
        "tmdate": 1700319570162,
        "mdate": 1700319570162,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3co9XfWL2J",
        "forum": "14rn7HpKVk",
        "replyto": "14rn7HpKVk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5325/Reviewer_RbXH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5325/Reviewer_RbXH"
        ],
        "content": {
            "summary": {
                "value": "This paper presents SALMONN, a novel method for equipping LLMs with hearing abilities by leveraging additional adapter modules and LoRA weights to train the LLM on a range of speech, audio, and music understanding tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The overall architecture design is quite clever, and its use of generally open source models and datasets is very important to the audio-text field as a whole.\n- The amount of time spent towards analyzing and mitigating the models failure modes is quite useful, and the authors provide an incredibly detail analysis of model behavior under different finetuning configurations.\n\nDespite some of the concerns mentioned below (which mostly address overall clarity rather than content), the paper presents an incredibly detailed analysis of a novel architecture and how to adapt LLMs for audio-based reasoning, and thus I recommend acceptance."
            },
            "weaknesses": {
                "value": "- The explanation of the pretraining stage could use a bit more depth. Namely, how is it that the Q-Former and LoRA weights are actually trained during the pretraining stage?\n- I think in general, the sections on task-overfitting and activation tuning are relatively hard to parse reading-wise and could be simplified. Unless I am misunderstanding something, task-overfitting is simply the idea that SALMONN overfits to the overrepresented tasks in the dataset, which I think the math in section 3.2 overcomplicates.\n- It is hard to tell in the ablations (5.2-5.4) what is being held fixed and what training configurations are being used. Namely, in section 5.2 are the results on the reduced LoRA scaling factor done with or without activation tuning?\n- The authors claim that the Level 3 tasks are harder directly, but given the myriad of evaluation metrics for each task it's hard to tell *why* these tasks are necessarily in their own class. Is there some way to show how these tasks are by nature an entirely more difficulty class of problems? Especially as SAC seems to be evaluated by ChatGPT outputs, it is hard to tell much about the actual performance of the model."
            },
            "questions": {
                "value": "- What is \"monotonic alignment\" as mentioned in the Q-Former section?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5325/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5325/Reviewer_RbXH",
                    "ICLR.cc/2024/Conference/Submission5325/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5325/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698615984021,
        "cdate": 1698615984021,
        "tmdate": 1700259654752,
        "mdate": 1700259654752,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kYwKzroSmA",
        "forum": "14rn7HpKVk",
        "replyto": "14rn7HpKVk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5325/Reviewer_KyXG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5325/Reviewer_KyXG"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed SALMONN, which is a single unified multimodal model to integrate speech and audio encoders with a pre-trained text LLM. The paper shows that SALMONN can achieve competitive performance on a variety of speech tasks used in training, including ASR, ST, emotion recognition, audio QA, speaker verification, audio and music captioning etc. The paper also studies the capabilities of SALMONN on zero-shot capabilities such as ST on untrained languages, SLU, SQA, audio-based story telling, speech-audio co-reasoning etc. The paper also explores a new few-shot activation tuning approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1)\tValue to the community: The proposed SALMONN model unifies modeling a wide variety of speech, audio, and music perception and understanding tasks into a single framework, which is a useful step towards the research for AGI. \n\n(2)\tThe innovation of the paper is mostly in choosing and piecing together existing approaches for this unified framework, using and evaluating a diverse set of speech/audio/music tasks for pre-training/instruction-finetuning, studying emergent capabilities, analyzing task-overfitting issue and exploring cheap activation tuning to alleviate catastrophic forgetting to training tasks.  SALMONN reasonably adopts several existing approaches. For speech and non-speech audio encoding, SALMONN integrates a speech encoder from Whisper and a BEATS audio encoder. SALMONN uses a Q-Former to convert encoder output to audio tokens for the text LLM (Vicuna in this work). LoRA is applied to align the augmented input space with output space to improve cross-modal alignment for the text LLM.  Following other works, SALMONN used a diverse set of speech, audio, and music tasks in pre-training and instruction finetuning of Q-Former and LoRA.  Notably,  this paper analyzed the task overfitting issue and provided insights for activation tuning to alleviate catastrophic forgetting from instruction tuning. The paper also studies the capabilities of SALMONN on handling cross-modal emergent tasks.  \n\n(3)\tOverall, the paper is clearly written.\n\n(4)\tEmpirical evaluations are comprehensive. The three levels are helpful organizations, as  speech tasks used in instruction tuning, unseen speech-based NLP tasks which can effectively evaluate speech-text cross-modal alignments, and the proposed new audio-based story telling and speech audio co-reasoning tasks which require understanding mixture of speech and non-speech auditory information."
            },
            "weaknesses": {
                "value": "(1)\tIn empirical validations, the choice of reference values (as shown in Table 2) needs to be clarified, and more importantly, these choices need to be justified. It is not clear which model size is used for Whisper when it is used as reference values. Also, the choice of simply cascading Whisper + Vicuna needs to be justified as the reference value for many tasks, since it may not be as competitive as other E2E models (e.g., recent speech LLMs), including SOTA. Without clear knowledge how strong these reference values are, it is not easy to judge how strong SALMONN performs as shown in Table 3.\n\n(2)\tSome key implementation details are missing.  The training data as shown in Table 1 are highly unevenly distributed. It is not clear methods such as data upsampling are used, or batches are designed for multi-task instruction fine-tuning.\n\n(3)\tThe paper focuses on general hearing capabilities of speech/audio/music. It would be useful to discuss how to extend the model to speech/audio/music generation tasks."
            },
            "questions": {
                "value": "Please check the comments and concerns raised under Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5325/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699056966334,
        "cdate": 1699056966334,
        "tmdate": 1699636534490,
        "mdate": 1699636534490,
        "license": "CC BY 4.0",
        "version": 2
    }
]