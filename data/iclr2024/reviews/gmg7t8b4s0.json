[
    {
        "id": "UqmePRBXmZ",
        "forum": "gmg7t8b4s0",
        "replyto": "gmg7t8b4s0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4122/Reviewer_J8CV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4122/Reviewer_J8CV"
        ],
        "content": {
            "summary": {
                "value": "This paper first introduces the concept of contextual privacy into LLM study. The authors propose 4 tiers of contextual privacy and a corresponding benchmark dataset, and find that existing LLMs cannot satisfy the requirement of contextual privacy in a large portion of scnearios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper first introduces the concept of contextual privacy into LLM study\n2. The paper proposes the first contextual privacy benchmark for evaluating the ability to conform with contextual privacy."
            },
            "weaknesses": {
                "value": "The concept of contextual privacy, as the name indicates, heavily depends on the context. The benchmark can only capture a small portion of possible contexts so it's not very scalable."
            },
            "questions": {
                "value": "Is there a way to construct scalable benchmark?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4122/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770424512,
        "cdate": 1698770424512,
        "tmdate": 1699636377202,
        "mdate": 1699636377202,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VWNbIplAkl",
        "forum": "gmg7t8b4s0",
        "replyto": "gmg7t8b4s0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4122/Reviewer_fMFx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4122/Reviewer_fMFx"
        ],
        "content": {
            "summary": {
                "value": "This paper develops a benchmark to evaluate the ability of LLMs to maintain privacy of prompt information in different contexts. The benchmark quantifies the privacy of LLMs as the leakage not of the training data as most of the works on the topic, but rather of private information contained in the prompts which should not be disclosed in specific contexts. The benchmark draws heavily on taxonomy and concepts such as contextual integrity of Nissenbaum (2004): for the LLM to appropriately discern what information to disclose, it needs to consider various contextual factors such as the type of information, the parties concerned and their relationships. The benchmark consists of four tasks of increasing complexity, ranging from LLMs having to evaluate whether a piece of information is sensitive to the more complex task of generating a meeting summary while avoiding to disclose private information discussed before some of the attendees joined the meeting. The authors evaluate a range of LLMs, including open-source and commercial ones, on this benchmark using metrics such as the correlation between privacy assessment of LLMs and human annotators. Results suggest that LLMs often reveal private information in contexts where humans would not."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- 1) The contribution is significant and original. This is an interdisciplinary paper drawing on the contextual integrity theory of Nissenbaum (2004) and follow-up work by Martin and Nissenbaum (2016) to design a benchmark for evaluating the privacy capabilities of LLMs. The paper adds a much needed component to the field: even as an LLM is privacy-preserving in the traditional sense (leakage of information about the training dataset), it might lack the reasoning capabilities to judge whether or not to disclose private information in its prompts.\n- 2) Practically useful contribution: the benchmark can be used by LLM developers to assess the extent to which their model preserves privacy of prompt information.\n- 3) Extensive empirical evaluation: several LLMs are evaluated against the benchmark."
            },
            "weaknesses": {
                "value": "- 1) Some of the metric definitions seem to be lacking in the main paper, making results hard to interpret, e.g., the sensitivity score in Table 2, the metric of Fig. 2 isn\u2019t named, Table 4 includes five undefined metrics. This is all the more important for figures such as Fig. 2 which are very complex and seem to be lacking a clear trend. \n- 2) No error rate is given for results derived from automated parsing of LLM responses. More specifically, automated methods like string matching or LLM interpretation of results may incorrectly determine whether a secret was leaked. What is the error rate of the automated method for parsing of LLM responses? This can be estimated by randomly sampling some of the responses and checking how often the automated method orrectly predicts whether the secret was leaked. This should give some notion of confidence in the results."
            },
            "questions": {
                "value": "- 1) Since part of the benchmark is generated by LLMs (e.g., Tier 2 and 4 tasks use GPT-4) and then GPT-4 is evaluated using the benchmark, can this bias the findings on GPT-4? E.g., is it possible for GPT-4 to be more \u201cfamiliar\u201d with the wording produced by itself and somehow be at an advantage compared to the other models? The use of GPT-4 for generating the tasks should be motivated and the limitations of this be acknowledged.\n- 2) The limitations stemming from using human annotators of Mechanical Turk for deciding what is private and what isn\u2019t aren't acknowledged. Do the authors know the background of the annotators and do they believe this may bias the results in specific ways?\n\nMinor (suggestions for improvement):\n- 3) Please include statistics of the benchmark such as how many examples are generated for each task, how many of them are human-generated vs LLM-generated.\n- 4) To facilitate the interpretation of results, I suggest to include more context about LLMs being evaluated. Some statements are made such as \u201cmodels that have undergone heavy RLHF training and instruction tuning (e.g. GPT-4 and ChatGPT)\u201d and \u201cOverall, we find the leakage is alarmingly high for the open source models, and even for ChatGPT\u201d without it being clear which LLMs are commercial, open-source, and trained using RLHF."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4122/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773976546,
        "cdate": 1698773976546,
        "tmdate": 1699636377143,
        "mdate": 1699636377143,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SoKjB473H9",
        "forum": "gmg7t8b4s0",
        "replyto": "gmg7t8b4s0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4122/Reviewer_z5sf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4122/Reviewer_z5sf"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces CONFAIDE, a benchmark based on contextual integrity theory that aims to pinpoint fundamental gaps in the privacy analysis abilities of LLMs fine-tuned through instructions. CONFAIDE is structured across four levels of escalating difficulty, culminating in a tier that assesses the understanding of contextual privacy and theory of mind."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Pros:\n1. This paper proposes a new study for LLMs and has some interesting discoveries. Specifically, this paper introduces CONFAIDE, a benchmark based on contextual integrity theory that aims to pinpoint fundamental gaps in the privacy analysis abilities of LLMs fine-tuned through instructions. CONFAIDE is structured across four levels of escalating difficulty, culminating in a tier that assesses the understanding of contextual privacy and theory of mind.\n2. Extensive experiments are conducted to support the discoveries."
            },
            "weaknesses": {
                "value": "Cons:\n1. The \u201cContextual Integrity\u201d concept cannot be explained by the given example: \"For example, if your healthcare provider shares your medical history, which contains sensitive health details, with an insurance company for marketing purposes, it would be a violation of contextual integrity. In this definition, it is not only the nature of the information that determines whether it can be shared or not, it is the context surrounding it as well.\u201d \"medical history\u201d itself is private information in nature regardless of the people to share. It is unclear that \u201cit is not only the nature of the information that determines whether it can be shared or not\u201d. It is suggested that authors could explain the example more clearly or change the example.\n2. Some references are missing. For example, The reference for \"theory-of-mind (i.e., tracking mental states of others)\u201d.\n3. The experiment design is questionable. The authors find that \u201cGPT-4 and ChatGPT reveal secrets 22% and 93% of the time in Tier 3, and flow information to inappropriate actors 39% and 57% of the time in Tier 4, even though they are directly instructed to preserve privacy.\u201d\n    1. a) First, how to ensure that LLMs really understand the concept of \u201cprivacy\u201d? Does LLMs understand the concept of \u201cprivacy\u201d in the same way of humans? Does \u201cprivacy\u201d indicate \u201ccannot be shared\u201d for LLMs in nature?\n    2. b) Second, do the LLMs really understand the experiment setting? How to ensure LLMs really understand the experiment setting? How to ensure that the LLMs\u2019 responses are meaningful? Although this paper has some brief discussion: \u201c(3) Understanding private information sharing: We evaluate the model whether it understands the concept of shared private information. Instead of explicitly specifying the information piece, we ask the model with the following prompt: \u201cList all the characters who shared the private information of X\u201d. Similarly, models should not include Z while including Y in their response.\u201d  It is still not very clear. More detailed explanations are desired.\n4. It is not well explained what causes the bad performance of LLMs. Does it mean that LLMs do not have theory-of-mind? Or LLMs potentially just do not understand the setting well?\n5. The significance of this study is not very clear. It is not convinced that \"contextual privacy\u201d is a very important problem for LLMs because this paper does not show convincing real-world examples to illustrate the importance of LLMs' \"contextual privacy\u201d property. And the Tier 3 & 4 in Figure 1 are part of theory-of-mind experiments for LLMs, which have been extensively studied (there is a related survey [1]). It is suggested the authors could differentiate this work from previous Theory of Mind works in survey [1] better.\n6. It is suggested the authors could explicitly summarize their contributions in the introduction.\n\n\n[1] Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models https://arxiv.org/abs/2310.19619"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4122/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4122/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4122/Reviewer_z5sf"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4122/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839984610,
        "cdate": 1698839984610,
        "tmdate": 1699636377056,
        "mdate": 1699636377056,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IUf9SfWw0O",
        "forum": "gmg7t8b4s0",
        "replyto": "gmg7t8b4s0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4122/Reviewer_8YWe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4122/Reviewer_8YWe"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an official benchmark to evaluate the privacy reasoning capabilities of LLMs. The dataset is constructed via different tiers of difficulty following contextual integrity theory. The paper highlights the importance of theory-of-mind for an LLM's privacy reasoning capabilities."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- strong foundation of approach in contextual theory\n- thorough experiments\n- clear presentation\n- human preference collection"
            },
            "weaknesses": {
                "value": "- no discussion of limitations of study (i.e. small samples sizes), and how the performance metrics might be misleading"
            },
            "questions": {
                "value": "1. For tiers 1 and 2, we find our results to be closely aligned with the initial results of Martin & Nissenbaum (2016), demonstrating a correlation of 0.85, overall --> correlation between what?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4122/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698856440077,
        "cdate": 1698856440077,
        "tmdate": 1699636376991,
        "mdate": 1699636376991,
        "license": "CC BY 4.0",
        "version": 2
    }
]