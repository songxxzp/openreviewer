[
    {
        "id": "YmPS2WSlXi",
        "forum": "SIojR1ruNQ",
        "replyto": "SIojR1ruNQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6219/Reviewer_A6PK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6219/Reviewer_A6PK"
        ],
        "content": {
            "summary": {
                "value": "This paper presents TIGERScore, a Trained metric that follow Instruction Guidance to perform Explainable, and Reference-free evaluation over a wide spectrum of text generation tasks. TIGERScore is guided by the natural language instruction to provide error analysis to pinpoint the mistakes in the generated text."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "*This paper proposes a novel metric TIGERScore that can provide error analysis and explanations without needing reference texts, making it widely applicable.\n* Achieves state-of-the-art correlation with human judgments across multiple text generation tasks like summarization, translation, QA etc. This demonstrates its reliability.\n* The error analysis and explanations make the metric more transparent and trustworthy. The human evaluation results validate their accuracy."
            },
            "weaknesses": {
                "value": "* There could be more details provided on the prompt engineering strategies used to create MetricInstruct.\n* The generalizability to open-ended generation like storytelling seems limited currently. More investigation into enhancing this is needed.\n* The correlation gains over prior metrics, while substantial, are incremental in some cases. The gains do not feel dramatically superior.\n* There is no comparison of computational efficiency with other metrics. Efficiency can be a practical concern."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6219/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698566782389,
        "cdate": 1698566782389,
        "tmdate": 1699636678737,
        "mdate": 1699636678737,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3ZoGRl88II",
        "forum": "SIojR1ruNQ",
        "replyto": "SIojR1ruNQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6219/Reviewer_Az3X"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6219/Reviewer_Az3X"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces TIGERScore (Trained Instruction Guidance metric for Explainable and Reference-free evaluation) based on LLaMA 2 for NLG evaluation. For training this metric using natural language instructions, the MetricInstruct dataset is introduced, containing 48k examples and covering six text generation tasks (Summarization, Translation, Data-to-text, LongFormQA, MathQA and Instruction Following) across 23 datasets. The proposed metric provides error analysis of the generated text by 1) Locating the mistakes 2) Explaining the mistakes 3) Providing a penalty score for each of the mistakes which are then summed up to get the final score (0 meaning perfect generation). The metric is shown to empirically correlate well with human judgements."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- NLG evaluation is an interesting problem for the community which is the main focus of this paper. \n- The introduced MetricInstruct dataset would be interesting for the community to develop similar novel metrics for NLG evaluation. \n- Different data collection strategies, incorporating both real-world and synthetic data enhances the model's robustness.\n- TIGERScore provides error analyses, and human evaluation indicates that these analyses are reasonable, comprehensive, and effective in guiding users to improve text quality."
            },
            "weaknesses": {
                "value": "- Even though the results are promising at the initial level, the spearman correlation of TIGERScore is lower than the other existing metrics for 4 out of the 7 tasks (Fig 1). Table 4 also shows a huge difference between the reference-free and reference-based metrics which hinders its usage as a universal metric (main contribution of the paper).\n- The paper does not extensively explore scenarios where TIGERScore might fail to provide accurate error analyses. Understanding its limitations could be valuable for users. The paper briefly mentions the hardware used for fine-tuning but lacks an in-depth discussion of computational resources required for implementing TIGERScore. \n- It would be nice to have a comparison with 0-shot GPT-4 based evaluation (having limited computational complexity) which has also been recently used as a reference free and explainable metric [1,2]. \n- There are no results related to hallucination or factual correctness of the generated text, which would have made the results more compelling. \n\n[1] QLoRA: Efficient Fine Tuning of Quantized LLMs\n[2] Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"
            },
            "questions": {
                "value": "- Have the authors also experimented with InstructScore which seems to be very related to the proposed approach? \n- Have the authors also computed inter-annotator agreement when performing human evaluation? \n- Did the authors experiment with 0-shot abilities of the LLaMA-2 (both chat and non-chat) models as a reference-free evaluation using prompts? \n- It is not clear why the model's max context length is set to 1024 when it can ingest 4096 tokens. \n- Table 6 - do authors have any intuition why there is decreased performance for Data2Text when all tasks are included? \n- Fig 3 and Table 6 - is there any specific reason why the results on MathQA and StoryGen are excluded? It would be nice to place the Figure and Table together for better comprehension. \n- Is there any curriculum used for training on the different tasks? \n- Would the corresponding code be made available which is necessary for reproducibility.\n\n\nSuggestions/Comments:\nFig 1 caption: shows te -> shows the\nSection 5.2: Research on this fields -> in these fields\nPlease take care of the quotes in Section 2, Table 3 caption\nPlease specify and distinguish between LLaMA and LLaMA2 in the abstract and the main paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6219/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799716520,
        "cdate": 1698799716520,
        "tmdate": 1699636678628,
        "mdate": 1699636678628,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aIM8mrBeRa",
        "forum": "SIojR1ruNQ",
        "replyto": "SIojR1ruNQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6219/Reviewer_WhZB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6219/Reviewer_WhZB"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed TIGERscore, a reference-less evaluation metrics for text generation tasks that improves the interpretability of the evaluation metrics by generating itemized, structured errors with information on location, explanation/correction suggestion, and a penalty score for each error. TIGERScore is built on LLAMA foundation models finetuned with MetricInstruct dataset, in which each sample consists of instruction, context, and output from some system as Input, and a list of structured errors with all aforementioned information as Target. The MetricInstruct dataset is the key to the good performance of TIGERSCORE, and is derived from 23 distinctive public NLG datasets covering six tasks, and the target structured errors are generated by querying GPT-3.5-Turbo and GPT-4 models with heuristic filtering of the candidate errors. On five out of seven test sets, TIGERScore has outperformed a wide range of reference-based and reference-less metrics in terms of human correlation, and additional human evaluation has confirmed the majority of the provided structured errors by TIGERScore to be reasonable and correction suggestions to be helpful."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality:\n\nSimilar works have been done on related topics of creating an evaluation metrics with a foundation model backbone that provides more than just a final score, but TIGERScore's way of constructing the additional evaluation output (the structured errors: location, explanation, suggestion, and penalty) is original in the sense of the level of granularity at which people have attempted in the design of a highly interpretable evaluation metrics.\n\nQuality:\n\nThe overall analysis is solid, the choice of using correlational analysis with human evaluation score is appropriate and to the point (to aim for an evaluation metrics that emulate human judgement), and an extensive selection of both reference-based and reference-less metrics have been provided as baselines, ensuring the resulting performance of the proposed metrics to be convincing as well as up-to-date.\n\n\nClarity:\n\nThe paper is generally very easy to follow with comprehensive examples and illustrations provided in both the main text and the appendix. There are places with typos and possibly incorrect sentences, for which the reviewer will provide a list in **Questions** below.\n\nSignificance:\n\nThe importance of having a robust, interpretable, and relevant evaluation metrics for text generation task is beyond doubt in current NLP research (see for example: https://arxiv.org/abs/2202.06935), and this paper has offered not only a competitive reference-less evaluation metrics, but also a valuable dataset (MetricInstruct) for furthering research on more robust and versatile evaluation metrics in the community. The significance is clearly high."
            },
            "weaknesses": {
                "value": "The major weakness of the paper is the lack of discussion on the distribution of quality in the provided dataset. The authors explained clearly their efforts in ensuring coverage of error types, but there is no clear discussion on how the quality of the system outputs studied in the test sets is controlled. For example, it is unclear how TIGERScore would perform on notes with no errors. It is also unclear if an output with a higher TIGERScore would actually reflect higher human preference in an A/B testing or a ranking scenario. Also, the authors didn't provide an exhaustive list of error types ($E_i$) used in their model training, therefore making the generalizability of the score to possible unseen error types questionable.\n\nAnother weakness is the clarity in the writing, the overall language is very easy to follow, but there are some problems with typos, non-sensical words, incorrect sentences, and orphaned table/figure. Please see **Questions** below for a non-exhaustive list."
            },
            "questions": {
                "value": "General Questions:\n1. How many system outputs are actually correct (no errors)? How well does TIGERScore perform on those outputs? Would it hallucinate errors frequently? Have you tried running TIGERScore on reference outputs?\n2. How are the prompt templates in Appendix 2 created? There are minor variations from task to task, is this intentional (e.g., the way minor and major mistakes are explained in the prompts changed from template to template)? \n3. Why explaining major and minor mistakes in the prompts, when you can ask the model to infer from the aspect list? Is this for providing two criteria (major/minor label, and penalty score) to filter unreasonable candidate errors? \n4. Some of the templates (e.g., Table 11) are missing prefixes like \"References:\", \"Task instructions:\", is this intentional or a typo? \n5. Why is $aspect_list not used in translation and data2text prompt templates?\n6. Are the definitions of evaluation aspects ($aspect_list) for each task created based on consensus or by the authors?\n7. Why is a scale from 5 to 0.5 chosen? Any special reason?\n\nEditing Suggestions:\n1. Figure 2 and Table 5 are never referred to in the main text, please fix this.\n2. Section 1, *...Our analysis shows that MetricInstruct is attributed to three key aspects in MetricInstruct: ...*; Probably the first *MetricInstruct* should be sth like \"success of TIGERScore\"?\n3. Section 2.3, although **Training Setup** in Section 4.2 has explained what the model behind TIGERScore is, it is arguably better to include an explanation of the backbone model and training procedure here (maybe in section 2.4). Otherwise it reads like the method section doesn't actually explain what the method itself is.\n4. Section 3.2, *...carefully designed prompting templates...*; you can make reference to tables in Appendix A.2 here.\n5. Section 3.3, *...we employ Levenshtein Distance to check whether the illusory location in analyses and exclude evaluations with a low token set ratio...*; I find this sentence very hard to understand, how is *token set ratio* defined for Levenshtein distance (the former disregards order while the latter respects it explicitly)? Do you mean low LD score? So the Levenshtein distance used is calculated on characters, or tokens, or words? If tokens, what tokens, tokens generated by the tokenizer of the model? Also *check whether the illusory location in analyses* is an incomplete sentence, perchance you mean *check whether there is illusory location in the analysis*?\n6. Section 4.1, *...System outputs to be evaluated of each test dataset either from official releases...*; again, incomplete sentence (there is no verb here), please rewrite\n7. Section 4.3, ***...Nota bene*** *that TIGERSCORE significantly surpasses traditional metrics...*; typo?\n8. Section 4.3, *...This suggests that TIGERSCORE can effectively evaluate text generation models even when the tasks have not been seen during training...*; Please consider rewriting this sentence or elaborate with more analysis, TIGERScore performs very weak on StoryGen (lower end among reference-based metrics and bottom one among reference-less metrics), so it is a stretch to claim it performs even reasonably well on unseen tasks.\n9. Table 5, second column, *...Explaination Error?...*; typo, please fix it.\n10. Section 4.3, *...In 70.6% of the error analyses, no errors were found to be missing...*; I assume 70.6% was calculated based on the number of selections with a score 3 or 4 by human; this is not rigorous given the example answer options in Appendix A.3: 3 still corresponds to explanations with missing errors (arguably less critical, but still). This is a common problem with some of the percentages referred to later in the same section, please state clearly how those percentages were calculated, and explain the reason when scores less than the highest value for each category are used (e.g., it seems 3 to 5 are used for calculating 70.8% for Overall Rating)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6219/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6219/Reviewer_WhZB",
                    "ICLR.cc/2024/Conference/Submission6219/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6219/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811005762,
        "cdate": 1698811005762,
        "tmdate": 1700690387021,
        "mdate": 1700690387021,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xBHfHpIUSr",
        "forum": "SIojR1ruNQ",
        "replyto": "SIojR1ruNQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6219/Reviewer_2ZJr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6219/Reviewer_2ZJr"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces TIGERScore, a novel evaluation methodology designed for natural language generation (NLG) tasks. The primary concept involves creating a new training dataset containing human preference scores sourced from existing NLG datasets. The authors augment this dataset by generating synthetic scores using GPT-4. Additionally, they leverage GPT-4 to produce error explanations that delineate the facets of errors. Training TIGERScore involves utilizing LLaMA-2. As a reference-free evaluation method, TIGERScore solely requires instruction, context, and the model output for evaluation. Experimental results demonstrate that TIGERScore exhibits a high Spearman\u2019s correlation with human preference scores in held-in evaluation datasets and some held-out evaluation datasets. Through human expert ratings, TIGERScore offers explanations for the assigned scores. An ablation study highlights the impact of the synthetic dataset and the synergy achieved by combining multiple tasks in contrast to training with only one task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The primary strength of this paper lies in proposing a robust and adaptable NLG evaluation methodology. It utilizes several NLG datasets as a training dataset and operates as a reference-free methodology. This characteristic enables other AI and NLP researchers to extend this concept for evaluating their individual NLG models."
            },
            "weaknesses": {
                "value": "This paper raises two concerns: overclaim and human evaluation. TIGERScore exhibits limitations, particularly evident in held-out tasks (story generation), raising doubts about the claim of being a universal metric for all text generation tasks in the title. The title prompts questions about its efficacy in especially evaluating open-domain conversation tasks, given the considerable diversity in model outputs.\n\nRegarding human evaluation, there's ambiguity about the identity of the 'human experts' and the reliability of their responses. And the overall rating of TIGERScore is noted as 70.8%, but it's unclear whether this score is deemed acceptable or not. It would be beneficial to compare TIGERScore with other existing explainable metrics in Section 5.2, demonstrating the differences from the baselines."
            },
            "questions": {
                "value": "- Have other correlation metrics, like Pearson and Kendall rank correlation coefficients, been considered? These metrics might reveal different aspects of the correlation between model outputs and human preferences.\n- Could you clarify who the 'human experts' are for the human evaluation?\n- It would enhance clarity to cite the paper when introducing terms for the first time, such as ASQA and FeTaQA terms in section 3.1.\n- Errata: Replace \"te\" with \"the\" in the Figure 1 caption."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6219/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699066539297,
        "cdate": 1699066539297,
        "tmdate": 1699636678363,
        "mdate": 1699636678363,
        "license": "CC BY 4.0",
        "version": 2
    }
]