[
    {
        "id": "oPvooKME9D",
        "forum": "fUGhVYPVRM",
        "replyto": "fUGhVYPVRM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2548/Reviewer_4pZf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2548/Reviewer_4pZf"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel method for improving the desired properties (latency and accuracy) of CTC-based speech recognition models. The core idea of the proposed method, called AWP, is to distinguish different alignment paths by prioritizing the one exhibiting the better property using an additional loss term. To promote such properties, simple rule-based strategies are employed to modify the alignment. For example, one may shift 1 token to generate a \u2018worse\u2019 (delayed) alignment path. Experiments conducted on online/offline ASR models show that AWP can boost the desired properties compared to previous baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper is very well-written and easy to understand. Especially, the introduction and related work sections are such a joy to read.\n* The comparative experiments against previous methods (including very recent ones) are conducted under the same condition. The result demonstrates the effectiveness of AWP (Table 2).\n* The proposed method appears to be a novel CTC modification utilizing a sampling-based hinge loss function. The approach is clearly different from previous methods pursuing similar objectives."
            },
            "weaknesses": {
                "value": "* In Table 2, only the \u2018Stacked ResNet Online\u2019 model case is compared with other methods. It would strengthen the claim if there exists more comparison for Conformer-Online + AWP, Peak First CTC, Trim-Tail, etc.\n* While the latency reduction part presents extensive experimental results (various training data sizes, comparison, ablation, ...), there are not many results on minimum WER training. Furthermore, the gain from minimum WER training is marginal.\n* There seems to be room for improvement; for example, how about increasing the number of shifted frames (tokens) instead of selecting just one? How about applying AWP together with Trim-Tail? I appreciate the simplicity of the proposed method, but I am also curious about the limitations of this method."
            },
            "questions": {
                "value": "* What is WSR in Figures 6 and 7? Is it (100 \u2013 WER)?\n* It seems that AWP needs random sampling at each training step. How many alignments (N) do you sample for each step? How does AWP affect the overall training time/resource usage?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2548/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2548/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2548/Reviewer_4pZf"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2548/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697904585254,
        "cdate": 1697904585254,
        "tmdate": 1700800670431,
        "mdate": 1700800670431,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oxaApHlpU3",
        "forum": "fUGhVYPVRM",
        "replyto": "fUGhVYPVRM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2548/Reviewer_jMFR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2548/Reviewer_jMFR"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a plug-and-play framework to CTC loss so as to improve the performance of trained model on a specific perspective. The preference is achieved by a hinge loss calculated between an example and a better example. The experiments on ASR with different data scale show that the proposed method can help model to recognize text promptly or accurately."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The design of Align With Purpose is nice and interesting. The method makes use of a fact that in conventional CTC loss, all the perfect alignment are treated equally. In this case, the preference can be achieved by helping model compare to possible paths. The idea is clear and the paper is well-written. The experiments also verify the effectiveness of proposed method."
            },
            "weaknesses": {
                "value": "I do not witness obvious weakness of this paper."
            },
            "questions": {
                "value": "It seems that start epoch is a sensitive parameter, which is different in different model. So how is start epoch defined? By grid search?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2548/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698657360585,
        "cdate": 1698657360585,
        "tmdate": 1699636191251,
        "mdate": 1699636191251,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zGqLgy5Ocj",
        "forum": "fUGhVYPVRM",
        "replyto": "fUGhVYPVRM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2548/Reviewer_bLBJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2548/Reviewer_bLBJ"
        ],
        "content": {
            "summary": {
                "value": "For speech recognition task with CTC loss (and others) we assume equal weights between different alignments and we optimize the total probability of all correct alignments. Authors of the paper are concerned about the latter fact and push for considering different weights for different alignments. Authors propose an independent loss, plug and play, which will control what alignments are more preferable to inherit desired properties, like latency (emit tokens faster, without delay and drift) and minimum WER instead of loss. They claim that proposed method is simple compared to prior works and adds few lines of code with no need to change the main loss function (e.g. CTC) used in training. Results on low latency and minimum WER for different models (transformer, conv), data scale and tasks show validity of the proposed idea."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- exploring idea on reweighing alignments depending on the task / desired properties, e.g. better latency\n- simplicity of the method, including plugin property instead of modifying CTC loss itself"
            },
            "weaknesses": {
                "value": "- absence of comparison of proposed method with prior works and baselines (e.g. if we introduce reweighing of alignments directly in the CTC loss)\n- complexity of additional hyper-parameters choice (no robustness), e.g. when we start the additional proposed loss function optimization\n- missing details on how exactly the sampling of the word to be corrected is implemented, as it could be that no word is available for substitution. Why WER and not CER as language model could fix the errors? Is language model used in this process?\n- I believe that sampling alignments is not the right / optimal way: we could consider optimization of the top-N alignments instead, as otherwise we spend time on optimization low probability alignments. \n- A bunch of models in Table 2 (on latency empirical results) are not comparable: either latency should be fixed and WER is compared or vice versa. Right now it is hard to make any conclusions from Table 1 due to different values for latency and WER of different models.\n- Results in Table 3 are within std on Librispeech as 0.1 variation is normal between different models often for clean part of test set. Also it is not clear if improvement is consistent for both greedy and LM decoding or only for the latter one. Greedy decoding should be reported too to make full clear picture how the proposed method improves results.\n- I found it overall hard to formulate the proper reweighing between different alignments, rather than simple way of controlling the latency by restricting context or optimising directly mWER. It is not clear why proposed way of sampling alignments is sufficient or significantly beneficial. Overall, reported results are only marginal."
            },
            "questions": {
                "value": "There are many typos in the text, including missing dots in the end of sentences, usage of words with capitalisation for the first letter and some ambiguity in sentence formulation, style of citations in brackets or without brackets, dashes usage. Proof-read is needed for the final revision.\n\nComments / Questions / Suggestions\n- \"CTC posteriors tend to be peaky (Zeyer et al., 2021; Tian et al., 2022), and hence the posterior of one specific alignment is dominant over the others.\" I would smooth this formulation a bit, as likely several tokens are dominant for each time frame, and thus set of alignments (few) are dominant, not only one as discussed in a bit in Likhomanenko, T., Collobert, R., Jaitly, N., & Bengio, S. (2023, February). Continuous Soft Pseudo-Labeling in ASR. In Proceedings on (pp. 66-84). PMLR.\n- what will happen if we choose top-N alignments instead of sampling them? \n- what will happen if we use several tokens removal for the latency function instead of only 1 token? is it improving latency?\n- \"35K hours curated from LibriVox\" is it multilingual or English only? If multilingual, why not English only as then another confound factor is introduced?\n- I would suggest to report results with both greedy decoding and language model, also report both clean and noisy LibriSpeech as a lot of effects are not visible on clean anymore.\n- Throughout the text it is not clear where LM is used or not in the reported numbers.\n- Seems in Table 2 performance of conformer model is not so good as in the prior paper (3.7 vs 2.0), or check Squeezeformer baselines.\n- What is WSR abbreviation (I could not find this notation in the text)? Figure 6 is hard to parse in the current form.\n- Why are multilingual wav2vec used for experiments? why not English only?\n- I found it surprising that Gumbel softmax with temperature is similar to the standard sampling. Some discussion would be helpful on this topic in the main body, as seems we are very limited with potential improvements if we manipulate with alignments weights.\n- what is the relation between Table 2 and Table 4 for the prior methods? I see that Table 4 has better results in WER than in Table 2.\n- why are features computed with \"32ms window with a stride of 16ms\"? This is really very non-standard Mel filter banks extraction for ASR models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2548/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698876558629,
        "cdate": 1698876558629,
        "tmdate": 1699636191180,
        "mdate": 1699636191180,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kXxPZpCKUv",
        "forum": "fUGhVYPVRM",
        "replyto": "fUGhVYPVRM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2548/Reviewer_Nofc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2548/Reviewer_Nofc"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a framework to train a CTC model with a desired property by adding an auxiliary loss. It samples N alignments from a pre-trained CTC model, feeds them to a property-designed function to get N better alignments w.r.t. the property, then adds a hinge loss on each pair of alignments as an auxiliary loss to the original CTC loss, to increase the probability of the better alignments with the desired property. The proposed framework is experimentally tested in applications to optimize latency and WER respectively and has shown improvement of the designed property compared to the vanilla CTC training and some other existing approaches."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed framework is flexible and simple enough to generalize to different properties and provides a generic way to makes the CTC training more controllable.\n- For the low latency application, the proposed framework achieves better latency and quality tradeoff than a few existing approaches and be on par with another best approach (TrimTail).\n- For the minimum word error rate application, the proposed framework achieves some modest improvement over the MLE baseline."
            },
            "weaknesses": {
                "value": "- For the minimum word error rate application, there has been a number of work in optimizing it in a discriminative sequence training setting for ASR, e.g. MBR training, large margin training, etc. The proposed framework should be compared to those stronger baselines instead. Right now it is only compared to the weaker vanilla MLE baseline with some modest improvement. In particular, if the property function is to generate the ground truth alignment, instead of only allowing reducing 1 word error, then it should be closer to the traditional discriminative sequence training setup.\n- For the minimum word error rate application, Prabhavalkar et al. 2018 found that using the n-best beam search hypotheses is more effective than the sampling-based approach. This paper should do a similar comparison whether it should compute the property function in the n-best alignments instead of sampled alignments.\n- Latency and WER optimizations typically compete with each other. It would be great to utilize the proposed framework to optimize these two properties jointly with a single property function to see if they can be balanced better together, and also see how effective and generalizable the framework is.\n- How sensitive is the optimization to the specific choice of the property function? E.g. the currently designed latency function only allows one time step faster, and property function for the minimum WER application only allows one word error reduction. Are these choices made in order to stabilize the training, or actually they can be relaxed to allow more changes as well? The paper should compare more different property function choices for the same specific application.\n- For the latency application, another intuitive approach would be to sample an alignment corresponding to the ground truth label sequence, and then the property function would be to run a force aligner to get the more accurate time alignment for the label sequence. How would this compare to the current proposed approach?\n- Adding the latency optimization to an offline Conformer model doesn't make much sense, since the full-context model is not used in a streaming fashion and it has to wait for the entire sentence to come first, which by itself already has a much higher latency. Conformer can be implemented in a streaming manner as well by just using the left context, which can be optimized for edge devices as well. The latency experiment should be conducted on an online Conformer.\n- Using \"start epoch\" as a tunable hyperparameter to control the balance between latency and quality is a bit strange. How transferable is the optimal start epoch to different learning schedule, model architectures and data?"
            },
            "questions": {
                "value": "- In Figure 6: What is WSR? It is not a standard metric and it is not defined anywhere.\n\nSee other questions above in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2548/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2548/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2548/Reviewer_Nofc"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2548/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699664463228,
        "cdate": 1699664463228,
        "tmdate": 1699664463228,
        "mdate": 1699664463228,
        "license": "CC BY 4.0",
        "version": 2
    }
]