[
    {
        "id": "tDOzx90LnE",
        "forum": "i5da6iedW8",
        "replyto": "i5da6iedW8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6725/Reviewer_nUtE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6725/Reviewer_nUtE"
        ],
        "content": {
            "summary": {
                "value": "Due to privacy concerns, data owners and large language model (LLM) owners are reluctant to share data and models. This paper proposes FedBiOT, a method that ensures data privacy while enabling fine-tuning of LLMs on federated learning tasks. It formulates and solves a bi-level optimization problem to distill an emulator on a public dataset that can support local fine-tuning on private datasets without disclosing the LLM."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The combination of LLM and federated learning is interesting.\n2. The problem formulation is well presented.\n3. The layer selection and dropout mechanism is interesting."
            },
            "weaknesses": {
                "value": "However, there are some improvements for the paper:\n1. The number of clients is very small. In section 4.1, the number of clients is 4, which is relatively very small compared with that in real FL settings.\n2. The idea is straightforward, which is presented in existing works, e.g., Yosinski et al., 2014.\n3. The selection of dropout rate is not well elaborated. \n4. Tables 1 and 2 are not clear. The first line is not explained. The unit can be added, and the meaning of the numbers can be explained.\n5. The experimentation show that the performance of FedBiOT may be inferior than baselines.\n6. The classic FL approaches can be added as baselines."
            },
            "questions": {
                "value": "1. Tables 1 and 2 are not clear. The first line is not explained. What is the unit can be added?\n2. The classic FL approaches can be added as baselines. I wonder if the authors can compare FedBiOT with classic approaches."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6725/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697987699626,
        "cdate": 1697987699626,
        "tmdate": 1699636773380,
        "mdate": 1699636773380,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0BprflfTDA",
        "forum": "i5da6iedW8",
        "replyto": "i5da6iedW8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6725/Reviewer_hDuF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6725/Reviewer_hDuF"
        ],
        "content": {
            "summary": {
                "value": "This paper considers a relatively new setting: federated learning over large language models. There are two key considerations of this paper: limited computation resource of clients and intellectual property of server's full LLM. Based on these motivations, this paper proposes an FL algorithm FedBiOT, which trains adapter and emulator in a bi-level optimization manner. Experiments on three datasets shoe the effectiveness of FedBiOT by comparing with two baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper considers a relatively new setting in federated learning and large language models.\n- This paper proposes a new FL algorithm FedBiOT, which trains adapter over emulator to achieve parameter-efficient tuning.\n- Experiments show the effectiveness of FedBiOT by comparing with two baselines."
            },
            "weaknesses": {
                "value": "- The contributions need to be clarified. For me, I think the topic of this paper is interesting and worth exploring. However, it is not so clear what are the main contributions of this paper since previous work [1] has considered such setting and proposed FedOT (federated learning with offsite-tuning). Are the main contributions lying on improving FedOT via a bi-level optimization approach?\n- The motivations need to be further clarified. This paper claims that the clients cannot obtain the full model due to intellectual property of LLM. However, I wonder if such claim still holds after the release of Llama2.\n- Some meaningful experiments are missing.\n  - Some experiments for reference. It would be more helpful if the authors can provide the results when clients can obtain the full model, such that we could see how large the gap is.\n  - Computation resources comparisons. This method requires more training resources (e.g., more training steps) compared to baselines. However, this paper does not show such comparisons, which would promote readers' understanding.\n- Some confusions:\n  - \"Improvement 1\" at page 5. What are the definations of bottom / first / last layers. Suggest consistent expressions like first / last.\n\nCurrently, my rating is between 5 and 6. I would consider re-rating if the authors can address the above concerns.\n\n[1] Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, and Jingren Zhou. Federatedscope-llm: A comprehensive package for fine-tuning large language models in federated learning."
            },
            "questions": {
                "value": "- According to this sentence \"We apply offsite-tuning with one single client, where all data are loaded to the client.\", it seems like the Offsite-tuning is training over the gathered dataset of all clients. But why its performance is quite low? Please describe how you implement offsite-tuning with more details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6725/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698488820999,
        "cdate": 1698488820999,
        "tmdate": 1699636773263,
        "mdate": 1699636773263,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ta9TatQnmd",
        "forum": "i5da6iedW8",
        "replyto": "i5da6iedW8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6725/Reviewer_4sKJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6725/Reviewer_4sKJ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces FedBiOT, a method that guarantees the clients\u2019 data privacy and avoids the disclosure of an LLM. The authors conduct extensive experiments on LLaMA-7B training for various federated learning tasks and witness significant improvements over existing baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The topic is timely and interesting."
            },
            "weaknesses": {
                "value": "1. The experimental evaluation was only implemented in LLaMA-7B. How does it work on other mainstream models such as ChatGPT2?\n2. In the experiment, federated learning only considered 8 clients. There is a lack of experiments that vary the number of clients and the number of training samples each client own."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6725/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833475907,
        "cdate": 1698833475907,
        "tmdate": 1699636773156,
        "mdate": 1699636773156,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TnQupyYQ8Y",
        "forum": "i5da6iedW8",
        "replyto": "i5da6iedW8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6725/Reviewer_uuim"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6725/Reviewer_uuim"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes improvements to offsite tuning, a pre-existing large-scale federated learning model's partial tuning method. It is built upon the existing approach of offsite tuning, which involves splitting a transformer model into various sub-models by layer index, such as adapters and emulators. During FL training, clients receive a combination of adapters and emulators, with the emulator being frozen while the adapter is fine-tuned. This paper makes two key improvements. First, it selects the last few transformer layers as the adapter. Second, it introduces a public dataset on the server and reduces the KL divergence between the adapter-emulator and full model outputs through knowledge distillation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe paper is built upon a relatively recent work so that it may offer modern insights into the related research fields.\n\n2.\tExperimental results support the proposed improvements in the paper.\n\n3.\tThe proposed improvements in the paper are general and should be easy to adopt."
            },
            "weaknesses": {
                "value": "1.\tFrom a technical perspective, the two improvements proposed in the article may be incremental. One involves changing the index of the fine-tuning layers (based on observation), and the other relies on the traditional distillation method. Both methods are essentially at the level of tricks and are insufficient to serve as contributions to the paper.\n\n2.\tI have doubts about the \"intellectual property protection\" aspect of the paper. In this framework, although local clients can only obtain a portion of the model instead of the entire model, this sub-model can still be fine-tuned and used for inference, which implies that the majority of the model's functionality has been preserved. Essentially, malicious users can still steal this intellectual property. This framework does not seem to provide significant protection, so I do not consider the \"intellectual property protection\" mentioned in the title appropriate.\n\n3.\tThe paper should provide a detailed algorithm to help readers follow."
            },
            "questions": {
                "value": "Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6725/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698997985023,
        "cdate": 1698997985023,
        "tmdate": 1699636773056,
        "mdate": 1699636773056,
        "license": "CC BY 4.0",
        "version": 2
    }
]