[
    {
        "id": "3ABuKQfRop",
        "forum": "nFMS6wF2xq",
        "replyto": "nFMS6wF2xq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2579/Reviewer_YRdg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2579/Reviewer_YRdg"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a contextualized diffusion model ContextDiff to facilitate the learning capacity of cross-modal diffusion models. It incorporates the cross-modal interactions between text condition and visual sample into both forward and reverse processes, serving as a context-aware adapter to optimize diffusion trajectories. It is also generalized to both DDPMs and DDIMs for benefiting both cross-modal generation and editing tasks with detailed theoretical derivations. Experiments in text-to-image generation and text-to-video editing tasks show its effectiveness. Empirical results reveal that it can successfully improve the semantic alignment between text conditions and synthesis results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) This paper for the first time proposes the text-guided visual diffusion model to consider cross-modal interactions in both forwarding and sampling processes.\n\n2) The authors propose their contextualized diffusion model (ContextDiff) and generalize it to DDPMs and DDIMs through the derivations of theoretical formulas.\n\n3) The experiments show that the proposed method has improvements on two tasks: T2I generation and T2V editing, compared with other state-of-the-art methods."
            },
            "weaknesses": {
                "value": "1) This paper claims the problem that neglecting the cross-model context in the forward process may limit the expression of textual semantics in synthesis results, but there is no clear explanation of the specific reasons, and there is no intuitive and theoretical analysis of the necessity of adding cross-model to the forward process.\n\n2) As claimed by the authors: \u201cThus CONTEXTDIFF is theoretically capable of achieving better likelihood compared to original DDPMs\u201d. Please provide the quantitative results on ELBO/ likelihood compared to the baseline.\n\n3) Since this paper is a general improvement on the conditional diffusion model, more results on different conditional generation tasks, such as class-to-image/layout-to-image\u2026should be provided. \n\n4) Some configurations in T2V editing experiments are confusing, as the experiments based on pre-trained Stable Diffusion v1.4 are not enough to prove that the approach of this paper can enable diffusion models better editing ability."
            },
            "questions": {
                "value": "1) Are there any more experiments that can prove the text-based editing ability of this approach, for example, conducting T2I editing or T2V editing based on the well-trained T2I generation model of your first experiment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2579/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698219383590,
        "cdate": 1698219383590,
        "tmdate": 1699636195369,
        "mdate": 1699636195369,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "T1QGxCSZ4Y",
        "forum": "nFMS6wF2xq",
        "replyto": "nFMS6wF2xq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2579/Reviewer_zGd6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2579/Reviewer_zGd6"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a novel conditional diffusion model, ContextDiff. Rather than only modeling the cross-modal context in the backward process, ContextDiff propagates the context information to all timesteps in both forward and backward process to adapt the trajectories for facilitating cross-modal conditional generation. The proposed method can be generalized to DDPMs and DDIMs and achieves better results in text-to-image generation and text-to-video editing."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* The idea of modeling the cross-modal context in the forward process is interesting, as it differs from previous works that only consider conditional modeling in the backward process.\n* The method has sound theoretical foundations. Furthermore, the generalization to DDIMs is a clear strength that allows fast sampling.\n* The writing of the method section is clear. The adaptation to the previous diffusion process with a bias term is straightforward to understand.\n* The evaluation results show that the model performs better in terms of qualitative metrics of both automated evaluation and user study."
            },
            "weaknesses": {
                "value": "* Experiments on latent diffusion: the method uses an Imagen-based framework, which generates a low-res image and then performs super-resolution. However, the author does not evaluate the proposed method on latent diffusion architecture. There are mentions of LDM in Sec 5.3 (ablations), but the setting is not clearly described, and comparisons with other works (rather than the baseline) are not offered.\n* The author does not offer an inference latency evaluation. Does the method slow inference down compared to baseline diffusion methods that do not have context-aware adapters?\n* A small typo (which does not affect the rating): \"A red ross\" -> \"A red rose\"?"
            },
            "questions": {
                "value": "* How does the method compare to the baseline when integrated into latent diffusion (or Stable Diffusion)?\n* How does the method compare to the baseline in terms of inference latency?\n* What is the Stable Diffusion version used in Table 1? How does the method compare with different versions of Stable Diffusion?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2579/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2579/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2579/Reviewer_zGd6"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2579/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698600592422,
        "cdate": 1698600592422,
        "tmdate": 1699636195285,
        "mdate": 1699636195285,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EtHEMvSDEK",
        "forum": "nFMS6wF2xq",
        "replyto": "nFMS6wF2xq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2579/Reviewer_D2VS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2579/Reviewer_D2VS"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a  general cross-modal contextualized diffusion model (CONTEXTDIFF) that harnesses cross-modal context to facilitate the learning capacity of cross-modal diffusion models. The cross-modal interactions between text condition and image/video sample are incorperated into the forward process, serving as a context-aware adapter to optimize diffusion trajectories. The context-aware adapter to adapt the sampling trajectories, which facilitates the conditional modeling in the reverse process and aligns it with the adapted forward process. A series of experimental results and mathematical proofs are presented."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper proposes a method to enhance the multimodal relevance by incorporating multimodal contextual information during the forward process of the diffusion model.\n\n2. Adequate mathematical proofs are provided for both the forward and backward processes.\n\n3. The experimental results to some extent demonstrate that this method yields a high semantic correlation between the generated images and text."
            },
            "weaknesses": {
                "value": "1. Compared to the proposed method, existing methods also utilize cross-modal attention during the forward process to control the generated content of images based on information from different modalities.\n\n2. The article does not provide sufficient analysis for why adding textual information during the forward process enhances multimodal semantic relevance. It is based on intuitive reasoning rather than an in-depth analysis.\n\n3. In the provided experimental results, the method proposed in this paper shows limited improvements compared to existing methods (e.g. Imagen) in image generation tasks.\n\n4. The paper does not analyze the limitations of the proposed method."
            },
            "questions": {
                "value": "1. In Figure 6, the author claims that they conduct ablation study on the trade-off between CLIP and FID scores across a range of guidance weights, however, only FID scores are provided in the figure.\n\n2.  Analysis on why adding textual information during the forward process enhances multimodal semantic relevance.\n\n3. Whether this method can be incrementally trained on other pretrained generative models (e.g. Stable Diffusion), and if doing so would result in improved generation performance and faster convergence, is not discussed in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2579/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698636325159,
        "cdate": 1698636325159,
        "tmdate": 1699636195127,
        "mdate": 1699636195127,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wUHSQM4V9Z",
        "forum": "nFMS6wF2xq",
        "replyto": "nFMS6wF2xq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2579/Reviewer_DNxF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2579/Reviewer_DNxF"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the concept of contextualized forward and reverse diffusion processes, which is interesting. They have made modifications to traditional models and proposed a method that significantly improves semantic alignment. The experimental results section effectively demonstrates the efficacy of this approach. The authors provide detailed theoretical and empirical evidence, which lends strong support to this article."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The novelty of this article lies in the introduction of a new contextualized forward and reverse diffusion processes. They have made improvements upon existing methods and provided theoretical support.\n- The results presented in this paper have shown promising performance when compared to existing models, and the visualization section further supports the efficacy of this meth"
            },
            "weaknesses": {
                "value": "- Regarding evaluation metrics, while existing metrics are commonly used, aligning text, especially fine-grained text content, with images requires more refined evaluation criteria. I would like to hear the authors' opinions on the need for improved evaluation metrics for more fine-grained, context-aware image and video generation. Additionally, why not incorporate human evaluation of the generated data in this context?\n\n-  The authors have introduced some additional controls, and it would be beneficial to discuss the associated costs, such as extra parameters, training time, and testing time, to aid in understanding their proposed method.\n\n- The authors have achieved promising results in natural images or videos, but there is a need for further discussion regarding more fine-grained context-awareness. For instance, it would be interesting to explore whether the method remains effective when dealing with specific text from within an image, as generating precise text remains a challenge for most methods. Similarly, what happens when modifying specific parts of an image? I would like to see the authors' insights on these issues concerning their context-aware adapter."
            },
            "questions": {
                "value": "The questions I would like the authors to address have already been raised in the \"weakness\" section. I hope the authors can provide more information on these aspects."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2579/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698973247258,
        "cdate": 1698973247258,
        "tmdate": 1699636195044,
        "mdate": 1699636195044,
        "license": "CC BY 4.0",
        "version": 2
    }
]