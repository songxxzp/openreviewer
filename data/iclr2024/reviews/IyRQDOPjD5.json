[
    {
        "id": "G3XJlT81s3",
        "forum": "IyRQDOPjD5",
        "replyto": "IyRQDOPjD5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7245/Reviewer_HSfz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7245/Reviewer_HSfz"
        ],
        "content": {
            "summary": {
                "value": "This work studies the elastic weight consolidation (EWC) and tries to alleviate the computation burden of the Fisher Information Matrix (FIM). The main contribution is to propose a method for obtaining the full FIM. The performance is examined in the image classification and reinforcement learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is relatively easy to follow but there still remains room to improve the organization. The vector product or matrix decomposition in learning is reasonable, and this trick most applied to sparsity, e.g. kernel matrix in Gaussian processes.\nThe examination of continual learning is relatively but not all standard in terms of benchmarks."
            },
            "weaknesses": {
                "value": "**1. About the layout of this paper.**\n\nIt seems there is too much background knowledge about the computation bottleneck in EWC from Page2 to Page4. There misses some parse in paragraph or key points are not well highlighted. These make it a bit difficult to follow in logics.\n\n**2. About the contribution.**\n\nThe theoretical advantage to obtaining the full FIM is not well clarified in this work. Does it mean more information about the FIM brings better generalization in a theoretical sense? In the visual abstract, it seems the combined EWC can achieve more superiority than the full EWC, while this work focuses on the full EWC. Does this violate the research motivation? Meanwhile, it is necessary to connect some proposition to empirical observations.\n\n**3. About the evaluation in reinforcement learning.**\n\nI am afraid three Atari games are not typical in the continual learning domain. There exist more convincing benchmarks, e.g., Continual World, for continual reinforcement learning to examine the performance."
            },
            "questions": {
                "value": "See the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7245/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698121782045,
        "cdate": 1698121782045,
        "tmdate": 1699636863287,
        "mdate": 1699636863287,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "D6ItBQR1bl",
        "forum": "IyRQDOPjD5",
        "replyto": "IyRQDOPjD5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7245/Reviewer_mrqD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7245/Reviewer_mrqD"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the SHVP (Surrogate Hessian Vector Product) algorithm, as a means to compute the product of the Hessian matrix of a neural network with a given vector, without needing to compute the matrix itself (thus significantly reducing the computational complexity), but by relying on two nested backpropagations instead.\n\nUsing this, the authors are able to train models in a continual learning context, using the Elastic Weight Consolidation (Kirkpatrick et al. 2017) training objective to avoid catastrophic forgetting, while using the complete Hessian matrix, as opposed to a diagonal approximation as is often used in practice for computational reasons.\n\nThe paper experimentally shows that using the full Hessian matrix improves the model's capacity to retain its performance on older tasks, and that the best performance is obtained by combining the full EWC with its diagonal approximation, allowing an even equilibrium between older and more recent tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The article is clear, and provides good background and context to position its work in the larger literature.\n\nThe proposed algorithm is sound and well justified, and the experimental exploration of the impact of the different variants of EWC, in relation with the two training regimes is an interesting contribution.\n\nOverall, I believe the SHVP algorithm introduced here can be an useful tool to explore the Hessian matrices of neural networks."
            },
            "weaknesses": {
                "value": "I have one main issue with the paper and its contribution: the positioning of the authors with regard to how and when their proposed Combined EWC could be used is very unclear.\n\nAs far as I can tell, the proposed SHVP algorithm trades the one-time computation of the full Hessian of the model for a double backprop through the NN at every iteration of the training. In particular, this means that computing the gradient associated with the EWC regularization term *requires computing an expectation over the datasets of the previous tasks*.\n\nMy understanding is that the main appeal of EWC is to keep some kind of summary of the previous tasks as the Hessian of their losses (approximated as diagonal or block-diagonal for computational efficiency). This relies on the idea that keeping around the training data of the previous tasks is either not desirable or not possible, otherwise one would simply train their model on all tasks simultaneously.\n\nHere, the proposed \"Combined EWC\" requires keeping access to the data of the previous tasks, and appears to be more computationally expensive than just training the model on the multiple tasks simultaneously.\n\nAs a result, it is unclear to me what does \"Combined EWC\" actually bring to the table: while not stating it clearly, the paper frames it as an algorithm that could be used in practice to train models in the continual learning setting. But given the above remarks, I fail to see when one would actually want to do that."
            },
            "questions": {
                "value": "**How is the computation defined by Proposition 1 done in practice?**\n\nThe paper and appendix are not very detailed about it, and the joined code is barely documented, making it difficult to understand. I don't see how it would be possible to compute the double gradient defined for SHVP without retaining the whole computational graph associated with task A in memory (for performing the second backprop).\n\n**When would someone use SHVP or Combined EWC in practice?**\n\nGiven the previous discussion, I'm having a difficult time figuring when one would opt for using Combined EWC, instead of simply training the model on multiple tasks simultaneously, given EWC is supposed to be an approximation of that. What point is there for me to use an approximation that is not cheaper to compute than the actual thing?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7245/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698757050878,
        "cdate": 1698757050878,
        "tmdate": 1699636863145,
        "mdate": 1699636863145,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cK9J6qxpY5",
        "forum": "IyRQDOPjD5",
        "replyto": "IyRQDOPjD5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7245/Reviewer_gTVN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7245/Reviewer_gTVN"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method to calculate the full Fisher Information Matrix for EWC (instead of the more common diagonal Fisher) that is computationally and memory efficient. The paper then argues that combining the full EWC with a diagonal EWC is better than just one or the other. The paper then argues that EWC works better in the lazy training regime (with large parameter initialisations). Finally, in Section 5, the paper applies their method to an RL problem (3 Atari games sequentially shown)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. I really like the algorithm for Full EWC in Section 3. I like that the authors looked at empirically verifying it in Appendix A.1, and would have liked to see more of this! \n\n2. Applying to RL / Atari is not done often enough in continual learning, so it was nice to see that experiment in this paper. \n\n3. I liked the proof sketches in the main text; I thought they were well-written and useful."
            },
            "weaknesses": {
                "value": "1. It is important to talk about other related works that use eg block-diagonal Fishers for EWC in Section 2. For example, Ritter et al., 2018 (A Scalable Laplace Approximation for Neural Networks). \n\n2. Permuted MNIST is an old benchmark with many problems. It is difficult to draw many conclusions of note from it due to the various issues with it (see for example Farquhar and Gal (Towards Robust Evaluations of Continual Learning) or Swaroop et al (Improving and Understanding Variational Continual Learning) for discussions). Additionally, the authors in this paper only use 5 tasks, which is very few, making it difficult to draw any real conclusions. It would be great if the authors ran for significantly more tasks to see if their conclusions/results still hold (eg 20 tasks). \n\n3. When comparing Figures 2 and 3, I'm not really sure if the lazy training regime is better for EWC! It looks like Figure 2 often has higher performance, at least on everything but task A's accuracy after training on the last task. Could the authors report other metrics like overall average accuracy (and maybe forward/backward transfer) too? \n\n4. Proposition 2 seems like it is re-writing that, under conditions like constant covariance (ie a quadratic loss landscape?) and mean-squared error loss, that Laplace approximation is ideal. I am not sure that there is anything new here: this is the reason that eg the EWC paper used the Laplace approximation / FIM in the regulariser / Bayesian approach. I found it odd that, in the Appendix, the authors prove this via \"Bayes Optimal estimators\" (ie looking at predictions at a test point?) and then take the limit as the Gaussian approximation becomes a delta (ie reduce the covariance to 0). This looks like a simple MAP estimation to me? Please let me know if I am missing something here. \n\n5. In the text on page 7, the authors argue why the landscape may be (more) locally convex in the lazy training regime, because parameter values do not change much during training. I am not sure I am convinced by this: just because the parameter values do not change (relatively) very much, this does not mean that the loss landscape that the parameters move through is better-behaved: it could still be highly non-convex. Is there previous literature on this (or could the authors design an experiment to show this)? \n\n6. I do not understand the intuition for why Combined EWC is better than Full EWC or diagonal EWC. Diagonal EWC seems to perform very well in Figures 2 and 3 already. In Figure 4, it does not forget task 2 when training on task 3 (although it does forget task 1), against the authors' conclusion (that Diagonal EWC prioritises new tasks). I think I need much more evidence of these claims (eg that Diagonal EWC prioritises new tasks while Full EWC prioritises old tasks) to believe them sufficiently. \n- Also, although it is nice to have an RL experiment in Section 5, having only 3 tasks/games is too few to draw conclusions."
            },
            "questions": {
                "value": "Please see Weaknesses section. More minor questions / comments: \n1. Note that the authors are using the empirical Fisher, not just the Fisher / FIM, in Equation 1 (and throughout the paper). See for example Kunstner et al., 2020 (Limitations of the Empirical Fisher Approximation for Natural Gradient Descent) for a discussion. \n2. I think, in the text after Equation 6, the authors meant 'low uncertainty' and not 'high uncertainty'? \n3. At the bottom of page 8 (and in Sec A.7.1) the authors use a network of half the size and see lower performance. I do not see how this helps understand if the experiment in Section 5 is sufficiently over-parameterised or not."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7245/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786280095,
        "cdate": 1698786280095,
        "tmdate": 1699636862987,
        "mdate": 1699636862987,
        "license": "CC BY 4.0",
        "version": 2
    }
]