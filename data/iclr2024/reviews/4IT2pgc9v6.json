[
    {
        "id": "cTkh8jfrbF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2877/Reviewer_WF3H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2877/Reviewer_WF3H"
        ],
        "forum": "4IT2pgc9v6",
        "replyto": "4IT2pgc9v6",
        "content": {
            "summary": {
                "value": "The paper proposes One for All (OFA), a novel framework that addresses how to unify various graph tasks with various graph data.  OFA uses text-attributed graphs, allowing nodes and edges to be described in natural language, and introduces a new graph prompting method for diverse tasks without fine-tuning. When trained across multiple graph data domains, OFA demonstrates strong performance, making it a pioneering multi-purpose graph classification model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.LLM and graph prompting are both highly effective methods in their respective domains. Combining the two to address the cross-domain TAG problem is both convincing and well-motivated. I highly appreciate the idea of this paper, which is quite enlightening.\n2. This paper is well-articulated."
            },
            "weaknesses": {
                "value": "1. A work on graph prompt learning should be cited and discussed.\n* Tan, Zhen, et al. \"Virtual Node Tuning for Few-shot Node Classification.\" arXiv preprint arXiv:2306.06063 (2023).\n2. I highly recommend the authors to make the code public for readers to follow the work.\n3. I suggest exploring non-meta-learning scenario such as in GraphPrompt.\n* Liu, Zemin, et al. \"Graphprompt: Unifying pre-training and downstream tasks for graph neural networks.\" Proceedings of the ACM Web Conference 2023. 2023."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2877/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697461897797,
        "cdate": 1697461897797,
        "tmdate": 1699636231257,
        "mdate": 1699636231257,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HBmCCx0L56",
        "forum": "4IT2pgc9v6",
        "replyto": "4IT2pgc9v6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2877/Reviewer_A8Qx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2877/Reviewer_A8Qx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes One-for-All (OFA), a framework for building and training a single graph model that can perform various graph-related tasks across different domains. The experimental results on real-world datasets demonstrate the effectiveness and efficiency of the proposed framework."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tA unified graph model is designed for various tasks and different domains of graph data.\n\n2.\tThe introduced model can encode features of different graphs into the same embedding space.\n\n3.\tExtensive experiments are conducted on various settings and datasets from different domains."
            },
            "weaknesses": {
                "value": "1.\tThe design of prompt nodes and utilizing NOI-subgraph to unify different tasks seem to be similar to several previous works. For example,  [1] introduced a learnable prompt vector to unify tasks, and [2-3] proposed to utilize super nodes to perform pooling readout. The proposed NOI prompt node appears to be a combination of these two lines of works. Could you include more in-depth discussions and comparisons with these related works.\n\n2.\tComparison with methodologies that integrate LLMs and graph models, such as [4-7], is necessary. \n\n3.\tThe Few-shot and Zero-shot results, especially for TLP-SURGL and TLP-BGRL, do not outperform baseline models. Could you include further discussions and analysis of these underwhelming performance results? \n\n4. The ability of the LLMs to unify the feature space across datasets from diverse domains is very interesting. It would be insightful to see the unified feature representations of datasets within the same domain (e.g., two citation networks) and the datasets spanning different domains (e.g., a citation network and a molecular network). For example, some visual analysis will offer insights into the effectiveness of the LLM in harmonizing feature spaces across heterogeneous datasets.\n\n5. Additional ablation studies can be conducted to see whether the constructed graph structure is helpful. For example, directly using the unified node features (produced by LLMs) as the input feature of some backbone GNN models like GIN, GCN, GAT.\n\n6. The article introduces a method to employ LLMs to generate inputs for GNNs. Another line of works aims to utilize GNNs to generate inputs for LLMs, and it seems that this approach can better utilized LLMs' significant capabilities in addressing various problems on vast data. I'm curious about the difference between these two pipelines. Can you provide some explanations and compare the performance of these two pipelines.\n\n\n\n[1] Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. In WWW, 2023.\n\n[2] F. Hu, Y. Zhu, S. Wu, and T. Tan. Hierarchical graph convolutional networks for semi-supervised node classification. In IJCAI, 2019.\n\n[3] Matthias Fey, Jan-Gin Yuen and Frank Weichert. Hierarchical Inter-Message Passing for Learning on Molecular Graphs. In ICML, 2020.\n\n[4] Xiaoxin He, Xavier Bresson, Thomas Laurent, and Bryan Hooi. Explanations as features: Llm-based features for text-attributed graphs. arXiv preprint arXiv:2305.19523, 2023.\n\n[5] Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and Jian Tang. Learning on large-scale text-attributed graphs via variational inference. arXiv preprint arXiv:2210.14709, 2022.\n\n[6] Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. Natural language is all a graph needs. arXiv preprint arXiv:2308.07134, 2023.\n\n[7] Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, and Jian Tang. Graphtext: Graph reasoning in text space. arXiv preprint arXiv:2310.01089, 2023."
            },
            "questions": {
                "value": "see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2877/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2877/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2877/Reviewer_A8Qx"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2877/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698673622135,
        "cdate": 1698673622135,
        "tmdate": 1700729502981,
        "mdate": 1700729502981,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vWri1qqDk1",
        "forum": "4IT2pgc9v6",
        "replyto": "4IT2pgc9v6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2877/Reviewer_RxxK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2877/Reviewer_RxxK"
        ],
        "content": {
            "summary": {
                "value": "In the field of artificial intelligence, creating a single model to handle diverse tasks has been a longstanding goal. Large language models have excelled in language-related tasks, but applying this versatility to graph-based tasks is challenging. Graph data from different domains have unique attributes and distributions, making uniform representation difficult. Additionally, graph tasks encompass nodes, links, and graphs, necessitating distinct strategies. Furthermore, context-aware learning for graphs lacks a clear method.\n\nTo address these challenges, this paper introduced \"One for All\" (OFA), a framework that uses a single graph model to conquer these obstacles. OFA uses text-attributed graphs, unifying diverse graph data and standardizing task representation with \"nodes-of-interest.\" It pioneers a novel graph prompting approach for versatile task handling without fine-tuning. They train OFA using data from various domains and evaluate its performance in diverse learning scenarios. The results establish OFA as the first general-purpose graph classification model, revolutionizing the field of graph-based artificial intelligence."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The motivation of this paper is well-delivered and interesting\n\nThe proposed method is novel and seems to be good according to their experimental report"
            },
            "weaknesses": {
                "value": "W1. translating graph data with text feature sequences might lose some key information from graphs. \n\nW2. According to Figure 1, LLM enhances text for text-attributed graphs. it still needs a GNN as a predictor. (1) I wonder whether the GNN model is also pre-trained and frozen. and (2) what are the results compared with using LLM as a predictor instead of an enhancer?\n\nW3. It is unclear how to use NOI prompt nodes and class nodes with GNN to predict the downstream tasks. according to Figure 2, can I say that the downstream tasks (node, edge, graph classifications) are treated as predicting links between NOI prompt and class nodes? It seems that NOI prompt nodes can be treated as a special case of the prompt graph mentioned in the paper \"All in One\" (Sun et al., 2023), what's the differences between them? and why the authors use only one NOI prompt node instead of multiple NOI prompt nodes.\n\nW4. I wonder why the authors use sentence transformer instead of ChatGPT API, or LLAMA, etc since they claim that \"any kind of LLM can be used as the encoder, and a stronger LLM potentially yields better overall performance.\" Is it possible that a very large language model contains too much unrelated knowledge/intelligence that may reduce task performance?\n\nW5. I wonder what would happen if you Re-order the item in your prompt for ChatGPT.  It seems the reason why you use sentence transformer is that it is not sensitive to the order of your prompt? if you change to ChatGPT, different orders may generate entirely different sentences, making the downstream performance unpredictable"
            },
            "questions": {
                "value": "see W1-4\n\nI would like to see the rebuttal to the questions mentioned in the above section \u201cPaper Weakness\u201d. I\u2019m afraid that I might have not sufficient time to see a very long rebuttal. A concise and clear one would be good.\n\nThe potential weakness won't prevent me from raising my final score. I just want to make clear whether my understanding is correct."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2877/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816114838,
        "cdate": 1698816114838,
        "tmdate": 1699636231076,
        "mdate": 1699636231076,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lHmfE7v1Zt",
        "forum": "4IT2pgc9v6",
        "replyto": "4IT2pgc9v6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2877/Reviewer_LMu3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2877/Reviewer_LMu3"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a unified paradigm for learning different tasks over graphs and across different domains. Notably, the proposed algorithms achieve competitive performance in various tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The framework proposed by the author is very interesting. The idea of prompted nodes naturally unifies the three major tasks in one simple but powerful problem formulation. \n2. The presentation of the result is clear with both quantitative and qualitative results."
            },
            "weaknesses": {
                "value": "1. The soundness of the experiments seems lacking. Based on my understanding, LLM is one if the key component to achieve cross-task and cross-domain generalization. However, the choice of LLM is rarely discussed. I didn't find detailed description of the LLM used in the main experiments. Neither a comparison between different LLMs is missing in main paper and appendix. \n2. The modeling of link-level task lack details. For example, do you have two or one class node?"
            },
            "questions": {
                "value": "Please refer to the weakness for questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2877/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2877/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2877/Reviewer_LMu3"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2877/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699513922347,
        "cdate": 1699513922347,
        "tmdate": 1700641386427,
        "mdate": 1700641386427,
        "license": "CC BY 4.0",
        "version": 2
    }
]