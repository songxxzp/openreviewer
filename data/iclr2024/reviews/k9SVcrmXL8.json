[
    {
        "id": "SaCy9AIJ9k",
        "forum": "k9SVcrmXL8",
        "replyto": "k9SVcrmXL8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1421/Reviewer_wMvt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1421/Reviewer_wMvt"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a framework that comprises several part to better extract representations from images under the assumption that one single image in a batch corresponds to just one unique class. And also to address the issue of sample bias when few examples are playing crucial role even in cases where the sample is way higher. \n\\\nThe main contributions are: \\\n\na) the end-to-end framework terms BECLR that incorporates the modules that address sample bias and expands single image instance discrimination \\\n\nb) Very good results in a number of benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is detailed and relatively easy to follow. The underlying method is well explained and motivated and is substantiated by several experiments and ablations. \\\n\nThe results are promising and impressive and the method is well grounded."
            },
            "weaknesses": {
                "value": "The evaluation should have considered the various components in separation and with respect to other methods.\nE.g. is OPTA that improves performance or perhaps adding another base line method might have had a similar performance. \\\nI think in such multi-component frameworks this is an issue usually. \\"
            },
            "questions": {
                "value": "a) are the results presented throughout the paper are primarily based on the end-to-end process that involves all components, including OPTA? \\\nb) To be fair we need to see how other baselines work when replacing OPTA, e.g. other methods and/or linear evaluation principles for FSL. OPTA seems to have a considerable effect but is that down to the method itself or other baseline methods added to this framework could have a similar effect? \\\nc) Is there a reason why you did not consider running on the entire imagenet?\n\nTypos: \\\nthere are several typos throughout the paper - e.g. abstract \"Critical\" not \"Clinical\", \"downstream\" not \"downstrea\" and others."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1421/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1421/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1421/Reviewer_wMvt"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1421/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698659930733,
        "cdate": 1698659930733,
        "tmdate": 1700638182438,
        "mdate": 1700638182438,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "L2nXaBXuol",
        "forum": "k9SVcrmXL8",
        "replyto": "k9SVcrmXL8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1421/Reviewer_WH4q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1421/Reviewer_WH4q"
        ],
        "content": {
            "summary": {
                "value": "In the presented paper, the authors identify two primary limitations in the existing Unsupervised Few-Shot Learning (U-FSL) methods and introduce an integrated solution named BECLR. BECLR incorporates two novel components: a dynamic clustered memory module named DyCE, designed to improve positive sampling in contrastive learning, and an efficient distribution alignment strategy termed OpTA, devised to counteract sample bias in U-FSL. While BECLR is tailored for U-FSL, the authors highlight DyCE's potential broader applications in general self-supervised learning, noting its superior performance even without OpTA, and advocate for the inclusion of OpTA in all U-FSL methods, particularly in low-shot scenarios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Originality**: The paper showcases a distinct approach by addressing recognized limitations in U-FSL and introducing the BECLR solution, merging existing concepts innovatively and presenting fresh modules like DyCE and OpTA. \n\n**Quality**: The research is robust, with DyCE and OpTA being methodically developed and their effectiveness demonstrated through comparisons with established methods like SwaV, SimSiam, and NNCLR.\n\n**Clarity**: The authors articulate their findings and methodologies clearly, ensuring that readers can grasp the intricacies of BECLR, DyCE, and OpTA without ambiguity. \n\n**Significance**: With its potential to redefine self-supervised learning and its implications for U-FSL, especially in few-shot scenarios, the paper holds substantial importance in advancing the field and offers a direction for future research."
            },
            "weaknesses": {
                "value": "1. The overarching idea and structure of BECLR bear a striking resemblance to PsCo, particularly when observing Figure 2. It appears that the primary distinction is the concatenation of different views of 'X' from PsCo and the addition of a dynamic clustered memory. To differentiate their work more effectively, the authors should provide a comprehensive comparison with PsCo in both the introduction and related work sections, highlighting the unique aspects of their approach.\n\n2. The notation used in the algorithmic section needs elucidation. Clearly defining each symbol would make it more accessible and allow readers to follow the content with greater ease.\n\n3. The experimental section could benefit from an additional test: an evaluation of performance without merging 'X'. It would also be insightful to see results when PsCo is merged and when masking is applied, offering a more comprehensive understanding of the method's robustness and versatility."
            },
            "questions": {
                "value": "1. **Model Parameter Updates during Testing:** During the testing phase, are there any updates required for the model's parameters? If yes, how many times is the model updated?\n\n2. **Comparison with PsCo in Terms of Model Size and Inference Time:** How does BECLR compare to PsCo regarding the number of parameters in the model and the inference time? A direct comparison would provide clarity on the efficiency and scalability of BECLR.\n\n3. **Data Utilized for MiniImagenet Pretraining:** When pretraining with miniImagenet, which specific datasets were employed? Was the entire miniImagenet dataset utilized for this purpose?\n\nSuggestions:\n\n- **Enhanced Clarity on Parameter Update Mechanism:** A deeper dive into the model's updating mechanism during testing would be valuable. This would provide insights into the adaptability and robustness of the model, especially in real-world scenarios where data dynamics may vary.\n\n- **Detailed Comparative Analysis with PsCo:** Given the similarities noted between BECLR and PsCo, a side-by-side comparison in terms of model parameters and inference time would offer readers a clearer perspective on the advantages and potential trade-offs of adopting BECLR."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1421/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1421/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1421/Reviewer_WH4q"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1421/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675400692,
        "cdate": 1698675400692,
        "tmdate": 1700656973811,
        "mdate": 1700656973811,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kmYA8aFrnY",
        "forum": "k9SVcrmXL8",
        "replyto": "k9SVcrmXL8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1421/Reviewer_msQP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1421/Reviewer_msQP"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed an Unsupervised few-shot learning method (BECLR). The key idea is to extend the memory queue concept to dynamically updated memory clusters (DyCE). The second key idea is to address sample bias issue (distribution shift between the unlabeled Query set and the labeled support set) by introducing OpTA at inference time. BECLR is the name of the overall method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper tried to address several issues within a single framework. It benefits from contrastive pre-training, tries to address and distribution shift and address some of the issues around the memory queue concept. \nIt seems that empirical results are strong."
            },
            "weaknesses": {
                "value": "-SAMPTransfer (Shirekar et al 2023) is also based on membership but its performance is reported only on the miniImageNet-->CDFSL task (Table 3). It is missing from other experiments. \n-Prior FSL works that are related to the distribution shift (sample bias) issue are not discussed. \n-Table 1: For ResNet-50 and Wide ResNet backbones, some of the comparison methods are missing. Again, in Table 3 some of the comparison methods are missing. It seems that the subset of methods used in each experiment is an arbitrary subset of the available pool of the previous methods."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1421/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698714149180,
        "cdate": 1698714149180,
        "tmdate": 1699636070193,
        "mdate": 1699636070193,
        "license": "CC BY 4.0",
        "version": 2
    }
]