[
    {
        "id": "7sUFOcUfPk",
        "forum": "ByW9j60mvV",
        "replyto": "ByW9j60mvV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8172/Reviewer_6FQX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8172/Reviewer_6FQX"
        ],
        "content": {
            "summary": {
                "value": "This paper tries to describe RL algorithms in the context of BAMDP. It shows the regret of the algorithms in terms of BAMDP value and decomposes the value into the incremental value of information and value of opportunity. Finally, the regret is further analyzed using the concept of reward shaping."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper provides the theoretical analysis of the main topic."
            },
            "weaknesses": {
                "value": "The motivation and contribution of the paper is unclear. The presentation of the paper is unfocused, thereby hard to follow its main concept and insights. Also, the practicalness of the analysis is unclear because of the lack of practical examples or insights in terms of the usage of the analysis. (For example, how we can improve or design more practical RL algorithms using the insights.) Since multiple concepts such as BAMDP, RL algorithms, and reward shaping are used in the paper, it should be clear how each one is related and why it is important under a main message of the paper. But it doesn't, so it's confusing."
            },
            "questions": {
                "value": "1. The motivation and contribution of the paper is not clearly provided. It is unclear for what purpose RL algorithms are being described in the context of BAMDP. Is this because of specifying a proper way to use RL algorithms for BAMDP, or because of showing that RL algorithms are not suitable for BAMDP? \n2. In a similar context, it is not clear why the paper considers BAMDP. For meta-learning, considering BAMDP is reasonable since meta-learning tries to address multiple tasks in real-world applications together, which can be modeled as different task MDPs. Are the authors arguing that real-world problems that have traditionally been modeled with MDPs are actually more suited to be modeled with BAMDPs?\n3. What is \"hand-written information-state policies\" in the abstract standing for precisely? That is used only in the abstract and there is no clear description in the text.\n4. The rationale for applying reward shaping in analyzing the regret is unclear. Why it is important should be justified more clearly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8172/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8172/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8172/Reviewer_6FQX"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8172/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698635936869,
        "cdate": 1698635936869,
        "tmdate": 1700584697417,
        "mdate": 1700584697417,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pIUgp0vrRk",
        "forum": "ByW9j60mvV",
        "replyto": "ByW9j60mvV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8172/Reviewer_7dtM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8172/Reviewer_7dtM"
        ],
        "content": {
            "summary": {
                "value": "This work analyses reward shaping techniques from the perspective Bayes regret.\n\nFirst, they establish a notation for viewing RL algorithms as policies in the framework of Bayes-adaptive MDP (BA-MDP), a framework which constructs a history-MDP and proposes to maintain (and predict) posteriors over the unknowns of the MDP and of which a solution will optimally explore with respect to the prior given the to agent.\n\nThe Q-values of optimal solution to the BA-MDP are decomposed into \"incremental value of information\" plus \"value of opportunity\".\nThe first value, loosely, is the utility of the knowledge (improving MDP posterior accuracy) that comes from doing an action in a state at a particular time step (through better informed actions in the future).\nThe latter, again informally, is the actual (long term) utility expected under the current posterior that is gained from doing the action in the state.\n\nThen, the paper compares the Q values of such a optimal algorithm to a myopic (typical) RL algorithm which only considers the current information.\nConcretely, as an example, Q-learning approximates the Q-values given previous data, and not given potential future interaction based on uncertainty over the MDP.\nIt is shown that this corresponds to maximizing only for the \"value of opportunity\".\n\nFinally, the paper looks at various reward shaping approaches as compensating for the difference between the Bayes-optimal Q values and those of typical RL algorithms.\nThis analysis introduces the notion of shaping the Q signal or I (knowledge) signal.\nFor instance, adding minor rewards for getting closer to a goal state is reward shaping the Q signal, whereas positive reward for rare occurrences such as new observations means shaping the I-signal."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper proposes a very interesting concept, that of considering RL algorithms as policies in history-based MDPs.\nThe resulting derivation, one where myopic (typical) RL algorithms can be seen as optimizing only one of two terms of the optimal Q-value, has much potential for understanding and framing RL.\n\nThis is important, because the exploration-exploitation problem is a core issue unique to RL and one where progress should be of interest to the majority of RL research community.\nAnd progress, especially non-incrementally, often requires novel insights and perspectives which, I believe, this paper has managed to find.\n\nAs a result, I find this paper a good step in a promising direction and hope to see it fleshed out a little more."
            },
            "weaknesses": {
                "value": "Despite these strengths, my main concerns are its presentation and lack of formality (especially in the analysis) which affect the contribution significantly.\n\nSometimes this results in just vague statements, such as comments like \"... manually programmed RL algorithms.\" and \"... since regardless of how much meta-learning takes place, some algorithm must be written down eventually.\" in the introduction (and maybe even the usage of \"information-state policies\", which are never really defined or seemingly used technically, but pop-up occasionally?).\nOther times, however, this is detrimental to understanding the intent. \nFor instance, what is the \"true p(M) of the problem\" (before 6.2.1)?\nAlso, it is still unclear to me how a potential based shaping function is relevant to the paper:\nIt does not seem important, especially since the section talks about reward shaping in the BA-MDP:\nwhy would one want to shape the reward if a solution is already optimal with respect to exploration?\n\nMost importantly, I feel like the key contribution is missing / not obvious in its current version.\nThe paper contains of English text describing how various reward shaping techniques can be seen (categorized?) in different lights, but for those interested in exploration in RL there seems little novel.\nFor example, intrinsic reward is very well known (indeed motivated by the idea) to boost the Q-value based on the notion that \"knowledge\" has value.\n\nTo conclude, the definition of regret of typical RL algorithms compared in Q values with the Bayesian optimal solution seems like great step, but the actual analysis (in its current form) is lacking in formality and rigor that I believe is necessary for a paper at ICLR."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8172/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8172/Reviewer_7dtM",
                    "ICLR.cc/2024/Conference/Submission8172/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8172/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698698119935,
        "cdate": 1698698119935,
        "tmdate": 1700478363188,
        "mdate": 1700478363188,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PYLR9AhuJV",
        "forum": "ByW9j60mvV",
        "replyto": "ByW9j60mvV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8172/Reviewer_t2gv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8172/Reviewer_t2gv"
        ],
        "content": {
            "summary": {
                "value": "The paper studies reinforcement learning through the lens of Bayes-Adapative MDPs, in which the RL algorithm is viewed as a policy of the BAMDP. The paper first provides a formal description of the setting and its components. Then, it derives a decomposition of the BAMDP value into the sum of the Incremental Value of Information, which coarsely measure the information gain over the true MDP, and the Value of Opportunity, which measures the expected return given the current information. Finally, the decomposition is used to analyse a set of reward-shaping mechanisms that have been previously considered in RL literature, including intrinsic motivation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- (Originality) The paper provides a novel and interesting view of reinforcement learning algorithms as policies over BAMDPs.\n- (Categorization) The paper provides a valuable characterization of a handful of reward-shaping approach through the lens of BAMPD value."
            },
            "weaknesses": {
                "value": "- (Motivation) Whereas the formulation is interesting, BAMDP is also known to be an intractable problem in general, so it is unclear what benefit this new perspective can bring.\n- (Implications) The paper does not fully clarify how the introduced perspective should help analyzing existing RL algorithms and building more advanced algorithms in the future.\n\nThe paper is providing an original interpretation of RL as the problem of solving a BAMDP, and it does also report some insights, such as the value decomposition that explicitly separates exploration and exploitation contributions to the value. However, despite the promising formulation, the paper is somewhat falling short from providing a coherent set of implications resulting from this new perspective, beyond a few informal consideration over reward shaping methods. In my opinion, to clear the bar for acceptance this paper shall narrow its scope, e.g., presenting the contribution as a study of reward-shaping through BAMDP perspective, and provide more formal/deeper implications from the analysis, such as a study on how the different shaping methods impact the Bayesian regret and under which assumptions or prior any of those methods can be considered optimal. For this reason, I am currently providing a negative evaluation to the paper, but I encourage the authors to keep working on this problem, which looks like a nice research direction to pursue."
            },
            "questions": {
                "value": "1) What is the point of framing the RL problem as BAMPD, when it is well known that BAMDP cannot be solved efficiently in general? Especially, given that provably efficient RL algorithms exist (under somewhat restrictive assumptions), does this mean that the provided formulation is missing some of the structure of the underlying problem?\n\n2) While setting a prior over the tasks might be reasonable in meta-RL or analogous settings, in which some knowledge of the task distribution can be collected during training, is this also reasonable in standard RL? Of course any MDP can be seen as a sample from a very uninformative prior over all the possibile MDPs, but this does not seem to provide any benefit.\n\n3) Can the authors discuss how their framework will help producing a deeper analysis of existing RL algorithms and possibly guide the development of improved algorithms?\n\n\nMINOR\n- The use of \\citep and \\citet in the paper is sometimes confusing. I would suggest the authors to use \\citet only when the papers' authors name are part of the sentence;\n- Some choice of reference is somewhat odd, such as reporting Yang et al., (2021) for regret bounds in RL. There are a pletora of papers on that topic, and perhaps a more representative reference can be chosen;\n- The itemize at the end of the Introduction is missing a full stop after the last bullet point."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8172/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698936028590,
        "cdate": 1698936028590,
        "tmdate": 1699637013407,
        "mdate": 1699637013407,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hQKa27WSbl",
        "forum": "ByW9j60mvV",
        "replyto": "ByW9j60mvV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8172/Reviewer_h87r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8172/Reviewer_h87r"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new framework for understanding reinforcement learning (RL) algorithms as policies in Bayes-adaptive Markov decision processes (BAMDPs).\n\nA BAMDP is a species of \"meta\" sequential decision-making process that -- given a set of candidate tasks (represented as MDPs) within which the true task lives as well as a prior over those tasks -- formalizes the problem of balancing acquiring information about the task being solved with maximizing its lifelong expected return. Unlike policies for single tasks/MDPs that seek to myopically maximize rewards under a fixed policy without accounting for policy changes based on future information gain, a BAMDP policy can be viewed as a procedure for updating policies over time based on the task information they generate.\n\nIn light of the foregoing, the paper describes how RL algorithms can be viewed as BAMDP policies and investigates useful consequences of doing so. Specifically:\n* standard, \"myopic\" RL algorithms are formally characterized within the BAMDP framework;\n* the regret of such standard RL algorithms with respect to Bayes-optimal solutions to the BAMDP is characterized;\n* a decomposition of the regret into two familiar components corresponding to information gain (exploration) and value improvement (exploitation) is provided;\n* a characterization of potential-based reward shaping in the context of BAMDPs and its relationship to reward shaping of standard, myopic RL algorithms is given."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This paper provides a creative, insightful new formal framework for reasoning about RL algorithms as policies in BAMDPs. The work is theoretical in nature and will be of significant interest to the RL theory community. The connections it draws between reward shaping in the BAMDP realm and effects on standard, myopic RL algorithms may provide a catalyst for new developments in the experiment RL community as well. The paper is primarily a conceptual work synthesizing existing ideas (BAMDPs with RL algorithms) and providing new perspectives. The theoretical results are straightforward with no serious mathematical heavy lifting required, but they are clear and insightful. The paper is well-written and the recurring example pictured in Fig. 1 is used effectively to provide concrete illustration of main concepts throughout."
            },
            "weaknesses": {
                "value": "The primary weakness of this paper is the absence of a direct practical application of the ideas developed. This is natural, given the conceptual nature and theoretical focus of the work, but some experimental evidence supporting the main ideas would make the paper much more accessible to the experimental RL community. For example, it would be very helpful to have numerical experiments illustrating key aspects of the discussion of reward shaping provided in Sec. 6 on a non-trivial problem."
            },
            "questions": {
                "value": "* why is $R(r_t | s_t, a_t)$ included in the definition of $\\bar{T}$ in bullet point four of the BAMDP definition on page 3? can you clarify how $\\bar{T}( \\cdot | \\bar{s}_t, a_t)$ remains a pdf/pmf?\n* the analysis in the appendix seems to consist primarily of proving Theorem 3.1 and providing computations for the caterpillar example; are there any key innovations in the analysis that you consider worth highlighting?\n* do you have any ideas for how the discussion in Sec. 6 might be illustrated experimentally?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8172/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8172/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8172/Reviewer_h87r"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8172/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699061331681,
        "cdate": 1699061331681,
        "tmdate": 1699637013289,
        "mdate": 1699637013289,
        "license": "CC BY 4.0",
        "version": 2
    }
]