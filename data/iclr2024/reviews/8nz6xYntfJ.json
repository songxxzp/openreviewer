[
    {
        "id": "1szmyM8HBo",
        "forum": "8nz6xYntfJ",
        "replyto": "8nz6xYntfJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6243/Reviewer_cYiM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6243/Reviewer_cYiM"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to utilize pre-trained text-to-image diffusion models for few-shot segmentation. It points out three levels of misalignments that arise when utilizing pre-trained diffusion models in segmentation tasks: 1) text prompt may not generate desired instances; 2) may fail on multi-object scenes; 3) diffusion models cannot generate segmentation masks.\n\nTo solve 1), it binds an instance specific word embedding with the given real examples. To solve 2), it combines\nsynthesized instances with real images to generate training samples with more realistic layouts. To solve 3), it use semi-supervised learning (Wei et al., 2022) and condition the generative process on provided novel samples.\n\nThe experiments are done on Pascal-5, COCO-20, and FSS-1000. Compared to previous methods, the proposed method achieves the SoTA."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method seem robust as it achieves on-par performance even if it is combined with simple fune-tuning, while other methods suffer from significant decrease in performance."
            },
            "weaknesses": {
                "value": "1. As few-shot segmentation might refer to semi-supervised segmentation with very few annotated examples and lots of unannotated examples in some literature, different from the setting in this paper, I suggest to state the problem setting in the very beginning of the paper.\n2. If I recall correctly, using a special adjective token for a specific instance was proposed in [A], but it seems claimed as one of the contribution of this paper.\n3. This paper does not clearly explain the \"copy-paste\" process, but simply cites another paper. An example figure would be nice. The images in Figure 2 is too dark to see clearly. If the space is not enough, I suggest to remove the introduction of diffusion models as it is becoming a common sense in this area.\n4. The proposed method sounds very expensive. For every category, users need to personalize Stable Diffusion with text-inversion first, and then train the segmentor over and over again to contain the new generated masks into training set. However, the paper seems to only compare the mask generation speed. I would suggest to compare the whole process speed to benefit the community.\n5. The main performance gain seems to come from combining two papers: [B] and [C], which correspond to the contribution 2) and 3) in the introduction, respectively.\n\n[A] DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation\n\n[B] GAPS: Few-shot incremental semantic segmentation via guided copy-paste synthesis\n\n[C] An embarrassingly simple approach to semi-supervised few-shot learning."
            },
            "questions": {
                "value": "1. Related to weakness 3), does the \"copy-paste\" process requires post-process harmonious method? If not, how to make sure the generated image make sense to the layout? Does it harm the performance?\n2. Related to 1), with only a few examples of the new instance, how to obtain multi-object layout? Does the layout include the new instance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6243/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697682285164,
        "cdate": 1697682285164,
        "tmdate": 1699636682691,
        "mdate": 1699636682691,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jMNU9yO30V",
        "forum": "8nz6xYntfJ",
        "replyto": "8nz6xYntfJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6243/Reviewer_ZQEG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6243/Reviewer_ZQEG"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a data augmentation approach for few-shot image segmentation. The aim is to synthesise training samples of the novel object categories that are segmented from the few-shot samples. The approach is based on proposing three modifications to the text-to-image stable diffusion for image generation. First, the image sample generation is based on creating a bank of banks of embeddings with the proposed mask and normalising the loss related to the textual inversion (Gal et al 2022). Second, more training samples are generated in a copy-paste manner by including objects generated with stable diffusion in the available training data. Third, a few-shot segmentation (FSS) model is trained with a set of appropriate image-segmentation pairs.  The FSS model is used to find more adequate pairs and is trained again with a larger pool of samples. The approach shows reasonable performance on several standard benchmarks for few-shot segmentation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper is well written and easy to follow. In addition, the related work is complete and discussed in detail. The method is also well presented. \n\n+ The method provides solid results for almost all evaluations.\n\n+ The ideas proposed are easy to implement in any latent diffusion model. \n\n+ The paper addresses an open problem for segmentation-like tasks. It is challenging to generate scenes with pixel-level masks using diffusion models."
            },
            "weaknesses": {
                "value": "- (Major) The three main contributions of the paper are extensions of existing approaches. This is not a problem, but in all cases the new approach is minor. For example, the copy-paste idea is used out of the box. Similarly, the iterative training of the few-shot segmentation model does not contain any particular innovation. The paper has limited novelty.\n\n- (Major) The 1-shot results would be helpful as they are also common to the previous work.  In addition, a comparison with recent approaches would be important. For example, Xu, Qianxiong, et al. \"Self-Calibrated Cross Attention Network for Few-Shot Segmentation\". Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023. It would be useful to discuss when the paper does not achieve SOTA results and why. Overall, the results show that the proposed ideas do not show much improvement.\n\n- The term \"synthetic distribution\" is a bit confusing because it is usually associated with the generation of data from a simulator. This is not the case for the problem under consideration. Generated / realistic data distribution would be more appropriate. \n\n- The term \"out-of-distribution generation\" is also confusing. There is no discussion of what is in-distribution information in terms of prompts or generated images. The paper may refer to the additional image variations given a prompt as OOD. This is not clear. However, OOD here differs from the common use of it in uncertainty estimation."
            },
            "questions": {
                "value": "- It would be interesting to discuss whether the method is limited to latent diffusion models or generalisable to more diffusion approaches."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6243/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819620207,
        "cdate": 1698819620207,
        "tmdate": 1699636682553,
        "mdate": 1699636682553,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x5dWkdfcII",
        "forum": "8nz6xYntfJ",
        "replyto": "8nz6xYntfJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6243/Reviewer_bS35"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6243/Reviewer_bS35"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use the pretrained diffusion models to augment the training set for few-shot semantic segmentation. Specifically, this paper provides an algorithm that generates novel instances from the diffusion model conditioned on a few available training data with annotated semantic segmentation masks and class names. Experiments on standard benchmarks such as FSS-1000 demonstrate the proposed data augmentation method improves the overall performance of existing few-shot segmentation methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. It is technically sound and interesting to use pretrained image generative models for data augmentation. This paper pinpoints three major types of misalignment between the synthetic distribution and the target data distribution when a naive text-conditioned image generation method is applied and proposes a simple solution per aspect.\n2. Experiments demonstrate that the proposed data augmentation can significantly improve the existing few-shot segmentation methods, especially on novel categories. \n3. This paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. Even though the proposed data augmentation method improves the overall performance, the synthetic data can be harmful for many categories (see Fig. A.2). It is worth a more thorough analysis regarding this issue. For example, is that because the synthetic data misaligned with the target distribution, just like the text-conditioned image generation baseline? \n2. As stated in the limitation section, the proposed method degrades when the gap between the target distribution and the distribution covered by the generative model is large (e.g., medical images). However, I believe the few-shot semantic segmentation would be mostly useful for rare categories that are hard to collect enough instances for training a segmentation model. For common classes (e.g., chairs, sofa, boat) of which plenty of images can be found, it is hard to justify the necessity of using synthetic data. How is the performance of the proposed method on rare objects (e.g., the rare species from the iNaturalist dataset)?"
            },
            "questions": {
                "value": "See the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6243/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6243/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6243/Reviewer_bS35"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6243/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698904433536,
        "cdate": 1698904433536,
        "tmdate": 1699636682434,
        "mdate": 1699636682434,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MWjj2AcAKT",
        "forum": "8nz6xYntfJ",
        "replyto": "8nz6xYntfJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6243/Reviewer_ofKT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6243/Reviewer_ofKT"
        ],
        "content": {
            "summary": {
                "value": "- The paper aims to synthesize training images and masks of novel categories to augment few-shot learning.\n\n- They identify three issues with directly using text-conditioned stable diffusion model -- failure on OOD classes, object-centric bias of stable diffusion and coarse mask generation. These are referred to as instance, scene and annotation level misalignments.\n\n- Failure on OOD is addressed using normalized masked textual inversion. Object-centric bias is mitigated using copy-paste augmentation, and coarse masks are refined by updating the segmenter using semi-supervised learning.\n\n- Experiments in the GFSS (Pascal-5i, COCO-20i datasets) and FSS (FSS-100 dataset) settings show impressive results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-written and well-organized \n- The identification of issues arising in applying text-to-image models for data scarce few-shot semantic segmentation setting is valuable \n- Even though the solutions provided to each of the issues are not novel, they are simple and well-proven methods \n- Extensive ablation study is provided in the appendix (both qualitative and quantitative)"
            },
            "weaknesses": {
                "value": "- Limited comparison on the FSS-1000 dataset\n- Evidence of the finding that treating the learnable embedding as an adjective leads to a faster and more stable training convergence, is missing \n- A clear discussion of time taken by each step (textual inversion, semi-supervised mask generation), and comparison of total time with Grounded Diffusion (which is the only alternate baseline) \n- The evaluation setup describes that DiffuMask without their prompt engineering is used, but it it missing from Table1 and Table2"
            },
            "questions": {
                "value": "- Do the methods compared with in Table1 and Table2 also use base classes, or is it extra information used in this approach? (for copy-paste and semi-supervised learning parts) \n- An ablation on the amount of generated samples would be valuable in practice \n- See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6243/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699133378893,
        "cdate": 1699133378893,
        "tmdate": 1699636682334,
        "mdate": 1699636682334,
        "license": "CC BY 4.0",
        "version": 2
    }
]