[
    {
        "id": "wMaJHQDBls",
        "forum": "qhAx0fU9YE",
        "replyto": "qhAx0fU9YE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6947/Reviewer_Fi12"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6947/Reviewer_Fi12"
        ],
        "content": {
            "summary": {
                "value": "The authors empirically investigate whether biases contained in a pre-trained DNN is transferred to a fine-tuned DNN, in different experimental settings. They confirmed that such biases are actually transferred in (1) synthetic settings using backdoor attacks, (2) synthetic settings with naturally introduced biases of class information (even with de-biased target datasets in fixed-feature setting), and (3) standard transfer learning scenarios on ImageNet."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- Their motivation is clear and writing is easy to follow.\n- Their experimental scenarios are well-designed. In particular, the phenomenon of transferrability of backdoor attacks is new to me and seems intriguing, but less confident on its novelty since I'm not an expert of ML security.\n- Their experiments are thoroughly conducted on vision datasets, and the results are convincing."
            },
            "weaknesses": {
                "value": "- The novelty and contribution of their findings is limited. Previous works [1][2] already investigated such aspects of transfer learning, and some findings in this submission (particularly the bias transfer phenomenon in the scenario (2) and (3)) can be implied from their results.\n- The definition of \"bias\" in this submission is unclear. It should be specified to discuss \"bias\" transfer in a possibly rigorous way. Also, I'm less confident whether backdoor attacks should be considerred as \"bias\", but the research direction of transferrability of such attacks itself should be new and encouraged.\n- Discussions on previous works is not enough. The most related works [1][2] are not cited and not discussed. In relation to transferrability of backdoor attacks, I think [3] is one of very related works, but is not discussed. I recommend the authors to survey their previous works and make clear the novelty and contribution of this paper.\n\n[1] B. Neyshabur et al.,  \"What is being transferred in transfer learning?\" (NeurIPS'20)\n\n[2] E. Lubana et al., \"Mechanistic Mode Connectivity\" (ICML'23)\n\n[3] A. Shafahi et al., \"Adversarially robust transfer learning\" (ICLR'20)"
            },
            "questions": {
                "value": "1. What is the definition of biases in this paper? It should be specified first of all to discuss \"bias\" transfer."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6947/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6947/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6947/Reviewer_Fi12"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6947/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698635417625,
        "cdate": 1698635417625,
        "tmdate": 1699636810642,
        "mdate": 1699636810642,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YqEv9BXqYl",
        "forum": "qhAx0fU9YE",
        "replyto": "qhAx0fU9YE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6947/Reviewer_HsVn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6947/Reviewer_HsVn"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the transfer of biases as a result of transfer learning from the source dataset to the transferred models. For both natural and synthetically generated biases, it is shown with experiments that biases pre-existing in the pretraining data get transferred to the downstream tasks, even when the downstream dataset is balanced. The extent of the biases is lesser when finetuning is allowed into the entire network as compared to the case where only retraining the final layer is allowed."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The study is important, as using pretrained models to finetune on a downstream task is highly beneficial and a popular norm in the current times, hence understanding how the biases in the pretraining datasets creep into the downstream task is necessary to get unbiased predictions.\n2. The paper explores multiple settings. They show what happens when the pretraining dataset is biased, where the biases can be both synthetic and natural.\n3. The fact that biases are transferred even when the target dataset is debiased is very interesting.\n4. Three simple methods have been discussed to reduce the effect of the biases - full network transfer learning, reducing weight decay, mitigate biases in the target dataset."
            },
            "weaknesses": {
                "value": "1. Novelty is a concern for this paper: all the observations in the paper are expected and not surprising. For example, isnt it obvious that the full network transfer learning setting will be less affected by the source biases than the fixed one?\n2. I agree that identification of the problem is certainly important, and this paper does that - the authors demonstrate effectively how dangerous the pretraining data can be in terms of fairness. However, some mitigation strategies or atleast thoughts are expected. One of the solutions proposed is to use full network transfer learning. But if enough resources are not there for a model-user to finetune the entire network, the user has to rely on the fixed feature transfer learning - or settle for something in the model. How to solve the problem in that case?\n3. Wang et al [1] suggest manipulating the finetuning data to reduce the biases. No suggestion is proposed by the authors.\n4. For the synthetic bias case, what is termed as backdoor attack is simply adding a spurious correlation synthetcially to the dataset to increase/induce bias into it.\n\n[1] Wang et al. 'Overwriting Pretrained Bias with Finetuning Data', ICCV 2023."
            },
            "questions": {
                "value": "We use pretrained models for a multitude of tasks. \n1. What if the pretraining and finetuning data are not entirely similar, and the latter has its own biases? Any suggestions or experiments for such a situation?\n2. What happens when the latter is balanced?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6947/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6947/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6947/Reviewer_HsVn"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6947/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698757422293,
        "cdate": 1698757422293,
        "tmdate": 1699636810491,
        "mdate": 1699636810491,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ierrijYUYX",
        "forum": "qhAx0fU9YE",
        "replyto": "qhAx0fU9YE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6947/Reviewer_RsQk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6947/Reviewer_RsQk"
        ],
        "content": {
            "summary": {
                "value": "The authors demonstrate how dataset-induced biases persist after fine-tuning a model, even if the target set does not contain those biases.\nFor this purpose, the authors designed experiments to introduce or amplify a specific bias and to gauge its presence on the target domain.\nThe authors explore three mitigation strategies of this bias, including full-network fine-tuning, weight decay, and de-biasing the target domain.\n\n======\nUpdate after rebuttal: I appreciate the additional analysis the authors provided to explain the role of weight decay in mitigating the bias. In its current form the explanation only applies to simple linear regression, and does not extend to a non-linear deep neural network.\nOverall, I feel the authors made several points in their analysis which leave the reader with more questions than answers and wishing for more in-depth analysis.\nHowever, given the importance of those points, I am raising my overall score."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Studying bias transfer is important due to the heavy reliance on foundational models.\n- The results are insightful and their implications are nontrivial."
            },
            "weaknesses": {
                "value": "- The work is rather incremental to recent work in the literature, especially the work by Wang and Russakovsky [1]. I missed a reference to that work. The novelty would be more obvious e.g. had the authors demonstrated their results beyond the vision modality. See this recent survey for an overview of closely-related pieces of work, where a proper comparison would help highlight the novelty of the presented work https://arxiv.org/abs/2310.17626\n- The mitigations explored seem preliminary or non-straightforward to replicate:\n  - Full-network fine-tuning obviously has a better chance of reducing the bias in the pre-trained backbone, compared with a frozen backbone (where the bias mainly exists) + a linear head. \n  - The experiments about weight decay do not explain why it is helpful. Is it generally the case that regularization helps mitigate the bias? Is there something specific to weight decay that helps reduce the bias? What about other regularization strategies?\n  - Modifying the target dataset to counter the bias seems helpful but it is not obvious how it can be done in the general case (e.g. beyond balancing the sample in different subgroups or reintroducing the backdoor attacks in the target dataset at random).\n\n[1] Wang and Russakovsky: Overwriting Pretrained Bias with Finetuning Data (CVPR '23)\n\n\n\n\nA few typos:\ndatapoints => data points\ncan substantially reduces \nadjusting [..] entirely eliminate => eliminates\nwith of people => with people"
            },
            "questions": {
                "value": "- Would adversarial pre-training offer a good mitigation strategy as well?\n\n- The authors mention that they \"find that weight decay does not reduce bias transfer in the fixed feature transfer learning\nregime, where the weights of the pretrained model are frozen.\". How is weight decay applied to frozen weights?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6947/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6947/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6947/Reviewer_RsQk"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6947/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788705811,
        "cdate": 1698788705811,
        "tmdate": 1700675675772,
        "mdate": 1700675675772,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AfS5ZJwPoa",
        "forum": "qhAx0fU9YE",
        "replyto": "qhAx0fU9YE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6947/Reviewer_2a9y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6947/Reviewer_2a9y"
        ],
        "content": {
            "summary": {
                "value": "The paper shows empirically that bias in the source distribution can transfer to downstream tasks. The work conducts experiments for backdoor attacks, synthetically controlled biases, and naturally occurring biases. The paper analyzes the effect of various experimental parameters such as weight decay and full network fine-tuning versus frozen features."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The motivation and contributions are clear. Understanding how source datasets affect downstream performance, especially in the context of biases and backdoor attacks is highly relevant given how often pretrained models are used. \n\n- The experiments are extremely thorough, looking at various experimental parameters such as full network fine-tuning versus frozen encoder and the effect of weight decay. Various types of biases are analyzed such as backdoor attacks, natural biases, and synthetically induced biases. The experiments are performed with ImageNet as the source which is a reasonable scale and a common pretraining dataset. \n\n- The figures are illustrative and convey the main takeaways of the experiments.\n\n- The theoretical toy problem is interesting and gives potential intuition for why bias may persist through fine-tuning. It would be nice to see experiments looking at whether over-parametrization affects the amount of bias transfer."
            },
            "weaknesses": {
                "value": "- It would be useful to know how sensitive these conclusions are to fine-tuning hyper-parameters such as learning rate, momentum, and epochs."
            },
            "questions": {
                "value": "- Did you do experiments looking at the initial learning rate for fine-tuning and how that affects the amount of bias transfer? I would expect higher learning rates would lead to lower bias transfer. \n\n- Do you think these conclusions would hold for other pretrained models like SimCLR and CLIP?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6947/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829417211,
        "cdate": 1698829417211,
        "tmdate": 1699636810228,
        "mdate": 1699636810228,
        "license": "CC BY 4.0",
        "version": 2
    }
]