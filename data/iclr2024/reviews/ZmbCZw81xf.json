[
    {
        "id": "DEGkLsWzxN",
        "forum": "ZmbCZw81xf",
        "replyto": "ZmbCZw81xf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9124/Reviewer_qs93"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9124/Reviewer_qs93"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method for constructing unigram embeddings based on the syntax role of the tokens. They argue the resulting representations to be more interpretable than the representations produced by word2vec or GloVe."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "In the results, they appear to account for statistical significance in their evaluation, although they don't specify exactly how. The approach is an interesting one from a standpoint of forming hierarchical word embeddings, comparable potentially to approaches like Poincare embeddings, which I encourage the authors to look into. https://arxiv.org/pdf/1705.08039.pdf\n\nThey do appear to have results that are competitive with methods like word2vec on classic word similarity metrics and other benchmarks commonly used to test unigram embeddings. \n\nIn an era where we often see papers failing to cite a single paper from before 2021, it is actually charming and refreshing to read a paper that doesn't cite anything from after 2018."
            },
            "weaknesses": {
                "value": "Ultimately, the largest issue with this paper is that it does not address contemporary interests in NLP. It is about a contextless unigram embedding system tested with benchmarks that haven't been used for years. \n\nEven in the era that these citations are from, work like https://aclanthology.org/W16-2506.pdf was questioning the use of the benchmarks used. There was an entire ACL workshop dedicated just to evaluating these types of unigram word embeddings (RepEval). To step backwards into these benchmarks is to disown the work on evaluation and benchmarking done since then.\n\nThere is some missing detail about implementation. For example when they mention a normalization process but don't explain how it works. They also don't explain how they determine statistical significance in table 2.\n\nThere are two main weaknesses of this work:\n1. They failed to justify why anyone should be using contextless unigram embeddings when contextual embeddings work so much better for all applications people are interested in right now.\n2. Relatedly, they failed to account for polysemy. This problem is unrecoverable, as far as I can tell, because many words in practice can take on different parts of speech depending on context. For example, \"read\" could be a noun or verb depending on the context. This is a fundamental flaw in any kind of syntactic encoding system that does not account for context.\n\nI'm also somewhat skeptical of the interpretability results, as the number of classes is so small. There are far more parts of speech than those provided here, which exposes how limited this approach is, as they don't even have things like prepositions or determiners."
            },
            "questions": {
                "value": "How does the interpretability of these embeddings compare to post-hoc approaches to extracting syntactic information, like probing?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9124/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698440478401,
        "cdate": 1698440478401,
        "tmdate": 1699637147879,
        "mdate": 1699637147879,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fEs28UVIKI",
        "forum": "ZmbCZw81xf",
        "replyto": "ZmbCZw81xf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9124/Reviewer_CNTm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9124/Reviewer_CNTm"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new postprocessing method for embedding learning that transforms word vectors into syntactic Representations where each coordinate corresponds to one of the eight parts of speech. The resulting representations are interpretable with each new coordinate having a distinct meaning with respect to the newly defined basis. The authors further introduce hierarchical word vectors derived from these syntactic representations. Experiments on a wide variety of tasks generally show improvements."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors clearly described the background and motivations needed to understand the proposed postprocessing technique.\n2. The enduring challenge of interpretability in distributed representation, where meaning is entangled across all coordinates, is addressed in this paper. The authors introduced a new mechanism to convert word vector embeddings into interpretable representations by defining a new basis that is spanned by the eight parts of speech vectors.\n3. The authors performed both intrinsic and extrinsic tasks to show that the transformed word embeddings keep their meaning and improve performance on downstream tasks."
            },
            "weaknesses": {
                "value": "1. No uncertainty/confidence/error bars on experimental results, or significance testing.\n2. The experimental results were compared against a simple baseline thus the original embedding. It never showed how it compared against existing baselines."
            },
            "questions": {
                "value": "1. How sensitive is your proposed method to the size of the word list used to compute the eight parts of speech directions? Providing a similar plot shown in Appendix I of https://openreview.net/pdf?id=TkQ1sxd9P4 should be enough. You could check its sensitivity on an intrinsic or extrinsic task.\n2. Glove and Word2vec have been shown to have some inherent structural profile with most of the words being clustered along the long principal component (https://arxiv.org/pdf/1702.01417.pdf). After applying the proposed postprocessing technique could you measure how the structure of the space changes by providing a before and after number of the largest singular value?  \n3. One experiment to further show how useful the transformed space of your method encodes semantic and syntactic information would be to perform a cross-lingual alignment task between two monolingual embedding spaces. A simple way would be to measure the before and after condition number and singular value gap between the two spaces and report it. Check this paper https://aclanthology.org/2020.emnlp-main.186.pdf on condition number and singular value gap between two language spaces.\n4. Does your proposed method enforce an orthogonality between the new basis vectors?\n5. Could you include a visualization plot of the before and after postprocessing of the word embeddings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9124/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9124/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9124/Reviewer_CNTm"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9124/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719197035,
        "cdate": 1698719197035,
        "tmdate": 1699637147772,
        "mdate": 1699637147772,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gS29QdyalV",
        "forum": "ZmbCZw81xf",
        "replyto": "ZmbCZw81xf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9124/Reviewer_AnLo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9124/Reviewer_AnLo"
        ],
        "content": {
            "summary": {
                "value": "The word2vec and Glove word embeddings are post-processed in a way that words with identical POS tags will occur in the same subspaces of a vector space. These vectors are tested in a variety of NLP tasks, from similarity to sentiment analysis to question answering and produce good results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Attempting to understand word embeddings by imposing linguistic structure on them. Testing the results in a large range of tasks. Obtaining better results."
            },
            "weaknesses": {
                "value": "There is a last part to the paper where interpretability is discussed and measured. I did not understand this part and their measure of interpretability. In particular, how do you do the following?\n\n\"For assessing the interpretability of our model, we select words from WordNet and subject them to\nevaluation using the Interpretable Hierarchical Syntactic Representations.\""
            },
            "questions": {
                "value": "Can you please explain why the improved vectors do better in the tasks? What is the intuition behind it? Why should  a noun similarity taks be improved if the noun vectors are grouped together in one part of the space?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9124/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9124/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9124/Reviewer_AnLo"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9124/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698920089749,
        "cdate": 1698920089749,
        "tmdate": 1699637147652,
        "mdate": 1699637147652,
        "license": "CC BY 4.0",
        "version": 2
    }
]