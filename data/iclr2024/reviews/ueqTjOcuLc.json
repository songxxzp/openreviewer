[
    {
        "id": "2torAHFWST",
        "forum": "ueqTjOcuLc",
        "replyto": "ueqTjOcuLc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2639/Reviewer_9zJU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2639/Reviewer_9zJU"
        ],
        "content": {
            "summary": {
                "value": "This study explores various configurations of multi-agent collaboration in problem-solving across MMLU high-school multiple-choice, MATH, and Big-bench chess move validity task. The authors manipulate agent traits (overconfidence vs. easygoing), thinking pattern (debate vs. reflection), and collaborative strategies (permutations of agents' thinking patterns across multiple rounds). \n\nThe primary focus of the experiment is on a three agents, with the composition of four different types of societies based on distinct agent traits. For example, Society 1 consists of three overconfident agents, while Society 4 comprises three easygoing agents. There are three rounds and the total configuration is expanded to eight possibilities by permuting {debate and reflection}.\n\nThese experiments with diverse configurations are conducted using ChatGPT, yielding results that exhibit significant variability."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The exploration of collaboration dynamics involving multiple LLMs is interesting."
            },
            "weaknesses": {
                "value": "- The experiment results exhibit significant variance, making it challenging to derive meaningful insights and conclusions. The W-T metric, which the authors use as a complementary measure, also fails to reveal a consistent pattern.\n- The fact that all experiments were conducted on a single model, ChatGPT, further hurts the generalizability of the findings. Moreover, as ChatGPT is a closed proprietary model that silently gets frequent updates, replicating the results will be considerably challenging.\n- The writing style appears to prioritize flashy rhetoric over establishing clear connections to social psychology theories or frameworks. For instance, drawing parallels between the tendency of LLMs to conform to the majority and the concept of \"conformity\" in social psychology can be misleading. LLMs are well-known to show sycophant behaviors [1]. Furthermore, the selection of experimental design (e.g., trait types, thinking pattern) is not very well-grounded in social psychology. I suggest the authors to lower the tone and drop the emphasis on social psychology.\n\n[1] Perez et al., 2022: Discovering Language Model Behaviors with Model-Written Evaluations."
            },
            "questions": {
                "value": "- Given the considerable variance observed in all the experiments, what do the authors consider to be the primary takeaway or key message?\n- Regarding the results of the W-T metric, is there any discernible pattern or meaningful conclusion that can be drawn from them?\n- Have experiments been conducted involving two agents? I'm assuming that strategies involving p0p1 or p0p0 might potentially yield the best results.\n- Were any statistical tests performed on the results of the experiments?\n- Is there a reason for not testing the experiments with other models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2639/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2639/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2639/Reviewer_9zJU"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2639/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698736685896,
        "cdate": 1698736685896,
        "tmdate": 1700711452607,
        "mdate": 1700711452607,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hdcSfphPDA",
        "forum": "ueqTjOcuLc",
        "replyto": "ueqTjOcuLc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2639/Reviewer_wLih"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2639/Reviewer_wLih"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the potential for Large Language Models (LLMs) to exhibit human-like collaborative mechanisms in a multi-agent system. By creating four unique societies of LLM agents, each possessing distinct traits (being easy-going or overconfident) and thinking patterns (either debate or reflection), the study evaluates their collaborative mechanisms on three benchmark datasets. Results indicate that these LLM agents demonstrate a range of social behaviors, including active debating and introspective reflection. The paper promotes the use of social psychology insights to understand the collaboration of LLM agents better and provides a framework for evaluating multi-agent collaboration, emphasizing the potential of collaboration over mere scale in LLM performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The section provides a structured breakdown of the conceptual framework, detailing agent traits, thinking patterns, and collaborative strategies. It lays out the foundation for the study and justifies the relevance of the adopted strategies.\n\nThe inclusion of different datasets (High School Multiple-Choice, Math, and Chess Move Validity) to test the collaboration mechanisms is commendable, ensuring a broad evaluation spectrum.\n\nThe decision to frame the study using social psychological concepts is innovative. The puzzle-shaped agent representation is also a novel approach that aids in breaking down complex concepts into narrative visuals."
            },
            "weaknesses": {
                "value": "Assumption on Traits Influence: \nThe assertion that collaborative strategies overshadow the influence of agent composition may be premature. More extensive experiments or deeper analysis would strengthen this claim.\n\nUnclear Real-world Application: \nWhile the section details the mechanisms of collaboration and results in a simulated environment, its direct implications or applications in real-world scenarios are not clearly addressed."
            },
            "questions": {
                "value": "How generalizable are the findings beyond the datasets used?\nCould there be other potential agent traits or thinking patterns that were not considered in this study?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2639/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796921170,
        "cdate": 1698796921170,
        "tmdate": 1699636204106,
        "mdate": 1699636204106,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UMF68Qy5Df",
        "forum": "ueqTjOcuLc",
        "replyto": "ueqTjOcuLc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2639/Reviewer_Qj8g"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2639/Reviewer_Qj8g"
        ],
        "content": {
            "summary": {
                "value": "The paper looks into collaboration between language models in a societal setup, containing n agents with 2 traits (easy-going, overconfident) and 2 thinking patterns (debate, reflection). They compare collaborations between LLMs with human collaboration behavior backed by theories from Social Psychology. The collaboration behaviour is studied for three different tasks and they demonstrate interesting parallels with dynamics of human society. They also argue that scaling up is not always the key, specifically in the context of collaboration."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper explores a less explored area of collaboration between language models, giving a glimpse into how machines can potentially work in a collaborative set up and to what extend this parallels human society. \n- The description of the experimental setup and execution is clearly articulated. The methods are intuitive and supported by clear depictions and problem formalization making it easy to follow. \n- The experiments explore the desired research questions in a systematic manner and they observations are explained by drawing from theories in Social Psychology"
            },
            "weaknesses": {
                "value": "- The societal setup is oversimplified in terms of the number of traits and the size of the society. As a preliminary study, it is a good start. However, this is not clearly acknowledged in the paper. \n-  The study involves two identical language models interacting with each other, essentially sharing a common knowledge base. This setup differs from a typical human societal arrangement, and the impact of this factor  is not explicitly addressed. For instance, it is unclear what would be the impact of using different language models for the agents.\n- They argue that scaling is not the key and supports their claim with intuitive explanations. However, the scaling is limited to 3-4 agents and 2-4 rounds, which makes the observations seem a bit far fetched,"
            },
            "questions": {
                "value": "- How would a potential non-simplified collaboration setup look like and how this would influence the observations made in the paper ?\n- What is your motivation to chose agents backed by same language model rather than different model? How do you think using different models would influence the current experimental setup ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2639/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2639/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2639/Reviewer_Qj8g"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2639/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831072942,
        "cdate": 1698831072942,
        "tmdate": 1699636203924,
        "mdate": 1699636203924,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5oWlNyvthw",
        "forum": "ueqTjOcuLc",
        "replyto": "ueqTjOcuLc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2639/Reviewer_pe1t"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2639/Reviewer_pe1t"
        ],
        "content": {
            "summary": {
                "value": "The paper is based on study of societies of agents using LLMs that highlight the potential of collaboration mechanisms. The findings from the authors show influence of collaborative capabilities of LLM agents, with different agent traits, thinking patterns and collaborative strategies. The authors draw a parallel between the emergence of human-like behaviors in these agents with social psychology theories emphasizing their potential. The authors posit that collaboration mechanisms of machine society with multiple agents warrants deeper exploration. Looking at understanding how different LLM architectures influence these behaviors is also left as future study topic noting that integrating insights from social psychology could also guide the development of more socially aware NLP systems.\n\nThe paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. The study conducted utilizes four \u2018societies\u2019 comprised of LLM agents, where each agent is characterized by a specific \u2018trait\u2019 (easy-going or overconfident) and engages in collaboration with a distinct \u2018thinking pattern\u2019 (debate or reflection). The authors presents results and conclusions from evaluating these multi-agent societies on three benchmark datasets positing that LLM agents navigate tasks by leveraging diverse social behaviors, from active debates to introspective reflections. In addition, as per authors, certain collaborative strategies only optimize efficiency (using fewer API tokens), but also outshine previous top-tier approaches. Moreover, the authors results further illustrate that LLM agents manifest human-like social behaviors, such as conformity or majority rule, mirroring foundational Social Psychology theories. The authors also committed to sharing code and datasets to catalyze further research in this promising avenue.\n\nThere are key details missing (please see questions for specific details). In addition, the conclusion from the study lay on shaky grounds, subjective to interpretation from the figures (covered in detail in questions). I will encourage significant revision of this study."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is based on study of societies of agents using LLMs that highlight the potential of collaboration mechanisms. The findings from the authors show influence of collaborative capabilities of LLM agents, with different agent traits, thinking patterns and collaborative strategies. The authors draw a parallel between the emergence of human-like behaviors in these agents with social psychology theories emphasizing their potential. The authors posit that collaboration mechanisms of machine society with multiple agents warrants deeper exploration. Looking at understanding how different LLM architectures influence these behaviors is also left as future study topic noting that integrating insights from social psychology could also guide the development of more socially aware NLP systems.\n\nThe paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. The study conducted utilizes four \u2018societies\u2019 comprised of LLM agents, where each agent is characterized by a specific \u2018trait\u2019 (easy-going or overconfident) and engages in collaboration with a distinct \u2018thinking pattern\u2019 (debate or reflection). The authors presents results and conclusions from evaluating these multi-agent societies on three benchmark datasets positing that LLM agents navigate tasks by leveraging diverse social behaviors, from active debates to introspective reflections. In addition, as per authors, certain collaborative strategies only optimize efficiency (using fewer API tokens), but also outshine previous top-tier approaches. Moreover, the authors results further illustrate that LLM agents manifest human-like social behaviors, such as conformity or majority rule, mirroring foundational Social Psychology theories. The authors also committed to sharing code and datasets to catalyze further research in this promising avenue."
            },
            "weaknesses": {
                "value": "There are key details missing (please see questions for specific details). In addition, the conclusion from the study lay on shaky grounds, subjective to interpretation from the figures (covered in detail in questions). I will encourage significant revision of this study.\n\n1. If the agent composition of society doesn\u2019t have a marked difference then what is the utility of having such composition?\n\n2. on mmlu S1 is doing better on min for e.g. but not on the avg. how can we be confident that the differences are attribute to not by chance and are truly significant\n\n3. starting with P0 helps but for best perf. you need to add at least 1 P1 at end. Even with same two P0 and one P1. Why might that be the case\n\n4. How are the agents initialized and conditioned? is the prompt indicating how agents should behave enough? what specific LLM settings were used and how does it personifies how agents are implemented for this study (are overconfident agents default to 0 shot prompting and easy going multi shot?)\n\n5. what were the tasks details for subject of this study? if we just have one agent (either of two types) how many prompts (in case of reflection) it takes for it to get to the answer? are the tasks based on knowledge (like valid chess move pieces) that may be available to the agents as part of their training data? if yes, why will it change on reflection? and what is source of this change (is there a query in background that fill in the context)?\n\n6. It seems there are differences between all three tasks and not just Math vs MMLU and Chess move validity\n\n7. behavior in section 3.2 is contradictory. It will be good to have comprehensive analysis, how many times collaboration helped arrive at correct answer across all tasks? the qualitative explanation can go only so far. Also, a key point needing details is around the factual information (where the information may be a fact that can be recalled/learn from training data) vs the problem where answer may not be factual but available as a set of rules with multiple possibilities. \n\n8. section 4.1 \"we utilize the majority vote (Li et al., 2022; Cobbe et al., 2021) method to determine the answer for each round.\u201d: how much of this is if you do multishot prompting on an LLM and use ensemble of LLMs? Is there a society aspect to this? or is it just that the ensemble of LLMs gives better answer\n\n9. section 4.1 \"Wavering Answers resemble model hallucination due to the occurrence of self-contradictory answers.\": how can we be sure this is from hallucination? And not model changing it\u2019s output to what user prefers from set of possibilities\n\n10. \"We group samples from different societies under the same strategy because the effect of society is minimal\": what is the scientific evidence to prove that this is indeed the case? \n\n11. collaborative strategies play a significant role in performance: I dont see a significance test to lay this claim\n\n12 4.1 conclusion 2 For continuous reflection strate- gies, the proportion of \u201cWavering Answers\u201d occurrences is the highest among all strategies as seen: Doesnt seem to be the case from figure 4 d-f\n\n13. 4.1 conclusion 2 the strategy of \u201cPure Debate\u201d (i.e., p0 p0 p0 ) can effectively re- duce this fluctuation (hallucination): Doesnt seem to be exclusive to pure debate the case from figure4 d-f\n\n14. 4.1 conclusion 2: if that is true why is hallucination lower for P0P1P1 or P0P1P0\n\n15. section 4.2 number of agents: doesnt seem to be always case on both accounts\n\n16. section 4.2 more rounds: Doesn\u2019t seem to be always the case. The pattern seems inconsistent as is defintion of good or bad. Also, the improvement needs statisitcal significance.\n\n17. Section 4.2 Other Collaborative Strategies: Doesn\u2019t seem to be always the case. All except for 1 case the performance consistently drops. Also, what is significant drop needs defintion (at least for one case where they are pretty close). And same with what is good or bad performance to begin with."
            },
            "questions": {
                "value": "1. If the agent composition of society doesn\u2019t have a marked difference then what is the utility of having such composition?\n\n2. on mmlu S1 is doing better on min for e.g. but not on the avg. how can we be confident that the differences are attribute to not by chance and are truly significant\n\n3. starting with P0 helps but for best perf. you need to add at least 1 P1 at end. Even with same two P0 and one P1. Why might that be the case\n\n4. How are the agents initialized and conditioned? is the prompt indicating how agents should behave enough? what specific LLM settings were used and how does it personifies how agents are implemented for this study (are overconfident agents default to 0 shot prompting and easy going multi shot?)\n\n5. what were the tasks details for subject of this study? if we just have one agent (either of two types) how many prompts (in case of reflection) it takes for it to get to the answer? are the tasks based on knowledge (like valid chess move pieces) that may be available to the agents as part of their training data? if yes, why will it change on reflection? and what is source of this change (is there a query in background that fill in the context)?\n\n6. It seems there are differences between all three tasks and not just Math vs MMLU and Chess move validity\n\n7. behavior in section 3.2 is contradictory. It will be good to have comprehensive analysis, how many times collaboration helped arrive at correct answer across all tasks? the qualitative explanation can go only so far. Also, a key point needing details is around the factual information (where the information may be a fact that can be recalled/learn from training data) vs the problem where answer may not be factual but available as a set of rules with multiple possibilities. \n\n8. section 4.1 \"we utilize the majority vote (Li et al., 2022; Cobbe et al., 2021) method to determine the answer for each round.\u201d: how much of this is if you do multishot prompting on an LLM and use ensemble of LLMs? Is there a society aspect to this? or is it just that the ensemble of LLMs gives better answer\n\n9. section 4.1 \"Wavering Answers resemble model hallucination due to the occurrence of self-contradictory answers.\": how can we be sure this is from hallucination? And not model changing it\u2019s output to what user prefers from set of possibilities\n\n10. \"We group samples from different societies under the same strategy because the effect of society is minimal\": what is the scientific evidence to prove that this is indeed the case? \n\n11. collaborative strategies play a significant role in performance: I dont see a significance test to lay this claim\n\n12 4.1 conclusion 2 For continuous reflection strate- gies, the proportion of \u201cWavering Answers\u201d occurrences is the highest among all strategies as seen: Doesnt seem to be the case from figure 4 d-f\n\n13. 4.1 conclusion 2 the strategy of \u201cPure Debate\u201d (i.e., p0 p0 p0 ) can effectively re- duce this fluctuation (hallucination): Doesnt seem to be exclusive to pure debate the case from figure4 d-f\n\n14. 4.1 conclusion 2: if that is true why is hallucination lower for P0P1P1 or P0P1P0\n\n15. section 4.2 number of agents: doesnt seem to be always case on both accounts\n\n16. section 4.2 more rounds: Doesn\u2019t seem to be always the case. The pattern seems inconsistent as is defintion of good or bad. Also, the improvement needs statisitcal significance.\n\n17. Section 4.2 Other Collaborative Strategies: Doesn\u2019t seem to be always the case. All except for 1 case the performance consistently drops. Also, what is significant drop needs defintion (at least for one case where they are pretty close). And same with what is good or bad performance to begin with."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2639/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699223131232,
        "cdate": 1699223131232,
        "tmdate": 1699636203845,
        "mdate": 1699636203845,
        "license": "CC BY 4.0",
        "version": 2
    }
]