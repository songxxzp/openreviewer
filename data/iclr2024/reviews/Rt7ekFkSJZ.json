[
    {
        "id": "FjngykC81Y",
        "forum": "Rt7ekFkSJZ",
        "replyto": "Rt7ekFkSJZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3641/Reviewer_oKhh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3641/Reviewer_oKhh"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an approach for measuring the contribution of features to fairness metrics, namely equalized odds and demographic parity, in decision trees. This metric, called \"Fair Feature Importance Score\" (FairFIS), measures the change in group bias for a feature, akin to the traditional mean decrease in impurity. Through simulations and real-world tests on benchmark fairness datasets, the paper demonstrates the efficacy of FairFIS in providing valid interpretations for tree-based ensemble models and as surrogates for other ML systems."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper proposes a novel metric for quantifying the contributions to the overall feature measure based on the model features used in decision trees. Fairness is an important topic and the proposed metric uses equalized odds and demographic parity, which are two of the most commonly used metrics, making it appealing."
            },
            "weaknesses": {
                "value": "I think that the paper is fairly verbose and the presentation could be more concise. I had to go back and forth between pages to make sure that my understanding of the mathematical notation was correct. Nonetheless, the paper lacks two important discussions:\n* How should practitioner use the proposed metric? A discussion is needed on this. \n* Similarly to FIS, when features are correlated some may receive low FairFIS values even though they matter for fairness purposes. \n\nIn addition, I think that the method should be compared to other baselines. I can think of at least two:\n\n* First, in all the datasets that the authors employed, the number of features is small. Thus, it\u2019d be easy to fit p models where p is the number of features and each model is missing one feature, and then analyze how fairness metrics vary across models.  This idea is similar to leave-one-covariate-out (LOCO) inference, see https://www.stat.cmu.edu/~ryantibs/talks/loco-2018.pdf.\n* Second, one could fit two different models to the data and compare the disparities of the models with the tree-based method proposed by https://arxiv.org/pdf/1707.00046.pdf. The authors could analyze FairFIS for each model and see if they reach similar conclusions about which features contribute towards disparities as they would with the method from that paper. \n\nOther minor details:\n* What are the error bars in Figure 2? Are they confidence intervals? What are the groups? The legend should be improved because it is mentioned that they are G1, G2 etc only on page 7. \n* It seems that the authors are considering $\\Sigma=I$ in the main paper, and $\\Sigma\\neq I$ in the Appendix. Is this correct?\n* Proposition 1 could be shortened. It\u2019s taking up a lot of space and it\u2019s pretty clear how these metrics need to be computed. \n* There are a couple of typos."
            },
            "questions": {
                "value": "Mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698182858094,
        "cdate": 1698182858094,
        "tmdate": 1699636319844,
        "mdate": 1699636319844,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "d2VjURvARp",
        "forum": "Rt7ekFkSJZ",
        "replyto": "Rt7ekFkSJZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3641/Reviewer_rM4n"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3641/Reviewer_rM4n"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors introduced the limited prior work on the understanding of how a feature affects the fairness of the model's predictions. The authors then proposed FairFIS, a surrogate model to interpret tree-based model which assigned the fairness score to each of the features, providing the protected atrribute are provided. The authors demonstrated in simulation and real data that FairFIS is capable of capturing the important features that increase/ decrease the bias w.r.t the protected variables in their definition."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The question the authors attempted to answer is novel and important: Given a protected feature, what are the contribution of other features with respect to minimizing  the contribution of the protected feature. This is a great approach to understanding the fluctuation of feature contribution w.r.t. a particular feature that is not of interests."
            },
            "weaknesses": {
                "value": "1. The paper's approach to addressing fairness through the proposed algorithm is not adequately substantiated. The algorithm requires users to define a set of \"protected\" features, then computes bias based on this selection. This methodology rests on the assumption that the chosen protected features are inherently unbiased\u2014a claim not demonstrated in the paper. To bolster the argument that the FairFIS algorithm genuinely addresses fairness, the authors should either provide a rigorous formal definition of what constitutes a \"protected feature\" or temper their claims regarding the algorithm's ability to ensure fairness.\n\n2. The generative model used in the simulation doesn't align with the authors' objective. In the generative model the authors proposed, they first defined the protected attribute z, and then generate x from z, and finally generate y from x. The graph can be expressed as z --> x --> y. This implicitly suggested that z is the root cause of y. Given this assumption, it is not clear why \"z\" should be designated as a \"protected\" attribute in the first place. Otherwise please simply substitute z as a variable should be protected (i.e. race) and y as any sensitive subject and see what happens."
            },
            "questions": {
                "value": "How should one select the protected features? Who should decide the fairness of the selection of protected features? Are income, country, religion considered as protected features?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698696230850,
        "cdate": 1698696230850,
        "tmdate": 1699636319722,
        "mdate": 1699636319722,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qjQ6962Lnu",
        "forum": "Rt7ekFkSJZ",
        "replyto": "Rt7ekFkSJZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3641/Reviewer_q1kB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3641/Reviewer_q1kB"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed a new feature importance score to investigate whether bias exists in the machine learning models. This score is adapted from the classical feature importance score over tree-based models by considering the difference between the bias of the nodes and their children. Through some experiments in the simulated settings, the authors can demonstrate that the proposed metric can provide reasonable explanations of why bias occurs in the predictions of the machine learning models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The problem of interpreting why and how bias occurs in machine learning systems is an important problem to study\n+ The authors proposed a simple and generic solution to solve this problem\n+ The authors tried to provide extensive experiments to empirically demonstrate the benefits of the proposed solution."
            },
            "weaknesses": {
                "value": "+ I have some concerns about the motivations of this paper. The authors claim that they want to interpret how a feature influences the prediction bias of one model. However, I feel that this problem could be solved by performing counterfactuals over the features that we want to explain and evaluating how the prediction bias gets changed. The authors need to provide more justifications for why this strategy is not satisfactory. \n+ Related to the above point, the baseline comparison in the experiments is very simple and lacks many critical baseline methods, in particular, the state-of-the-art feature importance score over general models. As mentioned above, we adapt such baseline methods by measuring how perturbing the target feature impacts the prediction bias. It would be essential to consider the adaptations of such a baseline and perform an empirical comparison\n+ Interpreting the influence of a feature over the model prediction bias through the decision tree surrogate is also problematic. I am not sure whether the feature that causes the most significant prediction bias for the surrogate model can really cause the same amount of bias in the original model. The authors need to verify this somehow, say through performing counterfactuals over the target features on the original models.\n+ I am also worried about the novelty of the proposed solution. In my mind, it is still like simply replacing the loss function in FIS score with bias metrics, which seems to be straightforward to me. More discussion on why the proposed metric is non-trivial is needed."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3641/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3641/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3641/Reviewer_q1kB"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698733885277,
        "cdate": 1698733885277,
        "tmdate": 1699636319655,
        "mdate": 1699636319655,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FS08GDL4PJ",
        "forum": "Rt7ekFkSJZ",
        "replyto": "Rt7ekFkSJZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3641/Reviewer_Vvdg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3641/Reviewer_Vvdg"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on the problem of explaining the unfairness of ML models. The proposed metric works for tree based models only. They key idea is to slightly alter the node splitting procedure to compute the unfairness score of a feature."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper correctly notes that using explanations to understand the unfairness of a model is an important desideratum in applications of ML.\n\n2. The proposed procedure is simple and easy to understand."
            },
            "weaknesses": {
                "value": "While it focuses on an interesting and timely problem, I think the paper still has some key issues which should be addresses before its ready for publication.\n\n1. **Framing:** The paper claims to be the first to consider fairness and explainability of ML models. For instance, the paper notes that \"we have no current way of understanding how a feature affects the fairness of the model\u2019s predictions\". However, there is already non-negligibly amount of work that focuses on fairness and explainability both. Consider for example [this blogpost](Explaining Measures of Fairness) showing how to use SHAP to understand model unfairness. On a more academic side, I would suggest that the paper factors papers 1-4 (and related references therein) in the related work section so that the readers can better frame its contributions.\n\n2. **Motivation:** The paper needs to motivate the design choices in a better way. I am not sure that \"trees have a popular and easy-to-compute intrinsic feature importance score known as mean decrease in impurity\" is the most important reason to focus on trees. There exist plenty of methods like SHAP, LIME and Integrated Gradients for explaining all kinds of other ML models. Similarly, why consider the mean decrease in impurity (MDI) score? There are other methods like TreeSHAP which seem to offer better theoretical properties. Also, the proposed FairFIS metric has a rather interesting choice in comparing a parent and the child node. The paper explains why the the computation of the style of Eq. 1 was not considered, but does not mention what are the pros and cons of the computation of FairFIS? How can we ascertain that this choice corresponds to how humans would expect model explanations to behave?\n\n3. **Evaluation:** The evaluation, while considering multiple datasets, is quite high level and relies on the parallels between FIS and FairFIS. I would suggest performing a more systematic analysis and consider quantitative evaluations metrics (e.g., those considered [here](https://arxiv.org/abs/1705.07874) and [here](https://arxiv.org/abs/1912.09405)).\n\n4. **Writing:** I think the writing should also be improved before the paper is ready for publication. Currently, the paper tends to simply provide information without first giving motivation and reasoning. Consider for instance the experiment in Section 3.1, where the paper directly dives into the details of the experiment without explaining why it was set up in this way, what metrics should the users watch out for, and what kind of values of should they expect to see.\n\n[1] [What will it take to generate fairness-preserving explanations?](https://arxiv.org/pdf/2106.13346.pdf)\n\n[2] [Biased Models Have Biased Explanations](https://arxiv.org/pdf/2012.10986.pdf)\n\n[3] [Cohort Shapley value for algorithmic fairness](https://arxiv.org/pdf/2105.07168.pdf)\n\n[4] [Exploring the usefulness of explainable machine learning in assessing fairness](https://theses.ubn.ru.nl/server/api/core/bitstreams/b019c49d-b4e9-4f81-a5e6-ac4fadcdbf0f/content)"
            },
            "questions": {
                "value": "Please see points 1-3 in the \"Weaknesses\" section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3641/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3641/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3641/Reviewer_Vvdg"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3641/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699650451440,
        "cdate": 1699650451440,
        "tmdate": 1699650451440,
        "mdate": 1699650451440,
        "license": "CC BY 4.0",
        "version": 2
    }
]