[
    {
        "id": "tPjQHS4W41",
        "forum": "IlQxeKrWDt",
        "replyto": "IlQxeKrWDt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2641/Reviewer_3B4m"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2641/Reviewer_3B4m"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a prompting method for LLMs in synthetic logical reasoning tasks. First, the model is prompted to output a \"mind-map\" representation of the problem, with rules and facts represented in symbolic first-order logic terms, and then connected to each other when they share a term (like the consequent or premise of different rules match). Then, a pruning stage reduces the mind-map, eliminating irrelevant rules. Finally, the mind-maps are used in chain-of-thought reasoning about the original question. Experiments on ProofWriter and PrOntoQA, two synthetic logical reasoning datasets, show improvements for GPT-3.5-Turbo and text-davinci-003."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Experimental results are strong for the two datasets that the paper uses. The results show notable improvement on PrOntoQA-OOD with OrIntro, which is interesting.\n\nThe method is also complementary to backward chaining (e.g., LAMBADA). \n\nCoP also takes less LLM cals than LAMBADA on average."
            },
            "weaknesses": {
                "value": "The clarity of presentation can be significantly improved. I don't fully understand the method (some questions & suggestions below).\n\nThe scope of the paper, with the current evaluations, is very narrow. The authors only use synthetic logical reasoning datasets. The problem of having lots of irrelevant context might be too specific for this particular kind of logical reasoning problem. It's unclear to me what is the takeaway for most of the ICLR audience.\n\nAlso, the authors discuss that the approach differs from LogicLM or other approaches that use an external logical solver. While I understand the difference, since here the LLM itself performs the logical reasoning and can thus be a bit more flexible with minor syntactic inconsistencies, the proposed method has some level of external symbolic processing if I understand, hand-written by the authors (e.g., partitioning the mind map by parsing the graph). Thus, it's not a \"fully LLM-driven\" method either."
            },
            "questions": {
                "value": "- What \"misleading information\" (as opposed to irrelevant) are the authors referring to in Section 3, first paragraph? In these problems, as far as I understand, everything in the question is to be taken as true (so it can at most be irrelevant).\n- Method: after context reconstruction, is the input in the same format as the original input (only with rules pruned / reordered)? Or does some of the structure of the mind map still remain?\n- Method: how do you unify terms that can be connected in different rules? Is it just exact match?\n- How does the number of tokens used compare to LAMBADA? LLM calls are not the most important metric here, since tokens are what drive overall API costs.\n- Are the results in a zero-shot or few-shot setting?\n- Why did LAMBADA perform so poorly with And rules? This should be discussed.\n- What other tasks other than synthetic logical reasoning problems can CoP be applied to? Any \"real-world\" examples? This could be datasets like FOLIO, or more broad logical reasoning tasks (e.g., LogiQA, or ReClor).\n- What exactly are the \"Incorrect Goal\" and \"Incorrect Sign\" failure modes in Figure 4?\n- What are the main failure modes that remain? Are they in the construction of the mind map, in parsing the rules into symbolic forms, or in reasoning even after both previous steps are correct?\n- Why is LogicLM only evaluated in the 5-hop setting?\n\n- Minor: the pseudo-code can be simplified by simply removing the \"else - pass\" twice"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2641/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2641/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2641/Reviewer_3B4m"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698628661233,
        "cdate": 1698628661233,
        "tmdate": 1699636204369,
        "mdate": 1699636204369,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k5bLHzOBwO",
        "forum": "IlQxeKrWDt",
        "replyto": "IlQxeKrWDt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2641/Reviewer_Fc11"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2641/Reviewer_Fc11"
        ],
        "content": {
            "summary": {
                "value": "This work studies complex deductive reasoning problems for large language models. They aim to reduce the difficulty of LLMs proof planning and propose a reasoning approach named Concise and Organized Perception (COP). The approach has three stages. It first creates concept maps to highlight the hierarchical relationships among the provided rules and facts, then identifies the relevant contexts and generates a mind map-like structure based on the provided question, and finally prunes the mind map and reconstructs the context for prompting the reasoning of LLMs until a true or false statement regarding the given question is made. They conduct experiments on three synthetic logical reasoning datasets and demonstrate that the approach is effective and efficient."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed COP outperforms the compared SoTA methods in different deductive rules. It greatly relieves the problem of CoT predicting a correct label with incorrect reasoning chains according to the manual check. COP can also adapt to different LLMs and be more efficient in inference calls. \nThe paper is well-organized and clearly written. It includes comprehensive experiments, and the results support their claim. The proposed method and the experimental findings provide valuable thoughts to the LLMs deductive reasoning problem."
            },
            "weaknesses": {
                "value": "Please see the questions listed below."
            },
            "questions": {
                "value": "Q1: What are the proofs generated by CoT are like? How does CoT concretely improve the proof planning of LLMs? It would be better if the authors could show some of the cases. \n\nQ2: The experiments are conducted on three synthetical logical reasoning datasets. I wonder if this approach can be adapted to real-world data."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2641/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2641/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2641/Reviewer_Fc11"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698681213372,
        "cdate": 1698681213372,
        "tmdate": 1699636204270,
        "mdate": 1699636204270,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KLUfd3ZALq",
        "forum": "IlQxeKrWDt",
        "replyto": "IlQxeKrWDt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2641/Reviewer_pmsh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2641/Reviewer_pmsh"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a novel technique on distilling and organizing relevant fact for reasoning. This technique can not only reduce the cost of querying the large language models but also lead to better reasoning performance comparing to the existing state of the art baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality: 3.5/5\n\nAlthough there are a few works on decomposing a question into smaller pieces where the language models are better at solving, this work focuses on distilling related knowledge to optimize the number of queries to large language models. \n\nQuality: 4/5\n\nThe work has been evaluated on a handful of datasets and greatly improved the existing baselines in both accuracy and query number to language models.\n\nClarity: 2.5/5 \n\nThe methodology part of the concept, mind graph generation, and mind graph pruning is a bit hard to read without a consistent running example and the prompt. \n\nSignificance: 3/5\n\nAs the LLM query is priced based on token numbers, reducing the cost and improving the performance is out of people's interest."
            },
            "weaknesses": {
                "value": "See Strength."
            },
            "questions": {
                "value": "1. What is the failure case analysis on the concept graph generation, mind graph generation, and mind graph pruning?\n\n2. Related work: Scallop[1] is a similar approach as logic-LM, but with probabilistic reasoning engine. \n\n[1] Hanlin Zhang, Jiani Huang, Ziyang Li, Mayur Naik, and Eric Xing. 2023. Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2641/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2641/Reviewer_pmsh",
                    "ICLR.cc/2024/Conference/Submission2641/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824435015,
        "cdate": 1698824435015,
        "tmdate": 1700554638366,
        "mdate": 1700554638366,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "B8KhgJx7kw",
        "forum": "IlQxeKrWDt",
        "replyto": "IlQxeKrWDt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2641/Reviewer_fpHM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2641/Reviewer_fpHM"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel approach titled Concise and Organized Perception (COP) that aims to enhance the deductive reasoning capabilities of Large Language Models (LLMs). They show that while LLMs like GPT-3 have shown promise in complex reasoning tasks, they often struggle with systematic reasoning and tend to produce errors due to misaligned information flow and lack of hierarchical understanding. They observe that the structured approach of COP can reduce the difficulty of multi-hop reasoning tasks for LLMs, potentially leading to more accurate and efficient processing of complex deductive reasoning questions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The COP approach takes advantage of both \"Concise\" and \"Organized\" strategies to improve LLMs' deductive reasoning capabilities, a technique that hasn't been explored extensively in previous works. This approach also involves generating mind maps and reconstructing context, which is a unique integration of visualized reasoning into LLMs.\n2. The COP's extension to pre-existing models like ProofWriter and ProtoQA demonstrates a level of creativity in enhancing the capabilities of LLMs beyond simple iterative refinements, potentially indicating a new direction for future LLM-based deductive reasoning research.\n3. The results show a significant improvement over established baselines, with the COP method outperforming traditional methods and even newer approaches like LAMBADA+COP, especially in multi-hop reasoning problems."
            },
            "weaknesses": {
                "value": "1. The paper seems to heavily build upon pre-existing methods such as \"graph-based reasoning systems\" and \"structured knowledge integration,\" which have been extensively explored in the literature (e.g.,logical NNs and other neuro-symbolic approaches)\n2. The justification for why the combination of concise and organized strategies is more effective remains unclear under a rigorous theoretical framework. The examples provided (Sec 3 and Fig 3) do not offer evidence that this combination offers a qualitatively different approach to reasoning in LLMs as compared to existing methods.\n3. The experimental results reported in Section 4.2 might not be robust enough. The benchmarks ProofWriter and ProtoQA, is not sufficiently demonstrating the general efficacy of COP across various datasets and reasoning tasks. Besides, the lack of a detailed error analysis or ablation study also keep the improvement of the whole system a mystery."
            },
            "questions": {
                "value": "Illustrated in the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699165358725,
        "cdate": 1699165358725,
        "tmdate": 1699636204102,
        "mdate": 1699636204102,
        "license": "CC BY 4.0",
        "version": 2
    }
]