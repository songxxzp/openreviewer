[
    {
        "id": "HXpAfmgh2t",
        "forum": "cMPm8YFXZe",
        "replyto": "cMPm8YFXZe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission602/Reviewer_1FNr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission602/Reviewer_1FNr"
        ],
        "content": {
            "summary": {
                "value": "The article proposes a novel approach called Alternating Denoising Diffusion Process (ADDP) for general-purpose representation learning in both discriminative and generative tasks. This method combines pixels and the Vector Quantization (VQ) space, alternating between the pixel space and the VQ space during the denoising process. It can generate diverse and high-fidelity images while achieving excellent performance in image recognition tasks. The authors validate the performance of this method in various tasks such as unconditional generation, ImageNet classification, COCO detection, and ADE20k segmentation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The article proposes a novel method for learning image representations that can be applied to both image generation and recognition tasks.\n2. The proposed method achieves promising results in unconditional image generation and image recognition tasks.\n3. Detailed ablation experiments are conducted to verify the effectiveness of each introduced module."
            },
            "weaknesses": {
                "value": "1. $z_{t-1}$ and $\\bar{z}_{t-1}$ are conditionally independent, but why does using $z_t$ to predict $\\bar{z}_{t-1}$ result in significantly better performance?\n2. The performance of the generative models on ImageNet-256 in Table 2 is no longer state-of-the-art. Updated results of recent image generation models need to be included.\n3. Due to the encoder being trained on noisy images, there is a significant drop in performance in Linear Probing.\n4. Does expanding the training dataset to include both original images and noisy images improve image recognition performance?\n5. Based on my understanding, VQ space is used for image generation while the output space of the encoder is used for image recognition. Are these two spaces independent, and is it possible to merge them into the same space? If not, is it feasible to directly fine-tune models based on the VQ space for image recognition? How does it perform? \n6. The illustrated masked tokens $\\bar{z}_{t-1}$ and $z_{t-1}^{pred}$ in Figure 4 are not matching."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission602/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission602/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission602/Reviewer_1FNr"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission602/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698568646946,
        "cdate": 1698568646946,
        "tmdate": 1699635987933,
        "mdate": 1699635987933,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GgE8gLhKO8",
        "forum": "cMPm8YFXZe",
        "replyto": "cMPm8YFXZe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission602/Reviewer_hM7K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission602/Reviewer_hM7K"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an alternating process that simultaneously generates tokens and pixels. This pipeline is suitable for both generation and recognition tasks. However, the additional encoder's effectiveness is unclear as it lacks clear motivation and results. In recognition tasks, the encoder is only applied to pure images, making its training on noisy images relatively meanless."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper investigates the feasibility of simultaneously generating pixels and tokens to design a university representation for both generation and recognition tasks."
            },
            "weaknesses": {
                "value": "Although this pipeline yields relatively good results for both generation and recognition tasks, the issue lies in its redundancy and lack of relevance. The additional encoder does not appear to significantly contribute to the generation task, as token-to-pixel and pixel-to-token iterations do not enhance generation. Rather, the additional encoder is solely utilized for recognition purposes. Essentially, this pipeline merely combines two effective submodels without discovering mutual benefits."
            },
            "questions": {
                "value": "What is the role of token-to-pixel and pixel-to-token at each step of the generation task? Perhaps performing it only once at the end would yield similar results in generation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission602/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission602/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission602/Reviewer_hM7K"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission602/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698753860464,
        "cdate": 1698753860464,
        "tmdate": 1699635987862,
        "mdate": 1699635987862,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GoH20b5odk",
        "forum": "cMPm8YFXZe",
        "replyto": "cMPm8YFXZe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission602/Reviewer_pb3B"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission602/Reviewer_pb3B"
        ],
        "content": {
            "summary": {
                "value": "The paper \"ADDP: Learning General Representations for Image Recognition and Generation with alternating denoising diffusion process\" attempts to construct models which can understand raw pixels while, at the same time, allowing the generation of visual representations. The proposed architecture is evaluated on the ImageNet-1k, COCO and ADE20k data sets. The paper's main contribution is the integration of image classification, segmentation and generation within a single architecture."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Joint representation learning, in this case for image understanding and generation,  is an interesting research problem. The paper's primary research question is a good fit for ICLR.\n- The number of cited papers in the related work section is extensive.\n- To the best of my knowledge, the main contribution is novel.\n- Experimental results are convincing, especially the extension to segmentation tasks on COCO and ADE20k."
            },
            "weaknesses": {
                "value": "- The text is hard to follow. Additional copy editing may help here.\n- The related work seems to focus on listing many papers. It would help if the related work would attempt to explain some of the key concepts instead of just listing them.\n- Figure text is often too small to read in print."
            },
            "questions": {
                "value": "- What is the structure of the VQ-Decoder in equation 1?\n- Are VQ-Tokens defined following van den Oord?\n- What is the VQ-Decoder in equation 3? Which one is used?\n- What is the encoder structure in equation two?\n- Where does the mask in Figure 5(a) come from?\n- Why can the MAGE-L network be discarded during inference?\n- What do the lock symbols mean in Figure 4? Perhaps the locks signify constant network elements?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission602/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698774637170,
        "cdate": 1698774637170,
        "tmdate": 1699635987790,
        "mdate": 1699635987790,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WCCJHH8W6S",
        "forum": "cMPm8YFXZe",
        "replyto": "cMPm8YFXZe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission602/Reviewer_SakC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission602/Reviewer_SakC"
        ],
        "content": {
            "summary": {
                "value": "Motivated by recent advances in recognition using pixels as inputs and generation via VQ tokens, this paper presents an alternative denoising diffusion process (ADDP) that leverages the strengths of both domains. ADDP utilizes both raw-pixel and VQ spaces to perform recognition and generation tasks. The proposed method includes a Token-to-Pixel decoding stage that generates visual image pixels from VQ tokens. Subsequently, in the Pixel-to-Token Generation stage, the proposed method predicts VQ tokens from noisy images. The process employs an alternative denoising approach that generates pairs of reliable and unreliable tokens before producing noise-free images. The overall architecture is then tested for visual recognition and generation tasks using the ImageNet, COCO, and ADE20K datasets. Moreover, the ablation study of unreliable tokens and the mapping function, as well as prediction targets and masking ratios, demonstrates the effectiveness of the model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-\tOverall, the proposed idea of leveraging both token space and raw space appears interesting. \n-\tThe paper's material is presented clearly, and from my perspective, the overall method seems sound."
            },
            "weaknesses": {
                "value": "-\tWhile the overall concept of the paper is appealing, I believe additional evaluations (as mentioned below) would enhance the paper. \n\n-\tI am concerned that the current approach of utilizing VQ-AE for the diffusion process and the token-to-pixel conversion could diminish the generative diversity of the model. \n\n-\tI think the paper should acknowledge the limitations of the methods more openly and aim for greater clarity and specificity."
            },
            "questions": {
                "value": "-\tCould you provide more details on the criteria used to generate reliable and unreliable tokens? It is unclear which specific mechanisms within the model determine a token's reliability. An ablation study focusing on this aspect could offer deeper insights into the significance and impact of this feature. \n\n-\tHow can we ensure that the pixel-to-token generation process does not become constrained by a limited range of samples, especially when utilizing a frozen VQ Encoder-Decoder? Additionally, it would be beneficial to see quantitative comparisons of the proposed method against non-VQ generative models, specifically concerning the diversity of generated samples. \n\n-\tEvaluating the dependence of the overall model's performance on the VQ Autoencoder's efficacy would likely yield valuable information about the model's robustness. Consider conducting such an evaluation to provide a clearer understanding of this relationship. \n\n-\tThe performance gap observed between ADDP and other methodologies in the Linear ImageNet benchmark warrants a comprehensive explanation. Could you elucidate the detailed reasons behind this discrepancy? \n\n-\tFinally, visualizing the token spaces through projection to a 2D plane could offer a more intuitive understanding of the model's performance. Have you considered including such visualizations or projections to aid in the evaluation of the model's effectiveness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission602/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699251010929,
        "cdate": 1699251010929,
        "tmdate": 1699635987729,
        "mdate": 1699635987729,
        "license": "CC BY 4.0",
        "version": 2
    }
]