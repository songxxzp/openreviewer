[
    {
        "id": "XIBGDe4oe6",
        "forum": "CCo8ElCT7v",
        "replyto": "CCo8ElCT7v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5630/Reviewer_RorU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5630/Reviewer_RorU"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a comprehensive comparison between ViTs and CNNs for face recognition tasks, focusing on face identification and verification. The study evaluates six models (EfficientNet, Inception, MobileNet, ResNet, VGG, and ViTs) on five diverse datasets, highlighting the performance, robustness, and inference speed of ViTs compared to CNNs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper conducts thorough experiments on various network architectures for different evaluation tasks and datasets in face recognition.\n2. It offers valuable insights for the design of network structures in face recognition applications."
            },
            "weaknesses": {
                "value": "1. This paper seems more like an experimental evaluation report, primarily focusing on the organization of test numbers.\n2. It would be beneficial for the authors to extrapolate some new insights from these figures, potentially providing fresh perspectives on training or evaluation in face recognition."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5630/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698504554281,
        "cdate": 1698504554281,
        "tmdate": 1699636583438,
        "mdate": 1699636583438,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0xwmQsMUKH",
        "forum": "CCo8ElCT7v",
        "replyto": "CCo8ElCT7v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5630/Reviewer_6T9x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5630/Reviewer_6T9x"
        ],
        "content": {
            "summary": {
                "value": "The paper presented a fair and comprehensive benchmark of 5 CNN-type networks and Vision Transformer on 5 face recognition benchmark datasets in terms of face verification and validation tasks. This benchmark enforced the exact training and testing set splits for fair comparison and compared the training and test accuracy, the number of parameters and inference time. The results show that the ViT network compares favorably to those CNN-type networks w.r.t. the face recognition performance and computation complexity on these benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper performed a rigorous evaluation of EfficientNet, Incpetion, MobileNet, ResNet, VGG and ViT networks by training them for face recognition tasks and compared their performance thoroughly."
            },
            "weaknesses": {
                "value": "Certainly this is a quite useful technical report on the evaluation of different popular network architectures for face recognition tasks. I am not convinced this work\u2019s \u201cparamount significance as it pioneers a comprehensive evaluation of ViTs against CNNs\u201d.\n\nThis benchmark compared the performance of 6 vanilla networks trained for face recognition tasks. In fact, there are many dedicated face recognition methods and pipelines including face detection and alignment, etc. The FR field probably cares more about the end-to-end performance of the whole face recognition pipeline.\n\nThe FRTE is probably the most thorough evaluation for the industry, which tests the performance of binary programs provided by different FR vendors on a blind set with no limitation of the training dataset or anything.\n\nFace recognition Technology Evaluation (FRTE) organized by NIST\nhttps://pages.nist.gov/frvt/html/frvt1N.html\n\nImportant references missing:\nDeepFace: closing the gap to human-level performance in face verification, CVPR 2014. \nDeep learning face representation by joint identification-verification, NIPS 2014\nComparing vision transformer and convolutional neural networks for image classification: a literature review, 2023."
            },
            "questions": {
                "value": "The performance appears saturated on some face recognition datasets. The results of several cases may affect the benchmark. Is there any more challenging test set for face recognition?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed.",
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5630/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744938261,
        "cdate": 1698744938261,
        "tmdate": 1699636583337,
        "mdate": 1699636583337,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "H3xefwZsNE",
        "forum": "CCo8ElCT7v",
        "replyto": "CCo8ElCT7v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5630/Reviewer_dZCP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5630/Reviewer_dZCP"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an empirical, mainly quantitative, comparison between CNNs and Vision Transformers (ViTs) as evaluated on both face identification and face verification. Five different CNNs and one VT are evaluated across six datasets. It is found that the ViT (ViT-B32) almost consistently outperforms all CNN architectures. The experiments are systematic."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Empirical investigations of the behavior and performance of neural networks is of large importance. It is brave of a paper to take a step back and systematically compare architectures rather than constantly presenting new -- potentially not as thoroughly tested -- models or add-on modules.\n\n* We are in the middle of a paradigm shift between vision Transformers and CNNs so it is certainly important right now to try and map out empirical differences between the two.\n\n* Five datasets and six models are used (extensive evaluation)\n\n* The code is made public.\n\nMinor but positive things:\n\n* The three paragraph of the introduction had a good flow and were easy to read (see only that the first face paragraph could be a bit more specific)\n* 2.1 is informative and well-written"
            },
            "weaknesses": {
                "value": "* No uncertainty (e.g., standard deviations across random seeds) are presented for the different results. This may be ok since the models are evaluated across many different datasets, but in that case the seed should be fixed and that should be stated.\n\n* Missing a clear motivation for why facial recognition is the investigated task. Has it previously not been done for this field, are CNNs still considered the main models there? Also, it would be constructive to discuss the ethical risks vs. benefits of surveillance computer vision applications. (An ethical statement could be in order.)\n\n* Missing a comment on how the hyperparameters common for all architectures were selected (e.g., following another paper's set up, just as standard hyperparameters, etc). It is important that they were not selected to optimize a specific architecture, and it would be good to convince the reader about this.\n\n* I would avoid referring to my own paper as having 'paramount significance' (strong wording, verging on over-selling). The number of times the word 'remarkable/y' (6) is a bit exaggerated as well.\n\n* No explanation is offered for why the ResNet surpassed the ViT in Fig. 7b.\n\nDetailed minor suggestions:\n* References should be in parentheses (Dosovitskiy et al. (2020))\n* This paragraph could be made more specific (since you claim that it in fact does present **very specific** challenges), it is currently not so informative: \"...that presents very specific features and challenges. The main challenges are related to the low inter-class variance and the high intra-class variance that can be observed in most face image datasets Cao et al. (2018); Huang et al. (2008). This makes face recognition a more difficult task than...\"\n* I would (sadly) avoid using the wording 'in spirit' in this context (2.1, page 3)\n* 'Convoluting' should be changed to 'convolving', and quotations should be removed\n* page 4, \"an for\" >> \"and for\"\n* It could be nice with a table summarizing the 5 datasets and tasks.\n* Fig. 6: would be nice to have accessible in the caption whether this dataset is made for face verification or face identification."
            },
            "questions": {
                "value": "* Page 5, \"is bounded between 0 (the worst measure of separability) and 1 (a perfect measure of separability), with 0.5 indicating that a network has no class separation capability whatsoever.\" >> if 0.5 already has 0 separability, what happens between 0.5 and 0? Maybe rephrase\n* 3.3: it is not clear to me if you use the checkpoint from the best epoch or from the 25th (last) epoch for the test results in Table 2? If you just use the last epoch (which I suspect since you say that each model has been trained for 25 epochs), it would be more informative to report the validation accuracy at this epoch (for the model you actually use.)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5630/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698775494902,
        "cdate": 1698775494902,
        "tmdate": 1699636583016,
        "mdate": 1699636583016,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s1fTxHxAJr",
        "forum": "CCo8ElCT7v",
        "replyto": "CCo8ElCT7v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5630/Reviewer_D6eT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5630/Reviewer_D6eT"
        ],
        "content": {
            "summary": {
                "value": "In this manuscript, the authors attempt to study the performance of general-purpose Vision Transformers in Face Recognition scenarios and contrast their findings against general-purpose Convolutional Neural Network architectures. They claim that ViT performance perform better than the compared CNNs in this scenario."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The document is mostly well-presented in its structure, use of the English language, and figures."
            },
            "weaknesses": {
                "value": "This work completely disregards other popular works in the face recognition literature. The authors compare general-purpose CNNs and ViT_B32 when efficient face recognition-specific approaches are already available such as MobileFaceNet (Chen et al. 2018), ShuffleFaceNet (Mart\u00ednez-Diaz et al., 2019), VarGFaceNet (Yan et al., 2019), GhostFaceNets (Alansari et al., 2022), EdgeFace (Geroge et al., 2023), among others. Furthermore, does not mention previous studies on transformers for face recognition (Zhong et al., 2021) and part-based face recognition with vision transformers (Sun et al., 2022), for example. They also do not comment on recently popular Hybrid (ViT+CNN) approaches, as in EdgeFace and MobileFaceFormer (Li., 2023). \nThe datasets described are not divided into scenarios and do not include relevant challenging datasets (e.g. using TinyFace (Zheng et al., 2018) and SurvFace (Zheng et al., 2018) to complement low resolution comparisons with SCface). In general, this study misses many comparisons in the state of the art for face recognition scenarios such as: cross age with AgeDB (Moschoglou et al., 2017), cross pose with CFP (Sengupta et al., 2016), racial-bias analysis with RFW (Wang., 2018), among many others."
            },
            "questions": {
                "value": "Suggestions:\n- Familiarize with recent literature specific on face recognition and gather benchmark on key datasets.\n- Analyze the components that make the face recognition-specific approaches more accurate on face recognition scenarios."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No particular comments in this section"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5630/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698948536518,
        "cdate": 1698948536518,
        "tmdate": 1699636582872,
        "mdate": 1699636582872,
        "license": "CC BY 4.0",
        "version": 2
    }
]