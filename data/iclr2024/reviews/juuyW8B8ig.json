[
    {
        "id": "ZAmziNmCU6",
        "forum": "juuyW8B8ig",
        "replyto": "juuyW8B8ig",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission320/Reviewer_jNDi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission320/Reviewer_jNDi"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method for learning concepts by distilling knowledge in a text-to-image generative model. The method assumes concept axes, in each of which specific information of an input image is encoded (like colors, materials, and object categories). The method learns a designated encoder for each concept axis with generated images. For training the encoders, the method uses an anchor loss for each axis based on a VQA model to further disentangle the axes and a reconstruction loss. The method is evaluated qualitatively and quantitatively (CLIPScore and human evaluation)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The paper is well-written. I can easily follow what is done in the method.\n\n(2) The method is simple and can be trained in 12 hours only with less than 1000 generated images, yet outperforming the similar existing methods. \n\n(3) The image generation results are really nice compared to the existing approaches."
            },
            "weaknesses": {
                "value": "(1) The performance of the method is mostly shown in qualitative evaluations. The quantitative evaluation only shows the performance of image generation by modifying some concept axes (and human evaluation). I think the paper would be better if it came with an objective quantitative evaluation of the obtained concepts themselves in some ways (though I didn\u2019t come up with any good approaches for this). \n\n(2) Related to (1), I\u2019m not sure if CLIPScore is really sensitive to arbitrary combinations of concepts. Some references or experimental results may help understand the experiment. \n\n(3) The paper\u2019s purpose is not sufficiently clear. Is it to learn concepts for image generation? Or is it for some other downstream tasks?"
            },
            "questions": {
                "value": "I would like to have some responses for (1)-(3) in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698570370310,
        "cdate": 1698570370310,
        "tmdate": 1699635958648,
        "mdate": 1699635958648,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "U0L4F5k725",
        "forum": "juuyW8B8ig",
        "replyto": "juuyW8B8ig",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission320/Reviewer_Bcg9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission320/Reviewer_Bcg9"
        ],
        "content": {
            "summary": {
                "value": "The authors claim that their proposed model can learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors claim that their proposed model can learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models."
            },
            "weaknesses": {
                "value": "1. What is concept representation learning? Is concept learning just the mutual translation of text and images?\n\n2. In the experiments, the authors primarily focus on conducting investigations using synthetic datasets. However, it raises concerns about the generalizability of the conclusions/findings obtained from synthetic datasets to real-world datasets.\n\n3. The concept learning should focus more on the understanding of concepts, especially at different granularities of the same concept."
            },
            "questions": {
                "value": "Please refer to Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698668162685,
        "cdate": 1698668162685,
        "tmdate": 1699635958574,
        "mdate": 1699635958574,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RoZNR8UakC",
        "forum": "juuyW8B8ig",
        "replyto": "juuyW8B8ig",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission320/Reviewer_dyyk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission320/Reviewer_dyyk"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a new framework for Visual Concept Learning. By introducing a set of concept encoders, concept embeddings could be extracted from the input image, which could be recomposed later to produce desired image. The experiments showed that these concept embeddings could capture the visual nuances and they are disentangled with each other. Besides, this framework can learn the shared concepts across instances(images), in other words, it is more efficient than previous methods.\n\nThe paper proposes a new framework for VCL task that avoids massive human annotation, and could boost the research in related fields. It also drew the attention to the research direction of using continuous embeddings (instead of relying on generic words) as the visual concept descriptors. The concept is commendable, although the depiction falls short of perfection and would greatly benefit from additional elaboration and intricate explanations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "a. Proposed the idea to use BLIP-2 generated embeddings as \u201canchor\u201d (pseudo ground-truth embeddings) to \u201cseparate\u201d the entangled concept embeddings.\n \nb. This work proposed a framework to tackle Visual Concept Learning without human annotation, which is much more efficient than the previous works.\n \nc. Unlike most of the Textual Inversion techniques, this work could capture the concept appearing in different images. Therefore, it does not require a retraining on each image. Also, the learning efficiency is higher because it can learn from a larger pool of images."
            },
            "weaknesses": {
                "value": "a. The datasets used for the experiment were small and simple. It is not guaranteed that the claimed conclusions could be maintained when this framework is applied on more complex datasets (with much more concepts). The idea of using anchors to ensure that the embeddings are disentangle is great, however more experiments on larger datasets should be done to prove it. Given that there are only 2 to 3 concepts in each domain, the sparsity of concepts might be one of the reasons why the embeddings are disentangle.\n \nb. The effectiveness of L^{anchor} is not fully explained. The L^{anchor} is omitted during the test-time optimization procedure to avoid \u201cover-committing to the coarse text anchors\u201d. However, in the ablation experiment, the paper claims \u201cdisabling L^{anchor} deteriorates the performance\u201d. It seems kind of contradictory, the paper should explain more about why \u201cdisabling L^{anchor}\u201d is desired during one phase but it leads to unsatisfactory results in general evaluation.\n \nc. The ablation test is not fully explained. In \u201cEditing Category\u201d column, the results of \u201cw/o Encoder & L^{anchor}_k\u201d is actually higher than the results of \u201cw/o L^{anchor}_k\u201d in two metrics. This does not fully conform to the conclusion, quote, \u201cand further removing the encoder decreases the overall decomposition performance\u201d.\n \nd. It is hard for this framework to generalize to new \u201cconcept\u201d. From what I understood, this framework could effectively generalize to new \u201cmode\u201d of seen \u201cconcept\u201d (like new style or new color), but not to new \u201cconcept\u201d. When applied to new concepts, e.g. \u201csize\u201d or \u201cshape\u201d, the corresponding concept encoders need to be trained. Also, from my perspective, we can\u2019t only train the concept encoder of the new concepts. Because the sentence template \u201ca photo of <e1> with <e2> color and <e3>  and <e4>...\u201d needs to cover at least the majority of the concepts appeared to generate an image close enough to the original input image. Based on this understanding, when this framework is extended to new concepts, the trained concept encoders (of the seen concepts) need to be retrained together with the new ones. This setting is not more efficient than previous methods."
            },
            "questions": {
                "value": "a. On page 5, in sentence \u201ctext encoder c_{theta} of the T2I diffusion model\u2026\u201d, it should be \u201cpart of the text encoder c_{theta}\u201d. Because a text encoder should take \u201ctext\u201d as input rather than \u201ctext embeddings\u201d. The original sentence might be confusing.\n \nb. On page 5, in formula (1), there is no explanation about the N and U notation. I assume they represent \u201cmultivariate normal distribution\u201d and \u201cuniform distribution\u201d respectively. It would be more clear to annotate.\n \nc. On page 5, in sentence \u201cso that they can directly inserted into the text embeddings\u201d, it seems that a \u201cbe\u201d is missed.\n \nd. The paper should mention the backbone T2I model earlier. It is first mentioned on page 5, it would be better to do it earlier.\n \ne. It would be better if the choice of using \u201c12 CLIP layers\u201d over \u201c6 out of 12 tokens like Gal et al. 2023\u201d is explained more in detailed.\n \nf. More details could be added about the test-time lightweight finetuning process."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698736498640,
        "cdate": 1698736498640,
        "tmdate": 1699635958480,
        "mdate": 1699635958480,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zWJ8Ddf3TE",
        "forum": "juuyW8B8ig",
        "replyto": "juuyW8B8ig",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission320/Reviewer_qJGK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission320/Reviewer_qJGK"
        ],
        "content": {
            "summary": {
                "value": "The authors use multiple visual encoders to disentangle various visual concepts from images. These visual concepts are defined as vector axes based on natural language description. The proposed framework performs a simple training on a synthetic dataset that learns disentangled vectors for each concept. These disentangled vectors can be combined with language (similar to textual inversion paper) to generate images containing combined concepts. They show how their method disentangles and generates new images with variably joined concepts better than existing work."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper explores an interesting idea of separating concepts in images in the visual domain in a novel manner\n2. Interesting use of a VQA model to augment their training setup\n3. Good use of diagrams to explain idea \n4. Clearly showcase qualitative improvements for selected cases"
            },
            "weaknesses": {
                "value": "1. The generality of method on real world images (i.e. where visual concepts are not that easily disentangled) is unclear\n2. Limited evaluation (only one set of quantitative numbers)\n3. Some missing details (refer questions below)\n\n* Table 1: Are you reporting CLIP score and human evaluation on same table?? \nPlease point out CLEARLY that these are two different metrics in the Table caption. Or please separate into two Tables. This is highly confusing."
            },
            "questions": {
                "value": "* Immediate question - why don't concept axis vectors collapse to same as text embeddings? Explain this more. \n* DeepFloyd - please cite appropriately \n* Consider more discussion on Textual Inversion (as related work), maybe in supplementary at least. Highlight cases where this is better than directly using text. \n* The work in [1] explores language defined concept axes in video domain - maybe an interesting comparison to discuss in related work  \n* Please include BLIP-based baseline results also in Table 1\n* Can you add more CLIP-score based (or a different new metric based) evaluations for other task (like concept extrapolation)? More quantitative evaluation could really strengthen the paper\n\n[1] Ranasinghe, K., & Ryoo, M., Language-based Action Concept Spaces Improve Video Self-Supervised Learning, NeurIPS 2023"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission320/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission320/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission320/Reviewer_qJGK"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785932643,
        "cdate": 1698785932643,
        "tmdate": 1699635958402,
        "mdate": 1699635958402,
        "license": "CC BY 4.0",
        "version": 2
    }
]