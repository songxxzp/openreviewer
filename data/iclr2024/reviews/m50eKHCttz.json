[
    {
        "id": "piHByz2CZJ",
        "forum": "m50eKHCttz",
        "replyto": "m50eKHCttz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4151/Reviewer_yxq3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4151/Reviewer_yxq3"
        ],
        "content": {
            "summary": {
                "value": "The authors provide an empirical study of the ability to transfer complementary knowledge between different pretrained models without performance degradation. This paper analyzes existing approaches in knowledge distillation and find it insufficient, especially in the case of distilling specific information from weaker teacher models. They go on to propose a data partitioning-based method (into regions of desired teacher behavior and desired student behavior retention) to achieve complementary knowledge transfer between the pretrained models considered in this paper."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The paper empirically shows that complementary knowledge exists between a large suite of models defined by different architectures and sizes (and even in weaker models that are less well performant than other stronger models). This complementary knowledge is localized to particular classes (what the authors deem as relative areas of expertise)\n\n2. The authors propose a new data-partitioning approach to transfer complementary knowledge, where data is partitioned by which model has a higher probability of the ground truth class (or chosen simply by maximum probability in an unsupervised case).\n\n3. Extensive experimental results that demonstrate that the proposed distillation approach transfers at a higher rate and transfers complementary knowledge from weaker teacher models.\n\n4. The paper also studies different properties of student models that better allow for knowledge transfer."
            },
            "weaknesses": {
                "value": "Overall, I think the paper is quite comprehensive. A few points that may be lacking:\n\n1. The results in studying properties of student models is a bit surprising to me. This isn\u2019t a huge weakness, but more exploration of why CNN student models improve with scale and why transformer student models seem to worsen with would strengthen these results.\n\n2. The data partitioning heuristic is reasonable, but some ablations on this approach would be more enlightening. Perhaps in some instances, the student model may be overconfident about particular data points (that either have incorrect labels or are inherently difficult examples to classify), and this data partitioning approach would maintain this overconfidence."
            },
            "questions": {
                "value": "1. Do you have any intuitions as to why student models that are CNNs exhibit better transfer at scale (while other architectures do not)? (Figure 8 in Supplement)\n\n2. In Table 3, unsupervised DP outperforms supervised DP on several tasks. This seems a bit surprising; in the cases where these methods would be different, your DP approach would be distilling information from the teacher model on instances where the model is both quite confident and incorrect. Do you have an ideas about how this would be beneficial, and does this match your intuitions as to why this method works in general?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4151/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4151/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4151/Reviewer_yxq3"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4151/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698471448428,
        "cdate": 1698471448428,
        "tmdate": 1699636380886,
        "mdate": 1699636380886,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "H4fQ2LLjaV",
        "forum": "m50eKHCttz",
        "replyto": "m50eKHCttz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4151/Reviewer_QCZm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4151/Reviewer_QCZm"
        ],
        "content": {
            "summary": {
                "value": "This paper studies general knowledge distillation (KD), where, given any two models, the student can infer missing information from the teacher model and potentially improve the performance. The authors begin with a comprehensive analysis showing that such complementary knowledge generally exists in any paired models regardless of model capacity or architecture, and existing KD methods cannot leverage the information that student models already carry, i.e., trained students. To this end, the authors propose a continual learning-based extension to the existing KD methods and a data partitioning scheme that, according to the highest prediction probability., simultaneously maintains the useful knowledge of students while learning from the teacher. The extensive experiments conducted on more than 400 models sufficiently verify the effectiveness and provide many insightful analyses."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The problem studied in this paper, i.e., general knowledge distillation, is interesting and practical. It bridges the gap in the existing literature that a trained student might degrade during the knowledge distillation process.\n\n2. The additional analysis also indicates almost every model could benefit from the other teacher models, even if the teachers are weaker than the students. The result and the evaluation methodology may motivate the community for further research.\n\n3. The proposed method is sound yet easy to implement in practice. The authors also consider different scenarios of distillation, including a single teacher and multiple teachers distilled in different ways/orders.\n\n4. The large-scale analysis (over 400 models) validates the claim and provides various insights, such as the properties of student models."
            },
            "weaknesses": {
                "value": "1. The student models considered in this paper are relatively strong. As the authors claim **general** knowledge distillation, the readers will also be interested in the weaker models or even from scratch. However, the authors only consider powerful architectures, such as transformers or ResNet, in the paper.\n\n2. The proposed method is slightly confusing to me. My understanding is that the proposed data partition is built upon the continual learning method since Figure 4(a) clusters KL-Dist + DP Transfer into continual learning. If so, the contribution of each component is not clear enough to me. Though some experiments, e.g., Figure 4(b), present the partition improves the MCL transfer, the contribution of the partition itself is not investigated in the experiments.\n\n3 (Minor) Consistency of italic type. Some \"KL+DP\" are italics in the text, while some are not."
            },
            "questions": {
                "value": "1. What would happen if one applies the proposed method to weaker models, e.g., randomly initialized? I guess it will be degenerated to conventional KD methods. A specific section for weaker/smaller models would be interesting to the community of edge device users/researchers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4151/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834793664,
        "cdate": 1698834793664,
        "tmdate": 1699636380802,
        "mdate": 1699636380802,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1zrM6BHhkF",
        "forum": "m50eKHCttz",
        "replyto": "m50eKHCttz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4151/Reviewer_epvt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4151/Reviewer_epvt"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the phenomenon that different models have complementary information, reflected in their different predictions on each sample. Such complementary information could be due to model architectures, training settings, etc. Then the authors study how to transfer the complementary knowledge from a teacher model to a student model. The authors formulate this as a continual learning problem, which effectively improves upon the knowledge distillation baseline  and achieves better knowledge distillation on diverse models and ImageNet."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper identifies the \"complementary knowledge\" in different neural networks with grounded evidence. I think the claims are reasonable and well-supported.\n\n2. The authors proposed improvement to the general knowledge distillation approaches from the continual learning perspective, including constraining the weight updates and transfer data. Both of the approaches are reasonable and straightforward to apply.\n\n3. The authors conduct experiments on a wide range of models on ImageNet and show improvement with their improved continual knowledge distillation approach."
            },
            "weaknesses": {
                "value": "1. Present the key terms more clearly. For example, I haven't found the definition of transfer delta $\\Delta_{transf}$, which is an important evaluation metric.\n\n2. I think the proposed regularization, including the constraining of weight updates and transfer data, are some tricks for knowledge distillation. Although I don't work directly in knowledge distillation and I will wait for other expert reviewers to justify the novelty, I think the authors need to clarify more about their motivation or form a more explicit connection with continual learning. I also raised a question in the next section, elaborating my concerns.\n\n3. I suggest the authors add some more experiments to make the arguments more thorough. Specifically, a more detailed breakdown of the accuracy numbers would help. For instance, (a) the percentage of [teacher correct, student wrong] samples are changed to correct answers after the distillation, (b) the percentage of [teacher incorrect, student correct] samples are changed to incorrect labels, to understand if the transfer is indeed complementary."
            },
            "questions": {
                "value": "1. Explain more about the terms. The readers are likely to have an ambiguous understanding of them, including: transfer delta $\\Delta_{transf}$, \"available complementary knowledge per class,\" and transfer rate.\n\n2. I am wondering if the methods have to reason from the \"continual learning\" perspective. In my opinion, the regularization techniques proposed by the authors seem like generally applicable tricks for knowledge distillation. If any procedure involving multiple steps has to be treated as continual learning, maybe training a neural network with SGD is also a continual learning process? I hope the authors can clarify this motivation better in the paper.\n\n3. See the third weakness above.\n\n4. I suggest the author add an oracle study to strengthen the argument. In the examples (e.g. Table 1), the final improvement seems small in scale. To argue that this is actually challenging, the authors can run several ensembles of the teacher-student model and compare it to the improvement from knowledge transfer. Of course, I welcome other variants of similar analytical studies from the authors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4151/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4151/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4151/Reviewer_epvt"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4151/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699297296552,
        "cdate": 1699297296552,
        "tmdate": 1699636380727,
        "mdate": 1699636380727,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k9d69AXIWO",
        "forum": "m50eKHCttz",
        "replyto": "m50eKHCttz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4151/Reviewer_LBqG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4151/Reviewer_LBqG"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates if it is possible to transfer complementary knowledge from one model to another without performance degradation. To this end, authors propose different heuristics to design how to switch the knowledge transfer between models. Experiments with various pairs of models demonstrate the effectiveness of the model-agnostic knowledge transfer."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The writing is clear and easy to follow.\n\n2. There are consistent performance improvements compared to different types of baselines."
            },
            "weaknesses": {
                "value": "1. As addressed by the authors, different models are trained with different data augmentations, architectures and optimization techniques. The performance improvements are relatively marginal (e.g., Table 1), especially considering some models are not fully trained.\n\n2. In Table 4, do the authors train all the variations with the same number of training steps? The sequential knowledge transfer may benefit from more training steps."
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4151/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4151/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4151/Reviewer_LBqG"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4151/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699408735416,
        "cdate": 1699408735416,
        "tmdate": 1699636380669,
        "mdate": 1699636380669,
        "license": "CC BY 4.0",
        "version": 2
    }
]