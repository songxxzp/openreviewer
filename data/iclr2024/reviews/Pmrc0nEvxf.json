[
    {
        "id": "Roj4SKmU1B",
        "forum": "Pmrc0nEvxf",
        "replyto": "Pmrc0nEvxf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8086/Reviewer_4nwA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8086/Reviewer_4nwA"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the problem of adversarial attack on temporal graph neural networks.  Two challenges (e.g., Noise Decaying and Knowledge Missing) in attacking  temporal GNNs are first proposed. To address these challenges, the authors proposes an effective adversarial attack framework called MemStranding. Experimental results show that MemStranding can significantly decrease the TGNN models\u2019 performances in various tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper focuses on adversarial attacks on TGNNs, studying the robustness of TGNNs to benefit their applications.\n2. The authors provide a clear explanation of why typical attacks fail in dynamic graph scenarios.\n3. In response to the failure of typical attacks, two attack goals are proposed: noise persisting and noise propagating, effectively addressing the issue."
            },
            "weaknesses": {
                "value": "1. The studied problem is interesting but the application scenarios of attacking TGNNs should be introduced.\n2. The paper provides a good explanation of why typical attacks fail, but does not provide experimental results. I am curious about the effectiveness of typical attacks on dynamic graphs.\n3. Section 4.3, Stage 1, lacks further analysis on victim node sampling. The choice of high-degree nodes as root nodes is explained, but there is no analysis of what would happen if low-degree nodes were chosen as root nodes. Moreover, are there criteria for selecting support nodes?"
            },
            "questions": {
                "value": "1. If the persisting loss is removed, how would the similarity between the root node's memory and the initial noisy memory change in Figure 8? I'm curious about this.\n2. Still regarding Figure 8, how does the similarity of memory between the root node and its 1-hop and 2-hop neighbors change during normal training of TGNNs?\n3. The noise persisting loss and noise propagating loss proposed in this paper effectively prolong the efficacy of noise, even after multiple new actions. I'm curious about the effect if we combine the attack loss proposed in this paper with meta attack.\n4. Question about Section 4.3, Stage 2. The paper mentions, 'Lastly, we can add the solved noisy message as a fake node or fake edge accordingly and remove it after the attack.'  Is this similar to directly modifying the victim node's memory? Will this noisy message's impact spread with the arrival of the next action? \n\n[1] Adversarial Attacks on Graph Neural Networks via Meta Learning, ICLR 2019."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8086/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698729411433,
        "cdate": 1698729411433,
        "tmdate": 1699637001308,
        "mdate": 1699637001308,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NMBtskJK6r",
        "forum": "Pmrc0nEvxf",
        "replyto": "Pmrc0nEvxf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8086/Reviewer_LiVK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8086/Reviewer_LiVK"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the MemStranding framework, which is designed to launch adversarial attacks on Temporal Graph Neural Networks (TGNNs) by utilizing node memories to generate persistent and propagating adversarial perturbations in dynamic graphs. The authors empirically validate the efficacy of MemStranding in diminishing the performance of TGNN models across a spectrum of tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper stands out for its innovative approach to attacking TGNNs. It identifies the limitations of existing adversarial methods and introduces a novel framework that uses node memories to create persistent and spreading adversarial disturbances in dynamic graphs, an unexplored concept in prior research.\n\n2. The paper is lucidly written and easily comprehensible. It offers clear explanations of the paper's concepts and techniques, ensuring accessibility to a broad readership. \n\n3. This paper makes a significant contribution by introducing a previously unexplored approach to TGNN attacks. The proposed framework is practical and effective, as demonstrated through compelling experimental results in various scenarios. The paper's findings carry vital implications for TGNN model security and advocate for further research in this domain."
            },
            "weaknesses": {
                "value": "1. The paper lacks a comprehensive discussion of the limitations of the proposed method, including performance variations with different TGNN architectures and graph data characteristics.\n\n2. It is not clear that fakenode is the state-of-the-art attack method. However, the paper only compare the proposed method with fakenode only."
            },
            "questions": {
                "value": "1. Can you provide more insights into the limitations of the proposed method, such as its sensitivity to the size and density of the graph, the number of target nodes, and the choice of the attack budget?\n\n2. Can you provide more insights into the potential defenses against the proposed method, such as the use of adversarial training, graph regularization, or outlier detection?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8086/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765476760,
        "cdate": 1698765476760,
        "tmdate": 1699637001185,
        "mdate": 1699637001185,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ucis1VpSXn",
        "forum": "Pmrc0nEvxf",
        "replyto": "Pmrc0nEvxf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8086/Reviewer_aaPY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8086/Reviewer_aaPY"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework called MemStranding, which is used to attack Temporal Graph Neural Networks (TGNNs) by leveraging node memories to create adversarial noises in dynamic graphs. The authors provide experimental results to demonstrate the effectiveness of MemStranding in decreasing the performance of TGNN models in various tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Attacking graph under GNNs is promising\n2. The paper discusses real-world scenarios where dynamic graphs are prevalent, which highlights the relevance and importance of the proposed framework.\n3. The authors identify the limitations of existing adversarial attacks on TGNNs and explore the challenges of adapting them to TGNNs within these constraints."
            },
            "weaknesses": {
                "value": "1. The experiments are weak\n2. The presentations are unclear\n3. The comparisons are weak"
            },
            "questions": {
                "value": "1. The authors do not provide a comprehensive comparison of MemStranding with other existing adversarial attacks on TGNNs. For example, TIGIA [1] and a lot of methods in surveys [2] propose injective attacks. But the author only compares one the fakenode baseline.\n\n2. The experimental results are limited to small datasets, which may not be sufficient to generalize the effectiveness of MemStranding in real-world applications.\n\n3. It only uses limited TGNNs for evaluations. Recently, researchers have proposed more powerful TGNNs, such as roland [3]\n\n4. It only uses raw TGNN for evaluations without using current GNN defenders for evaluations. Considering current platforms may use GNN defenders instead of raw GNNs, it should explore the effectiveness of attackers under GNN defenders.\n\n5. The paper assumes that the attacker has complete knowledge of the graph structure and node attributes before each time t, which may not be realistic in real-world scenarios.\n\n6. unclear part: Section 4.1 is unclear, which should add more explanations on why coverage state is so important.\n\n[1]Zou, Xu, et al. \"Tdgia: Effective injection attacks on graph neural networks.\" Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 2021.\n\n[2] Sun, Lichao, et al. \"Adversarial attack and defense on graph data: A survey.\" IEEE Transactions on Knowledge and Data Engineering (2022).\n\n[3] You, Jiaxuan, Tianyu Du, and Jure Leskovec. \"ROLAND: graph learning framework for dynamic graphs.\" Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper focuses solely on attacking TGNN models and does not explore the potential defenses against such attacks."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8086/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8086/Reviewer_aaPY",
                    "ICLR.cc/2024/Conference/Submission8086/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8086/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834519261,
        "cdate": 1698834519261,
        "tmdate": 1700643982470,
        "mdate": 1700643982470,
        "license": "CC BY 4.0",
        "version": 2
    }
]