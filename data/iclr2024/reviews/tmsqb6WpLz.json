[
    {
        "id": "6MAbHbYsrO",
        "forum": "tmsqb6WpLz",
        "replyto": "tmsqb6WpLz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9346/Reviewer_o2MC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9346/Reviewer_o2MC"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the effects of finetuning on an LM. The authors disentangle the effects of the finetuning along three \u201cdimensions\u201d: topic, style, and factual knowledge. To achieve that, they leverage ChatGPT to generate a series of texts that are only different in one of those facets. Provided such texts, one can estimate log-likelihood ratio of different styles (for instance), by calculating differences of cross-entropies of the model on the texts.\nThe generated texts were verified by human judgements. \n\nThe experimental study is performed using two corpora (BioMed and C4) and three LMs (GPT-2 XL, LLaMa 2 - 7B & 13B). \n\nThe paper reports the following findings: (a) topic and style changing rapidly,(b) topic and style biases are independent, (c) topic and style require minimal capacity to be learned, in contrast to knowledge, (d) mixing in unbiased data only reduces the biases to a certain degree."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* I find the topic of the investigation quite novel. I believe that the approach taken is original and innovative, in particular building a corpus that allows disentangling style/topic/factual knowledge. I also like the way LoRa was used to measure the capacity required for learning different facets.\n* The authors are sharing the data and code.\n* The reported experiments have provided some applicable insights, e.g. wrt the data mixing."
            },
            "weaknesses": {
                "value": "* Using synthetic data, generated by ChatGPT, might introduce some hidden biases. It is not given that the same findings could be found if we had natural data.\n* It is not clear if the same approach can be generalized to any other characteristics?\n\nTypos:\n* \u201cby just changing the order of decomposition in 1\u201d -> \u201c...in Eq. 1\u201d"
            },
            "questions": {
                "value": "* I wonder if there are other factorizations which can be studied in the same setup, apart from style/topic/knowldge?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9346/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698486488509,
        "cdate": 1698486488509,
        "tmdate": 1699637176101,
        "mdate": 1699637176101,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KX6fDquf1N",
        "forum": "tmsqb6WpLz",
        "replyto": "tmsqb6WpLz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9346/Reviewer_QBAp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9346/Reviewer_QBAp"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates the impact of finetuning language models on domain-specific texts and how it affects their general performance. The authors show that finetuning alters the model's preferences for topics and styles significantly, learning these features quickly and with minimal capacity. Factual knowledge, however, is acquired more slowly and requires greater capacity. The study's insights into language model learning dynamics could guide future enhancements in domain adaptation and help address the challenge of model forgetting during continuous learning.\n\nIn this study, the authors fine-tuned three models in increasing scales (GPT2-XL, LLaMa2 7B, and 13B) on PubMed abstracts with different scales of datasets up to 1M abstracts."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Investigating the changes language models undergo after finetuning continues to be a highly relevant and evolving area of study, despite prior coverage in academic literature.\n\n2. The research offers key empirical insights into the differential impact of finetuning on language models, revealing a more pronounced effect on style and topic preferences compared to factual knowledge. These findings enhance our understanding of language model training dynamics and are instrumental in formulating more effective training methodologies.\n\n3. The researchers conducted extensive experiments on three language models of considerable size, particularly from an academic perspective."
            },
            "weaknesses": {
                "value": "* The assertion that each prediction by a language model can be broken down into components of writing style, topic, and factual knowledge requires further justification or explanation. The paper should present a stronger argument or provide additional evidence to substantiate this claim.\n\n* The primary message or conclusion of the paper is ambiguous. The authors need to clarify the central thesis to ensure that readers can grasp the main contribution of the work. what is the takeaway from this research? \n\n* While the prose is generally lucid, the paper's structure, particularly the introduction, could use refinement to enhance its readability and impact.\n\nFor improved clarity and presentation, the following suggestions are offered:\n1. The introduction would benefit from concrete examples illustrating the key domains of style, topic, and factual knowledge to help contextualize the subsequent findings.\n2. Details regarding the fine-tuning process are scant. Clarification of which specific models are assessed and the precise nature of the fine-tuning would provide a more robust understanding of the study's scope.\n3. The transition from discussing fine-tuning effects to probing methods in the third paragraph of the introduction is somewhat abrupt. A smoother segue that connects these topics would aid reader comprehension.\n4. In the fourth paragraph, where style and topic biases are introduced, it would be helpful to include examples or elaborate on what these biases entail to furnish readers with a clearer picture of these concepts."
            },
            "questions": {
                "value": "* The basis of the method assumes that p(x)=p(topic,style,factual), but is there a justification to that decomposition? what about arithmetic? how does it fall to this decomposition?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9346/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9346/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9346/Reviewer_QBAp"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9346/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698580562967,
        "cdate": 1698580562967,
        "tmdate": 1699637175990,
        "mdate": 1699637175990,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IeQh1AMom9",
        "forum": "tmsqb6WpLz",
        "replyto": "tmsqb6WpLz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9346/Reviewer_xGn4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9346/Reviewer_xGn4"
        ],
        "content": {
            "summary": {
                "value": "This paper dissects the effect of fine-tuning on learning and forgetting of language style, topic, and factual knowledge. The authors use instruction-following LLMs to automatically construct corpus with controlled factors above. The authors performed extensive analysis across different LM types and summarized several empirical findings, among which they show topic and style priors are easy to learn but factual knowledge is not."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The method how the analysis is performed is novel. Creating training and evaluation corpora with controlled differences (topics, style, factual knowledge) by prompting instruction following LLMs is interesting and inspiring.  \n- The analysis is extensive and is performed under various configurations (like the choice of LM, size of the training corpora)\n- The outcomes of analysis are interesting and relevant to future research that study lifelong learning of LMs."
            },
            "weaknesses": {
                "value": "- Although other configurations are very extensive, the choice of training and evaluation corpora and exclusively original or variants of PubMed and C4. \n- The three text-generating factors (styles, topics, facts) may not always be clearly separable of extensive enough in every corpora. The authors discussed this limitation in their limitation section.\n- Clarity issue: I feel the plots very hard to read because the captions are too generic and not self-contained. I suggest to briefly summarize the findings or implications in the captions.\n- Clarity issue: some legends in plots such as Figure 4 are not explained in text (e.g. readers may be confused about \"C4 -factuals\" before they associate them with \"C4-counterfactual\" in Table 1)\n- Though the authors pointed out the hardness of learning factual knowledge without learning style and topic bias, the authors' attempts failed to improve such performance at the end of Sec. 3. I suggest to provide some future directions about how the analysis will be beneficial the challenge of learning factual knowledge above.\n- The authors focused on evaluation of LM loss throughout the paper. I think this is fine for style and topics, but factual knowledge, evaluating LM loss is not clean enough because only a few tokens in a sentence are related to facts. The authors could create cloze-style  or question answering evaluation sets that focus exclusively on generation of factual knowledge."
            },
            "questions": {
                "value": "- There is a \"side note\" in page 7: \"When capacity is limited, the topic ratio and factual ratio simultaneously reduce on Pubmed in Figure 6.\" I did not see topic ratio reduces in Figure 6. Is this information supposed to be told by Figure 6?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9346/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698657942204,
        "cdate": 1698657942204,
        "tmdate": 1699637175869,
        "mdate": 1699637175869,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tAmKnN5JGR",
        "forum": "tmsqb6WpLz",
        "replyto": "tmsqb6WpLz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9346/Reviewer_DT5M"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9346/Reviewer_DT5M"
        ],
        "content": {
            "summary": {
                "value": "- This paper presents a detailed analysis of the effects of fine-tuning of large language models on domain-specific downstream tasks/datasets.\n- In doing so, authors break down the probability distribution of a text into its fundamental factors i.e., topic, style and factual knowledge, and study the effects of fine-tuning on the probability distribution over these three factors.\n- It has been shown that in the early cycles of fine-tuning, the language model easily captures the topic and style information of the underlying text data thus introducing learning bias, which ultimately leads to an increase in the forgetting of the previous knowledge. However, the model is able to capture the factual knowledge in the later cycles of fine-tuning and also requires significant model capacity as compared to the model capacity required for capturing topic and style information.\n- Extensive experimental evaluation asserts the claims made by authors and opens a new research direction in continual learning research."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Quality\n\t- The motivation is well-founded and the claims are sound.\n\t- Experimental analysis is very detailed and explanatory.\n- Clarity\n\t- Paper is clearly presented and easy to follow."
            },
            "weaknesses": {
                "value": "- Quality\n\t- As the topic of a document can be determined by the factual knowledge it contains then it might be redundant to keep the topic as a relevant factor in the text generation process and only style and factual knowledge might suffice which then could directly align with the syntax and semantics of the underlying text respectively.\n- Significance\n\t- This paper presents a detailed technical analysis of the fine-tuning process of a language model on domain-specific downstream tasks/datasets. However, the outcomes of the study conform with the expected outcomes of fine-tuning a model on domain-specific data and hence this paper misses to provide any significant gainful insight into the fine-tuning process due to the following reasons:\n\t\t- In the PubMed dataset, as academic style is present across all abstracts with different factual knowledge, it is expected that the model will readily adapt to the academic style first before capturing the diverse type of factual knowledge.\n\t\t- Just like in topic modeling, the topic of a document is a broad sentiment and can be easily determined using a set of keywords. So, it will be easy for the model to detect/understand the topic of a document before reading the whole document and capturing the factual knowledge inside it. Therefore, it is expected for the model to easily understand the topic and style factors before capturing the factual knowledge inside it.\n\t- I am keen to hear the response of the authors on this and hope that they can change my point of view."
            },
            "questions": {
                "value": "- The C4 dataset could possibly contain the documents written in the \"academic\" style although in a different domain. Similarly, the C4 dataset could also contain documents related to the biomedical domain although having different factual information. So, it is possible that the model is adapting fast to the academic style and biomedical domain topic because it has already seen them in the pretraining data, but the diverse factual information in the PubMed dataset is new for the model, and that is why model is possibly taking time to capture that knowledge. Have authors taken this into consideration in their analysis of the finetuning process?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9346/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698752847344,
        "cdate": 1698752847344,
        "tmdate": 1699637175765,
        "mdate": 1699637175765,
        "license": "CC BY 4.0",
        "version": 2
    }
]