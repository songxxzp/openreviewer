[
    {
        "id": "7ZaSdQcnLR",
        "forum": "MiPacpmtmy",
        "replyto": "MiPacpmtmy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6061/Reviewer_CZT6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6061/Reviewer_CZT6"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a study around the proficiency of multimodal foundation models in comprehending sequential activities. The authors curate the COMPACT dataset, which is a stratified subset of the Epic Kitchens dataset. They use this dataset to validate the performance of various models such as ImageBind, MERLOT Reserve, as well as Llama2."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is clear and well-written. The study of multimodal models on natural video datasets is quite timely.  \nThe idea of curating a subset of an existing dataset, rather than creating one from scratch, is a sound idea, especially since there are many similar datasets out there."
            },
            "weaknesses": {
                "value": "The paper is essentially an evaluation of well-defined tasks using an existing dataset and pre-trained models. \nOut of the three tasks, the problem of action classification is only slightly different from next-utterance noun prediction and verb prediction (i.e., action is a combination of noun + verb, and the other two are predicted separately.) \nVarious pre-trained models are run in a zero-shot manner to predict the next entity in the sequence. While the evaluation results could be of interest to someone who is looking to build these models, the paper main content offers little beyond this evaluation."
            },
            "questions": {
                "value": "The result show that multimodality provides minimal advantage in the next noun and verb predictions tasks. However, the BLEU scores for the action classification are significantly improved. This is counterintuitive, since the actions are essentially nouns + verbs combinations. Therefore, if these individual units are incorrect, the overall BLEU score cannot be significantly better?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6061/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731250114,
        "cdate": 1698731250114,
        "tmdate": 1699636652775,
        "mdate": 1699636652775,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VunhYePF1j",
        "forum": "MiPacpmtmy",
        "replyto": "MiPacpmtmy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6061/Reviewer_w67G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6061/Reviewer_w67G"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to understand the capabilities of multimodal foundation models in terms of compositional generalization. To enable this study, the authors carefully constructed a dataset, CompAct (Compositional Activities), by reusing multimodal data and annotations from an existing benchmark, EpicKitchens-100, while defining new tasks and forming new training, validation, and testing splits. \n\nEach data instance is an instructional video, containing video, audio, and step-by-step descriptions, where each step has a verb and a noun. CompAct is formed such that atomic concepts (verbs or nouns) are consistently distributed across the training and evaluation sets, while compositions of these atomic concepts are novel in the evaluation set. \n\nCompAct allows for the diagnosis of the compositional generalization capabilities of both unimodal and multimodal models. The authors conduct an assessment of several unimodal and multimodal models, and their findings highlight the limited capabilities of prior foundation models for compositional generalization, as well as the importance of multi-modality over single-modality for certain challenging tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The data distributions between training and evaluation in the CompAct benchmark are carefully controlled, allowing for the diagnosis of models' compositional generalization capabilities, which could be useful to the research community.\n\n2. The authors present experimental results for approximately ten different unimodal or multimodal models. Some of the results are intriguing; for example, the language-only method outperforms the multimodal method in noun classification. However, for verb classification or next utterance prediction, the multimodal methods demonstrate superior performance.\n\n3. The proposed method for curating train/eval splits to diagnose compositional generalization appears to be applicable to many other existing video datasets."
            },
            "weaknesses": {
                "value": "The authors have overlooked several works and benchmarks that are highly similar (see Questions below). Compared to these existing works, the contribution of this paper does not seem to be very significant. Additionally, the conclusions drawn from the experiments (e.g., recognition that compositional generalization is an area requiring improvement or that multi-modality could be more important than single-modality for certain challenging tasks) lack depth and insight."
            },
            "questions": {
                "value": "1. The CrossTask [1] dataset and its associated paper focus on an extremely similar study and settings. How does CompAct differ from the CrossTask benchmark? What unique contributions does your work make compared to the CrossTask paper?\n\n2. The GAIN [2] benchmark is also a similar testing ground to the proposed CompAct in terms of evaluating models\u2019 compositional generalizability and robustness under distribution shift. Unlike CompAct, whose atomic concepts are verbs or nouns and compositions are different verb-noun combinations, the atomic concepts in GAIN are steps, and the compositions are multi-step tasks. The authors should acknowledge these similar benchmarks and research efforts and clearly describe how this work advances the field.\n\n3. Would experimental results on CompAct be translatable to these other similar benchmarks like GAIN, CrossTask, etc.? It would be interesting to find out.\n\n4. Why are Maximum Compound Divergence and the Chernoff coefficient good measures for curating a dataset that requires compositional generalization, as opposed to other possible alternatives?\n\n5. At the beginning of Section 4.1, it is mentioned that the first baseline is a text-only model to account for unexpected biases in CompAct. Why does a text-only model account for this?\n\n6. There are many other instructional video datasets. Why was EpicKitchens-100 chosen?\n\n7. For Noun Classification, there is the MROH baseline, which stands for Most Recent Object Heuristic. Why are there no results for the Most Recent Verb Heuristic in the Verb Classification task?\n\n8. Why is the keyframe selection method different for ImageBind?\n\n\n[1] Zhukov, Dimitri, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic. \"Cross-task weakly supervised learning from instructional videos.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3537-3545. 2019.\n\n[2] Li, Junlong, Guangyi Chen, Yansong Tang, Jinan Bao, Kun Zhang, Jie Zhou, and Jiwen Lu. \"GAIN: On the Generalization of Instructional Action Understanding.\" In The Eleventh International Conference on Learning Representations. 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6061/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698803401906,
        "cdate": 1698803401906,
        "tmdate": 1699636652615,
        "mdate": 1699636652615,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UrA4si1dpH",
        "forum": "MiPacpmtmy",
        "replyto": "MiPacpmtmy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6061/Reviewer_7kwf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6061/Reviewer_7kwf"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the compositional generalization ability of multimodal approaches including baselines that are trained from scratch and existing large-scale pre-trained models. To do that, the introduces a new dataset called COMPACT, which is curated from the EK-100 dataset by ensuring that the individual concepts (verbs and nouns) exist across training and evaluation sets, while their compositions are novel in the evaluation set. The paper proposes two tasks for evaluation: (1) next utterance prediction: predicting the descriptions of the event in next video clip and (2) atom classification, predicting only the verb/noun involved in the event in the next video clip. The paper benchmarks several neural network models on the proposed tasks, including (train-from-scratch) text-only (unimodal) and multimodal models with different combinations of modalities as well as several large scale pretrained models using prompting techniques. Results show that all multimodal models surpass the text-only baseline."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The detailed strengths are as follows:\n1. This paper is interesting because it is trying to understand the compositional generalization capabilities of foundation models. This is a crucial skill for intelligent agents and yet there are limited work and benchmarks proposed to investigate the question. Paper in this topic should be encouraged.\n1. It investigates the important topic of compositional generalization capabilities in foundational models. This is a crucial skill for intelligent agents and yet there are limited research and benchmarks in this domain. Studies like this should be encouraged.\n  - However, the paper appears to have limitations in addressing this issue for large-scale pre-trained foundational models. See weaknesses for details.\n2. To answer this question, the paper presents a carefully curated novel dataset from real-world videos which could be much useful for future studies.\n3. The paper also designs a set of multimodal models use different combinations of modalities (including unimodal) and different ways of fusing the multi-modal information. This investigation provides valuable insight on how multi-modality inputs could influence the performance of models' compositional generalization ability."
            },
            "weaknesses": {
                "value": "1. The paper does not sufficiently investigate the compositional generalization ability of **foundation** models. Addressing this is challenging due to the potential distributional discrepancies between training and testing splits during their pretraining, as noted in the paper. Consequently, emphasizing \"foundation models\" in the title may be somewhat overstated.\n   - Could incorporating domain-specific fine-tuning offer additional insights?\n\n2. The dataset's domain-specific nature results in text descriptions that lack diversity. As a result, unlike foundation LLM, language models trained on these specific tests might be prone to overfitting and lack reasoning skills. On the other hand, other modalities, such as the vision input processed by a pretrained ResNet model, inherently resist overfitting, potentially leading to enhanced generalization. Thus, the conclusion that multi-modality contributes to improvements and that visual features consistently enhance results could potentially be invalid."
            },
            "questions": {
                "value": "Please see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6061/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813400969,
        "cdate": 1698813400969,
        "tmdate": 1699636652433,
        "mdate": 1699636652433,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "M2jEVfY3qn",
        "forum": "MiPacpmtmy",
        "replyto": "MiPacpmtmy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6061/Reviewer_JtuP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6061/Reviewer_JtuP"
        ],
        "content": {
            "summary": {
                "value": "This work studies the compositionality of vision language model. Specifically, it studied EPIC Kitchens-100 dataset and tailor the dataset with Maximum Compund Divergence heuristic for compositional generalization analysis.  The evaluations are performed on a number of methods, like VL, AL, OL, AVL, OAL, yet they all underperform the most recent object heuristic. The paper does not present novel algorithm or dataset, but provide an analysis for established ones."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper focus on studying the composition of foundation models on many variants, on Epic kitchen dataset that is tailored for composition evaluation."
            },
            "weaknesses": {
                "value": "The paper draws a conclusion that multimodal helps composition, yet from Table 1, the trend is not very clear."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6061/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698875442755,
        "cdate": 1698875442755,
        "tmdate": 1699636652290,
        "mdate": 1699636652290,
        "license": "CC BY 4.0",
        "version": 2
    }
]