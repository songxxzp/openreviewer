[
    {
        "id": "dFiZPvm4Q9",
        "forum": "Q3aKBKCqG8",
        "replyto": "Q3aKBKCqG8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9356/Reviewer_isHH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9356/Reviewer_isHH"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the settings for early-exiting thresholds, which determines which samples will be output at each early exit during inference. This is an interesting topic in both NLP and CV. An algorithm is proposed to decide the confidence thresholds without groud-truth labels. Experiments on NLP tasks show that the method outperforms existing early-exiting approaches. However, there are some overclaims, and important references are missing. Moreover, the experiment results are not convincing enough (see weakness)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The studied topic is interesting, and the motivation is clearly explained;\n2. The method is technically sound."
            },
            "weaknesses": {
                "value": "1. **Overclaim**. The authors claimed that existing methods usually use labeled datasets to determine the early-exiting thresholds, and the proposed method removes the need for ground-truth labels. This is not correct. Maybe the cited baselines in NLP do need ground-truth labels. But if the authors pay attention to the dynamic models in the CV field (see the second part, **Missing references**), they can find that the decision of confidence thresholds can purely rely on the confidence distribution on the training/validation set. Specifically, one can decide the ratio of samples exiting at different exits, and solve the threshold based on the confidence scores of each exit without touching the ground-truth labels. In summary, the main contribution claimed by the authors, may not hold.\n\n2. **Missing references**. It is recommended that the authors compare their method with the aforementioned strategies in the CV field [1,2,3]. \n\n3. **Inconvincing experiments**. In Tab. 2, the proposed method is compared with other baselines at a **fixed** computational cost. However, the main advantage of dynamic early exiting is one can adjust the thresholds for different computational budgets (see the smooth curves in [1,2,3]. It is kindly suggested that the proposed method is compared with the \"ratio -> threshold\" pipeline in the CV field.\n\n[1] Huang et al, Multi-Scale Dense Networks for Resource Efficient Image Classification. \n\n[2] Yang et al, Resolution Adaptive Networks for Efficient Inference.\n\n[3] Han et al, Dynamic Perceiver for Efficient Visual Recognition."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9356/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698289194894,
        "cdate": 1698289194894,
        "tmdate": 1699637176931,
        "mdate": 1699637176931,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZBWoJQNC8y",
        "forum": "Q3aKBKCqG8",
        "replyto": "Q3aKBKCqG8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9356/Reviewer_P3Sw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9356/Reviewer_P3Sw"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an online algorithm based on multi-armed bandits for adjusting the confidence threshold of BERT model with early exit gates. The objective is to reduce the model latency while maintaining high accuracy. The authors build on the existing rich literature of adding early exit classifiers on top of intermediate layers. Here, they focus on the challenge of selecting a good confidence threshold for deciding for each exit gate if to \"exit\" or not. Specifically the authors assume a domain shift of the test data and propose an online algorithm for adjusting the threshold according to the observed data.\n\nA multi-armed bandit online algorithm is proposed for updating the exit threshold. The reward is designed as the difference in confidence between the last layer and exit layer subtracted by the increased cost, for instances that didn't exit. To normalize the two measures to a similar range, the cost $o$ is some value in [0,1].\n\nFirst, an algorithm for single exit is described, then extension to multiple exits is presented. The experimental setting is focusing on OOD evaluation and examines the number of transformer layers computed vs. accuracy: training on one classification task and evaluating on another related dataset with similar classification task (repeated for 5 pairs)."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Focusing on online adjustment of the early exit threshold is novel and interesting. The proposed method is based on multi-armed bandit and provides  an upper bound on the regret. Detailed algorithms are provided and experiments on 5 classification NLP datasets."
            },
            "weaknesses": {
                "value": "I value the novelty of the method and find it interesting. However, I see several weaknesses in the current paper:\n\n1. While the proposed method is presented as general, the applicability beyond a single exit layer significantly increases the complexity and the solution space, possibly leading to long exploration stage before converging (the regret bound is only in expectation).\n2. Also, many of the hyper-parameters feel pretty specific to the examined setting and justified in the paper with hand-wavy statements (e.g. \"strategically positioned\", \"due to overthinking similar to overfitting during training\" etc.), or with references to the appendix that don't fully explain them. This limits the generalizability of the solution.\n3. The value of the cost parameter $o$ that is given to the end-user as a handle for controlling the desired cost is a uninterpretable value between [0,1]. Therefore, at the end of the day it feels like the user will still need to have some further calibration for tuning the value of $o$ to match whatever practical cost they can afford in their own measure and units.\n4. While I see novelty and value in online adjustments of the threshold. The unsupervised novelty is less clear: see for example [1, 2, 3]. [1] and [3] seem to work with unlabeled data, and [3] seem to focus on threshold calibration which might be good to compare against.\n5.  The experiments feel a bit underwhelming and unclear:\n* The evaluation metric only measures the number of transformers layers and doesn't take into account any potential overhead of the exits and the calibration (and the use of \"Time\" as the column heading) is confusing.\n* Since the method focuses on online setup, it would be interesting to see the patterns over time.\n* The baselines model are not described well (for example, unclear what is the difference between ElasticBERT and DeeBERT).\n* It is unclear how come the UBERT models could be better than the baselines in both accuracy and cost? If the backbone model is identical and roughly monotonic (as assumed throughout the paper), then the threshold should only control the tradeoff between the two but cannot improve on both?...\n\n[1] https://aclanthology.org/2020.acl-main.537/\n\n[2] https://aclanthology.org/2020.acl-main.593/\n\n[3] https://aclanthology.org/2021.emnlp-main.406/"
            },
            "questions": {
                "value": "see points in weakness section above. Also:\n1. In eq.1 : are $C_p$ and $C_l$ always computed by max over softmax? the argmax can be different between the layer $p$ and $l$, making the use of the delta as reward less convincing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9356/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698604518532,
        "cdate": 1698604518532,
        "tmdate": 1699637176820,
        "mdate": 1699637176820,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i8vgY10ORY",
        "forum": "Q3aKBKCqG8",
        "replyto": "Q3aKBKCqG8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9356/Reviewer_sWQ2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9356/Reviewer_sWQ2"
        ],
        "content": {
            "summary": {
                "value": "Inference latency is a key issue in any pre-trained large language models like BERT. Typically, side branches are attached at the intermediate layers with provision of early exit to minimize the inference time. This paper proposes an online learning algorithm, dubbed as, \"UBERT\" to decide if a sample can exit early through an intermediate branch."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Paper is well-written and the problem setup is mostly clear."
            },
            "weaknesses": {
                "value": "I am not an expert in this domain. However, I have few concerns.\n1. Is it necessary to formulate the problem as multi-armed bandit setup? As RL usually resource hungry algorithms and they can take huge time to optimize."
            },
            "questions": {
                "value": "Refer to weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9356/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9356/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9356/Reviewer_sWQ2"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9356/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764583695,
        "cdate": 1698764583695,
        "tmdate": 1699637176673,
        "mdate": 1699637176673,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Hcdlgaq0OJ",
        "forum": "Q3aKBKCqG8",
        "replyto": "Q3aKBKCqG8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9356/Reviewer_nNLW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9356/Reviewer_nNLW"
        ],
        "content": {
            "summary": {
                "value": "This work presents an early exit method for BERT. The authors use MAB to adaptively find threshold value in an online manner. The evaluation is conducted on 5 classification tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The ablation seems good.\n2. The concept of finding adaptive threshold seems interesting.\n3. The proposed method outperforms the compared baselines."
            },
            "weaknesses": {
                "value": "1. Evaluation is restricted.\n2. The authors claim are often missing supporting literature or validating experiments. \n3. Authors exaggerates the efficacy of their method, even though they cannot also resolve the problem.  \n4. The paper is missing the essential information, which prevents the paper to stand alone.\n5. Missing details."
            },
            "questions": {
                "value": "1-1 Considering recent SOTA NLP models/LLMs, BERT variants are quite older and their size is much smaller, which can already run smoothly with restricted resources under the current HW. Therefore, the only evaluation with BERT-variant in this work makes me doubt about the motivation of this work. The authors should've considered larger language models and showed the generalizability of the proposed work. If the scope is only limited to BERT, its applicability/practicability is questionable as BERT is barely used in real applications.   \n\n1-2 One of closely related work is F-PABEE that outperforms all existing methods in the literature. I highly recommend to add its result for comparison and analysis. \n\n1-3 Other previous papers like PABEE and F-PABEE, CoLA MNLI MRPC QNLI QQP RTE SST-2 (STS-B) are standardized for comparison. However, authors does not follow. \n\n2-1 \"Even though it is anticipated that the final layer of the NN can have better accuracy than the intermediate layer\": any support literature or experiments? This is the fundamental assumption, which is not validated throughout the paper. \n\n2-2 \"The threshold is often determined using a labeled dataset during training and serves as a crucial reference point for decision-making during inference.\": in what cases or papers?\n\n2-3 \"The optimal threshold value depends on the distribution of confidence levels at the attached exit, which can vary depending on the data distribution. \": any proofs?\n\n2-4 \" UEE-UCB Hanawal et al. (2022) leverage the MAB framework to learn the optimal exit in EENNs\": How is their use of MAB different from this work? \n\n2-5 In Sec.2 in lines starting \"LEE Ju et al. (2021b), DEE...\", these works seem quite close to the proposed method. It would be recommended to have details comparison against the proposed work.\n\n3-1 As authors noted, using a fixed threshold may yield suboptimal results. However, the proposed method finds the threshold based on the observations of previous samples, which cannot be also free from the same issue. So it seems \"Consequently, UBERT sets itself apart\" is not an appropriate claim.\n\n3-2 The term online learning is quite confusing in this work. As in Sec. 6, the pretrained model is finetuned and this finetuned model is used to adaptively find threshold in an online manner. The adaptive finding is of course online, but this term (online learning/algorithm) is exaggerated and providing confusion. \n\n3-3 The authors keep using the term \"optimal threshold\" throughout the paper. However, it is optimal only if the given specific setting in Algorithm 1 and 2 is used. With naive changing the cost such as adding a value or scaling, it varies. It is hard to conclude that the proposed method optimally trade-off between latency and accuracy. Is there a curve, for example, UBERT-2 shows best accuracy with -59.5 time while the accuracy reduces with -58 or -60 time? If not, the use of this term seems not proper in this context. \n\n4-1 The detailed description of ElasticBERT and MAB is not provided. \n\n5-1 \"Though confidence and latency are in different units, we add them after using a conversion factor.\": I cannot find details of this process in the paper.\n\n5-2 What should I do if I want to improve the latency while sacrificing the performance or vice versa? The new model should be trained again? If so, although the authors adaptively find the threshold with reward, it is hard for me think it as a benefit compared to other exiting methods. Other work simply change the number and run the model to adjust latency-performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9356/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698958925676,
        "cdate": 1698958925676,
        "tmdate": 1699637176556,
        "mdate": 1699637176556,
        "license": "CC BY 4.0",
        "version": 2
    }
]