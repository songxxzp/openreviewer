[
    {
        "id": "jHX7TAkNnc",
        "forum": "vZEgj0clDp",
        "replyto": "vZEgj0clDp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2470/Reviewer_RjCq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2470/Reviewer_RjCq"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to address the knowledge state representation and the core architecture design challenges of knowledge tracing (KT). To this end, the authors propose the ReKT model. They first take inspiration from the decision-making process of human teachers and propose the knowledge state of students from three different perspectives. Then, the authors propose a Forget-Response-Update (FRU) framework as the core architecture for the KT task. They finally demonstrate the effectiveness of their model in terms of efficiency in computing and effectiveness in score prediction through experiments on 7 public real datasets. Their experimental results show that their proposed method can reach the best performance in the question-based KT task and the best/near-best performance in the concept-based KT task, and their proposal only requires 38% computing resources compared to other KT core architectures."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper introduces a multi-perspective approach to modeling the knowledge state of students, considering questions, concepts, and domains, which is logically self-consistent.\n\n2. The FRU framework designed as the core architecture of ReKT is lightweight yet effective. According to experiments, ReKT can achieve competitive performance with significantly fewer parameters and computing resources compared to other core architectures.\n\n3. The experimental results demonstrate the superior performance of ReKT in question-based KT tasks and its competitive performance in concept-based KT tasks, showcasing the effectiveness of the proposed model."
            },
            "weaknesses": {
                "value": "1. In terms of the methodology, the authors did not provide theoretical analysis about the spatial-temporal complexity of FRU. I hope the authors append such analysis to make the efficiency of their proposal in terms of computing resource more persuasive.\n\n2. In terms of experiment, the authors only presented results in score prediction and computational resource cost. However, as the goal of the KT task is not only to predict students\u2019 score sequences, but also to track the dynamic change of their knowledge states. Therefore, it will be helpful if the authors append such experiments (e.g., case study and visualization of student knowledge states) and use them to explain how their proposal can model student knowledge states better"
            },
            "questions": {
                "value": "1. Can you further explain the design and workflow of the FRU framework? For example, what is the connection between FRU and human cognitive development models, what are inputs, what are learnable parameters and what are outputs? Besides, what does $I_\\alpha$ mean in Section 3.4? I cannot find it on Figure 3.\n\n2. In terms of sequence modeling, RNN and LSTM are also simple but effective. What are the advantages of FRU compared to them, especially in the context of knowledge tracing? Does your proposed FRU have the potential to be applied to other areas except for knowledge tracing (e.g., sequential recommendation)?\n\n3. There are some syntax and spelling errors need to be solved, such as the index \u201cI\u201d in the formula of loss function, $Loss_{KT}$. I guess it should be replaced with $t$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2470/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698556114220,
        "cdate": 1698556114220,
        "tmdate": 1699636183438,
        "mdate": 1699636183438,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uqXzUeTkHa",
        "forum": "vZEgj0clDp",
        "replyto": "vZEgj0clDp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2470/Reviewer_UX1Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2470/Reviewer_UX1Z"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel approach to Knowledge Tracing (KT) using the Forget-Response-Update (FRU) framework. KT is essential in online education systems for assessing and predicting student performance based on their interactions with educational content. \n\nThe FRU framework, designed based on human cognitive development models, stands out due to its lightweight nature, consisting of just two linear regression units. The proposed model, named ReKT, was extensively compared with 22 state-of-the-art KT models across 7 public datasets. Results demonstrated that ReKT consistently outperformed other methods, especially in question-based KT tasks. In concept-based KT tasks, an adapted version of ReKT, termed ReKT-concept, achieved top or near-top performance across datasets. \n\nFurthermore, despite its simplicity, the FRU framework required only about 38% of the computing resources of other architectures like Transformers or LSTMs, showcasing its efficiency. The paper underscores the effectiveness, scalability, and efficiency of the FRU design in the realm of Knowledge Tracing."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The introduction of the Forget-Response-Update (FRU) framework offers a fresh perspective in the realm of Knowledge Tracing. While many models in the literature focus on complex architectures, the FRU's simplicity, relying on just two linear regression units, stands out as a unique proposition. The research brings a blend of cognitive learning principles and machine learning, fostering a more holistic approach to Knowledge Tracing.\n\nThe empirical evaluation of the proposed ReKT model is thorough. By benchmarking against 22 state-of-the-art KT models across 7 public datasets, the authors ensure a comprehensive assessment of their model's performance. The paper's methodological rigor is evident in the detailed descriptions of the FRU framework, the equations used, and the training methodologies employed.\n\nThe paper is well-structured, with distinct sections dedicated to introducing the problem, presenting the methodology, showcasing results, and discussing implications. The inclusion of figures, tables, and illustrative examples enhances the reader's understanding and provides a visual representation of the model's performance and capabilities."
            },
            "weaknesses": {
                "value": "The core of the proposed Forget-Response-Update (FRU) framework seems to be composed of two linear regression units. If this can be easily mirrored or replicated using two multi-layer perceptrons (MLPs), then the novelty of the FRU framework can be challenged. A deeper exploration or comparison with simple neural architectures, like MLPs, would provide clarity on the unique advantages of the FRU.\n\nThe use of terminology like \"Forget\", \"Response\", and \"Update\" in naming the modules of the FRU framework may imply distinct, targeted functionalities. However, in complex learning scenarios, such naming conventions can be misleading. In intricate neural architectures, a module named \"Forget\" might not necessarily perform a straightforward forgetting operation but might instead learn a more nuanced or intermediate representation. Over-reliance on such naming can lead to misconceptions about the actual functions and complexities of the modules, especially for those looking to adapt or build upon the framework.\n\nWhile the lightweight nature of the FRU is emphasized, there's limited exploration on how the FRU can be integrated into or combined with deeper or more complex neural network architectures."
            },
            "questions": {
                "value": "How does the Forget-Response-Update (FRU) framework differ fundamentally from a structure consisting of two multi-layer perceptrons (MLPs)? What advantages does the FRU bring over a simple MLP setup?\n\nGiven the naming conventions like \"Forget\", \"Response\", and \"Update\", can you provide deeper insights into the exact functionalities and representations learned by each module during complex learning schedules?\n\nHow does the FRU framework integrate into more complex neural network architectures? Have there been experiments or considerations in this direction?\n\nThe paper mentioned that the FRU requires only about 38% of the computing resources compared to architectures like Transformers. Could you delve deeper into the parameter distribution within the FRU? Which module (Forget, Response, Update) consumes the most parameters?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2470/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807180101,
        "cdate": 1698807180101,
        "tmdate": 1699636183329,
        "mdate": 1699636183329,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mHK0570CBz",
        "forum": "vZEgj0clDp",
        "replyto": "vZEgj0clDp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2470/Reviewer_5ixm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2470/Reviewer_5ixm"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an improvement of the deep knowledge tracing (DKT) algorithm, ReKT. The authors revisited the DKT algorithm to design it from three perspectives.: 1)question: whether the question was attempted before, 20 concept: performance on questions with similar concepts, and 3) the entire trajectory. \nEmpirical results demonstrate the superior performance of ReKT compared to other variations of DKT."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Superior performance while 38% less resource usage"
            },
            "weaknesses": {
                "value": "1. The paper employs similar approaches to previous DKT methods, such as RAKT [1], AKT [2], and [3]  except for the FRU unit. All of three papers also implemented the FRU unit with exponential time decay as part of the attention mechanism in the transformer architecture. The authors have used MLP units as FRU and concatenated the hidden state to the final representations.\n\n2. The authors did not provide any interpretations of the model's performance---which is very important in educational settings for both students' and teachers' perspectives. From a student's perspective, interpretation can help in recommending learning materials. From a teacher's perspective, it can be helpful to identify which questions or concepts students are struggling with.\n\n\nReferences.\n1. Pandey, S. and Srivastava, J., 2020, October. RKT: relation-aware self-attention for knowledge tracing. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management (pp. 1205-1214).\n2. Ghosh A, Heffernan N, Lan AS. Context-aware attentive knowledge tracing. InProceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining 2020 Aug 23 (pp. 2330-2339).\n3. Farhana E, Rutherford T, Lynch CF. Predictive Student Modelling in an Online Reading Platform. InProceedings of the AAAI Conference on Artificial Intelligence 2022 Jun 28 (Vol. 36, No. 11, pp. 12735-12743)"
            },
            "questions": {
                "value": "The Rasch difficulty is determined from students' question and response binary matrix.\n\nAs the authors have three different representations of question interactions, did the authors compute the Rasch difficulty from three different interaction matrices?\n\nHow did the authors handle multiple submissions for computing the Rash difficulty?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2470/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2470/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2470/Reviewer_5ixm"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2470/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698875264400,
        "cdate": 1698875264400,
        "tmdate": 1699636183237,
        "mdate": 1699636183237,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wkb7OTek5c",
        "forum": "vZEgj0clDp",
        "replyto": "vZEgj0clDp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2470/Reviewer_c4BU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2470/Reviewer_c4BU"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a simple yet power knowledge tracing (KT) model called ReKT. The method consists of 1) three levels of knowledge state modeling including question-, concept-, and domain-level, and 2) a forget-response update (FRU) unit. Extensive experiments show that ReKT achieves state of the art KT performance on an array of datasets comparing many baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method is claimed to be simple yet powerful.\n- the evaluation appears to be comprehensive."
            },
            "weaknesses": {
                "value": "1. I am not convinced that the FRU gate is \"simple\". It appears to me as a variant of the gated recurrent unit (GRU) without the reset gate. Compared to the GRU, FRU has a similar forget gate and the hyperbolic tangent function in the end (without the affine combination of the update gate). I think the FRU architecture design, though interesting, does not qualify it as \"very lightweight, as it consists of only two linear regression units\". Otherwise, I can make the same \"very lightweight\" statement for GRU, which only consists of three linear regression units. Why not just use GRU? GRU takes into account not just forgetting (as in FRU), but also remembering/resetting, which might make more sense and have more modeling power? What exactly in the reference article \"Toward a theory of instruction\" do the authors get the inspiration to build FRU? This is an important question that the author should answer because they claim FRU as one of their core contributions, whereas I think FRU is not much different from GRU, which diminishes the value of this contribution. The authors should also cite GRU as important alternative modeling choices to compare to FRU (in addition to LSTM).\n\n2. The proposed approach to represent knowledge at question, concept, and history level is not entirely new; methods such as learning factor analysis (https://link.springer.com/chapter/10.1007/11774303_17), performance factor analysis (https://files.eric.ed.gov/fulltext/ED506305.pdf), additive factor models (http://www.cs.cmu.edu/~ggordon/chi-etal-ifa.pdf, ), knowledge factoring machines (https://arxiv.org/pdf/1811.03388.pdf) also take into account of modeling students' knowledge at concept (sometimes called skills in these literature), question, or entire history levels. In the spirit of \"revisiting\", the authors neither mentioned nor compared to these classic knowledge tracing methods.\n\n3. Some of the results need more clarifications. For example, AKT-R (https://arxiv.org/pdf/2007.12324.pdf) can achieve an AUC of __0.8346__ on ASSIST09 (see Table 5 in the AKT paper), beating the AUC of 0.7917 by the proposed method. Several other baselines in the AKT paper also achieve AUC > 0.8."
            },
            "questions": {
                "value": "1. How is FRU different from GRU? What motivate the differences?\n2. How is the proposed method contexualized within, and compared to, some classic literature such as LFA, PFA, AFM, KFM, and others?\n3. Why are some results different (sometimes by a large margin) to existing published results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2470/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698995420577,
        "cdate": 1698995420577,
        "tmdate": 1699636183170,
        "mdate": 1699636183170,
        "license": "CC BY 4.0",
        "version": 2
    }
]