[
    {
        "id": "88bkuxnlpX",
        "forum": "X1p0eNzTGH",
        "replyto": "X1p0eNzTGH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5478/Reviewer_zMV7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5478/Reviewer_zMV7"
        ],
        "content": {
            "summary": {
                "value": "This paper considers auto-curriculum learning over the minigrid testbed and aims to train a visual policy that can generalize across various generated minigrid levels in a zero-shot manner. The paper first demonstrates the insights that the mutual information between representations and the training level. Then the paper further develops an auto-curriculum algorithm that leverages a smart sampling strategy using a pretrained VAE sampler. By combining both parts, the experiment results suggest strong zero-shot visual generalization can be achieved."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. I really **appreciate the discussions in Section 4** on the connection of generalization capability and the mutual information between the representation and level identity, which is insightful. The discussion makes the proposed MI criterion natural and intuitive. Although the paper can be much stronger by presenting proof rather than simply stating it as a hypothesis. \n\n2. **The experiment results look strong**. I also appreciate the discussion on the importance of VAE, which should be crucial intuitively considering the testbed is minigrid. Btw, I personally guess that VAE might be unnecessary if you adopt another testbed with a goal-conditioned flavor (e.g., those environments where you create a new instance by setting a new goal)."
            },
            "weaknesses": {
                "value": "1. **The presentation can be improved**. There are many notations that are introduced without definitions. For example, in Theorem 3.1 you didn't introduce $L$ (although I can understand its meaning after reading the whole paper). In the paragraph after equation (3), $\\hat{V}_t$ is not defined either. The authors seem to have a strong intention to pack a lot of knowledge in Section 3, ranging from notation to existing theorems and algorithms. Section 3 looks a bit messy to me and hard to follow if the reader is not an expert who _masters_ all the related works. I think the section can be much better organized and self-contained. You may want to have some sub-sections with some high-level mathematical descriptions of previous works (in addition to the related work section). \n\n2. **citation issues:** Most related works cited in this paper are within the past 3 years. I think the authors ignore a large portion of works in curriculum learning literature, such as those working on goal generation, open-ended learning, and multi-task learning. Although these works do not work on the visual minigrid test, they do share a similar high-level principle to this work and should be acknowledged. For example, the paper states \"_we find that strategies de-prioritising levels with low value loss, as **first** proposed in prioritised level replay_\". Well, I have to say in curriculum learning, many works have leveraged the idea of using value function as an indicator for prioritization. [Here](https://proceedings.neurips.cc/paper/2020/file/566f0ea4f6c2e947f36795c8f58ba901-Paper.pdf) is an example. I think you can do a brief survey to get more. \n\n2. **minor issues:** Fig 2 is derived from the results of a non-recurrent policy, as stated in the paragraph below equation (7). Why not use an LSTM, as what you have done in the experiments?"
            },
            "questions": {
                "value": "Although I personally keep a positive perspective on this paper, I would still expect the authors to update the paper for an improved presentation, which can make my judgment firm. \n\nIt would be also great if the authors could further provide more analysis or even theoretical analysis of the hypothesis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5478/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697720931083,
        "cdate": 1697720931083,
        "tmdate": 1699636559091,
        "mdate": 1699636559091,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RazJiA5j1C",
        "forum": "X1p0eNzTGH",
        "replyto": "X1p0eNzTGH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5478/Reviewer_mvxM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5478/Reviewer_mvxM"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to connect a working technique in active domain randomization with a bound on mutual information between histories and contexts/levels. The authors also introduce a method (SSED) to sample more contexts from a similar distribution than the training set by training a VAE on the context-parameters. Both claims are evaluated on maze navigation tasks, which have been modified such that the optimal policy on the training set is not the same as on the entire parameter set. Results show that SSED improves the performance while reducing the optimality gap slightly (but significantly)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper introduces a, to the best knowledge of this reviewer, novel concept of over-generalization to the space of all possible parameters. The proposed method SSED is not terribly novel, but makes sense and shows the effect nicely. The open sourced maze environment is another good contribution of the paper."
            },
            "weaknesses": {
                "value": "**TLDR:** the paper is interesting, but in a bad state. On the one hand, it is very confusingly written, overpromises and does not do all it claims. The first contribution is also dubious: the derivation is incomplete (many terms just appear without definition), seem to rely too much on intuition, and the results do not show the effect. On the other hand, the second contribution (SSED) is simple but interesting, and the results demonstrate that it works. Nonetheless, the reviewer cannot recommend to accept the paper in its current form.\n\n1. The paper overpromises many things, for example from the abstract:\n\t- \"As a first step, we measure the mutual information (MI) between the agent\u2019s internal representation and the set of training levels, which we find to be well-correlated to instance overfitting\": there is no empirical measurement of MI or its correlation with overfitting in the paper. All evidence is circumstantial and the interpretation of Figure 2, in this reviewer's opinion, wrong.\n\t- \"adaptive sampling strategies prioritising levels based on their value loss are more effective at maintaining lower MI\", \"We then turn our attention to unsupervised environment design (UED) methods, which [..] minimise MI more effectively than methods sampling from a fixed set.\", \"SSED generates levels using a variational autoencoder, effectively reducing MI\": again, MI is never measured and the claim that the evaluated methods minimize MI is pure conjecture.\n\n2. The paper is confusingly written and introduces many terms without defining them. For example, $\\text{MI}(i,\\pi)$ is never defined in the main paper. $S^V_i$ is defined with $\\hat V_t$ and $V_t$, which have never been defined. On Page 5 the authors state $V_i^\\pi(s) = V^\\pi(s) + v_i^\\pi(s)$, but $V^\\pi(s)$ has never been defined (only for a general POMDP, the CMDP only knows context-specific values called $V_{i_x}^\\pi$). Sometimes the used PPO baseline uses non-recurrent intermediate representations (p.5), another time the same implementation uses an LSTM-based architecture (which is recurrent, p.7). The reviewer has the impression that many of these details could be understand upon reading the cited papers, but as is, the paper does a poor job explaining them.\n\n3. The connection between generalization, MI and scoring the value loss ($S^V$) is indirect, and not rigorous enough. The connection between MI and generalization is not very clear: in Figure 1 both following the black and the green cells leads to the goal in (a) and (b), but only one of the two generalizes to (c). Both have thus the same MI, but different generalization. While the statement \"An agent learning level-specific policies implies high MI between its internal representation and the level identities\" (p.1-2) makes generally sense, the conclusion that agents with high MI \"will not transfer zero-shot to new levels\" is less clear. Moreover, it is not apparent why the negation *agents with low MI generalize well*, which seem to be the basis for this paper, should be true. So the only clear connection seems to be the bound in (eq.2). The reviewer also missed a connection why \"Sampling levels associated with highly negative classifier cross-entropies therefore results in less mutual information\" (p.5). Finally, the idea that $V_i^\\pi(s) = V^\\pi(s) + v_i^\\pi(s)$ separates into two errors and the second is somehow connected to MI (without any clarification how exactly) is flawed, as the errors of $V^\\pi$ and $v_i^\\pi$ could also cancel each other out, yielding low error, but high MI.\n\n4. The connection of MI to generalization and to the value-error scoring is not supported by the presented data in Figure 2. Here the first two rows are not significantly different, which would lead to the conclusion that using $S_2=S^{MI}$ does *not* change the algorithm's behavior much. How does this justify the statement \"$S = S^V$ therefore appears to strike a good balance between sample efficiency and mutual information minimisation\", if MI is not necessary for performance. The third row shows $S^{MI}$ having a smaller generalization gap, but the performance is also much smaller, and these two metrics are highly correlated! The results can therefore also be interpreted as \"using MI reduces the performance, and *as a consequence* has a smaller generalization gap\". For the same reason the second plot to the right is useless without knowing the algorithms' performances. \n\n5. The entire Section 4 should be either removed or replaced by an experiment that actually links generalization (not just the generalization gap) to MI, demonstrates or proves that sampling levels with high MI *reduces* the overall MI, and shows a correlation between MI and the value-error score."
            },
            "questions": {
                "value": "- The approximation in Lemma 4.1 only works if the levels are constantly sampled with $p(i)$, but your method aims to change this distribution. How does this work when your scoring selects the levels that make up the batch $B$? \n- Your goal is to produce policies that have low MI on the training set, but approximated MI is not used to change the policy, only the sampling of levels. Is there a reason why you do not simply reward actions with lower MI?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5478/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772682347,
        "cdate": 1698772682347,
        "tmdate": 1699636558988,
        "mdate": 1699636558988,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YTzcwcVRYE",
        "forum": "X1p0eNzTGH",
        "replyto": "X1p0eNzTGH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5478/Reviewer_GmmJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5478/Reviewer_GmmJ"
        ],
        "content": {
            "summary": {
                "value": "One of the fundamental problems in reinforcement learning is generalization of the learned policies to new environments. One solution approach is to use an adaptive sampling strategy over a wide range of environments. However, the level of sampling required to achieve the desired generalization remains unknown. The authors propose a new theoretical framework to answer this question using mutual information and minimization of an upper bound on the generalization error from adaptive sampling. Once this relation is established, now, the authors study the problem of creating new environments systematically and improve generalization. Specifically, the authors propose a self-supervised environment design (SSED) to minimize mutual information for zero-shot generalization. The authors provide a theoretical bound and proof for the generalization gap using mutual information with training level and reinforcement learning policy. Then, use level scores from rollout trajectories to define adaptive sampling distribution. \n\nSSED consists of two components: a generative phase, in which a variational autoencoder (VAE) is employed as a generative model and a replay phase, in which we use an adaptive distribution to sample levels. The algorithm alternates between the generative and replay phases, and only perform gradient updates on the agent during the replay phase, while the VAE weights remain fixed throughout training. The authors use a complex environment benchmark, ProcGen from OpenAI, and Minigrid, a gridworld navigation domain (ChevalierBoisvert et al., 2018) for empirical evaluation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality & Significance. In some sense, it feels like a no-brainer to use variability in the new environment setup and maximize diversity (minimize mutual information) for better generalization. Similar to classical system identification methods for control systems. I believe the originality comes off from measuring the diversity in the environment to quantify generalization, instead of randomly exploring over a large set of environments. \n\nQuality & Clarity. The paper is well-written. And the explanations are clear. There are not many grammatical errors. \n\nThe authors baseline their approach to other state-of-the-art approaches."
            },
            "weaknesses": {
                "value": "The authors originally use ProcGen to describe some of the concepts but later on all the empirical experiments are in Minigid. While Minigrid is a good toy problem to start with, it lacks the complexity of the most real-world environments where the generalization is the most important. Having to include ProcGen examples would have been a good mid-step towards addressing real-world challenges in generalization to new environments."
            },
            "questions": {
                "value": "I was surprised that while the authors started out with ProcGen, then switched to Minigrid. While the Minigrid provides a good framework as a starting point to showcase the generalization issue, I believe it lacks many of the real-world complexities for generalizing the RL policies to new environments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5478/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5478/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5478/Reviewer_GmmJ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5478/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784009289,
        "cdate": 1698784009289,
        "tmdate": 1699636558878,
        "mdate": 1699636558878,
        "license": "CC BY 4.0",
        "version": 2
    }
]