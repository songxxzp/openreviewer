[
    {
        "id": "dtk4qvrENP",
        "forum": "IsGsv8qEHp",
        "replyto": "IsGsv8qEHp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7023/Reviewer_kBXX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7023/Reviewer_kBXX"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a human-centric multi-task decoder that extracts varied task information from Ego4D videos. This learnt representation is subsequently utilized to refine pre-trained visual encoders. The results demonstrate that their novel task fusion decoder consistently enhances the performance of three leading visual encoders, namely R3M, MVP, and EgoVLP, for subsequent manipulation policy learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper presents an innovative approach to maximize the extraction of hidden information from human tasks in Ego4D videos. This information is then employed as an initialization for refining premier pre-trained encoders for robotic manipulation. The study offers a comprehensive set of cross-simulator experiments and real-world domain tests, underscoring the importance of these human-oriented features from Ego4D for pre-training robotic encoders."
            },
            "weaknesses": {
                "value": "1. Recent progress in VLM has showcased vast capabilities in reasoning about state-changes, object detection, and identifying irreversible states. Could these representations or features from a pre-trained VLM be harnessed to refine R3M, MVP, or EgoVLP, rather than the method proposed?\n  \n2. An observation suggests that tasks across various environments and models tend to underperform in toggle on/off tasks. Is there an underlying reason for this trend?\n\n3. The ablation studies indicate that both spatial and temporal tasks, when treated independently, don't yield impressive results. However, their combination surpasses the baseline. Can the authors provide an intuition behind this phenomenon?\n\n4. Figure 5 could benefit from a reduction in font size for the percentage scores. In Figure 6, it might be more efficient to omit repetitive legends across plots.\n\n5. The authors might consider leveraging other task or robot demonstration datasets featuring a third-person perspective with actual robot embodiment beyond Ego4D. A few notable examples include:\n    - Padalkar et al. \"Open X-Embodiment: Robotic learning datasets and RT-X models.\" arXiv:2310.08864 (2023).\n    - Kumar et al. \"RoboHive: A Unified Framework for Robot Learning.\" arXiv:2310.06828 (2023).\n    - Duan et al. \"AR2-D2: Training a Robot Without a Robot.\" arXiv:2306.13818 (2023).\n    - Chen et al. \"Learning generalizable robotic reward functions from 'in-the-wild' human videos.\" arXiv:2103.16817 (2021).\n\n6. Have the authors evaluated their method on tasks with longer horizons or those more intricate than mere simple action primitives?"
            },
            "questions": {
                "value": "All my questions and doubts are listed in the weakness section, i hope the author could address them."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7023/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7023/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7023/Reviewer_kBXX"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7023/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698466788851,
        "cdate": 1698466788851,
        "tmdate": 1699636823459,
        "mdate": 1699636823459,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zONDiGe8gU",
        "forum": "IsGsv8qEHp",
        "replyto": "IsGsv8qEHp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7023/Reviewer_UxCD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7023/Reviewer_UxCD"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a task-fusion decoder architecture that fine-tunes representations for robot manipulation tasks using ego-centric video and semantic information about tasks and task stages present in these. The key idea is to us supervision around object state changes (when and if this occurred) and hand/object presence to improve representations, with the hypothesis that representations learned by mastering multiple skills allow for improved robot manipulation. Experiments are conducted to fine-tune a number of back-bone models using the Ego4D dataset, and then measuring their behaviour cloning task success performance on a number of simulated benchmarks )Kitchen, MetaWorld, Adroit) in addition to a real world setting with a Fanuc arm. Results appear to show improved success rates across most tasks evaluated, and ablations indicate that both spatial (object localisation) and temporal information (when state changes occur) contribute to this effect. In general the idea is clear, although the paper lacks some clarity around the methodology that inhibits the core ideas getting across."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The proposed approach looks to be very flexible, and easily added to existing backbones. \n* The proposed approach introduces a useful means of exploiting more readily available data of humans performing tasks in robot manipulation tasks where data is typically not as easily obtained, without the need for direct correspondences between human and robot actions or behaviours."
            },
            "weaknesses": {
                "value": "My primary concerns with the paper are around clarity, and the motivation for the choice of auxiliary task decoder objectives.\n\nMajor:\n\nAs I understand it, the proposed approach learns to decode 3 aspects related to embodied tasks from video/image representations, object state change classification, point of no return temporal localisation and state change object detection, using self attention to learn relationships between tasks, and cross attention to decode tasks. However, the paper does not have a clear definition of these different objectives, and I found this relatively hazy throughout. I think the work would be much clearer if this was defined much earlier (eg. pg 4). It may also be worth rethinking the acronyms or descriptions used here, since these terms (OSCC,PNR,SCOD) do not seem particularly explanatory, and raise more questions to the reader. For example, it is not apparent to me what the difference between state change object recognition and object state change classification is, or what the point of no return is. These need to be more crisply defined and explained earlier. The text notes that these are defined in Fig 3, but fig 3 contains no definition, just a set of loss functions and model visualisations. The associated descriptions on page 5 are also confusing to me, and raise more questions (see below) than answers.\n\nFig 2 shows a nice depiction of the decoder, and has helpful labelling to link to equations. It would be helpful for the global and temporal features $(h_*)$ to be represented on here too, and for a functional description of these operations to be provided. As it stands, I am unclear whether the + in the figure denotes concatentation, attention or something else. The appendix figures seem to indicate a transformer look-up, but this would be a lot clearer if these figures were more explicitly mapped to the nomenclature in the text.\n\nIt is unclear to me why the particular tasks to be decoded were selected, and why this one should be used over various other options or formulations exploiting the human data. The task losses seem somewhat arbitrary to me. This is motivated from a cognitive science perspective (Kirkham et al., 2002), but this aspect could be strengthened to make this choice seem less heuristic.\n\nResults seem to be strong, but no indication as to the significance of these is provided (eg. error bars etc.), so this is difficult to evaluate.\n\nQualitative results in 5.1 are not very informative (t-sne produces different results when re-run), so unclear if this is just a projection effect. Similarly, attention maps have known issues. I think a stronger piece of evidence would be to decode the robot policies and see if these actually map to reasonable event changes, and object/hand bounding boxes.\n\nMinor:\n\nTraining takes three days on 5 A6000 GPUs - this seems to be a very expensive fine-tuning operation for the performance gains observed. I recognise that the convention is to evaluate these tasks in terms of success rate, but it may be time to think in terms of more than success and evaluate performance as a function of compute."
            },
            "questions": {
                "value": "The decoder outputs information about state changes. Could you define what constitutes a state change, or what the state is in these tasks? This knowledge or choice seems to explicitly impose structure on the type of control task being considered.\n\nThe task fusion decoder uses 10 tokens? Why was this number chosen? \n\nThe point of no return is defined as the task to localise the keyframe with state change in the video clip. Where does the \"no-return\" terminology come from and what does this mean?\n\nCan you clarify what the distributions are in the KL loss function in (4)?  These aren't categorical, since I assume there can be multiple state changes in a sequence... How is this loss actually computed?\n\nWhy are task 1 and task 2 different? It seems to me that predicting when a state change occurs already includes the task of predicting if a state change occurred.\n\nThe SCOD task is defined as localisation the hand object positions during the interaction process, which seems to apply continuously, but the terminology state change object detection implies this is only when a state change occurs. Which is is?\n\nI appreciate the novelty in the proposed task decoding approach, but is it an overreach to claim that this paper represents a *paradigm shift* in representation learning? The idea of learning representations by exploiting human data is relatively well established eg. Sermanet et al. Time-Contrastive Networks: Self-Supervised Learning from Video 2018.\n\nTable 1. Please provide more detail on this - are success rates averaged over multiple repetitions (if so, how many?) Are there error bars, are these results significant?\n\nWill the dataset in Fig 5. be released fro reproducibility?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7023/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698639053157,
        "cdate": 1698639053157,
        "tmdate": 1699636823342,
        "mdate": 1699636823342,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rZ0a0t7qmy",
        "forum": "IsGsv8qEHp",
        "replyto": "IsGsv8qEHp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7023/Reviewer_hzuR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7023/Reviewer_hzuR"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a multi-task learning framework for fine-tuning pre-trained visual deep representations on data relating to human-object interaction from the Ego4D dataset. The tasks used for fine-tuning are object state change classification (OSCC), point-of-no-return temporal localization (PNR), and state change object detection (SCOD). OSCC is performed based on entire video clips, while PNR detects at which particular frame in the video the state change happened, and SCOD applies to each frame of the video to localize the hand and objects that the hand is interacting with. The authors fine-tune a transformer which maps from the videos to the task outputs on top of three pre-trained, large visual models (R3M, MVP and EgoVLP). They show that the multi-task fine-tuning generally improves downstream success rates for agents interacting with three environments: MetaWorld, Adroit, Kitchen, and a selection of real-world robot tasks like opening a drawer or pushing a cube."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper tackles an important problem: how to adapt large pre-trained models to be useful for downstream tasks such as robotics. This is an increasingly important area as foundation models become more common, but such models can be too expensive to train for many research groups. Therefore, having methods that successfully adapt these models with less computationally expensive training is important. The main finding, that fine-tuning video models on specific tasks relating to hand / object interactions, is surprising given that this is the same data that the video models were trained on, just with a different objective. I believe the community will find this interesting.\n\nThe paper is thorough in its evaluation of the multi-task transformer (referred to as the \u201ctask fusion decoder\u201d) fine-tuning. The paper covers three different pre-trained vision models as backbones, and three different downstream robotics environments for testing. While the fine-tuning does not universally help the performance in downstream tasks, it does improve the performance on average, and in some cases this improvement can be very significant (up to ~25% in success rate).\n\nThe paper is clearly written and easy to follow. Figures 2 and 3 are especially well constructed. They depict the model with sufficient clarity that it is easy to understand both the basic structure of the transformer, as well as the specific loss functions for each of the different tasks considered. The tables are well constructed, with helpful color coding to determine where the fine-tuning helps and where it does not. The methodology was clearly explained for how downstream tasks were evaluated. My only high-level figure recommendation would be to remove Figure 5, as the paper does not evaluate across all the tasks in the FANUC dataset, and so understanding the variety of actions within this dataset is not particularly important for the reader. It could easily be moved to the supplement."
            },
            "weaknesses": {
                "value": "There are two weaknesses to the paper that I believe can be addressed in rebuttals.\n\nFirst, while the body of the paper is well written and easy to follow, the introduction is inappropriate given the results shown. The claimed message, that the presented fine-tuning represents a method for human alignment without requiring human labels, is not supported by the paper\u2019s results and experiments. What the paper shows is that fine-tuning video models on hand-object interaction tasks improves the representation for robotics. This is absolutely not the same as having a scalable proxy for human evaluation. To make that claim, one would need to run a study comparing representations fine-tuned with human feedback to those fine-tuned with these hand-object interaction tasks.\n\nInstead, I think what will interest the broader community is the fact that hand-object interactions are an exceptionally good choice of task for fine-tuning video models *even when exactly the same input data is used*. I was surprised that the base video models were also trained on Ego4D. Since the fine-tuning is also done on Ego4D, this means that the only difference is using the hand-object task for fine-tuning. But this simple change results in dramatic improvements on downstream success! This is critical for the community to know, since there are many possible tasks that could be chosen for fine-tuning.\n\nTo further underscore this point, another ablation that separately removes each of the different task heads would be very valuable. While there are space vs. time ablations in the current paper, there are three different task heads, and it would be nice to see the effect of each one on the downstream performance. Similarly, the loss uses values of \u201csigma\u201d for each task contribution, and it would be great to see what the final learned values of these sigmas are (since that will be a proxy for how much the model is using the different task outputs for fine-tuning).\n\nThis is the main, original point of the paper. The presented task fusion decoder is not particularly original, there are many multi-task learning architectures involving transformers with different task heads (e.g. Bhattacharjee et al, CVPR 2022, MulT: An End-to-End Multitask Learning Transformer). Since this particular architectural choice is not the main message of the paper, it is not necessary to include comparisons to such architectures.\n\nSecond, and related to the point above, the paper is missing a key piece of related work for the main claim that hand-object interactions are an excellent task for fine-tuning models. The paper is Bahl et al, CVPR:2023, Affordances from Human Videos as a Versatile Representation for Robotics. Rather than fine-tuning a video model, Bahl et al train a model to predict hand-object interaction locations directly, and find that this supports many downstream robotics tasks. It would be valuable to know why one would prefer a fine-tuned video model on these interaction tasks, as opposed to directly predicting them from the outset."
            },
            "questions": {
                "value": "* Why would we expect fine-tuning on the task not to improve the performance on the task? (In relation to section 4.5)\n* What about ablating each loss type separately?\n* \u201cAlso, different vision tasks should have information interaction, for the human-like synesthesia.\u201d\n  * This does not really add anything. What is human-like synesthesia?\n* For the real world experiments: how many test interactions were performed?\n* Can the authors commit to open-sourcing the real-world dataset? \n* \u201cThis is analogous to humans who, in specific environments, focus solely on one particular perception.\u201d\n  * Please remove this. Humans do not, in specific environments, focus solely on one particular perception. If they do, I would like to see the citation backing this up.\n* Learned sigmas: what are they after training? When you say they are learned, is that per scene, or just differentiable but global?\n* Please provide ablations for each individual task head"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7023/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698777351139,
        "cdate": 1698777351139,
        "tmdate": 1699636823222,
        "mdate": 1699636823222,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lDoqIPnx1v",
        "forum": "IsGsv8qEHp",
        "replyto": "IsGsv8qEHp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7023/Reviewer_uRTQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7023/Reviewer_uRTQ"
        ],
        "content": {
            "summary": {
                "value": "A common visual representation for diverse manipulation tasks can be trained by fine-tuning a pre-trained visual representation on a small set of well-chosen, surrogate perceptual tasks that are related to or part of general manipulation tasks. Motivated by this intuition, this work proposes the Task Fusion Decoder (TFD), a transformer-style decoder that can be grafted on top of pre-trained vision encoders. The resulting system is trained, in a supervised fashion, (apparently) end-to-end on three different surrogate tasks involving classification, localization and detection of crucial state changes in egocentric videos of human manipulations (Ego4D)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This work addresses the important question of how to meaningfully learn general-purpose visual representations for manual interaction. A central idea is to leave behind conventional self-supervised training and leverage human examples of manipulation, and train in a supervised fashion on state transitions, which are arguably crucial aspects of any manipulation. The system leverages an existing, labeled dataset (Ego4D), but, thanks to training on surrogate tasks, generalizes to a broad class of manipulation tasks. The concept is not tied to any particular vision encoder; the paper showcases three different instances. The results indicate a clear performance increase in most cases compared to the original, pre-trained encoder."
            },
            "weaknesses": {
                "value": "The intuition of the paper and the grand lines are largely clear but I have spent considerable time trying to understand crucial details. The following statements describe my conclusions, referring to R3M, MVP, and EgoVLP as three possible \"backbones\" as the paper does:\n- The encoder part of a pre-trained backbone is connected as input to the TFD.\n- For training, the system is trained end-to-end on each of the three surrogate tasks simultaneously, training the weights of the TFD from scratch while fine-tuning the encoder in the process.\n- For testing, the fine-tuned encoder is connected to its conventional downstream network (as applicable). The resulting network is then trained (keeping the encoder frozen?) and evaluated in its conventional fashion on manipulation benchmarks. The TFD has no role.\n\nI am not entirely sure that my above understanding is correct. To arrive there, Appendix A.1 was essential reading. The number one improvement this paper should undergo is to clarify (in the main body) how exactly the backbone is connected to the TFD, how exactly the \"fine-tuning\" is done, and how the benchmark networks are set up, trained, and tested.\n\nAnother weakness is the evaluation. The TFD comprises a lot of computational machinery and $K$ stacked blocks (where $K$ is left undocumented) and involves many design choices, but the ablation study merely looks at the importance of the \"time-related\" and \"spatial-related\" tasks - and leaves the reader guessing which exactly these are, and what exactly the numbers in Table\u00a03 mean. Even so, the results are not overly convincing, as one of the five environments performs worse under the TFD, and a second does not benefit. In contrast, it would be interesting to know the impact of the two attention mechanisms (as these implement the core motivation of this work) and how many of these blocks are required.\n\nThe paper tends to oversell a bit. As far as I can see, leveraging surrogate tasks in this manner is a novel idea but I'd hesitate to call it a \"paradigm shift\". It is said to \"consistently\" improve performance over the raw backbones, but the results are in fact mixed; on some problems performance actually decreases.\n\nFigures 2, 3, 9 and 10 are crucial for understanding the paper but leave room for improvement. It is not obvious how the various inputs and outputs connect; the notation is not consistent everywhere. For example, is $j=10$ in Figure\u00a02? Figures 2, 9 and 10 unnecessarily use different visual styles, making it difficult to see which differences are important and which are not. Figure\u00a02 would probably be easier to understand if there was just one TFD block instead of the abstraction in the middle and the zoomed view on the right."
            },
            "questions": {
                "value": "- If it is true that the TFD has no role after training (\"fine-tuning\"), I'm not convinced that this is the best possible setup. The idea is to force the latent representation formed by the backbone encoder to generalize across tasks, but it is not clear how much of this generalization power lies within the considerable computational machinery of the TFD itself and is thus no longer available after fine-tuning. I'd like the authors to comment on this.\n- The wording in Sec.\u00a03.2 suggests there is exactly one state change frame. Why can't there be more than one?\n- My two main concerns are lack of clarity and inconclusive results. At least the former can perhaps be fixed; I can perhaps be convinced to upgrade my ratings."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7023/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778650937,
        "cdate": 1698778650937,
        "tmdate": 1699636823102,
        "mdate": 1699636823102,
        "license": "CC BY 4.0",
        "version": 2
    }
]