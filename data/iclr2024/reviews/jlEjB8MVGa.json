[
    {
        "id": "2KOOKp43sz",
        "forum": "jlEjB8MVGa",
        "replyto": "jlEjB8MVGa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1419/Reviewer_ZN1X"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1419/Reviewer_ZN1X"
        ],
        "content": {
            "summary": {
                "value": "This study addresses wild Out-Of Distribution (OOD) detection, benefiting from the availability of more unlabeled data to enhance OOD data identification. The paper proposes the utilization of the top singular value as a criterion to differentiate between In-Distribution (ID) and OOD samples. The authors, grounded in novel theoretical insights, posit that ID samples should exhibit larger top singular values compared to OOD samples. Both experimental results and theoretical analyses corroborate the effectiveness of the proposed method, Separate And Learn (SAL). Overall, this work stands out for its solid foundation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Using gradients to make distinctions is a common and intuitive approach that makes sense to me. How ever, the method of distinguishing based on the top singular value is both intriguing and non-obvious. This innovative discovery holds significant importance for OOD detection.\n\n- The availability of extra unlabeled data undoubtedly enables wild OOD detection to outperform traditional OOD detection, which does not utilize unlabeled data. However, what I find astonishing is that the performance of wild OOD detection surpasses even that of outlier exposure, showcasing its remarkable effectiveness.\n\n- The method is supported by a theoretical examination of its generalization bounds, ensuring a solid foundation for reliable ML."
            },
            "weaknesses": {
                "value": "- One of my major concerns with the paper lies in the discrepancy assumption stated in Theorem 1. To validate this, additional experiments with diverse datasets are imperative to ascertain whether the assumption is consistently met in practical scenarios. While I acknowledge that assumptions are essential for theoretical foundations, and the experiments in Appendix F provide a good start, they are insufficient for a comprehensive evaluation. I recommend extending the empirical analysis to additional datasets (e.g., CIFAR-10 as ID and another data as OOD), with the goal of thoroughly investigating the generality of the assumption and its ability to underpin SAL robustly.\n\n- In the majority of the experiments conducted, the value of pi is set to 0.1. To ensure a comprehensive evaluation, it is crucial to conduct additional experiments with varied values of pi."
            },
            "questions": {
                "value": "Please address the issues outlined in the weaknesses. This work is solid and good. So addressing all of my concerns comprehensively will lead me to reconsider and potentially raise my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1419/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1419/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1419/Reviewer_ZN1X"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698499784974,
        "cdate": 1698499784974,
        "tmdate": 1699636070329,
        "mdate": 1699636070329,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YPpZnPrDRp",
        "forum": "jlEjB8MVGa",
        "replyto": "jlEjB8MVGa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1419/Reviewer_dSEG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1419/Reviewer_dSEG"
        ],
        "content": {
            "summary": {
                "value": "Leveraging unlabeled data has shown potential in enhancing the safety and reliability of machine learning models for out-of-distribution (OOD) detection, despite the challenges posed by the heterogeneity of both in-distribution (ID) and OOD data. This paper introduces SAL (Separate And Learn), a novel learning framework that addresses the existing gap in understanding how unlabeled data aids OOD detection by providing strong theoretical guarantees and empirical effectiveness. SAL operates by isolating potential outliers from the unlabeled data, training an OOD classifier with these outliers and labeled ID data, and achieving state-of-the-art performance on standard benchmarks, thereby validating the theoretical framework and its components."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The manuscript presents some theoretical analyses as well as a number of intriguing illustrations.\n2. The experimental results look promising, with comparisons made against numerous baseline methods."
            },
            "weaknesses": {
                "value": "1. The manuscript lacks crucial baselines, such as [1] and [2], which are essential for a comprehensive evaluation, and fails to provide an analysis or comparison with them.\n2. The essential topic of the article is weakly supervised out-of-distribution detection, although it is described from different perspectives.\n\n[1] Zhou, Zhi, et al. \"Step: Out-of-distribution detection in the presence of limited in-distribution labeled data.\" Advances in Neural Information Processing Systems 34 (2021): 29168-29180.\n\n[2] He, Rundong, et al. \"Topological structure learning for weakly-supervised out-of-distribution detection.\" arXiv preprint arXiv:2209.07837 (2022)."
            },
            "questions": {
                "value": "1. Near OOD Scenario: It is unclear how effective the method presented in this article would be in a near OOD scenario, such as treating the first 50 classes of CIFAR-100 as ID and the last 50 classes as OOD. This specific situation could pose challenges since near OOD samples may share similarities with the ID data, potentially affecting the method's performance.\n2. Generalization Performance Across Different Backbones: The article does not provide information on how well the method generalizes when different backbone architectures are used. The performance stability of the method when transitioning between various model architectures is a critical aspect to consider for its widespread applicability.\n3. Generalization Performance on unseen OOD data: There is no discussion on the method's effectiveness when the test OOD data and the OOD data in the unlabeled set are not identically distributed. Understanding how the method handles such disparities is crucial for evaluating its robustness in real-world scenarios."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1419/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1419/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1419/Reviewer_dSEG"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698590237525,
        "cdate": 1698590237525,
        "tmdate": 1700043254560,
        "mdate": 1700043254560,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IQbxpbruWt",
        "forum": "jlEjB8MVGa",
        "replyto": "jlEjB8MVGa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1419/Reviewer_XJRS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1419/Reviewer_XJRS"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel setting for OOD detection, termed as \u201dwild OOD detection,\u201d building upon the foundation established by the preceding work \u201dTraining OOD Detectors in their Natural Habitats.\u201d A novel methodology, denoted as SAL, is presented, encapsulating at wo-stage process comprising filtering and classification components. Empirical evaluations have demonstrated that SAL achieves SOTA performance, boasting substantial improvements over existing methods. Additionally, theoretical underpinnings are provided to bolster the credibility and effectiveness of SAL."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. A theory has been established to investigate aspects of separability and learnability. This contribution is both novel and significant. \n\n2. Experimental evaluations conducted on standard benchmarks demonstrate that SAL achieves SOTA performance. \n\n3. A novel method grounded in theory has been developed to advance safe machine learning practices. Theory serves as a crucial driver in this endeavour. I am very happy to see the novel work on provable OOD detection."
            },
            "weaknesses": {
                "value": "1. Could you provide explanations or conduct experiments to elucidate the factors contributing to the decreased ID accuracy depicted in Table 1? \n\n2. Why is pi set to 0.1 in most experiments? Could you conduct additional experiments to investigate whether pi remains robust across a range of values? Furthermore, does the performance of pi align with theoretical predictions? \n\n3. It appears that the top singular vector is crucial for SAL. Have you conducted any experiments to demonstrate the performance when considering the top 2, top 3, ..., top k singular vectors?\n\n4. What would occur if you were to use the gradient norm in place of the top singular vector? Could you elucidate the rationale behind opting for the top singular vector instead of the norm?"
            },
            "questions": {
                "value": "Please refer to the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1419/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1419/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1419/Reviewer_XJRS"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698701001277,
        "cdate": 1698701001277,
        "tmdate": 1700688502186,
        "mdate": 1700688502186,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hnjDxVo5iO",
        "forum": "jlEjB8MVGa",
        "replyto": "jlEjB8MVGa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1419/Reviewer_BtG8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1419/Reviewer_BtG8"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel framework for Out-Of-Distribution (OOD) detection, named SAL, which aims to improve machine learning models through regularization using unlabeled data. SAL comprises two main components: (1) Filtering\u2013distinguishing potential outliers from the general dataset, and (2) Classification utilizing the identified candidate outliers to train an OOD classifier. The paper includes pertinent theoretical proofs to substantiate the proposed method, and experimental results are provided to demonstrate its effectiveness."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1 SAL\u2019s methodology is structured around two distinct phases\u2014screening and classification\u2014which can be independently optimized, offering enhanced flexibility.\n2 Utilizing a Large Volume of Unlabeled Data: SAL effectively leverages substantial amounts of unlabeled data to extract valuable information, thereby bolstering its detection capabilities.\n3 Theoretical Support: Beyond its impressive empirical performance, SAL is underpinned by robust theoretical foundations."
            },
            "weaknesses": {
                "value": "1 In scenarios where the actual OOD data markedly diverges from the outliers present in the unlabeled dataset, there arises a question regarding the preservation of SAL\u2019s performance.\n 2 The efficacy of SAL is significantly influenced by the quality of the unlabeled data employed, indicating a substantial dependence on data integrity."
            },
            "questions": {
                "value": "1 Could you design experiments to further confirm and answer issues in weaknesses?\n2 In table 1, how can this discrepancy be accounted for in datasets or scenarios that perform well in other methods but have degraded performance (ID ACC) in SAL?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698835114810,
        "cdate": 1698835114810,
        "tmdate": 1699636070067,
        "mdate": 1699636070067,
        "license": "CC BY 4.0",
        "version": 2
    }
]