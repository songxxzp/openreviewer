[
    {
        "id": "wjL08wH5tY",
        "forum": "LVp217SAtb",
        "replyto": "LVp217SAtb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3431/Reviewer_UG4J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3431/Reviewer_UG4J"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an adapter-based approach to adapt reinforcement learning agents to new tasks. The method uses a separate adapter module that adjusts the outputs of a base agent using proximal policy optimization. Experiments are conducted on RTS games."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The idea of using adapters for transfer in RL could be beneficial for sample efficiency and overcoming catastrophic forgetting.\n\n- Adapters provide a modular approach for incremental learning without interfering with the base agent.\n\n- Method can work with any base agent including rule-based and neural network agents."
            },
            "weaknesses": {
                "value": "1. The novelty is limited. Adapters have been widely studied for transfer learning in NLP and computer vision. Their application to RL is straightforward with no new techniques proposed.\n2. There is no analysis on the learned adapter parameters to provide insights into how adaptation is occurring. Visualizations or other analysis would strengthen the approach.\n3. The experimental evaluation is weak. Testing is limited to a simple RTS game with no comparisons to other multi-task or transfer RL techniques. More complex domains should be evaluated.\n4. Only win rate and training curves are reported. Standard RL metrics around sample efficiency, reward, etc. should be included. Ablations on adapter design choices are also lacking.\n5. The proposed method relies heavily on the base agent capabilities. Performance when base agent is sub-optimal is not explored.\n6. The temperature coefficient analysis claims a stable range of [1/1000, 1/10] but there is no clear justification for this range.\nThere is no evaluation on real-world robotics tasks. The approach may not transfer from simulation."
            },
            "questions": {
                "value": "1. Have other adapter design choices besides the temperature coefficient been explored?\n2. How well does the method transfer to other RL domains beyond RTS games?\n3. Is the stable temperature coefficient range task-dependent or generalizable?\n4. How does the approach perform when the base agent skills are sub-optimal for the task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698031043062,
        "cdate": 1698031043062,
        "tmdate": 1699636295077,
        "mdate": 1699636295077,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cSYmW8FITd",
        "forum": "LVp217SAtb",
        "replyto": "LVp217SAtb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3431/Reviewer_dR9v"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3431/Reviewer_dR9v"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of adaptation in Reinforcement Learning (RL) and proposes an adaptation strategy to improve training efficiency. This method is compatible with pre-trained neural networks and rule-based agents. The authors provide experiments in the nanoRTS environment to demonstrate the strength of their method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "(1) Interesting topic: Adaptation is an interesting and motivating area for reinforcement learning."
            },
            "weaknesses": {
                "value": "(1) Lack of related works discussion. The authors state that adaptation for RL is a largely unexplored area, while I would recommend doing a further literature review on similar topics. Some examples contain [1, 2, 3]\n\n(2) Insufficient experiments. The provided experiment results are limited. I would recommend doing more experiments with diverse tasks. For example, if the domain is focused on Reinforcement learning, you may try other tasks in the OpenAI gym [4].\n\n(3) Lack of README file to run the code. I appreciate the authors for providing the code. However, it would further benefit the reviewers, and potential users with a README file to guide how to run the code.\n\nReference:\n\n[1] Nagabandi, Anusha, et al. \"Learning to adapt in dynamic, real-world environments through meta-reinforcement learning.\" arXiv preprint arXiv:1803.11347 (2018).\n[2] Arndt, Karol, et al. \"Meta reinforcement learning for sim-to-real domain adaptation.\" 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2020.\n[3] Guez, Arthur, David Silver, and Peter Dayan. \"Efficient Bayes-adaptive reinforcement learning using sample-based search.\" Advances in neural information processing systems 25 (2012).\n[4] Brockman, Greg, et al. \"Openai gym.\" arXiv preprint arXiv:1606.01540 (2016)."
            },
            "questions": {
                "value": "(1) What is the relationship between the proposed method with adaptive RL such as [5, 6, 7]\n\nReference:\n\n[1] Nagabandi, Anusha, et al. \"Learning to adapt in dynamic, real-world environments through meta-reinforcement learning.\" arXiv preprint arXiv:1803.11347 (2018).\n[2] Arndt, Karol, et al. \"Meta reinforcement learning for sim-to-real domain adaptation.\" 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2020.\n[3] Guez, Arthur, David Silver, and Peter Dayan. \"Efficient Bayes-adaptive reinforcement learning using sample-based search.\" Advances in neural information processing systems 25 (2012).\n[4] Brockman, Greg, et al. \"Openai gym.\" arXiv preprint arXiv:1606.01540 (2016).\n[5] Khan, Said G., et al. \"Reinforcement learning and optimal adaptive control: An overview and implementation examples.\" Annual reviews in control 36.1 (2012): 42-59.\n[6] Rigter, Marc, Bruno Lacerda, and Nick Hawes. \"Risk-averse bayes-adaptive reinforcement learning.\" Advances in Neural Information Processing Systems 34 (2021): 1142-1154.\n[7] Eghbal-zadeh, Hamid, Florian Henkel, and Gerhard Widmer. \"Context-adaptive reinforcement learning using unsupervised learning of context variables.\" NeurIPS 2020 Workshop on Pre-registration in Machine Learning. PMLR, 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3431/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3431/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3431/Reviewer_dR9v"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698439225768,
        "cdate": 1698439225768,
        "tmdate": 1699636294990,
        "mdate": 1699636294990,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6yfRyqb5vg",
        "forum": "LVp217SAtb",
        "replyto": "LVp217SAtb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3431/Reviewer_tvEM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3431/Reviewer_tvEM"
        ],
        "content": {
            "summary": {
                "value": "This paper applies an \u201cadapter\u201d module to RL agents."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The method seems reasonable."
            },
            "weaknesses": {
                "value": "* My main concern with this paper, and why I\u2019m voting for rejection, is that it does not have a related work section that compares other adaptation techniques in RL, and does not compare the method introduced with baselines from prior work. It is hard to gauge the merits if this work without proper discussion of related work (specifically, adaptation methods in RL). \n* The text in the plots in Figure 3 is unreadable.\n* Typos:\n    * Top of 2.3 \u201cWhich is a actor-critic paradigm\u201d (PPO is not a paradigm, and should be \u201can\u201d).\n    * Others + clunky wording throughout\u2014please read through the paper with a careful eye."
            },
            "questions": {
                "value": "* None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698696869796,
        "cdate": 1698696869796,
        "tmdate": 1699636294915,
        "mdate": 1699636294915,
        "license": "CC BY 4.0",
        "version": 2
    }
]