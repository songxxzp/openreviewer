[
    {
        "id": "5mohXHBvNM",
        "forum": "uikf2Ue0XQ",
        "replyto": "uikf2Ue0XQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1978/Reviewer_qRXz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1978/Reviewer_qRXz"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on improving Visual Grounding using Transformer-based models. They first analyze the correlation between the model fusion/decoder layer's attention score within the ground truth bounding box and the corresponding IoU value for multiple benchmarks. Based on their observations, they propose two objectives: (1) Rho-modulated Attention Constraint (RAC); and (2) \nMomentum Rectification Constraint (MRC) for attention regularization. RAC constraints the model to generate attention maps focusing on the text included in the bounding box. MRC is a momentum distillation module for rectifying RAC in cases in which background information is needed. They also introduce Difficulty Adaptive Training to make the model pay more attention to hard samples. Finally, they conduct experiments using multiple transformer-based models on several benchmarks including RefCOCO, RefCOCO+, and RefCOCO-g. Their experimental results show better performances than the baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) They design the objectives from the analysis and observations, which makes the design easy to follow and intuitive. \n(2) The proposed RAC, MRC loss, and the DAT training strategy can be adapted to different transformer-based visual grounding models, therefore the methods are general to use in the future. \n(3) They conduct comprehensive ablation experiments to show each proposed element is needed to get the best performances on multiple benchmarks. Also, their main results show consistent improvements over these benchmarks using different backbone models."
            },
            "weaknesses": {
                "value": "(1) The benchmarks used in the paper are RefCOCO, RefCOCO+ and RefCOCO-g. But I see in the supplementary material the authors also try ReferItGame and Flicker30K. Could you also provide additional experimental results on these benchmarks to compare with the baselines?\n(2) By adding the RAC and MRC, how long did you train the model? Can you compare it with the baseline training time as well?"
            },
            "questions": {
                "value": "Please answer the questions in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1978/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698685753388,
        "cdate": 1698685753388,
        "tmdate": 1699636129540,
        "mdate": 1699636129540,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lJt8MlALLV",
        "forum": "uikf2Ue0XQ",
        "replyto": "uikf2Ue0XQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1978/Reviewer_zMiJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1978/Reviewer_zMiJ"
        ],
        "content": {
            "summary": {
                "value": "This study first pinpoints a general problem that the loss function adopted in visual grounding tasks does not differ from that in object detection tasks, failing to consider the alignment between the language expressions and the visual features. To that end, the authors propose an attention-driven constraint balancing (AttBalance) module to explicitly constrain attention to focus more in the range of the bounding box. Experimental results are presented on various datasets using different models to validate the effectiveness of AttBalance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors point out an important problem that loss functions in visual grounding is not specially designed to consider vision-language interactions.\n    \n- An attention balance method is proposed based on the found positive correlation between the attention value inside a bounding box and the model\u2019s performance.\n    \n- The proposed module is able to achieve performance gain across different methods on major visual grounding tasks."
            },
            "weaknesses": {
                "value": "- My major concern is about the intuitive motivation of this study. The authors argue that \u201chigher attention values within the ground truth bounding box (bbox) generally indicate a better overall performance\u201d through two individual experiments. Specifically, a Spearman\u2019s rank correlation between the attention values and the models\u2019s predicted IoU is shown to indicate the positive correlation between the model\u2019s prediction and the attention value. Even though the results using TransVG-R101 generally comply with the assumption, the results using VLTVG-R101 barely do so. The trend on unc testA even contradicts the argument.\n    \n- Moreover, one could list a lot of cases where the attention values inside a bounding box do not necessarily correlate with the models' performance. Take small objects for example, the most influential factors would lie in the background for determining the semantics of the object instead of the attention inside the bounding box. Therefore, the motivation of this study fails to persuade the reviewer of the effectiveness of the proposed module in various scenarios.\n    \n- The introduced regulation on the attention value is to guide the attention to focus more on the area inside a bounding box. Even though the authors assert that the loss could apply to different architectures, more hyperparameters are also introduced which would also hurt its transferability on other visual grounding models. For example, in the sentence \u201cwe partition the attention values within the ground truth region of VLTVG\u2019s last layer into 8 equal number parts, omitting the extreme intervals, i.e. those less than 0.1 or greater than 0.9\u201d, one would identify at least 3 hyperparameters whose ablation is also missing in the manuscript.\n    \n- Some major writing issues. Each figure annotation should explain itself whereas the annotations in this study e.g., Figure 1 and Figure 3 to too short to comprehend without referring to the main manuscript.\n    \n- Minor writing issues, e.g., \u201canalyzing\u201d -> \u201canalyze\u201d in 4.3. The authors are encouraged to further proofread the manuscript."
            },
            "questions": {
                "value": "The major concerns would be how to prove its motivation in general. I would like to also raise some other question regarding the experimental details.\n\n- In regards to the ablation study, it is weird that only applying MRC even causes degradation of the performance. In contrast, it is able to largely benefit the model when combined with RAC. This is an interesting yet strange phenomenon, especially given that the feature map distillation from a moving-averaged teacher has been effective on various self-supervised/semi-supervised task. This is not well addressed in the manuscript with only one sentence \u201c \u201csuggesting that merely smoothing the attention behavior by MomModal does not bring benefits\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1978/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698754701346,
        "cdate": 1698754701346,
        "tmdate": 1699636129457,
        "mdate": 1699636129457,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nZTY0FQFr8",
        "forum": "uikf2Ue0XQ",
        "replyto": "uikf2Ue0XQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1978/Reviewer_L37Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1978/Reviewer_L37Q"
        ],
        "content": {
            "summary": {
                "value": "The paper tackles the problem of visual grounding, aiming to produce relevant object bounding boxes in the image based on free-form text input. The proposed method imposes explicit constraints to ensure that self-attention within the transformer layers are focused on the ground truth bounding box areas during training. Specifically, the authors introduce (1) Rho-modulated Attention Constraint (RAC), a BCE loss that promotes the sum of the attention values within the ground-truth bounding box to be 1 and 0 elsewhere, (2) Momentum Rectification Constraint (MRC) to help the model converge smoothly with RAC, and (3) Difficulty Adaptive Training (DAT) to dynamically change the weight of losses. When combined with existing models. The boost is large for QRNet and TransVG, but marginal for VLTVG."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1. The proposed approach consistently enhances the performance of existing models when integrated. Notably, when paired with QRNet, it outperforms all other methods that are compared in the paper. However, the paper lacks comparison with more recent SOTA methods such as VG-LAW [a].\n\n[a] Language Adaptive Weight Generation for Multi-task Visual Grounding, Su et al., CVPR 2023\n\nS2. The paper includes an ablation study to assess the impact of the various components proposed in the paper. \n\nS3. The paper provides an analysis of correlation between the grounding performance (IoU of the predicted bounding boxes against the ground truth bounding boxes) and the summation of the attention value within the ground truth bounding boxes that motivates the proposed method. \n\nS4. The paper provides qualitative results, showing the attention maps both with and without the proposed method, which I found to be insightful."
            },
            "weaknesses": {
                "value": "W1. The paper assumes that the lack of explicit attention guidance results in suboptimal performance. However, it is difficult for me to buy the assumption, given that the model is trained in an end-to-end manner. Factors like the size and diversity of the training dataset could also be responsible if the attention appears dispersed.\n\nW2. The constraints proposed in the paper involve numerous hyperparameters and heuristics, including adding constraints and subsequently introducing other ones to temper the initial constraints. \n\nW3. The discussion of the prior work that uses object detection losses only, which is one of the motivations of the paper, references papers published over two years ago. It might be worth considering more recent work such as [a] that leverages focal loss and a segmentation loss (DICE loss) similar to Segment Anything Model (SAM) and [b] which uses both object-text and patch-text alignment losses. It might be interesting to compare the attention maps with [a] as well, to validate whether there's a genuine need for explicit guidance on attentions.\n\n[a] Language Adaptive Weight Generation for Multi-task Visual Grounding, Su et al., CVPR 2023\n[b] YORO - Lightweight End to End Visual Grounding, Ho et al., ECCV 2022\n\nW4. The third contribution highlighted in the paper, namely,  \u201c(iii) Our framework can be seamlessly integrated into different transformer-based methods.\u201d is not a valid contribution, but a feature of the proposed framework mentioned in (ii). I suggest merging it with the contribution (ii). \n\nW5. Presentation: Both the main text and the captions omit explanations for legends and notations in the figures and tables (Figure 1, Table 2). The quality of writing could also be further improved. \n\n[Minor comments] \nI suggest revising the abstract for enhanced clarity. \n\nIn \u201cSpecifically, we achieve uniform gains across five different models evaluated on four different benchmarks.\u201d, I suggest \u201cconstantly improves over\u201d rather than \u201cuniform gains\u201d, because the gains are not equal. \n\nAlso, references can be adjusted. Instead of \u201cTransVG Deng et al. (2021)\u201d, consider using \u201cTransVG proposed by Deng et al. (2021)\u201d or  \u201cTransVG (Deng et al., 2021)\u201d."
            },
            "questions": {
                "value": "Q1. The performance improvement is significant for QRNet and TransVG, yet only slight for VLTVG. I'm curious if this variation is mirrored in the attention map. When examining the attention map, similar to what's shown in Figure 4, is the shift in attention for VLTVG more subtle compared to that of QRNet?\n\nQ2. In Section 2\u2019s analysis, I want to double check if the \u201cIOU value\u201d in \u201cThen we record the IoU value of corresponding data points\u201d is the IoU between the object bounding box predictions against the GT bounding boxes. \n\nQ3. In Section 2\u2019s analysis, were the attention values normalized in any way?\n\nQ4. The paper delves into directing visual features towards specific regions (bbox regions) in an image. Instead of their proposed method, why not introduce a learnable 2D regional weighting layer, akin to the approach in [a], to modulate the attention? This would be done as Attention_i = weighted_2D_mask * Attention_i, where the weighted_2D_mask is produced by a shallow convnet that takes a CxWxH feature map and is trained end-to-end without extra supervision. This could serve as another baseline to validate whether there's a need for explicit guidance/supervision on attentions. \n\n[c] Large-Scale Image Retrieval with Attentive Deep Local Features, ICCV 2017"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1978/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1978/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1978/Reviewer_L37Q"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1978/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822278314,
        "cdate": 1698822278314,
        "tmdate": 1699636129345,
        "mdate": 1699636129345,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8tORFRqy6X",
        "forum": "uikf2Ue0XQ",
        "replyto": "uikf2Ue0XQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1978/Reviewer_6bMP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1978/Reviewer_6bMP"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a novel framework named Attention-Driven Constraint Balancing (AttBalance) in Visual Grounding task which analyze the attention mechanisms of transformer-based models, and optimize the behavior of visual features within language-relevant regions. This approach propose a framework to incorporate language-related region guidance for fully optimized training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The article explores balancing the regulation of the attention behavior during training and mitigate the data imbalance problem, The idea of AttBalance is interesting.\n\n(2) Compared with benchmark methods, with the guidance of AttBalance, all transformer-based models consistently obtain an impressive improvement."
            },
            "weaknesses": {
                "value": "1.\tthis paper\u2019s main contribution is attention mechanisms of transformer-based models. While I believe in 2021, there was already a work which put attention transformer in visual grounding, which called Word2Pix, what is the strength of this paper compare to Word2Pix? Some innovative points are needed to demonstrate the superiority of this method in visual grounding.\n2.\tThe related work part can be put in section 2 rather in section 5.\n3.\tAs shown in Table 2, could you explain why the incorporation of only the MRC module results in a decline on average, and when the MRC is used to rectify the RAC can it lead to an increase. Can you provide a more detailed explanation of how MRC affects RAC and lead to an increase?"
            },
            "questions": {
                "value": "Please see the above weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1978/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1978/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1978/Reviewer_6bMP"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1978/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698899180137,
        "cdate": 1698899180137,
        "tmdate": 1700726528072,
        "mdate": 1700726528072,
        "license": "CC BY 4.0",
        "version": 2
    }
]