[
    {
        "id": "h9Un8cYIIy",
        "forum": "89XNDtqhpL",
        "replyto": "89XNDtqhpL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1481/Reviewer_EdnT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1481/Reviewer_EdnT"
        ],
        "content": {
            "summary": {
                "value": "The authors propose techniques for a) training Transformer models such that they have configurable hidden size inside the FFN projection after training and b) mixing/matching hidden sizes across the FFNs in a Transformer model to produce a range of quality / cost tradeoffs for Transformer serving. They evaluate their technique on text and image tasks and demonstrate potential applications like speculative decoding."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper is well written and organized. I found it easy to follow.\n\nThe most compelling part of the paper was the application to speculative decoding, I think. Potential wins from shared parameters, shared attention cache and consistency between the base/full model are very clear."
            },
            "weaknesses": {
                "value": "There are a number of small things I\u2019d encourage the authors to revise to strengthen their paper.\n\n1) Saying that MatFormer is \u201czero additional cost\u201d seems inaccurate given the cost of the additional \u2018g - 1\u2019 forward passes during training. \n\n2) The authors\u2019 don\u2019t explain how the mix\u2019n\u2019match models on the pareto frontier are selected. Given there are many candidates this seems like an important detail.\n\n3) The claim that the FFN is responsible for the largest chunk of latency during inference seems questionable to me. I\u2019d encourage the authors to present a more nuanced perspective on Transformer inference based on previously published data. For example, the study from Kim et. al [1] presents relevant data for CPU serving in Figures 7 and 8.\n\n4) In the speculative decoding experiments, I\u2019d encourage the authors to briefly describe how consistency between the two models can translate into latency reductions. This will make the impact more clear to readers who aren\u2019t familiar with prior work/do not read the reference you direct them to.\n\n[1] https://arxiv.org/abs/2302.14017"
            },
            "questions": {
                "value": "One ablation I would like to see is how a Transformer performs if the 'g' additional forward passes aren't used during training with your Mix'n'Match procedure. Basically, how bad does a normal Transformer perform with this post-processing if it's not trained appropriately?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1481/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698695625847,
        "cdate": 1698695625847,
        "tmdate": 1699636077211,
        "mdate": 1699636077211,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PjswXCTfve",
        "forum": "89XNDtqhpL",
        "replyto": "89XNDtqhpL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1481/Reviewer_J4dY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1481/Reviewer_J4dY"
        ],
        "content": {
            "summary": {
                "value": "The paper presents MatFormer, a model architecture based on the Transformer model. MatFormer employs the concept of matryoshka representation learning to introduce nested sub-structures in feed-forward network (FFN) blocks. The architecture aims to facilitate elastic inference by allowing for the extraction of numerous smaller sub-models without additional training."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper introduces an interesting idea of elastic inference within the Transformer architecture, a concept that can be potentially beneficial for a wide range of deployment scenarios."
            },
            "weaknesses": {
                "value": "1. Motivation and Efficiency: While the paper does address the limitations of current large foundation models, it falls short in clearly explaining the training efficiency gains provided by MatFormer. Specifically, there seems to be a lack of substantial evidence to support the claim that training costs are significantly reduced.\n\n2. Similarity to Existing Techniques: The paper does not sufficiently distinguish MatFormer from existing techniques such as mixture-of-experts and other conditional computation methods. This raises questions about the novelty of the work.\n\n3. Scope of Applicability: The focus on FFNs leaves the attention mechanism of the Transformer model unaddressed. This limitation narrows the effectiveness of MatFormer in improving Transformers comprehensively.\n\n4. Evaluation and Support for Claims: The paper could benefit from a more rigorous evaluation to substantiate its claims. As it stands, the contributions asserted lack sufficient empirical validation.\n\n5. Prior Work Comparison: The work could benefit from a clearer discussion of how MatFormer advances beyond or differentiates from key prior work, specifically Kusupati et al., 2022, in terms of the nested structure."
            },
            "questions": {
                "value": "- The paper seems to use MatLM, MatFormer, and MatViT interchangeably.\n- In Figure 2 and the associated text, what are the differences in model architecture / settings among MatFormer, Mix'n'Match, and baseline?\n- How does the proposed method improve speculative decoding? Section 4.1.1 is missing certain details to help understand this claim."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1481/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1481/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1481/Reviewer_J4dY"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1481/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698711644809,
        "cdate": 1698711644809,
        "tmdate": 1699636077113,
        "mdate": 1699636077113,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "b0LNpTHIMl",
        "forum": "89XNDtqhpL",
        "replyto": "89XNDtqhpL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1481/Reviewer_UoXf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1481/Reviewer_UoXf"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a nested Transformer architecture called MatFormer based on principle of matryoshka representation learning, designed to offer elasticity in a variety of deployment constraints. During training, each feed forward network (FFN) block of a MatFormer model is jointly optimized with a few nested smaller FFN blocks. And this paper propose to jointly optimize all the submodels together by combining their loss together. During elastic inference, MatFormer allows for the Mix\u2019n\u2019Match of model granularities across layers, i.e., a trained universal MatFormer model enables extraction of hundreds of accurate smaller models. The design of MatFormer can be applied to both decoder and encoder networks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed nested Transformer for elastic inference is possible to extract exponentially many submodels based on the deployment constraints rather than only few submodels. I appreciate this high flexibility of MatFormer.\n\n2. The authors fully explore the design and extension of this solution. For example, they explore to reweight different granularities of submodels in Table 6, and evaluate MatFormer spanning from different modalities, i.e., language and vision, model classes, i.e., decoder and encoder, and scales (up to 2.6B parameters) in their main experiments.\n\n3. I think this research is valuable and general for the LLM inference, and has a great potential impact on the design and application deployment of large foundation models in the future."
            },
            "weaknesses": {
                "value": "1. The parameters g in the article, i.e., logarithmically spaced granularity, is an important parameter for MatFormer. The authors selected g = 4 for experimental verification and analysis. I wonder about the impact of different values of g on the flexibility, training efficiency and effectiveness of MatFormer, and the consistency and accuracy of submodels. This should be an interesting and important exploration which is currently missing.\n2. The Mix'n'Match procedure can freely combine hundreds of consistent submodels based on MatFormer layers to meet various specific constraints, but the experiments in Section 4 and Appendix seem to have only 9 submodels evaluated (as shown in Figure 2 and 4). I think the author can give more model loss and consistency data results of different combinations of submodels to better support the above advantage of MatFormer.\n3. As mentioned in the first paragraph of the Introduction, many similar practical solutions provide models with 7B, 13B, 33B and 65B parameters, but the experiments of this paper only verify and analyze models with parameters between 0.8-2.6B. If the authors have the enough computing resource, it will be quite valuable for LLM research to evaluate the effectiveness and application potential of MatFormer over a even larger model in your future work. This is not a big concern of this work.\n4. As authors say in the Introduction: \u201cMatFormer can also form a similar sub-structure on attention heads\u201d. It will be more clear if authors can illustrate the nested structure of attention heads just like Figure 1.\n5. The core method is similar to a previous work NetAug [1], which expands a tiny model into several larger models, and the tiny model obtains extra supervision via additional gradients. However, the starting points of two papers are different. NetAug aims to make tiny models more accurate while MatFormer aims to allow elastic Transformer inference. If authors can explain more comparison/analysis between them, I will appreciate the value of this work even more.\n6. Although the experiments are evaluated on wide applications and settings, the baseline seems weak to support the improvement of MatFormer. I recommend authors to add more comparisons with stronger baseline methods including other elastic inference techniques.\n\n[1] Cai H, Gan C, Lin J, et al. Network augmentation for tiny deep learning[J]. arXiv preprint arXiv:2110.08890, 2021."
            },
            "questions": {
                "value": "All of my concerns are explained in the Weaknesses part above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1481/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809532515,
        "cdate": 1698809532515,
        "tmdate": 1699636077029,
        "mdate": 1699636077029,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OhR4y1aYXm",
        "forum": "89XNDtqhpL",
        "replyto": "89XNDtqhpL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1481/Reviewer_CZp9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1481/Reviewer_CZp9"
        ],
        "content": {
            "summary": {
                "value": "This research introduces MatFormer, a novel nested Transformer architecture that addresses the challenges of deploying models across diverse constraints. It optimizes each FFN block in a way that allows for the flexible use of different model sizes across layers, even when they were not explicitly optimized. MatFormer proves to be effective in various model classes, modalities, and scales, with the ability to extract smaller models from a 2.6 billion parameter decoder-only MatFormer language model. These smaller models maintain comparable performance to independently trained counterparts. Additionally, MatFormer-derived encoders maintain the metric-space structure for large-scale retrieval, and speculative decoding with submodels extracted from MatFormer reduces inference latency. This work offers a promising solution for deploying Transformers in a wide range of settings while maintaining fine-grained control over trade-offs like latency, cost, and accuracy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper is well-written and easy to follow.\n- To my best knowledge, the research is the first work to introduce an interesting concept such as a nested Transformer architecture for LLMs. The proposed method aims to address the challenges of deploying models across diverse constraints.\n- The proposed method demonstrates the ability to obtain a variety of models without the need for additional training after a single learning process.\n- The study encompasses a broad spectrum of experiments, spanning both language and vision domains, and incorporating a range of modalities, classes, and scales."
            },
            "weaknesses": {
                "value": "The reviewer has two main concerns:\n\n**1. Weak baselines: there is no comparison with efficient LLM techniques that enhance inference speed without fine-tuning or additional training.**\n- MatFormer's baseline is limited to a vanilla transformer of the same size. However, it would be meaningful to compare it with recent techniques that improve inference speed without fine-tuning or additional training. For example, approaches like Dejavu [1], which leverages contextual sparsity during the inference phase to achieve enhanced inference speed within a given budget, could be an option. It is necessary to conduct such comparisons with several of these baselines to thoroughly evaluate the performance of MatFormer.\n\n**2. Lack of technical contributions compared to pre-trained supernet (the largest transformer)-based hardware-aware NAS methods.**\n\nBackground: In the field of NAS, there are studies that consider the largest supernet in the search space as a nested network of subnetworks and train the supernet to provide pre-trained subnetworks optimized for inference budgets [2, 3]. The definition of a search space may vary, but it typically includes various architectural design choices such as the number of blocks, layers, and hidden dimensions of layers.\n\nWhile the proposed MatFormer focuses on pre-trained LLMs, hardware-aware NAS methods [2] based on pre-trained transformers focus on traditional transformers. Therefore, their settings are not exactly the same. Nevertheless, both method\bs are fundamentally grounded in transformer structures, with the shared objective of constructing, training elastic transformer-based models, and delivering pre-trained transformer-based models that are optimized for specific inference budget constraints. Therefore, the reviewer finds it meaningful to compare the two in terms of their technical contributions.\n\n- Lack of search algorithm: MatFormer leaves the search algorithm as future work and employs a rather naive method called \"Mix\u2019n\u2019 Match\" to select the final optimal model within a given inference budget. However, as well-known in NAS research, searching for an optimal model from a search space significantly impacts the final performance and is not a trivial problem. The reviewer thinks that MatFormer would be better off including an algorithm to search for the best combination among pre-trained blocks.\n\n- Simple search space: The proposed MatFormer defines a search space by only considering the number of FFN blocks as the architectural design choice (If I am wrong, please let me know). This seems much simpler than the search spaces designed by NAS methods (e.g., [2]).\n\n\n[1] Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time, ICML 2023\n\n[2] HAT: Hardware-Aware Transformers for Efficient Natural Language Processing, ACL 2020\n\n[3] Once-for-All: Train One Network and Specialize it for Efficient Deployment, ICLR 2020"
            },
            "questions": {
                "value": "The reviewer believes that the proposed approach is valuable because it introduces elastic LLMs that provide multiple pre-trained transformer-based models optimized for specific inference budget constraints with a single training for the first time. However, its technical contributions are limited, and the baseline models used for comparison are weak.\n\n- Please address the concerns in Weaknesses section.\n- Q. How do training time and memory usage change when using the proposed approach for model training compared to training a single model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1481/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699146459429,
        "cdate": 1699146459429,
        "tmdate": 1699636076968,
        "mdate": 1699636076968,
        "license": "CC BY 4.0",
        "version": 2
    }
]