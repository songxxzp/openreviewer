[
    {
        "id": "VKlvNBYKQs",
        "forum": "qHGgNyQk31",
        "replyto": "qHGgNyQk31",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1684/Reviewer_KPNS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1684/Reviewer_KPNS"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the Seer model, a Language-Instructed Video Prediction with Latent Diffusion approach, for the text-conditioned video prediction (TVP) task. It enhances the U-Net architecture by incorporating a spatial-temporal attention module and introducing a Frame Sequential Text (FSText) Decomposer. This paper conducts experiments on three different datasets, demonstrating the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The authors propose a Frame Sequential Text Decomposer, which extends the capabilities of pretrained text-to-image (T2I) diffusion models along the temporal axis.\n- Seer achieves superior performance on three datasets: Something Something V2 (SSv2), Bridgedata, and EpicKitchens-100."
            },
            "weaknesses": {
                "value": "- While the paper repeatedly claims to have designed a sample and computation-efficient model, the only comparison provided in the appendix is in Table 8, focusing on temporal attention. It would be beneficial to include more computational comparisons to validate this claim.\n- The writing could be improved for better clarity. Additionally, the positioning of figures could be optimized for easier readability; consider rearranging Figures 3 and 4.\n- It would better to enhance the paper to include analysis of failure cases and generated video examples for a more comprehensive evaluation."
            },
            "questions": {
                "value": "Please refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1684/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1684/Reviewer_KPNS",
                    "ICLR.cc/2024/Conference/Submission1684/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1684/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698489044705,
        "cdate": 1698489044705,
        "tmdate": 1700659397590,
        "mdate": 1700659397590,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Qz7ywAgAb1",
        "forum": "qHGgNyQk31",
        "replyto": "qHGgNyQk31",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1684/Reviewer_zbgX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1684/Reviewer_zbgX"
        ],
        "content": {
            "summary": {
                "value": "The authors propose Seer, a video prediction model created from extending a pretrained text-to-image StableDiffusion model and finetuning additional elements.  When performing a denoised output, the per-frame convolutions are maintained from a pretrained text-to-image StableDiffusion model, however their outputs are subsequently updated by a temporal attention layer that can condition on reference/initialization frames as well as past temporal frames.  This serves to visually learn temporal consistency.  Secondly, the authors propose learning and using per-frame sub-instruction embeddings as conditioning information.  Ultimately this helps to guide each frame's generation with respect to sub-portions of the input instruction.  The authors perform experimentation on text-video datasets, and thorough ablations over different design choices of their approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "There are multiple works that have proposed generating video conditioned on text, particularly how to do so leveraging models that are pretrained to do text-to-image generation.  A large contribution of this work, in this reviewer's opinion, is the thorough exploration into what design decisions are important for the text-video generation task while making limited adjustments to a pretrained text-to-image model.  In other words, what are the important considerations that enable text-to-image models to cheaply extend to modeling videos.  The design decisions (such as what type of temporal attention is useful) are interesting insights.  This paper also focuses on natural (or photo-realistically simulated) videos, which is appreciated.  This reviewer makes the following additional, specific keyword comments:\n\n- Originality: this work is not particularly original, in that there are many efforts that try to extend a pretrained text-to-image diffusion model with minimal adjustments to perform text-to-video generation.  The architecture design decisions to extend static image diffusion models were reused from prior work, with additional novel components in the form of temporal window attention and per-frame subinstruction conditioning.\n- Quality: the quality of the work was high due to the experimentation on natural-looking videos, and the ablation insights.\n- Clarity: the paper was not the most easy to read, with certain design decisions described using unnecessarily confusing prose.  It often took multiple reads to understand what should have been an easy to describe design decision.  Furthermore, the figures were small (to the point of illegibility on printed paper) and often not particularly intuitive to understand.  I recommend the authors highlight the components of their method with larger and more intuitive figures (beyond arrows and flow-charts).  For example, in Figure 4, the output of part (a) is sub-instructions, but there is also an arrow that leads back with a \"step 1\" label, confusing the reader on how the flow of computation should be interpreted.  There is a similar issue in Figure 5, where deciphering the steps and how the arrows should be interpreted are naturally confusing (there are arrows pointing up as well as down, and all steps are shown simultaneously).  Perhaps such figures that depict multiple steps can be split into larger visuals with each step independently shown for clarity.\n- Significance: this work is of mediocre significance; it builds off of existing text-to-image models and past text-to-video approaches, but adds additional insight into what might be important to consider for future efforts along this direction (such as improving temporal consistency and fine-grained conditioning)."
            },
            "weaknesses": {
                "value": "This reviewer believes that learning text-grounded subinstructions for conditioning is an interesting and fruitful direction forward for solving fine-grained video generation, particularly for long-form video generation.  The authors therefore make useful initial efforts towards this effort.  However, the authors generate subinstructions by 1) initializing subinstructions using as the global embeddings and 2) performing cross attention with the global embeddings.  Therefore, it appears that subsets of the same information within a global embedding is selected for each subinstruction embedding, similar to performing attention on the global embedding.  Is it possible for a model to learn proper subgoals/subinstructions in such a way, when it is constrained as a manipulation of global information.  Intuitively, for a global instruction of \"cook a pasta dinner\", subinstructions might naturally be about \"boiling water\" and \"heating up sauce\", but such information may not inherently be extractable from a global embedding of \"cook a pasta dinner\".  This reviewer would be more convinced if the global text was expanded to include intermediate info, such as by thinking \"step by step\" via a pretrained LLM, which is then converted into subinstructions per timestep.  The current formulation does not seem conceptually convincing beyond being a slightly more advanced attention head, which may be inherently limiting as an approach.  Additional investigations into what the subinstructions may be encoding in terms of textual information would be helpful.\n\nThe second weakness is with the length of video generation.  The authors state that they only generate 12 frames for SSv2 and Epic100, and 16 frames for BridgeData.  Let us focus on SSv2 for the sake of example, but the point generally holds true.  SSv2 videos are stated to have a [FPS of 12](https://developer.qualcomm.com/software/ai-datasets/something-something).  Then, essentially the Seer model finetuned on SSv2 is only able to generate 1 second of video.  Put in this context, can enough happen within one second where finegrained details such as subinstructions are necessary.  On the other hand, if the authors are downsampling videoclips by sampling frames from the entire video equally spaced out by time, then essentially Seer is only learning to produce intermediate snapshots of a given natural video.  Only for very short sequences would the result of Seer then be treated as a smooth, natural video.\n\nText-Video Prediction (TVP) was motivated by the authors as a way to help robots follow human instructions (specifically, how to map abstract language instructions to policy actions).  Indeed, the authors motivate it by stating that humans employ an imagining-planning approach, where future trajectories are imagined in their minds.  This reviewer agrees with the authors, but notes that an additional step needs to be performed to ground generated text-conditioned videos to actual policies; given the motivations, it would be stronger to demonstrate an example (even synthetic) where the videos generated by Seer was able to help policy learning either in a robot, simulated environment, or game."
            },
            "questions": {
                "value": "1. Why were 2 reference frames used for Something Something-v2, but not for Bridgedata or EpicKitchens100?  Is there something special about SSv2, or were the generated results simply empirically worse for 1-reference frame SSv2 (and why might this be the case for this particular dataset)?\n2. Seer was demonstrated for 12 and 16 frame generations, but how does such a model perform for long-form generation?  Can the final frame(s) of an output sequence be used as a reference to generate subsequent frames using the same text prompt to create a longer video?  Or would the downsampling assumptions used prevent that from being meaningful (since it might generate 12 frames spanning an entire video all at once)?  It would strengthen the work to showcase and evaluate longer-form video generation.\n3. Are there ways to probe, textually, the subinstructions learned to understand what they are encoding?\n4. The paper makes repeated reference to \"initializing\" the subinstruction embeddings to be CLIP global embeddings.  However, at the end of Section 4 on Page 6, a loss equation is provided.  Is this an additional objective that is utilized during optimization of the FSText decomposer during finetuning, or are the subinstructions explicitly initialized to the global CLIP embeddings before performing self-attention, cross-attention, and temporal attention?  This distinction is important.\n5. Given that the global CLIP embedding is used for initialization, as well as for cross-attention, there does not seem to be any additional textual or contextual data to utilize for subinstructions.  Then, subinstructions seem to be generated purely as a combination of information captured in the specific words of the global instruction; to this reviewer, the FSText decomposer therefore acts as a form of attention network over existing the global embedding.  Can the authors highlight how this is conceptually different, given that the information generated in each subinstruction is simply a subset of the global instruction?  Furthermore, this approach seems highly dependent on the initial global instruction utilized.  Are there other choices for the text embedding that could be ablated over?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1684/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698612314524,
        "cdate": 1698612314524,
        "tmdate": 1699636096534,
        "mdate": 1699636096534,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Tcp0EkTTql",
        "forum": "qHGgNyQk31",
        "replyto": "qHGgNyQk31",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1684/Reviewer_xJcW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1684/Reviewer_xJcW"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a novel method for Text conditioned Video Prediction (TVP) that utilizes pretrained CLIP embeddings for language and inflates pretrained UNets from static image generation models to produce a language and image-conditioned next-frame prediction model. They perform experiments on something-something-v2, the Berkeley Bridge dataset, and Epic-Kitchens data demonstrating improved video prediction in comparison to competing methods on FVD and KVD. They also perform human studies and report that their model shows an improvement in the prediction task in comparison to existing baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n2. The method utilizes pretrained models thereby requiring less data than training from scratch. \n3. The experiments are thorough and performed on a variety of benchmarks\n4. The authors also perform human studies to validate the quantitative claims made in the paper"
            },
            "weaknesses": {
                "value": "1. Improve the motivation for the paper: The first few paragraphs of the introduction focus on robot learning, but the paper completely eschews this domain. It would be useful to motivate the paper from a different perspective that does not involve robotics. I appreciate that there are experiments conducted on the bridge dataset but I still believe this is primarily not a robotics paper as there are no resultant policies. \n2. Difficulty in comparison to text-to-video generation: Playing devil\u2019s advocate here, one could argue that performing text-to-video generation involves learning both the initial state distribution along with the transition distribution, while prediction involves only learning the transition distribution. Therefore it is unclear which is harder and it would be useful to avoid making such claims."
            },
            "questions": {
                "value": "1. How does sequence length of the video along with number of steps of prediction affect performance?\n2. What is the inductive bias that forces the Text Decomposer to produce coherent sub-instructions? Its unclear why it would segment the instructions into substeps on its own."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1684/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789263550,
        "cdate": 1698789263550,
        "tmdate": 1699636096471,
        "mdate": 1699636096471,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ziEmR1oxxd",
        "forum": "qHGgNyQk31",
        "replyto": "qHGgNyQk31",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1684/Reviewer_shhp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1684/Reviewer_shhp"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a novel text-conditioned video prediction \"Seer\". Seer leverages pretrained text-to-image stable diffusion model, extending it along the temporal axis to generate video. The key contributions of the authors are a novel Frame Sequential Text Decomposer module for frame-aligning sub instructions and a computation-efficient spatial-temporal attention to improve efficiency of temporal attention. FSText dissects global textual instructions into temporally aligned sub-instructions for precise integration into video frames, enabling the effective use of prior knowledge embedded in pretrained T2I models. Seer's architecture allows for high-fidelity, coherent video generation that aligns with textual instructions with only fine-tuning of a few layers on a small dataset. The model demonstrates superior video prediction performance on 3 egocentric task datasets, achieving significant improvements in Frechet and Kernel video distances and parameter efficiency compared to previous models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors propose and combine several ways to make text conditional video generation computationally tractable: reusing LDMs pre-trained on text to image tasks, and introducing a novel spatiotemporal attention that combines the benefits of bidirectional and directed temporal attention. I think the proposed spatiotemporal attention method is quite well thought and explained, and the empirical results and ablations suggest that it is effective for various task based video prediction tasks."
            },
            "weaknesses": {
                "value": "1. The design choices behind FSText are explained by the authors in Sec 4.3, but the overly complicated design choices could limit the utility of SEER outside of highly specific stepwise instruction videos. Did the authors perform any ablation studies that justified the various design choices in FSText (e.g. using cross-attention, using directed temporal attention, starting with CLIP text embedding etc.) \n\n2. For the ablations, could these be reported on more than one dataset? Results on one particular task in the SSv2 are quite convincing, but I am wondering if their could be dataset biases that seem to justify these design choices. For example, the BridgeData is a robotic task dataset versus human, so maybe that could be a good candidate to ensure results are not cherry picked."
            },
            "questions": {
                "value": "In Sec 1. Introduction *'existing text-conditioned video generation task (use) text specifying the general content, such as \u201ca person is skiing\u201d, while our aim in the TVP task is to use the text as a \u201ctask descriptor\u201d in robotics, such as \u201ctipping a beer can\u201d'*. Could the authors please explain how does using text as a task descriptor differ from text specifying general content? Does the VLM model encode the intent of text in the former, and if so are there significant differences across the two representations (e.g. CKA value)? How does it relate to the frame aligned sub-instructions in the FSText decomposer?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1684/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699106590043,
        "cdate": 1699106590043,
        "tmdate": 1699636096399,
        "mdate": 1699636096399,
        "license": "CC BY 4.0",
        "version": 2
    }
]