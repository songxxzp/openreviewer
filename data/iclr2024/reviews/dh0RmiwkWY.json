[
    {
        "id": "TaWzz8mqVU",
        "forum": "dh0RmiwkWY",
        "replyto": "dh0RmiwkWY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission446/Reviewer_3Hzn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission446/Reviewer_3Hzn"
        ],
        "content": {
            "summary": {
                "value": "-\tThe paper proposes a private generation method by assuming the support of the public data, which contains the private dataset and can be assumed as the Imagenet dataset for the public data. Then, the authors introduce DP-MGE and DP-DRE, where DP-DRE demonstrates prominent performance in private generation using the IC-GAN architecture."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-\tThe paper exhibits its own novelty in designing the density ratio estimator rather than fine-tuning the pre-trained models trained with public data. The latter is widely used in private generation tasks involving public data recently.\n-\tBy using the discriminator to distinguish whether the latent vector $\\boldsymbol{v}$ is obtained from public or private sources, the authors identify the optimal density ratio with a simple optimal assumption and convert the latent information from public data to private data with straightforward multiplication.\n-\tThe paper demonstrates the performance improvement of DP-DRE across a wide range of datasets, outperforming existing methods. The authors include various tasks such as the FID score, precision, and recall.\n-\tThe paper is well-written and easy to read."
            },
            "weaknesses": {
                "value": "-\tPlease refer to questions."
            },
            "questions": {
                "value": "I will happily raise the score if the authors can address the following questions:\n\n1.\tI agree the discriminator method is very interesting. However, considering that conditional generation is important, how can you inject conditioning into your framework? This is my major concern.\n\n2.\tEven though the authors mentioned the weakness of DP-MGE and highlighted the strength of DP-DRE in the comparison results in Section 4, I'm somewhat curious whether the performance of DP-MGE is influenced by the large amount of noise in $\\mu$ and $\\sigma$ in the latent dimension or the assumption of a unimodal Gaussian approach. Considering that conventional GANs use sampling starting from the unimodal normal distribution, the unimodal assumption itself might not be that harmful. Instead, the difference during training, such as injecting noise into $\\mu$ and $\\sigma$ versus DP-SGD training of the discriminator, could be a reason. (Minor) Alternatively, the authors might consider exploring complex distributions such as a mixture of Gaussians (MoG) for private latent space.\n\n3.\t(Minor) Can you explain how to ensure $||v_i|| \\leq 1$ in the normalization operator in IC-GAN? Does the operator clip the norm to 1 or use other methods to ensure the norm?\n\n4.\tI agree that using public data in terms of the discriminator is different from existing approaches. However, one of my concerns is that recent approaches in DP synthesis emphasize the importance of DP-SGD rather than existing methods [1,2], which is also true for classification tasks [3] using larger network sizes. One of my concerns is that DP-GAN-FT may be too na\u00efve to test the performance of DP-DRE. As you mentioned, adopting fine-tuning is easy for existing methods, as you discussed in Section 5. Can you provide additional experimental results using state-of-the-art (SOTA) architectures or optimizations without using public data, on top of pre-trained models?\n\n5.\tAlthough the proposed method focuses on GANs, there are various existing models that use pre-trained models in private diffusion synthesis. As they use pre-trained models in the same setting as the proposed methods, the authors should address these topics [4,5].\n\n6.\tThere are some typos, such as two neibouring datasets in Appendix A. Please re-check the grammar once again.\n\n[1] Differentially Private Diffusion Models, TMLR 2023.\n[2] Private GANs, TMLR 2023.\n[3] Unlocking High-Accuracy Differentially Private Image Classification through Scale, Arxiv 2022.\n[4] Differentially Private Latent Diffusion Models, Arxiv 2023.\n[5] Differentially Private Diffusion Models Generate Useful Synthetic Images, Arxiv 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper deals with the major situation of differentially private deep learning. Thus, the authors should provide the code of ethics."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission446/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698147290991,
        "cdate": 1698147290991,
        "tmdate": 1699635971093,
        "mdate": 1699635971093,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Vt30g2xxTf",
        "forum": "dh0RmiwkWY",
        "replyto": "dh0RmiwkWY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission446/Reviewer_KC1E"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission446/Reviewer_KC1E"
        ],
        "content": {
            "summary": {
                "value": "This paper studies differentially private image generation under the assumtion that large-scale public data is available. More specifically, the public data assumption posits that the support of the public data distribution contains the support of the private data. This work incorporates the IC-GAN and reduces the problem of learning the private image distribution to learning a private feature distribution in the latent space of the encoder. This work proposes two methods DP-MGE and DP-DRE, for differentially private embedding estimation. The experiments evaluation includes 6 different image tasks and achieves compelling results across different privacy levels."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The evaluation is comprehensive, across different datasets and different privacy levels and achieves compelling results. \n- The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "- It seems to me that the most significant improvements by DP-DRE/DP-MGE stem from the use of IC-GAN. For example, for DP-DRE, even at epsilon=0, i.e., by directly using the public data embedding, the results are already much better than most private results by DP-GAN-FT/DP-MEPF/DP-GAN-MI. Also, the DP-MGE looks similar to DP-MEPF. Both of these methods estimate the first and second order statistics. According to the non-private results, the improvement for DP-MGE over DP-MEPF is that it would be better to directly use the embedding instead of training another network to approximate such embedding.\n\n- Another concern I have is regarding the assumption of public data. This work assumes that the support of the public data distribution contains the support of the private data and the authors validate that DP-DRE/DP-MGE perform poorly when the assumption is severely violated. However, recent works show that even out-of-distribution public data can still help private training [R1]. It would be better to study how to adapt to the private embedding domain when this assumption is violated.\n\n[R1] Arun Ganesh, Mahdi Haghifam, Milad Nasr, Sewoong Oh, Thomas Steinke, Om Thakkar, Abhradeep Guha Thakurta, Lun Wang. Why Is Public Pretraining Necessary for Private Model Training? ICML 2023"
            },
            "questions": {
                "value": "- In Table 2, it is somewhat counterintuitive that the private result of DP-MGE at epsilon =1 is better than the result of DP-MGE at epsilon=infinity. I wonder if there is a brief explanation for this. \n- If I understand correctly, DP-GAN-FT and DP-GAN-MI share the same architecture but the trainable parameters are different. While DP-GAN-FT has more trainable parameters, for the non-private results, DP-GAN-MI performs better. Does this indicate that the pretrained features in IC-GAN matter most and updating it using the private data would distort the pretrained features? This might indicate that DP-MGE/DP-DRE improves by the advantage of IC-GAN, while facing the issue that cannot adapt to the private domain when the public data assumption is violated."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission446/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission446/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission446/Reviewer_KC1E"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission446/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698632826975,
        "cdate": 1698632826975,
        "tmdate": 1699635971024,
        "mdate": 1699635971024,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tJHxOQwcPZ",
        "forum": "dh0RmiwkWY",
        "replyto": "dh0RmiwkWY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission446/Reviewer_Kyfi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission446/Reviewer_Kyfi"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors studied the differential privacy (DP) image generation. The author's goal is to train a Generative Adversarial Network (GAN) to generate samples from the private data distribution while preserving differential privacy of the private data. I am not expert of DP image generation. However, I am concerned about the following issues."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors' proposed algorithm outperforms existing benchmark aspects of FID scores as well as other distribution quality metrics."
            },
            "weaknesses": {
                "value": "Lack of comparison of state-of-the-art comparison methods."
            },
            "questions": {
                "value": "Major comments:\n1.\tPlease add the latest comparison methodology (e.g., for 2023).\n2.\tIn subsection 4.2, \u201cThese artifacts may be due to the fact that DP-MEPF does not use a pretrained public encoder.\u201d, In subsection 4.2, \"A\" is described, is there any experiment added to prove it. For example, artifacts can be eliminated by adding pre-trained public coding to DP-MEPF.\n3.\tThe paper can be compared with more adversarial training-based generative models, such as the few-shot migration methods such as Diffusion-GAN, to demonstrate the advantages of the methodology. \nMinor comments\uff1a\n1.\tThere are grammatical problems in the paper. It is suggested to check and modify it carefully. For example:\n1)\tIn the abstract, \u201cwhile the private data consist of images of a specific type\u201d->\u201d while the private data consists of images of a specific type\u201d.\n2)\tIn subsection 2.1, \u201cand now considered\u201d->\u201dand is now considered\u201d\n3)\tAnd so on.\n2.\tSubsection 5, related work, is proposed to be placed after introduction.\n3.\tPlease add a reference to the comparison method DP-GAN-FT."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission446/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698668298221,
        "cdate": 1698668298221,
        "tmdate": 1699635970955,
        "mdate": 1699635970955,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "h2AIPnjA7l",
        "forum": "dh0RmiwkWY",
        "replyto": "dh0RmiwkWY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission446/Reviewer_VNLs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission446/Reviewer_VNLs"
        ],
        "content": {
            "summary": {
                "value": "This submission investigates the task of differentially private (DP) image generation while leveraging public data. Specifically, an \"autoencoder\" (a feature extractor that encodes images into a feature space, combined with an ICGAN generator that produces images from these features) is pre-trained on large-scale public datasets such as ImageNet.\n\nTo achieve DP generation, the authors first estimate the feature distribution of the private data using existing DP density estimation methods. Samples are then drawn from this estimated distribution and fed into the pre-trained decoder (i.e., the ICGAN generator). \nDP distribution estimation is realized through either:\n- A DP Gaussian estimation, termed the DP-MGE variant.\n- Training an auxiliary GAN (that approximately acts as a density ratio estimator) in the feature space, termed the DP-DRE variant.\n\nExperimental results demonstrate that the proposed method generally surpasses several baseline methods in generation quality, including DP-GAN-FT, DP-Merf, and DP-GAN-MI."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is generally well-written and easy to follow\n- Utilizing public data for DP generation is a natural idea for improving privacy-utility trade-off\n- The key assumption (i.e., the support of the public data distribution contains the support of the private one) is clearly stated \n- Experiments generally demonstrate promising results"
            },
            "weaknesses": {
                "value": "- The assumption is a bit strong (i.e., the private distribution should be fully covered by the public one) which may greatly restrict the applicability of the proposed approach. While this limitation has been acknowledged by the authors in section 4.2, a potential adaptation or extension of the method to fit the common practical scenario is currently missing but would greatly strengthen the submission.\n\n- This work's originality seems to offer little innovation due to its dependence on two established components: the use of public data (numerous references) and the implementation of an autoencoder-style pipeline (refer to [1,2]). Both of these strategies have previously been investigated for DP generation.\n\n- The experimental comparison appears to overlook some essential baselines or references, making the current work less convincing. For example, given that it is quite a natural idea to use foundation models for DP learning, a critical baseline would be using strong pre-trained generative models (such as strong GANs like StyleGAN2 and Diffusion models [3]) and directly fine-tune such models or filter the outputs. \n\n\n[1] \"Differentially private data generative models\", Arxiv 2018  \n[2] \"Differentially private mixture of generative neural networks\", IEEE Transactions on Knowledge and Data Engineering, 2018  \n[3] \"Differentially Private Diffusion Models Generate Useful Synthetic Images\", Arxiv, 2023"
            },
            "questions": {
                "value": "- As discussed above, using external public data for DP generation/learning seems both intuitive and promising. However, I would have expected the paper to delve deeper into why the proposed usage stands out as superior, especially when there are straightforward alternatives that leverage powerful foundational models. While I'm not aware of published work that focuses on this idea, the significance of this paper's contribution may still be diminished if no clear advantages are highlighted in comparison to these straightforward options. This is particularly true since I've observed superior results from other options when dealing with distribution shifts, and such shifts are inevitable when using public data.\n\n- From what I understand, private generation in the proposed approach simply requires sampling of features that follow the private distribution, rather than a full density estimation. Given this, why wouldn't it be more effective to train the GAN to generate samples directly, rather than using an approximate density estimation for the DP-DRE variant? Especially considering that GANs don't natively support the density estimation and it could lead to inaccurate results.\n\n- I feel there's a lack of context regarding how ICGAN operates and why it's particularly suitable. A concise description of its mechanism and a few sentences highlighting its key advantages would be beneficial."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission446/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698747479375,
        "cdate": 1698747479375,
        "tmdate": 1699635970890,
        "mdate": 1699635970890,
        "license": "CC BY 4.0",
        "version": 2
    }
]