[
    {
        "id": "fT4Qq4IG4f",
        "forum": "sK2A7Ve2co",
        "replyto": "sK2A7Ve2co",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5473/Reviewer_XKu5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5473/Reviewer_XKu5"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method to obtain Gaussian approximations of posterior distributions in Bayesian deep learning. The experiments compare the proposed method against several related approaches on toy experiments as well as classification on CIFAR-10/100 and ImageNet."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The authors report that their method tends to produce samples quicker than competitor methods."
            },
            "weaknesses": {
                "value": "The paper is definitely still a work in progress and not ready for publication at a conference like ICLR.\nThus, I vote for rejection and encourage the authors to completely revise their manuscript and submit to another venue.\n\nThe writing style and organization of the paper is very bad, which makes it extremely hard to follow. In particular, the theoretical exposition is lacking:\n- The theory is mixed with the related work (Eqs. (1)-(3), last Sec. of 1.1)\n- Central notions and symbols are not introduced, the exposition remains very handwavy. To name only a few examples:\n  - what do the authors mean by \"transforming a pretrained into a Bayesian model\"?\n  - background on MCMC, Metropolis-Hastings corrections\n  - definition of a \"perfect sampler\"\n  - how do the authors define a \"mode-specific MH\"\n  - it remains unclear in which sense the proposed method better deals with multi-modal posteriors than related work\n  - definition of notion of time step $t$ and $\\theta_t$ in Eq. (4)\n  - definition of $D_x$, $D_y$ in Eq. (15, 16)\n  - definition of $\\mathrm{Conf}$ in Eq. (20)\n  - ...\n- The experimental evaluation is not convincing.\n  - While the authors report fast sampling, their approach is outperformed by competitor methods most of the time.\n  - On the simplest toy example (unimodal Gaussian posterior), the authors report good results in terms of effective sample size (which is not very surprising because they use the correct approximation). However, they do not report ESS on the mixture model (Figure 2 RHS). \n  - The authors argue that their method deals well with multi-modal posteriors. Thus, they should compare\n against other methods that capture multiple modes, i.p., Deep Ensembles [1] and Multi-SWAG [2].\n  - As the authors employ a Gaussian posterior approximations, they should compare against variational Gaussian approximations, e.g., BayesByBackprop [3].\n\n[1] Lakshminarayanan et al., \"Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles\", NeurIPS 2017\n\n[2] Wilson & Izmailov, \"Bayesian Deep Learning and a Probabilistic Perspective of Generalization\", NeurIPS 2020\n\n[3] Blundell et al., \"Weight Uncertainty in Neural Networks\", ICML 2015"
            },
            "questions": {
                "value": "Please elaborate on the concerns raised below \"Weaknesses\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5473/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5473/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5473/Reviewer_XKu5"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5473/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698340335396,
        "cdate": 1698340335396,
        "tmdate": 1699672907827,
        "mdate": 1699672907827,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oiS3C3yQhm",
        "forum": "sK2A7Ve2co",
        "replyto": "sK2A7Ve2co",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5473/Reviewer_xaq1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5473/Reviewer_xaq1"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an adaptive proposal sampling (APS), a mode seeking sampler that adapts the proposal to match a posterior mode."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed ``adaptive proposal sampler'' appears to be new in the literature."
            },
            "weaknesses": {
                "value": "1. Extension of the proposed sampler to high-dimensional problems is questionable. As mentioned in the paper, the parameters are regarded as independent of each other, making the proposed sampler less accurate and thus less attractive. \n\n2. When the modes of the target distribution are well separated, it is difficult to believe that the proposed sampler can efficiently traverse the entire energy landscape because, similar to the Metropolis-Hastings algorithm, the proposed sampler lacks a mode-escaping mechanism. \n\n3. For the exact Gaussian proposal sampler, the acceptance rate can be low when the dimension of \\theta is high."
            },
            "questions": {
                "value": "1. If the exact GPS is applied to the numerical examples of the paper, will the reported results be improved? How much?   \n\n2. The proposed method needs to compare with more baseline methods, such as SGHMC [1]  and adaptively weighted SGLD [2], on multi-modal and high-dimensional problems.\n\nReferences: \n\n[1] Chen et al. (2014) Stochastic Gradient Hamiltonian Monte Carlo. ICML 2014. \n\n[2]  Deng et al. (2022) An adaptively weighted stochastic gradient MCMC algorithm\nfor Monte Carlo simulation and global optimization. Statistics and Computing, 32:58."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5473/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698616789279,
        "cdate": 1698616789279,
        "tmdate": 1699636558195,
        "mdate": 1699636558195,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xxEPCrMOcu",
        "forum": "sK2A7Ve2co",
        "replyto": "sK2A7Ve2co",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5473/Reviewer_zCTz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5473/Reviewer_zCTz"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new sampling algorithm for multi-modal distributions, especially deep neural network posteriors. Specifically, the authors learn an adaptive Gaussian proposal along with sampling. Several experiments, including synthetic distributions and deep learning tasks, are conducted to test the proposed method."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe studied topic of sampling on multi-modal distributions is important.\n2.\tThe proposed algorithm is simple to implement in practice."
            },
            "weaknesses": {
                "value": "1.\tThe proposed method does not achieve what it claims to \u201chaving both exactness and effectiveness\u201d. Apparently, the method is not exact without the MH correction step. The method is only exact when the target distribution is a Gaussian with a diagonal covariance, which is a trivial case. I\u2019m not sure what \u201cperfect sampler\u201d means in the paper. Overall, I think many claims need to be modified in order to be accurate and rigorous. \n2.\tThe methodology of the proposed method is confusing. The algorithm does not have a component to encourage exploring multiple modes. It is unclear to me how the method manages to find diverse modes. \n3.\tAlgorithm 1 seems to find a Gaussian distribution to approximate the target distribution. How is it different from variational inference? What are the advantages?\n4.\tWhy does the proposed method require a pretrained solution, theta_MAP? Will it work if training from scratch? \n8.\tI do not follow the reason for introducing the variance limit lambda. Why does the method need it?\n9.\tThe experimental setups and results are confusing. It is unclear if the authors also use a pre-trained solution for the baseline NUTS in S3.1. If not, then it is unfair to claim faster convergence of the proposed method than NUTS. Besides, given that the method uses a pre-trained solution, it is unsurprising that \u201cWe found that a-GPS converges so fast that a burn-in period was unnecessary\u201d. For the time comparison, it is unclear if the authors include pre-training time.\n10.\tFor deep learning experiments, it will be better to include MCMC baselines, e.g. Zhang et al, as the proposed method belongs to MCMC methods. To show the samples are from diverse modes, the authors can visualize weight space and function space, similar to those in Zhang et al.\n\n\nZhang et al, Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning, ICLR 2020"
            },
            "questions": {
                "value": "1.\tWhy is LA\u2019s inference time even less than MAP? Why is the proposed method\u2019s inference time less than SWAG? Does the proposed method use Bayesian model averaging during inference?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5473/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698618917865,
        "cdate": 1698618917865,
        "tmdate": 1699636558088,
        "mdate": 1699636558088,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iqWC7yGWAH",
        "forum": "sK2A7Ve2co",
        "replyto": "sK2A7Ve2co",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5473/Reviewer_RZPX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5473/Reviewer_RZPX"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a sampler that samples weights via traversing the loss landscape of a pre-trained deep neural network via a series of normal distributions. The approach is evaluated on a series of classification and out-of-distribution detection tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper proposes a sampler that samples weights via traversing the loss landscape of a pre-trained deep neural network via a series of normal distributions. The approach is evaluated on a series of classification and out-of-distribution detection tasks."
            },
            "weaknesses": {
                "value": "- The main weakness of the paper is in the experimental evaluation. The experiments show convincingly that the proposal works with several architectures and several classification data sets (no regression tasks were evaluated). What it does not show is that it works better than its baselines, i.e., why should it be used instead of SWAG, or SGD-MC? E.g., SGD-MC almost always outperforms it (it is missing from Table 4, but the results in Table 13, show that it clearly performs better), except for the strange behavior in Table 6.   \n\n\n- The presentation of the paper is rather sub-optimal. E.g.,\n    - parameters such as $c$ and $\\lambda$ appear in the text long before they are even introduced, if at all. The important $\\lambda$, e.g., only is further detailed in Algorithm 1.\n    - The writing contains a lot of typos, e.g., for the first paragraph on the second page\n        - \"full-gradient MCMC similar **to** SG-MCMC\"\n        - \"SGLD **has** fast computations but **suffers** form inefficient explorations\"\n        - \"Previous **works** on state dependent\"\n    - Dropout's absence in most of the results is not explained in the main text but only appears in the one table where it is present rather than absent\n    - The writing is somewhat repetitive\n    - The reference list is full of arxiv preprints instead of the actual publications \n    - Table 4 contains wrong highlights in two columns (ECE and NLL), the same is true for several tables in the appendix.\n    - On the positive side, however, other details, like definitions of performance metrics are highlighted prominently\n\n### Minor\n- SGD-MC is mentioned in the text for Table 4 but not in the actual results\n- LA is missing in Table 3 without an explanation\n- Sec 2.1: \"the loss function, ..., typically cross-entropy is interpreted as the negative log-likelihood\". Cross-entropy is typical for classification tasks, but not for any other tasks. And in this case, it is not just interpreted as a negative log-likelihood, _it is_ the negative of a categorical distribution. \n- For the posterior in  (15). A Gaussian prior is $\\exp(-||\\theta||)$, similarly for the loss factor. This directly provides you with (17) instead of having to redefine anything.\n- Sec 3.2.2 \"separated by high loss area\". As Draxler et al. (2018) and Garipos et al. (2018) show there are a lot of paths of similar loss between a lot of maxima instead of a clear separation. (These motivated the SWA baseline of the present work)\n\n\n\n_____\nDraxler et al., _Essentially no Barriers in Neural Network Energy Landscape_, ICML 2018  \nGaripov et al., _Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs_, NeurIPS 2018"
            },
            "questions": {
                "value": "- The conclusion only discusses a-GPS' performance with respect to SWAG and Laplace. Can the authors additionally provide a deeper discussion on their relation to SGD-MC and in general summarize why their approach should be picked instead of these established baselines?\n- SGLD is mentioned in the related work, but never used in the experiments. Can the authors comment on this lack of comparison? Especially since they cite Izmailov et al. (2021) who showed good results for this approach.\n- A lot of approaches and networks diverged or failed otherwise throughout the experiments. Can the authors give further details? E.g., it seems rather strange that a simple model such as VGG should diverge on a straight-forward classification task such as CIFAR100.\n- The method was only tested on classification tasks. What about regression problems? Do the authors expect a similar performance? \n- How is the split in CIFAR10 and CIFAR 100 in 5/50 classes decided? _(Apologies if I missed it somewhere in the appendix)_"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5473/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698652042813,
        "cdate": 1698652042813,
        "tmdate": 1699636557963,
        "mdate": 1699636557963,
        "license": "CC BY 4.0",
        "version": 2
    }
]