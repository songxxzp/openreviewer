[
    {
        "id": "DN6C9PcyW2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8641/Reviewer_H9dR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8641/Reviewer_H9dR"
        ],
        "forum": "LYGHdwyXUb",
        "replyto": "LYGHdwyXUb",
        "content": {
            "summary": {
                "value": "This paper advocates for the selective adoption of shareable behaviors across tasks while concurrently mitigating the impact of unshareable behaviors, a proposition that is well-motivated and promising. However, certain sections, notably the abstract and introduction, require further elucidation. More comprehensive conceptual and empirical comparisons with existing literature in this domain are required. Considering the extensive body of relevant work in this field, the evidence presented in the paper falls short of substantiating the acceptance of this work, leading me to recommend a weak rejection."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(a) The intuition behind the algorithm design is novel and interesting: using Q-value to identify potentially shareable behaviors and encourage exploration.\n\n(b) The algorithm part (Section 4) is well-presented and easy to follow.\n\n(c) The empirical analysis is detailed and informative about the properties of the proposed algorithm."
            },
            "weaknesses": {
                "value": "(a) Section 1 requires further elucidation concerning comparisons with prior works and descriptions of the proposed algorithm.\n\n(b) It would be advantageous to include studies on \"skills\" within related works, given their conceptual similarity to the \"shared behaviors\" discussed in this paper.\n\n(c) The algorithm's design, which learns a distinct policy for each task, could potentially diminish sample efficiency.\n\n(d) The shareable behaviors can be adopted in a more efficient manner (e.g., forming a hierarchical policy), rather than only used for gathering training data.\n\n(e) The selected baselines for comparisons are kind of weak and can be further strengthened.\n\n(f) The comparisons with baselines depicted in Figure 4 fail to demonstrate notable improvements conferred by the proposed algorithm."
            },
            "questions": {
                "value": "(a) The definition of \" conflicting behaviors\" should be elaborated in Section 1.\n\n(b) In Section 2, the authors mention \"However, unlike our work, they share behavior uniformly between policies and assume that optimal behaviors are shared across tasks in most states.\" More explanations are required for \"share behavior uniformly\" and \"assume that optimal behaviors are shared across tasks\", where the latter one seems not to be true.\n\n(c) \" Yu et al. (2021) uses Q-functions to filter which data should be shared between tasks in a multi-task setting.\" It would be good to provide more detailed comparisons with this related work.\n\n(d) It should be \"argmax\" for the equation in Section 3.\n\n(e) Theoretically, the policy network is trained to give actions with the maximized Q-value. That is, $i = \\arg\\max_jQ_{i}(s, a_j)$ (Line 9 of Algorithm 1) should hold in most cases, which may make this key algorithm design trivial.\n\n(f) The baseline \"Fully-Shared-Behaviors\" cannot be viewed as a fair comparison, since the agent cannot identify which task it is dealing with. The task identifiers should also be part of the input. There are many research works in this area, such as [1-3] and the ones listed by the authors in Section 2. It would be beneficial to provide comparisons with these works as well.\n\n[1] Sodhani, Shagun, Amy Zhang, and Joelle Pineau. \"Multi-task reinforcement learning with context-based representations.\" In International Conference on Machine Learning, pp. 9767-9779. PMLR, 2021.\n\n[2] Yang, Ruihan, Huazhe Xu, Yi Wu, and Xiaolong Wang. \"Multi-task reinforcement learning with soft modularization.\" Advances in Neural Information Processing Systems 33 (2020): 4767-4777.\n\n[3] Hessel, Matteo, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado Van Hasselt. \"Multi-task deep reinforcement learning with popart.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, pp. 3796-3803. 2019."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697145845273,
        "cdate": 1697145845273,
        "tmdate": 1699637081976,
        "mdate": 1699637081976,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yKck3XYFly",
        "forum": "LYGHdwyXUb",
        "replyto": "LYGHdwyXUb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8641/Reviewer_neqy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8641/Reviewer_neqy"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Q-Switch Mixture of Policies (QMP) to facilitate the selective sharing of behaviors across tasks, enhancing exploration and information gathering in Multi-task Reinforcement Learning (MTRL). Assuming that different tasks demand distinct optimal behaviors from the same state, QMP employs the Q network of the current task to determine the exploration policy from a pool of all tasks and generate rollout data for efficient training. The method showcases performance improvements across various benchmark scenarios."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper is generally well-written and is easy to follow.\n2. The concept of sharing \"behavior\" instead of data or parameters is intriguing.\n3. Empirical results demonstrate that QMP effectively handles multiple tasks with conflicts, and the experiments are detailed and comprehensive."
            },
            "weaknesses": {
                "value": "1. The central issue with this paper lies in its use of $Q_i$ to evaluate shareable behaviors. In reality, $Q_i(s, a)$ estimates the \"expected discounted return of policy $\\pi_i$ after executing action $a$ in state $s$,\" emphasizing that the trajectory to the left is generated by $\\pi_i$. However, the authors employ it to evaluate \"behavior,\" which could be interpreted as an action sequence following state $s$.\" Although the authors acknowledge that \"the Q-function could be biased when queried with out-of-distribution actions from other policies,\" even if we assume that the Q-function fits well, it still struggles to accurately evaluate another policy using only $Q_i(s, \\pi_j(s))$.\n2. The method, while simple, appears more heuristic in nature and lacks guarantees.\n3. The discussion of related works is insufficient and comes across as disjointed and poorly structured."
            },
            "questions": {
                "value": "Regarding the Weaknesses mentioned, do you think there are ways to address these concerns or clarify the usage of $Q_i$ for evaluating behaviors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697950758219,
        "cdate": 1697950758219,
        "tmdate": 1699637081782,
        "mdate": 1699637081782,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vwcZ2zLYRV",
        "forum": "LYGHdwyXUb",
        "replyto": "LYGHdwyXUb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8641/Reviewer_Zv3o"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8641/Reviewer_Zv3o"
        ],
        "content": {
            "summary": {
                "value": "This paper considers sharing learned behaviors across tasks in multi-task reinforcement learning (MTRL). To preserve optimality, authors propose a method called Q-switch Mixture of Policies (QMP). When training the multi-task policies, QMP estimates the shareability between task policies and incorporates them as temporally extended behaviors to collect training data. Experiments on a wide range of manipulation, locomotion and navigation MTRL task families demonstrate the effectiveness of QMP."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThis paper is well-written and easy to follow\n2.\tSharing learned behaviors among tasks is an interesting and important topic in RL.\n3.\tAuthors conducted extensive experiments and analysis to empirically illustrate the effectiveness of QMP."
            },
            "weaknesses": {
                "value": "1. Lack of theoretical analysis on the convergence (rate) of QMP.\n2. Lack of intuition and detailed analysis on why the proposed method works (i.e., why could QMP make sharing desired behaviors among tasks possible? Please see Question 2 for my concern).\n\nPlease refer to my questions below for my concerns."
            },
            "questions": {
                "value": "1. Why do you choose to roll out H steps instead of just one step after choosing one policy to collect data? QMP just uses the Q function under a particular state to choose the behavioral policy, which cannot guarantee that the selected behavior policy is helpful for the current task after stepping out of the considered state.\n2. Will QMP cause undesired behaviors sharing? As I mentioned in Question 1, the selected behavioral policy will roll out for H(>1) steps, which may incurs sub-optimal behaviors.\n3. Why choosing the behavioral policy based on the learned Q-function to collect data will essentially share desired behaviors among tasks? I understand that using Q-function as a proxy may help select more better actions. However, the Q-function may be biased and not learned well during training, which could even hurt the learning process.\n4. Will QMP even slow down the training process? Say, the learned policy for the current task proposes an action, which is optimal but has an under-estimated Q function. Due to the biased Q function, QMP selects another policy to collect data, which chooses a sub-optimal action. Although the TD update will fix the estimated Q value of the sub-optimal action, it may be more efficient if we directly update the Q value of the optimal action, the thing that we really care about.\n\nI am willing to raise my scores if you could solve my concerns."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8641/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8641/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8641/Reviewer_Zv3o"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697966360630,
        "cdate": 1697966360630,
        "tmdate": 1699637081650,
        "mdate": 1699637081650,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WcTVR9uL2k",
        "forum": "LYGHdwyXUb",
        "replyto": "LYGHdwyXUb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8641/Reviewer_QRge"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8641/Reviewer_QRge"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes QMP, a behavior-sharing method in multitask reinforcement learning. QMP uses a mixture of policies to determine which policy is better to collect data for each task. Experiments are conducted on various robotics domains, showing the superior performance of QMP."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea of this paper is clear and easy to follow.\n\nExperiments are extensive and results are promising, significantly outperforming baselines."
            },
            "weaknesses": {
                "value": "Some technical details need to be clarified:\n\n1) Since the Q-switch evaluates each $Q_1(s,a_j)$, it inherently assumes all tasks share the same state-action space, or at least assumes the same size of state-action dimension. \n\n2) The reviewer noticed that the paper mentioned 'out-of-distribution action proposal ' in Section 4.3, how does the agent know the action is out-of-distribution? Do you mean the action may be [-10,10] while the action space is [-1,1]?  \n\n3) The reviewer is not convinced by the criterion used for collecting data. If the Q-value of some task $j$'s action $a_j$ at state $s$ is the highest, then the QMP will rollout $\\pi_j$ for H steps. What if the next state's $Q(s', a_{j'})$ is the worst among all tasks? How does this work, though the results are very promising? When sampling from the dataset, do you filter the samples with lower rewards?\n\n4) How to sample the data from $\\pi_i$ and QMP? Is there a specific fraction or equal sampling?\n\n5) \n\nSome questions about experiments:\n\n1) The reviewer feels the comparison is unfair regarding the shared-parameters baseline. The key problem in MTRL is to address the interference when multiple tasks use one network to train the policy. Therefore, a lot of papers come up with different ideas, such as conflict gradient resolution (PcGrad, CAGrad)  and pathway finding (soft modularization, T3S). However, this paper only compared to the basic Multi-head-SAC. While QMP has a designed criterion to select what behaviors to share and how to share. That's why the results in Figure 5 show that 'Parameters + Behaviors' performs worse than 'Behaviors Only'.\n\nThe literature review lacks related works such as [1-4].\n\n[1] Conflict-averse gradient descent for multi-task learning\n\n[2] Structure-Aware Transformer Policy for Inhomogeneous Multi-Task Reinforcement Learning\n\n[3] T3S: Improving Multi-Task Reinforcement Learning with Task-Specific Feature Selector and Scheduler\n\n[4] Provable benefit of multitask representation learning in reinforcement learning"
            },
            "questions": {
                "value": "Please refer to the pros and cons part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699076499868,
        "cdate": 1699076499868,
        "tmdate": 1699637081547,
        "mdate": 1699637081547,
        "license": "CC BY 4.0",
        "version": 2
    }
]