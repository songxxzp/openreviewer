[
    {
        "id": "mzkL1zuEvc",
        "forum": "1M0qIxVKf6",
        "replyto": "1M0qIxVKf6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2630/Reviewer_rGTL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2630/Reviewer_rGTL"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the intermediate representation of Transformer by viewing that each token embedding is decomposed into (i) position-wise information and (ii) sequence-wise information. There are 3 main findings --- (a) (i) forms spiral curves in a low-dimensional space, (b) (ii) contains a cluster structure, (c) (i) and (ii) are almost orthogonal --- are observed on pre-trained language models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The analysis based on the decomposition is original. I haven't seen this type of decomposition of token embeddings.\n\nThe third finding (c) is an interesting property, which might open a new research direction."
            },
            "weaknesses": {
                "value": "The first two findings (a and b) sound relatively trivial. I think the behavior of (a) mainly comes from the sinusoidal positional embedding. The effect of positional embedding propagates to upcoming layers via skip connections, which would explain why the spiral patterns are consistently observed across layers. For (b), since ctx vector is computed by averaging token embeddings over each sequence, it's natural to contain topic-like information. More precisely, each token embedding should contain context (or topic) related information to predict the next word. Taking the average will emphasize the context information, which should be distinguished from other context information obtained from a different document.\n\nThe paper analyzes the Transformer models in many aspects. However, each analysis is not tightly connected, and it's hard to capture concrete outcomes."
            },
            "questions": {
                "value": "Why does Equation (7) not include the residual term?\n\nSection 4.2 starts with the following question:\n\"positional information is passed from earlier layers to later layers \u2026 How does transformer layer TFLayer enable this information flow?\"\nIsn't this simply because of the skip connections? Also, why do you consider the low-rank plus diagonal form in Equation (8)? Don't you observe the alignment with the positional basis by svd(W) instead of svd(W - diagg(W))?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2630/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2630/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2630/Reviewer_rGTL"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2630/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838053103,
        "cdate": 1698838053103,
        "tmdate": 1700525484877,
        "mdate": 1700525484877,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BNuWDiM6JC",
        "forum": "1M0qIxVKf6",
        "replyto": "1M0qIxVKf6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2630/Reviewer_Jv3k"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2630/Reviewer_Jv3k"
        ],
        "content": {
            "summary": {
                "value": "The paper attempts an investigation of geometric structures of embeddings leaned by transformers. It first proposes a decomposition of the embeddings to a positional component (mean vectors across context) and a contextual component (mean vectors across position). Then, it studies the geometry of each of these components. For the positional component, they find that that it is low-dimensional and smoothly varying. Concretely the Gram matrix of positions is low-rank in the Fourier domain. For the contextual component, they identify clustering structures. Finally, they find that contextual component is incoherent (almost orthogonal) to the positional component."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-- The investigation of geometry of token embeddings is in my opinion interesting and could shed light on the operation of LLMs\n\n-- I like the proposed decomposition of embeddings into their global mean, positional, contextual and residual part. It is simple, but interesting\n\n-- The authors have conducted rather thorough investigation with multiple experiments \n\n-- The discussion on low-rankness of positional embeddings and its connection to smoothness via fourier analysis is interesting"
            },
            "weaknesses": {
                "value": "I am torn on my decision about the paper. I like the investigation and there are ideas in the paper which I find nice. At the same time though, I  believe the paper could benefit from an attempt to better discern and articulate the messages of the findings. Moreover, my opinion is that by discussing too many (and many of them incomplete) topics, main (and potentially interesting) messages are \"lost\". \n\n-- Several topics discussed feel incomplete, such as (1) clustering of contexts in Sec. 3; (2) content of Sec. 4.2; (3) Last paragraph on Section 5.2 (there doesn't seem to be anything informative being said here including App E other than reporting of figures)\n\n-- The discussion on induction heads is distracting and I don't see the relevance to the rest of the paper\n\n-- I find the presentation of the paper particularly after Sec 2 confusing. There is no clear coherence between sections/subsections. Eg., not made clear how Sec. 4.2 and 4.3 fit within the story. Overall the paper would benefit from a careful read."
            },
            "questions": {
                "value": "-- Do you have an intuition/interpretation for the spiral shape? I believe I understand the point you are making on smoothness, but what does the particular shape tells us (if anything)? If nothing, then why is it emphasized?\n\n-- Any explanations on the non-spiral trends in Figs 9-11 in the appendix?\n\n-- last paragraph \"Why smoothness\" of Sec 2: Can you please elaborate why smoothness allows attention to neighboring tokens easily? Also, you discuss there about QK scores, but those involve the WQ,WK matrices which from what I understand are not considered in Sec 2 (only gram matrix of positional embeddings)\n\n-- How the clustering property of the contextual part of embeddings on per document basis is informative? Also, how would the results change based on the four sampled documents?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2630/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698903868343,
        "cdate": 1698903868343,
        "tmdate": 1699636202609,
        "mdate": 1699636202609,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Iaxr7mv6wN",
        "forum": "1M0qIxVKf6",
        "replyto": "1M0qIxVKf6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2630/Reviewer_K9yz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2630/Reviewer_K9yz"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to demystify the internal workings of transformer models by presenting a novel decomposition of the hidden states (or embeddings) into interpretable components. For a layer's embedding vectors, a decomposition is achieved which distinguishes the mean effects of global, positional, and contextual vectors, as well as residual vectors, providing insights into the input formats and learning mechanisms of transformer"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**S1**. The paper introduces a novel decomposition method that separates the mean effects of global, positional, and contextual vectors within transformer embeddings. This approach offers a fresh perspective on understanding the internal mechanisms of transformers, revealing insights into how they encode and process information.\n\n**S2**. The paper provides extensive numerical experiments on a variety of models, including Llama2-7B and BLOOM, to validate the proposed decomposition approach. These experiments include token randomization and arithmetic tasks, which demonstrate the ability of the decomposition to capture different aspects of transformer embeddings."
            },
            "weaknesses": {
                "value": "Please see the Questions below."
            },
            "questions": {
                "value": "**Section 1:**\n\nThe significance of Transformers in research is well-known, but the paper's introduction does not clarify the purpose of the proposed method. Could the authors detail how this new decomposition relates to ANOVA and previous work on positional embeddings and induction heads? Moreover, what practical benefits does this decomposition provide? The main outcomes of the experiments in both the paper and appendix also need clarification.\n\n**Section 2:**\n\n- Please define 'smoothness' in the context of your paper. It is essential to relate this to DFT and IDFT for better understanding.\n\n-  The term $|| ||_{op}$ is used but not defined. \n\n- In Equation (6) (LHS) of Theorem 1, there is a dimension mismatch; the first term is \\(T \\times T\\) and the second is \\(k \\times k\\). \n\n**Sections 3 and 4:**\n\n- The writing in these sections needs improvement. Starting with a summary of the key findings before referring to figures would enhance clarity. What are the main points of these sections?\n\n- What does \\(O(1)\\)-sparse representation by bases mean in Theorem 2?\n\n**Section 6:**\n\nThe claim of providing a \"complete picture\" is too broad. How does this research stand apart from earlier studies on positional embeddings and induction heads?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N.A."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2630/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2630/Reviewer_K9yz",
                    "ICLR.cc/2024/Conference/Submission2630/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2630/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699577254288,
        "cdate": 1699577254288,
        "tmdate": 1699640006585,
        "mdate": 1699640006585,
        "license": "CC BY 4.0",
        "version": 2
    }
]