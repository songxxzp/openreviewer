[
    {
        "id": "62SrNlTWFL",
        "forum": "gjfOL9z5Xr",
        "replyto": "gjfOL9z5Xr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3441/Reviewer_ydTt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3441/Reviewer_ydTt"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces DYVAL, a novel and flexible evaluation protocol for assessing Large Language Models (LLMs). DYVAL addresses two fundamental challenges in current LLM evaluation: potential data contamination in training data and the static nature of existing benchmarks that inadequately gauge LLMs' evolving capabilities. DYVAL dynamically generates evaluation samples using directed acyclic graphs (DAGs), allowing for controllable complexities in reasoning tasks. The authors evaluate various LLMs using DYVAL across mathematics, logical reasoning, and algorithmic problems, highlighting the importance of dynamic evaluation. They also demonstrate the effectiveness of DYVAL-generated data in fine-tuning LLMs on existing benchmarks. Key findings include inconsistencies between DYVAL and existing benchmarks, LLMs' performance decline with increasing complexity, and insights into failure patterns and prompt engineering methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. DYVAL presents an innovative approach to evaluating LLMs by dynamically generating evaluation samples, mitigating concerns about data contamination and providing a more realistic assessment of LLMs' capabilities.\n\n2. The paper conducts extensive experiments across various reasoning tasks and LLMs, offering valuable insights into LLM performance, failure patterns, and the impact of different prompt engineering methods.\n\n3. DYVAL's ability to improve LLMs' performance on existing benchmarks through fine-tuning with DYVAL-generated data demonstrates its practical utility in enhancing LLM capabilities beyond evaluation"
            },
            "weaknesses": {
                "value": "1. The claim on \"co-evolution\" is not clear. I do not quite understand what co-evolution means. It seems that the evaluation process is not dependent on the LLM, then how they are correlated from each other.\n\n2. The data contamination problem is not clear. Notably, the data generated by the proposed method is rather limited type as it can not generate narrative generation tasks and others related to common sense.  I am wondering how the existing datasets have the contamination problem. I think such a problem may not happen frequently in the logical reasoning and algorithm domains (especially, these abilities may be majorly from finetune from code and scientific papers). However, they are much easier to happen on those storytelling data.\n\n3. The potential bias may exist in the graph generation. The paper focuses on how to conduct constraints for the graph to avoid illegal ones. Nonetheless, there may be lacked of details on how the graph is generated to meet those constraints. I am concerned that the graph generation algorithms remain biased. Therefore, there will be bias in the generated text, leading to the potential issue."
            },
            "questions": {
                "value": "1. Can you clarify the concept of \"co-evolution\" in the context of DYVAL's evaluation process?\n\n2. Could you show the data contamination problem in existing datasets on the proposed problems?\n\n3. Is there a risk of potential bias in the graph generation process that could lead to biased text generation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3441/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3441/Reviewer_ydTt",
                    "ICLR.cc/2024/Conference/Submission3441/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697759251904,
        "cdate": 1697759251904,
        "tmdate": 1700333933899,
        "mdate": 1700333933899,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DG1xErIvIc",
        "forum": "gjfOL9z5Xr",
        "replyto": "gjfOL9z5Xr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3441/Reviewer_MYvY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3441/Reviewer_MYvY"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new dynamic generation of samples that can be used to evaluate or fine-tune LLMs. Roughly speaking, a sample corresponds to a DAG with controllable complexity that can be translated into a comprehensible natural language description. This translated sample and a task description can then form an evaluation task for the LLM. The proposal is to use dynamic draws of graph-informed samples to evaluate LLMs and potentially train and fine-tune them on specific tasks. Since the space of large DAGs is exponentially large, it's very unlikely to observe repetitive samples, and hence the algorithm addresses two potential flaws of static benchmarks: data contamination and saturation due to static complexity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Extensive experiments are conducted. \n- Graph-based notions of complexities can be used as a means to control the compositional complexity of the examples.\n- Address data contamination and static complexity of the benchmarks."
            },
            "weaknesses": {
                "value": "- A common challenge associated with this framework is the need to manually specify a problem as a computation graph with valid constraints. This requirement is only understandable if LLM is intended to acquire specific skills written in these formats. \n- Before reading this paper, I believed that generating a large number of mathematical problems of specific types and evaluating LLMs on them was primarily for debugging specific LLM capabilities, such as compositionality, rather than as an evaluation framework. I'm not sure if these types of problems are fundamental questions about LLMs. In fact, prior studies, such as those by Dziri et al., have already highlighted the limitations of transformers in these settings, using a very similar setup for demonstration.\n- It's not clear if LLMs are losing some skills when fine-tuned on DyVal as DyVal examples and the chosen existing benchmarks are from very similar domains. The generalization of the fine-tuned model on DP is interesting though.\n\nRecommendation:\nAs a person who has worked on dynamic adversarial data collection, or more broadly dynamic benchmarks, I think your review of this literature is underestimating their importance. In fact, in dynamic adversarial data collection annotators can be provide interesting problem instances hard to find in static benchmarks and even hard to manually specify as a DyVal task. So, I encourage you to include a better review of these works. If you are concerned with the human-in-the-loop, I believe the recent theoretical frameworks of dynamic benchmarking are still valid if humans are replaced by generative tools which you may consider mentioning. So, I encourage you to revisit page 2 paragraph 1 at your discretion. \n\nOverall, I believe that in the era of LLMs, we should explore new methods of evaluation, and this paper's framework might be one of them. The ICLR audience may find this work interesting, so I will maintain a positive rating despite the concerns I have."
            },
            "questions": {
                "value": "Feel free to respond to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3441/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3441/Reviewer_MYvY",
                    "ICLR.cc/2024/Conference/Submission3441/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698633051369,
        "cdate": 1698633051369,
        "tmdate": 1700177130832,
        "mdate": 1700177130832,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mkhIBGrSpw",
        "forum": "gjfOL9z5Xr",
        "replyto": "gjfOL9z5Xr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3441/Reviewer_sT7J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3441/Reviewer_sT7J"
        ],
        "content": {
            "summary": {
                "value": "Evaluating LLMs is important in current literature as LLM has boosted significant performance in various tasks. This paper proposes a new evaluation method that evaluates the performance of various LLMs in their reasoning abilities by generating dynamic evaluation samples. Results show that several tasks are still hard for current LLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The motivation of this paper is clear. As many LLMs tend to memorize static data for evaluation, this paper proposes a dynamic approach to avoid this kind of problem.\n\n2. The idea of generating tasks with different difficulties in a DAG style sounds interesting.\n\n3. The problem is clearly described with sufficient notations and examples.\n\n4. Experiments are conducted in various aspects, including 7 reasoning tasks, 1 human evaluation, on about 8 well-known LLMs. Fine-tuning experiments are also conducted to demonstrate that the LLMs' ability in learning to reason."
            },
            "weaknesses": {
                "value": "1. The title is somewhat misleading. The evaluation tasks in this paper are mostly about reasoning on maths, logic, algorithms, etc. However, the title reflects no information about this point. The abstract could be also clearer if this point can be mentioned earlier.\n\n2. For the fine-tuning results in Section 5, I wonder when these LLMs are fine-tuned for the reasoning tasks proposed in this method, will the general abilities be influenced? Or to what extent will they be influenced? \n\n3. As the samples for evaluation are dynamic, the comparison may be unfair when the generated data are different in different evaluation stages."
            },
            "questions": {
                "value": "1. Can you discuss the influence of fine-tuning on reasoning tasks on the general language understanding ability?\n\n2. Can you provide how to fairly evaluate the different models, especially if this evaluation method is released as a public leaderboard?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3441/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3441/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3441/Reviewer_sT7J"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761255353,
        "cdate": 1698761255353,
        "tmdate": 1700455012093,
        "mdate": 1700455012093,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9mU1CVhcbM",
        "forum": "gjfOL9z5Xr",
        "replyto": "gjfOL9z5Xr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3441/Reviewer_iwJv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3441/Reviewer_iwJv"
        ],
        "content": {
            "summary": {
                "value": "Presents a general framework to generate certain \"graph-based\" evaluation tasks for LLMs randomly, implements 7 example tasks, and presents and analyzes empirical results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1. Simple, yet flexible framework.\nS2. Dynamic task generation with controllable complexity\nS3. Extensive evaluation of selected LLMs / prompting strategies for seven simple reasoning tasks.\n\nOn S1. The general idea of the proposed benchmarking framework is to generate tasks that can be described by a directed acyclic graph. This includes \"compute graphs\" (e.g., evaluate a numerical expression or perform logical reasoning) or \"data graphs\" (e.g., determine connectivity between vertices). The framework takes care of graph generation, task implementations add contraints, labels, solutions, and verbalization. This is a very natural approach and (most probably) how many of the existing benchmarks of this form are generated in the first place. Such a framework may increase usability, especially when many tasks were implemented in it.\n\nOn S2. Tasks are generated automatically and with varying complexity (mainly graph size). Again, this is a simple, very natural approach. Here the framework proposed by the paper may make comparative evaluation across a range of tasks more feasible, as all share the same notion of \"complexity\".\n\nOn S3. The paper reports performance results on simple computational tasks (such as evaluating simple equations). Generally, all models break down when complexity goes up so that the benchmark may be used as a way to evaluate progress. Also, the performance reported on these simple tasks sometimes contradict performance results published on related, static benchmarks."
            },
            "weaknesses": {
                "value": "W1. Certain computational tasks only\nW2. Discussion of related work / results lacking\nW3. Limitations in generated graphs\nW4. Code/data availability unclear\nW5. Limited insight of experimental study\n\nOn W1. By the nature of the benchmark, it focuses on problems that can be expressed as (currently small) compute graphs or data graphs and are somewhat artificial. It only tests a very limited field of LLM functionality.\n\nOn W2. There are benchmarks for all of the tasks that are implemented in this framework already. The paper states that its performance results contradict the ones on some of these benchmarks, but does not say which ones and, perhaps more importantly, does not provide any insight into why this is the case. Also, the data generation strategies used by existing benchmarks are not discussed. Finally, to what extent the benchmark can be used to really do new things (beyond existing benchmarks) is not discussed.\n\nOn W3. First, the paper focuses solely on DAGs, but it's unclear why this is done for data graphs (e.g., reachability, max-sum). Second, it's unclear whether graph size is the right complexity measure. E.g., for reachability appears easier is source and target are neighbors, no matter how large the graph. Finally, the system does not seem to generate balanced datasets. For example, the paper reports in the appendix that the proportion of true answers for reachability is not controlled, leading to \"paradoxical\" results.\n\nOn W4. It's important for benchmarking papers such as this one to make all code, datasets, prompts, results, etc. public. The paper currently does not provide any ressources (or, at least, I did not see them).\n\nOn W5. The insight that can be drawn from the experiments is somewhat limited. I do not count this against the paper, however. It does show exposed limitations of LLMs and prompting strategies, and it does show that the generated tasks are useful for fine-tuning.\n\nMinor points:\n\nI am not sure how useful the comparison to human performance is. Clearly, all of the tasks can be solved \"easily\" by humans, it's just a pain to do so."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699018229752,
        "cdate": 1699018229752,
        "tmdate": 1699636296099,
        "mdate": 1699636296099,
        "license": "CC BY 4.0",
        "version": 2
    }
]