[
    {
        "id": "gRDT3dVUvh",
        "forum": "CtOA9aN8fr",
        "replyto": "CtOA9aN8fr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5051/Reviewer_orZi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5051/Reviewer_orZi"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on improving the training and data efficiency of CLIP-style models by pushing the limits of pruning large-scale multimodal datasets. By adapting the pruning rate to the complexity of different concepts within the dataset, the authors are able to reduce training costs to a quarter of regular training. They outperform the LAION-trained OpenCLIP-ViT-B/32 model on ImageNet zero-shot accuracy by 1.1 percentage points while using only 27.7% of the data and training compute. Additionally, they achieve a new state-of-the-art ImageNet zero-shot accuracy and competitive performance on 38 evaluation tasks in the DataComp Medium benchmark. The findings demonstrate the potential of pruning methods for improving the efficiency of training multimodal models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The method proposed in this paper exhibits a certain degree of novelty.\n2. The paper extensively validates its findings using a large-scale experimental dataset, although experiments with even larger models were not performed.\n3. The practical applicability of the approach presented in this paper suggests its value in real-world scenarios."
            },
            "weaknesses": {
                "value": "1. We cannot determine if the current data is effective for training ViT-B-16 and ViT-L-14 models.\n2. The majority of the experimental evaluation metric is Imagenet zero-shot top1. Is there any bias in the algorithm towards Imagenet data?\n3. Detailed results for each dataset in VTAB are not provided."
            },
            "questions": {
                "value": "In this paper\uff0c the author scales SSP-Pruning to web-scale datasets and demonstrates that the pruning criterion can also transfer to the DataComp benchmark.\nQ1: In Fig. 1, the author shows they reduced the LAION-CAT440M to 166M and improved the zero-shot performance on Imagenet, can you provide more results? such as zero-shot transfer and linear probe performance on different datasets.\nQ2: Does the robustness of model training change when the dataset size is reduced? Please provide robustness evaluation results.\nQ3: The difference between DBP and SSP-Pruning is the choice of how many examples are taken from each cluster. It seems that innovation has some shortcomings\nQ4: In Fig.3, 222M performs worse zero shot accuracy than 166M, please give a reasonable explanation.\nQ5: Why weren't models of the same size, such as CLIP ViT-L-14, OPENCLIP LAION400M ViT-L-14, EVA ViT-L-14, and DINOv2 ViT-L-14, used for clustering comparisons?\nQ6:"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5051/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698683490075,
        "cdate": 1698683490075,
        "tmdate": 1699636494983,
        "mdate": 1699636494983,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Eh7f1NejHJ",
        "forum": "CtOA9aN8fr",
        "replyto": "CtOA9aN8fr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5051/Reviewer_mnGt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5051/Reviewer_mnGt"
        ],
        "content": {
            "summary": {
                "value": "This work scales SSP-Pruning to web-scale datasets, investigate how the complexity of different concepts within a dataset can be used for pruning, and report further improvements over regular SSP-Pruning. They evaluate it with CLIP pre-training and report the results on classification task: it shows data efficiency in terms of pre-training."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is well structured and the motivation is clear. It is interesting to study the data efficiency of CLIP pre-training to reduce the computational resources."
            },
            "weaknesses": {
                "value": "1. The biggest concern I have is about the technical novelty. The main idea is based on SSP-Pruning. Therefore, the technical contribution is very trivial. \n2. This work only reports the results on image classification. How about the zero-shot results of CLIP in retrieval tasks, such as COCO/Flickr30k image-to-text and text-to image retrieval? The zero-shot retrieval performance is also very important to measure the quality of pre-training and the effectiveness of the proposed method. The paper mentions about the retrieval task and shows some results in Figure 3 but I am not sure the implementation details: which dataset do they use? It would be helpful to report the results on standard benchmark (COCO/Flickr30k). \n3. The method involves CLIP-score filtering to curate a new dataset to train CLIP. I am unsure if such involvement is fair, as the filtering itself is similar to distilling some knowledge from a well-trained CLIP. Then, the new CLIP is somehow learning from the well-trained CLIP on large-scale datasets. Therefore, it may not be fair to claim the data efficiency for the proposed method."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5051/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698739423840,
        "cdate": 1698739423840,
        "tmdate": 1699636494882,
        "mdate": 1699636494882,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "M3yUVstU89",
        "forum": "CtOA9aN8fr",
        "replyto": "CtOA9aN8fr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5051/Reviewer_u5qD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5051/Reviewer_u5qD"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a dataset pruning method broadly applicable to web scale datasets such as LAION. The authors build upon important prior work at the ImageNet scale (SSP-Pruning, Sorscher et al., 2022), which attempts to rebalance a dataset by clustering, and then sampling datapoints inversely proportional to their cluster centroids. The authors improve upon this procedure by noting that clusters have varying degrees of importance, both as a result of the average distance of points from the centroid as well as the average distance of the centroid from other centroids, and determine the proportion of samples to be pruned proportional to the importance. The authors demonstrate the supremacy of this method, DPB, through an array of experiments that the pruned datasets jointly save on compute while increasing overall performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper's extensive analysis of their new method across multiple datasets and benchmarks makes a compelling case for the high performance of their method. The dimensions of ablations, and the high level takeaways from those ablations in the results section are clear -- DBP outperforms SSP Pruning, outperforms CLIP filtering, DBP outperforms SemDeDup alone, and Deduplicaiton is crucial to the success of the method. These takeaways make the contribution quite compelling."
            },
            "weaknesses": {
                "value": "The core weaknesses concern clarity of the results, and the overall difficulty in connecting the many different - at times seemingly disparate - results that are thrown together in the narrative.\n\nFor one, the paper begins by proposing an improvement to SSP, but defers any result about the paper to a small table in the appendix, for a single model and single dataset -- it would help to do a more comprehensive evaluation to make clear the improvement being proposed. The authors attempt to briefly make a broader connection that their work is akin to density based pruning -- this is where the relative lack of optimization over the number of clusters -- which very much controls how the density is approximated -- is puzzling. Indeed, the 5 point plot of Figure 6-d leaves much to want, where we are led to believe 500 is the best, because it is better than 100 (?) and 10000. More ablations here would help clarify things. In general, more time assessing Density based pruning methods (e.g. for a datapoint, if its k nearest neighbors approximate it well, it can be discarded) would significantly improve the narrative of the paper. \n\nOn the other hand, the quadratic program, though interesting and unexpected in a paper of this type, seems like overkill -- assigning the maximum number of points to each cluster whose expected number of samples is above the maximum, and redistributing the remaining samples proportionally among the remaining clusters (and repeating) seems like it would achieve what the authors are seeking to do (if not exactly what the QP does) without the overhead of a QP solver, and all the space in the paper it consumes. \n\nThe discussion section would benefit from a more clear discussion of where the shortcomings of the method are relative to the other methods presented in the paper. For example, DBP does a tiny bit better than TMARS on Imagenet, but more significantly worse on Imagenet Dist and Average but a discussion of the latter point is entirely missing. Whether pruning in a density based manner affects model robustness seems like a subject at least worth touching on. Same for the retrieval plot in Figure 4 -- why is it that SemDeDup does worse than LAION-440B, but DBP catches up? An analysis of what is happening here would help understand the paper considerably. \n\nBeyond that, the many different numbers in the paper -- as a result of different filtering models (CLIP-B16 or CLIP-L/14), differing number of epochs, differing initial dataset size (LAION-50M or 440M) -- make the results very difficult to connect and piece together across tables and figures. If some consistency across variables were afforded -- or at least enough ablations such that there is an extra column in each table connecting the numbers in other tables to that table, it would make the information much easier to process."
            },
            "questions": {
                "value": "If the intuition is that it is density based pruning, why only 500 clusters? The 5 point plot of figure 6-d suggests that clusters between 100 and 10000 should be better explored since there are sharp peaks in that range. \n\nFigure 5: Why not use concatenate CLIP's text + image embedding?\n\nHow is it that SemDeDup does worse than LAION-440M for VTAB, but DBP does better?\n\nA notable decrease in performance on ImageNet dist shifts and Retreival tasks -- does the DBP type of pruning hurt generally hurt robustness? \n\nWhy use CLIP-B16 Score for Figure 3, but CLIP-L/14 Score for Table 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5051/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699523018027,
        "cdate": 1699523018027,
        "tmdate": 1699636494795,
        "mdate": 1699636494795,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aH8qbuGhvN",
        "forum": "CtOA9aN8fr",
        "replyto": "CtOA9aN8fr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5051/Reviewer_7m8Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5051/Reviewer_7m8Z"
        ],
        "content": {
            "summary": {
                "value": "This paper seeks to prune large-scale multimodal datasets (e.g., LAION) for training CLIP-style models for training and data efficiency. Building upon SSP-pruning, Density-Based-Pruning picks the number of samples per cluster based on the overall complexity of a particular cluster, and achieves state-of-the-art results on LAION."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThis paper proposes Density-Based-Pruning to improve data efficiency on large-scale multimodal datasets.\n2.\tThis paper conducts detailed hyperparameter selection experiments, which made the experimental results more convincing."
            },
            "weaknesses": {
                "value": "1.\tThe related work section contains excessive content, and there is a considerable amount of duplication. Besides, it is better to introduce \u201ccoreset selection[1-3]\u201d, as it is closely related to your work.\n\n2.\tMissing some baselines for comparison, including random selection and other data pruning methods [3-4].\n\n3.\tThe description in the methods section is very confusing.\n[1] Moderate coreset: A universal method of data selection for real-world data-efficient deep learning, ICLR 2023\n[2] Large-scale Dataset Pruning with Dynamic Uncertainty, arxiv23\n[3] RETRIEVE: Coreset Selection for Efficient and Robust Semi-Supervised Learning, NeurIPS 2021\n[4] Too Large; Data Reduction for Vision-Language Pre-Training, ICCV\n2023"
            },
            "questions": {
                "value": "1.\tIn Fig. 4(right), more sample points are needed for convincing conclusions.\n2.\tIn Table 2, it would be better to include SSP-Pruning as a baseline for comparison.\n\n3.\tThe tasks are relatively simple (ImageNet zero-hot). Could we compare DBP with other baselines on different tasks?\n4.\tIn the methods section, the filtering pipeline includes deduplication, CLIP score filtering, and Density-Based Self-Supervised Prototypes pruning. However, there is no description about CLIP score filtering. The description about deduplication does not provide me with any useful information."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5051/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5051/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5051/Reviewer_7m8Z"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5051/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699675691859,
        "cdate": 1699675691859,
        "tmdate": 1699675691859,
        "mdate": 1699675691859,
        "license": "CC BY 4.0",
        "version": 2
    }
]