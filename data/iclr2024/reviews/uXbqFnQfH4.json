[
    {
        "id": "O8muXcgB6I",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8707/Reviewer_AiMo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8707/Reviewer_AiMo"
        ],
        "forum": "uXbqFnQfH4",
        "replyto": "uXbqFnQfH4",
        "content": {
            "summary": {
                "value": "This paper proposed a new algorithm based on bilevel optimization. The inner level is an weighted MGDA and the upper level is OT make perference more alginable."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method is validated both on extensive experiments and theory."
            },
            "weaknesses": {
                "value": "see questions."
            },
            "questions": {
                "value": "1. Why we do not use a Pareto set learning (PSL) model, which is now a very mature techniques., (see COSMOS[1], Lin[2]) to solve the considered problems. Since PSL is now believed can run very fast and find the whole PS/PF for fairness problems. \n\n2. The considered simplex is m-1 not m. So you should use the notation $\\Delta^{m-1}$/$\\Delta^{n-1}$ in the paper. \n\n3. Why considered OT? Can a simple JS/KL/TV divergence also work?\n\n4. You claim EPO can not deal non-smooth and dis-continuous problem. But I think the proposed method also have such issues. When the solution is not smooth. It is actually impossible to apply any gradient-based method (to the best of my knowledge).\n\n5. As far as I have implemented, EPO works perfectly on ZDT1. The main problem EPO is it is slow. However, the EPO you have implemented only find a single solution. I suggest the author to re-implement EPO. \n\n6. According to my understanding, LS can work perfectly on MNIST/FMNIST problem since their PFs are almost convex. So I am skeptical to the results in Table 2. \n\n7. The Eq.4, using a weighted version of MGDA, do not have a direct meaning. It is hard to understand its result of Eq4 since the result of MGDA is unpredictable. So a weighted version of MGDA will not lead to very meaningful solutions (according to my try several month ago).  \n\n8. I am a curious about the distribution of final solutions (similar to 7).\n\n9. I am actually concerned about the design of the algorithm. I have to say, I think the algorithm is over-designed. Since MGDA does not have a guarantee to find a specific solution. I think COSMOS may be a better choice for the design in the inner loop.\n\n10. I have to say, many of the baselines are not what the author claims. Many of the baselines, (including 5 EPO) actually have much better performance than the author claims. \n\n11. For many-objective problems, MGDA is too slow since calculating m gradients seems not a main-stream method. In addition, how is the OT implement. As far as I know, sinkhorn alg for solving the OT is still slow. \n\n\n[1] Pareto Set Learning for Expensive Multi-Objective Optimization.\n\n[2] Scalable pareto front approximation for deep multi-objective learning."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8707/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8707/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8707/Reviewer_AiMo"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8707/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697172670991,
        "cdate": 1697172670991,
        "tmdate": 1700449958943,
        "mdate": 1700449958943,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NYxzyLD6Ea",
        "forum": "uXbqFnQfH4",
        "replyto": "uXbqFnQfH4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8707/Reviewer_EZcQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8707/Reviewer_EZcQ"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a novel Multi-objective multi-solution Transport (MosT) method to find a small set of diverse Pareto solutions for problems with a large number of objectives. MosT is formulated as a bi-level optimization problem, where the upper-level problem is to determine different objective weights for each solution via optimal transport (OT) based on their current performance, while the lower-level problem is to find a Pareto solution for each subproblem with weighted objectives. MosT can also be generalized to tackle multi-objective optimization problems with few objectives via random objective interpolation.\n\nThis work also provides theoretical analysis to prove MosT can find a set of Pareto solutions via solving the bi-level optimization problem. Experiments show MosT can achieve promising performance on different synthetic and application problems."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ This work is well-organized and easy to follow.\n\n+ The *small solution set for a large number of objectives* setting is important for many real-world applications. This work is a timely contribution to an interesting yet under-explored research direction.\n\n+ The proposed MosT method can achieve promising performance on different synthetic and application problems."
            },
            "weaknesses": {
                "value": "I enjoy reading this paper but also have many major concerns on the proposed MosT method, which makes it hard for me to vote for acceptance. My current rating is more like a weak reject (4). \n\n**1. Motivation**\n\nSome crucial design choices for MosT are not well motivated, and more clear discussions are needed.\n\n- **Weighted Objectives:** The key of MosT is to assign different weights to the objectives for different solutions. What is the relation of the Pareto sets for the problems with weighted objectives (1) with the Pareto set of the original unweighted problem? In addition, It is also unclear what makes the weighted problems good for diversity. On page 4, it mentions that the weighted problem can move a small gradient from the origin in Eq. (5). But the whole picture of how the (optimal) weighted problem can lead to a set of \"different but complementary and balanced\" solutions that best cover all objectives is not clear to me.\n\n- **Optimal Weights and OT:** If there is a set of optimal weights that can lead to a set of optimally distributed solutions, what properties such a set of weights should have? Is it unique? Why such optimal weights (if they exist) can be found by OT (e.g., eq.(2)) is also not clearly discussed.\n\n- **MGDA for Weighted Problems:** MosT uses MGDA to find a Pareto solution for each problem with different weighted objectives. However, MGDA can find any Pareto stationary solutions and the location is not controllable. For a non-trivial problem, the Pareto set could be a large $n-1$ dimensional manifold with a large $n$ for each weighted problem, where MGDA can lead to any possible solution on the manifold. Why the (uncontrollable) solutions found by MGDA for each weighted problem (1) can be guaranteed to be complementary and evenly distributed? \n\n**2. Gap between MosT and the Metrics**\n\nThere are different and not unique metrics to measure the quality of the obtained solution set. It is unclear why the solution set found by MosT could be optimal for a given criteria (e.g., hypervolume). \n\n- **Hypervolume for Few Objectives $(n << m)$:** For problem with few objectives (e.g., 2-3), if the goal is to maximize the hypervolume, why not directly use gradient-based hypervolume maximization to find the solution set [1,2]?\n\n- **Large Number of Objectives $(n >> m)$:** For problems with a large number of objectives, this work let each objective pick the best solution out of all $m$ obtained solution, and then calculate the overall performance. In this case, it seems that, at the end of optimization, the objectives covered by different optimal solutions should be mutually exclusive. For example, for a problem with 300 objectives and 3 solutions, solution 1 covers 95 objectives, solution 2 covers 103 objectives, and solution 3 covers 102 objectives, and hence each objective is covered by a single solution. In this case, the optimal solution should not cover other conflicted objectives that it does not work the best, since it might lower the solution's performance on the currently covered objectives.  Therefore, it is unclear why \"During later stages, ..., every objective has to be covered by sufficient models (solutions)\".\n\nIn addition, the final metric we care about is the mean(std) of average performance across all objectives (e.g., Table 2). Once the groups of objectives covered by different solutions are determined, why not just use simple uniform linear scalarization to optimize all objectives for each group?\n\n**3. Time Complexity**\n\n- MosT formulate the original multi-objective optimization as iterative bi-level optimization, which involves solving a OT problem and $m$ MGDA problems at each iteration. What is the time complexity of MosT? Will the OT solver has high computational overhead?\n\n- For MGDA with large $n$ (e.g., 205), even with the Frank-Wolfe algorithm, will the run time be very long for solving eq. (5)? Will it be very hard to find a valid gradient direction to optimize all 205 objectives at the same time?  \n\n**4. Extension to Few Objective Case ($n << m$)**\n\nFor problems with few objectives (e.g., 2 or 3), this work uses $n^{\\prime}$ dense interpolations among original objectives to create a large number of objectives, and then applies MosT to find a set of diverse solutions.\n\n- If $n^{\\prime}$ is large, will this approach lead to high computational overhead for problems with few objectives?\n\n- The weights of interpolation are uniformly drawn from a Dirichlet distribution over the simplex. However, uniform weights usually do not represent uniform solutions on the Pareto front. If $n^{\\prime}$ is small, will it hurt the performance? \n\n**5. Experiment**\n\n- **Toy Problems:** To my understanding, the algorithms considered in Section 5.1 are all for unconstrained multi-objective optimization problems. However, the ZDT problems have box constraints ($\\theta \\in [0,1]^d$), and their Pareto sets are all on the boundary (the constraints are activated). Will these algorithm's performances be significantly affected by the constraints? In addition, according to [3], EPO (if with a suitable reference point) and SVGD have much better performance on ZDT1-3. What is the reason for the poorer performance reported in this paper?\n\n- **More Problem with Large Number of Objectives:** The main motivation of this work is to tackle the *small solution set for a large number of objectives* problem. However, the experiment only has a single real-world application with many objectives on the federated learning application with FEMNIST. The other experiments are on synthetic problems or problems with few objectives. It is important to know MosT's performance on more realistic application problems with a large number of objectives, especially those other than federated learning.  \n\n- **Runtime Comparison:** Please report the runtime for different algorithms on all experiments."
            },
            "questions": {
                "value": "- Please address the concerns raised in the above weaknesses.\n\n- When citing multiple papers, it is better to put them in chronological order.\n\n**Reference**\n\n[1] Hypervolume indicator gradient ascent multi-objective optimization. International Conference on Evolutionary Multi-Criterion Optimization 2017.\n\n[2] Multi-Objective Learning to Predict Pareto Fronts Using Hypervolume Maximization. arXiv:2102.04523.\n\n[3] Profiling Pareto Front With Multi-Objective Stein Variational Gradient Descent. NeurIPS 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8707/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760178930,
        "cdate": 1698760178930,
        "tmdate": 1699637091791,
        "mdate": 1699637091791,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GRvZOPyQ8R",
        "forum": "uXbqFnQfH4",
        "replyto": "uXbqFnQfH4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8707/Reviewer_SZEd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8707/Reviewer_SZEd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to generate $m$ solutions over the Pareto Set of a MOO to maximize diversity among the solutions, especially in the case where the number $n$ of objectives are much larger than $m$. \n\nTo enforce diverse solutions, this paper uses an Optimal Transport(OT)-based approach to learn the weights of different objectives towards evaluating different solutions such that the solutions are diverse and then uses MGDA to solve the MOO.\n\nThe advantages of the method have been illustrated over several experiments involving toy and real datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The main strength of the method is summarized nicely after eq (6) in the paper. Incorporating the optimal transport between uniform distribution over $1$ to $n$, and $1$ to $m$ effectively reweights the objectives before running vanilla MGDA. Because of OT, these weights $\\Gamma_{i,j}$ are larger where $L_i(\\theta_j)$ are small, that is, when two objectives $i$, and $j$ are similar ($L_i(\\theta_j)$ small), large $\\Gamma_{i,j}$ decreases the influence of the gradients of such objectives over MGDA compared to objectives which are different from each other. In the absence of $\\Gamma_{i,j}$ the solution of MGDA gets biased towards the objectives with small gradients and may lead to smaller diversity. \n\n2. This nice intuition works well over several applications. The effect of incorporating OT is nicely demonstrated in Figure 4 which shows the diversity in the selected objectives."
            },
            "weaknesses": {
                "value": "The main weakness of the paper is the lack of theoretical support. **While the main claim of the paper is that it learns diverse solutions over Pareto set when the number of objectives is large, Theorem 1 and 2 only show convergence of the methods to any $m$ Pareto solutions which sheds no light on the diversity of the solutions.** The authors acknowledge this in the line just before Theorem 2. \n\nThe convergence results follow straightforwardly from existing literature as it just combines the proof of convergence of IPOT and MGDA. Moreover, it just characterizes the complexity of the outer loop whereas the inner loop (especially IPOT) can be quite computationally expensive. \n\nWithout proper theoretical justification, the claimed advantages of MosT, diverse solutions, and computationally cheaper, over other methods seem weak."
            },
            "questions": {
                "value": "1. Why is $\\epsilon$ needed in Algorithm 1?\n\n2. I would expect the hypervolume of the solutions to be higher if one runs MGDA with diverse weight vectors $m$ times instead of random seeds. Could you compare MosT with such a version of MGDA?\n\n3. While comparing to previous work it is stated in Page 2 that the number of preference vectors required to profile the Pareto set can be exponential in the problem parameters which might be discouraging when the number of objectives $n$ is large. When $m$ and/or $n$ are large, IPOT (Line 4 in Algorithm 1) needs to be solved for high dimensions which can still be computationally large. How does this paper compare with the previous literature on this aspect?\n\nMinor Points (do not affect my score):\n1. Typo in Page 6: MDGA --> MGDA\n2. The full form of the abbreviations should be written where they are first introduced. \n3. ``which gives higher priority to models selecting objectives at\nthe earlier stages and then transits to a higher priority of objectives selecting the best models.\" - Why is this desirable?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8707/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698892040090,
        "cdate": 1698892040090,
        "tmdate": 1699637091686,
        "mdate": 1699637091686,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GFsZcOPHtX",
        "forum": "uXbqFnQfH4",
        "replyto": "uXbqFnQfH4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8707/Reviewer_bAug"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8707/Reviewer_bAug"
        ],
        "content": {
            "summary": {
                "value": "The authors introduced a multi-objective multi-solution transport approach to optimizing multiple objectives using multiple solutions, which aims to achieve diverse trade-offs among objectives by treating each solution as a domain expert. The authors stated that the approach addresses cases where the number of objectives greatly exceeds the number of solutions and demonstrates superior performance in applications like federated learning, fairness-accuracy trade-offs, and standard multi-objective optimization benchmarks, providing high-quality, diverse solutions that cover the entire Pareto frontier. Moreover, the authors stated that the approach \u201caims to find m Pareto solutions (models) that achieve diverse trade-offs among n optimization objectives\u201d."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "NA"
            },
            "weaknesses": {
                "value": "The literature review does not provide a cohesive and structured presentation, and fails to accurately acknowledge established approaches and terminology within Evolutionary Computation (EC). This indicates a potential oversight in acknowledging state-of-the-art methods in EC such as the author do not properly reference existing work on this topic from EC. The approach lacks novelty and has been previously explored in the literature. The description of the approach and the literature review is not presented in a clear and understandable manner. The authors do not provide any new insights into the method, key experimental setup is missing (see section 5.1 Experimental setting). The solutions produced in Section 5 \u201cMOST APPLICATIONS\u201d are not clearly described and it is not clear that there is any significance to the results taking into account the benchmarks are insufficiently small and inadequate for comprehensive evaluation. The paper's contributions lack substantial significance and originality, and primarily represent incremental progress."
            },
            "questions": {
                "value": "How does this approach contribute to achieving diverse trade-offs among objectives? Can you provide more details about the theoretical foundation?\nCan you provide a more detailed comparison between the MosT approach and existing state-of-the-art methods in multi-objective optimization?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8707/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699513812625,
        "cdate": 1699513812625,
        "tmdate": 1699637091580,
        "mdate": 1699637091580,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "V47Mg3W3nD",
        "forum": "uXbqFnQfH4",
        "replyto": "uXbqFnQfH4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8707/Reviewer_YDRF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8707/Reviewer_YDRF"
        ],
        "content": {
            "summary": {
                "value": "This articled presented a multi-objective multi-solution transport framework aiming to find the solutions that achieve diverse trade-offs among n optimization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This article explored the feasibility of exploring a high dimensional Pareto frontier where there needs significant contribution is needed.\nThe authors framework had theoretically converges to a number of solution by optimizing the objectives and optimal transport.\nApplied the framework to some of the ML problems such as federated learning, fairness-accuracy trade-offs, some other multi-objective optimization benchmark problems.\nEmpirical articulation of convergence analysis"
            },
            "weaknesses": {
                "value": "Improvements and contributions towards federated learning would ensemble this article to a different altitude"
            },
            "questions": {
                "value": "Interesting analysis on n<<m in section 5.4. But the number of objectives is set as 2. Are there experiments conducted with more than 2 objectives? Did the solution converge when n increased to 3 or 5?\nWhat type of solver used to run needs to be detailed out\nKeen to understand federated learning in detail, how does the clients receive training as it is very diverse and also what's the effect of number of local sample (vi) in such scenario?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8707/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699586422483,
        "cdate": 1699586422483,
        "tmdate": 1699637091467,
        "mdate": 1699637091467,
        "license": "CC BY 4.0",
        "version": 2
    }
]