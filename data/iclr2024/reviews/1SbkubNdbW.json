[
    {
        "id": "HiO1OYVHot",
        "forum": "1SbkubNdbW",
        "replyto": "1SbkubNdbW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1183/Reviewer_EAeB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1183/Reviewer_EAeB"
        ],
        "content": {
            "summary": {
                "value": "The manuscript analyzes the potential risk of increased privacy leakage associated with traditional positive Label Smoothing in the context of Model Inversion Attacks. Additionally, it provides an analysis on how utilizing negative values can counter this risk."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors have for the first time considered the potential privacy leakage risks associated with the use of positive label smoothing under model inversion attacks, and have provided insights into the privacy benefits that may arise from using negative label smoothing. Additionally, the authors have presented a framework for analyzing the exposure of privacy in model inversion attacks due to label smoothing, and have examined how label smoothing may affect the sampling and optimization phases of model inversion attacks. Furthermore, the authors have provided a geometric intuition in the EMBEDDING SPACES to explain this phenomenon, all of which are relatively novel contributions. The quality of writing in the article is excellent, the presentation is clear, the experiments are comprehensive, and the work holds significant importance."
            },
            "weaknesses": {
                "value": "1. The paper lacks a theoretical analysis framework. The authors claim that negative label smoothing performs better than state-of-the-art defenses, but in fact, it only performs better under model inversion attacks within the analysis framework of this paper. However, privacy protection mechanisms are often capable of resisting multiple types of attacks (or even arbitrary attacks), and it is still unknown whether the approach of negative label smoothing is effective in protecting privacy against other types of attacks.\n\n2. It is generally considered that achieving both privacy and efficiency in Euclidean space through privacy protection mechanisms is an NP-hard problem. The paper does not discuss whether the performance improvement of using negative label smoothing is significantly less than that of positive label smoothing. It is noted that when negative label smoothing was previously proposed, the range of parameter choices was from negative infinity to 1. In this case, good performance does not mean that always choosing negative label smoothing will result in good performance. The authors do not discuss in the article how the performance of using negative label smoothing compares to that of positive LS or even not using this regularization measure at all, as well as whether negative LS is not applicable in many tasks."
            },
            "questions": {
                "value": "1. Can an analysis be provided comparing the performance of using negative label smoothing to the performance under positive label smoothing?\n\n2. Are there any known vulnerabilities introduced by using negative label smoothing?\n\n3. How generalizable are the findings of this paper? Are the observed effects of label smoothing on privacy specific to the datasets and models used in the experiments, or can they be applied to other domains and architectures as well?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This article might inspire people to use model inversion attacks to target some existing models trained with positive label smoothing."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1183/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1183/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1183/Reviewer_EAeB"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1183/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698306422130,
        "cdate": 1698306422130,
        "tmdate": 1699636044979,
        "mdate": 1699636044979,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7jO4Ojr9sd",
        "forum": "1SbkubNdbW",
        "replyto": "1SbkubNdbW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1183/Reviewer_uLpV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1183/Reviewer_uLpV"
        ],
        "content": {
            "summary": {
                "value": "This paper studies how label smoothing (during model training) can affect the performance of the model inversion attacks.\nSpecifically, the authors find that a positive label smoothing factor would facilitate the inversion attacks, while a negative factor would suppress the attacks.\nThis phenomenon underlines the importance of delving more into factors that influence machine learning privacy leakage."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper reveals a novel phenomenon that the positivity/negativity of label smoothing factors can affect privacy leakage in model inversion attacks. Actually, the mechanism behind this can also be explained as the robustness-privacy trade-off: [r1] first find that label smoothing with positive factor can improve adversarial robustness, and [r2] first find that there is a trade-off between adversarial robustness and membership inference attacks. Nevertheless, although the relationship between label smoothing and model inversion attacks is forecastable based on the aforementioned works, this paper is the first that empirically demonstrates the relationship. As a result, I think the results of this paper are fundamental and important.\n\n2. The paper is easy to follow and provides sufficient experiments details for reproducing its results.\n\n\n**References:**\n\n[r1] Shafahi et al. \"Label smoothing and logit squeezing: A replacement for adversarial training?\" arXiv 2019.\n\n[r2] Song et al. \"Privacy risks of securing machine learning models against adversarial examples\". CCS 2019."
            },
            "weaknesses": {
                "value": "1. In Section 3 and Figure 1, the authors provide an intuitive explanation of why label smoothing can affect model inversion attacks based on a simple experiment on 2D data. The explanation is based on the hypothesis that a more clear decision boundary would make training data less likely to be leaked through model inversion attacks. This hypothesis is odd (at least for me) and I think the authors may need to put more effort into explaining why the hypothesis makes sense.\n\n\n2. Negative-factor label smoothing would result in the smoothed label no longer a probability simplex. I think the authors may need to justify why this type of label smoothing is appropriate.\n\n\n3. Suggestion: As explained in Section \"Strengths\", the found phenomenon can be seen as a robustness-privacy trade-off. Since this paper finds that negative-factor label smoothing can mitigate privacy leakage by model inversion attacks, I suspect it could also harm the adversarial robustness of the model. Therefore, I suggest the authors include a discussion on the potential harm to models' robustness when protecting training data privacy."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1183/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1183/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1183/Reviewer_uLpV"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1183/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698512663272,
        "cdate": 1698512663272,
        "tmdate": 1700391527511,
        "mdate": 1700391527511,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "J62rMx5tHM",
        "forum": "1SbkubNdbW",
        "replyto": "1SbkubNdbW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1183/Reviewer_wJ7G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1183/Reviewer_wJ7G"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the impact of label smoothing, a commonly used regularization techniques in deep learning, on the privacy vulnerability of models to model inversion attacks (MIAs), which aim to reconstruct the characteristic features of each class. It is shown that traditional label smoothing with positive factors may inadvertently aid MIAs, increasing the privacy leakage of a model. The paper also finds that smoothing with negative factors can counteract this trend, impeding the extraction of class-related information."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- MIAs represent a major threat to the privacy of machine learning models. This seems to be the first paper studying the relationship between label smoothing and MIAs. \n- The work provides both empirical and analytical justification for the connections between label smoothing and MIAs. \n- The ablation study is interesting to show that label smoothing impacts MIAs mainly in the optimization stage."
            },
            "weaknesses": {
                "value": "- The findings are not very surprising. Given that label smoothing helps the model generalize, leading to better representation of each class (e.g., more smooth decision boundaries). It is intuitive that more smooth decision boundaries allow gradient-based method to better optimize the representation of each class. \n- There is trade-off between the model's accuracy and its vulnerability to MIAs. Label smoothing with negative factors reduces the privacy risks at the cost of model accuracy (see Table 1). Intuitively, a poorly trained model is more robust to MIAs."
            },
            "questions": {
                "value": "How does label smoothing impact the model's vulnerability to other attacks (e.g., adversarial attacks, backdoor attacks, etc)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1183/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1183/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1183/Reviewer_wJ7G"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1183/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698688661406,
        "cdate": 1698688661406,
        "tmdate": 1699636044810,
        "mdate": 1699636044810,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ge0NsER78l",
        "forum": "1SbkubNdbW",
        "replyto": "1SbkubNdbW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1183/Reviewer_dNRe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1183/Reviewer_dNRe"
        ],
        "content": {
            "summary": {
                "value": "This work considers the problem of defending against Model Inversion Attacks in white-box settings. The major contributions of this work are:\n\n1) In the context of model inversion attacks, this paper observes that positive label smoothing increases a model\u2019s privacy leakage and negative label smoothing counteracts such effects.\n\n2) Consequently, negative label smoothing is proposed as a proactive measure to defend against Model Inversion Attacks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) This paper is written well and it is easy to follow.\n\n2) To my knowledge, this is the first work to explore label smoothing in the context of defense against model inversion attacks."
            },
            "weaknesses": {
                "value": "At a high level, Model Inversion attack procedures can be decomposed into 2 stages: 1) Classifier training, and 2) Model Inversion attack on the classifier. My review below addresses weaknesses of this work corresponding to each stage.\n\n**Classifier training:**\n\n1) Section 3. Analysis is only remotely related to contemporary Model Inversion setups. In particular, it is unclear as to how observation from Section 3 lays groundwork to the remainder of the paper.\n\n- What is the optimization objective for both classifier training and model inversion attack for Figure 1? A clearly formulated problem definition with relevant equations is critical to understand this part.\\\n\n- How would Figure 1 change if the number of iterations = 5K for all setups. \n\n2) Section 4.3. A large number of observations from Sec 4.3 for Standard and Positive Label Smoothing has already been thoroughly investigated in prior works. \n\n- Muller et al. [A] and Chandrasegaran et al. [B] have already shown that positive label smoothing erases some relative information in the logits resulting in better class-wise separation of penultimate layer representations under positive LS compared standard training.\n\n- Figure 4, column 3 is unclear. What are the Training and Test accuracies of the classifiers used in Figure 4?  Recall that if the classifier trained with negative label smoothing is good, penultimate layer representations should be linearly separable. Therefore, does column 3 correspond to a poor classifier trained with negative label smoothing?\n\n3) Can the authors explain why standard training is required in the first few iterations before \u201cgradually\u201d increasing the negative label smoothing?\n\n\n\n**Model Inversion Attacks:**\n\n1) Limited empirical study. This work only studies one attack method for evaluating defense (No comparison against MID and BiDO defense setups even in Table 8 in Supp.). I agree that PPA works well in high-resolution setups, but SOTA attacks in well established test beds are required to understand the efficacy of the proposed defense.\n\n- It is important to include GMI, KEDMI, VMI [C], LOMMA [4] and PLG-MI [5] attacks to study the efficacy of the proposed defense (against other SOTA defense methods). Currently it is not possible to compare the proposed method with results reported in the MID and BiDO defense papers.\n\n2) There is no evidence (both qualitative and quantitative) to establish that unstable gradient directions during Model Inversion attack is due to negative label smoothing. Is it possible that such shortcomings could be due to the PPA attack optimization objectives? Addressing 1) above can answer this question to some extent. \n\n3) User studies are necessary to show the defense/ leakage of privacy shown by the inversion results. Since this work focuses on private data reconstruction, it is important to conduct user study to understand the improvements (See [F]).\n\n4) Significant compromise in model utility when using negative label smoothing questioning the findings/ applicability of this approach. Table 1 and Table 8 results suggest that Neg. LS reduces the Model Accuracy (model utility) by huge amounts, i.e.: A 3.5% reduction in Test Accuracy for CelebA (Table 1) compared to Standard training could be serious. Recall that lower model accuracy leads to lower MI attack results. I agree that generally some compromise in model utility might be required for defense, but large reduction in model utility makes this approach questionable, i.e., In practice, no one would deploy/ attack a weaker model.\n\n5) Error bars/ Standard deviation for experiments are missing.\n\n6) Missing related works [C, D, E].\n\n\nOverall I enjoyed reading this paper. But in my opinion, the weaknesses of this paper outweigh the strengths. But I\u2019m willing to change my opinion based on the rebuttal. \n\n===\n\n[A] M\u00fcller, Rafael, Simon Kornblith, and Geoffrey E. Hinton. \"When does label smoothing help?.\" Advances in neural information processing systems 32 (2019).\n\n[B] Chandrasegaran, Keshigeyan, et al. \"Revisiting Label Smoothing and Knowledge Distillation Compatibility: What was Missing?.\" International Conference on Machine Learning. PMLR, 2022.\n\n[C] Wang, Kuan-Chieh, et al. \"Variational model inversion attacks.\" Advances in Neural Information Processing Systems 34 (2021): 9706-9719.\n\n[D] Nguyen, Ngoc-Bao, et al. \"Re-thinking Model Inversion Attacks Against Deep Neural Networks.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[E] Yuan, Xiaojian, et al. \"Pseudo Label-Guided Model Inversion Attack via Conditional Generative Adversarial Network.\" AAAI 2023 (2023).\n\n[F] [MIRROR] An, Shengwei et al. MIRROR: Model Inversion for Deep Learning Network with High Fidelity. Proceedings of the 29th Network and Distributed System Security Symposium."
            },
            "questions": {
                "value": "Please see Weaknesses section above for a list of all questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1183/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1183/Reviewer_dNRe",
                    "ICLR.cc/2024/Conference/Submission1183/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1183/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789725658,
        "cdate": 1698789725658,
        "tmdate": 1700683982523,
        "mdate": 1700683982523,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "soowZUKynq",
        "forum": "1SbkubNdbW",
        "replyto": "1SbkubNdbW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1183/Reviewer_22v2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1183/Reviewer_22v2"
        ],
        "content": {
            "summary": {
                "value": "The paper shows that model trained with label smoothing can be more vulnerable to model inversion attacks while negative label smoothing can be a defense for the attacks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper looks at an interesting and important problem of how model training can affect model\u2019s vulnerability to MIAs.  \nThe experimental results in general justifies the statements well.\nThe presentation is clear."
            },
            "weaknesses": {
                "value": "I\u2019m still wondering how much we can conclude the relation between vulnerability of MIAs and training methods (normal / smoothing / negative smoothing) from the empirical results shown here. For example,\n\n- different training methods lead to different model test accuracy (Table 1) especially for CelebA. I think it\u2019s fairer to compare the attack accuracy under the same model accuracy (e.g. early stop the label-smoothed model at a lower accuracy iteration), as it might be normal for a higher accuracy model leak more. It might also be interesting to look at training accuracy of the models to understand how well they generalize.\n\n- we\u2019re looking at one particular attack algorithm here. I think it\u2019s natural to ask whether another algorithm, or maybe an adjusted version this this algorithm can achieve a different result. For example, if the attacker knows that a model is trained with negative label smoothing (and thus has a different calibration than a normally-trained model), can they possibly sample more initial latent embeddings in the first stage, or adjust their objective function to incorporate with the calibration of this model in the second stage?"
            },
            "questions": {
                "value": "In Fig 6b, why is the gradient similarity for label smoothing lower & with higher variance than that of hard label? I must admit that I don\u2019t have a good intuition here but I was kind of expecting the lines of smoothing and negative smoothing to stay on two sides of the hard label\u2019s line.\n\n(And those mentioned above.)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1183/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699244033466,
        "cdate": 1699244033466,
        "tmdate": 1699636044678,
        "mdate": 1699636044678,
        "license": "CC BY 4.0",
        "version": 2
    }
]