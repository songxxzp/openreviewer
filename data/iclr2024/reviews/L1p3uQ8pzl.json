[
    {
        "id": "JyOWp4qXFY",
        "forum": "L1p3uQ8pzl",
        "replyto": "L1p3uQ8pzl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2297/Reviewer_gZuL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2297/Reviewer_gZuL"
        ],
        "content": {
            "summary": {
                "value": "The paper explores the adaptability of LLMs to outworld knowledge by focusing on the Pokemon world, which is distinct from our human reality. The authors introduce an interactive Python environment called Pokemon-Py to simulate the Pokemon world and analyze the model's performance. They find that while LLMs can memorize facts about the Pok\u00e9mon world, they struggle with logical inconsistencies. To address this, they propose a new self-supervised learning method called \"Self-Training with Self-Competition,\" which allows the model to better adapt to new and unknown settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Innovative focus on the ability of LLMs to adapt to outworld knowledge, a largely unexplored area.\n2. Introduction of a specific Python environment (Pokemon-Py) for simulating and testing the model in a new context.\n3. Proposes a novel self-supervised learning method that shows promise in improving the model's adaptability to new worlds."
            },
            "weaknesses": {
                "value": "1. The paper focuses solely on the Pokemon world, so it's unclear how generalizable the findings are to other outworld settings.\n2. The work may not delve deep enough into the specifics of why LLMs struggle with logical reasoning in new worlds.\n3. There is a lack of discussion on potential real-world applications beyond gaming scenarios."
            },
            "questions": {
                "value": "1. Could the authors give some intuitions on why LLMs face logical inconsistencies when reasoning about the Pokemon world?\n2. How generalizable is the \"Self-Training with Self-Competition\" method to other outworld or fictional settings?\n3. Does the model's self-competition lead to any form of model destabilization or other unexpected behaviors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2297/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698724852033,
        "cdate": 1698724852033,
        "tmdate": 1699636162565,
        "mdate": 1699636162565,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zDdGhBZjQm",
        "forum": "L1p3uQ8pzl",
        "replyto": "L1p3uQ8pzl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2297/Reviewer_hwhv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2297/Reviewer_hwhv"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an exploration of how large language models (LLMs) can adapt to new world settings, such as the Pokemon world, which differs significantly from the human world. It introduces POKEMON-PY, a Python library that simulates the Pokemon world and provides an interactive environment for LLMs to learn. The paper also analyzes the awareness and reasoning of outworld knowledge in LLMs, highlighting severe distortions and contradictions. A self-supervised learning method based on self-competition is proposed, where the LLM improves itself by playing Pokemon battles against itself. The paper concludes by evaluating the outworld adaptation of the LLM on two downstream tasks, demonstrating significant improvement after self-training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper introduces a new setting called outworld knowledge, which might be a new direction for future (LLM) agents.\n2. This paper proposes a new environment Pokemon-Py. However, I think the environment is somewhat simplistic as it only involves combat operations.\n3. This paper shows that under the guidance of the winning rate, LLMs can learn new outworld knowledge without manual labeling."
            },
            "weaknesses": {
                "value": "1. I suggest the authors to give a definition or formulation on \"outworld knowledge\". And if this field has been previously researched before, the author should cite it. This can enable readers to more accurately get the problem the author wants to address, rather than having a vague concept.\n2. The method part seems to lack some innovativeness. Similar approaches might have been proposed in prior work, such as AlphaStar. Furthermore, it appears to share some similarities with RLHF or DPO, except that the annotator has transitioned from a human to the environment. However, I don't deny that this method can be helpful for transferring LLM to a new environment.\n3. I think this environment is a good contribution. Researchers can generate some entirely counterintuitive operations by configuring the environment's basic settings, such as making fire counteract water (which is not Pokemon but a completely unfamiliar environment that needs exploration). Therefore, I hope the author can provide a more detailed description of the extensions that this environment can offer and how they may be applied in research scenarios."
            },
            "questions": {
                "value": "1. Can the authors add a baseline \"playing against humans\"? This is a much stronger baseline than Random and MaxDamage.\n\n2. I believe that the main contribution of this paper is not the introduction of a learning approach to achieve a higher winning rate, but rather the demonstration of some form of transferability within LLM (because given the current context, game theory or search may potentially achieve better results than LLM.) Could the author discuss this issue?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2297/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2297/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2297/Reviewer_hwhv"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2297/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742206904,
        "cdate": 1698742206904,
        "tmdate": 1699636162468,
        "mdate": 1699636162468,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4a5zijvKfk",
        "forum": "L1p3uQ8pzl",
        "replyto": "L1p3uQ8pzl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2297/Reviewer_5UvA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2297/Reviewer_5UvA"
        ],
        "content": {
            "summary": {
                "value": "As LLMs show incredible capacity to gather knowledge about the human world through their supervised pre-training, the question of whether they are able to gather knowledge about completely other (such as fictional) world arises.\n\nThis paper proposes to investigate the ability from AI to be transferred from human world to other worlds, such as fictional worlds like the game of POKEMON, and refer to this as outworld adaptation/generalization.\n\nIn doing so, this paper contributes:\n\n1. 'outworld knowledge awareness' : a baseline evaluation of the pokemon-related outworld knowledge present in ChatGPT, LLaMA2-7b and Alpaca-7b, in terms of factual and reasoning-requiring Q&As, showing poor result on the reasoning-required Q&As and therefore goading us to the conclusion that the outworld laws of the pokemon world are confusing state-of-the-art LLMs.\nNote that the reasoning-required questions are based on accurate move selection for a pokemon in battle with an opponent.\n\n2. POKEMON-Py : a python library enabling using POKEMON battles as interactive playground for text-based state-of-the-art AI systems: text-based observations and instructions are provided to simulate a two-players pokemon trainer battle.\n\n3. Self-Training with Self-Competition : a novel self-supervised learning method to adapt pre-trained LLMs to new world settings, thus enabling growth.\n\nRegarding evaluation of the outworld adaptation, the paper proposes to instrumentalise two downstream tasks performance as a measure of outworld adaptation.\n\nExperimental evidences shows that LLaMA2-7b can be effectively adapted to the POKEMON world."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "## Originality:\n\nAs far as I know, outworld adaptation/generalization is a novel problem and the papers proposes a new environment in POKEMON-PY. \n\n## Quality:\n\nThe paper addresses a valuable problem and proposes interesting experiments.\nReproducibility seems fairly high.\n\n## Clarity:\n\nFigures and explanations are fairly thorough.\n\n## Significance:\n\nValuable problem to address and the proposed resource is hoped to be useful to the community."
            },
            "weaknesses": {
                "value": "## Novelty:\n\nAs far as I know, outworld adaptation/generalization is a novel problem and the papers proposes a new environment in POKEMON-PY. \n\nWhile the proposed self-supervised learning method to address this new outworld adaption problem is new, in the context of LLMs and outworld adaptation, it is actually very similar to Supervised Self-Play (S2P) in its 'scheduled' variant from [1], which was proposed in the context of Emergent Communication ; which can be indeed thought of as an outworld adaptation where the outworld is the natural language and the pre-trained world is that of the emergent language.\n\nIn more details, scheduled S2P is stage-wise identical to the algorithm proposed here.\nWhere a difference can be seen is in 2 points:\n\n1. firstly, the dataset that is used during the supervised learning stage is static in scheduled S2P because there is no RL-like environment to sample new data from. \n2. secondly, the self-play task is cooperative in scheduled S2P, it is a referential game, while the proposed algorithm here is using a competitive task. \n\nI find that those two discrepancies bring enough incremental innovation to make the paper worthy of publication provided that a proper discussion about the similarities and differences with scheduled S2P is included to the manuscript.\n\n\n[1] : Lowe, Ryan, et al. \"On the interaction between supervision and self-play in emergent communication.\" International Conference on Learning Representations. 2019.\n\n## Quality:\n\n### Unsupported Claim:\nIn Section 4.1, the following claim is made without evidence:\n`The choice of \u03b5 affects the convergence rate of the algorithm, and a larger one will make it slower.'\nPlease provide citation of a similar phenomenon in previous literature or supporting evidence such as a table or a graph showing, respectively, either the final performance or the learning curves of different runs within the different hyperparameter settings of $\\epsilon$.\n\n### Statistical Significance:\n\nWhile Table 3 and 5 show very interesting progressions, I think that the experiments both lack in terms of statistical significance:\n\n1. It seems that only one random seeded run has been performed, since there is no mention of how many random seeds have been run. I would expect at least 3 differently-seeded runs to be reported on, if possible, please?\n\n2. Then, following usage of multiple randomly-seeded runs, I would expect the reported statistics to be comprised of at least mean (or median) and standard deviation (or variance).\n\nI would expect to see in Section 4.3 the details about the different random seeds, if it is solely an omission.\n\n### Experimental design :\n\nSection 5 highlights boolean Q&A and Language Inference as the two downstream task to evaluate outworld adaptation, following the self-supervised learning stage.\n\nI am surprise that the experiments in Section 3 are not being repeated following the self-supervised learning stage in order to evaluate the outworld adaptation.\nCould you explain why is that?\n\n\n## Clarity:\n\n### Need for an algorithm?\n\nSection 4.1 details the proposed self-supervised learning method, but I find it difficult to follow.\nI think that the addition of formal notation to refer to each model and the training operations that they go through would help increase the clarity, maybe?\n\nOr, if space permits it, I think it would be helpful to provide an actual algorithm environment to refer to, on top of the information provided in Figure 3, maybe?\n\n\n### Hyperparameter choice:\n\nSection 4.3 highlights batch size and learning rate hyperparameter values but it is unclear how those are used:\nHas there been multiple runs of the experiment with each combination of the hyperparameter values?\n\n### Section 5 Test-Train Split Issue ?\n\nThe boolean Q&A fine-tuning and testing experiment is unclear with regards to the train-test split employed: from my understanding, the positive testing samples are the same as the positive training samples?\nIf so, then this is a critical issue in terms of the validity of the experiment.\n\n## Significance:\n\nIn the current state, both (i) the gap in related work comparison, and (ii) the missing statitics to truely evaluate the significance of the results make the significance of the paper difficult to evaluate.\n\nFor now, I can only vouch for a minimal significance.\nI am hoping to be able to increase my appreciation throughout the rest of the review process and hope to increase my scores accordingly."
            },
            "questions": {
                "value": "Please see Weaknesses above.\n\n\n# AFTER REBUTTAL :\n\nFollowing the rebuttal, I am very satisfied with the answers and ~~(proposed)~~implemented changes.\nIf accepted, I am hoping the authors will carry on in the current direction of their implemented changes, and therefore I am increasing my score to ~~6~~ 8."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2297/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2297/Reviewer_5UvA",
                    "ICLR.cc/2024/Conference/Submission2297/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2297/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833706596,
        "cdate": 1698833706596,
        "tmdate": 1700741116735,
        "mdate": 1700741116735,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0rQf9ZFydM",
        "forum": "L1p3uQ8pzl",
        "replyto": "L1p3uQ8pzl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2297/Reviewer_jvkM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2297/Reviewer_jvkM"
        ],
        "content": {
            "summary": {
                "value": "The paper explores the application of  LLMs to the unique and fictional universe of the popular strategy game POK\u00c9MON, demonstrating the outworld context can let LLMs suffer from knowledge distortions and logical flaws. It introduces POKEMON-PY, a Python library for interacting within the Pok\u00e9mon world, and proposes a novel self-supervised learning method called Self-Training with Self-Competition to adapt LLMs to new world settings by enabling continuous learning through self-play. Experiments demonstrate that this method significantly improves the adaptation of the LLaMA2-7b model to perform downstream tasks within the Pok\u00e9mon world."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "It is interesting to discuss that the outworld context can let LLMs fail. The paper presents an interesting phenomenon that the knowledge required for reasoning greatly overlaps with answers to factual questions, but LLM performs worse in the reasoning task.\n\nA Self-Training with Self-Competition strategy to train the LLMs to adapt to the new universe seems simple but useful, avoiding large amounts of annotated data."
            },
            "weaknesses": {
                "value": "The paper lacks comprehensive details in several areas, which are crucial for full understanding and replication:\n\nIt is significant for the author to give a specific prompt design to guide the LLM in playing Pokemon since it is well-known that LLM is sensitive to the given prompt.\n\nAnother important missed detail is for optimizing the LLM within the self-training and self-competition framework. There are many alternatives to train the LLM agent, given the collected win data. \n\nThe absence of source code and datasets further hampers the reproducibility of the results, as the paper omits many critical specifics."
            },
            "questions": {
                "value": "As mentioned in weaknesses, a deeper explanation of the training process for the LLM is necessary. For instance, the design of the loss function remains vague. Including pseudocode could greatly aid in comprehension. \n\nMoreover, there's a concern about the utility of the LLM's outputs in game scenarios; it's unclear if the data generated during gameplay is utilized as-is, which may not reflect optimal game strategies.\n\nI am not sure about the difficulty level of the proposed reasoning task and factual task. Can you provide more detail?\n\nI am curious about how the LLM agent performs in real-world factual answering compared with reasoning tasks while sharing the same required knowledge base."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2297/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2297/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2297/Reviewer_jvkM"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2297/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699129479344,
        "cdate": 1699129479344,
        "tmdate": 1699636162300,
        "mdate": 1699636162300,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TEawfimj8b",
        "forum": "L1p3uQ8pzl",
        "replyto": "L1p3uQ8pzl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2297/Reviewer_oQMe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2297/Reviewer_oQMe"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the potential of Large Language Models (LLMs) to generalize beyond human knowledge, by using \\textsc{Pok\u00e9mon} as a case study.\nUsing Pokemon as a case study, an analysis is conducted on how much outworld knowledge LLMs have.\nThe analysis reveals that while LLMs can memorize some aspects of outworld knowledge, they often demonstrate logical inconsistencies.\nThen, the author proposes a self-supervised learning method in order to adapt to a new world setting.\nExperimental results show that the proposed method achieves improvements in adapting LLMs to some downstream tasks within the pokemon world."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* The paper is well-written and well-organized.\n* The proposed method improves the performance of LLMs in some pokemon tasks."
            },
            "weaknesses": {
                "value": "* The motivation for choosing Pokemon as a case study of outworld is ambiguous and unclear. Even though I agree that \\textsc{Pok\u00e9mon} has unique and interesting worldviews, I think it is not adequate to examine LLMs' outworld knowledge through a single strategy game.\n* The proposed method, which is based on a self-supervised learning method, seems to be inefficient because it learns only from the actions of the winners during self-play. Reinforcement learning algorithms such as PPO would be more efficient because they also use the loser's actions for learning models."
            },
            "questions": {
                "value": "* Why is \\textsc{Pok\u00e9mon} considered as a good case study for outworld?\n* From the analysis of outworld knowledge in the pokemon world, what kind of behaviors can we infer that LLMs would exhibit in other new worlds in general?\n* Why is the proposed method based on a self-supervised learning algorithm rather than a reinforcement learning algorithm? Is the proposed method more efficient than reinforcement learning algorithms such as PPO?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2297/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2297/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2297/Reviewer_oQMe"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2297/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699283251361,
        "cdate": 1699283251361,
        "tmdate": 1699636162241,
        "mdate": 1699636162241,
        "license": "CC BY 4.0",
        "version": 2
    }
]