[
    {
        "id": "mNKQF5WEP4",
        "forum": "4znwzG92CE",
        "replyto": "4znwzG92CE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2766/Reviewer_HoUA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2766/Reviewer_HoUA"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a simulator supporting humanoid avatars and robots for the study of collaborative human-robot tasks in home environments. Diverse and realistic human models are constructed, addressing the challenges of accurate modeling of the human body as well as the human-like appearance and motion diversity of the models. Besides, The platform supports interaction between a real person and a simulated robot via a mouse and keyboard inputs or VR interface, enabling human-in-the-loop simulation. This paper also investigates two collaborative human-robot interaction tasks, social navigation, and social reorganization, and provides insights into learned and heuristic baselines for both tasks in the simulator."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- This work presents a Co-Habitat for Humans, Avatars, and Robots, offering a simulation environment for humanoids and robots within a wide range of indoor settings, which can promote the development of human-robot tasks in the Embodied AI field.\n- Habitat3 provides realistic human and robot simulation. In the process of realistic human modeling, this work addresses the challenges posed by efficiency realism and diversity in terms of appearance and movement.\n- This work develops a Human-in-the-Loop evaluation platform within the simulator, allowing the control of humanoid robots using a mouse, keyboard, or VR devices. It provides a method for interacting and evaluating with real humans. Furthermore, it supports data collection and reproducibility during the interaction, offering a convenient tool for further research.\n- This paper introduces two benchmark tasks for human-robot interaction, along with baselines for each task. This paper leverages end-to-end RL to study collaborative behaviors and examines the performance of various learning strategies. The Human-in-the-Loop evaluation in the social rearrangement task reveals potential avenues for improving social embodied agents.\n- This simulator can be used for end-to-end reinforcement learning for robot agents, significantly reducing the time required for reinforcement learning. It also provides a validation environment for a broader range of robot agents, thus reducing potential risks to the environment and humans."
            },
            "weaknesses": {
                "value": "- Current robots are equipped with only a depth camera, human detector, and GPS, providing a relatively limited amount of information. It is worth considering whether additional sensor types, such as LiDAR and sound sensors, can be integrated in the future to enhance obstacle avoidance and navigation capabilities.\n- It has been noted that in human simulation, fixed hand movements can lead to a decrease in visual accuracy. It may be considered to address the deformability of the skin during the simulation process and create hand motion animations during activities like grasping and walking.\n- This simulator has a wide range of potential applications and can be further explored to implement other embedded artificial intelligence tasks, such as visual-language navigation."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2766/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734072265,
        "cdate": 1698734072265,
        "tmdate": 1699636219488,
        "mdate": 1699636219488,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AP0VMSQyzK",
        "forum": "4znwzG92CE",
        "replyto": "4znwzG92CE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2766/Reviewer_ndxG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2766/Reviewer_ndxG"
        ],
        "content": {
            "summary": {
                "value": "The paper presents Habitat 3.0, an Embodied AI simulator designed to facilitate research in human-robot interaction and collaboration within complex indoor environments. The proposed platform aims to address the need for efficient simulation tools to study AI agents' capabilities in realistic and diverse human-robot interaction scenarios. The main contributions of the paper are as follows:\n\nDiverse Humanoid Simulation: Habitat 3.0 offers a framework for creating and simulating diverse humanoid avatars. These avatars encompass various appearances and motions, enhancing the realism of agent interactions within the simulated environments. By employing techniques like skeletal models and linear blend skinning, the simulator achieves a balance between efficiency and visual fidelity.\n\nHuman-in-the-Loop (HITL) Evaluation Tool: The paper introduces a HITL interface that allows real human operators to control humanoid avatars within the simulated environment. This tool enables online human-robot interaction evaluations and data collection, providing a unique platform for studying how AI agents collaborate with humans.\n\nSocial Navigation and Social Rearrangement Tasks: The paper explores two collaborative tasks\u2014social navigation and social rearrangement\u2014to assess AI agents' ability to interact with human or humanoid partners. These tasks require the agents to find and follow humans at a safe distance or to collaborate with a humanoid in rearranging objects within the environment.\n\nEvaluation of AI Agents: The paper compares multiple AI agent baselines in both automated and HITL evaluation settings. These agents include heuristic experts and end-to-end reinforcement learning policies. The evaluations highlight the agents' adaptability in working with different partners to some extent and provide insights into their efficiency and success rates.\n\nRobust HITL Assessments: By conducting HITL evaluations involving real human participants, the paper evaluates AI agents' coordination abilities in scenarios involving diverse partners. These assessments help in understanding how baseline AI agents impact human efficiency and reveal insights into the dynamics of human-robot interactions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Quality:\nThe paper demonstrates a fair level of quality in its methodology and execution. The development of Habitat 3.0 is well-detailed and addresses a clear need in the field of HRI research. The simulator provides an effective platform for investigating human-robot interaction. The evaluation of AI agents in both automated and HITL settings enhances the paper's quality.\n\nClarity:\nThe paper is generally well-written and clear, with detailed explanations of the simulator framework and the tasks studied. \n\nOriginality:\nHabitat 3.0 introduces valuable original contributions to the field. The combination of diverse humanoid simulation, HITL control, and the study of social navigation and rearrangement tasks provide some initial steps and ideas in this domain. Furthermore, the paper explores HITL evaluations involving human participants, which adds to the originality of the work. While the components themselves are not entirely novel, their integration and application within a single framework is original and significant.\n\nSignificance:\nHabitat 3.0, with its focus on embodied AI and human-robot interaction, addresses a critical aspect of AI development. The simulator has the potential to open new avenues for research, including collaborative AI, human-robot teamwork, and social embodied agents. The HITL evaluations are particularly significant, offering insights into how AI agents impact human performance and behavior. The paper's findings and methodology are likely to influence future research in these domains.\n\nOverall, its strengths can be listed as:\n- focus on human-robot interaction compared to previous platforms offering single-agent or multi-homogeneous-agent training.\n- efficient simulation implementation that enables faster progress in training/evaluating developed algorithms.\n- human-in-the-loop evaluation tool that can open up interesting use-cases and approaches to improve and analyze HRI methods.\n- a fair amount of evaluations."
            },
            "weaknesses": {
                "value": "The critical weaknesses are:\n- currently, the focus of simulations seem to be more on the visual realism, which is a valid concern. However, the movement of the agents lacks physical realism, which hinders the extend of how human-robot interaction can be evaluated accurately. \n\n- the focus of this work is not proposing novel learning algorithms but still the results indicate that none of the baselines achieve useful following rate (F) nor feasible collision rate (CR) in the social navigation task. Similarly, for the social rearrangement task, none of the methods seem to generalize and let the robots assist their partners effectively (checking the relative efficiency (RE) metric). Even for HITL evaluations, which would be simpler since humans adjust to robots on the fly, the results are not encouraging. This, then, makes it harder to evaluate and take some insights from these evaluations, which is a major component of the paper.\n\n- humanoid shape diversity has been considered, however, robotic agent diversity was not addressed. \n\n- similar to the previous point, the platform, as it is now, lacks task diversity as well.\n\n- it feels like (looking at the efficiency improvements (RE metric) when collaborated vs. solo cases) maybe the tasks do not offer enough opportunities for collaboration. \n\n- personally, I find the HITL evaluations more interesting, however, the paper does not cover detailed evaluation and analysis of these experiments."
            },
            "questions": {
                "value": "- what are possible solutions/integrations to alleviate the unrealistic humanoid locomotion problem, i.e., the agent first rigidly rotates to face the next target waypoint and then follows a straight path. The autonomous agents trained against such human movement models will not be directly transferable to real-world settings, nor the analysis would not be informative.\n\n- it is unclear how easy and flexible to import motion capture data. Can you elaborate on that?\n\n- it is also unclear how trivial it is to use the AMASS dataset along with VPoser to compute humanoid poses and then to import them into the simulator. Trying to use such external tools that the benchmark providers do not support/maintain themselves frequently becomes a huge hassle and ease-of-use of such external tools is critical, so can you also provide some clarification on their integration and/or usability? \n\n- About reliance on VPoser: Depending on the complexity of the task, simple interpolation between poses might not be sufficient, what would be possible solutions?\n\n- is it possible to incorporate physical collaboration scenarios, i.e., partners acting on the same object? would it require additional steps than what was explained on the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2766/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2766/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2766/Reviewer_ndxG"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2766/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766910839,
        "cdate": 1698766910839,
        "tmdate": 1699636219382,
        "mdate": 1699636219382,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wJ8ITufGCr",
        "forum": "4znwzG92CE",
        "replyto": "4znwzG92CE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2766/Reviewer_FYWe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2766/Reviewer_FYWe"
        ],
        "content": {
            "summary": {
                "value": "- This work introduces Habitat 3.0, a simulation platform for studying human-robot collaboration in home environments.\n- The environment includes accurate humanoid simulation and a human-in-the-loop infrastructure for real-time interaction and provides a way to evaluate different robot policies with real human collaborators.\n- It also allows the exploration of collaborative tasks: Social Navigation and Social Rearrangement. \n- Finally, the authors demonstrate that learned robot policies can effectively collaborate with unseen humanoid agents and human partners. Shows emergent behaviors during task execution, such as yielding space to humanoid agents."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This work complements existing works (e.g. Habitat, VirtualHome, etc.) in a human-centric way. It allows more flexible human models, human-object interactions, human-robot interactions, and human-in-the-loop evaluation, which are often ignored in previous works.\n2. This work is in general well-written, providing a good survey for the field of embodied AI environments.\n3. This work designs two social/collaborative tasks, navigation and rearrangement, and shows promising results."
            },
            "weaknesses": {
                "value": "The main limitation of this work lies in universality. While I believe this work is interesting and helpful to the field, I am wondering if it could be scaled to incorporate more elements, supporting more tasks, so that the progress wouldn't stop here at the two example tasks. While I understand that these aspects might be beyond the scope of a single work, it would be beneficial to demonstrate, or at least discuss, how future works can develop upon Habitat 3.0. For example,\n- physics simulation \n- fine-grained object interactions\n- sim2real deployment"
            },
            "questions": {
                "value": "1. For social arrangement, what is the motivation for using population-based approaches? More discussions would be helpful to understand the setting.\n2. Discuss relevant previous works. What is the relationship between the proposed social navigation task and visual tracking (e.g. [a])? Seems quite similar and more discussions are needed. Besides, [a] also contains humanoid simulation and end-to-end RL with Conv+LSTM.\n3. For object interaction, Sec. 3.1 explains that \"Once the hand reaches the object position, we kinematically attach or detach the object to or from the hand.\" Are all the objects simplified as a point particle? Are all the objects in the environment interactive? Is it possible to add more properties to them (e.g. geometry - shape, physics - weight, material, etc.)? It would be great to explore/discuss how to incorporate more general and complex object interactions, from pre-defined motion primitives (e.g. lie, sit) to freeform actions (e.g. grasp).\n4. Currently this work focuses restricted set of motions while more motions can be potentially added with the SMPL-X representation. In demo video 2:56, it also discusses complex motions (e.g. wiping, knocking). It would be beneficial to discuss how future works can incorporate more motions with interactive objects and form meaningful tasks. \n5. Another question is, what kind of tasks can Habitat 3.0 support in addition to navigation and arrangement? Again, I understand that these two tasks are already great for this work, but more discussions on the potential of Habitat 3.0 would make this work more general and influential.\n6. I wonder if the HITL tools would also be standardized and open-source.\n\n[a] End-to-end Active Object Tracking via Reinforcement Learning, ICML 2018."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2766/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2766/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2766/Reviewer_FYWe"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2766/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698899192158,
        "cdate": 1698899192158,
        "tmdate": 1699636219320,
        "mdate": 1699636219320,
        "license": "CC BY 4.0",
        "version": 2
    }
]