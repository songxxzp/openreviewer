[
    {
        "id": "HkAf7DCsjE",
        "forum": "XlkN11Xj6J",
        "replyto": "XlkN11Xj6J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2030/Reviewer_eSpe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2030/Reviewer_eSpe"
        ],
        "content": {
            "summary": {
                "value": "This paper investigate how to use generated images from diffusion models to improve discrimitive tasks in both in-distribution and out-of-distribution settings. The authors propose to render 3D data to 2D edge maps, fine-tune the large-scale diffusion model via ControlNet approach with the prompt augmented form LLM. After training, the generated images naturally have all the 3D object annotations. These generated data can be subsequently used as data augmentation for downstream tasks. The paper demonstrate the effectiveness of the proposed method on image classification and 3D pose estimation tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed framework is simple and straightforward to use. The main technical contribution seems to be how to better use 3D data, ControlNet and LLM for data augmentation.\n- Quantitative improvement looks promising. On the evaluated task (image classification and pose estimation), the quantitative improvement seems quite obvious, espcially for the OOD settings."
            },
            "weaknesses": {
                "value": "- The main method aims to produce (image, 3D annotation) pairs. Then is 2D image classification a good task for evaluation? The corresponding 3D ground truth is not used anywhere in this task. And you don't need 3D data to create (image, label) pairs. Even though experiments indeed show the improvement, I doubt this could be achieved without using 3D data.\n- As the main purpose is to use the generated data for downstream tasks, I think the paper needs to carefully examine the data quality and show the necessity of the proposed approach. From this aspective, some necessary ablations are missing. \n  - One simplest baseline to use 3D data is to just use the rendered image with background (e.g., random environment map). Would this kind of synthetic data improve the evaluated tasks? I think this baseline is needed to prove the necessity of using a image generative model.\n  - For image classification, a simpler approach is to just use the imagenet images, extract the edge map and then generate new image conditioned on the LLM prompt. Would this kind of synthetic data also give big improvement? This baseline is needed to show the necessity of using the 3D data, as least for the image classification task.\n- The title is a bit misleading. It seems to suggest a method that enable 3D control of the diffusion model (e.g., changing view points), but it's not. The proposed method merely use the 3D data and diffusion model to create (image, 3D annotation) pairs."
            },
            "questions": {
                "value": "What is the prompt to LLM for enriching the description?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2030/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698612312896,
        "cdate": 1698612312896,
        "tmdate": 1699636134575,
        "mdate": 1699636134575,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TLVcBUSG6w",
        "forum": "XlkN11Xj6J",
        "replyto": "XlkN11Xj6J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2030/Reviewer_B6MH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2030/Reviewer_B6MH"
        ],
        "content": {
            "summary": {
                "value": "- The author introduces a method named 3D-DST, aimed at enhancing the comprehension of 3D objects by diffusion models.\n- This method comprises two modules: the \"3D Visual Prompt\" module, which utilizes edge maps as prompts derived from rendering, and the \"Text Prompt\" module, which extends prompt words through LLM.\n- Through experiments, the author demonstrates that the images generated by the proposed method, along with paired labels, serve as an effective approach for data enhancement or pre-training. This leads to improved performance in tasks such as image classification and 3D pose estimation across multiple baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is easy to understand.\n- The paper presents an approach that incorporates edge maps as additional prompts to enhance the performance of the diffusion-based method."
            },
            "weaknesses": {
                "value": "- The framework is mainly inherited from Controlnet, so the technical contributions are limited and incremental.\n- The idea of 3D Visual Prompt via CG rendering and LLM Prompt is more like a combination of multiple previous effective techniques.\n- The author's excessive focus on introducing background knowledge of known technologies like diffusion or cross-attention is unnecessary if the method utilized in this article relies on off-the-shelf approaches. It is not recommended to extensively discuss these technologies in the main text.\n- The second challenge, \"simple text prompts,\" seems to be less directly relevant to the paper's introduction on adding 3D geometry control to diffusion models.\n- The experimental part of the paper lacks details on training the network."
            },
            "questions": {
                "value": "How to define camera extrinsic matrix and whether to use class-level canonical-space as the extrinsic matrix of identity. If this is the case, there are many symmetric objects whose poses are ambiguous (this issue has been extensively discussed in the work on object 6dof estimation). How to define objects with multiple symmetry axes such as round tables? In addition, how to align the definition of extrinsic coordinate systems between different classes?\nIt is counterintuitive to claim that edge maps are superior to depth maps because depth maps provide more 3D information, such as occlusion relationships, which goes beyond the 2D representation of edge maps. The conclusions presented in the author's paper are difficult to support with only a few selectively chosen qualitative examples."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2030/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2030/Reviewer_B6MH",
                    "ICLR.cc/2024/Conference/Submission2030/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2030/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672376944,
        "cdate": 1698672376944,
        "tmdate": 1700983777197,
        "mdate": 1700983777197,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aEvidPVvdB",
        "forum": "XlkN11Xj6J",
        "replyto": "XlkN11Xj6J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2030/Reviewer_5ugh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2030/Reviewer_5ugh"
        ],
        "content": {
            "summary": {
                "value": "A method to add 3D geometry control into the image generation process. To achieve the goal, three key techniques are leveraged, including 2D edge maps generation with 3D annotations via rendering, text prompt generation for improving the diversity, and conditional image generation from edge maps and text descriptions. The method can be utilized as a data augmentation strategy for many downstream tasks such as image classification. Experiments can show its promising application potential."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Adding 3D geometry control via 2D edge maps and text descriptions is interesting and reasonable. This way the generative model only needs to deal with controlling information represented in 2D images and texts. Then many existing powerful techniques can be leveraged for controllable generation. \n- The proposed method is reasonable. It can achieve plausible controllable and diverse generation results. Generated images are of good quality and well-related to edge prompts and text conditions. \n- The method can serve as a promising data augmentation strategy for many downstream tasks. It is a promising way to generate diverse 2D images with 3D information annotation."
            },
            "weaknesses": {
                "value": "- The technical significance is relatively limited. The problem of generating 2D edge maps from 3D models and generating text prompts from 3D CAD models can be solved by existing techniques. Though the idea is interesting, no new techniques are proposed. The overall method is rather like an application-guided strategy. Though with promising application potential, it is hard to say what general principles that can guide the research in other domains can be distilled from the paper. \n- It is not sure whether the generated images are very faithful to the edge maps conditions. For example, there is no good guarantee that the objects in the generated images are consistent with the geometry described via the edge maps."
            },
            "questions": {
                "value": "- Evaluations on whether the generated images are faithful to images and text conditions. \n- It would be better if more potential applications could be discussed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2030/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2030/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2030/Reviewer_5ugh"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2030/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698737109058,
        "cdate": 1698737109058,
        "tmdate": 1699636134419,
        "mdate": 1699636134419,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3hO5sdQdD2",
        "forum": "XlkN11Xj6J",
        "replyto": "XlkN11Xj6J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2030/Reviewer_Xyyh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2030/Reviewer_Xyyh"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel method for adding 3D geometry control to diffusion models such that recognition models pre-trained on diffusion models generated synthetic data and then trained on target datasets have performance gains on classic tasks like 2D image classification and 3D pose estimation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea in this paper is neat, simple yet effective.\n- The idea is also very novel.\n- The empirical improvements on ImageNet classification and pose estimations are solid, significant, and surprising."
            },
            "weaknesses": {
                "value": "- In table 4, why the baseline result NeMo w/ AugMix is missing?\n- Could you discuss or ablate using other rendering types other than canny edges? Does canny edges work the best and why?\n- There is no discussion for limitations."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2030/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821320994,
        "cdate": 1698821320994,
        "tmdate": 1699636134319,
        "mdate": 1699636134319,
        "license": "CC BY 4.0",
        "version": 2
    }
]