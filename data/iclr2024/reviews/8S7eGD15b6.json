[
    {
        "id": "i408zYjx2h",
        "forum": "8S7eGD15b6",
        "replyto": "8S7eGD15b6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7742/Reviewer_2Hxt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7742/Reviewer_2Hxt"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new defense evaluation tool called Subspace Grid-sweep. This tool uses deterministic inference to assess adversarial robustness more effectively, revealing vulnerabilities in a previously published defense. The paper also suggests that randomness may not be necessary for defense robustness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written, and I comprehended it easily.\n2. The motivation of this paper seems good and well-founded, as initially, researchers believed that randomness enhanced defense robustness. However, with random defenses being overcome, it remains unclear whether randomness is beneficial or merely complicates defense evaluation.\n3. The proposal Subspace Grid-sweep is simple and intuitive."
            },
            "weaknesses": {
                "value": "1. The only difference between deterministic defense and random defense is merely the choice between randomly selecting once and randomly selecting multiple times. In fact, random selection even just once still qualifies as random defense. This may not necessarily imply that defense does not require robustness.\n2. The idea of subspace grid sweep lacks novelty."
            },
            "questions": {
                "value": "1.Why do the results in Figure 2 and Figure 7(a) show differences?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7742/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7742/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7742/Reviewer_2Hxt"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7742/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675732085,
        "cdate": 1698675732085,
        "tmdate": 1699636945326,
        "mdate": 1699636945326,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OTQW6ULji8",
        "forum": "8S7eGD15b6",
        "replyto": "8S7eGD15b6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7742/Reviewer_LfCf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7742/Reviewer_LfCf"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes the Subspace Grid-sweep method. It argues that using a grid for evaluating robustness can provide additional perspective on existing robustness evaluation methods. Also the authors argue that randomness introduced in many popular defences could be redundant and fixed random seed could be used instead."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Originality. Using structured search for robustness evaluation was used before, in particular, for adversarial patches. See e. g. Derandomized Smoothing [1]. However, using it for Lp bounded perturbations seems to be novel. A look at making randomized attacks deterministic is also interesting. \n\n2. Quality. The paper proposed a simple and interesting idea. The authors discuss the limitations of their method. \n\n3. Clarity. The main idea is easy to grasp.\n\n4. Significance. Evaluating model robustness is important for safety-critical applications. Providing additional perspective on robusness evaluation methods makes them more interpretable. \n\n[1] Alexander Levine and Soheil Feizi. (De)Randomized Smoothing for Certifiable Defense against Patch Attacks. In NeurIPS 2020."
            },
            "weaknesses": {
                "value": "1. Clarity/Quality. In the main paper and Appendix I haven\u2019t found the explanation on how exactly Auto-Attack was restricted to the continious K-dimensional space? Given that K is extremely small (1-6), this could drastically affect the overall Auto-Attack performance. Without understanding that it is hard to interpret the results e. g. in Table 1.\n\n2. Quality. In Section 4.1 the Authors evaluate Randomized Smoothing with standard PGD. Although they claim to use up to 10 000 steps, it still doesn\u2019t seem like a suitable evaluation method. A-PGD or AutoAttack would be better.\n\n3. Quality. If I understood correctly, all the evaluations were performed on small-resolution datasets (CIFAR-10, SVHN, MNIST). However, Randomized Smoothing is able to provide provable defence e. g. for models on ImageNet. Given the exponential nature of the method, it is hard to understand whether it would scale to datasets of higher resolution.  \n\n4. In the limitations the authors admit that fixing random seed in e. g. Randomized smoothing makes it vulnerable for an attacker that knows it. I assume that with the knowledge of Subspace Grid-sweep\u2019s grid  a model could be trained that seems robust when evaluated in the predefined grid points but contains adversarial regions in-between. That could result in backdoor-attacks on seemingly robust models. Thus the method can provide false sense of security."
            },
            "questions": {
                "value": "1. What is the role of parameter \u03c3 in GridSweepPerturbationsL2(K, B, M, \u03c3)? (Section 3.1)\n\n2. How exactly was the orthonormal basis chosen (Section 3.1)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7742/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7742/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7742/Reviewer_LfCf"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7742/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764291765,
        "cdate": 1698764291765,
        "tmdate": 1699636945178,
        "mdate": 1699636945178,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dmRireYKtn",
        "forum": "8S7eGD15b6",
        "replyto": "8S7eGD15b6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7742/Reviewer_EJoi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7742/Reviewer_EJoi"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method to evaluate the effectiveness of defenses in enhancing an image classifier's robustness to adversarial attacks. At the core of many defenses is inherent randomness, which causes difficulty in evaluating whether the defense is truly effective or is merely obfuscating gradients. The authors study whether randomized defenses retain their robustness if they are made deterministic and find that deterministic defenses can be just as robust against white-box attackers as randomized defenses."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper studies an interesting problem: How do we correctly evaluate defenses against adversarial attacks?\n\n* The paper has nice Figures and is well-written \n* The paper features a detailed Appendix with more information"
            },
            "weaknesses": {
                "value": "**Implication.** On a high level, I do not understand what the authors advocate regarding using randomness in evaluating a defense. While they show that deterministic defenses are, at best, as good as their randomized counterparts, I do not understand in what sense the randomness limits the evaluation of a defense. In other words, if an attack succeeds against a deterministic variant of a defense, what implications does that have on the robustness of the randomized version?\n\n**Subspace Projection.** What is the advantage of using a grid in a subspace over simply randomly sampling noisy perturbations? Also, function $\\texttt{GridSweepPerturbationsL2}$ is unclear to me. What is $\\sigma$ (I assume this should be $\\epsilon$)? For g=0 and B=6, the new g assignment is -1, but the comment is misleading because it says the points are normalized between [0,1]. \n\n**Randomized Smoothing.** The insights gained from Section 4.1 is not conclusive to me. The authors state the following. \n\n> This indicates that the empirical robustness of Randomized Smoothing does not come from\nthe randomness of the noise, but from the self-ensembling effect of aggregating multiple inferences\nwithin proximity of the original point.\n\nFigure 2 shows that a white-box attacker achieves the same success rate regardless of whether the noised perturbations are being resampled during the optimization procedure. However, the inability of an attacker to find an adversarial example does not imply that the randomness does not help. If the attacker knows the corruptions used by the defender, more successful attacks are conceivable. \n\n**Minor.**\n\n* Table 1 is confusing to me. Why is the robustness higher for the undefended model? What is the difference between the first column and the second and third?"
            },
            "questions": {
                "value": "* Why is it meaningful to study the robustness of deterministic defenses? \n\n* What is the advantage of iterating over a grid in a subspace rather than over randomly sampled noisy perturbations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7742/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797521889,
        "cdate": 1698797521889,
        "tmdate": 1699636945053,
        "mdate": 1699636945053,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BaXp06lfUG",
        "forum": "8S7eGD15b6",
        "replyto": "8S7eGD15b6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7742/Reviewer_YZ44"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7742/Reviewer_YZ44"
        ],
        "content": {
            "summary": {
                "value": "Recent studies have demonstrated that many defenses, which appear to defend leading-edge attacks at first, may eventually fail against an adaptive attacks. Defenses can seem effective when adversarial examples aren't discovered due to issues like gradient masking, limited computing resources, or poor initial conditions. In this paper, the authors propose a step forward in improving defense assessment by developing a new tool called Subspace Grid-sweep. This tool uses deterministic inference to simplify the process of assessing adversarial robustness. By applying Subspace Grid-sweep, the authors illustrate that a defense method previously thought to be effective\u2014and later proven ineffective\u2014could have been identified as vulnerable without conducting an exhaustive adaptive attack. Furthermore, to extend the Subspace Grid-sweep\u2019s utility to defenses that incorporate randomness, the authors demonstrate methods for creating deterministic versions of these random defenses that maintain comparable empirical performance. Consequently, their findings suggest that randomness might not be essential for the robustness of such defenses."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper unveiled an interesting finding that some randomness-based defenses do not rely on their randomness for the improved robustness.\n2. The authors leveraged some interesting methods to transform the randomness-based defenses to deterministic ones to show their robustness.\n3. The paper is well-organized and clearly presented."
            },
            "weaknesses": {
                "value": "1. The grid-sweep-based attacks seem to be very straightforward, and I think if the hidden space is large, the search space for this attack could be still huge.\n2. The experiments of diffPure are not clear to me. I believe that diffPure still needs some randomness to be successful otherwise it would be no different from a plain denoiser network. Why is the robust accuracy still as high as around 65% in Figure 4?"
            },
            "questions": {
                "value": "Please refer to the weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7742/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698886126613,
        "cdate": 1698886126613,
        "tmdate": 1699636944946,
        "mdate": 1699636944946,
        "license": "CC BY 4.0",
        "version": 2
    }
]