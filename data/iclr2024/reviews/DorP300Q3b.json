[
    {
        "id": "YvDPUsYHhS",
        "forum": "DorP300Q3b",
        "replyto": "DorP300Q3b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission672/Reviewer_yTKp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission672/Reviewer_yTKp"
        ],
        "content": {
            "summary": {
                "value": "This work addresses the difficult problem of data mapping in 2D multiple object tracking (MOT), particularly in the context of object occlusions. While this problem is complicated in 2D, it is much easier to handle in 3D space using a 3D Kalman filter. The authors propose a new approach that uses 3D object representations to improve data mapping in 2D MOT.\n\nIn their method, referred to as P3DTrack, they use 3D object representations learned from monocular video data and monitored by 2D tracking labels, eliminating the need for manual annotation from LiDAR or pre-trained depth estimators. This approach differs from existing depth-based MOT methods in that it learns the 3D object representation along with the object association module.\n\nThe authors conduct extensive experiments and demonstrate the effectiveness of their approach by achieving the best performance on popular egocentric datasets such as KITTI and Waymo Open Dataset (WOD). They also commit to publishing the code for their method to make it accessible for further research and practical implementation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+The study presents a novel approach to the task\n+The proposed method is based only on RGB 2D input, which has the advantage that the hardware required is very simple and basic.  \n+Methods are evaluated on two public datasets. Choice of dataset is motivated and explained \n+The paper is well structured and written. Study is well motivated\n+Clear description of implementation and methods allows reproduction of experiments\n+Authors perform ablation study with various experiments showing advantages of proposed architecture over other SOTA methods."
            },
            "weaknesses": {
                "value": "-Not clear in which dataset the ablation was performed? Is it for both or just one? It should be done for both datasets\n-The paper lacks qualitative results. Instead, there is a figure in the supplementary material that clearly explains the problem and the improvement. The paper would benefit from more qualitative results like this. This could be done as a teaser figure on the first page, giving the reader a good overview of the topic and the contribution. \n-It is not stated on which device the inference times are measured. Is it on the same GPU on which it was trained? One GPU or more? How far is it from working in real time? It is not clear and SLAM related applications require working in real time."
            },
            "questions": {
                "value": "1. Why do authors use the specified object detection method instead of something newer like DETR or YOLOv8?\n2. How does the system work at night when visibility is reduced? Does the ability of 3D reconstruction remain the same or does it decrease?\nThe last \"-\" could be placed as question too."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission672/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698318207962,
        "cdate": 1698318207962,
        "tmdate": 1699635994537,
        "mdate": 1699635994537,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6zcC8kPBD8",
        "forum": "DorP300Q3b",
        "replyto": "DorP300Q3b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission672/Reviewer_e7hh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission672/Reviewer_e7hh"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to jointly learn 3D features and their tracking association on top of existing 2D detector.\nThe supervision is provided via 3D MVS reconstruction of the static scene from moving camera, which allows extraction of 3D object locations and associate them with corresponding views.\nThis is shown to marginally improve MOTA accuracy on standard datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well structured and readable.\nThe review of state of the art appears complete and up to date.\nThe central idea of joint association via auxiliary depth loss  and its learning approach are novel. The representation itself uses standard building blocks, but the specific architecture is of interest for the computer vision community.\nAblation study is included to validate parameter and component choices. \nImplementation details appear sufficient for replication of results."
            },
            "weaknesses": {
                "value": "Quantitative results constitute only incremental improvements over SOTA.\nThere is no analysis of failure cases, especially w.r.t non-static classes, which can be missed in the pseudo GT association due to rigid scene assumption."
            },
            "questions": {
                "value": "Provide class based results, i.e. for pedestrians and cars to show there is no significant static bias."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission672/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission672/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission672/Reviewer_e7hh"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission672/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699021437338,
        "cdate": 1699021437338,
        "tmdate": 1699635994449,
        "mdate": 1699635994449,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "imPi37Lysf",
        "forum": "DorP300Q3b",
        "replyto": "DorP300Q3b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission672/Reviewer_RXkh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission672/Reviewer_RXkh"
        ],
        "content": {
            "summary": {
                "value": "A 2D multiple object tracker is proposed that consists of 3 steps, an object detector, a 3d descriptor and then an associator consisting of matching 3d features.   The paper provides good results on datasets from KITTI and Waymo and compares to other tracking algorithms."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper provides results on 2 driving datasets.  There is a 3 stage process, the first is acquiring depth values using SFM which is an offline process.  The 2nd stage uses a MLP to derive a 3d representation which is based on clusters of 3d points.  The third stage is the data association.  The results are good, slightly better than the other algorithms compared to."
            },
            "weaknesses": {
                "value": "The paper does not really compare to state of the art methods for MOT.  There are only 2 driving datasets compared to, the WayMo and Kitty datasets.  Also, the datasets chosen do not really demonstrate the ability to do multiple object tracking as at most there is 2  or at most 3 objects being tracked.  With regards to MOT, the datasets chosen should have been from the MOT challenge https://motchallenge.net.  The authors claim that using 3d features simplifies the problem, which it does for the dataset used, as objects are clearly separated by depth.  In addition, datasets that should have been considered also include GMOT-40 and Animal Track.   Long term pixel tracking is of interest recently which makes sense when objects are not separated by depth as in the examples given, some papers to look at include Tap-vid, Tapir, Particle video revisited, and tracking everything everywhere all at once.  \nI do not see how this method can perform real time tracking at all, first you need to train on a dataset and then you use SFM Colmap which is an offline process.  No generalizations of the method are demonstrated.  It would be interesting to see how well the method works if it was trained on WayMo and tested on Kitty and visa versa."
            },
            "questions": {
                "value": "Is this more than just a simple tracking by detection?  You do not appear to do a predict and correct by detection or observation as would be necessitated by a kaman filter.\nFor all your examples, the objects are well separated by depth, what happens when they are not?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission672/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission672/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission672/Reviewer_RXkh"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission672/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699027395302,
        "cdate": 1699027395302,
        "tmdate": 1700006098141,
        "mdate": 1700006098141,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9hVvTT8c5q",
        "forum": "DorP300Q3b",
        "replyto": "DorP300Q3b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission672/Reviewer_kQ8t"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission672/Reviewer_kQ8t"
        ],
        "content": {
            "summary": {
                "value": "The paper introduce a pipeline for solving 2D multiple object tracking by learning 3D object representation, 2D object appearance feature and an object association model.\nThe 3D object representation is learnt from Pseudo 3D object labels created from monocular videos using structure-from-motion approach.\nThe object association model consists of two components: GNN-based feature aggregation and a differentiable matching.\nThe experiments conducting on KITTI and Waymo Open Dataset demonstrate the effectiveness of the method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Paper is well written with extensive experiments to support the proposed method.\n2. A 2D MOT method that can leverage the power of 3D representation without LiDAR data or a depth estimation model."
            },
            "weaknesses": {
                "value": "1. The novelty of the paper is limited. The main idea is to generate pseudo 3D object labels from monocular videos so that it can be used to train the model to obtain 3D location / representation. There are main issues and details need to address about the process of generating these pseudo labels. Thus, in terms of the methodology for MOT, there is not much new development in this paper, e.g. GNN-based aggregation, association model learning and appearance using reID feature, etc. are the existing techniques in MOT literature.\n2. The impact of the paper is limited given the ego-centric datasets with LiDAR data widely available."
            },
            "questions": {
                "value": "1. The author should provide more details on how to filter low speed of ego-motion videos and moving objects. It is also not clear how tracklet of those moving objects being handle.\n2. How can the model learn if there is only loss to supervised static object? The output o^t_j can be any values.\n3. I would like to see how is the quality of pseudo 3D object labels impact on the performance of learned 3D representation. One ablation study can be done is to use real 3D object labels to train the model and compare.\n4. In table 3, there is an increase of ID Sw when using Baseline + 3D representation. How do you explain this behaviour? Is it suppose for 3D representation to help reduce ID Sw."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission672/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699063281300,
        "cdate": 1699063281300,
        "tmdate": 1699635994321,
        "mdate": 1699635994321,
        "license": "CC BY 4.0",
        "version": 2
    }
]