[
    {
        "id": "0PC6ETcpq4",
        "forum": "8T7m27VC3S",
        "replyto": "8T7m27VC3S",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7137/Reviewer_R3EY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7137/Reviewer_R3EY"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new dense caption dataset for autonomous driving. They design baselines by combining BEV and LLM. Some exploration experiments are conducted to show the effectiveness of the dense caption.1. Using LLM for AD is a hot topic in recent days, which is intriguing\n2. The promise of open sourcing a dataset with the cost of five expert human annotators to work for about 2000 hours is useful for the community."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Using LLM for AD is a hot topic in recent days, which is intriguing.\n2. The promise of open sourcing a dataset with the cost of five expert human annotators to work for about 2000 hours could be helpful for the community."
            },
            "weaknesses": {
                "value": "The actual usefullness of such a dense caption dataset is still unclear since there could lots of complex obstacles or information should be noticed in the driving scene, which might not be easy to describe precisely in language or covered by the template. (It is note worthy that 3D occupancy and open-set scene understanding are hot topics discussed in the community to deal with such problems). How  the dense caption actually could help autonomous driving system is actually unclear.\n\nThough in Table 6, the authors try to show that dense captions are useful. However, after applying such a complicated (and slow) pipeline to obtain the the CLIP embedding of the suggestions, it makes no significant difference compared with the original random initialized planning query in UniAD. \n\nIn summary, I appreciate the efforts the authors put to curate the dataset and I think it could be valuable for the further study, though it might not be clear how to properly use it right now. Thus, I give a borderline accept."
            },
            "questions": {
                "value": "See weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7137/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697542998710,
        "cdate": 1697542998710,
        "tmdate": 1699636845029,
        "mdate": 1699636845029,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tEGtZXnLOk",
        "forum": "8T7m27VC3S",
        "replyto": "8T7m27VC3S",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7137/Reviewer_rzAd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7137/Reviewer_rzAd"
        ],
        "content": {
            "summary": {
                "value": "This paper targets the captioning task in the autonomous driving domain. For the dataset part, the authors use a LLaMa-Adapter and GPT-3.5 to describe the appearance of pre-labeled objects and generate spatial position-related descriptions with the ground truth from the original nuScenes dataset. For the methodology part, BEVFusion-MIT and BEVFormer are adopted to get the BEV feature and object proposals, which are then fused and sent to LLaMa via an adapter to predict the final captions. Experiments are conducted on 3D object proposals and 2D proposals. Besides, the authors also show that providing an End-to-end method (UniAD) with LLM-based planning queries can slightly improve the performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper has two parts, dataset and model. Different components, annotation processes, and model architecture are basically clearly written and easy to follow. The analysis of the captioning dataset is relatively comprehensive, including multiple graphs illustrating distributions of words and sentences. The authors also present a baseline method to realize the captioning task. The method builds upon the existing detection method and is easy to reproduce."
            },
            "weaknesses": {
                "value": "The reviewer is not fully convinced by the importance and soundness of the `comprehensiveness` of the proposed task and dataset.\n\n- The annotation process starts with pre-labeled 3D/2D bounding boxes. The proposed task **does not go beyond previous definitions in terms of object-level comprehensiveness**. Why not directly attach more metadata to the original bounding box information?\n- The authors claim that their description is beyond nouns compared to existing 3D bounding boxes. The current nuScenes dataset provides bounding boxes with position, heading angle, velocity, and category. Besides converting these annotations into language, the proposed dataset mainly gives more descriptions of objects' visual appearance. It can be seen in Fig. 2(a,b), that **most words are about objects' colors**, which are not so helpful for driving IMO. The dataset also **drops the heading angle and accurate velocity** information in classical box information which are sometimes critical for downstream tasks. In all, the new middleware, language, does not take more helpful information for driving.\n- The over-claim of 'scalable automatic labeling pipeline': The annotation takes human evaluation in the loop, which is not fully automatic and thus not easy to scale. The influence of human check should be explained if a finetuned expert captioner can realize full automatic annotation when scaling to more diverse data.\n\nClarifications on experiments are needed. \n\n- Comparisons to previous methods do not bring more insights. It is possible that the gains come from better detectors. What are the corresponding captions in 2D dense captioning?\n- Though the method serves as a baseline for the proposed task, it is beneficial to have some ablations to provide more insights, eg., ablations on adapters or training strategies.\n\nSome other points about writing:\n\n- Sec 2.3 (Related work: Learning-based / End-to-end Autonomous Driving) seems not so relevant to the main contents of the paper. The reviewer suggests removing this part.\n- The input modality in Tab. 1 may have some incorrect parts. For example, Talk2Car/Refer-KITTI builds on top of nuScenes/KITTI thus it can also take LiDAR as input.\n- P_{ego} instead of P_{O} in Equation 1. Equations 1-3 are relatively too naive to show them in a single line IMO."
            },
            "questions": {
                "value": "- Are the object proposals boxes (N x 9) or features (N x C)?\n- In Fig. 15, how to tell that UniAD fails to avoid the bus stop from the visualization?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7137/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698647333584,
        "cdate": 1698647333584,
        "tmdate": 1699636844906,
        "mdate": 1699636844906,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "O3UcGs4BFd",
        "forum": "8T7m27VC3S",
        "replyto": "8T7m27VC3S",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7137/Reviewer_a5o2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7137/Reviewer_a5o2"
        ],
        "content": {
            "summary": {
                "value": "The authors of this study investigate intermediate representations that connect perception and planning. They propose that a good representation should be comprehensive and standardized. However, existing representations are not compatible with recent language foundation models that have demonstrated exceptional reasoning capabilities. In order to address this, the authors explore an intermediate representation called DESIGN (3D dense captioning beyond nouns) to determine if it can enhance the intelligence and safety of autonomous vehicles. They develop a scalable automatic labeling pipeline to create a large dataset called nuDESIGN, which consists of 2,300k descriptions for 1,165k objects in 850 scenes. Additionally, the authors contribute DESIGN-former, a query-based network architecture that fine-tunes adapters on LLaMa. DESIGN-former outperforms existing dense captioning baselines by a significant margin. Lastly, the authors present a pilot study that demonstrates the impact of this new middleware representation on an end-to-end driving model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Importance of the work: The authors highlight an important direction for learning representations that connect perception and planning. While the community explores different intermediate representations such as bounding boxes, semantic maps, and occupancy grids, a consensus on the form of representations has not been reached. This topic will be of interest to the community studying representation learning and learning-based driving models.\n2. Challenge of the work: In this work, the authors aim to define a comprehensive and standard representation that also possesses reasoning ability. The objective is challenging and requires a comprehensive evaluation to demonstrate the effectiveness of the proposed representation.\n3. A new intermediate representation called DESIGN (3D dense captioning beyond nouns): The representation consists of five different components: Appearance, Direction, Distance, Motion, and Road Map. The authors propose a specific form for the representation that covers a wide range of information. Furthermore, these pieces of information (direction, distance, motion, and road map) can be extracted from existing representations. The reviewer finds DESIGN to be a standard representation that defines a clear format for forming the representation.\n4. A new dataset called nuDESIGN has been collected using the proposed automatic pipeline. nuDESIGN contains 2300k descriptions for 1165k objects in 850 scenes. To the best of the reviewer's knowledge, this is the largest dataset with language captions. Additionally, the dataset enables 2D/3D dense captioning in traffic scenes. As far as the reviewer know, the size of the dataset is the largest in traffic scenes.\n5. Method and Experiment: On the proposed dataset nuDESIGN, the proposed method DESIGN-former achieves favorable results in 2D/3D dense captioning, compared to"
            },
            "weaknesses": {
                "value": "1. My main concern is that the paper does not clearly explain the effectiveness of DESIGN for downstream tasks. The reviewer believes that this should be the main focus of the paper, as mentioned in the last sentence of the first paragraph of the introduction. However, most of the evaluations focus on 2D/3D dense captioning, rather than the original motivation of enabling smarter and safer autonomous vehicles. I recommend that the authors focus on demonstrating that the proposed nuDESIGN is a \"new addition\" for 2D/3D dense captioning in traffic scenes.\n2. The experiment on the end-to-end driving model is not convincing. While the authors show an approach to link the language model with the end-to-end model, the results do not explicitly demonstrate that the learned representations enable human-like reasoning. I suggest that the authors consider scenarios studied in ReasonNet (Shao et al., \"ReasonNet: End-to-End Driving with Temporal and Global Reasoning, CVPR 2023\"), which require explicit modeling of socially semantic spatial-temporal relationships.\n3. The smoother trajectory shown in Figure 15 is also not convincing, considering the significant effort in creating a dataset and large language models. The results could be generated through post-processing of the generated trajectories. Is it necessary to include the proposed method? I suggest that the authors consider scenarios that are challenging for existing methods.\n4. Limitation of DESIGN. While the middleware captures a wide range of information, two critical aspects are missing: uncertain intents of an object and the ability to facilitate forecasting. These aspects have been extensively studied and found to be valuable in the field of autonomous driving. I would like to learn the authors' thoughts on these aspects.\n5. There is a lack of comparisons with existing baselines for 3D dense captioning. For example:\n    1. Chen et al., \"End-to-End 3D Dense Captioning with Vote2Cap-DETR, CVPR 2023.\"\n    2. Cai et al., \"3djcg: A unified framework for joint dense captioning and visual grounding on 3d point clouds, CVPR 2022.\"\n    3. Chen et al., \"D3net: A speaker-listener architecture for semi-supervised dense captioning and visual grounding in rgb-d scans, ECCV 2022.\"\n    \n    Without a direct comparison, we cannot conclude whether the proposed method achieves state-of-the-art performance."
            },
            "questions": {
                "value": "The reviewer identify five major concerns in the Weakness section and would like to know the authors' thoughts on these points. Please answer each concern in the rebuttal stage. The reviewer will respond according to the authors' rebuttal in the discussion phase."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7137/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698679728186,
        "cdate": 1698679728186,
        "tmdate": 1699636844789,
        "mdate": 1699636844789,
        "license": "CC BY 4.0",
        "version": 2
    }
]