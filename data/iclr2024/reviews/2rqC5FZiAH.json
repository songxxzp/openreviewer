[
    {
        "id": "QjWvq6h92n",
        "forum": "2rqC5FZiAH",
        "replyto": "2rqC5FZiAH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3086/Reviewer_Mwh5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3086/Reviewer_Mwh5"
        ],
        "content": {
            "summary": {
                "value": "This paper explores how to design an adaptive attack to circumvent backdoor defenses. Specifically, the authors propose to partition selected samples into different parts where each part has a unique trigger pattern. In particular, the authors design a trigger-focusing module to ensure that a partition can only be attacked by its designated trigger, not by any other trigger or trigger combinations. The authors evaluate their method under 13 backdoor defenses on 4 datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The topic is of great significance. Figuring out how to design more stealthy backdoor attacks (against backdoor defenses) is one of those constant themes in the field.\n2. The main idea is easy to follow.\n3. The authors try to analyze the resistance to adaptive attacks and why their method is effective. It should be encouraged."
            },
            "weaknesses": {
                "value": "1. In general, the writing is poor. \n- The concept of bounded and under-bounded is unnecessary and misleading. Specifically, the 'bounded attack' means that the trigger is effective on any input, whereas the 'under bounded' means that attacks can be removed. The definitions of them are unrelated or even inclusive. I don't understand what purpose the author had in making those two words. \n- At the beginning of page 2, the authors claim that sample-specific attacks leverage adversarial training to encourage the model to focus on the correlation between the trigger and the input sample. However, to the best of my knowledge, none of them using adversarial training. They only claim that different poisoned samples contain different trigger patterns.\n- The definition part of Chapter 3 is also very confusing and redundant.\n- The design part of the method is also strange. It appears that the authors only select samples from a particular category and poison them, yet the authors don't state why. In principle, shouldn't the author select samples from all categories to poison? After all, as long as multiple triggers can activate the backdoor separately and independently, the backdoor is difficult to recover?\n- In addition, the authors have always emphasized that reducing the generalizability of triggers makes them more difficult to recover. I think that's right, but why not just reduce generalizability and instead still implant multiple different triggers? \n- Overall, the design of the authors' methodology is not consistent with their motivation (bypass existing trigger inversion). In my opinion, it's not as straightforward as just planting a number of different backdoors at the same time.\n- The caption of Figure 4 is too long.\n\n2. Missing important technical details. \n- Please provide more details about why you need to do the clustering.\n- Please provide the poisoning rates of all attacks. Are they the same?\n\n3. Please provide more details and explanations about why your attack is more effective under backdoor-removal attacks. To me, this doesn't seem to have a direct causal relationship to your method itself.\n\n4. Missing important experiments.\n- There is no ablation study about the trigger focusing module.\n- The authors claim that their designed modules are effective in preventing backdoor detection. However, the authors do not verify it in their ablation study.\n- Please exploit more advanced method instead of NC to design adaptive defenses. As we all know that NC is ineffective under many \ncases."
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3086/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672896039,
        "cdate": 1698672896039,
        "tmdate": 1699636254562,
        "mdate": 1699636254562,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mtwtBd7p9G",
        "forum": "2rqC5FZiAH",
        "replyto": "2rqC5FZiAH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3086/Reviewer_rKsD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3086/Reviewer_rKsD"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the security threat posed by backdoor attacks in Deep Learning applications. Existing backdoor attacks are often susceptible to detection and mitigation techniques due to their unbounded or under-bounded attack scope, where triggers can cause misclassification for any input. This paper proposes a novel backdoor attack called LOTUS, which is evasive and resilient by limiting the attack scope. LOTUS uses a secret function to separate samples in the victim class into partitions and applies unique triggers to each partition. It also incorporates a trigger-focusing mechanism to ensure that only the trigger corresponding to a specific partition can induce the backdoor behavior. Extensive experiments demonstrate that LOTUS achieves high attack success rates across four datasets and seven model structures while effectively evading 13 backdoor detection and mitigation techniques."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This work could be attributed to the class of \u201ccontrollable adversarial attack\u201d, which explores the potential to bound the backdoor attack and enhance its stealness, which could lead to more robust models.  \nThe strengths of this paper are as follows:  \n1. Originality: The paper presents a novel backdoor attack called LOTUS (\"Evasive and ResiLient BackdOor ATtacks throUgh Sub-partitioning\"), which effectively bounds the attack scope by dividing victim-class samples into sub-partitions and using unique triggers for each partition. This innovative approach sets it apart from existing backdoor attacks that rely on uniform patterns or complex transformations as triggers.  \n2. Quality: The authors address a key challenge in implementing LOTUS by introducing a novel trigger focusing technique to ensure that a partition can only be attacked by its designated trigger, not by any other trigger or trigger combinations. The paper's methodology is well-thought-out and thoroughly explained, demonstrating the quality of the research.  \n3. Clarity: The paper is well-written and well-organized, providing clear explanations of the concepts and techniques involved in the proposed LOTUS attack. The authors also provide a comprehensive overview of the related work, which helps contextualize their contribution within the broader field of backdoor attacks and defense.  \n4. Significance: The extensive evaluation of LOTUS on four datasets and seven model structures demonstrates its effectiveness in achieving high attack success rates while evading 13 state-of-the-art backdoor defense techniques. This result highlights the significance of the proposed attack and its potential impact on the security of deep learning applications."
            },
            "weaknesses": {
                "value": "While this paper introduces the novel LOTUS backdoor attack and demonstrates its effectiveness across various datasets and model structures, there are some weaknesses that could be addressed to improve the work:  \n1.  Extension to universal attacks: The paper mentions that LOTUS can be extended to universal attacks, but does not provide experimental evidence or a detailed discussion on how this can be achieved. It would be useful to provide more information on how LOTUS can be adapted for universal attacks and to demonstrate its effectiveness in this setting.  \n2. Potential countermeasures: The paper could discuss possible countermeasures that could be developed to defend against LOTUS, as well as the challenges in creating such countermeasures. This would provide a more balanced view of the attack and contribute to the ongoing research in backdoor defense."
            },
            "questions": {
                "value": "1. How does LOTUS perform in scenarios where the defender has partial knowledge of the sub-partitioning function or the corresponding triggers? Would this compromise the attack's evasiveness and resilience?  \n2. The paper focuses on label-specific attacks; can you elaborate on how LOTUS can be extended to universal attacks? What challenges might arise in this extension, and how do you plan to address them?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3086/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760086358,
        "cdate": 1698760086358,
        "tmdate": 1699636254434,
        "mdate": 1699636254434,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "T4i8ia3bsF",
        "forum": "2rqC5FZiAH",
        "replyto": "2rqC5FZiAH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3086/Reviewer_1k7k"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3086/Reviewer_1k7k"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a two-step approach for trojan injection into machine learning models. First, it suggests partitioning the training data and then injecting distinct triggers for each partition with respect to a specific victim class. Additionally, a novel loss function is introduced to ensure that only the trigger associated with the partition can induce the trojan behavior. The primary motivation behind this approach is to reduce the visibility of the trigger pattern, making it harder to detect trojans using existing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strengths:\n\n- The evaluation demonstrates that this method outperforms all existing trojan detection techniques when compared to previous approaches.\n- The paper provides a comprehensive analysis of various data partitioning methods and the impact of the number of partitions.\n- The concept of partitioning the data within the trojan class is a novel contribution to the field."
            },
            "weaknesses": {
                "value": "Weaknesses:\n\n- The paper lacks an in-depth analysis of each design component.\n- The writing could be improved for better clarity and understanding."
            },
            "questions": {
                "value": "Comments:\n\nI am not an expert in trojan injection due to the rapid developments in the field. Nevertheless, I have some questions and concerns about the paper, which I've outlined below:\n\n- Your paper mentions 'unbounded' and 'under-bounded' as key motivations. I understand the 'unbounded' part, but the 'under-bounded' aspect isn't clear to me. Could you explain why \"neglecting the combination of triggers\" makes adversarial poisoning more easily detectable?\n\n- I noticed the ablation studies in data partitioning, but I didn't find an ablation study regarding the loss function. Are all components equally critical in evading trojan detection? For instance, the 'Dynamic loss' term, which ensures a trojan's effectiveness for a specific partition, what happens if you remove it? Would it expose the trojan? \n\n- Can explicit/implicit \"data partitioning\" be combined with other trojan injection methods such as WaNet? It seems that the trojan patterns injected into each data partition resemble the human-designed trojan in BadNet (as given in Figure 15). Is my understanding correct?\n\nIn addition to these questions, I believe that overall, the paper is OK. However, I recommend improving the writing by highlighting your key ideas and incorporating essential analyses from the appendix into the main paper to make it self-contained."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3086/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808602355,
        "cdate": 1698808602355,
        "tmdate": 1699636254348,
        "mdate": 1699636254348,
        "license": "CC BY 4.0",
        "version": 2
    }
]