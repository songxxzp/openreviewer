[
    {
        "id": "ZiuUSW1fdR",
        "forum": "OgTMbLDQZu",
        "replyto": "OgTMbLDQZu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission565/Reviewer_VxaA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission565/Reviewer_VxaA"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method to incorporate vision-language backbones into the task of unsupervised domain adaptation, which would work universally for the tasks of semantic segmentation and object detection. They make three key contributions - which are universal backbones and proposal generation, language guided mask consistency and hierarchical visual-language alignment. They use Promptable Language-Task Learning (PLL) mechanism to collaboratively embed the language and task prompts. This is followed by mask-consistency on target samples with language-guided mask selection, and hierarchical VLA using proposals and prompts. Strong results are showcased on multiple detection and segmentation datasets on closed- and open-world settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper tackles a very challenging problem of open-world domain adaptation in segmentation and detection which is a practical real world problem.\n\n- The use of pre-trained CLIP models into UDA and open-world DA methods is interesting."
            },
            "weaknesses": {
                "value": "- I am not sure I understand where the open-world capabilities of the models come from. Detecting mask-proposals in an open-world has already been extensively explored in literature before (for example, Lseg[1] and FreeSeg[2]), so I do not think this paper proposes anything particularly targeted towards adapting to open-world categories. \n\n- Adding to the above, the claimed contribution of joint architecture for segmentation and detection seems to be not fully true, as the architecture still has different decoding heads for segmentation and detection, different loss function, different task prompts and different training methodologies.\n\n- Most importantly, the proposed components have very similar counterparts in existing literature. Specifically, the joint task-langauge prompt is very similar to the prompt design in FreeSeg [2], but FreeSeg was not even compared or mentioned. Likewise, the masked image consistency loss is already adopted to good effect in [3], but that work is not adequately cited. Finally, techniques similar to language-guided masking adopted in [4], which also needs to be highlighted. \n\n- It is also not clearly stated if the text encoder is learnable or frozen? Also, if the text encoder is frozen, how are the prompts learned? The gradients are only passed to the prompts? This needs further explanation. \n\n- Sec 3.2.1 states that a SWIN backbone transformer is adopted, while 4.2 states a DAFormer with MiT-B5 is adopted. This needs to be further clarified. \n\n- A large portions of the method section, including the motivation and the design, is unclear from the current text. I would request the authors to make a more polished version in the next version of the paper. \n\n- I am not sure I understand what is hierarchical in the hierarchical visual-langauge alignment? Isn't the loss very similar to the classic contrastive loss in CLIP, but at region level instead of image-level? Also, the sentence reads `In each training batch, the input images reflect three levels of domain hierarchy: intra-source, inter-source.`, what is the third?\n\n- I would recommend the authors to explore the possibility of also including open-world segmentation methods (like LSeg or more) as possible baselines as they also train with CLIP and open-vocabulary capabilities.\n\n[1]. Li, B., et al. \"Language-driven semantic 427 segmentation.\" arXiv preprint arXiv:2201.03546 428 (2022).\n\n[2] Qin, Jie, et al. \"FreeSeg: Unified, Universal and Open-Vocabulary Image Segmentation.\" CVPR. 2023.\n\n[3] Hoyer, Lukas, et al. \"MIC: Masked image consistency for context-enhanced domain adaptation.\" CVPR. 2023.\n\n[4] Li, Gang, et al. \"Semmae: Semantic-guided masking for learning masked autoencoders.\" NeurIPS 2022."
            },
            "questions": {
                "value": "- Overall, the method seems to involve several distinct components which are not necessarily related to the broad goal of open-world domain adaptation. I would request the authors address the questions posed above, and I would be happy to raise my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission565/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698347777724,
        "cdate": 1698347777724,
        "tmdate": 1699635983867,
        "mdate": 1699635983867,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8doeOVyByn",
        "forum": "OgTMbLDQZu",
        "replyto": "OgTMbLDQZu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission565/Reviewer_Nx7u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission565/Reviewer_Nx7u"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to propose an open-vocabulary domain adaptation framework based on vision-language models that can be applied to various core vision tasks, including object segmentation and semantic segmentation.\nThe proposed method is built by combining several technical components, but the core parts mainly contribute to unifying object detection and semantic segmentation tasks are: Unified Proposal Decoder (UPD) to decode the region to be decoded according to the task, and Promptable Language-Task Learning to explicitly embed task information into the text prompt.\nExperimental results on datasets for domain adaptive semantic segmentation show that the proposed method can achieve accuracy comparable to or better than the state-of-the-art domain adaptation methods in both closed- and open-vocabulary settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The biggest contribution of this paper would be to address the challenging problem of constructing a unified framework for open-vocabulary object detection and semantic segmentation based on vision language models, a task that has not been well studied.\n\nOn the datasets used in the experiments, the proposed method achieves better accuracy than the state-of-the-art domain adaptation methods."
            },
            "weaknesses": {
                "value": "1. The goal of this paper is to provide a unified framework for various \"core vision tasks\". However, this paper focuses only on object detection and semantic segmentation, and it is not clear how well it will perform on other tasks.\n\n\n2. In reality, the proposed method uses specialized modules and technical elements (region decoding and mask decoding) for object detection and semantic segmentation, respectively. Due to these parts customized for each downstream task, it may be a bit questionable whether we can say that the proposed method is truly a unified framework.\n\n\n3. Despite the considerable complexity of the overall design of the proposed method, the results of the ablation study in Table 4 show that EMA, a well-known idea in various learning tasks, provides the largest gain. This seems to obscure the technical innovation brought by this paper.\n\n\n4. One of the main components of the proposed method, Language-Guided Masked Consistency (LMC), generates masks (pseudo-labels) based on the output of the teacher model to (ideally) always mask out object regions. Unlike MLM/MIM/MIC, which randomly masks regions of the image, the proposed LMC always requires that objects be predicted from surrounding non-object regions, which seems to have the potential to enhance undesirable context bias (in an extreme example, if a cow is in the driveway, it would be recognized as a car), is there any justification or discussion on this point?\n\n\n5. Several details of the proposed method are missing. Specifically, I could not find details on \"Task-Conditioned Initialization\" depicted in Fig. 1 and \"Unified Proposal Decoder (UPD)\". \n\n\n6. I would say more related papers on open-vocabulary object detection/segmentation could be included in the paper. For example, [a] introduces a head architecture that conditions bounding box regression and mask segmentation by text embedding vectors for open-vocabulary instance segmentation. Inclusion of these highly relevant papers will clarify the technical contribution of the proposed method.\n\n[a] Tao Wang, Learning to Detect and Segment for Open Vocabulary Object Detection, in CVPR 2023.\n\n\n7. Some minor errors.\n* Fig. 1: Lean-able -> Learnable?\n\n\n* Fig. 1: The curly arrow from \"Target Domain Masked Image x_1^T\" to \"Target Domain Masked Image x_2^T\" should be from \"Target Domain Image x_2^T\" to \"Target Domain Masked Image x_2^T\". \n\n\n* I could not find \"Unified Proposal Decoder (UPG)\" in Fig. 1, which should be explicitly depicted for improving readability.\n\n\n* The abbreviation MLM first appears in Sec. 3.2.2, but what it stands for is never explained. While it is clearly masked language modeling from the context but should be specified\n\n\n* \"three levels of domain hierarchy: intra-source, inter-source.\" Considering Fig. 1, I am guessing this should be \"four levels of domain hierarchy: intra-source, multi-source-target, and multi-source-masked-target.\""
            },
            "questions": {
                "value": "My questions are listed in descending order of importance in the Weaknesses section above. I would be grateful if the authors could answer as many of my questions as possible."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission565/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698661802206,
        "cdate": 1698661802206,
        "tmdate": 1699635983767,
        "mdate": 1699635983767,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s5MU5v2ONm",
        "forum": "OgTMbLDQZu",
        "replyto": "OgTMbLDQZu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission565/Reviewer_QBkk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission565/Reviewer_QBkk"
        ],
        "content": {
            "summary": {
                "value": "The method proposed in this paper addresses the vocabulary-specific gap across different tasks and datasets, achieving commendable results in unsupervised domain adaptation benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper advances the performance of unsupervised domain adaptation benchmarks by introducing and employing more advanced  techniques."
            },
            "weaknesses": {
                "value": "1. The paper contains an many modules and abbreviations, making it challenging to read and comprehend. \n2. The image encoder is not adequately introduced, and it is unclear whether it was pre-trained on Im1k or 21k, along with its scale for comparison.\n3. Some of the techniques in the paper lack references. For instance, in Section 3.2, the paper mentions the use of MIM & EMA without referencing relevant literature. Similarly, the Unified Proposal Decoder in Section 3.2.1 appears to be derived from Detr and related variants. Sections 3.2.2 and 3.2.3 share a similar issue.\n4. This paper leverages advanced pre-trained models like CLIP and incorporates several advanced designs to outperform the baseline, which is not very surpursing. \n5. The paper mentions using the CLIP text encoder to address vocabulary-specific issues, which seems more akin to tackling open-vocabulary problems. What are the distinctions between the two, and how does the proposed approach compare to open-vocabulary methods?"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission565/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699198418873,
        "cdate": 1699198418873,
        "tmdate": 1699635983609,
        "mdate": 1699635983609,
        "license": "CC BY 4.0",
        "version": 2
    }
]