[
    {
        "id": "ippG4AUL6m",
        "forum": "xwKt6bUkXj",
        "replyto": "xwKt6bUkXj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5111/Reviewer_TkDB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5111/Reviewer_TkDB"
        ],
        "content": {
            "summary": {
                "value": "The authors explore how RNNs derive a solution during curriculum learning for working memory tasks. In particular, in pursuit of functional working memory the authors contrast the optimisation of intrinsic neuronal timescales, as seemingly favoured by single-headed RNNs which are trained on one task, with the optimisation of recurrent weights, as favoured by multi-headed RNNs which are trained on several tasks at once."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is generally well written \n- The authors discover, in my view, an interesting and convincing contrast in the solutions obtained by single vs multi-head learning with respect to intrinsinc vs network mediated timescales. \n- I appreciate the depth of analysis undertaken by the authors with respect to the performance and dynamics of the networks, and generalisability to other network architectures"
            },
            "weaknesses": {
                "value": "- I think this paper could and should be valuable to neuroscientists, but at the moment the link between the paper results and biology is not so clear. For example, do the authors surmise that optimised, heteregenous instrinsic neuronal timescales should not be employed in the brain? Would there be any experimental predictions one could infer from this work? \n- Related to the above, it is not clear to me if other potential enablers of short-term memory have been considered. For example, there is one experimental work (Hu et al. 2021, PLOS) which comes to mind which shows that short-term plasticity (STP) in neurons, rather than recurrent connectivity, is the more consistent enabler of short term memory in a visual task. Similarly, one computational work (Salaj et al. 2021, Elife) promotes spike frequency adaptation as a powerful tool for sequence processing. I would like to know the authors' opinion on these additional neural mechanisms known in biological circuits. \n- The tasks selected are relatively simple and it is not clear if these results generalise to harder temporal tasks, though this is addressed as a limitation of the study. More generally, I would caution the authors' with the implication that their findings can generalise to \"networks on sets of related tasks instead of a single task\".The authors do demonstrate their case well for the setting of learning different N simultaneously during curriculum learning, but whether these results apply when, e.g. language processing in two different languages, is not clear to me. \n- The motivation for two working memory tasks: N-parity and N-DMS, is not very well presented. What are the differences between the tasks. The authors note that Fig 6b reflects \"distinct computational requirements for each task\" but this is not elaborated upon."
            },
            "questions": {
                "value": "- In the 4th paragraph of section 2 it is written tau_{net} <= tau, should it not be the other way around?\n- For fig 3a, which task is that? N-parity? The learning curves on this plot also appear to be still going up; it is not also clear whether these N will eventually be learned. \n- Section 3.1: \"N -parity: The network has to output the binary sum (XOR) of the last N digits, indicating an odd or even number of non-zero digits\". I don't understand this; is it that the model should output whether the sum of the last N digits is odd or even?\n- Same N-Parity section: \"the value of the digit presented at t \u2212 N needs to be subtracted from the running sum\". I am not such a fan of this description as the authors' are suggesting the underlying mechanism for how the RNN solves the problem is known. Perhaps the RNN is incorporating another solution; e.g. perhaps N running sums are simultaneously encoded in the RNN and are reset every N timesteps. \n- For Figs 3a, 4a, do these results also apply for the N-DMS task? Why are only \n- is k > 1 ever considered? for the multi-head case the authors claim that tau -> k, and this seems true for k = 1; was this also tested for higher k?\n- For Fig 3 was tau fixed? so only recurrent connectivity optimised? This should be made clear\n- is Fig 5a for the single-headed network?\n- For Fig 5d in each task the network timescale tau_net increases up to about N=30 then shrinks. Is this shrink surprising? It is also unclear to me how these multi-headed network can perform the task so well, when their tau_net is overall considerably smaller (about half the size) of the worse-performing single-head networks\n- Fig 6b: I was surprised at how high dimensional these network activities are. In my experience trained RNNs produce quite low dimensional dynamics (< 10 or often < 3). Is this high dimensionality surprising to the authors?\n- \n\nTypos and suggestions: \n- add (AC) after autocorrelation in the Fig 1 caption. \n- In the 4th paragraph of section 2: tnet -> tau_{net}\n- I would appreciate a more formal, mathematical formulation of how tau_{net} was obtained for each neuron in the appendix\n- End of first paragraph of section 4.1, remove comma after K\n- I'd recommend the related work prior to the results\n- I'd recommend all supplementary figures together at the very end. Their appearance amongst the different appendix sections seems quite random/distracting"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5111/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5111/Reviewer_TkDB",
                    "ICLR.cc/2024/Conference/Submission5111/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5111/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697544729104,
        "cdate": 1697544729104,
        "tmdate": 1700666168424,
        "mdate": 1700666168424,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YFPfMvUlpV",
        "forum": "xwKt6bUkXj",
        "replyto": "xwKt6bUkXj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5111/Reviewer_XC2K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5111/Reviewer_XC2K"
        ],
        "content": {
            "summary": {
                "value": "Training RNNs can adjust the time scales of their responses in order to perform memory tasks. The authors point out that this may involve adjustment of the time scales of individual neurons, or of the network-generated timescales created by interactions between neurons. They set out to tease apart the roles of either component in different memory tasks and different training settings.\n\nThey train RNNs on two memory tasks whose difficulty increases with sequence length N (N-DMS and N-parity), and under two settings (successive Ns on a single network, and all-N-so-far with different heads for different Ns).\n\n They show that the single-head setting tends to favor changes in single-neuron timescale, while the multi-head setting tends to keep single-neuron timescale low and increases network-generated timescales instead.\n\nVarious ablations and analyses confirm this observation. The multi-head networks are shown to rely more on shorter-timescale neurons, while the single-head networks rely more on the longer-timescale neurons (whose tiomescales are much longer in the first place). Multi-head networks are more robust to parameter perturbation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- I'm not aware of this question (whether RNNs learn more by adjusting single-neuron or network-generated timescales) having been addressed previously.\n\n- The analyses are convincing, though some clarifications are needed."
            },
            "weaknesses": {
                "value": "I did not see any obvious \"weakness\" in the demonstration. I do note that the tasks and settings are quite specific, and it's not clear how instructive the results are for more realistic domains (this is also noted by the authors in the Limitations)"
            },
            "questions": {
                "value": "- Most glaringly: The authors themselves point out that tau=1 makes the network memoryless. Yet in Figure 4 they show the networks easily solve difficult memory tasks with tau fixed at 1? How is that even possible?\n\n- The initial text suggests that tau and tau_net are independent components (one strictly single-neuron, one strictly network-generated). However, later on it is acknowledged that longer tau causes longer tau_net. Some clarification would be in order."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5111/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697570484495,
        "cdate": 1697570484495,
        "tmdate": 1699636503066,
        "mdate": 1699636503066,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5l3H63jAfn",
        "forum": "xwKt6bUkXj",
        "replyto": "xwKt6bUkXj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5111/Reviewer_h2ki"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5111/Reviewer_h2ki"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates mechanisms in recurrent neural networks for solving tasks that require long memory representation. The motivation for this work is that organisms need to process signals from a variety of timescales, and discovering mechanisms for timescale adaptation may be useful. Prior work has studied timescales in both single neurons and networks, and this paper aims to study specifically how these timescales shape RNN dynamics and performance on long-memory tasks. \n\nThe paper introduces the model: an RNN where each neuron has a trainable leak parameter tau - for higher values of tau, memory is a higher component of the input; for lower tau, the incoming input and recurrent signals dominate. The paper considers dynamics in terms of the single-neuron timescales tau and the network timescales (estimated from the decay rate of the neuron activity's autocorrelation).\n\nExperiments are conducted for two tasks: N-parity and N-DMS. Two curricula are used: single-head, with a single response head that does the task for the input N, and multi-head, which adds a head for every subsequent N during training and therefore does the task for all integers up to N. This mitigates catastrophic forgetting seen in the single-head model as higher N come up in the curriculum.\n\nResults are presented for a variety of research questions. First, performance under different curricula: results here show performance without a curriculum (N alone) - these fail for N > 10. Then, single-head vs. multi-head: multi-head does far better. Next, investigation of the mechanisms underlying long timescales, in which the paper argues that while the single-head curriculum leads to long single-neuron timescales for memory purposes (large, heterogenous tau), the multi-head curriculum leads to shorter tau (and higher tau_net and activating recurrent interconnectivity), suggesting that the memory mechanism is more in the recurrent connections. Finally, ablations are investigated, looking at ablations of neurons with especially short or long tau values, perturbations to weights and timescales, and retraining, for all of which the multi-head approach is superior.\n\nThe paper finally takes us through a more detailed version of the related work in the intro, and concludes by suggesting that 1) training on sets of related tasks may lead to more performant and robust networks than training on single tasks and 2) long timescales are necessary for long memory but these can be achieved through connectivity rather than just single neurons."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "#### Quality\n- Experiments are highly intuitive\n- The paper is framed around comparison - N-parity and N-DMS, results on which both serve the same goal and also allow comparison on difficulty; single- and multi-head; single-neuron and network-mediated timescales. The results space is therefore very accessible to reason about.\n- Results are compelling and analysis is convincing - section 4 has a lot of useful material, and each subsection makes interesting claims. Overall, the results mostly back up the claims. \n\n#### Clarity\n- Figure 7 is useful and well-annotated, and a good way of presenting the data\n- Sections 2 and 3 is well-written. The model setup and experiments are presented very clearly - I am not confused at any point. \n- More involved concepts (tau_net, autocorrelation) are made easy to understand\n- Ablation/perturbation/retraining setup is particularly useful to understand the nature of the problem.\n\n#### Originality\nBased on related work, this paper does appear to position itself as an experimental realization of theory presented in prior works, leading to a novel and unknown result\n\n#### Significance\nThe setup is compelling, and the suggested conclusions (1 and 2 from my summary above) are promising and significant if useful."
            },
            "weaknesses": {
                "value": "#### Quality\n- Claims in conclusion seem overstated. This is more a significance issue than a quality issue.\n- I would say more experiments are needed, but it's not a matter of specific additional experiments - for what it is, this seems to be a careful and informative set of experiments. It's more that the paper needs considerable expansion, which may also be a significance issue. \n- Figure 8: the curves do show a difference but not a drastic one, making it hard to judge how much more robust multi-head is. Significance testing, grounding according to some baselines (including visually on the figure), or comparison to similar results in other contexts may help. \n\n#### Clarity\n- While Section 4 has a lot of good info, it is difficult to understand. The writing is prose-based and somewhat meandering and disorganized. The subsection structure is useful, but I think it would help to add subsubsections, especially because the subsections are topic-based rather than claim-based. It would help to add claim-based subsections (or paragraphs) - e.g. in section 4.2, a subsubsection summarizing in a sentence how tau and tau_net change as N increases.\n- Aside from fig 7, figures are under-annotated - it takes a lot of effort to understand the message. This could be easily rectified with more visual information. Figure 4a in particular needs better explanation - the x-axes are confusing.\n- The term \"time-lag\" is introduced without clear definition, and it takes a while to get the relevant info. \n\n#### Originality\nNo weaknesses as far as I am aware.\n\n#### Significance\nAs stated in the limitations, there is only one testbed in this paper and it is very toy. Unfortunately, the claims of this paper seem to be about improving RNN performance, which means that some kind of scale or breadth is requisite. The motivation connects to biological neural networks, but that stops at the intro - the paper never connects the investigated mechanisms back to biological intelligence in any way after the prose in the intro, let alone through experimentation. It's therefore unclear how significant these results are, though they are intriguing."
            },
            "questions": {
                "value": "- What other experimental settings/testbeds might be easily accessible for your setup?\n- How might this connect back to biological intelligence? I realize this isn't necessarily the focus of the paper, but it is discussed extensively in the intro. \n- Section 4.2 goes into this but it's hard to get key conclusions: it's clear that with the multi-head curriculum, single-neuron timescales are not the dominant memory mechanism. What intuition should we take away about the memory mechanism the multi-head curriculum *does* induce? We see that high, less inhibitory network connections are involved - anything more specific?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5111/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698911403236,
        "cdate": 1698911403236,
        "tmdate": 1699636502983,
        "mdate": 1699636502983,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6yLaNM3DIX",
        "forum": "xwKt6bUkXj",
        "replyto": "xwKt6bUkXj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5111/Reviewer_vRYi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5111/Reviewer_vRYi"
        ],
        "content": {
            "summary": {
                "value": "The paper trains RNNs on two tasks that require memory with two curriculum learning types of approaches, one with sequential learning of a single-head RNN and one with multi-head RNN. The former learns a single task sequentially and the latter simultaneously learns multiple closely related tasks that build upon one another simultaneously. The paper shows that these two approaches exhibit different properties in single-neuron and network-mediated timescales."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Originality**\n- The paper proposes and explicitly evaluates experiments on single-neuron and network-mediated neuron timescales properties for two overarching tasks, with different value $N$ subtasks, and two curriculum learning type approaches.\n\n**Quality**\n- The paper does a good job for the most part on evaluating and showing results on means and standard deviations and some sort of error or measure of spread of each of these for various properties of interest (e.g., $\\tau_{\\text{net}}$) in Figure 5 as $N$ changes.\n- Most claims are supported by empirical results.\n- The paper identifies or is inspired by other sources to make certain principled decisions, such as identifying that autocorrelation bias is an issue, and simulating network activity for long periods of time to mitigate this issue. (Mentioned in Page 3)\n\n**Clarity**\n- The paper generally does a great job explaining the motivation, related work, preliminaries, some experimental design, and some analysis of results. For example, page 3 contains an excellent explanation of the limitation posed by having non-linear dynamics and how the proposed approach overcomes these limitations through an approximation method that the paper explains as well.\n- Figures are generally well made. Some take a little effort to decipher, but I was able to understand all of them. However, accessibility to mildly or significantly colorblind people could be improved by differentiating the single and multi-head N-DMS line plots in Figure 4 and any other similar cases.\n\n**Significance**\n- With concrete results on specific tasks, I hope this work inspires people to further explore this area, especially in ways that show that it generalizes as well as cases that deviate from expected behavior with analyses and findings as to potential causes.\n\n---\n\nNote: I've raised my ratings of Soundness from a 2 to a 3, Presentation from 3 to a 4, and of the overall paper from a 3 to a 5 due to the extensive improvements made and clarifications provided by the Authors during the Author Rebuttal phase."
            },
            "weaknesses": {
                "value": "For Figure S12, it would be helpful to readers to point out that the y-axis is on a log scale, so even though visually the linear fit doesn't look good towards larger $t$, the residual errors are much smaller here because this regime has much smaller values than those for earlier $t$.\n\nIn Figure 6, I can infer, but I don't believe it is explicitly stated why N=30 is used as the maximum value for linear regression line fitting. Furthermore, I don't understand why the paper uses N=30 for N-DMS if the reason to the previous question is that N=30 was about the limit for single-head networks on N-parity.\n\nFor Figure 8c, I would've liked to see a mean line and N extend to a higher number to see at what relative accuracy each converge and how these compare to one another other.\n\nThe Limitations section is lacking significantly, especially with work as general as individual and network neuronal timescales in DNNs. Future work is also lacking, both as a result of lacking limitations as well as no suggestions for future work outside of the limitations section. This section seems like an afterthought that was thrown in to satisfy having \"Limitations\" of the work mentioned.\n\nWith the large amount of related work that is close to the proposed work, it seems like the proposed work is primarily an empirical investigation into very simple tasks and limited approaches. Even equations are generally adapted from another source, and the paper merely needs to plug in its variable symbols to create the equations shown. I think there is novel concrete empirical insights here, but I'm not confident as to how they generalize based on the limited number of experiments provided. I have more confidence in how they generalize based on the related literature's work that forms the basis for which the paper proposes and evaluates the experiments it does. Though the paper is generally well written and presented, I find that it being primarily, or even about entirely, empirical that it is too limited in its experiment design quality, empirical results, and scope to recommend it for acceptance in its current state."
            },
            "questions": {
                "value": "1. Page 3 states \"In the limit case \u03c4 \u2192 \u221e, the neuron\u2019s activity is constant, and the input has no effect.\" Does this assume convergence as $\\tau$ approaches infinity? Are there cases in which this wouldn't hold?\n\n2. The paper uses AIC to select the best fitting model. \n  a. Why do you use this information criterion?\n  b. Why use this criterion instead of another or others collectively?\n  c. For support in using this criteria, consider referencing [1] that used AIC alongside other information criterion to select the best fitting model for time-series data and won the IJCAI'23 Best Paper Runner Up award.\n\n3. Figure 4 caption says \"whereas in the multi-head curriculum, having \u03c4s fixed at k value is as good as training them.\" Should this include a specific numerical value or range of values for $k$?\n\n4. On page 5, how is optimal strategy defined in the statement \"Furthermore, the multi-head curriculum emerges when training on a multi-objective task without introducing an explicit curriculum (Appendix D), supporting the optimal strategy of this curriculum.\"\n\n5. In Figure 5c, 5d, do you have insight why the mean and standard deviation for the multi-head networks go up and then back down? I.e., they peak at ~30 and then go down.\n\n6. On page 7, could you further explain the statement \"The strong negative weights in single-head networks are required to create stable dynamics in the presence of long single-neuron timescales.\"\n\n7. For the \"retraining\" (Perhaps a slight typo) experiment details on Page 8 and 9, why use 20 epochs?\n\n[1] Pasula, P. (2023). Real World Time Series Benchmark Datasets with Distribution Shifts: Global Crude Oil Price and Volatility. arXiv preprint arXiv:2308.10846."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I have no ethics concerns for this work."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5111/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5111/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5111/Reviewer_vRYi"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5111/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699615801277,
        "cdate": 1699615801277,
        "tmdate": 1700740283553,
        "mdate": 1700740283553,
        "license": "CC BY 4.0",
        "version": 2
    }
]