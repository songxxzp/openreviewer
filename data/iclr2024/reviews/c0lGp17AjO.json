[
    {
        "id": "aIl8KBLZuz",
        "forum": "c0lGp17AjO",
        "replyto": "c0lGp17AjO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission276/Reviewer_2Kjt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission276/Reviewer_2Kjt"
        ],
        "content": {
            "summary": {
                "value": "The paper derives Wasserstein distance between two probability distributions associated with two stochastic differential equations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper tries to learn a SDE by bounding the W2 distance."
            },
            "weaknesses": {
                "value": "There are no generalization bounds."
            },
            "questions": {
                "value": "Can we prove some finite sample generalization bounds?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission276/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698556732314,
        "cdate": 1698556732314,
        "tmdate": 1699635952960,
        "mdate": 1699635952960,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "saMpITj8By",
        "forum": "c0lGp17AjO",
        "replyto": "c0lGp17AjO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission276/Reviewer_ioMy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission276/Reviewer_ioMy"
        ],
        "content": {
            "summary": {
                "value": "The paper analyzes the Wasserstein 2 distance (WD) between solutions of two stochastic differential equations of form $d X(t) = f(X(t),t) d t + \\sigma(X(t),t) d B(t)$ as distributions over continuous function space. The main result shows that under certain mild assumptions, the WD is upper bounded by differences over $f$ and $\\sigma$, which establishes the relation between estimation of $f,\\sigma$ and the resulting reconstruction. Furthermore, when only a finite dimensional distribution is observed, the WD between the finite projections and the original WD are shown to be close to each other. An approximator of the finite projected WD is also proposed, using which various examples are implemented."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is overall well written and presented, and the the ideas are original to the knowledge of the reviewer. Some strengths:\n1. The bound in Theorem 1 seems interesting and novel, which establishes how the solution to SDE evolves as the parameters change. This provides evidence that WD is a valid loss function for SDE reconstruction. Moreover, an analysis of WD over Banach space (continuous function space) seems interesting.\n2. The proposed finitely projected WD loss seems simple to compute, and the examples illustrate that it is suitable for various reconstruction tasks."
            },
            "weaknesses": {
                "value": "1. Theorem 1 is an interesting bound, but it is unclear how this gives a full characterization of what happens when using WD as a loss for reconstruction. In fact, a reverse of the inequality would also be needed to show that when minimizing the WD, the functions $f,\\sigma$ are both well approximated, as it is usually the WD that we can observe, rather than gaps for $f,\\sigma$.\n2. A major concern is the tightness of equation (17). As a simple example, if $f,\\sigma$ are both only function of $t$ not $x$, then the finite projections will be jointly Gaussian, the WD between which is not likely to be just sum of WD between marginal. Thus it is hard to assess how good the approximation is, even if it seems to work well in the provided examples."
            },
            "questions": {
                "value": "Please see above (section Weaknesses) for details. The major question is how tight the approximation (17) is, as it was extensively used in the examples, and is claimed to be an approximation of the original WD. It is hard to assess how well the reconstruction is from the current contents of the paper, as it does not seem to properly 'interpolate' between discrete time steps if only local information is used, and continuity of the construction is provided by the continuity of the neural network estimators. Alternatively the paper can also provide theoretical guarantees specifically for this loss."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission276/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission276/Reviewer_ioMy",
                    "ICLR.cc/2024/Conference/Submission276/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission276/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813355943,
        "cdate": 1698813355943,
        "tmdate": 1700605620653,
        "mdate": 1700605620653,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7wDaBwIA4V",
        "forum": "c0lGp17AjO",
        "replyto": "c0lGp17AjO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission276/Reviewer_fb1D"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission276/Reviewer_fb1D"
        ],
        "content": {
            "summary": {
                "value": "This paper delved into the poblem of reconstructing SDEs (using Neural Networks) with Wasserstein-2 loss ($W_2$). The authors introduced a new bound on the $W_2^2$ loss of 2 probability measures constructed on 2 random process. In addition, the authors also provided an estimation of the $W_2^2$ that can be carried out numerically. Finally, the authors conducted empirical experiments testing the perfomance of reconstructing SDEs using Neural Networks with this $W_2^2$ loss."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is written clearly with detailed explanation.\n\n2. The authors provided an upper bound on the $W_2^2$ distance that can be approximated numerically.\n\n3. The empirical performance of the new loss out-performs traditional MSE and KL loss."
            },
            "weaknesses": {
                "value": "I am not very familiar with this topic. There seems to be no major flaw in this paper, but I think it would be better if there are some more detailed comparisons with prior works and other losses, as the authors only provided some intuitive comparison with MSE and KL divergence losses in Appendix B."
            },
            "questions": {
                "value": "1. What kind of noise can the $W_2^2$ loss deal with?\n\n2. Can the authors provide some intuition about why $W_2^2$ loss is more robust to noise?\n\n3. Is eq(10) purely of intellectual interest or thers are some methods to estimate this expectation? It seems Theorem 2 didn't use this upper bound to construct an estimation of the $W_2^2$ loss.\n\n4. Finally, I am curious what method did the authors use to minimize the $W_2^2$ loss?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission276/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819168066,
        "cdate": 1698819168066,
        "tmdate": 1699635952792,
        "mdate": 1699635952792,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "l8xxOet8Io",
        "forum": "c0lGp17AjO",
        "replyto": "c0lGp17AjO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission276/Reviewer_NdWD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission276/Reviewer_NdWD"
        ],
        "content": {
            "summary": {
                "value": "Optimal transport (OT) metrics, aka Wasserstein distances, are ubiquitous in comparing probability measures since they capture the underlying geometric of the objects at hand. This paper introduces the Wasserstein distance between two probability distributions associated with two solutions of stochastic differential equations (SDE). The author(s) investigate the squared Wasserstein distance to reconstruct the drift and the diffusion components of an SDE through a neural SDE."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Investigating the 2-Wasserstein distance between two probability distributions associated with two solutions.\n- Theoretical guarantees of the 2-Wasserstein distance.\n- Reconstruction of the drift and the diffusions component of SDE through a neural SDE where the 2-Wasserstein distance serves as the loss training.\n- Numerical experiments of univariate and bivariate SDE for CIR, OU, and 2D geometric Brownian models."
            },
            "weaknesses": {
                "value": "- I suggest that the result in Theorem 1 should be highlighted in the case of **univariate** SDE ($d=1$), since in Definition 1, the Wasserstein distance is defined for $d$-dimensional processes. \n- In Remark 1, the author(s) mentioned that the upper bound could be generalized for $d$-dimensional case under mild assumptions. I think it is important to write this result and get some consistent presentation in the manuscript since Theorem 2 is valid for multivariate case.\n- Again with Remark 1, do the drifts $(f_i)$ and the diffusions components $(\\sigma_i)$ satisfy the same assumptions in Theorem 1?\n- In Equations (16) and (17),  the vectors  $\\boldsymbol{X}(t_i)$ and $\\hat{\\boldsymbol{X}(}t_i)$ are $d$-dimensional then the squared  $(\\boldsymbol{X}(t_i) -\\hat{\\boldsymbol{X}(}t_i))^2$ must be a squared-$L_2$ norm.\n\n- For the rotated  $W_2$, I couldn't understand the intuition behind this application of rotations. I think there is an \"embedding\"-like of the origin components $X_1$ and $X_2$ of the SDE, hence it seems that we have changed the origin SDE, by changing its drift and diffusions components. So, the rotated $W_2$ is it an upper or a lower bound for the origin one?\n- Does the rotated squared $W_2$ verify the same upper and lower bounds as in Theroems 1 and 2?\n- In the definition of the rotated Wasserstein, does $m$ the number of rotations? If yes, so in Tables 6 and 7 $N_{rotate}$ should be replaced by $m$.\n- According to Figure 4, I would prefer the reconstruction of $f$ and $\\sigma$ using Eq. 16 and Eq. 17 than the rotated $W_2$. Indeed, according to their definitions on Page 17,  they are acting on the empirical observations of the process in the time partition $(t_i)$. However, the rotated $W_2$ is acting on an embedding of the origin process, which might induce some loss of data information.\n\n- Tables 6 and 7 could be more readable if they were represented as plots."
            },
            "questions": {
                "value": "### Minor comments\n- Page 2: \"regularized W distance\", add the reference (Cuturi 2013, NIPS'13).\n- Page 2: \"reconstructingmultidimensional\"  -->  reconstructing multidimensional.\n- Page 4: \"MSE-based\": the acronym MSE is not defined.\n- Page 6: \"torchsde package (Lie tal, 2020) in Python\" --> \"torchsde Python package (Lie tal, 2020).\n- Page 7: \"Cox-Ingersoll-Ross\" --> \"Cox-Ingersoll-Ross (CIR)\".\n- Page 8: \"Python Optimal Transport package\": add its reference Flamary et al. JMLR'21.\n- Page 8, Caption Figure 4: \"the vectors $f(X_1, X_2) \\hat f(X_1, X_2)$\" --> add a comma $f(X_1, X_2), \\hat f(X_1, X_2)$.\n- Page 12: Equation (25): there is not a norm, it is only an absolute value. \n- Page 14: \"where $||X||^2$: $X$ would be in bold.\n- Page 14: $| \\cdot |$ is the $l^2$ norm: this is a little bit confusing because this notation is used before for the absolute value.\n- Page 14: In Equation (34), $\\ell$ is not defined.\n- Page 16: \"two vectors $(x_1(t_i), X_2(t_i)$: missing closed parenthesis.\n- Page 21: Caption of Table 7: \"This indicates the correlation .... in the drift term\", I think the correlation in the drift is more difficult to distinguish than the correlation in the diffusion because the relative error in $\\sigma$ gets much smaller."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission276/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission276/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission276/Reviewer_NdWD"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission276/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698873972051,
        "cdate": 1698873972051,
        "tmdate": 1700567255271,
        "mdate": 1700567255271,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hBS4edaSzK",
        "forum": "c0lGp17AjO",
        "replyto": "c0lGp17AjO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission276/Reviewer_pb7V"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission276/Reviewer_pb7V"
        ],
        "content": {
            "summary": {
                "value": "Given SDE of form (2), expressions for optimal transportation between the two such process is provided. An upper bound on the objective is shown in theorem1, which guarantees that when both deterministic and stochastic functions in the SDE are close, then the objective is close to zero. Then a time-discretised version (16) is presented, which by theorem2 convergence to the true objective as time-step becomes infinitesimally small.\n\nThe optimal objective is then employed as a loss to learn/estimate an SDE. Simulations on simple SDEs are presented."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. SDE estimation using optimal transport seems interesting. I feel this is not so well-understood from a learning perspective and perhaps deserves more attention."
            },
            "weaknesses": {
                "value": "1. The presentation can be improved to make it easily readable. Currently, I found it hard to understand what is happening. Brevity and notation compound the difficulty.\n\n2. Since the bound (7) is not elegant, perhaps it could be simplified appropriately to preserve the point about convergence. \n\n3. Some training set details for simulations seem missing like what was the step-size, time-horizon, number of samples etc ?\n\n4. There seem to be prior works which perhaps present a more comprehensive analysis of SDE wrt. optimal transport e.g., [1*], [2*], [3*], [4*]. It would be insightful if these are discussed in detailed in related work and perhaps appropriately compared with. I feel this is a major weakness of the work.\n\n[1*] https://arxiv.org/pdf/1902.08567.pdf\n[2*] https://arxiv.org/pdf/1603.05484.pdf\n[3*] https://arxiv.org/abs/1209.0576\n[4*] https://www.jmlr.org/papers/volume22/21-0453/21-0453.pdf"
            },
            "questions": {
                "value": "1. What do the measures \\mu,\\hat{\\mu} represent? It seems they are measures corresponding to X(t) and \\hat{X}(t) . If so, should not there be a \\mu(t), \\hat(\\mu}(t) ? In which case, they cannot be used in the LHS of 5 as the RHS is integrated over time. I think the RHS is a generalization of optimal transport to SDEs, but the optimal objective need not be a wasserstein distance (LHS). Am I correct ?\n\n2. why are different sets of baselines used in the various simulation examples ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission276/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission276/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission276/Reviewer_pb7V"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission276/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699032533519,
        "cdate": 1699032533519,
        "tmdate": 1700632314026,
        "mdate": 1700632314026,
        "license": "CC BY 4.0",
        "version": 2
    }
]