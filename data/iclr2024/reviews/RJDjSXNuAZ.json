[
    {
        "id": "0dKIxthvr0",
        "forum": "RJDjSXNuAZ",
        "replyto": "RJDjSXNuAZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5642/Reviewer_KB1H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5642/Reviewer_KB1H"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenge of expensive and time-consuming annotation requirements for training state-of-the-art object detection models. The authors propose a domain-specific weakly supervised object detection algorithm that leverages image-level annotations instead of annotated bounding boxes. By distilling the knowledge of a pre-trained model focused on virus presence/absence prediction, the proposed approach generates a set of pseudo-labels that can be used to train an object detection model effectively. The method utilizes an optimization approach with a shrinking receptive field, enabling the extraction of virus particles directly without relying on specific network architectures."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Addressing Expensive Annotation Requirement: The paper tackles the challenge of acquiring costly bounding box annotations by proposing a weakly supervised approach that relies on image-level annotations. This significantly reduces the manual labor and time required for annotation, making the training process more efficient.\n- Extensive Comparative Studies: The authors conduct comprehensive studies to evaluate the effectiveness of the proposed pseudo-labels. The results demonstrate that the generated pseudo-labels outperform other weak labeling methods and even ground truth annotations in scenarios where annotation time is limited. This indicates the superiority and practical value of the proposed approach."
            },
            "weaknesses": {
                "value": "- Limited Model Exploration. The paper primarily focuses on using Faster-RCNN with a ResNet-101 backbone as the detection model. It would be beneficial for the authors to consider exploring other models, such as DETR, to evaluate their effectiveness in the proposed approach.\n- Lack of Discussion on Low Signal to Noise Ratio (SNR) in EM Images: While the authors mention that low SNR in EM images can impact the performance of methods designed for other imaging modalities, there is a lack of in-depth discussion, algorithm design, and experiments addressing how to mitigate the low SNR problem and how it specifically affects the capacity of weakly supervised object detection (WSOD) methods in the EM scenario. Further exploration and discussion of this property would be valuable to enhance the understanding and applicability of the proposed approach."
            },
            "questions": {
                "value": "In general, I find this work to be commendable. However, there are certain limitations that should be addressed for further improvement. Specifically, it is crucial to include testing with DETR to evaluate the effectiveness of transformer-based architectures. Additionally, at least providing a thorough discussion on the low SNR problem would significantly enhance the quality of the paper. If these questions are well solved, I would like happy to raise my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5642/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698557709615,
        "cdate": 1698557709615,
        "tmdate": 1699636586583,
        "mdate": 1699636586583,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hZspqTp3hb",
        "forum": "RJDjSXNuAZ",
        "replyto": "RJDjSXNuAZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5642/Reviewer_zGyY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5642/Reviewer_zGyY"
        ],
        "content": {
            "summary": {
                "value": "The manuscript presents a class activation map (CAM)-based weakly supervised learning method for virus particle detection in electron microscopy images. Specifically, it first uses a pre-trained classifier to obtain an initial position of a virus using GradCAM (Selvaraju et al. 2017), and then iteratively refines the position with a Gaussian mask with a dynamic standard deviation. It repeats this process for each virus until all the viruses are detected in the input image. The proposed method is evaluated on 5 electron microscopy image datasets, and the experimental results are promising."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper introduces a simple yet promising method for weakly supervised object detection. Meanwhile, it conducts extensive ablation studies to show that the proposed weakly supervised method can outperform other more fine-grained annotation-based approaches (e.g., bounding box and point annotations), given a certain time budget.\n\n2. The paper designs a specific user study to demonstrate the effectiveness and efficiency of the proposed method."
            },
            "weaknesses": {
                "value": "1. In the experiments, the proposed method is not compared with other state-of-the-art weakly supervised learning methods, such as Zeng et al. 2019, Wei et al. 2022, and Lu et al. 2020. In addition, it is not compared with other CAM-based weakly supervised object detection methods in the experiments, such as Xu et al. 2022. Without a comparison with recent state of the art, it is difficult to determine the superiority of the proposed methods over other approaches.\n\n2. The method requires the object size to be known in advance. This needs additional effort to estimate the size of target objects before applying the method. It would be helpful to provide an in-depth discussion on this design (probably also including the effects of using different estimated object sizes).  \n\n3. It seems that the proposed method needs to repeat the optimization process (i.e., solving Equation (2)) for each virus. The time cost may be high if there is a large number of viruses in the input image.\n\n4. The method is evaluated on only virus detection in electron microscopy images, where viruses do not overlap. Thus, the method may not generalize to object detection (e.g., cell or nuclei detection) in other microscopy imaging modalities, such as hematoxylin and eosin (H&E) or immunohistochemistry (IHC) stained brightfield microscopy images, and fluorescence mages, which often have touching or overlapping cells or nuclei. In addition, the repeated optimization for each object would be expensive for H&E or IHC images that typically have thousands of or even more cells/nuclei."
            },
            "questions": {
                "value": "1. The proposed method is based on the GradCAM method. What if the GradCAM does not provide good initializations or even wrong saliency maps? What are the effects of inaccurate saliency map creation on the quality of the pseudo-labels generated by the proposed method? \n\n2. During the optimization process of the proposed method, i.e., solving Equation (2), is the classifier C fixed and not updated? If so, is the optimization of Equation (2) simply to find the position that has the highest value in the prediction map C(I * M(p_t)) at each time step? But if not, what algorithm is used to optimize Equation (2)? \n\n3. During the postprocessing, the method uses non-maximum suppression to eliminate virus particles that have low detection scores. Are the detection scores of virus particles obtained from the initial CAM map or the prediction map from the Gaussian-filtered input, C(I * M(p_t))?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5642/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5642/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5642/Reviewer_zGyY"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5642/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698613919139,
        "cdate": 1698613919139,
        "tmdate": 1699636586484,
        "mdate": 1699636586484,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jI2Mu3gUrf",
        "forum": "RJDjSXNuAZ",
        "replyto": "RJDjSXNuAZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5642/Reviewer_6zGs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5642/Reviewer_6zGs"
        ],
        "content": {
            "summary": {
                "value": "This work presents a method for weakly-supervised object detection (WSOL) of virus capsids in EM images, which can be used for rapid curation of bounding boxes. The overview of this method (presented in Figure 1) uses an iterative process in which: 1) Grad-CAM from a pretrained encoder is use to output saliency maps of the highest-scoring virus location, 2)  gradient descent is used to optimize the location of the virus, 3) virus is masked out using known information about the virus size. This process repeats until all viruses are removed (referenced as Ours (Opt)), with the bounding boxes created using this process usable for developing weakly-supervised object detectors (Ours (OD)). Comparisons against human annotators (weakly-supervised binary annotation, location, bounding box) and self-supervised detectors were performed, with comparison against human annotators (with and without time constraints) also performed."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Overall, this work presents a very unique methodology and study design for curating bounding boxes in EM images. A contribution not emphasized in this work is the simplicity of the method, using a very intuitive heuristic that outperforms current unsupervised, deep learning-based detectors such as SAM and CUTLER. Though specific to EM, I believe the uniqueness and simplicity of this work would still be of interest to the computer vision community. The related work section is all comprehensive, and the authors of this work reference related works in weakly-supervised and self-supervised object detection very well."
            },
            "weaknesses": {
                "value": "- Though the related work section provides a comprehensive overview of current progress in WSOL methods, was there a reason why this work does not compare against other WSOL methods such as Xu et al. [1] (CREAM), Wei et al. [2] (ISIC), and other more recent works such as LOCATE [3] and GenPromp [4]? Though specific to EM, many other works in the WSOL domain can also be readily adapted.\n- In addition to lack of comparisons, one of the main limitations of this work that may prevent broader interest in the ICLR community is that the proposed method is too specific to EM and is not evaluated on diverse tasks. Though EM is unique compared to natural images which are generally more object-centric, other modalities such as histopathology and multiplexed imaging share similar characteristics (as noted in [5]), with the image scale is objective with units per pixel being fixed. Specifically, the contributions of this work would be strengthened if shown that a simpler heuristic can also be created for other imaging domains.\n- Following other works which have found pretrained Vision Transformers (ViTs) to be strong in WSOL [4,5,6], was there as a reason why a ResNet-101 was used for classification instead of a ViT? Moreover, was the DINO-ViT used in CUTLER trained using EM images, or was it using a pretrained checkpoint from ImageNet? As ViTs have been also found to have natural fit for microscopy images [5], it would be interesting to explore how the a DINO-ViT for EM images would: 1) improve the WSOL results of this work, and 2) improve the CUTLER baseline reported in this work.\n- Though this work is well-written, it was difficult to understand the training dataset and the downstream dataset for evaluation and annotator labeling. Though described in text, including a table with the distribution of labels for train and test may be simpler to communicate.\n\nOverall, I found this work to have a unique contribution computer vision in proposing simpler techniques that outperform more complicated solutions that are more complex to train and underperform in domain-specific areas. At the same time, I feel that the scope of the work is too narrow for ICLR, and lacks comparisons to other WSOL works. I still feel that this work has many strengths, and believe that it would make a timely contribution in conferences specific to computer vision such as CVPR and ECCV where approaches for solving domain-specific challenges would be more broadly appreciated.\n\n1. Xu, J., Hou, J., Zhang, Y., Feng, R., Zhao, R.W., Zhang, T., Lu, X. and Gao, S., 2022. Cream: Weakly supervised object localization via class re-activation mapping. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 9437-9446).\n2. Wei, J., Wang, S., Zhou, S.K., Cui, S. and Li, Z., 2022, October. Weakly supervised object localization through inter-class feature similarity and intra-class appearance consistency. In European Conference on Computer Vision (pp. 195-210). Cham: Springer Nature Switzerland.\n3. Li, G., Jampani, V., Sun, D. and Sevilla-Lara, L., 2023. LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10922-10931).\n4. Zhao, Y., Ye, Q., Wu, W., Shen, C. and Wan, F., 2023. Generative prompt model for weakly supervised object localization. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 6351-6361).\n5. Chen, R.J., Chen, C., Li, Y., Chen, T.Y., Trister, A.D., Krishnan, R.G. and Mahmood, F., 2022. Scaling vision transformers to gigapixel images via hierarchical self-supervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 16144-16155).\n6. Murtaza, S., Belharbi, S., Pedersoli, M., Sarraf, A. and Granger, E., 2023. Discriminative sampling of proposals in self-supervised transformers for weakly supervised object localization. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 155-165)."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5642/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822789511,
        "cdate": 1698822789511,
        "tmdate": 1699636586390,
        "mdate": 1699636586390,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pcnJGVhMZM",
        "forum": "RJDjSXNuAZ",
        "replyto": "RJDjSXNuAZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5642/Reviewer_xKLD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5642/Reviewer_xKLD"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a domain-specific weakly supervised object detection method that relies on image-level annotations instead of bounding boxes. They use a pre-trained model to generate pseudo-labels for training, showing that these labels outperform other weak labeling methods and even ground truth labels in time-constrained scenarios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- the paper is well-written and easily to follow. \n- it proposed an relevantly simple but effective method for an impactful task. In their experimenst, aurthors sucessfully demonstrated the supriority over the consider baselines, includig supervised method as well as zero-shot learning with large scale pretrained models. \n- the authors utilized the spatial information and explored an novel way to refine the localization neural networks provide."
            },
            "weaknesses": {
                "value": "The proposed method has potential to work for not only electron microscope images but other medical images. It will be interesting and also brings broader impact if authors can provide discussions around this."
            },
            "questions": {
                "value": "The current setup with Gaussian as a prior assumes that the object to detect is in a round shape. How easily it can be extended to different objects and how accurately it will work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5642/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699284788420,
        "cdate": 1699284788420,
        "tmdate": 1699636586310,
        "mdate": 1699636586310,
        "license": "CC BY 4.0",
        "version": 2
    }
]