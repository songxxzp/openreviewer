[
    {
        "id": "sSP3h6hENz",
        "forum": "zb3b6oKO77",
        "replyto": "zb3b6oKO77",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7872/Reviewer_aQw7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7872/Reviewer_aQw7"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates a binding problem in which language models must represent the associations among entities and attributes. The authors empirically check the behavior of binding considering factorizability and position independence, and they introduce the idea of a binding ID vector, considering the additivity of binding functions. The paper shows several pieces of evidence that imply the existence of the binding ID vectors and their generalities."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper proposes novel binding ID vectors that can help understand LLM's behavior. \n- The paper presents empirical evidence of binding by checking substitution and position independence.\n- Analyses are provided for biding ID vectors, including geometry and generalization."
            },
            "weaknesses": {
                "value": "- The paper addresses a simple binding problem between a pair of entities. It is unclear how this can be generalized to the binding behavior on more than binary relations and entities involved in multiple binding. \n- Some details are not clear, as in the following questions."
            },
            "questions": {
                "value": "- When the authors substitute activations in Figure 3 or calculate median-calibrated accuracy in Figure 5 and Table 2, which part of the model is substituted and which is frozen? LLMs are autoregressive models, and during decoding, when and how are the parameters substituted? \n- Among what did the authors calculate the mean of log probs in Figure 4? \n- In equation (2), they do sampling, but from what did the authors sample? \n- The lines in the legend do not match those in the graph in Figure 4. In the bottom graph, the mean log probs increase or decrease with the positions, but the authors say they are small changes. How can they be said small?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No problem."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7872/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7872/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7872/Reviewer_aQw7"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7872/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698301465915,
        "cdate": 1698301465915,
        "tmdate": 1699636965689,
        "mdate": 1699636965689,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5dX9DAtlPa",
        "forum": "zb3b6oKO77",
        "replyto": "zb3b6oKO77",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7872/Reviewer_H1Kh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7872/Reviewer_H1Kh"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a hypothetical mechanism to explain how LLMs associate entities to specific properties in the in-context learning setting, which the authors call the _binding problem_. The phenomenon is mostly studied in the context of a very short and simple reasoning task, and the key investigative tool is _causal mediation analysis_, where carefully chosen sets of internal activations are substituted into to the network before generating responses to queries. The experimental results are consistent with a hypothesized \"binding ID\". Further experiments test other properties and behaviors of this mechanism."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Figure 2 was very helpful in illustrating the substitution scheme. \n\nI am not completely convinced that the claims follow from the observations due to some clarity (or confusion on my part) issues (see below), but assuming they hold: the _binding ID_ mechanism and its properties represent a really exciting discovery in terms of understanding LLM ICL phenomena. I suppose maybe this sort of lines up with the recent (Bricken et al 2023)[https://transformer-circuits.pub/2023/monosemantic-features/index.html] and associated previous superposition research. In any case, this mechanism would unlock a lot of promising research directions as well as practical applications.\n\nSection 3.1: the hypothesized mechanism and its consequences are, for the most part, laid out clearly and precisely. (nit: \"the only information ... that affects the query behavior\" _for our specific queries_)\n\nTo the extent that I correctly understood the experiments, the ability to, rather surgically, manipulate the \"beliefs\" of the model was fairly clear evidence consistent with the hypothesized mechanism. The setting where switching both entity and attributing binding IDs restores correct query answering was a \"wow\" moment for me."
            },
            "weaknesses": {
                "value": "Section 2.2: it was not immediately obvious to me whether the stacked activations completely \"d-separates\" (maybe not exactly this concept?) the intervention token from everything else, without some more detail on the LM architecture.\n\nSection 4.1 is very dense, and I found it difficult to follow without working it out myself on separate paper. Given the importance of these concepts to the rest of the paper, a diagram might help make it clearer. See questions below, but I had a central confusion about some core mechanism here even after working through this pretty closely. Without resolving the clarity a bit, it's difficult to fully accept the interpretations of the experiments, as significant as the findings would be.\n\nSection 5 supports the generality of the mechanism across other similar tasks, which further strengthens the central contribution. The existence of the alternative _direct binding_ mechanism was interesting to see and another good contribution of the paper, although it was not super clear without reading the corresponding Appendix."
            },
            "questions": {
                "value": "\"If there no context consistent...say that the LM is _confused_\" - I didn't fully understand this why or how this would happen, and also it didn't seem to come up elsewhere in the paper. Is it necessary to include?\n\n\"A subtle point is that ...\" this seems centrally important but only mentioned in passing.\n\nFigure 3 b seems like the wrong plot? It doesn't show what the text claims but rather seems to be a copy of Figure 3 a. \n\nSection 4.1: what are we sampling here: from different \"X lives in Y\"-style context sentences? I don't totally understand why / how we'd expect the $b(k)$ vectors to be consistent for a given value of $k$ across different contexts, or is that not required? *Actually, this may be my central confusion with Section 4 and beyond*: \n* in Figure 1, abstract shapes (triangle, square) are used to represent the abstract binding IDs\n* in Section 4.1, binding ID $k$ is represented by some pair of functions $\\[b_E(k),b_A(k)\\]$\n* the $\\Delta$ calculation scheme in Eqn 2 and the expectation sampling seem to crucially depend on, eg $b_E(k)$ being a stable/consistent function of (the seemingly arbitrary index?) $k$ across _any context_ $\\bf{c}$ \n\nSo, is the hypothesized mechanism that:\n1. LLM, internally, have some weird analogue of LISP `gensym` that generates arbitrary but unique abstract binding IDs like \"square\" and \"triangle\" (but of course are really lie in a linear subspace of vectors) when an entity/attribute is seen in-context?\n2. OR that LLM have some stable/consistent binding function $b_E(k)$, and the first time `llm-gensym` gets called, the result for $k=0$ is returned, and some internal entity counter (?) executes $k++$ ? \n3. OR some other process else altogether that I am missing?\n\nFigure 1 led me to believe case 1, but then Section 4.1 (as far as I can tell) requires case 2, and in fact the experimental results seem consistent with that mechanism. This is even more surprising to me, and I would argue needs to be laid out a bit more explicitly, if this is indeed the correct interpretation. Or possibly, it is case 1 and I am mistaken that the rest of the work depends on the \"stable $b(k)$\" described in case 2?\n\nEqn 2: is there any significance to the notation change of introducing $\\alpha$ ? \n\n\"These random vectors have no effect\" - I would have expected it to break things. so I guess this is even further evidence that the binding vectors are treated in a \"special\" way by the LLM machinery? If so, maybe this reasoning and conclusion could be emphasized.\n\nPerhaps due to my previous confusion about how binding ID functions work with $k$, but I could not understand what Figure 5 was conveying at all.\n\nFor any of this to make sense, I guess you are using a word-level tokenizer and not something like BPE/etc? Except it seems like the direct binding experiment uses sub-word tokens? Again, for clarity it might help to lay this out explicitly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7872/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7872/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7872/Reviewer_H1Kh"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7872/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698676057625,
        "cdate": 1698676057625,
        "tmdate": 1700270569892,
        "mdate": 1700270569892,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uCquymaUJd",
        "forum": "zb3b6oKO77",
        "replyto": "zb3b6oKO77",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7872/Reviewer_n1nh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7872/Reviewer_n1nh"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes how a language model (LM) correctly associates entities and their attributes described in context (i.e. as a prompt). The authors introduce the idea of binding ID vectors and show that an LM learns a pair of binding ID vectors for each entity-attribute pair so that the LM uses the vectors to correctly identify the right attribute when answering a query about an entity. They also present experimental results suggesting that the binding vectors reside in a linear subspace and are transferable across different tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper explores an interesting topic on representation learning that should contribute to a deeper understanding of LLMs.\n- The paper presents a novel idea to explain the phenomenon.and experimental results that support the idea."
            },
            "weaknesses": {
                "value": "- The paper is sometimes difficult to follow. This may be because the main body of the paper contains too many concepts and fails to provide important explanations and examples that would help the reader understand the concepts.\n- It is not entirely clear whether the authors\u2019 conclusions are supported by experimental evidence."
            },
            "questions": {
                "value": "- Section 2.1: What exactly do you mean by \u201cthe k-th entity\u201d and \u201cthe k-th attribute\u201d? Is the entity that appears first in the sentence called the 0-th entity?\n- Section 2.1: What did you sample N=100 contexts from?\n- Section 3.2: What is the experimental setting in the experiment?\n- Section 4.2: Does the experimental result really mean that the vectors that are not linear combinations of binding ID vectors do not work?\n- Figure 5: What are the x and y axes? Are they the coefficients of the basis vectors?\n- p. 8: What exactly is a cyclic shift?\n- p. 2: Does a \u201csample\u201d mean an example?  In statistics, a sample usually means a collection of examples (data points).\n- p. 1: alternate -> alternative?\n- p. 2: of of -> of\n- p. 8 In See -> See"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7872/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698872767233,
        "cdate": 1698872767233,
        "tmdate": 1699636965453,
        "mdate": 1699636965453,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mTpQglstbn",
        "forum": "zb3b6oKO77",
        "replyto": "zb3b6oKO77",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7872/Reviewer_f6eM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7872/Reviewer_f6eM"
        ],
        "content": {
            "summary": {
                "value": "This paper studied how language models handle a linkage between entity and attribute tokens. The authors argued that language models used a component of the residual stream of each token as a binding vector to mark entities and their corresponding attributes. The authors also claimed that the binding vectors are simply added to the residual stream.  \n\nTo prove this empirically, the authors presented a series of causal mediation experiment results on binding tasks, such as determining a city in which a person lives or the color of a shape based on a context. The context typically consisted of two to three entity-attribute pairs. First, the authors showed that binding vectors existed by swapping activations of an entity or an attribute. The results showed that binding changed (by means of changing the mean log probability of attributes.) On the other hand, shifting did not affect the mean log prob of the correct attributes -- suggesting that binding vectors are positional independent. The author then estimated the binding differences and showed that the authors could perform addition or subtraction to manipulate the bindings. In addition, the difference vectors are generalized across different binding tasks. But, the authors suggested one task had an alternative binding mechanism."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper presented a novel concept of binding mechanism in language models. \n2. The paper provided experiment results based on many datasets, albeit toy data. Figure 3, Tables 1 and 2 supported the main claims of the binding vectors and their additivity property. \n3. The paper was well-written. I found that the definitions and hypotheses were well articulated and precise. It also provided sufficient background to understand the paper."
            },
            "weaknesses": {
                "value": "**Significance of the Results**\n\n1. While the ideas presented in this work were novel, it was unclear how generalized they are. The authors presented a series of experiments based on somewhat synthetic datasets. Had the task been reading comprehension, we might not have observed the same mechanism. I think adding more tasks did not provide meaningful results unless they required different reasoning complexities. In addition, the experiments presented in Section 3 only provided anecdotal evidence from a few samples (Factorization and Position Independence claims). \n2. Although the paper presented the results using both Pythia and LLaMa as subject LMs, the results in Figure 6 showed opposite trends between the two models. I saw that the accuracies were further apart in Pythia as the sizes went up but opposite in the LLaMa cases. I think the authors should explain this. Did the binding mechanism localize to a family of models?\n3. It was unclear whether there was the proposed binding ID when reasoning involving spans of tokens. \n\n**Presentation**\n\n1. Figure 3 shows the same image for (a) and (b). I was not sure why, or was it a mistake?\n2. I found the notion of the *binding function* rather confusing in Eq (1). I had many questions about it. If the binding function specifies how an entity binds to an attribute, why is it a sum of entity representation and the binding vector? What is the relationship between the binding function and the residual stream?"
            },
            "questions": {
                "value": "1. How many samples did you use to generate Figure 3? (100 or 500)\n1. Were the samples you used to estimate the *differences* the same samples for testing in Table 1? If so, why?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7872/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698928873914,
        "cdate": 1698928873914,
        "tmdate": 1699636965350,
        "mdate": 1699636965350,
        "license": "CC BY 4.0",
        "version": 2
    }
]