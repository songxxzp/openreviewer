[
    {
        "id": "BZZLGQE142",
        "forum": "ikqcUzUogm",
        "replyto": "ikqcUzUogm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission382/Reviewer_1QQx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission382/Reviewer_1QQx"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the practical problem of avoiding \u201charmful\u201d behaviors of LLMs.\n\nTo meet the \u201cThree Laws of Robotics\u201d in usability, safety, and ethics, the authors introduce the Benchmark for Identifying Noncompliant Decisions (BIND), a framework for evaluating rule-following behavior in LLM assistants. \n\nThe proposed benchmark contains 15 text scenarios drawing from the field of computer security and common children\u2019s games. Each scenario defines a set of rules in natural language and an evaluation program to check model outputs for compliance with the rules. The authors also systematically collect a challenging hand-written test suite of 862 test cases across all 15 scenarios, against which they evaluate current state-of-the-art models and find lackluster performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper has successfully introduced the Benchmark for Identifying Non-compliant Decisions (BIND), a framework to programmatically evaluate rule-following in LLMs. The benchmark consists of 15 text scenarios in which the model is instructed to obey a set of rules while interacting with the human user to avoid \u201charmful\u201d behaviors from LLMs."
            },
            "weaknesses": {
                "value": "The comparison to the relevant and closed baselines is not conducted. Without this comparison, it is hard to justify the advancements of the proposed framework.\n\nThe threats to the validity of the proposed benchmark are not investigated."
            },
            "questions": {
                "value": "Can the designed benchmark affect and guide the LLMs\u2019 behavior? Please explain in detail with some concrete examples.\n\nIs the effectiveness of the benchmark only affected and available to the current testing versions of the used LLMs? When the new versions of LLMs are released, will the proposed framework still be valid? Can the authors explain in detail with some examples?\n\nHow is the proposed framework performance compared to the baselines regarding the number of rules and the effectiveness in guiding the LLMs to avoid \u201charmful\u201d behaviors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission382/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission382/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission382/Reviewer_1QQx"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission382/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698752592526,
        "cdate": 1698752592526,
        "tmdate": 1699635965288,
        "mdate": 1699635965288,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nvcPdvOnvU",
        "forum": "ikqcUzUogm",
        "replyto": "ikqcUzUogm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission382/Reviewer_do43"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission382/Reviewer_do43"
        ],
        "content": {
            "summary": {
                "value": "The paper asks the question, _does expressing simple rules in natural language as prompts and/or system instructions ensure the model is able to follow these rules?_ To conduct effective and automatic evaluation, the rules choses can be evaluated by a simple computer program. The test scenarios are based on some pre-defined dimensions-- (1) Environments grounded in software security and games, (2) rules that need to be adhered to (Positive) and rules that should not be broken (Negative), and (3) Strategies (context setup in natural language) that can be used to push the model to break the rules. The experiments show, with multiple seeds and statistical testing, that both closed and open-source models fail to respect simple system rules that can easily be validated using simple computer programs."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n2. The objectives are clearly stated and the evaluation is broken down into reasonable dimensions.\n3. The test-set will be a good benchmark for evaluating LLM's prowess at following simple rules in the future.\n4. The experiments consider prompt variances, efficacy of system vs user prompts for SOTA models, and authors seem to have conducted statistical testing to support/disapprove their claims.\n5. The authors report API results with timestamps and also consider automatic/optimized adversarial attacks on open-source models."
            },
            "weaknesses": {
                "value": "1. I would like to believe the finding in this paper should be reasonable obvious to most people at the conference, i.e. expecting a stochastic autoregressive model to follow deterministic objectives (that programs do) seems unreasonable to start with, although I have seen a suspension of disbelief from experts, alas! Truth be told, it seems like the season for papers along similar veins (LLMs can't plan, reason, solve NP hard problems, figure out game-theoretic equilibria); duh! Tbh, beyond the test set that will help others check to improve LLM capabilities at following simple rule (not sure why they need it though if we can write programs that can be called during orchestration), I am not fully sure of the contribution).\n2. The choice of testing scenarios (esp. the security ones and the game) seems a little arbitrary, lacking good motivation.\n3. The authors seamlessly refer to figures/tables in Appendix. While I did look at them for context and understanding, I feel this skews my evaluation towards other papers who have had to strictly adhere to the page limit to make their point."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Although it is obvious to me (and hopefully people of this community), unsure if the jailbreaks/strategies proposed will help naive users break closed-source models into revealing other secrets (eg Personal Identifiable Information or PIIs) from their training data. While I am aware of vulnerability disclosure strategies in software security, unsure if such a paradigm exists for LLMs (or major players have reporting obligations). Wanted to see if the authors did any testing to ensure this is a no-threat or already communicated with the model providers. Tbh, unsure if the authors should be penalized for a single paper while other related work they cite is openly releasing attacks."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission382/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818888767,
        "cdate": 1698818888767,
        "tmdate": 1699635965207,
        "mdate": 1699635965207,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aDrv39IJKb",
        "forum": "ikqcUzUogm",
        "replyto": "ikqcUzUogm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission382/Reviewer_UwWZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission382/Reviewer_UwWZ"
        ],
        "content": {
            "summary": {
                "value": "Motivated that the model\u2019s adherence to even simple rules needs human engagement, the authors propose a benchmark dataset where LLM rule-following can be programmatically evaluated. The proposed dataset consists of text scenarios that have an evaluation program to determine the model\u2019s adherence to given rules. The text scenarios are influenced by computer security systems and children\u2019s games. With the design of the dataset, the authors also use test suites of 862 hand-written test case templates to implement different high-level attacks for diverse analyses on rule-following behavior."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Suggest a benchmark dataset in which LLM\u2019s rule-following behavior can be automatically evaluated.\n- The evaluation of each rule-following is robust and cheap."
            },
            "weaknesses": {
                "value": "- As the author\u2019s motivation is to evaluate the rule-following behavior of LLMs automatically, the direct tackling to this motivation would be the automatic evaluation of rule-following behavior in arbitrary (at least diverse) domains and rules. However, the test scenarios are fixed in two domains, and the testing rules are limited to predefined contents for each domain. Fixed domain and rules can be evaluated by human, so harm the contribution of this work."
            },
            "questions": {
                "value": "- Can the suggested benchmark dataset be extended to other domains and rules with relatively little effort?\n- As the different test suites change the LLM's performance, why defense prompt doesn\u2019t work? Is there an explainable reason?\n- Does rule-following behaviour in computer security system and children's game have general impact for assessing LLM performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission382/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission382/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission382/Reviewer_UwWZ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission382/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836917531,
        "cdate": 1698836917531,
        "tmdate": 1699635965134,
        "mdate": 1699635965134,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pNsr561JdL",
        "forum": "ikqcUzUogm",
        "replyto": "ikqcUzUogm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission382/Reviewer_NyVg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission382/Reviewer_NyVg"
        ],
        "content": {
            "summary": {
                "value": "The authors describe a framework/benchmark named BIND that evaluates the ability of LLMs to follow rules under various scenarios (benign and adversarial).  They evaluate various LLMs using this benchmark and conclude that most LLMs in the status quo are not compliant with rules that are specified."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Benchmark is first of its kind."
            },
            "weaknesses": {
                "value": "1. Unclear what takeaways can be drawn from this work.\n2. Paper could benefit from some reorganization."
            },
            "questions": {
                "value": "Overall, this work is interesting and potentially exciting but the main takeaways are not communicated in a clear manner. This reviewer is wondering what I can learn from this paper, and how others can follow-up on this line of work.\n\n    1. The writing of the paper could benefit from some thought. For example, the authors could give examples of scenarios, rules and test cases to better highlight the difference between the 3 categories. \n    2. The paper provides limited takeaways from their experiment. It is incredible that the authors have come up with such a benchmark. But what can I learn because of it apart from the fact that LLMs do not follow rules (which was already a well know fact. Look at work from Percy Liang\u2019s group \u2014 https://arxiv.org/abs/2307.03172, or the fact that LLMs used in search e.g., BingChat can easily be subverted with prompt injection attacks)? The fact that there\u2019s nothing beyond the creation of this benchmark is making this reviewer apprehensive in recommending acceptance.\n    3. One conceivable application is one where the models are deployed in real-world systems (e.g., as in BingChat) and one would want to understand how brittle these are. But to validate such a scenario, the authors need to consider \u201clayered defenses\u201d i.e., add an output filter atop the generations from the LLMs and see how much information can be exfiltrated in such a setting. However, this was not done in this work.\n4. A lot of the findings presented by the authors have been covered earlier i.e., numerous prompt injection strategies discuss mechanisms of rule subversion. This work, to me, seems like a consolidation of those findings. Could the authors emphasize the difference?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission382/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699298135446,
        "cdate": 1699298135446,
        "tmdate": 1699635965063,
        "mdate": 1699635965063,
        "license": "CC BY 4.0",
        "version": 2
    }
]