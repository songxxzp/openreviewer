[
    {
        "id": "quwculi9cq",
        "forum": "nN1bEm8cna",
        "replyto": "nN1bEm8cna",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7426/Reviewer_Azjn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7426/Reviewer_Azjn"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the approximation capability of a certain type of spiking neural network (the noise-free version of the spike response model with a linear response function) from the perspective of continuous piecewise linear (CPWL) functions. The following conclusions are proved.\n\n1. A spiking neural network (SNN) is a CPWL function under certain encoding schemes and assumptions about synaptic weights (Theorem 1).\n\n2. Both ReLU artificial neural networks (ANN) and SNN can represent certain functions more efficiently (Theorem 2 and Example 1).\n\n3. Any ReLU ANN is equivalent to an SNN, and the size of the SNN is similar to that of ANN (Theorem 3).\n\n4. The maximum number of linear regions that a one-layer SNN generates scales exponentially with the input dimension (constant for one-layer ReLU ANN) (Theorem 4)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. To the best of my knowledge, this is the first theoretical paper that studies the number of linear regions for SNNs. This is an important aspect of understanding the expressive capacity of neural networks and has been widely investigated for traditional ANNs.\n\n2. The basic definitions of SNN used in this paper are listed and explained in detail.\n\n3. This paper provides a thorough comparison between the expressive capacity of SNN and ANN. Theorem 2 and Example 1 are two intuitive examples to understand the difference between SNN and ANN."
            },
            "weaknesses": {
                "value": "1. The theoretical conclusions are great but it seems that they are highly dependent on the realization of SNN. In Theorem 1, it is assumed that the sum of synaptic weights of each neuron is positive. Later in Remark 3, the authors claim that when the assumption is not met, the realization of the network is not well-defined. This seems to be a strong assumption to me and I am wondering how can we guarantee the positive sum of synaptic weights. In practice, is this assumption met in experiments? What will happen in practice if this assumption is not satisfied?\n\n2. The paper is self-consistent and rigorous but might be hard to follow for common readers. In Section 2, there are many formal definitions, which are general but may lose readability. For example, I think it would be better to provide a concrete formulation (or example) to introduce SNN and leave the formal definition in the appendix. The formal definition using a graph is general but might be hard to understand for most readers."
            },
            "questions": {
                "value": "1. To my understanding, Theorem 2 can serve as a comparison between SNN and ANN and implies that ANN can express the ReLU function more efficiently. Then I think it would be better to put Theorem 2 in section 3.2, where Theorem 2 and Example 1 study the advantages of ANN and SNN, respectively.\n\n2. The title directly indicates the central concern of this paper but the answer seems to be incomplete. To my understanding, Theorem 3 implies that SNN is at least as expressive as ANN from the perspective of equivalence of approximation. Then a natural question is can SNN be more expressive than ANN, i.e., how many parameters are needed for an ANN to express an SNN? If we can get a similar result as Theorem 3, then we can conclude that ANN and SNN have similar expressive power, otherwise, a separation result confirms that SNN can be more expressive than ANN. Although this paper provides several results (Example 1 and Theorem 4) to demonstrate the advantage of SNN, Example 1 is not a separation result, and Theorem 4 is not demonstrated through the number of parameters. It seems that Theorem 4 can be translated into a separation result to make Theorem 3 more complete. For more literature about separation results, it might be helpful to read [1,2,3].\n\n3. The abbreviation CPWL occurs on page 3 (the 4th line of the 2nd contribution) for the first time, but its full name is introduced at the beginning of section 3.\n\n[1] The power of depth for feedforward neural networks, COLT 2016.\n\n[2] Benefits of depth in neural networks, COLT 2016.\n\n[3] Theoretical exploration of flexible transmitter model, TNNLS 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7426/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697719957965,
        "cdate": 1697719957965,
        "tmdate": 1699636891502,
        "mdate": 1699636891502,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9SOPnJ9DaN",
        "forum": "nN1bEm8cna",
        "replyto": "nN1bEm8cna",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7426/Reviewer_3s5T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7426/Reviewer_3s5T"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a theoretical investigation into the expressivity of SNNs. The theoretical framework relies on classical spike response models and neuron firing times, specifically focusing on SNNs employing linear-response-with-delay neuron models. The study begins by approximating the ReLU activation function using these spiking neurons. It then establishes that SNNs with such models possess at least the same approximation capacity as ANNs. Finally, the paper demonstrates that the maximum number of linear regions generated by a one-layer SNN grows exponentially with input dimension, outperforming ReLU-based ANNs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is technically sound, employing formulas appropriately. Additionally, the remarks provided assist readers in comprehending the paper.\n\n2. The paper provides a comprehensive introduction and discussion of prior studies. Furthermore, it adequately addresses both its contributions and limitations."
            },
            "weaknesses": {
                "value": "1. About the novelty. The current version of the paper has not sufficiently clarified its novelty. While the study of SNNs for universal approximation of continuous functions is mentioned, the function classes and settings explored in this paper, while different, may still be considered as an incremental progress in this field.\n\n2. About the quality. The title may be somewhat overstated. This paper demonstrates that SNNs have the capability to approximate ANNs, rather than fundamentally surpassing them. As a result, the potential advantages of constructing non-continuous functions and utilizing sparse computation are not fully explored. Essentially, this work places a strong emphasis on establishing a bound that demonstrates SNNs can approximate ANNs, rather than thoroughly investigating whether and to what extent SNNs outperform ANNs. Therefore, the overall significance of this work may be somewhat limited.\n\n3. About the contributions. There are several assumptions or settings that may not be feasible in practical scenarios. It is crucial to address these limitations. Additionally, an important aspect to consider is the impact of timing length on the approximation complexity. This is essential as SNNs maintain a specific mapping from spike sequences to other sequences, which can be either continuous or non-continuous."
            },
            "questions": {
                "value": "1. Is there any unique difficulties for spiking neurons but not ReLU neurons?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7426/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7426/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7426/Reviewer_3s5T"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7426/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698310953213,
        "cdate": 1698310953213,
        "tmdate": 1700562726165,
        "mdate": 1700562726165,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7n3woHjUuo",
        "forum": "nN1bEm8cna",
        "replyto": "nN1bEm8cna",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7426/Reviewer_bNbs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7426/Reviewer_bNbs"
        ],
        "content": {
            "summary": {
                "value": "This paper proves that SNNs generate continuous piecewise linear mappings when using the SRM spiking neuron model. It also shows that the maximum number of linear regions generated by a spiking neuron scales exponentially with respect to the input dimension, which is more powerful than the ReLU."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is well-written and technically solid."
            },
            "weaknesses": {
                "value": "The proving of Theorem 1, and 2 are similar to those in [Yar17] and [SCC18]. It seems that the only difference is using different neurons."
            },
            "questions": {
                "value": "The universal approximation theorem of SNNs has been proved in [Maass, 1996c]. What is the difference between this paper and [Maass, 1996c]?\n\nIn fact, the SRM model is not the most popular spiking neuron model in deep SNNs. Why do the authors use the SRM model, rather than the LIF neuron in [Wu et al., 2018]?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7426/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7426/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7426/Reviewer_bNbs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7426/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698490728271,
        "cdate": 1698490728271,
        "tmdate": 1700710068411,
        "mdate": 1700710068411,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "m5ciMqWHBO",
        "forum": "nN1bEm8cna",
        "replyto": "nN1bEm8cna",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7426/Reviewer_2Xv7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7426/Reviewer_2Xv7"
        ],
        "content": {
            "summary": {
                "value": "The paper is a theoretical study of a model of spiking neurons, namely a linear Spike Response Model. It shows first that a network of such neurons is a continuous piecewise linear function, second that it can emulate any ReLU network, and third that the pieces of the piecewise function scales at most exponentially with the input dimension."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is in a field that is attracting growing interest. Moreover, I find that the authors have chosen to address a quite significant question, broadly speaking, namely the question of whether spiking neural networks have any theoretical advantages over more conventional models. Furthermore, I find the analogies between the linear spike response model and the ReLU insightful, to some extent."
            },
            "weaknesses": {
                "value": "The paper studies only a specific spiking model, namely a linear SRM. However, the abstract and the main text suggest in multiple occasions that the paper's results apply to a much broader class of models, i.e. SNNs.\nThe title in particular suggests that the paper could rule out a comparative advantage in the expressive power of SNNs, but the paper has not studied the full class of SNN models, so it cannot possibly do that. In fact, other works have shown such advantages (see next paragraph), so the title's question has arguably already been answered in the literature.\n\nThe paper has not reviewed certain quite relevant prior results from the literature.\nOther spiking neuron models, for example models with spike-based short-term plasticity, have been theoretically studied and shown to be more powerful than several non-spiking models in certain settings of temporal input. Moreover, that model showed in practice that networks of spiking neurons can actually outperform conventional models even in accuracy, not only in energy efficiency.\nIn addition, networks consisting partly of spiking neurons have been shown in both theory and practical benchmarks that they can learn to express not only _what_ has occurred in the input (classification), but also when (in the timing of the network's output), speeding up inference without trading off classification accuracy. This is another theoretical difference in expressivity from other neuronal models.\nThese recent but prior results have shown concrete advantages in the expressive power of spiking models in both theory and practice, but have they have not been mentioned in the present manuscript.\n\nSimilarly, other relevant works have shown that spiking neurons can represent multiple variables simultaneously, but they have also not been cited [3, 4].\n\nThese prior works take away some of the novelty of the present work. In addition, they suggest that today it is possible to couple theoretical insights on the expressive power of SNNs along with practical demonstrations of any differences - which is missing from the manuscript that is under review here. Only theoretical results are presented here.\n\nMoreover, it appears difficult even for future researchers to translate these theoretical insights into practical outcomes, as the authors here have not provided any directions.\n\nFurthermore, it appears to me that the results of the paper are not surprising. Using linear response functions with different gains at each input synapse, of course results in a piecewise linear combined function. Such a function of course can be shown equivalent to a certain combination of ReLUs. Having a different linear function at each synapse, of course can generate a larger number of linear pieces for the resulting function than a single ReLU. Or at least so it seems to me. Essentially, isn't the work merely based on expressing the input and the output as functions of time, but otherwise everything else is equivalent to conventional networks?\n\nLastly, that supposed difference between the spiking unit's and the ReLU perhaps is not significant. Isn't a spatially encoded input that is provided to a layer of multiple ReLUs equivalent to the temporal encoding and the SRM unit that the authors have assumed?\n\n\n[1] Moraitis et al., \"Optimality of short-term synaptic plasticity in modelling certain dynamic environments\", arXiv 2021\n\n[2] Jeffares et al., \"Spike-inspired rank coding for fast and accurate recurrent neural networks\", ICLR 2022\n\n[3] Moraitis et al., \"Spiking neural networks enable two-dimensional neurons and unsupervised multi-timescale learning\", IJCNN 2018\n\n[4] Izhikevich, \"Polychronization: computation with spikes\", Neural computation 2006"
            },
            "questions": {
                "value": "Could the authors address the weaknesses in their responses here and in the manuscript? I am open to revising my evaluation if these issues are sufficiently addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7426/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7426/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7426/Reviewer_2Xv7"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7426/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698957935987,
        "cdate": 1698957935987,
        "tmdate": 1700662616105,
        "mdate": 1700662616105,
        "license": "CC BY 4.0",
        "version": 2
    }
]