[
    {
        "id": "D18TsDSMQY",
        "forum": "bXeSwrVgjN",
        "replyto": "bXeSwrVgjN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4559/Reviewer_wwkN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4559/Reviewer_wwkN"
        ],
        "content": {
            "summary": {
                "value": "The authors propose an attribution methods that maximizes the a deletion/insertion metric, namely the AUC of the logit or probability. \nThe authors show that this optimization problem can be stated as a combinatorics one that, although being NP-hard can be approximated using existing methods.\nUnsurprisingly the method outperforms any baseline with respect to that metric since it optimizes it.\nThe paper contains extensive analyses especially on the difference between masking the most relevant features first or the least ones, what kind of masking, the use of logits or probabilities, etc."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper tackles an important issue and proposes an elegant solution.\n* The paper is well written an easy to follow.\n* The paper contains extensive analyses of different aspects of the problem."
            },
            "weaknesses": {
                "value": "* It's a journal paper that is cut to be conference paper. Important points are in the supplementary that are often difficult to find and thus rarely read. \n* Maybe I overlooked something, but the relevance of deletion/insertion methods and their conformity to what humans/experts expect is somehow missing. Maybe some qualitative results in the main paper would be nice.\n* The proposed method obviously outperforms existing ones. I think a ranking of the baselines is missing."
            },
            "questions": {
                "value": "* Could you provide a ranking the baseline methods and an analysis thereof. Yes, TRACE outperforms. It has to since it optimizes the metric. Yet it would be interesting do use these analyses to learn something about other more widely used existing methods.\n* Speaking of existing method, I believe LRP (which is a principled method leveraging the weights) is missing.\n* Blurring is never clearly defined.\n* Plots are not always clear, as many curves may overlap. Especially, Fig2a is unreadable as -GO curves are difficult to find, even when zoomed in. Maybe represent the difference, instead. The true value is not very interesting here.\n* Table 1: are the experiments repeated? do you report means? This table is not very well discussed and it contains anyway too many information. I don't have a solution, I just point it out. But again , it bols down again to the fact that only TRACE is studied, although insights on other methods would be interesting, eg. standard deviation between architectures.\n* The references seem to be in an arbitrary order. I am not sure if it is ICLR's template that is so, but an alphabetical order would be better.\n\n* There are several typos or un-introduced abbreviations. \n* Double check the references some are broken, eg. Wang and wang 2022a.\n* p3: you defined $\\psi=[.1,.5,.3,.2]$ and then use $\\phi_f(x)$ this is confusing and clutter the text. I suggest, to avoid using $\\phi_f at all$:\nFor example, if the attributions are $\\phi_f(x)=\\psi=[.1,.5,.3,.2]$ then $\\sigma(\\psi)=[1,4,3,2]... etc\n* The first paragraph of page 4 is convoluted although the point is simple. \n* p6 ... which is proved to be __a__ lower bound.. not [the] lower bound\n* p6 typo: GO-MO solves for an index [s]$s_k$.."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4559/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698229060503,
        "cdate": 1698229060503,
        "tmdate": 1699636433572,
        "mdate": 1699636433572,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XK6OdwXvX5",
        "forum": "bXeSwrVgjN",
        "replyto": "bXeSwrVgjN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4559/Reviewer_HLaZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4559/Reviewer_HLaZ"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors report the evaluation results of insertion/deletion metrics (MoRF, LeRF) across several benchmarks.\nThey proposed a method called TRACE to approximately optimize MoRF and LeRF using simulated annealing or greedy algorithms, and compare TRACE with existing saliency map methods using LeRF and MoRF.\nAs a result, TRACE-Mo, which optimizes MoRF, outperforms existing methods in terms of MoRF, and TRACE-Le, which optimizes LeRF, achieves superior results compared to existing methods in the context of LeRF.\nThe authors also conducted experiments varying the background used to fill the deleted pixels and by altering the sizes of the deleted pixels.\nThese experiments demonstrate that TRACE consistently yields favorable results irrespective of the background choice, and that larger sizes of deleted pixels tend to produce stable outcomes.\nMoreover, while TRACE-Le performs well for both MoRF and LeRF, TRACE-Mo yields less favorable results for LeRF.\nConsequently, the authors concluded that LeRF is a preferable evaluation metric for saliency maps based on these findings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The strength of this paper is the in-depth investigation of TRACE-Mo and TRACE-Le, which optimize MoRF and LeRF, respectively.\n\n**Originality, Quality**\n\nThe authors compared simulated annealing and greedy algorithms as optimization methods for TRACE-Mo and TRACE-Le.\nFurthermore, they reported that these approximate solutions yield practical and favorable results by exploring the lower bounds of MoRF and LeRF.\nThese comparative studies are both innovative aspects of this research and essential evidence that reinforce the validity of the experimental results.\n\n**Clarity, Significance**\n\nPlease refer to the \"Weaknesses\" below."
            },
            "weaknesses": {
                "value": "The weaknesses of this paper include \"question about the validity of the research approach\" and \"the absence of important related studies.\"\n\n**Question About the Validity of the Research Approach**\n\nIf I understand correctly, the primary aim of this research is to investigate the properties of MoRF and LeRF in an explanation-method-agnostic fashion.\nThe authors propose to achieve this by directly optimizing MoRF and LeRF.\nHowever, there is a lack of description of how directly optimizing these metrics leads to an explanation-method-agnostic investigation.\nThe direct optimization of MoRF and LeRF, as proposed in this study, gives rise to explanation methods such as TRACE-Mo and TRACE-Le.\nThe use of TRACE-Mo and TRACE-Le could be considered explanation-method-dependent rather than agnostic.\n\n**Absence of Important Related Studies**\n\nWhile this research suggests the direct optimization of MoRF and LeRF, similar studies have been conducted in the past.\nOne notable early study is [Ref1], which proposed optimizing MoRF as a continuous relaxation problem using gradient-based methods.\nIt also reported that the optiimzation of MoRF tends to generate artifact noise.\n[Ref2] also proposed optimizing MoRF and LeRF using greedy methods or continuous relaxation with gradient-based optimization.\nIt also reported adversarial noise appears in the context of MoRF optimization.\nFurthermore, [Ref2] concluded that LeRF is a more reliable metric than MoRF, as observed in this research.\nMany of the key results in this research have already been established in these related studies.\n\nAdditionally, there is a relevant study [Ref3] regarding saliency map evaluation metrics.\n[Ref3] investigated and evaluated the properties of MoRF and LeRF using a different approach than both this research and [Ref2].\nGiven its distinct perspective on exploring MoRF and LeRF properties, [Ref3] can be considered an important related study.\n\n* [Ref1] \"Interpretable Explanations of Black Boxes by Meaningful Perturbation,\" ICCV, 2017.\n* [Ref2] \"Feature Attribution As Feature Selection,\" OpenReview, 2018.\n* [Ref3] \"Sanity Checks for Saliency Metrics,\" AAAI, 2020.\n\nAs a minor weakness, Theorem 3.1 contains an error.\nMoRF and LeRF are defined as sums of f, as shown in Equation (1), while the proof in Theorem 3.1 defines them as sums of x.\nThis proof is only valid for the case where f is a linear function and is inappropriate for general f."
            },
            "questions": {
                "value": "* What is the new finding of this current paper over [Ref1] and [Ref2]?\n* How is the optimization of MoRF or LeRF relevant to the investigation of their properties?\n* Why the optimization of MoRF or LeRF can be considered as explanation-method-agnostic approach?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4559/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698387658893,
        "cdate": 1698387658893,
        "tmdate": 1699636433470,
        "mdate": 1699636433470,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VMnR67T1wC",
        "forum": "bXeSwrVgjN",
        "replyto": "bXeSwrVgjN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4559/Reviewer_VCEJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4559/Reviewer_VCEJ"
        ],
        "content": {
            "summary": {
                "value": "The work presents a framework for evaluating deletion metrics in explanation techniques for vision tasks. The approach proposes a formalization of the problem that relies on combinatorial optimization for finding good deletion trajectories and is evaluated on Imagenet and ResNet-18 (and other convolutional models)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1 - The formalization of the TRACE framework is meaningful and can be a useful tool for comparing different types of explanations. \n\nS2- The framework also has considerations for the OOD problem in explanations, which arise when features are replaced with \"null or zero\" values during deletion leading therefore to unnatural images."
            },
            "weaknesses": {
                "value": "W1- The paper has major presentation and linguistic problems in several areas. First, the main body of the paper does not provide enough intuition or sketches for the theoretical proofs. At the very least, the main body of the paper should provide a sketch of the proof flow. Second, the related work is moved to the appendix, and making it difficult for the reader to understand or position the novelty of the work with respect to previous work. Third, in several places, the paper is written too vaguely or in an informal manner, which makes it difficult to judge the specifics of the results. In particular, the introduction, the discussion of results and conclusions miss important specifics or at least linguistically difficult to understand. Lastly, the paper requires a revision throughout to make sure that the notation is consistent within the main body of the paper and the appendix, and that all explanation methods and acronyms are properly defined and introduced.\n\nW2- The paper does not take a strong position on making claims about which explanation methods perform better according to the TRACE framework. Results are mostly presented numerically but they are not discussed analytically."
            },
            "questions": {
                "value": "Further questions:\n\n- How does TRACE deal with correlated features? \n\n- How does (or may) TRACE operate on tabular datasets?\n\n- Do the authors observe any qualitative differences between the different model architectures and size when it comes to the quality of the generated explanations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4559/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4559/Reviewer_VCEJ",
                    "ICLR.cc/2024/Conference/Submission4559/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4559/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811899449,
        "cdate": 1698811899449,
        "tmdate": 1700635019851,
        "mdate": 1700635019851,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MRJbsOyiQ0",
        "forum": "bXeSwrVgjN",
        "replyto": "bXeSwrVgjN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4559/Reviewer_8p78"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4559/Reviewer_8p78"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method for finding the best-performing deletion sequence under a given deletion metric as a way of comparing different deletion metric settings and existing attribution-based explanation methods. The authors frame this problem as a combinatorial problem and find a computationally feasible way of finding a near-optimal solution. They compare different approaches in this regard. They consider various settings for the deletion metrics and study the performance of their method experimentally on image data. They demonstrate that indeed their method finds the best-performing explanations vs existing explanation methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Deletion metrics are often used in XAI and should be better understood, so this work contributes to an important area. \n\nThe paper is well organised and easy to read."
            },
            "weaknesses": {
                "value": "The proposed method seems to focus mainly on understanding better the behaviour of deletion metrics themselves. The authors show that various explanation methods then have a significant gap to the optimal deletion-sequence generated by their method. It\u2019s not clear to me if and how this insight could be used to also understand better how good a given explanation is."
            },
            "questions": {
                "value": "Nothing to add"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4559/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4559/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4559/Reviewer_8p78"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4559/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832466171,
        "cdate": 1698832466171,
        "tmdate": 1699636433273,
        "mdate": 1699636433273,
        "license": "CC BY 4.0",
        "version": 2
    }
]