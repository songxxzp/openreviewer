[
    {
        "id": "zGu4QcQjyw",
        "forum": "Km3Kprwyua",
        "replyto": "Km3Kprwyua",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2964/Reviewer_fopk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2964/Reviewer_fopk"
        ],
        "content": {
            "summary": {
                "value": "The author casts the speculative decoding problem as an online knowledge distillation problem. Online speculative decoding works by continually updating draft models on observed user query data using the abundant excess computational power in an LLM serving cluster. This approach frames the learning problem based on the auxiliary information as online knowledge distillation, where the teacher and student models correspond to the target and draft LLMs in speculative decoding, respectively. By doing so, the draft model can be refined in real-time, leading to more effective suggestions and improved predictive accuracy. The benefits of continually updating draft models include more accurate predictions, particularly on data originating from query distributions, and the ability to efficiently and effectively optimize the draft model in real-time."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Presentation of the idea is clear and straightforward. \n2. Evaluation is done thoroughly to understand how online speculative decoding performs under distribution shift to mimic real world scenarios."
            },
            "weaknesses": {
                "value": "1. In a few places in the paper, the author claims a translation between token acceptance rate and latency reduction. Is this done empirically or theoretically? Throughout the paper, the baseline seems to be against the offline distilled model and how the online model converges and eventually exceeds the performance of the offline distilled model, but the comparison did not include a vanilla model.\n\n2. The author claims an expected improvement over vanilla speculative decoding but does not show it empirically.\n\n3. Fine-tuning would require more computational resources. With more resources, the author could have fitted a larger draft model and performed vanilla speculative decoding. Why do we need an online distilled model in the first place?\n\n4. The author showed the results of the online distilled model after two epochs. What's the performance like during the first two epochs of fine-tuning? \n\n5. If we know that the performance improvement only shows after a certain amount of fine-tuning, does the real-world workload motivate this scenario? It's nice that the author considers the case of distribution shift, but the duration of each phase is also set arbitrarily and does not necessarily reflect the deployment scenario.\n\nOverall, I believe this is a nice paper on online knowledge distillation but the empirical analysis did not capture how it is better than prior speculative decoding approaches. The author proposed a framework that improves the performance of speculative decoding by doing fine-tuning but does not account for how that extra compute can benefit speculative decoding. All comparisons are against other knowledge distillation methods but lack an analysis against known speculative decoding techniques, such as vanilla speculative decoding and tree-based decoding. Also, there are no ablation studies on how hyperparameters in speculative decoding (such as draft model number of new tokens per generation) affect the performance."
            },
            "questions": {
                "value": "I have listed my questions above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2964/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2964/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2964/Reviewer_fopk"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698605346348,
        "cdate": 1698605346348,
        "tmdate": 1700938824956,
        "mdate": 1700938824956,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "b1MryYkybC",
        "forum": "Km3Kprwyua",
        "replyto": "Km3Kprwyua",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2964/Reviewer_rfi3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2964/Reviewer_rfi3"
        ],
        "content": {
            "summary": {
                "value": "Online speculating decoding proposes the idea of continuously updating the draft model when performing speculative decoding.\nThe main idea is that \"there is spare compute\" available when performing auto-regressive decoding. This spare compute can be used to fine-tune the draft model based on the distribution produced LLM."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea is fairly simple. Continously modifying the draft model can improve the token acceptance rate and provide higher speedups when using speculative decoding.\nThe authors have explored the space of distillation quite well."
            },
            "weaknesses": {
                "value": "There are certain points where added clarification of more evaluation will be appropriate. In general I found the evaluation to be underwhelming. Following are specific instances which can be improved.\n\n1. The authors claim there is spare compute as LLM serving is Memory Bandwidth bound. And based on this insight they propose OSD. However, concrete numbers regarding these are missing.Further the evaluation do not talk about runtime, only about token acceptance rates. Here is why I believe this is important, because in my opinion/experiments for most Large LLMs we are on a roofline where we are memomry bandwith bound, even the draft model is going to consume some amount of Memory Bandwidth when performing training. This could adversly effect LLM being served, due to interference. Therefore concrete numbers are going to be useful.\n\n\n2. My second concern is regarding data mixes. To be fair the authors have done a fair evaluation. However, I believe the evaluation is merely focussed on showing that OSD work. To me to some extent it is straightforward that as a model is fine tuned on the same distribution it starts mimicing, therefore the offline evaluation is kind of straightforward. However, as the authors very well understand (from their online evaluation) it is not very straightforward. I am curious why did the authors decide to have a separate model for each language. Is it a typical scenario for deploying speculative decoding. Further can the authors report speculative decoding numbers on english language without filtering.\n\n3. I would really like to see where the authors think their approach will fail. Are there dataset mixes where this idea will fail. Can we evaluate straight up on LMSys-chat to see how is works without all the filtering."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2964/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2964/Reviewer_rfi3",
                    "ICLR.cc/2024/Conference/Submission2964/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778764088,
        "cdate": 1698778764088,
        "tmdate": 1700938001157,
        "mdate": 1700938001157,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u3UUmvpahF",
        "forum": "Km3Kprwyua",
        "replyto": "Km3Kprwyua",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2964/Reviewer_g3SB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2964/Reviewer_g3SB"
        ],
        "content": {
            "summary": {
                "value": "Distilling LLM to smaller models for effective online performance is an active area of research and authors focus on this and propose an online speculative decoding approach to effectively perform this.\nThey use knowledge distillation using KL divergence loss and train a smaller model from teacher model.\nThey show that their model outperforms static FLAN-T5 in performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Shows that the online decoding (i am assuming trianing as well) helps improve acceptance rate compared to offline static training."
            },
            "weaknesses": {
                "value": "A bit hard to understand the novelty and contribution.\nExperiment baselines seem a bit lacking."
            },
            "questions": {
                "value": "I am may have missed somethings, but below are some of my questions.\nIt is unclear on what the true novelty of the paper is. If i understand correctly you are performing online decoding and training of draft model to adapt to distribution shift.\nAlso during the online distribution shift evaluation you do a sequential evaluation, what happens when you mix the data and evaluate? and what is the performance of static model on the same?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2964/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2964/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2964/Reviewer_g3SB"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813322163,
        "cdate": 1698813322163,
        "tmdate": 1700939775313,
        "mdate": 1700939775313,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MN3rC3yH0z",
        "forum": "Km3Kprwyua",
        "replyto": "Km3Kprwyua",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2964/Reviewer_y71B"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2964/Reviewer_y71B"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes online speculative decoding, which utilizes online knowledge distillation to update the small draft model, to improve the acceptance rate. The results show a substantial increase in the token acceptance rate by 0.1 to 0.48, which translates into 1.22x to 2.42x latency reduction."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This work is the first one that introduces the online draft model update to speculative decoding models, while previous speculative decoding models all assume a static draft model. \n2. This paper provides a thorough theoretical analysis to evaluate the speedup, latency, and flops."
            },
            "weaknesses": {
                "value": "1. Lack of comparison with SOTA works using \"multiple draft models\". One example [1].\n2. The speedup is theoretically estimated. Lack of real-hardware evaluation. \n\n[1] https://github.com/FasterDecoding/Medusa"
            },
            "questions": {
                "value": "1. Could the authors compare the proposed online speculative decoding to the multi-head speculative decoding work [1]? For example, can the proposed online update [1]? What are the potential challenges?\n2. Could the authors show real hardware evaluation results?\n\n[1] https://github.com/FasterDecoding/Medusa"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2964/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2964/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2964/Reviewer_y71B"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699245663756,
        "cdate": 1699245663756,
        "tmdate": 1699636240174,
        "mdate": 1699636240174,
        "license": "CC BY 4.0",
        "version": 2
    }
]