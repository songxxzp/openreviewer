[
    {
        "id": "OxP69zMBYX",
        "forum": "OdGyza5FO1",
        "replyto": "OdGyza5FO1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1267/Reviewer_isat"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1267/Reviewer_isat"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Motion PointNet for point cloud video action recognition. The Motion PointNet builds on PointNet++ encoder and a PDEs-solving module to capture input dynamics. The author adopts two state training to train the model. In first stage, the model is trained under the spatial feature reconstruction objective from the temporal features. In second stage, the model is fine-tuned for action recognition. The method outperforms previous approaches across different benchmarks, while the model is also lightweight compared with others."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- **Strong recognition performance**. The proposed Motion PointNet outperforms previous approaches on 3 public datasets in action recognition. \n\n - **Light model**.  While the performance is strong, the model is quite light in model size and computation. \n\n- **Rich comparison**. The paper makes a thorough comparison with state-of-the-art methods."
            },
            "weaknesses": {
                "value": "- **Understanding reconstruction objective** The loss function in (15) is not fully make sense. Given the model is trained from scratch, the \nGT $F_s$ used as supervision is also random features at the beginning, how would the contrastive objective leads the model towards the correct direction as it is the only loss used in pre-training stage?\n\n\n- **Motivation of PDE is not clear**. After reading the paper, I still do not quite understand why we need a PDE to build the mapping from temporal features to spatial features. What will be changed if we replace the spectral model with some MLP or transformer like networks as   long as we make a nonlinear mapping between two spaces. \n\n- **Why 2 stage training**. Could we jointly train a classification head along with the contrastive objective? What will the performance like.  \n\n- **Missing simple baselines**. There are some simple baselines the method should compare with. 1, train a model with the same encoder and classification head using the same number of iterations of two stage training. 2, only fine-tune the classification head while freeze the encoder to evaluate the pre-trained presentation."
            },
            "questions": {
                "value": "- **Feature response in figure 3**. How does those orange points are being selected? It seems like binary selection (I expect to see more colors represent the strongness of different points instead of the binary setting) What is the response from other methods (like PointNet ++) , this visualization comparison could tell the model indeed capture those dynamics. In second row of Side Kick, it seems the binary response still contain many irrelevant points based on locality instead of temporal features. \n\n\n- **Does the pre-trained features generalizable**. It seems in first stage training there is no labels required, so it is fully unsupervised. I wonder if the learned representation could be generalized or quickly adapted to other domains like what visual representation work did (i.e. MAE)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1267/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1267/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1267/Reviewer_isat"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1267/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818522618,
        "cdate": 1698818522618,
        "tmdate": 1699636053475,
        "mdate": 1699636053475,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yGx21V2v2B",
        "forum": "OdGyza5FO1",
        "replyto": "OdGyza5FO1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1267/Reviewer_KFbJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1267/Reviewer_KFbJ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method called Motion PointNet for dynamic capture in point cloud video human action recognition. The key contributions are:\n\n- They propose to view the dynamic capture process as solving partial differential equations (PDEs) in the feature space. This provides a new perspective to model the temporal dynamics.\n\n- They design a lightweight PointNet-like encoder to generate spatio-temporal features from point cloud sequences. \n\n- They introduce a PDEs-solving module to reconstruct the spatial features from the temporal features. This establishes a temporal-to-spatial mapping and enhances dynamic modeling. \n\n- The proposed method achieves state-of-the-art results on MSRAction-3D, NTU RGB+D, and UTD-MHAD datasets, with high efficiency in terms of parameters and FLOPs.\n\n- Ablation studies demonstrate the effectiveness of the PDEs-solving module in improving dynamic capture."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper presents a highly original approach for point cloud video action recognition by formulating the dynamic modeling as a PDEs-solving problem. Here are the key strengths:\n\n**Originality**: The perspective of using PDEs-solving for point cloud video modeling is novel and has not been explored before. Converting the dynamic capture to a PDEs problem with a temporal-to-spatial mapping provides a new way to establish temporal guidance.\n\n**Quality**: The proposed method achieves state-of-the-art results on multiple benchmarks with high efficiency, demonstrating its effectiveness. The comparisons to previous works are comprehensive. The ablation studies verify the contribution of each component.\n\n**Clarity**: The method is clearly explained with sufficient details and illustrations. The problem formulation of PDEs-solving for dynamic modeling is intuitive. The network architecture and training process are well elaborated. \n\n**Significance**: This work opens up a new direction of using PDEs-solving techniques for point cloud sequence modeling. The concept of converting dynamic modeling to a PDEs problem can inspire more future work. The high performance and efficiency also make the method attractive for real-world applications.\n\nIn summary, it proposes a novel perspective for dynamic point cloud modeling, achieves strong results, and clearly explains the key ideas. The PDEs-solving concept introduces new possibilities for point cloud video analysis."
            },
            "weaknesses": {
                "value": "While the paper presents a novel and effective approach, here are some weaknesses that could be improved:\n\n- The formulation and explanation of the PDEs-solving could be more rigorous mathematically. Some key equations lack details on the formulations.\n\n- The design space of the PDEs-solving module could be explored more thoroughly. For example, how are the basis operators and reconstruction loss function chosen?\n\n- The comparisons to some recent works like PointMapNet are missing. This could help better demonstrate advantages over other lightweight models.\n\n- The evaluations are limited to action recognition. It remains unclear how the dynamic modeling capability would transfer to other tasks like segmentation or detection.\n\n- There lacks ablation and analysis on different encoder architectures. Can other lightweight encoders also benefit from the PDEs-solving?\n\n- The computational complexity and efficiency analysis is incomplete. Actual runtime comparisons could better demonstrate the speed.\n\n- The model interpretability is limited. Visualizations or analyses connecting the PDEs-solving to improved dynamics are lacking."
            },
            "questions": {
                "value": "Please see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1267/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833591572,
        "cdate": 1698833591572,
        "tmdate": 1699636053413,
        "mdate": 1699636053413,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u9WoeMsHYj",
        "forum": "OdGyza5FO1",
        "replyto": "OdGyza5FO1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1267/Reviewer_RMAg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1267/Reviewer_RMAg"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method that extends PointNet++ for point cloud video processing. To tackle the motion information in point cloud videos, a partial differential equation (PDE) method is proposed. Experiments on the MSRAction-3D and NTU RGB+D datasets show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The method is effective and efficient. \n2. Using PDE to solve point cloud video problems looks novel."
            },
            "weaknesses": {
                "value": "1. It is not that clear what the most important part in the PDEs-solving module. To my understanding, it is basically a variant of Transformer. More comparision with vanilla Transformer is encouraged. \n\n2. It cloud be better to provide more details of PDEs and explain more the reason to use  the PDE method."
            },
            "questions": {
                "value": "The PDEs-solving module seems independent of point clouds. I wonder whether  the proposed module can be used for traditional video understanding."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1267/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1267/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1267/Reviewer_RMAg"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1267/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698841118078,
        "cdate": 1698841118078,
        "tmdate": 1699636053347,
        "mdate": 1699636053347,
        "license": "CC BY 4.0",
        "version": 2
    }
]