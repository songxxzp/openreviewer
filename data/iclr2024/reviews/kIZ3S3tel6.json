[
    {
        "id": "WcQoRUnDoe",
        "forum": "kIZ3S3tel6",
        "replyto": "kIZ3S3tel6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2132/Reviewer_scXv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2132/Reviewer_scXv"
        ],
        "content": {
            "summary": {
                "value": "This paper identifies a new phenomenon in neural network training, where some training examples exhibit opposite effects in training dynamics. Such opposite effects are signals that guide the network to learn specific features. These opposite signals influence the training dynamics significantly, sometimes producing loss spikes. Such opposite signals lead to sharpening, which happens in early layers.\n\nThere are numerous consequences of the opposite effects: (1) it allows neural networks to pick up finer features progressively (1) Batch normalization can stabilize the training dynamics (2) Adam and gradient clipping helps to mitigate the negative effects of training"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I think this paper presents an interesting (though potentially controversial) method of studying neural networks. Overall it contains a consistent and plausible story of what optimizing neural networks is doing internally, thus providing a mechanistic explanation to many empirical phenomena. While I find that many notions and claims are vague, I tend to believe that there are many valuable insights based on the extensive experiments and examinations of the literature.\n\nHere are some key strengths:\n1. The phenomenon of opposing signals is plausible and seems to be a fundamental cause of many existing techniques and artifacts about neural nets.\n2. The paper presents some heuristics that connect optimization with generalization, which has the potential to inspire new research\n3. There are various types of experiments, ranging from simple MLPs to ResNets and Transformers."
            },
            "weaknesses": {
                "value": "An obvious weakness is the vagueness that persists in the entire paper, but this seems to be a deliberate choice by the authors. As a consequence, I find many claims confusing to understand precisely, but overall this paper gives me a better understanding of training dynamics. So I would not object to this style as much as some readers do using a scientifically more rigorous criterion. Despite the weaknesses, I am inclined to give a favorable score.\n\nSome other weaknesses:\n1.  The writing can be organized in a better way. There are many claims that are buried in long paragraphs. I would suggest that the authors number and list their claims/hypothesis/conjectures clearly, and then provide explanations for each of the claims. Even if they don't have conclusive evidence, a clear statement should be presented clearly so that future work can cite the statement unambiguously.\n2. Currently there are no clear quantitative measurements. I do suggest that the authors provide one or two measurements to quantify \"opposing signals\" so that later work can examine their claims more precisely."
            },
            "questions": {
                "value": "1. Figure 1 suggests that at different phases of training, neural nets pick up very different features: initially features that rely on global information, and later features that rely on localized or detailed information. What is the reason for this? Do we believe that \"global information\" has stronger signal strength so it is learned first?\n2. With opposing signals, do neural networks produce differentiated neurons, each learning a different signal? Perhaps some examination of learned neurons/representations will tell.\n3. The authors mentioned 'In fact, in many cases, these features perfectly encapsulate the classic statistical conundrum of \u201ccorrelation vs. causation\u201d'. If a feature is irrelevant to a target task, why \"do meaningfully correlate with it\"? Why does a neural net learn features that are irrelevant to a classification task? \n4. The authors mentioned that sharpness is found in the early MLP layers and the embedding layer. What about the self-attention components (considering that they are the most important part of transformers)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2132/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698378962135,
        "cdate": 1698378962135,
        "tmdate": 1699636146142,
        "mdate": 1699636146142,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YR6LByQSyo",
        "forum": "kIZ3S3tel6",
        "replyto": "kIZ3S3tel6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2132/Reviewer_u2Ni"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2132/Reviewer_u2Ni"
        ],
        "content": {
            "summary": {
                "value": "This work discovers a phenomenon throughout the gradient descent training dynamics of deep neural networks: several paired groups of outlier samples emerge in the training set, where the samples of each group in a pair have a strong opposing signal to the other pair, i.e. their corresponding gradients are large and point at opposite directions. The authors hypothesize that this observation could be the reason behind the \"edge of stability\" and some other phenomena in deep learning optimization."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* To my knowledge, the observation is novel and can have significant impacts on our understanding of the optimization dynamics of deep neural networks.\n\n* Many connections are established with other observations in the recent literature.\n\n* There are experiments on a variety of setups to ensure the observations are consistent across different settings."
            },
            "weaknesses": {
                "value": "* My main concern with the current state of the submission is that the notion of outlier training samples does not seem to be precise enough and might require further clarification. For example, the authors show that the loss of almost half of the training samples may go up during a single iteration, but this change is more significant for a smaller number of outlier samples.  It is not clear what fraction of the training samples are outliers and more generally what the distribution of the loss change over all samples is.\n\n* The relationship between the theoretical statements and the empirical observations of the paper is not entirely clear to me. The theory seems concerned with analyzing the sharpness throughout training with *gradient flow* which monotonically decreases the training loss, while a core part of the observation is to justify the spikes in the training loss that occur due to *large step size gradient descent*."
            },
            "questions": {
                "value": "* As mentioned above, it might help the readers have a better understanding if there is a visualization of the distribution of the change in loss for training samples, highlighting the frequency of the outliers in comparison with the entire training set.\n\n* The statement of Theorem 3.2 mentions that \"sharpness will increase linearly in $\\Vert \\beta \\Vert$ until some time $t_2$\". Can an exact formula (or at least more explicit upper/lower bounds) be given for sharpness as a function of time? I believe this should be possible due to having explicit formulae for the weights during training for the linear model. Additionally, is it possible to highlight the role of the problem parameters including $k, \\alpha$, and $\\Vert \\beta \\Vert$, and their relationship to the observations for deep networks?\n\n* The example of opposing signals in text does not show the training error of the opposing outlier groups throughout training. Is it possible to recreate Figure 1 for text data?\n\n* As a minor comment, the sharpness is typically referred to as the \"largest eigenvalue of the loss\", while it could be more accurately described as the \"largest eigenvalue of the Hessian of the loss\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2132/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2132/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2132/Reviewer_u2Ni"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2132/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789281927,
        "cdate": 1698789281927,
        "tmdate": 1699636146074,
        "mdate": 1699636146074,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6t08Wvkxkr",
        "forum": "kIZ3S3tel6",
        "replyto": "kIZ3S3tel6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2132/Reviewer_5nbF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2132/Reviewer_5nbF"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to understand how optimization is affected by a \"opposing signals\" --- subset of signals that can be associated with different classes depending on the rest of the signals ---  from small groups. For example, skies appear in backgrounds of planes most often, but they also appear in backgrounds of boats. The main claim of the paper is that such signals affect the progress of optimization by forcing the model to approach a basin in loss landscape that balances the opposing the forces. The paper shows experimenting track instability."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper seems to identify an interesting phenomenon where the loss landscape that models end up in can be affected by opposing signals from small groups in data. There are some interesting plots in the paper that show case how the model depends on certain features (like the sky color)."
            },
            "weaknesses": {
                "value": "There is too much informal language and the paper does not consider other confounding factors in leading to the instability. Specifically, the paper uses \"per-step largest loss change\" to define groups. Of course these groups will have large gradient norm because they have the largest errors in them. \n\n1. There are words like like genuine, correct, and random noise used but without formal definition. The authors acknowledge this but without clarity on what these are, the proposed phenomenon is too vague. You can define features are the groups of data that the feature helps correctly classify,  which becomes a formal definition.\n\n2. The synthetic experiment has a non-linear ground truth (absolute value of x_0), is that what contributes to the reduction in sharpness? \n\n3. What about the roles of batchsize, learning rate, lr schedulers, other kinds of regularization?\n\n\nOne of my issues with the writing is loose language. For example, why call $x_0$ unimportant? Clearly it helps predict the output, just on a subset of data. The idea of opposing signals seems very similar to have a \"shortcut\". The text in section 3 seems tedious because the authors use language the seems to be intentionally removed from terminology in shortcut learning  or distribution shift, but they use an example that is commonly called shortcut learning (model rely on skies to classify planes because many planes have sky in the background).\n\n\nFinally, the main claim is \"opposing signals affect shaprness like so ... \". Then, there should be a single plot that shows this exactly. I could find one such experiment. Why not directly measure sharpness, at least for the first few layers? It seems like this was done in \"Additional findings\", but there is not plot and supporting text. It would be great if there was a plot that showed, training instability occur exactly when sharpness rises up, and then controlling opposing signals via some robust loss or remove some samples both removes the sharpness and training instability. \n\n\nOverall, the idea is interesting but the experiments need to be more than a collection of simple post-hoc checks to show that the cause of optimization is certain groups. One direction, maybe is to randomly change the backgrounds of the supposed outlier groups (upto rho fraction) and show the sharpness behavior is avoided."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2132/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699240035840,
        "cdate": 1699240035840,
        "tmdate": 1699636145951,
        "mdate": 1699636145951,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HCqzo6nvxq",
        "forum": "kIZ3S3tel6",
        "replyto": "kIZ3S3tel6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2132/Reviewer_VsCH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2132/Reviewer_VsCH"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the influence of samples w/ large, opposing features, and provides explanations for several prior observations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper explores the Outliers with Opposing Signals on the training process of neural networks. \nThe authors support their findings with extensive experiments and bolster their claims through the use of toy models. \nAs applications, the authors also provide new understanding of Progressive Sharpning and EoS."
            },
            "weaknesses": {
                "value": "Please see \"Questions\"."
            },
            "questions": {
                "value": "1. In EoS, vibrations are observed along the maximal eigen-direction. Could the authors explain how the presence of outliers impacts this phenomenon?\n\n2. In [1], the authors proposed the concept of multi-view features, which seems to be similar to the Outliers with Opposing Signals in this work. Could the author provide some comparisons?\n\n[1] Allen-Zhu and Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. (ICLR 2023)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2132/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2132/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2132/Reviewer_VsCH"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2132/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699258053390,
        "cdate": 1699258053390,
        "tmdate": 1699636145889,
        "mdate": 1699636145889,
        "license": "CC BY 4.0",
        "version": 2
    }
]