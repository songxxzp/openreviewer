[
    {
        "id": "zmyjMUNntp",
        "forum": "dHng2O0Jjr",
        "replyto": "dHng2O0Jjr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1154/Reviewer_u7K9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1154/Reviewer_u7K9"
        ],
        "content": {
            "summary": {
                "value": "Tool utilization is an important capability of LLMs to extend their task scope. Although closed-source LLMs have achieved powerful performance by calling external tools, it is still important for open-source LLMs to enhance their capability in tool use. In this paper, the authors proposed ToolLLM to facilitate the large language model to master 16,000+ tools. Moreover, the contributions of this paper can be summarized in three parts: 1) ToolBench, a benchmark which generate instructions involving different tool utilization; 2) a depth-first search-based decision tree algorithm is introduced to enhance the capability of LLMs in tool utilization; 3) an evaluation platform called TaskEval. Experimental results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Although tool utilization has received much attention in LLM applications, building a standard benchmark is still challenging. To address this, this paper releases a high-quality instruction tuning dataset, called ToolBench, which covers many different tools.\n2. Based on the constructed ToolBench, this paper also designs a strategy for solution path annotation, which uses a depth-first search-based decision tree to search for a possible valid path.\n3. Experimental results validate that tuning LLMs with generated samples can effectively improve performance. \n4. The writing of this paper is good and easy to follow."
            },
            "weaknesses": {
                "value": "1. This paper introduces TaskEval, which encompasses two metrics, *Pass Rate* and *Win Rate*. Specifically, *Pass Rate* detects whether LLM can successfully execute user instructions, and *Win Rate* is designed to judge which solution path is better for a given instruction. However, to some degree, I think these metrics still have some deficiencies and cannot efficiently the capability of LLMs in tool utilization. For example, *Pass Rate* can only reflect LLM whether can execute user instruction. Sometimes, powerful LLMs (e.g., ChatGPT) can always generate answers for any user instructions without any tool use, and besides, the hallucination of LLMs can also let it generate some executed but counterfactual answers. Besides, *Win Rate* can only reflect the capability of different LLMs, not their ability of tool utilization. Even two LLMs without tool utilization (e.g., ChatGPT v.s. LLaMA-7b), will also demonstrate differences in performance. Therefore, I think the proposed metrics can only reflect the capability of LLMs but not their tool-use ability. Of course, I also admit that it is challenging to build such a metric since there have been no metrics in this area before.\n2. I appreciate that the paper releases a high-quality dataset in this area. However, it will be better to provide a detailed statistical of this dataset (e.g., distribution in domain, error analysis)."
            },
            "questions": {
                "value": "1. In the design of solution path annotation, authors prefer to use DFS instead of BFS. However, DFS could also possibly backtrack and search multiple answers if it does not obtain a valid path. So, any experiments to prove the cost of DFS to find a valid path when compared with BFS?\n2. This paper mainly uses RapidAPIs for training. So is the fine-tuned ToolLLaMa suitable or adapted for APIs in other sources/formats (e.g., Pytorch/Tensorflow/HuggingFace in Gorilla, ChatGPT plugins)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1154/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698309574228,
        "cdate": 1698309574228,
        "tmdate": 1699636041816,
        "mdate": 1699636041816,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4tFKOGkjW9",
        "forum": "dHng2O0Jjr",
        "replyto": "dHng2O0Jjr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1154/Reviewer_2o14"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1154/Reviewer_2o14"
        ],
        "content": {
            "summary": {
                "value": "- The paper introduces ToolLLM, a framework to facilitate tool use capabilities in open-source large language models (LLMs). It includes data construction, model training, and evaluation components.\n- A new instruction tuning dataset called ToolBench is constructed using ChatGPT. It contains over 16,000 real-world APIs from RapidAPI spanning 49 categories. The dataset covers both single-tool and multi-tool instructions.\n- An automatic evaluator ToolEval is developed to assess tool use capabilities. It incorporates pass rate to measure executability and win rate to compare solution quality.\n- By fine-tuning LLaMA on ToolBench, ToolLLaMA is obtained. Experiments show it matches ChatGPT's performance and generalizes well to unseen APIs. An API retriever is also trained to automatically recommend relevant APIs. ToolLLaMA demonstrates strong generalization on the out-of-distribution APIBench dataset, despite not being trained on it. This validates its capabilities on new domains."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Leveraging existing techniques in the literature, authors build a framework for developing models capable of tool use. This framework encompasses dataset building, model training, and model evaluation. The scope is comprehensive, and the execution is generally solid. \n\nThe authors do a good job documenting and elaborating design decisions such as dataset filtering and issues with prompting with a limited context window. \n\nThe authors make their artifacts (ToolBench, ToolEval, and model artifacts) publicly available so others can build on their work."
            },
            "weaknesses": {
                "value": "Most of the technical ideas in this work are from past works, with the exception of DFSDT, which is a simple application of DFS to prompting. In this sense, the current work's technical contribution is low."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1154/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698703405499,
        "cdate": 1698703405499,
        "tmdate": 1699636041744,
        "mdate": 1699636041744,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u2EWfymWK8",
        "forum": "dHng2O0Jjr",
        "replyto": "dHng2O0Jjr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1154/Reviewer_k5uq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1154/Reviewer_k5uq"
        ],
        "content": {
            "summary": {
                "value": "The paper collected a new instruction-tuning dataset (called ToolBench) for improving LLM's tool-use capability. To construct ToolBench, the author collected more than 16k REST APIs from RapidAPI and constructed synthesized instructions based on these APIs. To automatically obtain the desired behavior of the LLM, the author prompted prompt gpt-3.5-turbo-16k. By finetuning LLaMA on ToolBench, the author showed that ToolLLaMA achieves higher score according to a ChatGPT evaluator than Text-davinci-003 and Claude-2 in ToolBench."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is valuable to those who are trying to build LLMs that can use tools. ToolBench is a well-engineered dataset and is shown to be more diverse than prior datasets in Table 1."
            },
            "weaknesses": {
                "value": "1. There are lots of papers that studied how to instruction-tune an LLM for tool-use, like GPT4Tools (in NeurIPS 2023), Gorilla, ToolAlpaca, etc. The techniques adopted in ToolLLM, such as constructing instruction-tuning samples by prompting ChatGPT, is very standard and has limited novelty. The paper is mainly about applying a popular synthetic instruction generation pipeline to distill knowledge from gpt-3.5 and has almost no research value.\n\n2. The author proposed \"Depth First Search-based Decision Tree\" (DFSDT) that seems to outperform ReAct prompting. However, the technique is closely related to self-consistency + CoT and Tree-of-thoughts. It is also unclear how DFSDT will perform for other types of LLMs not in the category of LLaMA / GPT / Claude.\n\n3. The evaluation metrics are based on ChatGPT, which is highly unreliable and may favor models tuned with ChatGPT-prompted datasets. The author mentioned in Appendix A.5 (Paragraph \"Comparing Human Evaluation and ToolEval\") that `Our ChatGPT evaluator demonstrates a high agreement of 87.1% in pass rate and 80.3% in win rate with human annotators. `. This shows that there is **close to 20% disagreement** between human evaluation and ChatGPT evaluation. This is a large discrepancy and should not be ignored."
            },
            "questions": {
                "value": "How does the models in Table 4 compare with each other if we adopt human evaluation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1154/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1154/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1154/Reviewer_k5uq"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1154/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784362014,
        "cdate": 1698784362014,
        "tmdate": 1700952843553,
        "mdate": 1700952843553,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wInYZBZ1HZ",
        "forum": "dHng2O0Jjr",
        "replyto": "dHng2O0Jjr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1154/Reviewer_dyqS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1154/Reviewer_dyqS"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces ToolLLM, a whole pipeline for creating and evaluating instruction-tuned language models that can use tools.\n\nThe authors created ToolBench, an instruction-tuning dataset by i) collecting a large number of real-world APIs from multiple categories; ii) creating instructions with seed demonstrations and tool sets; iii) annotate the api call solution paths with ChatGPT.\n\nThey fine-tuned several baselines on ToolBench and evaluated them with the proposed ToolEval pipeline, which also uses ChatGPT to evaluate whether the solution path is successful and whether the solution path is better than the one annotated by ChatGPT.\n\nAdditionally, they also propose to use an API retriever and a tree-search algorithm (DFSDT) during inference."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The scale of ToolBench is unprecedented compared to previous tool learning datasets. It has more APIs, more tools, more task instances. This makes ToolBench much closer to real-world settings of tool-augmented language models.\n2. The authors conducted extensive experiments to show that their instruction-tuning pipeline enables language models to generalize to new tools and new domains without seeing them in the fine-tuning dataset."
            },
            "weaknesses": {
                "value": "### 1. The current annotation/evaluation pipeline is not rigorous and may lead to false impressions about model\u2019s performance.\n\nToolBench is entirely annotated by ChatGPT. I understand that the annotations of the instruction-tuning data don\u2019t have to be perfect, but I think as a benchmark for evaluation, it should be held to higher standards.\n\nToolEval solely relies on ChatGPT to evaluate whether a solution path \u201cpasses\u201d and whether it \u201cwins\u201d the solution path from ChatGPT.\n\nWhile I appreciate the authors\u2019 efforts in comparing ChatGPT\u2019s evaluation with human subjects\u2019, I think the current evaluation pipeline is problematic because of the following reasons:\n\n- Many tool-related tasks can have a definite set of correct answers or correct solution paths. For these tasks, the real correct answers should be used to judge the correctness of generated answers. Relying on ChatGPT\u2019s annotation of win rates and pass rates can lead to over-confidence about wrong answers that look satisfactory.\n- ChatGPT (and other language models) as an evaluator is known to have order bias (gives preference to an option based on their order), egocentric bias (prefers its own outputs), length bias [1], selection bias [2]. It also chooses style over substance [3]. I would expect more discussion on if and how ToolEval mitigates these biases.\n- The evaluation rules for pass rates and win rates seem too complex even for human annotators to follow, which severely undermines my confidence in your human evaluation results. According to Appendix A.5, the rules to determine whether a solution path gets a \u201cpass\u201d form a 3-layer decision tree with as many as 10 leaves and each decision in the tree requires some non-trivial and thorough examination of the instructions, the available APIs and the solution path. I would really love to learn more about your human evaluation process. For example, did the annotators only submit the final flag of pass/fail/unsure or did they also submit all the decisions they made to get to the final results? Is there any evidence that can show that human annotators were faithfully following your rules instead of relying on human cognitive biases? If the human annotations themselves were not reliable, nor would the correlation between human and ChatGPT be good enough.\n- Win rates are computed by comparing model generation with annotations from ChatGPT+ReAct. Could inference algorithms that are more similar to the annotation pipeline (instead of better) lead to better results?\n\n**Reference**\n\n[1] Koo, Ryan, et al. \"Benchmarking Cognitive Biases in Large Language Models as Evaluators.\"\u00a0*arXiv preprint arXiv:2309.17012*\u00a0(2023).\n\n[2] Zheng, Chujie, et al. \"On Large Language Models' Selection Bias in Multi-Choice Questions.\"\u00a0*arXiv preprint arXiv:2309.03882*\u00a0(2023).\n\n[3] Wu, Minghao, and Alham Fikri Aji. \"Style over substance: Evaluation biases for large language models.\"\u00a0*arXiv preprint arXiv:2307.03025*\u00a0(2023).\n\n### 2. The evaluation results are constantly changing over time, making it very expensive and difficult to compare new methods and older baselines. This also ruins the reproducibility of the evaluation pipeline.\n\nI appreciate the authors\u2019 consideration about the temporal variability on RapidAI.\n\n> *Considering the API\u2019s temporal variability on RapidAPI and the infinite potential solution*\n*paths for an instruction, it is infeasible to annotate a fixed ground-truth solution path for each test instruction. Moreover, when comparing different models, it is crucial to ensure they employ the same version of APIs during evaluation.*\n> \n\nI think this is also part of the reason why they used ChatGPT as an evaluator instead of using ground truth annotation.\n\nHowever, I still think it\u2019s problematic.\n\nFirst, people are not able to know roughly how good a method is by looking at the reported pass rates and win rates, because the reported results can only be compared with the results evaluated during the same time period.\n\nSecond, it makes evaluation much more difficult. As the authors pointed out, each evaluation run needs to use the same version of APIs to ensure fair comparison. This means every new method that needs evaluation on ToolEval must also run every baseline they compare to in a very short period of time. If some highly variable API were used (for example, APIs that query the availability of restaurants or realtime weather), this period could be as short as a few hours. This creates an extremely heavy burden for developers and researchers, because not only do they need to run all evaluation experiments multiple times, they also have to run them in parallel.\n\nTherefore, at the very least, I would expect some qualitative analysis of the tool set that can point out how many task instances involve these temporally variable tools and how much temporal variability impacts the evaluation results over time.\n\nI think a better solution to this problem might be creating a \u201csnapshot\u201d of the API call results at a certain time and release this snapshot with the evaluation suite. This way, it\u2019s much easier to get reproducible results.\n\n### 3. The comparison between DFSDT and ReAct.\n\nWhy isn\u2019t DFSDT always better than ReAct? According to your description, it seems that ReAct is a special case of DFSDT where the branching factor is 1. Therefore, I expected DFSDT to be better than ReAct in most cases. In other words, the win rate for ChatGPT+DFSDT against ChatGPT+ReAct should be close to 100%. According to the first two rows in Table 4, that is clearly not the case. Is there some explanation on why that happened?"
            },
            "questions": {
                "value": "How were ToolBench split into tuning and evaluation subsets? Could you explain more about this part?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1154/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1154/Reviewer_dyqS",
                    "ICLR.cc/2024/Conference/Submission1154/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1154/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698940011065,
        "cdate": 1698940011065,
        "tmdate": 1700491430998,
        "mdate": 1700491430998,
        "license": "CC BY 4.0",
        "version": 2
    }
]