[
    {
        "id": "Lp4QnFFDqI",
        "forum": "mNYF0IHbRy",
        "replyto": "mNYF0IHbRy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4903/Reviewer_kt1C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4903/Reviewer_kt1C"
        ],
        "content": {
            "summary": {
                "value": "This proposed a framework for generating images from complex and detailed prompts, which took more work for previous models or frameworks. They utilize LLM's capability to generate augmented textual and visual prompts for better generations. And the framework comprises global scene generation and an iterative refinement scheme to align with conditional cues."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This work steps toward longer textual descriptions for image generations to ensure the fidelity of complex textual prompts. \n\n- Scene blueprints with the iterative refinement step ensure high prompt adherence recall, quantitatively validating its effectiveness."
            },
            "weaknesses": {
                "value": "- Missing information on human study. Detailed information on human study could be provided to assess the reliability of the outcome. How do you recruit and select them based on what qualification? Isn't there any conflict of interest for the subjects and authors? How many subjects are recruited? What was the confidence of the votes? This issue is the major reason for leaning toward rejection. I am eager to see the author's feedback for reassessment.\n\n- This framework relies on recently proposed models like a strong LLM, CLIP, and an image composition model. The collection of previous works provides shallow techniques compared with them.\n\n- A missing related work. DenseDiffusion [1] may be worth being included in the layout-to-image generation subsection in Sec. 2 and the box-level multi-modal guidance in Sec. 3.4. DenseDiffusion tried to manipulate attentional weights to control the regions for layout guidance selectively. It would be appreciated if you could compare your method with it for readers in the upcoming revised manuscript. Note that, due to the narrow accessibility to this work at the time of submission, this is not considered in the score evaluation.\n\n[1] Kim, Y. et al. (2023). Dense Text-to-Image Generation with Attention Modulation. http://arxiv.org/abs/2308.12964"
            },
            "questions": {
                "value": "- Minors:\n  - In Sec. 3.1, Models. -> Models (Please exclude the period in the section title.)\n  - In Fig. 2, it would be inappropriate to include the logo of a commercial product (ChatGPT from OpenAI) in an academic paper. And the company may not allow the usage of their logo to promote the work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I leave the comment that \"In Fig. 2, it would be inappropriate to include the logo of a commercial product (ChatGPT from OpenAI) in an academic paper. And the company may not allow the usage of their logo to promote the work.\" for the possible legal issue."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4903/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698389254900,
        "cdate": 1698389254900,
        "tmdate": 1699636475101,
        "mdate": 1699636475101,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Fsv6fsKfEV",
        "forum": "mNYF0IHbRy",
        "replyto": "mNYF0IHbRy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4903/Reviewer_Br9r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4903/Reviewer_Br9r"
        ],
        "content": {
            "summary": {
                "value": "This work aims to solve the problem that prior text-to-image models cannot accurately follow the object specifications in lengthy prompts. The work proposes a novel two-step pipeline that first uses a scene blueprint, which is an LLM-generated layout with object descriptions, to generate the overall image. A CLIP-based guidance is then applied to perform iterative refinement in order to make sure the content of each box is correct. The method enables accurate and diverse image generation with intricate and lengthy input text prompts. The user study and qualitative comparison indicate a non-trivial improvement over baseline methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. In addition to previous methods using the layouts for initial image generation, the method further proposes box-based refinement to improve the ability to generate all the objects mentioned in a lengthy and intricate prompt.\n2. The method points out the fact that there currently lacks a pipeline for benchmarking text-to-image methods with lengthy prompts and proposes a metric called prompt adherence recall (PAR) to evaluate their method and several baselines.\n3. Their method has better prompt adherence compared to baselines in prompt generation. The user study also confirms that the method can faithfully generate objects in the prompt."
            },
            "weaknesses": {
                "value": "1. This work builds on LLM-grounded Diffusion, as mentioned in Sec 3.3. However, the difference between LLM-grounded Diffusion and the current work is not clearly explained. This work lists \"scene blueprints using LLMs\" as one of the contributions, but both LLM-grounded Diffusion and LayoutGPT (with both works cited in related work section) propose using LLM to generate the scene layouts with object descriptions. The authors show that the scene representation enhanced by the proposed method works better for long text prompts, but this claim of \"proposing a structured scene representation\" is not a contribution as it has been proposed and used in previous works.\n2. The interpolation of k generated layouts could potentially cause degenerate or undesired results. For example, for \"a cat and a dog, one on each side\", the LLM can generate 1) a cat on the left, a dog on the right, or alternatively 2) reverse the position for the cat and the dog. The interpolation of layout 1) and 2) results in two objects placed both at the center.\n3. Missing details: In Sec 3.3, the authors mention \"image-to-image translation\" with latent/stable diffusion that leads to generation with higher quality, but no details such as the prompts or the exact approach are given. Is this simply adding some noise and denoise?\n4. The evaluation of the work does not investigate whether the proposed method degrades the fidelity of the image. The user study only involves asking whether objects are present and layouts are accurately generated rather than comparing the overall image quality. The author needs to present evaluation results that show their method does not have significant degradations on quality. If only the presence of objects is considered, a simple \"copy-and-paste\" from baseline text-to-image model with individual object generation using the generated layout would also result in high object presence without bringing much utility to the research community and potential applications.\n\nA typo that does not affect the rating:\n1. api -> API (page 7)"
            },
            "questions": {
                "value": "The authors are encouraged to respond to and address the weaknesses in the section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4903/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4903/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4903/Reviewer_Br9r"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4903/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698535407259,
        "cdate": 1698535407259,
        "tmdate": 1699636475011,
        "mdate": 1699636475011,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CiOJLmPhWz",
        "forum": "mNYF0IHbRy",
        "replyto": "mNYF0IHbRy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4903/Reviewer_c4Qg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4903/Reviewer_c4Qg"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel approach for improving text-to-image generation in diffusion-based models when processing complex scenes with multiple objects and intricate text prompts. The authors leverage Large Language Models (LLMs) to extract the layout information, detailed text descriptions, and background information from text prompts. Then the proposed layout-to-image generation model is composed of two stages: Global Scene Generation and Iterative Refinement Scheme. The Global Scene Generation phase uses object layouts and background context to create an initial scene which roughly represents the target image layout but not very accurate. Then the Iterative Refinement Scheme iteratively evaluates and refines box-level content to align them with their textual descriptions and recompose objects to ensure consistency. Extensive experiments are conducted to validate the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed approach can handle the complex scenarios of text-to-image generation with long text prompts very well.\n2. The paper is well-written and easy to follow.\n3. The iterative refinement loop provides a possible solution to generating images of complex scenes."
            },
            "weaknesses": {
                "value": "1. I think the major limitation of the proposed approach is efficiency. The complexity of the proposed approach increases as the number of objects increases for complex scenes. This might also be the major issue presenting this approach to be applied in real usage.\n2. It seems that although the generation of objects can be iteratively refined, the bounding box locations cannot be refined. If the LLM predicts unreasonable bounding box layouts at the first stage, it cannot be corrected. Have the authors think of introducing the refinement of bounding box locations into the pipeline?\n3. How many layouts are needed to interpolate? How does the number of layouts for interpolation affect the results?"
            },
            "questions": {
                "value": "Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4903/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699284577085,
        "cdate": 1699284577085,
        "tmdate": 1699636474897,
        "mdate": 1699636474897,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GSQu5jx5YE",
        "forum": "mNYF0IHbRy",
        "replyto": "mNYF0IHbRy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4903/Reviewer_XXwY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4903/Reviewer_XXwY"
        ],
        "content": {
            "summary": {
                "value": "The paper extends recent works which leverages layouts to generate scenes corresponding to complex text-prompts. This work first shows that for complex prompts, existing layout to image generation methods have certain failure modes and proposes some practical modifications which are augmented with existing layout to scene generation methods.  First, the authors propose a scene blueprint to represent complex text-prompts; Secondly the authors design an iterative refinement process which improves the alignment of generated images with the complex text-prompts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The research question is a very practical problem \u2014 usually most of the text-to-image generation models are not good at coherent images corresponding to complex prompts, so providing a solution for it is important.\n- The method consists of various components (a lot of these components are existing though) and is conceptually intuitive!\n- The framework obtains strong results on human-study for fidelity of images generated for long prompts."
            },
            "weaknesses": {
                "value": "Cons / Questions\n\n- While the writing is satisfactory, it can still be improved! The authors should provide more information in the paper on how Eq. (5) is used to guide the sampling process.\n- Can the authors provide more intuition on the interpolation step? \n- Given that there are stronger open-source diffusion models (e.g., DeepFloyd) \u2014 the authors should provide some context on how long prompts work in those cases, as they use a stronger text-encoder like T5. \n- While the authors comment that the size of the tokens (77 in CLIP) is one of the potential reasons on why SD cannot generate compositional prompts \u2014 I believe this is only partially true. Even for non-complex compositional prompts, SD is not able to generate coherent images. Can the authors comment in general on some potential reasons why these text-to-image models are not able to generate images corresponding to simple compositional or complex prompts? I think both are related somehow and it will be beneficial to provide some context regarding it."
            },
            "questions": {
                "value": "See Cons/Questions;\nOverall, the paper is practical, but the various components though intuitive are not technically novel. While I do agree that not everything in a paper needs to be novel \u2014 the authors should provide solid justifications on the design of each component.  \n\nI am happy to revisit my scores after the rebuttal!"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4903/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699300622095,
        "cdate": 1699300622095,
        "tmdate": 1699636474793,
        "mdate": 1699636474793,
        "license": "CC BY 4.0",
        "version": 2
    }
]