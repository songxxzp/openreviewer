[
    {
        "id": "kpbUenrIoP",
        "forum": "4KqkizXgXU",
        "replyto": "4KqkizXgXU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6325/Reviewer_NqgB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6325/Reviewer_NqgB"
        ],
        "content": {
            "summary": {
                "value": "This paper points out that the current red teaming approaches based on RL can not generate test cases with high diversity. To address this, the authors propose a curiosity-driven exploration method to train the read team models. This approach jointly maximizes the red team effectiveness and also a diversity reward, where the authors tried different metrics.  Experimental results show that the proposed approach can not only maintain the effectiveness but also increase the test case diversity, compared with previous RL-based approaches. However, the experiments are mainly based on small models GPT2 with 137M parameters, making the results and the claim of red-teaming for \"Large Language Models\"less convincing."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper introduces a novel approach to training models that generate more diverse red team test cases. A diversity reward is introduced to achieve that, by encouraging the red team models to explore more diverse cases. The authors test different ways to define the rewards.\nResults on GPT2 models show that the approach can indeed increase the test case diversity while maintaining effectiveness."
            },
            "weaknesses": {
                "value": "The experiments chose GPT2 as the target model in the main results, making the results not too convincing. I recommend testing on a wider range of LLMs, including proprietary models like ChatGPT and open-sourced models like LLaMA-2-chat (the user has already done but there are not enough results and details) and Vicuna."
            },
            "questions": {
                "value": "1. Are the RoBERTa hate speeh classifier strong enough to detect the toxic generations?\n2. Why not try \"real\" LLMs like proprietary models like ChatGPT and open-sourced models like LLaMA-2-chat and Vicuna? I do not think the results are GPT2 with 137M parameters are convincing enough."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6325/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6325/Reviewer_NqgB",
                    "ICLR.cc/2024/Conference/Submission6325/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6325/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698560074214,
        "cdate": 1698560074214,
        "tmdate": 1700867994499,
        "mdate": 1700867994499,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HcFRu4IXX3",
        "forum": "4KqkizXgXU",
        "replyto": "4KqkizXgXU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6325/Reviewer_TuLd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6325/Reviewer_TuLd"
        ],
        "content": {
            "summary": {
                "value": "- The paper proposes an rl-based red teaming method that can explore novel test cases by adapting a curiosity-driven exploration technique. \n- The proposed method avoids redundant test cases by utilizing novelty reward during the rl optimization procedure. \n- The empirical results show that the proposed curiosity-driven red teaming method achieves superior performance in both the red team accuracy and the diversity compared to the baseline red teaming methods."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea is intuitive and easy to understand.\n- The writing is clear.\n- The authors provide experimental results with two diversity measures, 1-self-bleu and 1-cosine similarity.\n- The empirical results are amazing. The proposed method shows higher red teaming performance than the rl-based red teaming methods while maintaining a diversity score comparable to the zero-shot baseline method."
            },
            "weaknesses": {
                "value": "- Since the paper handles the problem of red-teaming which is possible to make ethical issues in society, I think it would be better to contain subsections for ethical comments in the red-teaming.\n- The empirical results are limited to the text-to-text generation models. \n- The proposed automated red-teaming method is based on the pre-trained offensiveness classifier. As [1] raised, there exists a risk of discovering test cases that over-fit the red team classifier, resulting in false positive test cases. But there isn't any analysis about this risk.\n\n[1] Query-Efficient Black-Box Red Teaming via Bayesian Optimization, Lee et al., ACL 2023."
            },
            "questions": {
                "value": "- Can you provide simple experimental results on text-to-image models like stable-diffusion?\n- Can you provide an analysis of the classifier overfitting problem? For example, you may provide a confusion matrix for each method. \n- Can you provide experimental results on large language models such as the text-davinci-003 model or gpt-3.5-turbo prompted chatbots?\n- It is just a curiosity question. In a realistic scenario, each query on the target model usually incurs costs. Hence, the number of model queries during the red teaming process can be an important factor for the overall cost of the method. Can you provide the number of queries used during the end-to-end process of RL+\"curiosity\"? Also, can you estimate the price when we red-team gpt4 api using RL +\"curiosity\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper has the potential to be maliciously utilized by attackers to make adversarial prompts for publicly released AI models. Hence, some ethical comments should be added to the paper."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6325/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6325/Reviewer_TuLd",
                    "ICLR.cc/2024/Conference/Submission6325/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6325/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806470701,
        "cdate": 1698806470701,
        "tmdate": 1700505306725,
        "mdate": 1700505306725,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "v6MPdFc6Jr",
        "forum": "4KqkizXgXU",
        "replyto": "4KqkizXgXU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6325/Reviewer_ZhdE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6325/Reviewer_ZhdE"
        ],
        "content": {
            "summary": {
                "value": "- The paper proposes to add an entropy bonus and novelty reward in addition to the standard RL objective to maximize toxicity in the target LLM\u2019s responses. Two variations of the novelty reward are proposed: A SelfBLEU novelty reward and a cosine similarity novelty reward.\n- The proposed method of red-teaming is evaluated in a text continuation task(evaluated on GPT2), an instruction following task (GPT2-alpaca and Dolly-v2-7B). The method is also evaluated on LLMs fine-tuned with human preference (LLaMA2-7b-chat-hf)\n- The authors conduct ablations on the various KL penalty coefficients, sampling temperature, and the necessity of each reward. Studies on the hyperparameter choices are shown in the appendix."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper tackles an important problem and contributions are clear and well presented.\n\n- The method (RL+Curiosity) is simple and sound, borrowing from known methods in the RL literature.\n\n- Experiments cover a good number of models, in a number of settings, and performance of RL+Curiosity is noticeably better than baselines.\n\n- Ablation studies alleviate concerns that test case diversity is not simply fixed by increasing sampling temperature or modifying the KL penalty."
            },
            "weaknesses": {
                "value": "- The authors choose an objective of maximizing test case novelty but do not show how their method influences target LLM response novelty.\n\n- In A.7, you mention you sample 100 sets of 100 test cases and calculate both diversity metrics across each subset. What is the test case size for a select number of thresholds in your experiments, and why is this sampling method used?\n\n- This is not too important but it is worth mentioning that the novelty of the method is somewhat limited. The cosine similarity reward is very similar to the RL+TDiv baseline, except it is applied to the test cases.\n\nIn general, the paper tackles a relevant problem and is well motivated and presented. I have some limited concerns (see above) and would like to hear the author's rebuttal, after which I may modify my score."
            },
            "questions": {
                "value": "- Could you clarify what number of test cases exceed each toxicity threshold for RL+Curiosity for the experiments in section 4.2 and 4.3?\n\n- How does your method compare with RL+TDiv if we look at the diversity of target LLM responses? I presume that both test case diversity and target LLM response diversity are important when red-teaming, and I would like to know whether RL-TDiv essentially achieves target LLM response diversity without necessarily going through the intermediate step of test case diversity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6325/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6325/Reviewer_ZhdE",
                    "ICLR.cc/2024/Conference/Submission6325/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6325/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699167091456,
        "cdate": 1699167091456,
        "tmdate": 1700522787942,
        "mdate": 1700522787942,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SrIYG3t5sV",
        "forum": "4KqkizXgXU",
        "replyto": "4KqkizXgXU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6325/Reviewer_8YjF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6325/Reviewer_8YjF"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method for automated red teaming. The method is a spiritual successor to those in the Red Teaming LMs with LMs paper from Perez et al, and should be viewed in that light (rather than in comparison to recent alternative approaches like ARCA and GCG). Specifically, the method adds a curiosity objective to the RL red teaming method of Perez et al."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper continues an interesting line of work in fine-tuning LLMs to be better at red teaming other LLMs. This type of research is complimentary with more recent optimization-based methods like GCG\n\n- The idea of incorporating curiosity into the RL red teaming method from Perez et al. is a good idea\n\n- The results are strong; the success rate of the red teaming method remains as high as the RL baseline, and the diversity is much higher\n\n- The ablations are reasonable and anticipate questions that readers would have"
            },
            "weaknesses": {
                "value": "- The distinction between adversarial attacks and red teaming seems artificial. I wouldn't want this distinction being introduced in the community. Surely adversarial attacks can be semantic, and surely red teaming can include gibberish adversarial examples as an interesting failure mode of LLMs.\n\n- I would suggest adding more visual separation between diversity and quality plots. Currently it's hard to tell which is which in Figures 1 and 2 without turning my head sideways to read the axis."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6325/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6325/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6325/Reviewer_8YjF"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6325/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699313688366,
        "cdate": 1699313688366,
        "tmdate": 1700707677334,
        "mdate": 1700707677334,
        "license": "CC BY 4.0",
        "version": 2
    }
]