[
    {
        "id": "5zDUjGsSC4",
        "forum": "VGLU5N1AD2",
        "replyto": "VGLU5N1AD2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4374/Reviewer_jHme"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4374/Reviewer_jHme"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduced an incentivized black-box model sharing framework that equitably distributes ensemble predictions and rewards parties based on their contributions. The authors (1) introduced a Weighted Ensemble Game to quantify the contribution of black-box models towards predictions; (2) derived a closed-form solution for fair reward allocation based on Weighted Ensemble Game and  Fair Replication Game; (3) theoretically proved that approximate individual rationality is satisfied. Finally, the authors also conduct numerical experiments on real-world data to confirm the efficacy of their method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall, this paper is well written and clearly addresses the three main questions that the authors proposed to address, each corresponding to (1) how to quantify the contributions made by each model, (2) how to ensure that each party receives a fair payment/reward and (3) how to ensure individual rationality is ensured. It also provides solid theoretical results for each of the aforementioned questions, accompanied by empirical evaluations. \n\nNonetheless, I am not an expert in the field of Black-Box Model Sharing and hence have limited expertise in evaluating the merit/weakness of this work."
            },
            "weaknesses": {
                "value": "See questions."
            },
            "questions": {
                "value": "(1) Could you provide one specific example that motivates why individual rationality is chosen as one of your key metrics? \n\n(2) Why do you consider Shapley fairness as your main fairness notion? Any other fairness notions that might fit into your framework?\n\n(3) In Sec 5 you suggested that \"We will later empirically show that the virtual regret $\\epsilon$ is not needed and the strict IR is satisfied\". Is this a purely empirical observation or do you believe stronger theoretical results can be established here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4374/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4374/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4374/Reviewer_jHme"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4374/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698387140011,
        "cdate": 1698387140011,
        "tmdate": 1699636410055,
        "mdate": 1699636410055,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0qXDDoYG0A",
        "forum": "VGLU5N1AD2",
        "replyto": "VGLU5N1AD2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4374/Reviewer_hnks"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4374/Reviewer_hnks"
        ],
        "content": {
            "summary": {
                "value": "The paper studies how to incentivize different agents to participate in black-box model sharing. \n\nMore specifically, given a set of points S, the host wants each agent to share their predictions on those points, and the host incentivizes them by giving the final ensemble predictions over these points (every agent's predictions are weighted by some weights beta), which can be used to get a new and hopefully improved model h'. The number of these additional points and the ensemble predictions on these points given to each agent is proportional to the contribution of the agent. They show a principled manner of how to measure contribution of each agent. Also, they show how to incentivize each agent to actually participate here: i.e. there's incentive for them to report their predictions because the new model h'trained with the addition of the points and ensemble predictions performs better than the previous model h. \n\nEach agent can make a payment to collect more of those points and their ensemble predictions. And the paper shows how to set up these payment values and reward values so as to guarantee some form of fairness (T1 on pg 5). \n\nThey also evaluate their approach on some datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-The main problem that they study is well-motivated, and the guarantees that they seek seem reasonable as well. It's nice that they can verify the theoretical claims in their experiments."
            },
            "weaknesses": {
                "value": "-My main complaint of the paper is that the overall presentation was pretty hard to follow, resulting in some confusion over few details of the paper.  For instance, I\u2019m a little confused about how the weights beta_{i,x}\u2019s are set if the true label for point x is unknown. See more detailed question below. And also, it seems that there\u2019s an assumption about the unique of the optimal ensemble weights. Anyway, I think it would be helpful to add more prose to improve the overall presentation of the paper; I think the valuation part in section 6 is not too surprising but can be used as a sanity check and be moved to the appendix, which will allow more room to add more prose throughout the paper."
            },
            "questions": {
                "value": "-The paper describes once how the ensemble weights are set in 4.1. However, here it\u2019s assumed the host actually knows the ground truth. So, is it just that in the very beginning where the host has access to a data set that\u2019s held off, the host asks the clients to participate and find these weights in the very beginning and use these weights going forward?  But more realistically, the host would want to query each party to provide predictions for points for which the true label is unknown. In those cases, how would want find these weights? Note that the way things are written, the weight beta_{i,x} is set differently for each point x, meaning one can\u2019t estimate these beta_{i,x} differently for each x, if the true label for that y is not known, but rather set a weight beta_i that\u2019s the same across all the points. This should still maintain proposition 1, as all the arguments are always averaged over the entire distribution D anyway. \n\n\n-I think there\u2019s an inherent assumption that the optimal weights beta\u2019s are unique. Consider a following example where every party has the same exact model h. Then, the ensemble model will be the same no matter how the weights beta\u2019s are set.  In this case because everyone has the same model, one should be rewarded the same reward, meaning the beta\u2019s should be uniform across every client. However, setting beta\u2019s such that it places all its weight on a single model is also an optimal solution, which results in only that client receiving all the rewards. I think this is not just an artifact of this toy example, but if the data that each client has is pretty homogenous and resulting in similar overall loss, this can be very possible (assuming that as I described above the weights should be chosen not over (party i, point x) but rather over just the parties)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4374/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4374/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4374/Reviewer_hnks"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4374/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698775701660,
        "cdate": 1698775701660,
        "tmdate": 1699636409974,
        "mdate": 1699636409974,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "J4xuBBaJu9",
        "forum": "VGLU5N1AD2",
        "replyto": "VGLU5N1AD2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4374/Reviewer_gZai"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4374/Reviewer_gZai"
        ],
        "content": {
            "summary": {
                "value": "* This paper proposes a theoretical framework for incentivized black-box model sharing, based on cooperative games.\n* On the first stage of interaction, each party $i\\\\in[n]$ trains a multiclass classifier $h_i(x)$ using distribution $\\\\mathcal{D}_i$, but are interested in maximizing performance on a different distribution $\\\\mathcal{D}$. \n* The trained classifiers are sent to a trusted party, and combined into an ensemble model $h_N(x)=\\\\sum_i \\\\beta_{i,x} h_i(x)$. The trusted party evaluates $h_N$ on a dataset $U\\\\sim\\\\mathcal{D}^T$ from the target distribution, and performance is translated into fair rewards $r_i$ for each party by the weighted ensemble game (WEG) mechanism.\n* The WEG mechanism is based on Shapley values of a fully-additive cooperative game. The contribution of the $i$-th party is assumed to be equal to the average ensemble weight of their predictor ($\\\\sum_{x\\\\in U} \\\\beta_{i,x}/T$).\n* On the second stage, each party is allowed to add $p_i$ monetary funds to increase their reward, and additional rewards $r_i^+$ and payments $p_i^+$ are distributed fairly by the fair replication game (FRG) mechanism, relying on Theorem 1.\n* Once the final reward values are set, rewards ($r_i+r_i^+$) are realized as iid samples from the set $\\\\{(x,h_N(x)\\\\}_{x \\\\in U}$, and offset payments $p_i-p_i^+$ are realized as monetary transfers.\n* Empirical evaluation is performed on MNIST, CIFAR-10 and SVHN, demonstrating accuracy gains in several settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Problem is well-motivated. Two-stage collaborative game structure is an interesting design approach.\n* Makes effort to support key assumptions (e.g for valuation functions).\n* Empirical evaluation supports claims and provides confidence bounds. Documented code is provided."
            },
            "weaknesses": {
                "value": "* Limitations of the proposed method are not discussed clearly.\n* Unclear applicability for practical ensemble methods: Average ensemble weight is uncorrelated with the objectives of the parties (Table 1), experiments are performed with an \"ideal method\" (Section 4.1).\n* Presentation is dense, and was hard for me to follow. Many remarks which were very helpful to my understanding only appeared in Appendix A."
            },
            "questions": {
                "value": "* Motivation: Under which conditions is the model incentive structure realistic, and the valuation assumption applicable? In the hospital example mentioned in Appendix A (Q2), it is reasonable to assume that every hospital has access to a data source $\\\\mathcal{D}_i$ based on their local population, however it doesn\u2019t seem intuitive to me that the hospital would desire a classifier that has good performance on a population $\\\\mathcal{D}$ which is different than their own, and common to all other hospitals. Can you clarify this example, or give a different practical example where assumptions intuitively hold?\n* How does the method perform under practical (non-ideal) ensemble methods?\n* Price of fairness: If I understand correctly, it seems that the overall welfare of the parties ($\\\\sum_i L_{\\\\mathcal{D}}(h_i)$) would be maximized by sharing all target-dataset data $\\\\{(x_t,h_N(x_t)\\\\}_{t=1}^T$ with all parties. What are the shortcomings of this approach? How does its welfare compare to the mechanism presented in the paper?\n* What is the relation between the objective $L_\\\\mathcal{D}(h_i)$ and the utility $u_i$ presented in Theorem 1? Also, is it possible to quantify the relation between payment and accuracy increase for a given problem instance?\n* Technical questions: What is the meaning of the notation $\\\\hat{L}_{\\\\mathcal{D}}(h,h_N)$ in Section 5.2? Is there an upper bound on the size of realized reward $T_i$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4374/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4374/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4374/Reviewer_gZai"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4374/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778238536,
        "cdate": 1698778238536,
        "tmdate": 1699636409890,
        "mdate": 1699636409890,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kqvKqXzZ66",
        "forum": "VGLU5N1AD2",
        "replyto": "VGLU5N1AD2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4374/Reviewer_1Wgz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4374/Reviewer_1Wgz"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a framework for model sharing across parties. In relation to prior work, this paper considers incentives, as well as parties only sharing their model (rather than data which can be sensitive). The framework distributes rewards in proportion to the contribution of each party, and also allows for payments between parties."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Tackles an important and practical problem of considering incentives in the context of model sharing\n- Model enforces desirable properties such as fairness and IR, and combines many practical considerations together\n- Analysis is thorough"
            },
            "weaknesses": {
                "value": "The main weakness is in the exposition - I was not able to understand the model. It seemed like the model and problem formulation were not comprehensively specified. The fact that there is an FAQ section on the model speaks to how the model is not completely clear. Here are my questions that I couldn\u2019t find answers to:\n- How should we compare prediction error to monetary payments to \"rewards\" (samples of ensemble predictions)? (Do they use the same unit of measurement?) \n- Relatedly, what is the formula for the utility of party i? \n- Payments can be made from one party to another. Does each party, decide on their own, how much to pay to each other party, or is this transfer also specified as part of the mechanism? Does each party have a budget?\n\nThe model has two main parts, as described in Figure 5. Can we simply de-couple these two stages and study each part separately, or are there interactions that require studying them together? Just studying one aspect would make the paper simpler and more clear."
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4374/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4374/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4374/Reviewer_1Wgz"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4374/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806548533,
        "cdate": 1698806548533,
        "tmdate": 1699636409808,
        "mdate": 1699636409808,
        "license": "CC BY 4.0",
        "version": 2
    }
]