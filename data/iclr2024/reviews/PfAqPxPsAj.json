[
    {
        "id": "HjXALpvTFn",
        "forum": "PfAqPxPsAj",
        "replyto": "PfAqPxPsAj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5778/Reviewer_fe34"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5778/Reviewer_fe34"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the challenge of learning a robotic policy to grasp objects based on natural language instructions. The authors introduced a Language-conditioned Equivariant Grasp (LEG) method that leverages CLIP features to align image and text observations and SO(2)-steerable kernels to improve sample efficiency. The effectiveness of LEG is tested on the Language-Grasp Benchmark, which consists of 10 varied language-driven grasping tasks. The method's efficiency and performance are also evaluated on an actual robot."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper leverages a general frame to leverage the symmetry of language-conditioned grasping.\n- The paper presented a dynamic kernel generator that maps language instructions to steerable kernels with rotation symmetry.\n- The paper proposed a new grasping benchmark with ten categories of language conditions and corresponding expert demonstrations.\n- The proposed methods show strong grasping performances on the proposed benchmark."
            },
            "weaknesses": {
                "value": "- The paper claims that the proposed inductive bias leads to high sample efficiency, but there is no direct evidence in the experiments.\n- Neural Descriptor Field [1] is an SE(3) equivariant grasping method with a similar design to the proposed method. Please cite and analyze the similarities and differences between this paper and [1].\n- A large body of recent papers (to name a few, [2-4]) in language-conditioned grasping are not included for comparison and analysis.\n- The writing of this paper is unclear and uncareful.\n    - The experiment section mentioned designing and obtaining a reward, suggesting that the proposed method is RL-based. However, there is no mention of reinforcement learning or any related formulation in the method section, except naming $p(a_t|o_t,l_t)$ as a policy once.\n    - The paper did not explain how the model is trained. If it is an RL-based method, what are the observations and rewards, the algorithm used, and what hyperparameters are used? If not, what is the loss function, and how is it trained?\n    - This paper has multiple typos; for example, at the end of page 3, there is a `\\leq`, which I believe should be `\\leg`.\n\n[1] Simeonov, Anthony, et al. \"Neural descriptor fields: Se (3)-equivariant object representations for manipulation.\"\u00a0*2022 International Conference on Robotics and Automation (ICRA)*. IEEE, 2022.\n\n[2] Xu, Yinzhen, et al. \"Unidexgrasp: Universal robotic dexterous grasping via learning diverse proposal generation and goal-conditioned policy.\"\u00a0*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2023.\n\n[3] Sharma, Satvik, et al. \"Language embedded radiance fields for zero-shot task-oriented grasping.\"\u00a0*7th Annual Conference on Robot Learning*. 2023.\n\n[4] Chen, Yiye, et al. \"A joint network for grasp detection conditioned on natural language commands.\"\u00a0*2021 IEEE International Conference on Robotics and Automation (ICRA)*. IEEE, 2021."
            },
            "questions": {
                "value": "- The introduction mentioned, \u201cdirectly interleaving language features with image features breaks the geometric symmetries underlying the optimal policy\u201d. Please elaborate on why interleaving features break symmetry.\n- How are the gripper position and orientation obtained from the output feature map?\n- The description of section 4.1 (For example, if there are 4 toys presented in the workspace, each successful grasp will be credited a reward of 0.25. The successful grasp is defined as the grasp lifting the object and satisfying the language goal. A maximum of n + 1 grasping trials is set for each task.) indicates that there are N objects in a scene, and the goal is to pick up each object individually, according to its language description. Only when all objects are picked up successfully can it obtain the full reward. Is my understanding correct?\n- How is the grasping-by-part success rate measured? Is it by manual inspection or by some automated function?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5778/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5778/Reviewer_fe34",
                    "ICLR.cc/2024/Conference/Submission5778/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5778/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698393758866,
        "cdate": 1698393758866,
        "tmdate": 1700641444158,
        "mdate": 1700641444158,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FUF9UWnTTk",
        "forum": "PfAqPxPsAj",
        "replyto": "PfAqPxPsAj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5778/Reviewer_n2g1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5778/Reviewer_n2g1"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new 2D equivariant grasp selection algorithm that's language conditioned. After discretizing output action to different buckets of angles, the paper propose to lifting 2D feature map to Fourier domain, in which conditioning embeddings like CLIP features are multiplied. The paper demonstrates the effectiveness of the method on simulated benchmarks as well as real robots. The paper also contributes a new benchmark on language conditioned grasps featuring 10 environments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The evaluation is solid by combining a variety of baselines, environments. It also features real-world grasping experiments featuring unknown objects. \n- Previous method achieves equivariance at global observation level (rotating entire observation) while this paper focuses on local symmetry. \n- Writing is clear and easy to understand"
            },
            "weaknesses": {
                "value": "- The language part and equivariant part are not directly related to equivariance itself. I think what the paper tries to claim is finding a better way to condition and ground constrained networks here. However, starting with language conditioning can mislead the readerss to focus on the wrong thing. \n- No equivariant baselines that simply takes in CLIP features by concatenation. If previous equivariant grasp models are not applicable here, please explain clearly why concatenation doesn't work."
            },
            "questions": {
                "value": "1. Do you have results for baselines in real-world experiments?\n2. I am wondering how would the performance change according to the number of grasp bins.\n3. In the writing, I hope the authors can state what's the input and output of the mapping that's equivariant to define the problem in a clearer way."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5778/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5778/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5778/Reviewer_n2g1"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5778/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831432596,
        "cdate": 1698831432596,
        "tmdate": 1699636607521,
        "mdate": 1699636607521,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nbs4an425x",
        "forum": "PfAqPxPsAj",
        "replyto": "PfAqPxPsAj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5778/Reviewer_vD3Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5778/Reviewer_vD3Z"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors propose a novel method to train language-conditioned grasping policies that are equivariant under the SE2 symmetry. To achieve this, the authors first divide the inputs into two encoders: One processes both language and RGB to obtain pixel-wise attention, and another part; Another one to expand language embedding to a steerable kernal which satisfies the equivariant constraints. Then the steerable kernal and the pixel-wise attention are \"fused\" together through cross correlation and inverse fourier transform to produce the action activations. The authors test the proposed method on diverse 2D pickup tasks and show that, their method can outperform naive baselines with only a few demonstrations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The proposed method is quite novel as it leverages the symmetry that exists in robotic problems. \n2) The method also demonstrate good 1-shot or few shot performances comparing with baselines."
            },
            "weaknesses": {
                "value": "1) It is rather strange to see that rotation is applied to the language embedding, but not the image space...Need an explanation on this.\n2) Also, CLIP language features may not the best representation of actions such as \"pick\", \"move\" etc. Since the policy is grasping only it may not impose a bottlneck as of now.\n3) Need some discussions on why the whole policy, not just the steerable kernel, is also equivariant under the SE2 symmetric group. \n4) SE2 also includes a translation part, but I don't see that the steerable kernel is equivariant under translation.\n5) In the paper it is not clear if there is a pre-training phase. It is hard to believe that the network weights (of \\phi, and psi) are properly learned with just one trajectory if they are always randomly initialized from the beginning."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5778/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699237543455,
        "cdate": 1699237543455,
        "tmdate": 1699636607375,
        "mdate": 1699636607375,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mr9RkSYNf6",
        "forum": "PfAqPxPsAj",
        "replyto": "PfAqPxPsAj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5778/Reviewer_C3bk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5778/Reviewer_C3bk"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a method called Language-conditioned Equivariant Grasp (LEG), which utilizes language instruction to guide robotic grasping. It presents a language-conditioned grasp network with a dynamic kernel generator to showcase the effectiveness of LEG on the Language-Grasp Benchmark that includes expert demonstrations. This benchmark comprises 10 language-conditioned grasping tasks in a simulated environment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method, Language-conditioned Equivariant Grasp (LEG), leverages the symmetries of language-conditioned robotic grasping by mapping the language instruction to a steerable kernel.\n- The authors analyze the symmetry of language-conditioned grasp and propose a general framework to leverage it.\n- The proposed method achieves high sample efficiency and grasping performance in both simulated and real-world robot experiments."
            },
            "weaknesses": {
                "value": "- The mainstream of grasping research is in SE(3), but this paper remains in SE(2).\nThis paper seems to have conducted implicit pose estimation or simply learned the rotation angles of objects. However, it appears to lack novelty.\n- Real-world experiments are limited to a single task (pick-by-part) and a small set of objects. Some of the experimental objects are not sufficient to demonstrate effectiveness, as they are mostly cylindrical in shape. As long as the object is between the grippers, closing the grippers will definitely be able to grasp it.\n- There are some obvious typos in the article, like the third line from the bottom of Section 4."
            },
            "questions": {
                "value": "Could you explain the reason for adding language embedding in the \u03c6 branch? It seems to be redundant and does not have significant impact on performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5778/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5778/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5778/Reviewer_C3bk"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5778/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699351118446,
        "cdate": 1699351118446,
        "tmdate": 1699636607278,
        "mdate": 1699636607278,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eRIaaES5ng",
        "forum": "PfAqPxPsAj",
        "replyto": "PfAqPxPsAj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5778/Reviewer_FTbR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5778/Reviewer_FTbR"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to tackle the problem of 2D grasping conditioned on language instructions. To be more specific, the language instructions are provided as grasping an object by the name, color or a specific part and they use behavior cloning to learn the grasping policy from language annotated demonstrations. Moreover, in order to incorporate language into the grasping policy in a way that exploits the symmetry and geometry in the task, the authors proposed a framework that maps the language instruction to a SO(2) steerable kernel. They demonstrate the effectiveness of their method on their proposed grasping benchmark in the simulation and on the real robot."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The author proposed a novel dynamic kernel that maps the language instructions and has provided theoretical guarantee for the rotation symmetry.  The paper also shows its strength of high sample efficiency and better grasping performance both in simulation and on real world. The author also proposed a grasping simulation benchmark annotated with language instructions that can be used for future work on 2d grasping."
            },
            "weaknesses": {
                "value": "The author fails to compare their method against some most recent work in language conditioned grasping. For example, this work[1] also has the task of semantic grasping(grasping some object by some particular parts). Moreover, this paper only focuses on 2D grasping with language instructions but some objects may not be graspable from just a 2d top-down grasp. Some work[2] have also shown their method on 4D grasping in cluttered scene with language instructions. \n\nIn the real world experiment, the tested objects and variations are very limited compared to the simulation. It is more convincing to show more objects in the real world experiments and potentially show the results of the different methods not just LEG-Unet in the real world.\n\n[1]Sundaresan, P., Belkhale, S., Sadigh, D., & Bohg, J. (2023). Kite: Keypoint-conditioned policies for semantic manipulation. arXiv preprint arXiv:2306.16605.\n\n[2]Tziafas, G., Yucheng, X. U., Goel, A., Kasaei, M., Li, Z., & Kasaei, H. (2023, August). Language-guided Robot Grasping: CLIP-based Referring Grasp Synthesis in Clutter. In 7th Annual Conference on Robot Learning."
            },
            "questions": {
                "value": "1. The author proposed a grasping benchmark in pybullet simulation with objects imported from YCB and GraspNet Dataset with language annotations. However, the paper doesn't compare the diversity and the complexity of the grasping objects in this grasping benchmark  with other grasping benchmark commonly used by the robotic grasping research. It would be more convinced to have a table with comparisons with other grasping dataset/benchmarks.\n2. The current language instructions used in the experiments are provided in the appendix. I am wondering if this method can generalize to other language instructions that have the similar meaning but don't exist in the training dataset or the language instructions that are in the free-form. \n3. For the experiment section, the authors mention that LEG-Unet is better than LEG-CLIPort on most tasks. Could the authors provide more explanation for this result?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5778/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5778/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5778/Reviewer_FTbR"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5778/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699593491351,
        "cdate": 1699593491351,
        "tmdate": 1699636607176,
        "mdate": 1699636607176,
        "license": "CC BY 4.0",
        "version": 2
    }
]