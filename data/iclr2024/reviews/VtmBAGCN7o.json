[
    {
        "id": "a2Wdn7SSsA",
        "forum": "VtmBAGCN7o",
        "replyto": "VtmBAGCN7o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5488/Reviewer_uj5i"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5488/Reviewer_uj5i"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces MetaGPT, an innovative meta-programming framework for multi-agent collaborations based on LLM, which encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows. It selects a group of agents as a simulated software company, to generate a variety of code-based softwares. Through extensive experiments, MetaGPT achieves state-of-art performance on multiple code benchmarks HumanEva, MBPP and a software development benchmark SoftwareDev."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of encodes SOPs of software development into LLM-based multi-agent systems is very interesting and also pracitical to use.\n2. The framework is very sound and solid, with Specialization of Roles, PRD workflow across Agents, Structured Communication for complex tasks, and a compute-efficient Message Pool mechanism with both global memory and Subscription Mechanism. It also introduces an executive feedback mechanism to enhance code generation quality during runtime. \n3. MetaGPT achieves state-of-art performance on multiple benchmarks such as HumanEva, MBPP and a software development benchmark SoftwareDev. It opens new possibility for the software development paradigm."
            },
            "weaknesses": {
                "value": "1. Most of the experiment are on GPT4, which is expensive to access, how is the performance on the benchmarks or real development demands when used with open-source LLMs? Can you share some insight on which abilities of the LLMs matters most for the success of using this multi-agent framework and how to choose proper LLMs for use?"
            },
            "questions": {
                "value": "1. Does the framework have strong generalizability when switched to GPT 3.5 or other open-source models? how far can the framework or practical use go with open-source models?\n2. In table 3, In this multi-agent collaboration with different roles, do you have some qualitive analysis or case study to show where each role contributes to the final performance, except for an executability score? Can you provide more detailed analysis?\n3. Whether the framework can also solve problems other than code, such as math and QA in Autogen? Is there other challenges for these kinds of problems for the framework?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5488/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723559148,
        "cdate": 1698723559148,
        "tmdate": 1699636560713,
        "mdate": 1699636560713,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qUr42RO1IH",
        "forum": "VtmBAGCN7o",
        "replyto": "VtmBAGCN7o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5488/Reviewer_itRX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5488/Reviewer_itRX"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces MetaGPT, a meta-programming framework designed to enhance the efficiency and accuracy of large language model (LLM)-based multi-agent systems. By incorporating Standardized Operating Procedures (SOPs) into prompt sequences and adopting an assembly line approach, MetaGPT enables agents to specialize in diverse roles and collaboratively solve complex tasks with reduced errors. \n\nI believe this paper is interesting and makes a contribution by introducing an instrumental tool for automated code generation. I have several questions that I believe need to be addressed before publication. Aside from the questions below, my biggest concern is the relevance of the paper to Learning Representations. This paper mostly seems to be a nice fit for a journal focused on systems and applications.\n\n- It appears that this system needs existing expertise and strategy, correct? How primal/high-level the initial instructions can be? For instance, is a novice user going to have a hard time using this because they don\u2019t have enough knowledge about breaking an initial task into sub-steps?\n\n- Does the level of initial input from human affect the performance? For instance, if the human gives a very high-level input vs a more detailed, structured input instruction?\n\n- The human user side of the system is never discussed, although I believe it\u2019s very critical to understand the requirements, skills, knowledge, etc. the system imposes on or requires from a human user. The initial task or input instructions from the user are also never discussed.\n\n- The paper uses the term collaborative multi-agent systems, but later mentions that they define multi-agent frameworks as LLM-based multi-agent systems. I think the term collaborative multi-agent systems is very broad and comprehensive and it could be misleading to readers. The \u2018LLM-based\u2019 attribute of the multi-agent systems must be reflected in Title. Also, this leads to some misleading sentences such as \u201cMost of current multi-agent frameworks utilize natural language as a communication interface.\u201d Without using the term LLM-based multi-agent frameworks, this sentence is incorrect.\n\n- Is the shared communication/message pool interpretable to humans? Is it accessible by humans during or after the process?\n\n- Would the system ever need human interaction, or further input from human to address its potential questions?\n\n- There are multiple agents introduced in the system each with different roles, however, it is never discussed how are these specialized agents built or trained, algorithmically. Are they using specifically trained models?\n\n- Can there be multiple agents with the same role? i.e., multiple engineers that would work on different parts of implementations in parallel? How\u2019s the parallelization process performed? For example, if multiple engineers will be working different parts of the problem designated by the architect, how will they choose which part to work on? Is there priority assigned, or is there a decision-making problem being solved? Sometimes the order by which a problem is solved could significantly affect its efficiency.\n\n- What are some of the limitations of the system at its current stage? Limitations, both in the system and on the human user side must be discussed.\n\nAt current states I vote weak reject (mostly due to relevance), although the system seems to be sound and working. I need to see more discussions and revisions as suggested above, as well as suggested by my fellow reviewers to make this an ICLR-ready paper. I\u2019d be happy to increase my score when authors satisfactorily addressed the questions and comments."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "See above."
            },
            "weaknesses": {
                "value": "See above."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5488/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5488/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5488/Reviewer_itRX"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5488/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742009299,
        "cdate": 1698742009299,
        "tmdate": 1700503750191,
        "mdate": 1700503750191,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "haoCfWsayk",
        "forum": "VtmBAGCN7o",
        "replyto": "VtmBAGCN7o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5488/Reviewer_WkUQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5488/Reviewer_WkUQ"
        ],
        "content": {
            "summary": {
                "value": "MetaGPT is a framework designed for programming in multi-agent collaborative environments, specifically utilizing the Large Language Model (LLM). Its main contribution is the integration of human workflows into LLM-based multi-agent collaboration, which is effective for complex software engineering tasks. The framework assigns different roles to the GPT model, similar to a pipeline in software development, including roles such as product manager, architect, and engineer. Its main advantage lies in generating coherent solutions and decomposing complex tasks through the collaboration of multiple AI agents."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Problem-solving: MetaGPT can solve complex tasks by using different AI agents to handle different parts of the problem.\n\n2. Clear solutions: MetaGPT provides sensible solutions that favor tasks that require different elements to work together seamlessly.\n\n3. Flexibility: MetaGPT can be used for different tasks, and provide multiple outputs ranging from project plans to technical documentation."
            },
            "weaknesses": {
                "value": "1. Methodological innovation: A key issue with MetaGPT is the lack of innovation in its methodology. While it effectively utilizes Large Language Models (LLMs) in multi-agent systems, this approach may not be significantly different from existing approaches. Or MetaGPT is different from other methods in a trivial way, I don't really see their differences as being significant and requiring exploration with greater depth.\n\n2. Fairness of experimental comparisons: The comparison methods used to assess the effectiveness of MetaGPT do not appear to be fully equivalent to what MetaGPT offers. Such differences may lead to biased or misleading conclusions about their superiority or efficiency.\n\n\n3. Experimental validation of statements: Current experiments may not adequately validate the authors' claims about the efficiency and robustness of MetaGPT."
            },
            "questions": {
                "value": "1. My primary concern is that MetaGPT lacks methodological and theoretical innovation. To gain a more innovative edge, MetaGPT could deeply explore what deep-seated challenges in integrating newer AI techniques or unique collaboration strategies make it more clearly distinguishable from other LLM-based frameworks. (Honestly, I think MetaGPT is more suited for system demonstration conferences or tracks)\n\n2. In the Introduction, \"MetaGPT stands out as a special solution that allows efficient meta-programming through a well-organized and specialized group of agents\". How does the author define these professional concepts? For example, \"efficient\" and \"well-organized\".1) I haven't seen any evidence or convincing analysis of why it can be \"efficient\", nor have I seen experimental validation.2) How do you define \"specialized group\"? This seems trivial and can be varied.\n\n3. In the Introduction, \"MetaGPT achieves a 100% task completion rate, further demonstrating the robustness and efficiency of our design\", and \"Our innovative integration of human-like SOPs throughout MetaGPT\u2019s design significantly enhances its robustness\". 1) What is the definition of \"robustness\" here? I didn't see any discussion of \"robustness\" except in the Introduction section. 2) What are the deeper methodological challenges of integrating human-like SOPs?\n\n4. In the Related Work, \"In this paper, we identify the main challenges in multi-agent cooperation: maintaining consistency, avoiding unproductive cycles, and controlling beneficial interactions\". There seems to be inconsistency in the description of the research challenges between the \"Related Work\", the \"Introduction\" and the rest of the paper. Furthermore, is it trivial to maintain consistency in MetaGPT?\" What are the professional definitions of \"unproductive cycle\" and \"beneficial\"? Are there experimental results to validate these advantages?\n\n5. In the Experiments, \"We compare recent domain-specific LLMs in the code generation field ...\" Is it fair to compare with these LLMs?\n\n6. In the Experiments, \"We modified certain role-based prompts in order to instruct the MetaGPT framework to generate individual functions instead of entire classes\" 1) Is it fair to compare with these frameworks? LangChain and AutoGPT, for example, which are not specific to the tasks involved in this commit. 2) How can we ensure that \" prompt modification\" does not lead to unfair comparisons?\n\n7. Figure 4. What are the results of multiple attempts and are they stable? Were there any problems, like robustness?\n\n8. In Section 4.2, what is \"the cost of human revision\" and how can it be rigorously measured?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This submission does not discuss any potential ethical issues, which I think is uncritical, especially for generative AI technologies. There are potential privacy, security, and bias issues that accompany the collection, processing, and use of data, exchange between agents, communication, and so on, in open-ended tasks with multi-person collaboration. For example, I perceive at least the following concerns and considerations.\n\n1. Unemployment and skill obsolescence: Automation of complex tasks in software engineering and other fields may lead to job losses. Professionals in these fields may need to adapt and acquire new skills to remain relevant. How to manage and communicate these changes is an ethical issue.\n\n2. Transparency and accountability: As AI takes on more complex collaborative tasks, it becomes critical to maintain transparency in decision-making. It should be clear how MetaGPT arrives at solutions and who is accountable for those decisions, especially in critical applications.\n\n3. Privacy and data security: The use of large-scale language models and AI in processing potentially sensitive information raises concerns about data privacy and security. Ensuring that user data is protected and used ethically is a key consideration."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5488/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5488/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5488/Reviewer_WkUQ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5488/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699739370167,
        "cdate": 1699739370167,
        "tmdate": 1699739370167,
        "mdate": 1699739370167,
        "license": "CC BY 4.0",
        "version": 2
    }
]