[
    {
        "id": "0flBwcNw4p",
        "forum": "AnuHbhwv9Q",
        "replyto": "AnuHbhwv9Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8365/Reviewer_FNwT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8365/Reviewer_FNwT"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the problem of regression generalization to out of sample distributions. This is an important problem as many models suffer from poor performance when applied to different samples. The authors focus on the case of linear regression with fixed covariate shift and derive a clean decomposition of the out of sample error in terms of the model primitives. Through the decomposition the authors identify a cause of generalization error which they coin Spectral Inflation. Spectral Inflation occurs when the training set and evaluation set are misaligned in the sense that the dimensions along which most of the variation is explained do not coincide. \n\nThe paper proposes a novel post-processing algorithm that projects the OLS solution to a subspace that is well aligned with the evaluation set. The authors offer theoretical guarantees on how to choose the projection subspace given the data and researcher chosen hyper-parameters. Finally, they show the performance of the proposed method in a simulation exercise and across various empirical applications.\n\nOverall the paper is very well written and I enjoyed reading it a lot."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper tackles and important problem by considering a clean and simple setting. I found this very useful as it helps highlight the crux of the problem and offer an intuitive solution. Furthermore, the paper is well written, the exposition is good and the theoretical results are clean and technically correct.\n\n* In terms of relevance, the decomposition results are not very surprising, but they do offer a straightforward and intuitive way to think of the generalization error in the context of linear regression. Researchers may find them useful as a framework for OOD error in these settings.\n\n* The algorithm proposed is intuitive and the theory developed offers an interesting way of thinking why it is a good algorithm under different potential underlying models. \n\n* The simulations and empirical examples are careful and offer a wide range of settings in which the method performs well."
            },
            "weaknesses": {
                "value": "* While the theoretical results are clean and correct, I wonder if the authors have solved the problem they set to solve rather than just offer one solution that is weakly better than OLS. I expand on this in the questions.\n\n* While the proposed method performs well through the simulations and empirical tests it is unclear if it is better than other methods. For example, in Table 2 and 3 accounting for the errors it does not seem to be statistically different from the other methods (for example ERM in Table 2). The authors also do not compare it in simulations with alternative methods besides simple OLS. For instance, it seems that PCR might perform well in this setting."
            },
            "questions": {
                "value": "* In the decomposition results, what is the expectation being taken over? I thought that X and Z are treated as random, but in the proof in page 13 in the appendix it seems that X and Z are fixed. Are X and Z random or fixed? Are the decomposition results conditional on X and Z?\n\n* Theorem 3 states that S^* is weakly better than the OLS solution, not that it is the best solution amongst all possible S. However in the paragraph above it is stated that it is the ideal set. Is it the case that S^* is the projection set that minimizes the expected loss amongst all possible S sets? It may be trivial, but it would be worth it fully explaining this in the main body of the text. If S^* is the the projection set that minimizes the expected loss then it should be stated as a theorem, if not then you should explain why you focus on S^* rather than the minimizing S. \n\n* Can you use the plug in variance estimator to estimate the variance on the same data? Why is a sample split not necessary? (this could be trivial given the assumptions) \n\n* How do you choose the hyperparameter alpha? Is there a data driven way to choose it or an optimal way of choosing it? \n\n* It would be useful to decompose the MSE into bias and variance in the simulations to check that Spar indeed is trading off bias and variance as described by the theory in relation to OLS. \n\n* What would change if Z was noisy? If we assume conditionally independent errors like for X, the conditional expectation would still be the same so it seems that most of the theory would go through with little changes (and the additional Z variance)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8365/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8365/Reviewer_FNwT",
                    "ICLR.cc/2024/Conference/Submission8365/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8365/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698540333370,
        "cdate": 1698540333370,
        "tmdate": 1700665274123,
        "mdate": 1700665274123,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iMG1CW7vZJ",
        "forum": "AnuHbhwv9Q",
        "replyto": "AnuHbhwv9Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8365/Reviewer_11sP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8365/Reviewer_11sP"
        ],
        "content": {
            "summary": {
                "value": "The authors study how deep regression models can be adapted to perform better under covariate shifts.\n\nThey do a detailed theoretical analysis of the ordinary least squares method and how it is affected by covariate shifts. Motivated by these findings, they propose a post-hoc method that can be used to update the final layer of pre-trained deep regression models, utilizing unlabeled data from the target distribution.\n\nThe proposed method is evaluated on three real-world regression datasets, two tabular datasets and one image-based. The regression performance is compared to that of standard training and C-Mixup (with or without the proposed final layer update)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I agree with the authors that out-of-distribution generalization specifically for _regression_ problems is relatively unexplored. Thus, the problem studied in this paper is definitely interesting and important.\n\nThe paper is well written overall, and the authors definitely seem knowledgeable.\n\nAlthough I did not entirely follow all parts of the theoretical analysis in Section 3, I did find it quite interesting. Especially Figure 2. The resulting proposed method then also makes some intuitive sense overall."
            },
            "weaknesses": {
                "value": "I found it quite difficult to follow parts of Section 3, especially Section 3.4.\n\nThe experimental evaluation could be more extensive. The proposed method is applied just to three real-world datasets, of which two are tabular datasets where small networks with a single hidden layer are used.\n\nThe experimental results are not overly impressive/convincing. The gains of ERM+SpAR compared to the ERM baseline in Figure 3, Table 2 and Table 3 seem fairly small.\n\nThe computational cost of the proposed method (Algorithm 1) is not discussed in the main paper, and only briefly mentioned in the Appendix."
            },
            "questions": {
                "value": "1. Could the discussion of the computational cost be expanded and moved to the main paper? How does the cost of Algorithm 1 scale if X and/or Z contains a large number of examples? How about the memory requirements?\n\n2. Could you evaluate the proposed method on at least on more image-based regression dataset? (one of the datasets in _\"How Reliable is Your Regression Model\u2019s Uncertainty Under Real-World Distribution Shifts?\"_ (TMLR 2023) could perhaps be used, for example?)\n\n3. The results in Table 3 seem odd, do all other baseline methods really degrade the regression performance compared to ERM?\n\n4. Can you please discuss the results in Figure 3, Table 2 and Table 3 a bit more, the gains of ERM+SpAR compared to ERM seems quite small? Does the proposed method actually improve the performance of ERM in a significant way?\n\n\nMinor things:\n- Section 3.4, last paragraph: \"the the variance\" typo.\n- I would consider modifying the appearance of Table 1 - 3, removing some horizontal lines."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8365/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8365/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8365/Reviewer_11sP"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8365/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828502694,
        "cdate": 1698828502694,
        "tmdate": 1699637040175,
        "mdate": 1699637040175,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wxZ0tlj3Zv",
        "forum": "AnuHbhwv9Q",
        "replyto": "AnuHbhwv9Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8365/Reviewer_TGia"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8365/Reviewer_TGia"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce a novel post-processing technique to address the unsupervised domain adaptation challenge under the premise of covariate shift. This method stems from an intricate analysis of Ordinary Least Squares (OLS). The authors delve into the theoretical examination of the OLS loss in the context of covariate shift, leading to a proposal to project the estimator into a distinct subspace. The authors contend that selecting this subspace based on a comparative analysis of loss with respect to bias and variance across eigenvectors ensures enhanced performance on the target distribution. Empirical evaluations on multiple datasets substantiate the efficacy of the proposed approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper offers a rigor analysis of OLS in the presence of covariate shift. The proposed projection technique is sound, and the decomposition of the loss function is notably interesting.\n2. The estimation strategy derived from finite samples is also sound.\n3. The paper is generally well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. The paper sheds light on an interesting post-processing technique, aligning the linear layer from the source to the target domain. However, in the realm of deep learning, adapting the representational function across both domains is crucial. It raises the question: Can the proposed technique outperform other domain adaptation methods that also focus on refining the representation function? If such outperformance is challenging, is it feasible for the proposed post-processing technique to boost the performance of existing domain adaptation methods?\n2. Stemming from the aforementioned concern, it would be enlightening to see the proposed method compared with a broader spectrum of domain adaptation baselines in Table 3 for the CommunitiesAndCrime and Skillcraft datasets. Moreover, an exploration of the combination of the proposed method and these baselines could be insightful.\n3. For the PovertyMap-WILDS dataset, the setting aligns more with an out-of-distribution generalization task rather than traditional domain adaptation. Hence, it may be more judicious to include OOD methods for comparison. Furthermore, the performance enhancement attributed to the proposed method on this dataset seems marginal since the variance in performance exceeds the difference between the proposed method and the top-performing baseline."
            },
            "questions": {
                "value": "See the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8365/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8365/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8365/Reviewer_TGia"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8365/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698842317142,
        "cdate": 1698842317142,
        "tmdate": 1700582068187,
        "mdate": 1700582068187,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IB9uGOkuoQ",
        "forum": "AnuHbhwv9Q",
        "replyto": "AnuHbhwv9Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8365/Reviewer_ZgCX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8365/Reviewer_ZgCX"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the problem of out-of-distribution generalization for regression models. The authors first analyze the sensitivity of the Ordinary Least Squares (OLS) regression model to covariate shift and characterize its out-of-distribution risk. Then they use this analysis to propose a lightweight spectral adaptation procedure(called Spectral Adapted Regressor) for the last layer of a pre-trained neural regression model. The paper demonstrates the effectiveness of this method on synthetic and real-world datasets, and it works well with data enhancement techniques such as C-Mixup."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Although there has been extensive research on distribution shifts in classification tasks, the authors focus on regression models which is a relatively unexplored problem of out-of-distribution generalization. The authors provide a novel analysis of the sensitivity of the Ordinary Least Squares (OLS) regression model to covariate shift. And they propose a spectral adaptation procedure specifically tailored for regression models, this adds to the originality of the paper.\n2. The authors provide a thorough analysis of the OLS regression model's out-of-distribution risk, utilizing the spectrum decomposition of the source and target data.\n3.  The paper provides a concise abstract that outlines the problem, methodology, and results."
            },
            "weaknesses": {
                "value": "1. The proposed method assumes access to unlabeled test data for estimating the subspaces with spectral inflation. However, in practical scenarios, obtaining unlabeled test data may not always be feasible. It would be beneficial to explore alternative approaches or modifications to the method that do not rely on unlabeled test data.\n2. The compared methods are limited to me. The persuasiveness of the proposed approach would be stronger if more comparative data could be provided."
            },
            "questions": {
                "value": "How computationally efficient is SpAR? The paper does not provide a detailed analysis of the computational efficiency of the proposed method. Considering the increasing complexity and size of neural regression models, it is important to assess the computational cost of the spectral adaptation procedure."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8365/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698854078000,
        "cdate": 1698854078000,
        "tmdate": 1699637039949,
        "mdate": 1699637039949,
        "license": "CC BY 4.0",
        "version": 2
    }
]