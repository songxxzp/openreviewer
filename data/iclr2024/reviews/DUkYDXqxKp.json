[
    {
        "id": "3t1U28SNP9",
        "forum": "DUkYDXqxKp",
        "replyto": "DUkYDXqxKp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7421/Reviewer_2Geq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7421/Reviewer_2Geq"
        ],
        "content": {
            "summary": {
                "value": "Despite the rapid progress of autonomous driving, these systems do not interact with human in natural language and the dominant approach decomposes the problem into perception, prediction and control. Depart from these approaches, this paper presents DriveGPT4, an interpretable end-to-end autonomous driving system leveraging large language models. DriveGPT4 takes as input a video sequence captured by a front-view RGB camera, along with the vehicle\u2019s historical control signals. It predicts the control signal for the next step and can provide natural language responses, such as describing the vehicle\u2019s actions and explaining the reasoning behind its behavior. \n\nTo train DriveGPT4 to communicate with natural language, the paper follows LLaVA (Liu et al., 2023) and creates a visual instruction tuning dataset based on the BDD-X dataset (Kim et al., 2018) using ChatGPT. The model DriveGPT4 is based on Valley(Luoetal.,2023) and fine-tuned on the created dataset. The model is mainly compared with ADAPT (Jin et al., 2023), Action-aware driving caption transformer."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Leveraging large language model LLaMA 2, the paper develops a multi-modal action model which takes video, text, historical control signals as input and outputs the control signals for the next step and can respond in natural language to explain driving actions and behavior.\n\n2. It creates a visual instruction tuning dataset based on the BDD-X dataset (Kim et al., 2018) using ChatGPT. \n\n3. The model is compared with ADAPT (Jin et al., 2023) on question answering and control signal prediction tasks."
            },
            "weaknesses": {
                "value": "1. The model relies on behavior cloning for end-to-end driving. This is the first end-to-end method tried out. The limitation is very well-known, e.g. can not handle distribution drift. For more information, please see\n\nEnd-to-end Autonomous Driving: Challenges and Frontiers\nLi Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, Hongyang Li\n\n2. The paper relies on ChatGPT to evaluate on vehicle action description, action justification, question and answering. ChatGPT can exhibit  well-known bias such as position bias, style bias. Given the data is generated by ChatGPT and the baseline ADAPT does not use ChatGPT, ChatGPT evaluation could be very well biased in favor of DriveGPT4 simply because it prefers its own style.\n\n3. The action prediction task should be evaluated with strong end-to-end autonomous driving baselines."
            },
            "questions": {
                "value": "I have a number of additional questions.\n\n1. For architectural choices, why not just fine-tune Video-LLaMA with the visual instruction tuning dataset based on the BDD-X dataset? Do you employ position encoding for video tokens?\n\n2.  In ablation study, it will be great to see the contribution of global features by removing F0^G \u2295 F1^G \u2295 ... \u2295 FN^G.\n\n3. The paper uses Yolo-8 for object detection, I wonder if the authors can comment on whether open-set detectors like grounding DINO could be better.\n\n4. The training stages, methods, datasets, and tasks are scattered. It would help if they can be summarized in a table. Furthermore, the architecture figure should have information on the training stages, which part is frozen or trainable.\n\n5. DriveGPT4 dataset is based on BDD-X dataset. DriveLLM2023 (https://github.com/OpenDriveLab/DriveLM) dataset augments NuScene with QA + Scene Description from the perspective of perception, prediction and planning with Logic. Can the model be evaluated on this dataset as well? Specifically how well DriveGPT4 performs on high level planning decisions compared with DriveLLM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Since this is a research prototype, there is not much ethics concerns."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7421/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698525408173,
        "cdate": 1698525408173,
        "tmdate": 1699636890311,
        "mdate": 1699636890311,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HO9RR3kL8c",
        "forum": "DUkYDXqxKp",
        "replyto": "DUkYDXqxKp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7421/Reviewer_EsLd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7421/Reviewer_EsLd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a multimodal LLM-based interpretable end-to-end autonomous driving system. Based on the BDD-X dataset, this paper constructs an instruction-tuning dataset comprised of fixed-form QAs and free-form QAs & conversations with ChatGPT. The model is comprised of a video tokenizer to extract video input features and an LLM to process the multi-modal inputs and make textual responses. The model also predicts control signals in the text format. The training has an alignment stage and fine-tuning stage similar to other multi-modal LLMs. The experiments evaluate the interpretability on action description&justification and QAs tasks and the end-to-end control ability on speed and turning angle prediction, which shows the superiority of the proposed model."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper constructs an instruction dataset for AD and shares the pipeline for its construction, which benefits the community for future research.\n2. This work shows promising results in letting multi-modal LLMs understand autonomous driving scenarios.\n3. The presentation is clear and easy to follow."
            },
            "weaknesses": {
                "value": "1. As the model is an end-to-end autonomous driving model, the ability to make driving plans or control predictions is critical. However, from the model design and the ablation experiments, it seems that the model might just simply extrapolate the input control signals. The authors should also include more possible baselines for this task (for example, a simple transformer with the same input as the proposed model which is directly trained on the prediction task). Besides, only predicting the speed and steer angles might not be enough to claim it as an end-to-end autonomous driving model.\n2. Although the paper claims that it is interpretable end-to-end autonomous driving, there are no explicit constraints between the model's textual explanation and its predicted action during training. And there are no experiments to validate this as well.\n3. The paper claims that the proposed model still has the general multi-modal conversation ability. Then the author should evaluate it on the general multi-modal benchmarks to validate this.\n4. For the experiments about interpretability, the proposed method shows consistently inferior performance in terms of standard metrics. It is true that the classic metrics might have certain problems, but so does the ChatGPT-based evaluation. As the training data is partially generated by ChatGPT, ChatGPT might prefer the responses similar to its own generated ones during evaluation."
            },
            "questions": {
                "value": "Why do you think removing the historical control signals as inputs makes the performance much worse if the model really learns the driving policy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7421/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7421/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7421/Reviewer_EsLd"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7421/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698565068248,
        "cdate": 1698565068248,
        "tmdate": 1699636890186,
        "mdate": 1699636890186,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QnHJq5Xybv",
        "forum": "DUkYDXqxKp",
        "replyto": "DUkYDXqxKp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7421/Reviewer_yQKg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7421/Reviewer_yQKg"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to apply large language models in the end-to-end autonomous driving domain. First of all, it constructs a question-answering dataset based on the BDD-X dataset. By providing ChatGPT with fixed question-answer pairs, and more privileged information from BDD-X, it generates more conversations, and descriptions about the scene or the reasoning process. The proposed DriveGPT4 model takes video as input, tokenized videos, questions and past control signals, and then finetunes LLaMA 2 to decode the required answers and control signals. The training process also involves image-text pairs from other domains to facilitate out-of-domain questions. The experiments are mainly compared with ADAPT on QA and action prediction metrics."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is basically clearly written and easy to understand.\n- The authors demonstrate that the LLM-based method has the zero-shot generalization ability to other datasets. It is a good point to involve LLM or large-scale training for images and videos."
            },
            "weaknesses": {
                "value": "Multiple overclaims and issues about the soundness.\n\n- *Important:* There are multiple sentences saying that \"constrained by the limited capacity of smaller language models, they can only address predefined human questions and provide inflexible answers, hindering their widespread application in real-world scenarios\". The reviewer admits that LLMs can answer out-of-domain questions, but is still wondering if the limitation lies in the LLM. I will explain this point based on the experiments.\n  - The improvement of the fixed question-answering experiment is limited (82.1->82.4, especially considering this is evaluated by ChatGPT with great uncertainty). Meanwhile, LLMs take much longer inference time to output the much longer answers compared to small models and the original QAs. The computational complexity is not that important at the current stage but is still valuable for illustration.\n  - The additional question-answering experiment demonstrates the effectiveness of DriveGPT4 which is finetuned on the generated data. Then how about fintuning ADAPT and other works on the data? The direct comparison seems unfair.\n  - For the control prediction task, DriveGPT4 takes historical control signals as input while others do not. It can be observed from the ablation study, that the results of 'No control signals' are close to ADAPT-32. The contribution comes from the history information, rather than LLM, which is unfair.\n  - It is an open question if flexible answers can help real-world applications of autonomous driving.\n- It is not the **first** work on interpretable end-to-end autonomous driving. The authors have already listed several in the related works. Even language-based interpretability is not accurate as there are multiple types of interpretability. In my opinion, it is fair to say LLM-based. \n- From the very beginning, is **interpretability** indeed the unsolved problem that hinders the commercialization and development of autonomous driving? As described in the first paragraph of the introduction, the popular modular-based methods' issue is not the interpretability.\n\nTechnical contribution is limited. There are contributions of instruction tuning for the dataset generation. The video tokenization part is based on Valley and the action part is very close to existing works such as RT-2."
            },
            "questions": {
                "value": "- Why not use ground truth from the BDD dataset? The detection results from YOLOv8 could be inaccurate. I am wondering if there is a truck in the provided figure (Table 1).\n- How bounding box coordinates are normalized? In which coordinates are they normalized?\n- If the model can generalize to video games, it is worth trying generalizing to CARLA and evaluating it in a closed-loop manner.\n- The short name 'DriveGPT4' is not very appropriate, as the core method is finetuned from LLaMA 2 and does not use GPT4."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7421/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698652656907,
        "cdate": 1698652656907,
        "tmdate": 1699636890065,
        "mdate": 1699636890065,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eu2SMfivk5",
        "forum": "DUkYDXqxKp",
        "replyto": "DUkYDXqxKp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7421/Reviewer_u2wu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7421/Reviewer_u2wu"
        ],
        "content": {
            "summary": {
                "value": "In the paper, the authors made a contribution by introducing a new image-and-language dataset derived from the BDD-X dataset and enriched using ChatGPT. This customized visual instruction tuning dataset is specifically designed for the application of large language models (LLMs) in autonomous driving. Their system, DriveGPT4, uses this dataset for fine-tuning, serving as a baseline in the field. Notably, DriveGPT4 demonstrates good zero-shot generalization capabilities, akin to the performance metrics observed with ChatGPT. This research offers a new focus on achieving interpretability in end-to-end autonomous driving systems through the use of LLMs. The DriveGPT4 can process multi-modal input data and provide text responses as well as predicted control signals.\n\nThe key contributions are 1) a new vision-language dataset for autonomous vehicle; 2) a new chatGPT style system trained on the new dataset; 3) such chatGPT style system DriveGPT4 performs better than alternative baselines on the same dataset, can produce something that ChatGPT variant (GPT-4v) can do. I recognize that GPT-4v was not available at the time when this paper was done."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper breaks new ground by applying large language models (LLMs) like ChatGPT (GPT-4) to the domain of autonomous driving. It offers an invaluable resource in the form of a customized visual instruction tuning dataset, setting a robust baseline for future research that aims to incorporate LLMs into autonomous systems.\n\n2. One of the standout features of the proposed DriveGPT4 system is its impressive zero-shot generalization capabilities. This ability to adapt to previously unseen scenarios mirrors the robustness and flexibility observed in ChatGPT, making it a compelling advance in the field."
            },
            "weaknesses": {
                "value": "1. A notable limitation of the paper is its focus on a dataset comprised solely of salient images related to autonomous driving. This does not accurately represent real-world conditions, where sensors capture a multitude of irrelevant or non-critical images. This selective approach raises questions about the model's susceptibility to overfitting and its reliance on human oversight for attention guidance. Future work could benefit from evaluating the fine-tuned LLM on a more diverse set of images to assess the model's generalization capabilities beyond the curated dataset.\n\n2. The paper's scope could be considered narrow, as it only tests the fine-tuned LLM on the custom-created BDD-X dataset. Extending evaluations to include other benchmark datasets like Waymo Open could provide a more comprehensive understanding of the model's applicability and robustness in different autonomous driving scenarios. The current evaluation strategy thus limits the paper's contributions to a more confined context.\n\n3. The paper may be perceived as over-ambitious in its claims, particularly given the title \"DriveGPT4,\" which implies a comprehensive solution for Level 4 autonomous vehicles. However, the proposed system focuses narrowly on generating control signals, neglecting other essential aspects like perception, planning, and behavior. This narrow focus limits the model's utility in real-world applications. Additionally, the paper does not sufficiently demonstrate the model's ability to address long-tail or zero-shot cases, further constraining its practical relevance. While DriveGPT4 shows promise for specific tasks like auto-labeling, its current form falls short of making it a broadly applicable solution in the autonomous driving ecosystem."
            },
            "questions": {
                "value": "Could you elaborate on the measures taken to assess whether the fine-tuned LLM retains its pre-existing knowledge base or if it has overfitted to the custom dataset? Specifically, has the model's performance been evaluated on diverse image sets from alternate data sources to gauge its generalization capabilities?\n\nHow does the DriveGPT4 model, trained on the BDD-X dataset, perform on other benchmark datasets like Waymo Open, particularly in tasks such as behavior prediction?\n\nCould you address how the proposed system manages the issue of hallucination? Have there been instances where the model exhibited hallucinatory behavior, and if so, how was this mitigated?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7421/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734862226,
        "cdate": 1698734862226,
        "tmdate": 1699636889956,
        "mdate": 1699636889956,
        "license": "CC BY 4.0",
        "version": 2
    }
]