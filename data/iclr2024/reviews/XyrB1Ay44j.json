[
    {
        "id": "kNbacmHtzc",
        "forum": "XyrB1Ay44j",
        "replyto": "XyrB1Ay44j",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5565/Reviewer_hY8P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5565/Reviewer_hY8P"
        ],
        "content": {
            "summary": {
                "value": "This manuscript concerns the robust multi-modal representation learning, which are positioned well away from the discriminative multi-modal decision boundary. To address this issue, they theoretically discover that larger uni-modal representation margins and more reliable integration for modalities are essential components for achieving higher robustness. Inspired by our theoretical finding, we introduce a training procedure called Certifiable Robust Multi-modal Training (CRMT), which can alleviate this influence from modality preference and explicitly regulate essential components to significantly improve robustness in a certifiable manner. Experiments validate the effectiveness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe multi-modal robustness learning is meaningful and challenging. The paper is well-written and the proposed method is easy to understand.\n2.\tThe authors theoretically discover that larger uni-modal representation margins and more reliable integration for modalities are essential components for achieving higher robustness.\n3.\tExperiments on various datasets validate the proposed method."
            },
            "weaknesses": {
                "value": "The manuscript claims that they focus on the commonly used joint multi-modal framework, more multi-modal fusion method, and different multi-modal backbones should be compared. For example, the early fusion, and hybrid fusion strategy. On the other hand, different modalities can employ various backbones, the reviewer is curious about the influence of different backbones, and more ablation studies are expected.\n\nIn related work and comparison methods, more state-of-the-art multi-modal robustness approaches should be introduced and compared.\n\nHow can this setup be extended to three modalities? More explanations and experiments are needed."
            },
            "questions": {
                "value": "refer to the weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5565/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734859255,
        "cdate": 1698734859255,
        "tmdate": 1699636572371,
        "mdate": 1699636572371,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yefVXVRptk",
        "forum": "XyrB1Ay44j",
        "replyto": "XyrB1Ay44j",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5565/Reviewer_BCRJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5565/Reviewer_BCRJ"
        ],
        "content": {
            "summary": {
                "value": "This work employs an orthogonal-based framework that formulates an alternative bound, eliminating the interrelation and explicitly presenting integration.  Building on the theoretical analysis, they introduce a two-step procedure called Certifiable Robust Multi-modal Training (CRMT) to progressively enhance robustness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) Following a more comprehensive analysis, the researchers furnish compelling evidence that demonstrates the constraining effect of multi-modal preference on the robustness of multi-modal systems, which contributes to the vulnerability of multi-modal models to specific modalities.\n\n(2) Building upon their theoretical insights, they present a two-step training protocol designed to alleviate the limitations stemming from modality preference. The suggested approach significantly enhances both the performance and robustness of multi-modal models across different real-world multi-modal datasets."
            },
            "weaknesses": {
                "value": "(1) Since FGM and PGD are the two white-box attacks chosen in the adversarial robustness experiments, why not consider the stronger white-box Auto Attack?  It is suggested to add the experiment results about Auto Attack in Section4.\n\n(2) \"Robustness against multi-modal attacks\" mentioned In Section 4.2, since multi-modal attacks are considered, the experimental results in Table 1 only consider single-mode attacks (#a,#v). Is the method proposed in this paper effective when co-attacks (both modality attacks) are existing?  In [1,2], more effective multi-modal attack methods are proposed than uni-modal(such as #a and #v) attack. Can the proposed method effectively resist these multi-modal attack methods? It is suggested that the relevant experiments should be added to Section 4.2, otherwise the conclusion of \"Robustness against multi-modal attacks\" is somewhat not convincing.\n\n\n[1]\tZhang J, Yi Q, Sang J. Towards adversarial attack on vision-language pre-training models[C]//Proceedings of the 30th ACM International Conference on Multimedia. 2022: 5005-5013.\n[2]\tLu D, Wang Z, Wang T, et al. Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models[J]. arXiv preprint arXiv:2307.14061, 2023."
            },
            "questions": {
                "value": "No"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5565/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758294195,
        "cdate": 1698758294195,
        "tmdate": 1699636572256,
        "mdate": 1699636572256,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uuPktQCWhe",
        "forum": "XyrB1Ay44j",
        "replyto": "XyrB1Ay44j",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5565/Reviewer_zQSE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5565/Reviewer_zQSE"
        ],
        "content": {
            "summary": {
                "value": "The paper studies adversarial robustness for multi-modal learning by building a new lower bound for the perturbation radius through uni-modal margins and the Lipschitz constraint. Based on the proposed lower bound, a two-step adversarial training framework has been provided to improve the robustness of multi-modal learning. Experimental results on three benchmark datasets were provided regarding multiple attack methods, compared with several strong baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- **New findings**: While discussing a new lower bound with the Lipschitz constraint is nothing new for adversarial robustness, the proposed method provides theoretical and insightful analyses of how the attack on a preferred modality would impact the overall robustness. This is a practical and common problem in multi-modal integration, as one modality often dominates the others. \n- **Good presentation**: The reviewer enjoyed reading the presentation of the proposed method, where each step was well demonstrated with theoretical supports and clearly developed through proper treatments. One minor suggestion is to provide a pseudo `Algorithm` to outline the method, as well as add a `Remark` to better summarize and explain the training steps. \n- **Decent experiment design**: Despite some minor issues, the experiment is overall well-designed and sufficient by 1) comparing with two groups of strong baselines, 2) adopting multiple attack methods (e.g., FGM, PGD, and missing modality), and 3) providing detailed ablation study and model discussions."
            },
            "weaknesses": {
                "value": "- **Missing implementation details**: I may have missed something; however, I did not find any implementation details about the multi-modal encoders. What are the backbones used in the experiment? Can the proposed method apply to different backbones? \n- **Unclear model-specific weights/classifiers**: The exact role of introducing model-specific weights $a^{(m)}$ is somewhat unclear to me. How will it be used to guide the orthogonal classifier of each modality? Also, it remains unclear to me how the proposed eventually gets the prediction result upon different modalities's classifiers. \n- **Lacking empirical evidence**: One main motivation of the proposed approach is one modality may be more vulnerable than the others.  While the adversarial accuracy (between uni-modal and multi-modal attacks) could support this observation empirically, it would be more convincing to provide more evidence that can be used to back-up the theoretical results, such as plotting the vulnerability indicator ($\\eta$) values, visualizing the perturbation radius over modalities, etc."
            },
            "questions": {
                "value": "Please refer to the questions raised in the *Weakness* section. Plus, the reviewer is interested in the following questions:\n- Can the proposed method apply to multiple modalities larger than 2?\n- What's the selection criterion in choosing datasets for the experiment? \n- Are the provided theoretical results applicable to vision-text data? Any empirical evidence? \n- Could the proposed method be incorporated into the pre-trained multi-modal model (e.g., CLIP or BLIP)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5565/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699213010075,
        "cdate": 1699213010075,
        "tmdate": 1699636572141,
        "mdate": 1699636572141,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ky9FfK2aQH",
        "forum": "XyrB1Ay44j",
        "replyto": "XyrB1Ay44j",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5565/Reviewer_yeWY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5565/Reviewer_yeWY"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors tackle the challenge of improving the robustness of multi-modal models against perturbations, such as uni-modal attacks and missing modalities. They provide valuable theoretical insight, emphasizing the importance of larger uni-modal representation margins and reliable integration for modalities in achieving higher robustness. They introduce a training procedure, Certifiable Robust Multi-modal Training (CRMT), which effectively addresses modality preference imbalances and enhances multi-modal model robustness. Experimental results validate the superiority of CRMT in comparison to existing methods, demonstrating its versatility and effectiveness. Overall, this paper contributes to the field by providing a theoretical foundation and a practical method for enhancing the robustness of multi-modal models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Overall, the paper advances multi-modal robustness understanding and presents a practical solution in CRMT with strong empirical results, and the potential for broader applications in the ML/Multimodal community.\n- The paper offers a fresh perspective on multi-modal robustness, emphasizing the importance of larger uni-modal representation margins and reliable integration within a joint multi-modal framework.\n- The research is methodologically sound, with well-designed experiments and clear presentation.\n- The authors effectively communicate complex concepts, enhancing accessibility."
            },
            "weaknesses": {
                "value": "The paper included some results on transformer as fusion models, particularlly the Multi-Modal Transformer-based framework with hierarchical attention on the VGGS dataset. However, all experriments, especailly the one with transformerr adopt training from scratch and did not consider any pre-training strategies, such as uni-modal pretraining, or multi-modal pretraining. It will be interesting to consider such methods as baselines and also to see how much CRMT can help to improve. \n\nAlso, except for experimenting, it will be good if authors can discuss how does their method generalize to other fusion mechanisms, besides late fusion."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5565/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699218807559,
        "cdate": 1699218807559,
        "tmdate": 1699636572030,
        "mdate": 1699636572030,
        "license": "CC BY 4.0",
        "version": 2
    }
]