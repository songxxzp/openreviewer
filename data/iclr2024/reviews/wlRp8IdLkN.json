[
    {
        "id": "IcQu5OFAfl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9302/Reviewer_tJXC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9302/Reviewer_tJXC"
        ],
        "forum": "wlRp8IdLkN",
        "replyto": "wlRp8IdLkN",
        "content": {
            "summary": {
                "value": "The authors propose to use RL to learn a policy for sampling diverse instructions to generate a dataset for instruction tuning for downstream LLM alignment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "--there is an interesting idea at the core of this paper: rather than just prompting to generate new diverse instructions for your initial instruction tuning prompt set, you can actually finetune the model for generating that dataset in the first place. this kind of suggests a \"hierarchy\" of sorts in the data generation, where you generate your data on multiple levels, starting from just your action set in 3.1.1."
            },
            "weaknesses": {
                "value": "--performance still seems a bit mixed compared to LLAMA, despite you leveraging ChatGPT/GPT4 and also WizardLM13b. Also, is the comparison to WizardLM7b in Fig 6 fair, given that you used WizardLM13b extensively in your pipeline?\n\n--similarly, it seems like you rely on having a strong instruction-tuned model already (WizardLM13b) as the \"Advanced LLM\" in Fig1 to be able to train your initial policy for sampling diverse instructions, which seems like maybe a bit of a chicken-and-egg problem. i think it might be more convincing if you were able to use a weaker model to do the initial judgments (e.g., why not use LLaMA7b, or WizardLM7b? are those not good enough for your purposes?), or show that you can later outperform whichever model you use for the initial Advanced LLM.\n\n--unless i missed it, there's no comparison on final downstream performance to any baseline that generates its instruction set just by prompting an LM rather than finetuning, which seems like it'd be the most direct baseline\n\n--there are typos/grammar errors even in the model prompts- these are arguably \u201cbugs\u201d"
            },
            "questions": {
                "value": "--i don't understand step 4 in algorithm 1 - how is chatgpt/gpt4 also being used to help you generate the complex instructions? are you just prompting it for more instructions to add to your instruction set, in addition to what you generated previously using your smaller RL-trained model?\n\n--nit: some typos and tense changes here and there, might want to proofread"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9302/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9302/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9302/Reviewer_tJXC"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9302/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697401439167,
        "cdate": 1697401439167,
        "tmdate": 1699637171399,
        "mdate": 1699637171399,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GwnzbyTHuJ",
        "forum": "wlRp8IdLkN",
        "replyto": "wlRp8IdLkN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9302/Reviewer_6muu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9302/Reviewer_6muu"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel approach to generate complex instruction-tuning data through reinforcement learning. It negates the need for subsequent RLHF stages. Their method can diminish the dependence on human instructors and moderates the need for constant queries to external models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper proposes a novel way to evolve instructions through reinforcement learning. The experiment results on LM-Eval benchmark demonstrate the effectiveness of their method."
            },
            "weaknesses": {
                "value": "1. The paper focuses on enhancing the instructions by iteratively optimizing the policy through RL. However, directly evolving instructions through WizardLM or Tree-Instruct prompts also avoids the need for training a large language model. The benefit brought by their method is constrained.\n2. Whether RLHF will further facilitate human alignment is not verified in this paper. Involving RL in the stage of SFT is computationally expensive."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9302/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9302/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9302/Reviewer_6muu"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9302/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830910859,
        "cdate": 1698830910859,
        "tmdate": 1699637171270,
        "mdate": 1699637171270,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1rOW916DJU",
        "forum": "wlRp8IdLkN",
        "replyto": "wlRp8IdLkN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9302/Reviewer_1H8F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9302/Reviewer_1H8F"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to first train a language model to generate instructions diverse from the seed. This is done using RL where the reward comes from another LM on whether or not the output instruction is of good quality. Then this model is used to generate diverse instructions, responses to which are generated by gpt-3.5 and other LMs, to create a dataset for instruction fine-tuning. When fine-tuned using this data, models like Llama2-chat-7b and WizardLM-7b are shown to improve on some benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The direction of lowering the cost of collecting instruction fine-tuning data and eliminating need for human feedback is important in making conversational LLMs more accessible."
            },
            "weaknesses": {
                "value": "This work seems to be leveraging additional instruction fine-tuning data (see Step 4 & 5) derived from ChatGPT and GPT-4 without clearly describing how it does so in the corresponding Sections. The contribution of this work seems weak if the responses are generated primarily using external models.\n\nMixed results with marginal gains compared to the checkpoints they start with, in some cases a drop (Fig 4 & 5). TruthfulQA performance of llama-2-chat-7b is under-reported as 38.98, Table 14 from the Llama2 paper reports the performance of the 7B chat model to be at 57.04.\n\nI find it very hard to comprehend the problem being solved and the approach being proposed in this submission. At least some parts of the paper seem to be LLM-generated."
            },
            "questions": {
                "value": "Questions on steps of the Algorithm proposed\n\nStep 1: Desigin of actions - do you simply use the same actions as proposed by WizardLM?\n\nStep 2: What does discrete value-based action space S mean?\n\nStep 3: How is TRPO used here? The binary feedback ('reward') that you get on diversity of the generated instructions is used to train the base LM, this appears to be the rejection sampling approach.\n\nStep 4 & 5: How is ChatGPT or GPT-4 used here? \n\nWhy are LLMs called Advanced LLM in Fig 1 & 2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9302/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9302/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9302/Reviewer_1H8F"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9302/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699152339193,
        "cdate": 1699152339193,
        "tmdate": 1699637171161,
        "mdate": 1699637171161,
        "license": "CC BY 4.0",
        "version": 2
    }
]