[
    {
        "id": "rnuWP8LMjM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9222/Reviewer_5wSC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9222/Reviewer_5wSC"
        ],
        "forum": "buC4E91xZE",
        "replyto": "buC4E91xZE",
        "content": {
            "summary": {
                "value": "This paper proposes a zero-shot anomaly detection framework using CLIP and object-agnostic text prompts with learnable prompt tokens and a modified attention mechanism."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper is well written and easy to follow overall\n\nExperiments are comprehensive, with many datasets and evaluation metrics, as well as the ablation study\n\nVLMs for anomaly detection is relatively under-explored and prompt learning is especially new amongst AD literature."
            },
            "weaknesses": {
                "value": "Little/no discussion about the additional cost of training\n\nNo comparison of different ViT backbones of CLIP\n\nFigure 6 feels confusing and unnecessary\n\nIt would be better to provide more intuition why the Focal and Dice loss are necessary\n\nFigure 2 caption should be more explanatory"
            },
            "questions": {
                "value": "Besides all of the other modifications such as DPAM, why are object-agnostic prompts better than object-aware prompts for a specific object class?\n\nIs it true that hidden layer visual embeddings are only used for the segmentation map, while the final output embedding is only used for the image-level score?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9222/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9222/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9222/Reviewer_5wSC"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9222/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697376022035,
        "cdate": 1697376022035,
        "tmdate": 1699637160685,
        "mdate": 1699637160685,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YXXoHHc3su",
        "forum": "buC4E91xZE",
        "replyto": "buC4E91xZE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9222/Reviewer_vcXy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9222/Reviewer_vcXy"
        ],
        "content": {
            "summary": {
                "value": "This paper targets ZSAD leveraging visual language pretrained models. To learn object-agnostic information, this paper proposes to use the well-known text prompt technique, allowing the model to focus on the abnormal image regions rather than the object semantics. Experiments are conducted on several anomaly detection benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe contribution is clear, with well-organized paper architecture.\n2.\tThe motivation sounds reasonable.\n3.\tExperiments show the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. The motivation behind DPAM appears to be relatively weak. It is not entirely clear why employing V-V self-attention would preserve more localized information compared to the original Q-K attention. Relying solely on a single instance shown in Figure 3 may not furnish sufficient evidence to support this assertion. Furthermore, the rationale for treating these features separately remains unclear. What might be the outcome if we were to combine Q-Q, K-K, V-V, and Q-K features into an ensemble?\n\n2. The ablation studies lack clarity regarding the specific details of each component, making it challenging to assess the actual contributions of the proposed method:\n(i) The experiments in Table 4 involve the incremental addition of modules (T1-T4), but a more informative approach would be to examine the performance when each of them is removed. For instance, it is unclear how DPAM (T1) performs when used in conjunction with both T2, T3, and T4. So an importment setting should be removing T1 but keeping using T2-T4. Additionally, the increase in AUC from 47.9 to 54.8 by introducing T1 does not signify a significant improvement (both values are close to random guessing) for a two-class classification problem.\n(ii) In Table 5, it remains unclear how the models are optimized when both the global and local losses are absent."
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9222/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697703426216,
        "cdate": 1697703426216,
        "tmdate": 1699637160560,
        "mdate": 1699637160560,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ln4gSCZ16b",
        "forum": "buC4E91xZE",
        "replyto": "buC4E91xZE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9222/Reviewer_GzuZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9222/Reviewer_GzuZ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a zero-shot anomaly detection/segmentation method utilizing CLIP and prompt learning. The authors observe that CLIP-like VLMs learn class semantic features of the object and pay less attention to normality/abnormality, e.g., associated with fine-grained local features. They assume that the normality/abnormality features can be object-agnostic. For adapting the pretrained CLIP to focus on normality and abnormality features, they proposed to learn two text prompts for each respectively. To learn these generic prompts, they utilize both global optimization, i.e., maximizing similarity between image embeddings and text embeddings, and local optimization, i.e., maximizing similarity between image-patch embeddings and text embeddings. They also show that adding more learnable tokens to the text encoder and image encoder of CLIP can improve results, and using V-V attention can help in refining the local features. The paper provides a comprehensive empirical evaluation of 17 real-world datasets showcasing the effectiveness and advantages of the proposed method over existing zero-shot anomaly detection baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper brings the principle of prompt learning for CLIP-like VLMs in zero-shot anomaly detection. \n2. The proposed method achieves remarkable zero-shot anomaly detection performance on both industrial anomaly detection datasets and medical datasets.\n3. The paper provides a large body of ablation studies for each involved design component individually."
            },
            "weaknesses": {
                "value": "1.  My largest worry is the paper relies on an optimistic assumption of abnormality, namely, the abnormality is object-agnostic.  The assumption might hold in some cases. The learned model is able to detect the abnormality that has similar abnormal patterns to the features in the auxiliary training set, e.g., detecting the defects of a manufactured part. However, the assumption can also fail in some cases. For example, the model may misclassify a product with texture patterns similar to scratches as an abnormal one, even though the scratch-like textures are normal texture patterns of the product.  It would be better to explicitly discuss the successful cases and the potential failures of the proposed method.\n\n2. Minor comment: $S_n$ and $S_a$ are first-time used in $L_{local}$ on page 5, but are clearly defined on page 6. Having the definitions and equations of $S_n$ and $S_a$ before $L_{local}$ would be good.\n\n3. The discussion of some related work about zero-shot anomaly detection, e.g., [1,2,3], is missing.\n\n[1] Philipp Liznerski, Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Klaus Robert Muller, and Marius Kloft. Exposing outlier exposure: What can be learned from few, one, and zero outlier images. Transactions on Machine Learning Research, 2022.\n\n[2] Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei Shu. Zero-shot out-of-distribution detection based on the pretrained model clip. In Proceedings of the AAAI conference on artificial intelligence, 2022\n\n[3] Aodong Li, Chen Qiu, Marius Kloft, Padhraic Smyth, Maja Rudolph, Stephan Mandt. Zero-Shot Batch-Level Anomaly Detection. Thirty-seventh Conference on Neural Information Processing Systems, 2023."
            },
            "questions": {
                "value": "1. Why is learning two text prompts necessary? Do you have comparisons to $g_n = [V_1][V_2]...[V_E][object]$ and $g_a = [V_1][V_2]...[V_E][damaged][object]$?\n1. How is the performance of the variant with all components except DPAM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9222/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698440348290,
        "cdate": 1698440348290,
        "tmdate": 1699637160434,
        "mdate": 1699637160434,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ra8HO7eYIk",
        "forum": "buC4E91xZE",
        "replyto": "buC4E91xZE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9222/Reviewer_w1Ea"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9222/Reviewer_w1Ea"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new method named AnomalyCLIP which deal with the Zero-shot anomaly detection problem. It leverages the CLIP prompt learning techniques to capture the normal/abnormal information within an image regardless of its foreground objects. The experiment results show the effectiveness of their method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tLeveraging pre-trained model like CLIP to address anomaly detection is a good direction and an interesting topic.\n2.\tThe paper shows comprehensive and detailed experiments and results which outperforms other baselines."
            },
            "weaknesses": {
                "value": "1.\tThe idea and some techniques, like glocal context optimization, are similar to a concurrent work[1]. The author may compare with highly related works.\n[1] Gu, Zhaopeng, et al. \"AnomalyGPT: Detecting Industrial Anomalies using Large Vision-Language Models.\" arXiv preprint arXiv:2308.15366 (2023).\n2.\tThe DPAM strategy is confusing. The author claims that the Q-Q, K-K, V-V self-attention suffers from different issues, and V-V self-attention derives the best result. However, these three variants are very similar in the DPAM mechanism. It is better to explain it in the paper and show the results of the original Q-K attention.\n3.\tI doubt the contribution of some modules. The module ablation results indicate that the T2 object-agnostic text prompts module contributes the most, similar to CoOp method. However, the author employed a different prompt for the CoOp experiment. For instance, the text prompt template for normality is defined as [V1][V2]...[VN][normal][cls], while that for abnormality is [V1][V2]...[VN][anomalous][cls]. This variation in prompt settings doesn't allow for a fair comparison."
            },
            "questions": {
                "value": "See Weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9222/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698643021148,
        "cdate": 1698643021148,
        "tmdate": 1699637160325,
        "mdate": 1699637160325,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "B2vrJZslkM",
        "forum": "buC4E91xZE",
        "replyto": "buC4E91xZE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9222/Reviewer_xREe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9222/Reviewer_xREe"
        ],
        "content": {
            "summary": {
                "value": "In the paper, the authors address Zero-shot anomaly detection (ZSAD) where training data is unavailable. They introduce AnomalyCLIP, which improves CLIP's generalization for ZSAD by using object-agnostic prompt learning. This allows for generic recognition of normality and abnormality across diverse images. AnomalyCLIP also incorporates both global and local anomaly contexts for optimization. Tested on 17 datasets, AnomalyCLIP demonstrates superior ZSAD performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper introducing AnomalyCLIP for Zero-shot anomaly detection (ZSAD) stands out for its originality, exemplified by its novel approach of using object-agnostic text prompts for anomaly detection, a creative departure from traditional methods. The quality of the work is underscored by its robust methodology and extensive validation across 17 diverse datasets. The authors effectively communicate their ideas with clarity, making the complex concepts accessible. Significantly, AnomalyCLIP's ability to generalize across various domains without needing prompt redesign, particularly in industrial and medical settings, marks it as a notable advancement in the field of anomaly detection."
            },
            "weaknesses": {
                "value": "A weakness in the paper is the unexplained initial use of the term \"glocal.\" Clarifying this key term when first mentioned would improve understanding and clarity."
            },
            "questions": {
                "value": "See the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9222/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808189305,
        "cdate": 1698808189305,
        "tmdate": 1699637160203,
        "mdate": 1699637160203,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wQfLtPFbAh",
        "forum": "buC4E91xZE",
        "replyto": "buC4E91xZE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9222/Reviewer_NCKk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9222/Reviewer_NCKk"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a zero-shot anomaly detection (ZSAD) approach using CLIP, named AnomalyCLIP. Different from the previous ZSAD methods, AnomalyCLIP attempts to learn object-agnostic text prompts of normality and abnormality to segment abnormal part. To reach this, the authors leverages textual prompt tunning and DPAM technologies. Extensive evaluations conducted on 17 publicly datasets demonstrates the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The empirical evaluations are extensive covering 17 diverse benchmarks.\n\n2. The proposed approach attempts to learn class-agnostic prompts which seems to contradict the pretrained CLIP that is mostly used to classify semantic objects. The authors successfully mitigate this issue by several technical modules, e.g. object-agnostic text prompt design and DPAM layer."
            },
            "weaknesses": {
                "value": "1. The prompt template might be too restrictive. \"damaged [cls]\" may not well represent all types of anomalies. For example, if a component is missing or applying the method to other domains than defect identification the proposed prompt template may not work well.\n\n2. According to Figure 2, the pipeline needs to feed the same images into two visual encoders, this would introduce additional computation overhead.\n\n3. The proposed DPAM layer lacks a theoretical basis. It is not clear why replacing Q-K attention map with V-V attention map without updating any visual encoder's parameters into CLIP would improve the performance.\n\n4. The paper has not discussed why the proposed approach is able to learning class-agnostic feature from the pretrained semantic CLIP as the CLIP is trained to be sensitive to semantic classes."
            },
            "questions": {
                "value": "It is important to discuss why the \"damaged [cls]\" is suitable for more diverse anomaly detection scenarios.\n\nExplanations on the effectiveness of DPAM and computation overhead are necessary."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 6,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9222/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839936654,
        "cdate": 1698839936654,
        "tmdate": 1699637160099,
        "mdate": 1699637160099,
        "license": "CC BY 4.0",
        "version": 2
    }
]