[
    {
        "id": "icaExzlRH5",
        "forum": "VIEbRFp6s3",
        "replyto": "VIEbRFp6s3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5706/Reviewer_YX5P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5706/Reviewer_YX5P"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces OG-MARL, an expansive repository made for cooperative offline multi-agent reinforcement learning (MARL). Addressing the current lack of standardized datasets and baselines in offline MARL, the authors offer a collection mirroring real-world system complexities, such as heterogeneous agents and non-stationarity. These datasets, classified into types like Good, Medium, Poor, and Replay, undergo thorough quality assurance checks. The authors have made OG-MARL publicly accessible."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper addresses a significant gap in the field of offline multi-agent reinforcement learning.   This initiative targets the lack of standardized datasets and baselines, a challenge often overlooked by many in the field of reinforcement learning.  In terms of quality, the datasets were curated and validated.   The use of diverse real-world system parameters, like heterogeneous agents and non-stationarity, makes the paper better.  This paper is also easy to follow. By providing a public repository, it contributes to the MARL research community."
            },
            "weaknesses": {
                "value": "1. The categorization of datasets heavily based on the quality of experience may inadvertently introduce biases. A more well-rounded evaluation could be achieved by integrating additional qualitative and quantitative metrics. \n\n2. The results section provides an overview of algorithmic performance but lacks analytical depth. Explain the reasons behind the observed performances, such as the underperformance of vanilla QMIX, could offer more substantial insights. \n\n3. There is a similar work, \"Off-the-Grid MARL: Datasets and Baselines for Offline Multi-Agent Reinforcement Learning,\" has been previously published in AAMAS. Does this submission introduce novel datasets or environments that extend beyond those covered in the AAMAS paper? Are there any innovative algorithmic approaches, evaluation metrics, or experimental setups that were not addressed in the prior publication? Further, how does the current paper tackle the challenges and limitations identified in the AAMAS publication? \n\n4.  While this paper undeniably provides significant aid to the research community in terms of establishing a baseline database and engineering groundwork for MARL, its depth seems somewhat superficial.  The ideas, though functional, are straightforward by testing different algorithms in different environments and producing new datasets (by using the old method).   Meanwhile, while the authors have laid out certain frameworks and methodologies, there isn't clear documentation on how one might go about implementing novel algorithms or introducing new environments within the given context.  This work is engineering important but has little contribution to the theoretical underpinnings or conceptual advancements in the field."
            },
            "questions": {
                "value": "Please see the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Non"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5706/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5706/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5706/Reviewer_YX5P"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5706/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697910853160,
        "cdate": 1697910853160,
        "tmdate": 1700515904750,
        "mdate": 1700515904750,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XTIBKGsNUz",
        "forum": "VIEbRFp6s3",
        "replyto": "VIEbRFp6s3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5706/Reviewer_4ULc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5706/Reviewer_4ULc"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes datasets for offline multi-agent reinforcement learning including games(real world problems) with both discrete and continuous actions. The paper also provide different types of the datasets: Good, Medium, Poor. Evaluation results of offline multi-agent reinforcement learning baselines are provided."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.The dataset for offline multi-agent reinforcement learning is missing, which is important for this community. 2.The paper provides a comprehensive dataset including both games and real world problem, both discrete and continuous. 3.The paper is well written."
            },
            "weaknesses": {
                "value": "1.The major concern of the paper is the correctness of the implementation of the baselines. OMAR definitely outperforms CQL in many tasks, as reported in \"Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning https://arxiv.org/abs/2307.01472\". However, this is not true in Table D.5. As a dataset and benchmark paper, I think it's crucial to ensure that the results are replicable and the claims made for previous baselines are correct.\n2.The paper does not provide explanations on why an algorithm outperforms another algorithm."
            },
            "questions": {
                "value": "See the above section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5706/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698321141758,
        "cdate": 1698321141758,
        "tmdate": 1699636597316,
        "mdate": 1699636597316,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6TC8h2r34W",
        "forum": "VIEbRFp6s3",
        "replyto": "VIEbRFp6s3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5706/Reviewer_DuA2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5706/Reviewer_DuA2"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed off-the-grid MARL (OG-MARL) datasets with baselines for cooperative offline MARL. The datasets provide settings include complex environment dynamics, heterogeneous agents, non-stationarity, many agents, partial observability, suboptimality, sparse rewards and demonstrated coordination. The OG-MARL provides a range of different dataset types and profiles the composition of experiences for each dataset."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper proposed the datasets of offline MARL by extending the idea of single-agent offline RL datasets such as D4RL (Fu et al., 2020) and RL Unplugged (Gulcehre et al., 2020). The datasets provide settings include complex environment dynamics, heterogeneous agents, non-stationarity, many agents, partial observability, suboptimality, sparse rewards and demonstrated coordination.\n- This paper also provided baselines for existing cooperative offline MARL such as Behaviour Cloning (BC), QMIX (Rashid et al., 2018), QMIX with Batch Constrained Q-Learning (Fujimoto et al., 2019), QMIX with Conservative Q-Learning (Kumar et al., 2020) and MAICQ (Yang et al., 2021). The results concluded that on PettingZoo environments, with pixel observations, MAICQ is the current state-of-the-art offline MARL algorithm in discrete action settings.\n- The paper is well-written and mostly has clarity."
            },
            "weaknesses": {
                "value": "Although this paper includes a novelty about MARL extension from single-agent RL datasets and baselines, other points seem to be ordinary.  More challenging benchmarks and more real-world scenarios, might provide more significance, as described below. \nThere were also some unclear points described below."
            },
            "questions": {
                "value": "1. P6: The authors said that \u201cWe chose these environments because they have visual (pixel-based) observations of varying sizes; an important dimension along which prior works have failed to evaluate their algorithms\u201d. What are the prior works specifically and why did they fail the evaluation?\n\n2. For human data (KAZ), the detailed description will be described because humans have diversity and usually the property of the human participants (e.g., age and the game experience) should be reported. If possible, comparison with the data from RL algorithms will estimate the property of human data.\n\n3. The paper mentioned about KAZ that \u201cThe players where given no instruction on how to play the game and had to learn through trial and error.\u201d but does it mean the data may include not only \u201clearned\u201d data but also \u201clearning\u201d data? The data acquisition process can be clarified.  \n\n4. More challenging MARL benchmarks such as team sports (e.g., [1] [2]) or more real-world robotics data might provide more significance. \n[1] Kurach et al. Google Research Football: A Novel Reinforcement Learning Environment, AAAI, 2020\n[2] Liu et al. From motor control to team play in simulated humanoid football, Science Robotics, 2022"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5706/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698499865571,
        "cdate": 1698499865571,
        "tmdate": 1699636597206,
        "mdate": 1699636597206,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ic1NIc47kE",
        "forum": "VIEbRFp6s3",
        "replyto": "VIEbRFp6s3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5706/Reviewer_UqF2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5706/Reviewer_UqF2"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the OG-MARL openly available offline datasets and baselines for MARL. The datasets cover a range of scenarios, including micromanagement in StarCraft 2, continuous control in MAMuJoCo, diverse environments in PettingZoo, train scheduling in Flatland, and energy management in Voltage Control/CityLearn. The paper tries to address the lack of benchmark datasets and baselines in offline MARL and aims to facilitate research and comparison of MARL algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- the paper addresses the lack of commonly shared benchmark datasets and baselines in the field of offline MARL.\n- the proposed data contains data on a large collection of various MARL environments, including SMAC, MAMuJoCo, PettingZoo, Flatland, and Voltage Control/CityLearn. \n- the authors provide detailed descriptions of the different environments and datasets, including information about the composition of the datasets and visualizations of the behavior policy."
            },
            "weaknesses": {
                "value": "- it would be beneficial to include performance comparisons with more existing algorithms and baselines on the provided datasets. For instance, federated offline MARL, etc\n- it would be beneficial if the paper could provide a list of the size of the data and the approximate amount of computation resources required for training the baseline.\n- it seems the dataset has fewer scenarios with competitive case, adding more competitive datasets would probably be helpful to make it more general."
            },
            "questions": {
                "value": "- For different levels of data (good, medium, etc), how do you make sure that the dataset contains a wide variety of experiences and is not biased to a certain type of policy?\n- Based on the C.1 paper, is the size of the dataset sufficiently large for large-scale experiments such as federated offline MARL?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5706/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698783722551,
        "cdate": 1698783722551,
        "tmdate": 1699636597095,
        "mdate": 1699636597095,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iwqk8H58o0",
        "forum": "VIEbRFp6s3",
        "replyto": "VIEbRFp6s3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5706/Reviewer_KxKX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5706/Reviewer_KxKX"
        ],
        "content": {
            "summary": {
                "value": "This work introduces Off-the-Grid Multi-Agent Reinforcement Learning (OG-MARL), a repository aiming to address the lack of standardized benchmark datasets and baselines in the emerging field of offline multi-agent reinforcement learning (MARL). The motivation is to leverage large datasets from real-world industrial systems, where distributed processes can be recorded during operation. The provided datasets in OG-MARL exhibit characteristics of complex real-world environments, including partial observability, suboptimality, demonstrated coordination, etc."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Benchmarking is essential in machine-learning communities as well as multi-agent learning communities.\n- This benchmark contains a variety of settings in multi-agent, such as team & individual rewards and homogeneous & heterogeneous agents.\n- This paper is well-written to some extent."
            },
            "weaknesses": {
                "value": "- An explanation and comprehensive analysis of the baselines tested on the proposed dataset should be provided as well.\n- Is there any measurement of the diversity of the trajectories in the dataset?\n- Please clarify the difference between this work and another recent work [1].\n\n[1] Off-the-Grid MARL: Datasets and Baselines for Offline Multi-Agent Reinforcement Learning, AAMAS 2023"
            },
            "questions": {
                "value": "Please refer to the weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5706/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5706/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5706/Reviewer_KxKX"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5706/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699736985588,
        "cdate": 1699736985588,
        "tmdate": 1699736985588,
        "mdate": 1699736985588,
        "license": "CC BY 4.0",
        "version": 2
    }
]