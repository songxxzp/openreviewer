[
    {
        "id": "bRhcHpF2rK",
        "forum": "ibggY9ZJ1T",
        "replyto": "ibggY9ZJ1T",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8795/Reviewer_feP3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8795/Reviewer_feP3"
        ],
        "content": {
            "summary": {
                "value": "The paper starts with an observation that the vector direction of LLM parameters can be a good fingerprint to identify the original base model. However, doing this will expose the model parameters and is not robust to attacks like linear mapping or permutation on model parameters or word embeddings. Instead, the paper proposes the invariant terms that are sharable by the model owners, more robust to the above attacks, but sacrificing a small amount of performance. It then converts the invariant terms to human-readable fingerprints using a conditional StyleGAN2 to generate dog images. Experimental results demonstrated the effectiveness of the proposed method when the LLMs undergo different training steps like SFT, RLHF, or fine-tuning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The proposed invariant terms are novel and effective in identifying the variances of the LLMs.\n+ The idea of using visual information for explanation is creative."
            },
            "weaknesses": {
                "value": "+ The paper has a citation format issue (possibly by converting from an IEEE-formatted source).\n+ The paper lacks a solid theoretical foundation for developing the proposed method.\n+ The proposed invariant terms may not work with the distillation attacks where the original LLMs are used as a teacher for training. The student may have a different architecture than the teacher.\n+ Mapping from invariant terms to dog images relies too much on the disentanglement quality of StyleGAN2. For example, in Fig. 5, the Guanaco dog differs from the others. In Fig. 6, there are some similar but unrelated pairs, such as [Qwen-7B, Galactica-30B], [ChatGLM-6B, LLaMA-13B], [Bloom-7B, OPT-30B], and [Baichuan-7B, Pythia-12B].\n+ The current training scheme for the convolutional encoder may lead to overfitting. There is no evidence of the separation of training and test data, which should be mutually exclusive."
            },
            "questions": {
                "value": "Please refer to the comments in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8795/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8795/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8795/Reviewer_feP3"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8795/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698224538772,
        "cdate": 1698224538772,
        "tmdate": 1699637105480,
        "mdate": 1699637105480,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3WX6HCWoVE",
        "forum": "ibggY9ZJ1T",
        "replyto": "ibggY9ZJ1T",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8795/Reviewer_qUfX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8795/Reviewer_qUfX"
        ],
        "content": {
            "summary": {
                "value": "This paper aims at identifying the original base model of an LLMs which are fine-tuned or even continue-pretrained.\nThe paper first points out that for the LLMs from the same baseline model, the cosine similarity would be close.\nBased on this observation, the invariant terms are proposed to represent a LLM considering that the actual parameters are accessible in some scenarios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper targets at an interesting topic which has not been explored thoroughly.\n- Using the invarient term to represent a network sounds reasonable."
            },
            "weaknesses": {
                "value": "- For the experiment showing that LLM models from a same base model would have a higher cosine similarity, this result is quite predictable as some models are finetuned on only several modules instead of the whole network. Have you also tried to compute another similarity score? It would be interesting whether a simple Euclidean distance could lead to the same result or not. \n- Representing a LLM into a readable dog image is an interesting idea, but is not practical and scientific. The visual similarity is subjective. For example, Guanaco and LLaMA in Figure 5 looks more different than Qwen-7B and Galactica-30B in Figure 6.\n- Given two concerns mentioned above, the contributions of this paper are limited and I doubt this work can bring the impact the community."
            },
            "questions": {
                "value": "- What are the points to use a figure instead of numerical difference to represent a LLM? Could you please specify some advantages?\n- Have you tried to use different similarity functions for the experiment shown in Table 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8795/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819047338,
        "cdate": 1698819047338,
        "tmdate": 1699637105322,
        "mdate": 1699637105322,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tkzmNjUoYZ",
        "forum": "ibggY9ZJ1T",
        "replyto": "ibggY9ZJ1T",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8795/Reviewer_8z4E"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8795/Reviewer_8z4E"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel approach called HuRef, which is a human-readable fingerprint for large language models (LLMs) that uniquely identifies the base model without exposing model parameters or interfering with training. The authors observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, and this stability can be used to identify the base model. They also address vulnerability to attacks by defining three invariant terms that identify an LLM's base model. The authors then propose a fingerprinting model that maps these invariant terms to a Gaussian vector and converts it into a natural image using an off-the-shelf image generator. Experimental results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1) The paper presents an innovative and practical methodology for identifying the base model of LLMs without exposing model parameters.\n2) The authors provide insightful empirical findings by showing the stability of the vector direction of LLM parameters and the effectiveness of the proposed invariant terms.\n3) The paper is well-structured and provides a clear review of relevant literature."
            },
            "weaknesses": {
                "value": "1) The paper lacks detailed implementation details, making it difficult to reproduce the study.\n2) The authors should provide more in-depth insights into why the proposed method is effective."
            },
            "questions": {
                "value": "1) Could you provide more details about the implementation of the proposed method, including the specific architecture and training settings?\n2) Can you clarify how the proposed method can be applied to LLMs that are not open-sourced or have restricted access to their parameters?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8795/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8795/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8795/Reviewer_8z4E"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8795/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831786195,
        "cdate": 1698831786195,
        "tmdate": 1699637105214,
        "mdate": 1699637105214,
        "license": "CC BY 4.0",
        "version": 2
    }
]