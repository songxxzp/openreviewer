[
    {
        "id": "jcxLFEb9j1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8915/Reviewer_xgDh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8915/Reviewer_xgDh"
        ],
        "forum": "Qn4HEhezKW",
        "replyto": "Qn4HEhezKW",
        "content": {
            "summary": {
                "value": "The paper proposes a method to adapt pretrained masked language models to diffusion language models, using it to adapt MLMs of different sizes to diffusion LMs, and then perform extensive task finetuning and instruction finetuning on the diffusion LMs. Results show that diffusion LMs can perform similarly to autoregressive LMs on tasks and generalize to unseen tasks after instruction finetuning. Diffusion LMs are also shown to manifest in-context learning and reasoning abilities, suggesting that diffusion LMs could have similar capabilities as autoregressive LMs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This is one of the first papers to investigate the scalability of diffusion LMs, highlighting the potential of diffusion LMs with the increase of model size. The results could potentially draw more attention to and stimulate further research on non-autoregressive LMs.\n* The proposed diffusion adaptation technique bridges existing masked LM with diffusion LM using lightweight continual pre-training. It potentially serves as an effective strategy for training diffusion LMs under computation constraints.\n\n* The paper provides a comprehensive evaluation of diffusion LM using multiple supervised, few-shot, and zero-shot tasks, giving an overall complete perspective on the performance of diffusion LMs."
            },
            "weaknesses": {
                "value": "* Discussion is limited on diffusion adaptation, a core method contribution of the paper. When using an existing MLM checkpoint instead of extensive pre-training, the performance of diffusion LM hinges on the effectiveness of diffusion adaptation. However, the paper does not provide enough performance metrics to validate the effectiveness of diffusion adaptation. For example, how does it compare to training from scratch, and how does the number of adaptation steps affect downstream performance? Without these results, it is unclear if diffusion adaptation is an effective approach to building diffusion LMs.\n\n* While showing diffusion LM can be promising and shows similar capabilities as auto-regressive LM, it is hard to get a concrete understanding of the performance of diffusion LM from the current paper. \n\n  * No effective performance comparison between diffusion LM and other kinds of LMs. While it is convenient to adapt an existing MLM to diffusion LMs and evaluate it, as the authors pointed out, the current available MLM checkpoints are out-of-date, and diffusion adaptation inevitably loses performance compared to training from scratch with diffusion objective. This makes the current result potentially considerably inferior to what diffusion LM can achieve in theory (with a state-of-the-art training recipe and trained from scratch). As a result, it is hard to appreciate the exact capabilities of diffusion LM from the current results. This could also make some discussions ineffective, for instance, it cannot be ruled out that the deficiencies in reasoning are due to insufficient training. \n\n  * Discussion on in-context learning and reasoning abilities is vague/ineffective. From Figure 5, it is hard to observe performance improvement due to in-context learning. For auto-regressive LM, in-context learning can be evaluated on raw pre-trained LM prior to instruction-finetuning. Maybe a similar evaluation on diffusion LMs could better visualize their in-context learning abilities. Also, as the authors pointed out, the limited context length of the original model limits the evaluation of few-shot performance. One possible solution without training from scratch could be using interpolation of position embeddings (e.g. [1]).\n\n    Similarly, there is no concrete conclusion from the discussion on reasoning abilities due to insufficient reasoning performance. One interesting discussion is on the generation order of diffusion LM and its difference from auto-regressive LMs, but a concrete conclusion would probably require a benchmark or a probe dataset for a systematic evaluation, showing the advantage of the generation order employed by diffusion LMs.\n\n  * In my opinion (without any discouragement to the authors), the evaluation of diffusion LMs is probably better done with sufficient computing resources for a head-to-head comparison with auto-regressive LMs under similar training conditions. The results would be much more useful and enable effective discussion on the capabilities of diffusion LMs."
            },
            "questions": {
                "value": "* How does the performance of diffusion LM compare to other non-autoregressive language models?\n* What about the inference computation cost of diffusion LMs, compared to auto-regressive LMs? How does the number of denoising steps affect the performance-computation tradeoff, as it is a crucial parameter in image diffusion models?\n* The performance comparison in Table 1 is a bit confusing: 1) are the RDM models trained by the authors? if so, why not evaluate on all three datasets? 2) \"The performance of finetuned XLM-R models is competitive or superior to the common encoder-decoder Transformer baseline.\" may not be so safe to say as the decoder-only models are larger than encoder-decoder models. 3) how is XLM-R (autoregressive) evaluated on IWSLT14, as it is an encoder-only model?\n* In Table 2, why not use zero-shot AR as a reference but use supervised AR?\n* Do the AR models used in the paper use beam search during decoding?\n* Given that diffusion LMs use a different receptive field and generation order than auto-regressive LMs, could there be specific types of tasks or domains where diffusion LMs particularly excel or fall short? That could be a valuable discussion (but probably beyond the scope of the paper)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8915/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8915/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8915/Reviewer_xgDh"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8915/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697355612992,
        "cdate": 1697355612992,
        "tmdate": 1699637121943,
        "mdate": 1699637121943,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8RBtO1xV8b",
        "forum": "Qn4HEhezKW",
        "replyto": "Qn4HEhezKW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8915/Reviewer_VY65"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8915/Reviewer_VY65"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the scalability of discrete diffusion language models on leveraging pretrained knowledge, model scaling and zero to few shot generalizations. The authors first draw an elaborate introduction on diffusion language models. Starting from off-the-shelf pretrained language models, the authors further fine-tune them with diffusion objective following RDM, with either task-specific tuning (MT, Summ) or instruction fine-tuning (FLAN). The authors conduct comprehensive experiments to explore the performance on both down-steam tasks and generalizations of discrete diffusion models varying their scale."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper studies an important topic on the scalability of diffusion language models, and goes through extensive experimental explorations, which makes a valuable contribution to developing better diffusion language models.\n- This paper poses comprehensive experimental investigations on scaling, tuning and generalizations of discrete diffusion language models, and compares them against the T5 family.\n- The author also explores emergent capabilities of autoregressive (L)LMs like in-context learning and reasoning, on their adapted large diffusion language models.\n- The paper is well written, clear to follow."
            },
            "weaknesses": {
                "value": "- The main method in this paper is similar to existing works:\n\n  - The idea of leveraging pretrained masked models as a knowledge base and starting point for tuning discrete diffusion language models (named as \"diffusive adaption\") is introduced in [1].\n\n  - The main methods in this paper (tuning pretrained MLMs with diffusion objective) is also mainly adhering to the work RDM-Diffusion [2].\n  - The \"prompt-response\" format (partially diffusion on target tokens) essentially adopts the setting of DiffuSeq [3].\n\n- While referring BERT-like diffusion language models as 'decoder-only' models, (GPT-like) AR decoder models (which is currently most popular) is not compared or discussed throughout this study. Besides, I believe that authors need further clarifications on their definitions of 'decoder' models, since we do not generally refer BERT-like models as decoder-only models, which might cause some confusions.\n\n- The paper does not fully explore the configurations of Diffusion LMs, such as the number of diffusion steps, the noise level, and the length predictor. It would be valuable to explore how these factors affect the quality and diversity of the generated texts, especially when the inference latency due to the demand on multiple diffusion steps would limit the scaling of diffusion models. For example, would larger models remedy the demand for more steps & generalize better to predicted lengths or  opposite?\n\n[1] He, Z., Sun, T., Wang, K., Huang, X., & Qiu, X. (2022). Diffusion Bert: Improving generative masked language models with diffusion models. *ArXiv preprint arXiv:2211.15029*.\n\n[2] Zheng, L., Yuan, J., Yu, L., & Kong, L. (2023). A reparametrized discrete diffusion model for text generation. *ArXiv preprint arXiv:2302.05737*.\n\n[3] Gong, S., Li, M., Feng, J., Wu, Z., & Kong, L. (2022, September). DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models. In *The Eleventh International Conference on Learning Representations*."
            },
            "questions": {
                "value": "- The paper lacks some important baselines and comparisons in Table 1. For example, why are there dashes on most AR transformer baselines? Besides, only small AR models (<100M) are compared against the 9.7B diffusion model. Other important baselines (e.g., GPT-structured decoder models, vanilla AR transformers at similar scale to diffusion variants (~80M), and pretrained + AR fine-tune) should be incorporated to compare against the performance of scaled diffusion models.\n- Why are the performance gaps of using oracle length and predicted length opposite in the two MT tasks in Figure 3? Are there any further investigations or explanations?\n- On table 2, what is the vanilla performance of XLM-based diffusion models before fine-tuning on FLAN? Since XLM models already adopt a multilingual pretraining objective, it seems natural that it elicits German knowledge and scalable, since larger models undertake greater-scale multilingual pretraining and has more parameters to store this knowledge.\n- Why BBH-NLP seems not following the scaling trend in Figure 4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8915/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723123023,
        "cdate": 1698723123023,
        "tmdate": 1699637121827,
        "mdate": 1699637121827,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KDDLMGhpDA",
        "forum": "Qn4HEhezKW",
        "replyto": "Qn4HEhezKW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8915/Reviewer_bJbT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8915/Reviewer_bJbT"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to pre-train a masked language model, and fine-tune this model as a generative discrete absorbing diffusion model for translation, summarization, and instruction-following tasks. Experiments are conducted using the XLM-RoBERTa family of MLMs (86M, 304M, 2.8B, 9.7B variants) finetuned as generative models on IWSLT14 and WMT14 (translation) Gigaword-10K (summarization) and Flan 2022 (instruction-following) datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper contains a diverse set of experiments using finetuned MLMs as diffusion models, which complements previous work using this methods (e.g., DiffusionBERT). The experiments are described well, and appear to be well-executed. The results show consistent task performance improvements using the larger-scale models, which is consistent with broader observations of the importance of scale in the ML community (although perhaps unsurprising at this point)."
            },
            "weaknesses": {
                "value": "There does not seem to be a significant methodological contribution. As far as I can tell, the methodological approach is the same as previous work adapting pre-trained MLM's for discrete diffusions [1]; the difference here is the use of the XLM-RoBERTa family of models, vs. previous results using BERT. The paper is clear that its contributions are about scale, not methodology, but it's a little frustrating that so much of the paper (up until Section 4) is dedicated to summarizing the contributions of previous work in the field.\n\nThis is not the first work to study the scaling behavior of diffusion models for text. For example, multiple diffusion models are trained in [2] together with a scaling law for these models and comparisons to scaling laws for baseline autoregressive models. Because this paper's central contribution is an empirical study of scaling behavior of text diffusions, a more thorough discussion of previous work on scaling text diffusions would be helpful.\n\n[1] DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models\nZhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, Xipeng Qiu\n\n[2] Likelihood-Based Diffusion Language Models\nIshaan Gulrajani, Tatsunori B. Hashimoto"
            },
            "questions": {
                "value": "I am curious how the approach described in this paper compares to direct MCMC sampling from an MLM. e.g., the approach described in [3]. To be clear, I am quite sympathetic to the idea of finetuning to directly supervise generation rather than inference-time MCMC; but I do think some discussion of this alternative line of work and connections/tradeoffs vs. the proposed approach would be enlightening and strengthen the discussion in Section 3.2.\n\n\"There exist large discrepancies in architecture and data engineering between our foundation models, XLM-R (Conneau et al., 2019), which were built years ago, and the state-of-the-art large language models like LLaMA (Touvron et al., 2023a;b).\"\n\nI do not work at a scaling lab, so my insight into these questions is limited, but one explanation I have heard for the lack of recent developments in the MLM space is that the performance of these models simply does not scale well in comparison to autoregressive LM's. I'm curious what you think of that claim, and whether (potential) limitations in our ability to scale the performance of the base MLM could create a ceiling on the performance of diffusion models trained using masked lanuage modeling as a pre-training objective.\n\n\n[3] Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings\nKartik Goyal, Chris Dyer, Taylor Berg-Kirkpatrick"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8915/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698946621997,
        "cdate": 1698946621997,
        "tmdate": 1699637121702,
        "mdate": 1699637121702,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XJVZukrNgO",
        "forum": "Qn4HEhezKW",
        "replyto": "Qn4HEhezKW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8915/Reviewer_gyd2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8915/Reviewer_gyd2"
        ],
        "content": {
            "summary": {
                "value": "This work presents an initial exploration into the effects of scale and fine-tuning on large diffusion language models. In particular, the work is interested in answering whether scale and instruction-tuning for large diffusion models can unlock similar generalization capabilities that are observed in more standard, auto-regressive LMs.\n\nTo test this, the authors use Reparameterized Discrete Diffusion models (RDM), a method from prior work, which is able to adapt pre-trained masked language models (MLMs) such as BERT into generative diffusion models. The authors use pre-trained XLM-R models, of various sizes, as the backbone of their diffusion models.\n\nThe authors use RDM to fine-tune XLM-R on 3 generative NLP tasks (2 translation tasks, and one summarization task). They find that: (a) diffusion models adapted from pre-trained models outperform randomly initialized XLM-R models, suggesting that RDM can leverage pre-training; (b) that performance on all 3 tasks improves with XLM-R backbone size, suggesting that RDM improves with model scale.\n\nThe authors next explore the zero-shot capabilities of diffusion LMs after fine-tuning on an instruction-tuning dataset (FLAN), and find that these models are able to achieve relatively strong zero-shot capabilities after instruction-tuning. Finally, the authors explore the ability of these models to perform in-context learning after instruction-tuning, and once again find that diffusion LMs can perform comparably to other in-context LMs after instruction-tuning on certain tasks.\n\nThe authors end with an interesting qualitative evaluation of a diffusion LMs reasoning order over a complex reasoning task, showing an example where their diffusion LM generates in an order that holds aligns with a _causal order_, which differs from autoregressive models which must generate a response linearly."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The work demonstrates that diffusion models can be adapted from other pre-trained models, and, when done at scale, can achieve strong performance on certain downstream tasks both via fine-tuning as well as in-context learning.\n- The observation that \n- The paper is well-written; in particular, it clearly explains prior work on how diffusion LMs operate, what their issues are, and what RDM is.\n- The final example, which serves to highlight the differences between diffusion generation and autoregressive generation for chain-of-thought reasoning is very interesting and a novel observation to the best of my knowledge."
            },
            "weaknesses": {
                "value": "- The finding that pre-training helps an adapted diffusion model, and that pre-trained model size influences the adapted models generalization, is perhaps not surprising. In particular, (a) it is already known XLM-R performance generally improves with scale on several downstream tasks, so the benefit of scale likely comes from the pre-training procedure, rather than the diffusion adaptation and (b) given that pre-training helps over no pre-training, it is not very surprising that _better_ pre-trained models help more.\n- The paper claims to be interested in how diffusion LMs scale with respect to data, but that is not tested in this work. What is tested is how diffusion LM capabilities scale w.r.t. pre-trained model size and how diffusion LMs respond to instruction-tuning.\n- The final note about how diffusion models generate causally is very interesting, but is only explored with a single qualitative example.\n- The results are sometimes a bit confusing to parse.\n   - In particular, for Table 1, there is a number of results missing, particularly for XLM-R-BASE (AR), which is perhaps the most comparable portion of the table as it directly compares autoregressive generation to RDM on the exact same model. It's also not clear what the value of having the Encoder-Decoder models there are, since the models are pre-trained differently, have different sizes, and different architectures; the take-away from this comparison is very unclear.\n   - I find it a bit surprising that the number of examples used in in-context learning have little effect on model performance (and sometimes have a very negative effect). It would be very useful to have these plotted against a model that is known to perform well at in-context learning, to see if the same trends hold. As it stands, the results suggest to me that the diffusion LMs presented are good zero-shot learners, but poor in-context learners."
            },
            "questions": {
                "value": "What is the reason for missing fields in Table 1? Are these rows results obtained from other papers? If so, I feel that should be made clearer.\n\nWould it be possible to have Flan-T5 compared to Flan-XLM-R for various numbers of in-context exemplars?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8915/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699132488254,
        "cdate": 1699132488254,
        "tmdate": 1699637121553,
        "mdate": 1699637121553,
        "license": "CC BY 4.0",
        "version": 2
    }
]