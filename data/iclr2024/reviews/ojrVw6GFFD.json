[
    {
        "id": "YSKoZwrzDa",
        "forum": "ojrVw6GFFD",
        "replyto": "ojrVw6GFFD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4010/Reviewer_5CVc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4010/Reviewer_5CVc"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of federated learning from clients with highly heterogeneous data distribution and small datasets. To achieve this problem, the paper propose a PFL algorithm named PAC-PFL for learning probabilistic models within a PAC-Bayesian framework. The PAC-PFL learns a shared hyper-posterior in a federated manner, which clients use to sample their priors for personalized posterior inference. Both theoretical analysis and empirical results are provided to show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper addresses a lot of issues for PFL, such as small datasets, highly heterogeneous data distribution, uncertainty calibration and new clients, which are critical problems.\n2. The paper extensively provides both theoretical analysis and empirical results to show the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. The studied problems are not well-driven and illustrated. For example, the descriptopm of uncertainty calibration is insufficient which is unfriendly for new readers. The studied issues, such as small datasets, highly heterogeneous data distribution, uncertainty calibration and new clients, should be further organized and summarized.\n2. The novelty of the proposed method PAC-PFL is limited, since it seems that the PAC-PFL only combine some techniques, such as PAC-Bayesian, FedAvg and SVGD.\n3. The related works[1-3] for tackling uncertainty calibration and FL for small datasets are omitted. \n[1] Guo C, Pleiss G, Sun Y, et al. On calibration of modern neural networks. ICML, 2017: 1321-1330. \n[2] Minderer M, Djolonga J, Romijnders R, et al. Revisiting the calibration of modern neural networks. NeurIPS, 2021, 34: 15682-15694.\n[3] Fan C, Huang J. Federated few-shot learning with adversarial learning. WiOpt, 2021: 1-8."
            },
            "questions": {
                "value": "1. For experiments results, almost all FL methods usually outperform the Pooled GP baseline, which is strange and should be further explained.\n2. In experiments results of tables and figures, the reported baselines are generally different with each other, which is confusing.\n3. The studied problems are not well-driven and illustrated. For example, the description of uncertainty calibration is insufficient which is unfriendly for new readers. The studied issues, such as small datasets, highly heterogeneous data distribution, uncertainty calibration and new clients, should be further organized and summarized.\n4. The novelty of the proposed method PAC-PFL is limited, since it seems that the PAC-PFL only combine some techniques, such as PAC-Bayesian, FedAvg and SVGD."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698483223124,
        "cdate": 1698483223124,
        "tmdate": 1699636363253,
        "mdate": 1699636363253,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "p1EolWkGVK",
        "forum": "ojrVw6GFFD",
        "replyto": "ojrVw6GFFD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4010/Reviewer_HvmF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4010/Reviewer_HvmF"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the personalized federated learning (PFL) through the lens of hierachical PAC-Bayes, similar to a previously studied PAC-Bayesian framework for meta-learning. a hyper-posterior is learned by a data-indpendent hyper-prior and the data from all clients, and a personalized posterior for each client is learned by a data-dependent prior sampled from the hyper-posterior and that client's data. To handle the data-dependence of the prior, an assumption on differential privacy is made and verified for optimal hyper-posterior, which is a Gibbs distribution. Based on this framework, a PAC-PFL algorithm is then proposed that updates the hyper-posterior is updaetd via SVGD."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Personalized federated learning is an important task, and as far as I can tell, the technical results are sound.\nThe experimental results show the proposed algorithm have better personalized performance."
            },
            "weaknesses": {
                "value": "There is no particular weakness of the paper. \nThe algorithm presented in the paper has too little details, especially how is the hyper-prior/posterior formulated (Gaussian distribution over the parameters?), make it less readable without knowledge of Rothfuss et al.\nThough in applications, it is intractable to acheive the optimal Gibbs hyper-posterior, and thus lead to some concerns of the theory. I guess it is still possible to claim differential privacy for finite number of SVGD udpates?"
            },
            "questions": {
                "value": "Plese see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698548291196,
        "cdate": 1698548291196,
        "tmdate": 1699636363145,
        "mdate": 1699636363145,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TF2jVncZKd",
        "forum": "ojrVw6GFFD",
        "replyto": "ojrVw6GFFD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4010/Reviewer_QnoR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4010/Reviewer_QnoR"
        ],
        "content": {
            "summary": {
                "value": "Personalized Federated Learning (PFL) tailors a global model to individual clients' data, especially useful for diverse clients. To overcome challenges in PFL with limited client data, PAC-PFL is introduced. PAC-PFL employs a PAC-Bayesian framework and differential privacy, collaboratively learning a shared hyper-posterior while preventing overfitting through a generalization bound. Empirical tests on heterogeneous datasets confirm that PAC-PFL delivers accurate and well-calibrated predictions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. PAC-PFL introduces a systematic, non-heuristic regularization of the hyper-posterior, allowing for the training of complex models without falling into overfitting.\n2. This  approach accommodates the accumulation of fresh data over time.\n3. It can be interpreted through Jaynes' principle of maximum entropy\n4. Experiments confirm PAC-PFL's accuracy in heterogeneous and bimodal client scenarios, along with its ability for efficient transfer learning from small datasets."
            },
            "weaknesses": {
                "value": "1. As for baslines, only 1 and the latest of them are proposed in 2022, methods that were proposed in 2023 should also be considered.\n2. One dataset seems not enough for demonstrate the scalability and generalization of the proposed framework.\n3. The theoritical analysis is pretty solid. However, the experiments are not convincing and strong enough in contrast."
            },
            "questions": {
                "value": "1. More state-of-the-arts methods should be included in the experiment.\n2. More datasets should be performed on to illustrate the generalization of proposed framework.\n3. The percentage of experiment should be increased compared with the theoritical analysis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698716608236,
        "cdate": 1698716608236,
        "tmdate": 1699636363059,
        "mdate": 1699636363059,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HoJKRoqRbK",
        "forum": "ojrVw6GFFD",
        "replyto": "ojrVw6GFFD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4010/Reviewer_D4J2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4010/Reviewer_D4J2"
        ],
        "content": {
            "summary": {
                "value": "This paper developed a PAC-Bayes framework for personalized federated learning. The PAC-PFL algorithm imposes a hyper-prior and a hyper-posterior on the server level. Based on a theoretical analysis (Theorem 4.1) similar to that of the client level (Theorem 3.1), the optimal hyper-posterior has a closed-form solution (Corollary 4.2.1). The final algorithm is based on several approximations (see Sec.5). Empirical studies on regression and classification problems show that PAC-PFL can outperform existing methods, especially when the sample size is small."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. An algorithm developed from a theoretical perspective\n\n2. Reasonable empirical results\n\n3. Writing is mostly clear"
            },
            "weaknesses": {
                "value": "1. The reason for using two samples per the client remains unclear. The algorithm requires two samples $S_i$ and $\\tilde{S}_i$ as mentioned in the first paragraph of Sec.3. However, what they are used for specifically is not very clear. For example, in (8), shouldn\u2019t the first $S_i$ be $S_i\\cup\\tilde{S}_i$ while the second one be $\\tilde{S}_i$?\n\n2. The computation complexity of the algorithm can be very high. For approximating the optimal hyper-posterior using a set of priors (see Sec.5), the communication overhead is increased from one to k, which can be unbearable for large models. It would be useful to see an ablation study on the choice of k.\n\n3. Some experiment details require clarification\n\n    3.1. we can see that PAC-PFL even outperforms the Pooled method in Tables 2 & 3, which is surprising as Pooled is essentially an oracle. Additional explanation would be helpful. Also, the Pooled method is missing for FMNIST.\n\n    3.2. It is common to use Dirichlet partition (Marfoq et al., 2021, Wang et al., 2020) for other image datasets to simulate heterogeneous clients, so it would be more comparable to other baselines if the paper can include such an experiment.\n\nRef:\n- Marfoq, O., Neglia, G., Bellet, A., Kameni, L. and Vidal, R., 2021. Federated multi-task learning under a mixture of distributions. Advances in Neural Information Processing Systems, 34, pp.15434-15447.\n- Wang, H., Yurochkin, M., Sun, Y., Papailiopoulos, D. and Khazaeni, Y., 2019, September. Federated Learning with Matched Averaging. In\u00a0*International Conference on Learning Representations*."
            },
            "questions": {
                "value": "Please clarify the questions mentioned in the Weaknesses section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813489531,
        "cdate": 1698813489531,
        "tmdate": 1699636362928,
        "mdate": 1699636362928,
        "license": "CC BY 4.0",
        "version": 2
    }
]