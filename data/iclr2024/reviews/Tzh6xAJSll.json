[
    {
        "id": "OX3YBzduQI",
        "forum": "Tzh6xAJSll",
        "replyto": "Tzh6xAJSll",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5072/Reviewer_vzgF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5072/Reviewer_vzgF"
        ],
        "content": {
            "summary": {
                "value": "The paper studies scaling laws in a simple, linear associative memory model. The model is aimed at capturing trends in LLMs, which use similar mechanisms. The authors derive scaling laws for the error of the model under varying amounts of data and memory capacity. The results reveal an optimal method of storing information in the model to minimize error. Next, the authors demonstrate that memorizing via gradient updates can be modeled in the framework derived earlier, and show trends of error with respect to learning rate and batch size. The authors finally discuss some additional considerations related to optimization, layer normalization and learned embeddings."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "**Originality**\nThe approach taken by this paper is novel, and contrasts with perspectives in prior papers that focus on continuous inputs. What is particularly notable is that the paper considers both errors arising from finite data size and finite capacity and derives relatively explicit expressions for each of these.\n\n**Quality**\nAlthough the proposed model is relatively simple, the theoretical analysis in the paper is extensive as evidenced by Appendix A. The theoretical statements are also backed up by experiments where applicable.\n\n**Clarity**\nThe paper is mostly adequately well-written. Figures are well illustrated. The notation is well chosen.\n\n**Significance**\nThe paper appears relatively significant to the field of associative memory. Although it considers a relatively simple memory model, the analysis is quite extensive and could be used in future work. Moreover, connections are drawn to practical LLMs which greatly improves the paper's relevance."
            },
            "weaknesses": {
                "value": "In my view, the main weaknesses of the paper are related to its clarity. The paper is quite dense with theoretical results, which is good in that the authors provide many contributions. On the other hand, it makes it difficult to interpret and contextualize the results. I would suggest that the authors use more space discussing their results and interpretation, and move some theoretical results to the supplement. One possibility to consider might be adding an extended discussion subsection at the end of each of sections 3 and 4.\n\nAnother point of weakness is the description of related work; it would be ideal to significantly expand this section, particularly with respect to the theory on associative memory models. It may be helpful to highlight key results in the associative memory and scaling laws in the related work section (e.g. results on the capacity of other associative memory models, scaling laws for LLMs). This is important to establish the significance of the results in this paper relative to prior work.\n\nOne key assumption in the paper is that inputs take discrete values, and that unseen input values lead to errors. It would be helpful to further discuss the realism of the assumption. In particular, when inputs are continuous-valued, we may expect generalization to unseen input values that are similar to previously seen values. When is it (or is not) reasonable to expect this kind of generalization in the discrete setting?\n\nFinally, it would be worth discussing in further detail what the key gaps remain from using the theory developed in this paper to explain scaling in actual, practical LLMs (e.g. what remaining architectural features of LLMs prevent the theory from applying to them).\n\n**Minor Comments**\n\nThe placement of figures is sometimes far from where they are referenced in the text\n\nIt is unclear what the error margins in figures 3 and 4 represent\n\nThe trends in Figure 7 are difficult to interpret due to the variation- it would be ideal to plot an average of many trials\n\nAdding some additional models to Table 1 could be helpful; it might not be worth having a table here if there are only two rows\n\nThe log scaling symbol in equation 9 is not formally defined in the main text"
            },
            "questions": {
                "value": "What are the key differences between this work and related work? What are the scaling results for similar memory models that have been previously proposed?\n\nWhat is the practical significance of having discrete input values? How does this affect how one may consider generalization to unseen inputs?\n\nWhat key gaps remain between the model considered and practical LLMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5072/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698176977435,
        "cdate": 1698176977435,
        "tmdate": 1699636497799,
        "mdate": 1699636497799,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "80BB2VsQYE",
        "forum": "Tzh6xAJSll",
        "replyto": "Tzh6xAJSll",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5072/Reviewer_fK8b"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5072/Reviewer_fK8b"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an extensive rigorous theoretical analysis of the associative memory capacity of simplified transformer layers. Associative memories are formalized as cross products of input and output tokens, which are assumed to be associated deterministically in the analysis. These associations are combined in the key matrix of an attention layer with hard argmax attention. The analysis is carried out assuming that the input tokens follow a Zipp law, which is commonly observed in naturalistic data. \n\nFinally, the paper contains a numerical analysis of SGD leaning in these layers and it provides some recommendations for Transformers training."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I must start by stating that I am not very familiar with the kind of proofs given in the paper and I did not have the time to study them in detail. Therefore, my judgment is conditional on the validity of the statements.\n\nI found the analysis to be useful as it offers a detailed theoretical view on an essential component of modern language models. While the paper makes several simplifications, I do think that the resulting model captures several of the main components of commonly used attention layers.\n\nAll in all, I do think that research of this kind is highly needed toi bridge the gap between our understanding of language models and our ability to use them. While this paper is just a small step in this direction, I do think that it is a much needed one. In particular, I highly appreciated the focus on token memorization as the phenomenon seems to be behind most of the capabilities of generative models."
            },
            "weaknesses": {
                "value": "- Some of the assumptions are rather strong and it is therefore unclear if the insights will generalize to more realistic scenarios. In particular, deterministic associations are rare in real data.\n\n- There is some evidence on the importance of lower weighted components of the attention blocks in the performance of Transformers, which are entirely ignored in the hard argmax model.\n\n- While I do think that the theoretical analysis is insightful, I am not sure that the result of the SGD experiments on the simplified model can cast much insight on actual Transformer training. In fact, the recommendation of small batches and larger step sizes seem to be the opposite of what is known to work in large architectures."
            },
            "questions": {
                "value": "- Is it possible to extend the analysis to probabilistic associations?\n- Is it possible to analyze the softmax model, or is the hard softmax assumption central to the tractability of the model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5072/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5072/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5072/Reviewer_fK8b"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5072/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698422843331,
        "cdate": 1698422843331,
        "tmdate": 1699636497706,
        "mdate": 1699636497706,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Db1Hewjxcl",
        "forum": "Tzh6xAJSll",
        "replyto": "Tzh6xAJSll",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5072/Reviewer_fGhC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5072/Reviewer_fGhC"
        ],
        "content": {
            "summary": {
                "value": "The authors provide scaling laws for the error of a specific model of associatice memory (it takes inputs x and predicts outputs y, which deterministically depends on x) in terms of the strategy of the construction of this models parameters, the number of input-output pairs seen, and the distribution of the input tokens.\nThey further experiment how optimized, rather than prescribed weights, relate to the error scaling recovered in the theory. They investigate how several specific architectural and optimization choices affect this error in practice.\n\n---- Update ----\nThanks to the authors clarifications during the rebuttal, my confusion got cleared up. I now understand the paper to be not only an interesting theoretical contribution, but also one that belongs in this context. During our discussion we converged on the points that lead to my misunderstanding, and the authors intend to improve some aspects in the camera-ready version. In the light of this, I improved my score and think that modulo the changes the authors promise, the paper should be accepted."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The derived results look interesting in the context of the chosen model and construction of its parameters.\n- Looking at discrete data with a real-word-like distribution is a promising idea.\n- I think re-framed in the correct context, the result could add nice insights to scaling laws, even though in their current presentation they are more confusing than insightful."
            },
            "weaknesses": {
                "value": "- While the introduction and title suggest that the paper considers the memory capacity of associative memories, it seems that in fact it is investigating the error scaling laws of a specific learning problem, where a discrete input x determines an output y. The suspicion that this is learning, is corroborated by the  fact that giving more data for a fixed dimension (e.g. Fig.3 right) improves the error. If the model was truly memorizing, eventually there would be a cut-off and no new data could be taken up by the model for a fixed dimension d (as there is in Hopefield networks, the original 'associative memories'). Under the present title and introduction I would expect scaling laws of the memory capacity in terms of the input parameters, and this is not what the paper is giving. This is the main weakness of the paper; that the motivation, theory and experiments do not form a coherent line of arguments which improve understanding of associative memories and their memorization capacity.\n- I find it difficult to comment on the results of the paper in the light of this mismatch, for me, the stated goal to investigate \"[...] how different key elements in the training of a transformer influence storage in our memory model.\" which motivates the experimental section, is not answered at all. \n- I want to note that I would be happy to read a rebuttal about why the authors believe their theoretical and empirical analysis is connected to memory capacity as discussed in Figure 2 - it could be that I am missing a piece. Otherwise, I think the results, stated differently, could still be useful to the community, but this would require a complete revision of the paper's motivation and contextualization."
            },
            "questions": {
                "value": "Abstract\n- 'We derive precise scaling laws with respect to sample size and parameter size,' -> it seems there is a subejct missing \"We derive precise scalings laws of quantity XY with respect to ....\"\n\nSection 1\n- It would be nice to give an example of a 'behaviour' of models that can be accessed with scaling laws.\n- what is the criterion to qualify a scaling law as 'improved'?\n- what is exactly meant by a 'statistical rate' in the present context?\n- 'theoretical schemes' -> theoretical predictions?\n- 'based on specific' -> 'for specific'?\n\nSection 2\n- 'number of data *samples* '\n- 'The first/second ones' seems like a wrong english construction of mixing singular and plural.\nSection 3\n-  as is the case at initialization -> of a neural network/transformer?\nSection 4\n- m = 5. -> M = 5?\n\nFigures\nFig 5 batch one -? batch size one?\nFig 8 is it SignGD, Adam, or SGD in the plots?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5072/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5072/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5072/Reviewer_fGhC"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5072/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768885208,
        "cdate": 1698768885208,
        "tmdate": 1700725806744,
        "mdate": 1700725806744,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dArbaocGYT",
        "forum": "Tzh6xAJSll",
        "replyto": "Tzh6xAJSll",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5072/Reviewer_qN1v"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5072/Reviewer_qN1v"
        ],
        "content": {
            "summary": {
                "value": "The authors explore the behavior of a simple model for associative memory as a weighted sum of outer products of query and key vectors for tokens (matrix). In particular they provide bounds for its generalization error as the number of tokens and the encoding vector size varies and for different choices of the weights in the sum (memory scheme). \n\nMore specifically they provide scaling laws in the case memory or data is infinite (respectively for finite data or memory) and memory performance characteristics for weights that are constant or seen-data specific (frequencies). They also study optimization based learning of memorization and how training choices and hyperparameters as in transformers affect its characteristics."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The presentation is excellently organized, the notations, definitions and associated propositions and theorems are carefully stated and accompanied by clean supporting simulation plots, the cases explored make up a comprehensive and complete narrative for this interesting theoretical work."
            },
            "weaknesses": {
                "value": "- The current setup is synthetic/artificial: it is a drastic simplification of configurations found in practice, e.g. for real transformers. Although there are clear notes in the text for the potential deviations of this simplified model to a real one, it remains to be seen how well analogies hold. To this end, perhaps crisper (albeit riskier) predictions of how some of these results would translate/map to tangible observations in a real transformer would help the reader better appreciate the implications of the theoretical results."
            },
            "questions": {
                "value": "- For ranges of values for T and d  for data distributions that could map to / feed actual transformers what would be the recommended memory scheme to try in order to minimize generalization error? (This could be a high level and practical direction to the reader who seeks a brief takeaway message)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5072/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824842123,
        "cdate": 1698824842123,
        "tmdate": 1699636497528,
        "mdate": 1699636497528,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Oc77EHhYtM",
        "forum": "Tzh6xAJSll",
        "replyto": "Tzh6xAJSll",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5072/Reviewer_cFvv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5072/Reviewer_cFvv"
        ],
        "content": {
            "summary": {
                "value": "This paper performs a study of the scaling laws from the perspective of associative memory, studying the phenomena by formalizing a highly controlled experimental setting. They test this phenomena across the amount of training data and the embedding (memory) dimension of that data and observe scaling law trends that resemble those of LLMs, indicating that the conclusions drawn in this paper will likely extrapolate beyond the scope of small associative memories."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "## Admirably formalizes the scaling laws in Transformers as a memorization/memory retrieval task in Associative Memories\n\n- (+ +) The paper clearly and thoroughly defines a \"sandbox\" problem setting where we can study scaling laws (of discrete data domains, like the vocabulary tokens in NLP) using principles of Associative Memory\n- (+) The paper includes experiments using the Associative Memory sandbox to draw conclusions about good optimizers, learning rates, and batch sizes in larger models.\n- (+) Supplementary includes complete and well-organized code for all experiments in the paper.\n\n**Originality**: I am not aware of existing works studying the scaling laws from the perspective of Associative Memory.\n\n**Quality**: The paper is of high quality, though I did not read the Appendix.\n\n**Clarity**: The paper is very clear, though accessibility to the average reader could be improved.\n\n**Significance**: Medium-High -- this is another work drawing formal connections between foundation models and associative memory, providing theoretical structure to a field designed primarily by empirical results."
            },
            "weaknesses": {
                "value": "## Experiments not able to scale to large models\n\n1. (-) It took several readings to understand the experimental setup. The clarity of the paper would be improved with a small architectural diagram describing the setting.\n2. (-) To my understanding, the proposed method can only study Transformer blocks individually, not the entire Transformer as a whole (This is my understanding of Sec 4 paragraph 1: \"our model is a proxy for the inner layers of a transformer\")\n3. (-) Like 2., the proposed method does not allow words in an input sequence the ability to talk to each other, which is how the attention mechanism in Transformers actually works (see Question 1). Thus, the sandbox is a very limited tool to study larger language models."
            },
            "questions": {
                "value": "1. Sec 2 par 1: \n\n> \"For example, $N$ could be the number of potential sequences of fixed word length in the English language, while $M$ would be all the potential words to complete the sequence\" \n\nUnfortunately, there is no modern model that actually treats all possible sequences of a fixed word length as a single token. But a recently proposed method derives the Transformer as an Associative Memory (see [Energy Transformer](https://arxiv.org/abs/2302.07253)). Could you explain how the experimental setup could be adapted to more advanced associative memory structures that contain multiple weight matrices and allow token-token interaction?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5072/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698939450563,
        "cdate": 1698939450563,
        "tmdate": 1699636497436,
        "mdate": 1699636497436,
        "license": "CC BY 4.0",
        "version": 2
    }
]