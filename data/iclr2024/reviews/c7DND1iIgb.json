[
    {
        "id": "5gpcq1k4Ev",
        "forum": "c7DND1iIgb",
        "replyto": "c7DND1iIgb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2720/Reviewer_WYuk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2720/Reviewer_WYuk"
        ],
        "content": {
            "summary": {
                "value": "- This paper proposes a novel training-free FGVC pipeline.\n\n- The key idea is to use multiple LLMs to generate the visual concepts to represent the imae of a fine-grained category, which allows not only trainin-free but also zero-shot inference.\n\n- Besides, a novel Pokemon dataset is proposed to further foster FGVC.\n\n- Extensive experiments on multiple standard FGVC datasets and the proposed dataset show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper is well-written, easy-to-follow and well-presented.\n\n- A novel Pokemon dataset is proposed to benefit FGVC.\n\n- The proposed method is novel and interesting, which provides a new way to do training-free FGVC based on LLM.\n\n- The proposed method is experimentally better than existing state-of-the-art methods."
            },
            "weaknesses": {
                "value": "- Although the novelty and technique score is satisfactory for ICLR, a major issue is whether it is necessary to only leverage LLM for FGVC under the proposed setting. In fact, as far as the reviewer concerns, some other only learning prompt on VLM can already achieves more than 90% accuracy for zero-shot FGVC. For example:\n\n[1] Conditional Prompt Learning for Vision-Language Models. CVPR 2022.\n\n[2] Learning to Prompt for Vision-Language Models. IJCV 2022.\n\nIn fact, this issue is critical, at least from the vision community. What should be the correct role of LLM for FGVC? Should only the concepts from LLM be used for FGVC? Or it should be jointly used with visual representation to aid FGVC? Soley relying on LLM for vision tasks is in a way that the reviewer does not appreciate.\n\n- The idea to use LLM to generate concepts for FGVC, is already not new now. Some recent works implement idea:\n\n[3] Learning concise and descriptive attributes for visual recognition. ICCV 2023.\n\n- The proposed dataset also has multiple issues for publication. For example:\n\n(1) The exact number of samples, how the training and testing set is divided, and etc. Many details are missing in the main submission and the supplementary materials.\n\n(2) As the dataset is Pokemon, not real-world FGVC categories, some further questions raise: Is it able to describle the fine-grained patterns? Besides, is the LLM leart from real-world able to differentiate the fine-grained patterns on Pokemon. These issues make the contribution of FGVC to be doubted."
            },
            "questions": {
                "value": "My concerns on Q1, 2, 3 have been well addressed. Although I still have some minor concerns, it does not impede that this work has reached the accept threshold. So, I decide to improve my rating to borderline accept.\n\n%%%%%%% before rebuttal\n\nAlthough the strength seems to overwhlem the weakness, as an expert in vision and FGVC, the reviewer still holds a critical view towards this LLM based work, which may be good enough for pulibcation. Some specific questions:\n\nQ1: As there is already some good VLM solution to do zero-shot FGVC and achives nearly 90% accuracy, is it necessary to solely address FGVC by pure concepts from LLM? Besides, is it necessary to solely rely on LLM instead of VLM or visual representation for FGVC?\n\nQ2: The idea of this paper is actually not very new now. The ICCV paper [3] implements similary idea. The difference and comparison should be made and clarified.\n\nQ3: So many details of the proposed dataset are missing. The specific comments have been listed in the weakness.\n\nQ4: Is the proposed Pokemon dataset can really fit the fine-grained patterns? Or, the LLM and VLM learnt from real-world data, can really discriminate the fine-grained pokemon? These issues may make its contribution actually limited for FGVC community."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2720/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2720/Reviewer_WYuk",
                    "ICLR.cc/2024/Conference/Submission2720/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2720/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697634735417,
        "cdate": 1697634735417,
        "tmdate": 1700329978828,
        "mdate": 1700329978828,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aedDAZRLDB",
        "forum": "c7DND1iIgb",
        "replyto": "c7DND1iIgb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2720/Reviewer_3hkC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2720/Reviewer_3hkC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a fine-grained semantic category reasoning method for fine-grained visual recognition, which attempts to leverage the knowledge of large language models (LLMs) to reason about fine-grained category names. It alleviates the need of high-quality paried expert annotations in fine-grained recognition."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe presentation of this paper is clear and the proposed method is easy to follow. \n2.\tThis paper proposes to extract visual attributes from images into the large language models (LLM) for reasoning the fine-grained subcategory names, which is a promising way to alleviate the high need for expert annotations in fine-grained recognition."
            },
            "weaknesses": {
                "value": "The weaknesses are as follows\uff1a\n\n1.\tThe novelty of this paper should be further demonstrated. The proposed method seems an intuitive combination of existing large-scale models, such as the visual question answering model, large-language model and vision-language model, etc. Besides, extracting visual attributes from images for recognition is widely used in generalized zero-shot learning methods such as [a]. \n\n2.\tThe effectiveness of the proposed method should be further verified. The recognition accuracy on the CUB dataset is relatively low. Besides, existing generalized zero-shot learning methods such as [a] should be compared and more corresponding analyses should be added.\n\n3.\tThere lacks a complete recognition example for explaining the whole procedure, which should be added for better understanding.\n\n[a] Progressive Semantic-Visual Mutual Adaption for Generalized Zero-Shot Learning. CVPR 2023"
            },
            "questions": {
                "value": "The novelty should be clarified and the difference from existing generalized zero-shot learning methods using visual attributes for recognition should be explained. More corresponding experiments and analyses should be added."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2720/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698117215443,
        "cdate": 1698117215443,
        "tmdate": 1699636214313,
        "mdate": 1699636214313,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sq0zJfeGHh",
        "forum": "c7DND1iIgb",
        "replyto": "c7DND1iIgb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2720/Reviewer_oRxX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2720/Reviewer_oRxX"
        ],
        "content": {
            "summary": {
                "value": "FineR leverages large language models to identify fine-grained image categories without expert annotations, by interpreting visual attributes as text. This allows it to reason about subtle differences between species or objects, outperforming current FGVR methods. It shows potential for real-world applications where expert data is scarce or hard to obtain."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This is a good paper, and the advantages are:\n1. The utilization of LLM to vocabulary-free FGVR tasks is novel;\n2. The paper is generally well-written and easy to follow;\n3. Good performance in Table 1 \n4. The well utilization of LLM."
            },
            "weaknesses": {
                "value": "Advice:\n1. Add the citation of some highly related missing works:\n(1) Transhp: image classification with hierarchical prompting; it also focuses on the fine-grained image classification task. Also, it takes advantage of the recently proposed prompting technique in CV. Is your used LLM better or prompting better?\n(2) V2L: Leveraging Vision and Vision-language Models into Large-scale Product Retrieval; it also focuses on the vision language model and fine-grained visual recognition. The paper is the champion of FGVC 9. A discussion of comparison seems necessary.\n2. Could you show more examples of Fig. 2? Or any failure cases of Fig. 2?\n3. What is your view of NOVELTY of a work using LLMs without training anything?"
            },
            "questions": {
                "value": "I want to discuss with the authors that:\nWhat is your view of NOVELTY of a work using LLMs without training anything?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2720/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2720/Reviewer_oRxX",
                    "ICLR.cc/2024/Conference/Submission2720/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2720/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699103089112,
        "cdate": 1699103089112,
        "tmdate": 1700351634904,
        "mdate": 1700351634904,
        "license": "CC BY 4.0",
        "version": 2
    }
]