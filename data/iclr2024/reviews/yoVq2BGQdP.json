[
    {
        "id": "z7rGaZVzCf",
        "forum": "yoVq2BGQdP",
        "replyto": "yoVq2BGQdP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7562/Reviewer_TT7q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7562/Reviewer_TT7q"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a fairness function for multi-agent RL that ensures equitable rewards across agents. The authors then propose two algorithms for obtaining the optimal policy; first, they study the online RL setting in which they show that their method achieves sublinear regret in terms of number of episodes. Furthermore, they discuss a policy gradient variant of this method. Second, they propose an offline RL algorithm and bound its optimality gap. Experimental results are carried out on a very limited toy problem."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-motivated and studies an important problem in RL. \n2. The authors provide theoretical analysis of two algorithms for online and offline RL. Although I have not checked the proofs carefully."
            },
            "weaknesses": {
                "value": "1. The experimental results are very limited and cast doubts on the applicability of the proposed algorithms:\n* The toy MDPs are very small and have a short horizon.\n* The method is not compared with any other approach from the literature on fair MARL. There is basically no baseline in the experimental results. \n* Since the authors are talking about resource allocation throughout the paper and that problem setting seems to be the main goal. I was surprised to see the experiments are on some randomly generated toy MDPs. \n* I would have preferred to see the experimental results in the main body of the paper, perhaps instead of the proof outlines. \n\n2. I find the paper hard to follow, and I think both the writing and the outline of the paper needs some improvements for more clarity and coherence. Some examples are: \n* Some of the notation is cluttered and overloaded (e.g., equations 10-13) which makes the math hard to understand. \n* While I appreciate the existence of proof outlines, in their current form they are vague (at least to me). Also, I am confused by their positioning in the text; for example, proof outline of Theorem 1 is not immediately after the Theorem.\n* Some section titles are too generic without any useful information. For example Section 5 is titled \u201cAlgorithm\u201d whereas Section 6 is titled \u201cOffline Fair MARL\u201d. It makes sense to rename Section 5 to something more informative (e.g., Online Fair MARL)."
            },
            "questions": {
                "value": "1. Why is the horizon extremely short (H=3) in the experiments in Appendix G? \n2. In Figure 3, why is the offline RL algorithm significantly outperforming the online RL algorithm? I find this a bit surprising because usually online only performs better and more importantly, your proposed online RL algorithm is optimistic whereas your offline RL algorithm is pessimistic."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7562/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7562/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7562/Reviewer_TT7q"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7562/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698301355185,
        "cdate": 1698301355185,
        "tmdate": 1700720640518,
        "mdate": 1700720640518,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FxAtrxU8G4",
        "forum": "yoVq2BGQdP",
        "replyto": "yoVq2BGQdP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7562/Reviewer_KfsC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7562/Reviewer_KfsC"
        ],
        "content": {
            "summary": {
                "value": "This paper studies fair MARL. It proposes online and offline RL algorithms and their deep policy gradient counterparts to maximize any fair social welfare from the alpha-fairness class in a multi-agent MDP. To this end, the paper notices that the Bellman operator cannot be readily applied when fairness is a concern, and proposes an alternative based on the occupancy measure. The regret bounds for the proposed algorithms are derived."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- MARL is relevant to the conference; fairness is relevant to MARL.\n- The paper provides an impressive amount of theory. It also explores online and offline settings and extends RL algorithms to deep counterparts. The contributions listed in the Our Contributions section are original and significant.\n- The theory is complemented by simple experiments."
            },
            "weaknesses": {
                "value": "Despite the vast contributions, I think the paper somewhat overclaims. I suggest to rewrite some parts of the Abstract, Introduction, and Related work. I elaborate below\n\n- Fairness in (deep) RL, and in particular alpha-fairness, has been studied in multiple prior works. The Related Work section is too concise and does not give the existing literature enough merit. Example: Zimmer et al. 2021 is cited, but is claimed to focus on gini fairness. This is not true: they focus on a general class of fair SW functions, including alpha-fairness, and they have long-term fairness, and they propose a policy gradient algorithm. Another example: Ivanov et al. 2021 is missing, and they have long-term max-min fairness.\n- In abstract, \u201cthe exploration of fairness in such systems for unknown environments remains open\u201d. Yet many papers from section 2 are in model-free setting.\n- The experiments are simple. I am not sure how to express it: they are as simple as they could be. One would actually have trouble to come up with simpler experiments. Two to four states, actions, agents, and transitions in an episode, all uniformly generated. This is ok by itself for a theoretical paper: after all, simple experiments are better than no experiments. But the authors claim that \u201c... we also developed a policy-gradient-based algorithm that is applicable to large state space as well.\u201d I would not call four a large number, so there is no evidence for this claim.\n\nOther than these problems, the contribution is primarily theoretical and I only skimmed the theory. I lower my confidence score to reflect this."
            },
            "questions": {
                "value": "- Given my comment about the relation to literature, would it perhaps be more fair (pun not intended) to frame the paper as theoretically justifying existing approaches and proposing their new implementations rather than proposing new approaches?\n- End of 5.1: why is the policy defined stochastically instead of deterministic argmax_a q? Wouldn\u2019t the deterministic policy maximize social welfare?\n- Isn\u2019t the online algorithm in 5.2 a variation/extension of UCB for bandits?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7562/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7562/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7562/Reviewer_KfsC"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7562/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698482408182,
        "cdate": 1698482408182,
        "tmdate": 1700653532093,
        "mdate": 1700653532093,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "G951ww7Kx5",
        "forum": "yoVq2BGQdP",
        "replyto": "yoVq2BGQdP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7562/Reviewer_o4PJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7562/Reviewer_o4PJ"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to address the issue of fairness in multi-agent systems and introduces a fairness function that ensures equitable rewards across agents.\nThe authors propose an online convex optimization-based approach to obtain a policy constrained within a confidence region of the unknown environment.\nAdditionally, the authors demonstrate that their approach achieves sub-linear regret in terms of the number of episodes and provide a probably approximately correct (PAC) guarantee based on the obtained regret bound.\nFurthermore, the authors propose an offline RL algorithm and bound the optimality gap concerning the optimal fair solution."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is theoretically grounded and introduces fairness algorithms for both online and offline settings. In the online setting, the authors establish a regret bound with a Probably Approximately Correct (PAC) guarantee. \nIn the offline setting, the authors initially demonstrate the suboptimality of the policy.\nOverall, the paper marks a promising start for bringing fairness into Multi-Agent Reinforcement Learning."
            },
            "weaknesses": {
                "value": "All concerns relate to the practicality of the method:\n\n1. While the regret bounds are proven with a PAC guarantee, the cardinality of S is often huge in real-world settings, making it difficult to use.\n\n2. Estimating the empirical average of p and r is often challenging in multi-agent settings, and the confidence interval may be unreliable.\n\n3. The occupancy measure is based on the frequency of appearance for each state-action pair, which may be biased and difficult to count since we cannot always experience the entire environment."
            },
            "questions": {
                "value": "1.\tCould you please provide some examples to demonstrate the effectiveness of fairness in a multi-agent system?\n2.\tI cannot understand what the experiment in the appendix demonstrates. Could you please provide a detailed description\uff1f(or how each agent can benefit from the fairness algorithm?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7562/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7562/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7562/Reviewer_o4PJ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7562/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698636661870,
        "cdate": 1698636661870,
        "tmdate": 1699636915156,
        "mdate": 1699636915156,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YxlAsxfifv",
        "forum": "yoVq2BGQdP",
        "replyto": "yoVq2BGQdP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7562/Reviewer_uKyX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7562/Reviewer_uKyX"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the concept of fairness in multi-agent systems in unknown environments, casting the problem as a multi-agent Markov Decision Processes (MDPs) with a fairness-related objective function. The authors propose a Reinforcement Learning (RL) approach that maximizes fairness objectives such as Proportional Fairness and Maximin Fairness. When representing the policy in terms of the visitation measure, this paper shows that the optimal policy can be solved via convex optimization in the space of visitation measures. Based on this observation, a UCB-based online RL algorithm and a pessimism-based offline RL algorithm are proposed."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The fairness metric in multi-agent RL has not been extensively studied. This paper extends the UCB and Pessimism, two standard techniques of online and offline RL to this new problem."
            },
            "weaknesses": {
                "value": "1. Novelty. It seems that the main novelty of this paper is showing that the optimal fairness-aware policy can be solved by convex optimization in the space of visitation measure. Moreover, the function $F$ is Lipschitz and monotone in each coordinate. Based on this observation, one can easily extend the performance-difference lemma for standard MDP to this new problem. \n\n2. The setting seems a bit restrictive. In particular, this paper considers the centralized setting and assumes all the $N$ agents have the same action $a$. This makes the problem no different from a single-agent MDP with $N$ different reward functions. Moreover, the assumption that the reward function is bounded from below by $\\epsilon/ H$ seems an unnatural assumption. \n\n3. It seems unclear whether the fairness notions studied in this work include most of the common fairness metrics. For example, existing works also study Generalized Gini Social Welfare Function in the context of MARL."
            },
            "questions": {
                "value": "1. Given existing works on UCB-based and pessimism-based RL, what are the technical novelties?\n\n2. Can you run some toy-ish experiments to showcase the performance of the algorithm? In simulation experiments, how do we measure fairness-related regret?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7562/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698735876497,
        "cdate": 1698735876497,
        "tmdate": 1699636915009,
        "mdate": 1699636915009,
        "license": "CC BY 4.0",
        "version": 2
    }
]