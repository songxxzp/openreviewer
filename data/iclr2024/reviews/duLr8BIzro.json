[
    {
        "id": "vEiSYLJ5Y2",
        "forum": "duLr8BIzro",
        "replyto": "duLr8BIzro",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6167/Reviewer_Utmf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6167/Reviewer_Utmf"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on a good question, i.e., the scalability of Graph Transformers (GTs). GTs suffer from quadratic complexity when the node number in a certain graph is very large. The authors propose a somewhat improvement of GTs. An improved (the authors define it as an alternative) dense attention mechanism is utilized to reduce the computing complexity of GTs. It is claimed that the proposed GECO can capture long-range dependencies. The proposed method shows some improvements in a limit number of datasets. The appendix is of a lot content including details of experimental settings, related work, a brief discussion of computational complexity discussion, etc."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-This paper focuses on a good and significant research question and poses a very smart improvement by modifying dense attention mechanisms of GTs.\n\n-This paper includes rich contents. An additional appendix containing many details that can help to clearly understand the paper.\n\n-The proposed method shows some acceptable experimental results when comparing with baselines and good ablation experiments."
            },
            "weaknesses": {
                "value": "-The innovation is not strong enough, which diminishes the significance of the paper. Essentially, the authors replaced the multi-head self-attention module in the original Transformer with a global convolutional module, and then they claim their proposal is to improve efficiency. However, the necessity of this replacement needs to be considered, and it appears to be of limited significance. The primary issue lies in the absence of self-attention mechanism, resulting in a diminished capability to capture long-range dependencies. Experimental results on large datasets, such as PCQM and COCO, indicate that the model's performance is inferior to other Graph Transformer methods.\n\n-Lacking experimental results to verify \u201cfast\u201d of the proposed method. Specifically, there is no emphasis in the experimental results, no parameter complexity analysis, no comparison of computing resource consumption or computing time. These are fundamental experiments in verifying \u201cfast\u201d of a certain method. And the results provided to demonstrate the 'effectiveness' of the proposed method in capturing long-distance dependencies, as shown in Tables 1, 2, and 4, may not offer sufficiently strong evidence for its superior performance. Overall, the title of this paper is ambitious and likely to capture attention with insufficient innovative approach, even though the authors claimed \u201cthey are the first to\u201d.\n\n-The design, organization, and writing of this paper are not very clear to me. Firstly, the motivation seems to enhance GT, but the authors care a lot about capturing long-range dependencies, which I have illustrated in last point, the results are not impressive enough. If the authors want to show the outperformance of trade-off between capturing long-range dependencies and fast calculation/computation, there is a lack of comparison of baselines including those methods not using Transformers. Then, If the authors want to show the improvement of the enhanced GT in effectiveness, the results are not competitive. And I think the authors also need to refer to some recent studies such as \u201cHierarchical Transformer for Scalable Graph Learning\u201d. Next, the authors aim to illustrate that their proposed method is fast and has distinct difference from GraphGPS. But why GraphGPS? It confuses me."
            },
            "questions": {
                "value": "-What exact problem the authors want to solve? And how you directly verified that the problem is well solved, with what metric/way? \n\n-How to balance the trade-off between fast and efficiency? Why is your method the best?\n\n-I am well aware that the comparison of computation complexity (theoretically) among several models including GECO. But what about the experimental verification?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6167/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6167/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6167/Reviewer_Utmf"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6167/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698322533370,
        "cdate": 1698322533370,
        "tmdate": 1700635655757,
        "mdate": 1700635655757,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sjDuoUS62P",
        "forum": "duLr8BIzro",
        "replyto": "duLr8BIzro",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6167/Reviewer_vPHt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6167/Reviewer_vPHt"
        ],
        "content": {
            "summary": {
                "value": "This paper targets the quadratic complexity issue in training graph transformers with full-attention over large graph datasets, and proposes GECO, which is a Hyena-based operator that captures both local and global dependencies to replace the original attention operator. The authors conduct extensive experiments in demonstrating GECO\u2019s effectiveness over long-range and large graph datasets. In addition, the authors empirically demonstrate GECO\u2019s insensitivity to node ordering wrt. performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe targeted quadratic complexity issue reside in graph transformers is meaningful. The designed GECO not only makes it sub-quadratic, but also remains in a considerable performance level.\n2.\tThe experiments are conducted extensively. The results seem promising."
            },
            "weaknesses": {
                "value": "1.\tThe presentation of this paper may be improved for coherence. For example, in Sec. 3.3, the GCB module is designed/modified based on Hyena, the key component for sub-quadratic complexity. The authors may want to include a short description of it in the main context rather than the appendix. Otherwise, it may introduce difficulties in comprehension. In addition, the proposition in the main context assists in analyzing the complexity, which is presented in the appendix. It seems like they can be excluded from the main context.\n2.\tThe motivation is to make the model parameters sub-quadratic to the number of nodes. While theoretical analysis is conducted, I would like to see empirical results (e.g., training time) in GECO\u2019s training efficiency compared with other baselines."
            },
            "questions": {
                "value": "1.\tIn the Graph-to-sequence conversion part, the authors state \u201ctime-correlated sequences, aligning node IDs with time (t)\u201d. Where does the \u2018time\u2019 come from? What does it mean?\n2.\tThe LCB module conducts neighborhood information propagation for each node. It directly utilizes the connectivity information via adjacency matrix. In the meantime, GECO is implicitly learning this \u2018connectivity\u2019 via the convolutional filters. Is there any information overlap here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6167/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6167/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6167/Reviewer_vPHt"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6167/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698546422362,
        "cdate": 1698546422362,
        "tmdate": 1699636669679,
        "mdate": 1699636669679,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RavV926M4o",
        "forum": "duLr8BIzro",
        "replyto": "duLr8BIzro",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6167/Reviewer_XpCe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6167/Reviewer_XpCe"
        ],
        "content": {
            "summary": {
                "value": "This article proposes a new operator-GECO to replace the graph transformer to solve the computational complexity problem of MHA (multi-head attention) on large-scale graphs. GECO introduces the Hyena architecture into graph convolution calculations, using a combination of long convolutions and gating to compute local and global context. Subsequent experiments have proven that GECO can ensure accuracy while reducing time complexity, on large-scale and small-scale data sets. The main contributions of the article are 1. There is no trade-off between quality and scalability while ensuring both; 2. It confirms that the Hyena architecture can replace MHA in graph neural networks, and global context can improve the performance of GNN."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "A new operator-GECO to replace the graph transformer to solve the computational complexity problem of MHA (multi-head attention) on large-scale graphs."
            },
            "weaknesses": {
                "value": "- The technical contribution is limited. According to  this survey [2], the proposed LCB module can be treated as the GNN-as-Auxiliary-Modules in the Alternatively form (Figure 1 in [2]). Additionally, the writing of this paper seems rushed. Many details are missing and hard to understand. For example, in Algorithm 1, line 3, what is $V_t \\leftarrow (P)_t FFTConv(F_i, V)_t$. Actually, I found more details of this algorithm in Algorithm 3,  page 8, [link](https://arxiv.org/pdf/2302.10866.pdf)[1]. The forward pass of GCB Operator is nearly identical to Hyena, which is not new. \n\n- Using positional  embedding to encode the graph structural information is not new. \n\n- The paper claims that the proposed model is \"fast\", and provides detailed time complexity analysis. Unfortunately, from the theoretical perspective, GECO has the same level complexity as Message-passing GNN $O(NlogN+M)$ and it can only surpass vallia transformer when $M<<N^2$. Additionally, no experiments regarding the running time efficiency are presented. \n\n- It's necessary make more comparisons with more baselines of Graph Transformer. Please refer to [2] for more baselines. \n\n[1] Hyena Hierarchy: Towards Larger Convolutional Language Models\n[2] Transformer for Graphs: An Overview from Architecture Perspective"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6167/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698904934592,
        "cdate": 1698904934592,
        "tmdate": 1699636669572,
        "mdate": 1699636669572,
        "license": "CC BY 4.0",
        "version": 2
    }
]