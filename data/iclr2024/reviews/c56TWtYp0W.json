[
    {
        "id": "1FbOoyCKlH",
        "forum": "c56TWtYp0W",
        "replyto": "c56TWtYp0W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8459/Reviewer_MEmY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8459/Reviewer_MEmY"
        ],
        "content": {
            "summary": {
                "value": "In this paper, authors present a novel approach for learning spatiotemporal structure and using it to improve the application of transformers to timeseries datasets. A key aspect of contributions lies in the creation of a specialized group embedding scheme designed specifically for transformer architectures. This scheme enables the adaptive learning of concise grouping tokens, which encompass both channel and temporal dimensions. By incorporating group-aware structural information into the representation space, GAFormer enhances the overall understanding and encoding of data patterns."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.The method proposed by the authors is intuitive and validated on multiple datasets, and the experimental results prove the effectiveness of GAFormer.\n\n2.GAFormer achieves the adaptive discernment of channel-wise and temporal groupings without relying on any predefined structure or additional supervision beyond classification or regression tasks. Through the assignment of group embeddings to tokens, GAFormer enhances the interpretability of the model's representations."
            },
            "weaknesses": {
                "value": "1.The GAFormer goes through the SGE and then the TGE, what happens if the order of the two modules is reversed? Why is it modeled from the spatial point of view first?\n\n2.Can the author explain Figure 4 again, I don't really understand the interpretability of the authors' proposal.\n\n3.Table 4 shows that SGE is more effective than TGE, can the authors explain why?"
            },
            "questions": {
                "value": "See Weaknesses for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8459/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698563405500,
        "cdate": 1698563405500,
        "tmdate": 1699637055769,
        "mdate": 1699637055769,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PbjNqlVAZl",
        "forum": "c56TWtYp0W",
        "replyto": "c56TWtYp0W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8459/Reviewer_Hkvr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8459/Reviewer_Hkvr"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel technique for improving the application of transformers to timeseries datasets by learning spatiotemporal structures. The proposed framework employs group tokens and an instance-specific group embedding layer to integrate structure into the learning process. The newly devised Group-Aware transFormer (GAFormer) demonstrates superior performance across various timeseries tasks, offering enhanced interpretability and the ability to discern latent structures in intricate multivariate datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of using *groupe mbedding*(GE) technique to learn both spatial and temporal structure in multi-variate timeseries datasets is novel.\n- The proposed model achieves the state-of-the-art performance on a number of time- series classification and regression tasks"
            },
            "weaknesses": {
                "value": "Though the approach claims to offer \"a more interpretable decomposition,\" this is a subjective claim. The degree of interpretability might vary based on the user or the specific use case."
            },
            "questions": {
                "value": "- How to tokenise the MTS data?\n\n- What's the different between a text tokeniser and a MTS tokeniser?\n\n- How to determine K? \n\n- What's the effect of different K?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8459/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698620852042,
        "cdate": 1698620852042,
        "tmdate": 1699637055644,
        "mdate": 1699637055644,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9S4NdqYGdt",
        "forum": "c56TWtYp0W",
        "replyto": "c56TWtYp0W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8459/Reviewer_cXyc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8459/Reviewer_cXyc"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method to learn PEs for transformers in the multivariate temporal data setting. Specifically, the authors propose a Group Embedding (GE) to capture the group aware dynamics along the channels and also the temporal dimension. This Group Embedding is then induced into the transformer architecture called GAFormer to learn the spatial and temporal structure of the multivariate time series datasets. Experimental results on the synthetic dataset show the robustness of the proposed GE over baseline PEs. Moreover, when GE is induced in other transformer architectures for the univariate datasets the performance is enhanced. GAFormer also achieves SoTA performance on the multivariate datasets tested. The authors also claim the interpretability of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Promising PE for multivariate time-series data. The PE show improved performance over the tasks.\n- The proposed GAFormer also achieves SoTA performance on the multivariate datasets tested."
            },
            "weaknesses": {
                "value": "- Since the model is designed to learn the group structure in a data-dependent manner, it may not be able to capture these properties effectively in the low data regime as the baseline PEs would.\n- The group structure exhibited in Figure 4 is claimed to provide interpretability of the model. However, these are some groups that have been learnt and the interpretability aspect is not clear as to what it means if some channels or time signals are grouped. This is similar to the groups formed in any attention mechanism and the method doesn\u2019t seem to provide any added explanations or interpretability to the decisions. Thus it may not be right to claim interpretability based on group embedding structure."
            },
            "questions": {
                "value": "- The authors don\u2019t seem to have mentioned the computational complexity of the method in the paper. From the description, it seems to inherit the quadratic computational complexity of the transformer which is prohibitive for large timesteps and multi chaneled data\n- The group structure exhibited in Figure 4 is claimed to provide interpretability of the model. However, these are some groups that have been learnt and the interpretability aspect is not clear as to what it means if some channels or time signals are grouped. Further analysis and explanation by the authors may help.\n- As the authors have mentioned other mechanisms (spot attention etc.) to induce group structure, I would expect some empirical study to compare the proposed method with these PEs. Specifically, is the proposed method a unique way to induce PEs for multivariate time series forecasting or could other techniques provide similar or better performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8459/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8459/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8459/Reviewer_cXyc"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8459/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698800417925,
        "cdate": 1698800417925,
        "tmdate": 1699637055496,
        "mdate": 1699637055496,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rpQoKtw8P8",
        "forum": "c56TWtYp0W",
        "replyto": "c56TWtYp0W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8459/Reviewer_q3iU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8459/Reviewer_q3iU"
        ],
        "content": {
            "summary": {
                "value": "The paper tries to improve the transformers to time-series datasets by learning spatiotemporal structure. Specifically, the authors introduce an instance-specific group embedding layer that assigns input tokens to a small number of group tokens to incorporate structure into learning. Based on the group embedding layer, they introduce the GAFormer to incorporate both spatial and temporal group embeddings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper studies the multivariate time series which is an interesting and important problem.\n2. The paper provides a detailed literature review and\n3. The paper discusses its limitations.\n4. The paper provides extensive experimental results to demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1, The proposed methodology is not well motivated and built on the existing Transformer. The paper tries to introduce the group embedding layer but does not provide clear motivation for the group embedding, i.e. how to define the \"group\" and why we need the group embedding.\\\n2. The explanation of the group embedding is vague. It is not easy to understand what the group embedding stands for.\\\n3. The paper aims to learn generalizable representation within multivariate datasets. I did not find the related experiments to demonstrate the generalization ability."
            },
            "questions": {
                "value": "1. The authors claim that the group embedding layer can assign input tokens to a small number of group tokens to incorporate structure into learning. I wonder how the structure information can be incorporated using group embedding."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8459/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698902052968,
        "cdate": 1698902052968,
        "tmdate": 1699637055385,
        "mdate": 1699637055385,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "K0C3Kgqksc",
        "forum": "c56TWtYp0W",
        "replyto": "c56TWtYp0W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8459/Reviewer_Jppq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8459/Reviewer_Jppq"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel transformer architecture called GAFormer, which models the interations of spatial and temporal groups separately. The proposed model demonstrates effectiveness on both synthetic experiments and real-data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is clearly written and motivated. \n2. The group embedding is novel and very effective as shown by synthetic experiments."
            },
            "weaknesses": {
                "value": "1. The details of transformer used seems to be missing. see questions\n2. Why does this model not compare to (Nie et al.), which seems to be an important related work."
            },
            "questions": {
                "value": "1. Does this model consisted of 2 layers of transformer? SGE and TGT?\n\n2. What's the specification of these transformers? It should be stated in the main paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8459/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699495913563,
        "cdate": 1699495913563,
        "tmdate": 1699637055267,
        "mdate": 1699637055267,
        "license": "CC BY 4.0",
        "version": 2
    }
]