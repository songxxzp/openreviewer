[
    {
        "id": "1wrYsfBIg8",
        "forum": "f37TVPH62h",
        "replyto": "f37TVPH62h",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2033/Reviewer_fKNs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2033/Reviewer_fKNs"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes the variance reduction property of compound returns. While compound returns, such as the lambda-return, are often viewed as helping with variance reduction via averaging, the authors claim that this variance properties is formally investigate for the first time. Under certain assumptions on the variance/covariance model, the authors prove for the first time that any compound return with the same contraction rate as a given n-step return has strictly lower variance. The studies shed light on the theoretical understanding of using compound returns in learning value functions. Subsequently, the authors propose a computationally friendly piecewise lambda-return and verify the efficacy of their approach on one Atari Freeway environment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper considers an interesting question. While it may be commonly believed that averaging helps with variance and hence learning in RL, the authors formally study the problem and show that compound returns admit a better bias-variance trade-off. The writing is overall very clear and organized. The proposed piecewise lambda-return is theoretically sound and seems to also perform in the limited experimental evaluations."
            },
            "weaknesses": {
                "value": "While a formal study on the variance reduction property is valuable, the theoretical contributions of this paper seem limited. The assumptions help abstract a lot of the difficulties and with the uniform variance/correlation assumptions, the derivation in this paper seems to be straightforward/follow standard arguments. As such, the technical depth is limited. Consequently, for such paper with limited theoretical innovations, one might expect a more comprehensive experimental evaluations, ablation studies and comparisons. The current manuscript unfortunately only evaluates on the Atari Freeway environment."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2033/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697580444170,
        "cdate": 1697580444170,
        "tmdate": 1699636134967,
        "mdate": 1699636134967,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TsTBD36SCD",
        "forum": "f37TVPH62h",
        "replyto": "f37TVPH62h",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2033/Reviewer_Ucxg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2033/Reviewer_Ucxg"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes the widely used $\\lambda$-compounded returns and show that they have lower variance than $n$-steps return if the temporal difference errors have equal correlation strictly less than one.\n\nIn addition they propose PILAR which is a practical deep RL approximation of the TD($\\lambda$) compatible with experience replay."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper discovers few new characteristics of a very common method in RL that is TD($\\lambda$)."
            },
            "weaknesses": {
                "value": "1) I think that most of the results are slightly incremental and not clearly novel.\n\n2) Assuming that all temporal differences error have the same correlation is a strong assumption in my opinion.\n\n3) In general in RL it is not clear if minimizing the variance of the return estimators is helpful to improve the sample complexity of an algorithm. Check for example this paper investigating the role of the minimum variance baseline in policy gradient as in [1].\n\n4) The experiments in Deep RL are limited to only one environment. I think that a larger empirical evaluation is necessary.\n\n[1] Wesley Chung, Valentin Thomas, Marlos C Machado, and Nicolas Le Roux.\nBeyond variance reduction: Understanding the true impact of baselines on policy optimization"
            },
            "questions": {
                "value": "Is it possible to use the results in this paper to show more informative results regarding the performance of TD($\\lambda$). For example that having a lower variance in the returns improves the sample complexity needed for either policy evaluation or for learning an $\\epsilon$-optimal policy ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2033/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2033/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2033/Reviewer_Ucxg"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2033/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698315588108,
        "cdate": 1698315588108,
        "tmdate": 1699636134876,
        "mdate": 1699636134876,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lXRfzjOQAP",
        "forum": "f37TVPH62h",
        "replyto": "f37TVPH62h",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2033/Reviewer_U4yb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2033/Reviewer_U4yb"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the variance of compound returns in reinforcement learning both theoretically and empirically. Under the uniform covariance assumption between TD error at different steps, it proves that any compound return has lower variance than corresponding $n$-step return with the same contraction rate as long as the TD errors are not perfectly correlated. The contraction rate measures that convergence speed of a $n$-step TD estimator for value function estimation of a policy. Therefore, it concludes that compound return in general has lower variance under the same convergence rate. They also conduct experiments to verify this effect in value estimation tasks. Empirically, the paper proposes an approximation of $\\lambda$-return using only the mixture of two multi-step returns named Piecewise $\\lambda$-Return (PiLaR). Experiments with DQN on a tabular example shows the effectiveness of PiLaR on top of the standard $n$-step TD learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The variance of compound returns is a fundamental problem in reinforcement learning. This paper provides new insights on this problem by verifying the compound returns have lower variance than $n$-step returns under uniform covariance assumption. The paper clearly lists convincing theoretical and empirical evidence to support this claim."
            },
            "weaknesses": {
                "value": "1. It is unclear whether the uniform covariance assumption is reasonable in real-world problems, since the hardness to approximate the covariance between different steps should not be an evidence to support the validity of this assumption. Intuitively, the variance of TD errors at further steps should be larger since the entropy of state should increase along the diffusion over the MDP. Therefore, it is appreciated to verify this assumption empirically on synthetic examples.\n\n2. The contraction rate measures the contraction level of the policy evaluation process. It is not clear the effect of this rate in the policy optimization process, nor is it discussed in the paper. Therefore, it is still not clear whether the faster learning with DQN is a consequence of smaller variance of PiLaR or smaller contraction rate in the policy optimization process as $n_2$ is generally larger than $n$. \n\n3. The theoretical results of the paper are mostly conceptual in the sense that it proves some variance reduction results but do not discuss how this lower variance accelerate the learning of optimal policies. The \"equally fast\" claim for two estimators with the same contraction rate is also conceptual without solid evidence. Does it correspond to smaller sample complexity in theory? The insight of this paper is also limited in both practice and theory, since the baseline is the $n$-step TD learning and DQN, which is away from current SOTA algorithms used in RL. Is is possible to compare the PiLaR (or more refined compound error with even smaller variances) with some SOTA RL algorithms such PPO or CQL?"
            },
            "questions": {
                "value": "See above.\n\nEqn. (8): the second $S_t$ --> $s$ \n\nEqn. (12): missing $\\kappa$ in the RHS"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2033/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822657895,
        "cdate": 1698822657895,
        "tmdate": 1699636134809,
        "mdate": 1699636134809,
        "license": "CC BY 4.0",
        "version": 2
    }
]