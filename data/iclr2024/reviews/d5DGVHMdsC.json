[
    {
        "id": "MgXRZxvdyZ",
        "forum": "d5DGVHMdsC",
        "replyto": "d5DGVHMdsC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8220/Reviewer_7v8j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8220/Reviewer_7v8j"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a memory architecture, CLIN, that generates causal abstractions as textual memory. A memory generator takes the execution results to update the memory. This enables language-based agents to improve their capability over iterations. The inclusion of meta-memory helps generalization to unseen tasks and environments. The experiments in the ScienceWorld benchmark show that CLIN outperforms the state-of-the-art reflective agent, Reflexion."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- CLIN shows that the language agent can continually update and improve its task performance even if they do not update weights.\n- This paper shows that it is possible to use prompting to Identify the causal abstractions necessary for agents to change their behaviors or fix previous errors.\n- The experiment shows that memory design helps generalization and can efficiently solve some tasks with a few trials."
            },
            "weaknesses": {
                "value": "- The key component of CLIN is the memory abstraction. However, it is hard to understand the criteria of how to determine the necessary causal abstraction and how to assess the uncertainty of the relevant concepts. While the related prompts are provided in the appendix, it will be hard to extend them without understanding the design criteria or rationale for the memory generator. Also, the accuracy of the generated memory is not evaluated, it is unclear whether the performance improves because the LLM generates the correct memory or just provides more details to the task which may not be causal.\n- A causal abstraction doesn\u2019t necessarily need to be structured. The drop in ablation when using the free-form advice suggests the importance of the structured sentence format rather than the causal abstraction. This ablation still doesn\u2019t show the usefulness of causal abstraction."
            },
            "questions": {
                "value": "The experiment results show that the longer tasks such as growing fruit in Fig 4 are harder for CLIN to adapt. But Fig 4c shows memory improves more episodes for the longer tasks, what are the improvements for long vs. short tasks? What kind of information is still missing for the long tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8220/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698722828378,
        "cdate": 1698722828378,
        "tmdate": 1699637020907,
        "mdate": 1699637020907,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aq3Yt5Ha9A",
        "forum": "d5DGVHMdsC",
        "replyto": "d5DGVHMdsC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8220/Reviewer_FEc9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8220/Reviewer_FEc9"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the continual learning abilities of LLM agents and proposes CLIN that maintains a memory of causal abstractions to help in future trials by reflecting on the last trial and current memory."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is clearly written and easy to follow.\n2. The experiments demonstrate a large improvement in multiple environments.\n3. The illustrations in the paper are informative."
            },
            "weaknesses": {
                "value": "1. The novelty is somewhat limited. Specifically, the authors discussed the difference between their method and Reflexion. But it is not clear if the causal abstractions can be generalized in distinct environments with different goals. Even if they can, I assume that the summaries kept in memory are pretty high-level and abstract. Can these summaries still benefit the performance of individual tasks? The experiments in the ScienceWorld benchmark make sense as different tasks share common rules. More experiments would be good to support the method, such as the benchmarks used in Reflexion (e.g., ALFWorld that the authors used as examples), where common rules may be hard to find useful for individual tasks. \n2. More comparisons to related works can be added, such as [1, 2] which also improve iteratively via trials.\n\n[1] AdaPlanner: Adaptive Planning from Feedback with Language Models. Sun et al.\n[2] Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency. Liu et al."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8220/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698732959414,
        "cdate": 1698732959414,
        "tmdate": 1699637020792,
        "mdate": 1699637020792,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AELtV1AU8l",
        "forum": "d5DGVHMdsC",
        "replyto": "d5DGVHMdsC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8220/Reviewer_9kZa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8220/Reviewer_9kZa"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a continuously learning language agent called CLIN, which improves the language-based agents. Unlike previous models, CLIN improves its performance without the need for parameter updates. CLIN uses a system centered on causal abstractions and a dynamic textual memory that regularly updates after each trial, the agent gradually learns and applies new knowledge for future trials. This framework is evaluated in the ScienceWorld benchmark, demonstrating its ability to adapt across different tasks and environments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The underlying mechanism of using causal abstractions for memory storage demonstrates a novel learning methodology.\n- The experiments conducted using the ScienceWorld benchmark are comprehensive and rigorous.\n- The paper is well-structured, making it easy to understand."
            },
            "weaknesses": {
                "value": "- The CLIN system is built on past experiences carrying forward context to new experiences. When there are potential issues such as misleading context, it will affect its performance.\n- Although CLIN claims to have the ability of self-exploration, it primarily focuses on known information, which may affect its performance in new environments.\n- Although CLIN presents some potential improvements over existing methods, it still does not have significant breakthroughs within the framework of current works."
            },
            "questions": {
                "value": "- CLIN's textual memory seems to depend heavily on causal abstractions. How does the system handle tasks where the relations are not causal or obscure?\n- How robust is the system when facing incorrect or misleading information stored in its memory? Is there any mechanism in place to correct or override these inaccuracies?\n- How does the system handle potential changes in state that are not immediately reacted to an action?\n- Could you provide more insights on how the executor modifies an action when it doesn't align with valid actions? Specifically, how does it ensure that the modified action still aligns with the intended goal?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8220/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698735920604,
        "cdate": 1698735920604,
        "tmdate": 1699637020665,
        "mdate": 1699637020665,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LGjLGrT2Jy",
        "forum": "d5DGVHMdsC",
        "replyto": "d5DGVHMdsC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8220/Reviewer_dtLL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8220/Reviewer_dtLL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use a memory module to help LLM-based agents to do continual learning using interactions with the world. The memory module takes an novel form of causal abstractions to summarize the agent's interaction trace into knowledge that can be reused for future interaction."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Significance: While the idea of having a memory to store experiences is not new, the novelty of the work lies in using LLM to summarize the experience into some form of knowledge, here in causal abstractions. I think this work could open door to more advanced and systematic study of how LLM agents can learn, i.e. gain knowledge from raw information, based on online interactions with the world. \n\nFor example, in a new gaming environment with different physics laws, the LLM-based agent needs to interact with the world to gain information on the actual physics in the new environment. There needs to be a process of learning and obtaining abstractions from these experiences to get generalizable laws. This can potentially help LLM gain new knowledge or domain-specific knowledge about unseen world and unseen domains. \n\nExperimental Results: the experimental results and comparison with baselines are convincing."
            },
            "weaknesses": {
                "value": "1. Technical Novelty: the idea of having a memory of past experiences and learned summary of knowledge is not very new; while this work proposes that the content in the memory is important, i.e., using causal abstractions helps agent's learning, the method is not that different in terms of the general idea.\n\nI think the causal abstraction seems to be only a specific instantiation of imposing some prior structure of memory. The authors would need to give more evidence on why this is universally applicable.\n\n2. The memory entirely depends on the agent's own exploration trace, so if the agent cannot explore enough and find some useful information, the agent cannot gain the corresponding knowledge.\n\n3. I think usually for an agent to be \"continually learning\", it needs to adapt to many different tasks over time. However, this work focus on accumulating knowledge continually for one task, which is not really continual adaptation."
            },
            "questions": {
                "value": "1. When generating causal abstractions, how does the LLM resolves credit assignment problem? If the agent do several actions and results in several different results, how does it know what actions contribute (and not contribute) to what result?\n\n2. Curious about whether the causal abstractions generation depends on LLM' known commonsense knowledge about the world, or depends on its own reasoning ability. What happens if the world follows some new logic, and does the causal abstraction generation still be correct?\n\n3. If the environment dynamics changes and old causal relationships are wrong, does the agent know when to update and unlearn the original knowledge?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8220/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8220/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8220/Reviewer_dtLL"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8220/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789729488,
        "cdate": 1698789729488,
        "tmdate": 1699637020542,
        "mdate": 1699637020542,
        "license": "CC BY 4.0",
        "version": 2
    }
]