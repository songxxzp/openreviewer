[
    {
        "id": "Sr3XTfyuxg",
        "forum": "vXrIQLzIKY",
        "replyto": "vXrIQLzIKY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission937/Reviewer_Md3A"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission937/Reviewer_Md3A"
        ],
        "content": {
            "summary": {
                "value": "In the proposed architecture, the authors design two branches to conduct these interaction modes. Both branches use an encoder-decoder setup to capture multi-scale features. An essential addition to this structure is the \"Bidirectional Connection Unit (BCU)\", which couples the learned representations from the two branches and facilitates better information fusion.\n\nThe combined designs enable the Xformer to effectively model global information in both spatial and channel dimensions. Through extensive experiments, the authors demonstrate that the Xformer achieves state-of-the-art performance on both synthetic and real-world image denoising tasks, all while maintaining comparable model complexity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper stands out in its innovative approach to image denoising by proposing a hybrid X-shaped Transformer. The clear presentation, combined with extensive experiments and state-of-the-art results, underscores its significance in the domain. The novel components, especially the BCU, and the creative combination of spatial and channel-wise blocks, emphasize its originality. The potential impact of this work on the broader image processing community is considerable."
            },
            "weaknesses": {
                "value": "The hybrid nature of the model, with its dual branches and BCU, might be challenging for some readers to grasp fully. While the description seems structured, visual aids might be lacking.\n7.Questions\uff1aCould you provide more insights into the design rationale behind the Bidirectional Connection Unit (BCU)? How does it ensure efficient information fusion between the two branches?"
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission937/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698589900835,
        "cdate": 1698589900835,
        "tmdate": 1699636020454,
        "mdate": 1699636020454,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4i7Ud1Jozd",
        "forum": "vXrIQLzIKY",
        "replyto": "vXrIQLzIKY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission937/Reviewer_wiTn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission937/Reviewer_wiTn"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a hybrid X-shaped vision transformer for image denoising, named Xformer. Xformer has two branches with one containing the spatial-wise transformer blocks and the other containing the channel-wise transformer blocks. Between these two branches, there are the bidirectional connection units which couple the learned representations from these two branches. The experimental results show that the proposed method performs well on the synthetic image denoising dataset, but the method does not achieve the SOTA on the real-world image denoising dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The X-shaped architecture is elegant and reasonable.\n2. The experimental results on the synthetic dataset are good.\n3. The overall paper writing is good."
            },
            "weaknesses": {
                "value": "There are several places that are not intuitive or clear:\n1. The authors claim that \"we make the last encoder involving STBs of two branches share parameters for the purpose of computational efficiency.\" However, it is unclear how much the performance will be influenced by the parameter-sharing strategy. It is also not clear why it is critical to share parameters for this place in the network. Why not share parameters in other places?\n2. The authors claim that \"In short, the STB utilizes non-overlapping windows to generate shorter token sequences for the self-attention computation, which can enable the network to obtain fine-grained local patches interactions.\" Shorter token sequences? Compared to what? Why does the shorter token sequences can enable the network to obtain fine-grained local patches interactions?\n3. The authors claim that \"In order to introduce contextualized information into self-attention computation, we choose to use 3\u00d73 depth-wise convolution (Conv) following 1\u00d71 Conv to generate query (Q), key (K), and value (V).\" Why not directly use a vanilla 3\u00d73 convolution?\n4. The authors claim that \"Specifically, we use a 3\u00d73 depth-wise convolution layer to refine the deep features from the spatial-wise branch for the purpose of saving computational consumption.\" Why not also using the 3\u00d73 depth-wise convolution layer to refine the deep features from the channel-wise branch? Will it influence the performance compared to using the 3\u00d73 vanilla convolution?"
            },
            "questions": {
                "value": "See the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission937/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811028391,
        "cdate": 1698811028391,
        "tmdate": 1699636020384,
        "mdate": 1699636020384,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ou9GWsi75i",
        "forum": "vXrIQLzIKY",
        "replyto": "vXrIQLzIKY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission937/Reviewer_GouV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission937/Reviewer_GouV"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed a hybrid X-shaped transformer for high-quality image denoising. The idea is good. Specifically, the technique consists of spatial-wise transformer blocks (STB) and channel-wise transformer blocks (CTB) to model global information. The authors provide extensive ablation studies to support the effectiveness of each proposed component, like STB, CTB, and BCU etc. The main comparisons with recent methods further show that the proposed method Xformer achieves better performance than others quantitatively and visually."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea is good and novel. The proposed Xformer exploits stronger global representation of tokens with a hybrid implementation of spatial-wise and channel-wise Transformer.\n\nThe bidirectional connection unit (BCU) is proposed to couple the learned representations from two branches of Xformer. It is simple but effective according to the ablation.\n\nThe authors provide extensive ablations to show the effects of some key components, like STB, CTB, BCU, and shift operation.\n\nThe main comparisons are also extensive. The authors provide both Gaussian and real image denoising results, where the proposed Xformer achieves better average quantitative results and also shows better visual results.\n\nThe writing is good and the work is well-prepared. The overall paper framework is well-organized.\n\nThe authors provide more results and analyses in supplementary file, where a sample code is also available. Such a code makes the reproducibility more faithful."
            },
            "weaknesses": {
                "value": "Some details are not clear enough for better understanding. How did the authors determine the final model when training is finished? For example, did the authors choose the model based on the best validation performance or just use the model from the final iteration?\n\nIn the ablation study, Table 1 (b), it seems that w/o BCU and BCU-1 is comparable, BCU-2 and Complete BCU is comparable. Please give more analyses about their difference.\n\nIf the proposed method could be used for other image restoration tasks? If so, please give some comments and discussions. Or is it specifically designed for image denoising?\n\nThe Xformer shows very good performance. Are there any failure cases for image denoising? Namely, the proposed method can hardly recover good details either."
            },
            "questions": {
                "value": "Please refer to the Weaknesses for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission937/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698851014925,
        "cdate": 1698851014925,
        "tmdate": 1699636020314,
        "mdate": 1699636020314,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JnabSIzSCZ",
        "forum": "vXrIQLzIKY",
        "replyto": "vXrIQLzIKY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission937/Reviewer_V2SV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission937/Reviewer_V2SV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes XFormer for image denoising, which aims to combine SwinIR which utilizes spatial-wise self attention and Restormer designing channel-wise self attention for image denoise, thereby leveraging the advantages from both methods. The designs, including dual-branch architecture and the bidirectional connection unit for bilateral interactions between two branches, are straightforward.m The paper is motivated well, whilst the technical novelty is incremental, especially compared to SwinIR and Restormer."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is motivated well. It is reasonable to combine the advantages of both spatial-wise self attention and channel-wise self attention to capture both the local fine-grained features and global features across channels.\n\n2. The paper is organized well and easy to follow despite some typos."
            },
            "weaknesses": {
                "value": "1. The technical novelty is incremental. There are two core designs: the dual-branch architecture and the bilateral interactions between two branches, which are both typical designs and have been extensively explored in other work. Thus, the technical novelty is limited, especially compared to SwinIR and Restormer.\n\n2. Compared to Restormer, Xformer has limited performance improvement, especially on real image denoising scenarios which is more important for evaluation."
            },
            "questions": {
                "value": "It is suggested to investigate both theoretically and experimentally what kind of denoising scenarios (noises) are STB and CTB suitable for, respectively."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission937/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission937/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission937/Reviewer_V2SV"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission937/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699108588462,
        "cdate": 1699108588462,
        "tmdate": 1700742032136,
        "mdate": 1700742032136,
        "license": "CC BY 4.0",
        "version": 2
    }
]