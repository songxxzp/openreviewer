[
    {
        "id": "WPrbzZ4mUl",
        "forum": "s90VIdza2K",
        "replyto": "s90VIdza2K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7936/Reviewer_hKcK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7936/Reviewer_hKcK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new framework for fairness, based on f-divergence and mutual information. This method is called f-FERM. From theory of minimax optimization, it shows the convergence to the optimum of f-FERM. Empirically, f-FERM also shows improvement over existing baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The method is quite straightforward. Suppose the output $y$ and the sensitive attribute $s$ are independent, then the f-Mutual Information term becomes zero. Therefore, the $f$-FERM encourages independence and thus fairness.\n2. Using variational bound and convergence theory of minimax, it proves the convergence of SGDA to a stationary point.\n3. Experimental results show the improvement of $f$-FERM over existing benchmarks."
            },
            "weaknesses": {
                "value": "1. This paper is missing a related reference called TERM (Tilted Empirical Risk Minimization) which also studies fairness.\n2. The main formulation (1) could be simplified. The f-divergence between the joint distribution and the product of marginals is called f-mutual information. See https://openreview.net/forum?id=ZD03VUZmRx for example and the references therein.\n3. SGDA for minimax optimization is highly inefficient and Theorem 2.2 has a really weak convergence result. How is it related to the experiments?\n4. The paper didn't discuss which $f$ (in f-div) is the best choice in detail.\n5. All the experiments are conducted on small-scale datasets.\n6. The definition of fairness is not clear to me. This paper considers both DP and EO, but which one is what f-FERM is trying to approach?"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7936/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697974515027,
        "cdate": 1697974515027,
        "tmdate": 1699636974668,
        "mdate": 1699636974668,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SzZ5qXIAqV",
        "forum": "s90VIdza2K",
        "replyto": "s90VIdza2K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7936/Reviewer_zLcF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7936/Reviewer_zLcF"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a unified stochastic optimization framework for fair ERM based on f-divergence measures.  The main idea is to reformulate it as a minimax problem using the Legendre-Fenchel conjugate function. This minimax formulation facilitates the development of the standard stochastic gradient descent and ascent algorithms for solving fair ERM. The convergence guarantees are stated in Theorem 2.2 which is mainly a corollary of the results in Lin et al. (2020) since this formulation is concave w.r.t. the dual variable.   The paper also addressed the problem of distribution shift and reformulated it using Lagrangian relaxation as a non-convex and non-concave problem. Extensive experiments are conducted to validate the proposed algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. A unified formulation for fair ERM using the f-divergence and then minimax reformulation which facilitates the application of SGDA. \n2.  A robust variant to address the distribution shift\n3. Extensive  and convincing experiments"
            },
            "weaknesses": {
                "value": "1. The proposed unified formulation and its minimax reformulation seem incremental as similar minimax forms have appeared in many existing works of fair machine learning \n2. The convergence analysis is straightforward from the paper by Lin et al. (2020)."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7936/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698155633516,
        "cdate": 1698155633516,
        "tmdate": 1699636974537,
        "mdate": 1699636974537,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "U1VOxWNnHv",
        "forum": "s90VIdza2K",
        "replyto": "s90VIdza2K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7936/Reviewer_PRWu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7936/Reviewer_PRWu"
        ],
        "content": {
            "summary": {
                "value": "Fairness constraints, such as DP and EO are hard to optimise directly thanks to their discrete nature. Existing bias mitigation approaches propose various differentiable proxies, typically correlation based, such as Zafar et al. They however do not allow unbiased small-batch estimation of the corresponding gradient.\n\nThe present paper uses f-divergence as a proxy for independence (they include EO & DP). They introduce a gradient based method, where that has unbiased estimate and therefore allows arbitrary small batch size. In the experiments they use batch size as small as 8. Arguably, this can allow to scale the method to larger problems.\n\nTo my knowledge, this paper presents the first theoretical result for gradient based optimisation of fair classifiers.\n\nI want to ask authors to clarify, how do they obtain classifier $ \\hat{y}$?\nTypically, one obtains differentiable probabilities $p_{\\theta}(y = j|x)$, and then maximising the probability yields the classification. However, the expression (4) suggest that the estimator is sampled.\nThis may harm the accuracy in practice. It is ok to use $F_j$ as a proxy, but it is better if you clarify that explicitly for applied people who may potentially use your method.\n\nSecondly, in Theorem 2.2 the stated convergence time is $O(\\epsilon^{-8})$ which seems rather slow. Does it affect the amount of epochs you use in the experiments, is it unusually large?\n\nThirdly, could you please clarify directly in Proposition 2.2. what is the dimension of variables $A_{jk}$"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "SGD with unbiased gradient updates for fair classification.\n\nAdditional result for robust fair classification.\n\nTheoretical result for SGD for fair classification."
            },
            "weaknesses": {
                "value": "mentioned in summary"
            },
            "questions": {
                "value": "mentioned in summary"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7936/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7936/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7936/Reviewer_PRWu"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7936/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698536070871,
        "cdate": 1698536070871,
        "tmdate": 1699636974431,
        "mdate": 1699636974431,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FY2xsvlm1x",
        "forum": "s90VIdza2K",
        "replyto": "s90VIdza2K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7936/Reviewer_o4nf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7936/Reviewer_o4nf"
        ],
        "content": {
            "summary": {
                "value": "This paper first presents a framework for fair ERM using f-divergence. Next, the authors propose a distributionally robust fair ERM framework, where the ambiguity set is constructed using $\\ell_p$-norm. For the theoretical part, a convergence analysis of the stochastic optimization for the ERM formulation is proposed, leveraging the results from Lin et al. (2020). The authors perform comprehensive numerical study to validate the superior performance of their proposed framework."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The formulation, optimization algorithm, and convergence analysis for the ERM formulation seem correct and novel.\n- The numerical study is comprehensive to demonstrate the effectiveness of their proposed algorithm."
            },
            "weaknesses": {
                "value": "- The only novel theoretical part is the convergence analysis of the SGDA for solving the ERM formulation. The corresponding sample complexity for finding $\\epsilon$-statariony solution of (f-FERM) is $O(\\epsilon^{-8})$. However, I wondered if the authors use the state-of-the-art optimization algorithm for solving nonconvex-concave min-max games and if the sample complexity is currently the optimal one in the literature. If so, the authors should highlight it in the literature. \n- For the distributionally robust formulation (8), the authors used $\\ell_p$-norm to model the ambiguity set, but I am afraid that this usage may not be a novel choice because it is not a flexible choice as Wasserstein, $f$-divergece, or MMD, to quantify the difference between distributions. I encourage the authors consider the extension to other choices of ambiguity sets.\n- When the ambiguity set of formulation (8) is small, the authors propose a first-order approximation formula to solve it (see Eq.(10)). Although the authors present the approximation error between formula (8) and (10), the complexity for solving Eq.(10) is not presented in this paper.\n- When the ambiguity set of formulation (8) is potentially large, the first-order approximation formula may not achieve good performance. To this end, the authors consider another formulation in Section 3.2. However, I have several concerns of the deviation:\n  * Can the authors add more details regarding the sentence \"One can easily see that the optimal $p_j=\\min(\\hat{p}_j+\\delta, 1)$ and $q_j=\\max(\\hat{q}_j-\\delta,0)$\"? It is not obvious for readers to check this point.\n  * What is the meaning of the scalar $\\delta>0$? It is not introduced. if I remember correctly, in standard f-divergence DRO it is the Lagrangian multiplier corresponding to the probability simplex constraint that needs to be optimized."
            },
            "questions": {
                "value": "See the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7936/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7936/Reviewer_o4nf",
                    "ICLR.cc/2024/Conference/Submission7936/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7936/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698933099075,
        "cdate": 1698933099075,
        "tmdate": 1700772221121,
        "mdate": 1700772221121,
        "license": "CC BY 4.0",
        "version": 2
    }
]