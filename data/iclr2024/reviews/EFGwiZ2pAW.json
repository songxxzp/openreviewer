[
    {
        "id": "iAeFHDlXBx",
        "forum": "EFGwiZ2pAW",
        "replyto": "EFGwiZ2pAW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4758/Reviewer_PJGj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4758/Reviewer_PJGj"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a very simple framework for learning on textual graphs. They conduct a two-step framework: 1) Finetune a language model on downstream tasks and obtain node representations; 2) Train a graph neural network model with the features from step 1 as node features. The authors then conduct experiments on three network datasets and perform model studies."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is very clearly written and easy to follow.\n2. The proposed framework is simple and useful."
            },
            "weaknesses": {
                "value": "1. Lack of comparison with existing works: GraphFormers [1], Patton [2]. There is another line of work [1,2] that tries to use only a language model to capture both semantic information and structure information in a textual graph. It would be more comprehensive to see how the performance comparison is between SimTeG and those methods. \n2. Excitement of the findings and studies. I appreciate the authors\u2019 detailed study of the two-stage pipeline. However, the finding is quite straightforward and not exciting enough to me. It is intuitive that the initial node feature vectors are very important and if a language model is trained on the downstream task first to generate the node feature vectors for the GNN methods, it will contribute to a very good performance.\n3. Technical novelty. Correct me if I\u2019m wrong, but this method can be seen as a single step for GLEM. The original GLEM involves iterative training of LM and GNN, while SimTeg contains only one round (LM training then GNN training). The performance comparison with GLEM is also very marginal regarding SOTA GNN.\n\n[1] GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph. NeurIPs 2021.\n[2] Patton: Language Model Pretraining on Text-Rich Networks. ACL 2023."
            },
            "questions": {
                "value": "Questions:\n1. What is the performance comparison between SimTeG, GraphFormers [1], and Patton [2]?\n2. See the second and third comments in the \u201cWeakness\u201d section.\n\n\nMinor suggestions:\n1. In Figure 2, please clarify which is referring to Arxiv and which is for products. Is X-SimTeG the embedding generated by LM (first stage) or GNN (second stage)?\n2. Page 5, typo \u201chow sensitive is GNN training sensitive to the selection of LMs?\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4758/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698072833477,
        "cdate": 1698072833477,
        "tmdate": 1699636458198,
        "mdate": 1699636458198,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZirLuJpKlQ",
        "forum": "EFGwiZ2pAW",
        "replyto": "EFGwiZ2pAW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4758/Reviewer_wDRw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4758/Reviewer_wDRw"
        ],
        "content": {
            "summary": {
                "value": "This paper studied a problem with textual graph learning by using the power of language models (LMs). The authors state that previous works focus on designing complex tasks or model structures for LMs on graph domains. However, the authors argue that it\u2019s not necessary for such complexity, so they propose a simple and efficient method (SimTeG) for textual graph learning with LMs. Their proposed SimTeG improves the performance of GNNs on large-scale graph datasets for both node classification and link prediction tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper studied an interesting problem about improving textual graph learning with language models (LMs). The author provides a thorough literature review in this domain.\n\n2. Compared with previous methods for designing novel architecture or complex tasks, this paper proposes a simple and effective, where the authors perform Parameter-Efficient Fine-Tuning (PEFT) on a language model. Then, they utilize this fine-tuned language model to generate node representations from the text by omitting the top layer.\n\n3. In this paper, the authors conduct extensive experiments on popular, large-scale graph datasets to evaluate both node classification and link prediction tasks. Their findings indicate that proficient language modeling can significantly enhance the performance of Graph Neural Network (GNN) models. Moreover, their straightforward approach demonstrated remarkable effectiveness in boosting performance."
            },
            "weaknesses": {
                "value": "1. The technical contribution of the paper appears to be limited, especially when considering the work of [1]. The core idea closely mirrors that of [1], which also leverages embeddings learned from a language model to enhance the learning of textual graph data via a variational expectation-maximization joint-training framework. The distinguishing factor in the proposed method is its two-step approach. However, I struggle to identify substantial contributions that differentiate it from [1].\n\n2. The authors argue that prior methods have crafted intricate tasks and structures to bolster the performance of textual graph learning with LMs. However, existing methods like [1,2,3,4,5] are conceptually simple and their frameworks are straightforward. Moreover, their training processes do not necessitate significant modifications to the prevalent model architectures.\n\n3. The paper's motivation is somewhat ambiguous. The authors predominantly focus on basic tasks, such as node classification and link prediction. Given that a rudimentary Graph Neural Network (GNN) can already yield satisfactory results for these tasks, the rationale for introducing a language model, which may be slower in inference and parameter-inefficient, is unclear. It might be more productive for the authors to highlight aspects like reduced inference time on test graph data or a more streamlined parameter set.\n\n4. While the authors have undertaken link prediction experiments, there is a noticeable absence of comparisons with some of the state-of-the-art (SOTA) methods that incorporate LMs. It would be beneficial for them to showcase, for example, the performance of GLEM or other notable methods on the link prediction task. Such a comparison could further attest to the efficacy of their proposed model.\n\n5. The review suggests compare the proposed method with GNNs which utilize bag-of-words feature not just the word-embedding feature in Ogbn-Arxiv.\n\nReferences:\n\n[1] Learning on Large-scale Text-attributed Graphs via Variational Inference. ICLR 2023\n\n[2] Explanations as Features: LLM-based Features for Text-Attributed Graphs. Arxiv\n\n[3] Node Feature Extraction by Self-supervised Multi-scale Neighborhood Prediction. ICLR 2022\n\n[4] Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. Arxiv\n\n[5] Natural Language is All a Graph Needs. Arxiv."
            },
            "questions": {
                "value": "1. On Ogbn-arxiv, SimTeG outperforms GLEM. From my understanding, GLEM can adaptively optimize the input embedding for GNNs, which will show better performance compared with SimTeG. Can the authors provide more discussions about this?\n2. Could the authors provide further insights into the specific LM variants that can significantly enhance GNN models? For instance, it would be valuable to understand whether larger LM parameters or other factors play a substantial role in this improvement."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4758/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698793078190,
        "cdate": 1698793078190,
        "tmdate": 1699636458097,
        "mdate": 1699636458097,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jSMlojxYbJ",
        "forum": "EFGwiZ2pAW",
        "replyto": "EFGwiZ2pAW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4758/Reviewer_cWJD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4758/Reviewer_cWJD"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces SimTeG, a simple yet effective method for graph learning on textual graphs (where nodes have text attributes). The key idea of SimTeG is to fine-tune a pre-trained language model (PLM) for downstream tasks (e.g., node classification) and then take the PLM output representations as the input features to GNNs for the same tasks. Experimental results show that SimTeG significantly improves GNNs' performance on various graph benchmarks, where the authors examine various choices of GNN backbones and PLM backbones. Through extensive studies, the authors also obtain some meaningful observations, such as that PEFT is necessary when fine-tuning PLMs, that can guide future research in this direction."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ Exploring the impact of PLMs on GNN learning and the importance of text attributes in various graph tasks is a meaningful task and has great potential given the recent breakthrough in large language models.\n\n+ The proposed idea (i.e., training PLMs with LoRA + training GNNs) is simple but intuitive and well-motivated, which should be appreciated.\n\n+ Experiments are quite comprehensive. Datasets from different domains (i.e., academic and e-commerce) are considered. Various GNN backbones and PLM backbones are examined, showing the generalizability of the proposed method.\n\n+ Extensive analyses are conducted to obtain meaningful insights, such as the necessity of LoRA and the unequal importance of text attributes on different datasets."
            },
            "weaknesses": {
                "value": "- Statistical significance tests are missing. It is unclear whether the gaps between SimTeG and the baselines are statistically significant or not. In fact, some gaps in Tables 1-3 are subtle and unlikely significant given the reported standard deviation.\n\n- An important baseline, GraphFormers [1], is not compared.\n\n- Only LoRA is examined in the proposed method as the PEFT strategy. It is unclear whether other strategies, such as Prefix-Tuning and Adapter, can also help tackle the overfitting problem. If so, the observed necessity of PEFT would be strengthened.\n\n[1] Graphformers: Gnn-nested transformers for representation learning on textual graph. NeurIPS'21."
            },
            "questions": {
                "value": "- Could you conduct statistical significance tests to compare SimTeG with the baselines and report the p-values?\n\n- Could you report the performance of GraphFormers?\n\n- Could you explore other PEFT strategies to check their effect on the overfitting problem?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4758/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699115796247,
        "cdate": 1699115796247,
        "tmdate": 1699636458009,
        "mdate": 1699636458009,
        "license": "CC BY 4.0",
        "version": 2
    }
]