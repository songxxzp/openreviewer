[
    {
        "id": "MNKzZEFz5j",
        "forum": "usrChqw6yK",
        "replyto": "usrChqw6yK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6622/Reviewer_E6zT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6622/Reviewer_E6zT"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Descriptor-Enhanced Open-Vocabulary Detector (DVDet) which introduces a conditional visual (region) prompting and textual descriptor dictionaries by using Large Language Models. The use of textual descriptors is motivated by the observation that the VLMs are good at capturing the fine-grained attributes of the objects. The textual descriptor dictionary is dynamically updated during training by tracking how frequent each descriptors are used, and by prompting what visual descriptors can help distinguishing the confusing categories. In addition, this work proposes a conditional region prompt to help the alignment between the detected region and text embeddings. When using both the textual descriptors and conditional visual prompts, DVDet improves the modern open-vocabulary detectors on the OV-COCO and OV-LVIS benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The use of Large Language Models to enrich the vocabulary description is well motivated.\n\n* The proposed conditional visual prompt is useful in bridging the gap between the image-text and region-text alignment.\n\n* The proposed DVDet method can be plugged into other SOTA open-vocabulary detection methods and consistently improve the performance."
            },
            "weaknesses": {
                "value": "* The use of region-level prompting for open-vocabulary detection is previously proposed in CVPR 2023 paper \"CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching\". The comparison (both at a idea level and a numeric comparison) with CORA work seems necessary regarding the technical similarity.\n\n* It is not clear what the optimal number of iterations is for the dictionary update. Is \"the number of high-frequency descriptors\" in Table 7 the number of iterations?\n\n* What is the effect of using the LLM on the running speed of the DVDet detector (using no LLM vs N-th interaction of LLM dictionary)? While the running speed might not be the main focus of open-vocabulary detection research, the multiple iterations of LLM use could make the speed not very favorable.\n\n* There are non-negligible number of hyper parameters (number of descriptors, number of update iterations, thresholds)."
            },
            "questions": {
                "value": "* The gain in the base vs novel categories are similar (e.g., + 2.7 novel AP and +2.2 base AP in Table 1). This leads to a question whether the DVDet method is beneficial in standard detection instead of open-vocabulary detection. That is, the proposed method benefits from augmenting the vocabulary description (text embeddings) in general, not being specifically helpful for the novel categories. What is the motivation of evaluating on open-vocabulary detection? How would DVDet perform in the standard detection setting?\n\n* Please compare with CORA (CVPR 2023) which also uses visual region prompting for open-vocabulary detection.\n\n* Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6622/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698721338063,
        "cdate": 1698721338063,
        "tmdate": 1699636756040,
        "mdate": 1699636756040,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qm2XRQAr8D",
        "forum": "usrChqw6yK",
        "replyto": "usrChqw6yK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6622/Reviewer_6yi2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6622/Reviewer_6yi2"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces DVDet, a Descriptor-Enhanced Open Vocabulary Detector featuring two innovative components: conditional context prompts and hierarchical textual descriptors. The conditional context prompts function in the Region of Interest (RoI) align to expand each region proposal's area, incorporating more context information and converting it into an image-like feature preferred by CLIP. Meanwhile, the hierarchical textual descriptors dynamically acquire fine-grained descriptors for each category through interaction with the Language Model (LLM) during training. Extensive experiments indicate that DVDet significantly enhances the performance of various Open-Vocabulary Detectors on two benchmarks, MS COCO and LVIS."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. I greatly value the use of LLM to dynamically generate detailed descriptors of categories. This approach inspires a novel method of employing LLM to enhance the performance of OVD tasks. \n\n2. The concept presented is both straightforward and potent, with the paper being lucid and easy to navigate. The author's introduction of Framework Figures (Fig 2) and the examples of iterative fine-grained descriptors (Figure 3) are commendable as they significantly aid the reader in grasping the idea more effectively.\n\n3. The experiments conducted are thoroughly robust. First, the author meticulously scrutinizes each component of their methodology, as detailed in Table 3 and Table 9. Second, the author conducts comprehensive experiments on a variety of OVD detectors and datasets to demonstrate that DVDet significantly enhances performance."
            },
            "weaknesses": {
                "value": "1. A comparison of computation costs with the baseline has not been conducted.\n\n2. The contribution of the technique is somewhat restricted.\n\n3. The prompt generated by LLM may possess an element of randomness, potentially making it challenging to reproduce.\n\n4. No ablation experiment about the hyper-parameter $m$ of Conditional Context Prompt"
            },
            "questions": {
                "value": "1. How about the extra computation overhead?\n\n2. LLM may generate different prompts. Does it significantly affect the performance?\n\n3. How about different $m$ in Conditional Context Prompt?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6622/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817973161,
        "cdate": 1698817973161,
        "tmdate": 1699636755923,
        "mdate": 1699636755923,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7hMuS0pLZX",
        "forum": "usrChqw6yK",
        "replyto": "usrChqw6yK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6622/Reviewer_shSW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6622/Reviewer_shSW"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an approach to boost open-vocabulary object detector with fine-grained language descriptors of the categories. The motivation is that existing works learn to align region embeddings with category labels only, disregarding the capability of VLM to align with object parts and other fine-grained descriptions. The fine-grained text descriptors are generated with an LLM in an interactive manner. In addition, the paper introduces conditional context prompt to augment region embeddings with contextual cues and make them more image-like for open-vocabulary detection."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The two approaches presented by this paper are intuitive and effective. The context prompt makes the region more image-like to aid open-vocabulary recognition and the use of LLM provides additional features for VLM to reliably detect novel classes. The main results in Table 1 and 2 show the effectiveness of proposed approaches on LVIS and COCO benchmarks based on many existing methods. Ablations show that each part is effective on OV-COCO."
            },
            "weaknesses": {
                "value": "1. The idea of using LLM to generate more textual description has been explored in recent/concurrent work [2]. It'd be great to see a comparison with the existing ideas. Similarly, the idea of context regional prompt has been explored in recent/concurrent work [1], although the exact instantiation may differ. It'd be helpful to tease apart the contribution of context (which makes the crop more image-like) vs prompting (which adapts the features for detection use cases) through an ablation, and compare/discuss the similarity/differences with existing work. I understand the references listed are very recent/concurrent works, but I think it'd still be valuable to have some comparison with them for scientific understanding.\n\n2. Although DVDet shows gains for all methods in Table 1 and 2. The gains on base categories are non-trivial and even comparable to the novel categories in most cases. For example, DVDet + Detic has +1.8 boost on mAPf and +1.3 on mAPr, and +2.5 on Base AP vs +1.7 on Novel AP. Given the method is based on Detic, I'm wondering what the implication of the gains on base vs novel categories are, since the method is primarily designed for open-vocabulary detection. \n\n3. Table 3 (row 2) shows that adding fine-grained descriptors by itself without prompting hurts the performance of the model. This seems counter-intuitive to me. It'd be helpful to see if the same observation holds for other OVD models e.g. ViLD or Detic.\n\n4. Table 4 shows transfer detection from COCO to PASCAL and LVIS. It's nice to see a clear boost there. I'd recommend trying out the LVIS-trained model on Objects365 instead since it's a more commonly used transfer detection setting by e.g. ViLD, DetPro, F-VLM. \n\n5. In Eq (2), the $\\textit{m, n}$ are said to be constants. How are they set? I'm wondering if they should be set proportional to the width and height of the ROI. Another option is to use the whole image box as a baseline and see how that performs. Some ablations/analysis on the choice of context would be interesting.\n\nReferences:\n1. CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching (CVPR 2023)\n2. Multi-Modal Classifiers for Open-Vocabulary Object Detection (ICML 2023)"
            },
            "questions": {
                "value": "See weaknesses. Point 1-3 are more important in my view.\n\nWhat's the variance of the proposed method on LVIS open-vocabulary benchmark over e.g. 3 or 5 independent runs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6622/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698870498477,
        "cdate": 1698870498477,
        "tmdate": 1699636755808,
        "mdate": 1699636755808,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UU6D7dFeFu",
        "forum": "usrChqw6yK",
        "replyto": "usrChqw6yK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6622/Reviewer_AtsG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6622/Reviewer_AtsG"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to a novel open vocabulary detection method which uses a fine-grained descriptor for a better region-text alignment and open vocabulary detection. It consists of two parts. First part is to transform region embeddings to image-like representation, second part is to use LLM  to generate fine-grained descriptors. The method improves open vocabulary detection consistently."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The method proposed in the paper looks interesting by using LLM as a better fine-grained descriptor. The idea is reasonable and the presentation is good.\n2. The progress from iterative extraction of fine-grained LLM descriptor is interesting and looks interesting to me.\n3. The results of experiments are good and the ablation study is convincing to me."
            },
            "weaknesses": {
                "value": "1. when LLM interacts with VLM, Could you give more examples of prompts to use and how the prompt may influence the final results?\n2. when iterative updating of LLM descriptor, It seems to just give a general description of objects. Then what will happen if we just let LLM do some general description of objects without seeing the objects?\n3. if the regional prompt is not accurate, how will the performance look like when interacting with LLM?"
            },
            "questions": {
                "value": "please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6622/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698887716560,
        "cdate": 1698887716560,
        "tmdate": 1699636755700,
        "mdate": 1699636755700,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1gUHy3QDKW",
        "forum": "usrChqw6yK",
        "replyto": "usrChqw6yK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6622/Reviewer_qAZ8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6622/Reviewer_qAZ8"
        ],
        "content": {
            "summary": {
                "value": "The submission introduces Descriptor-Enhanced Open Vocabulary Detection (DVDet), a method that integrates fine-grained descriptors from Vision Language Models (VLMs) into open vocabulary object detection (OVOD) using a Conditional Context visual Prompt (CCP). This approach utilizes large language models to generate descriptors without extra annotations and features a hierarchical update mechanism for descriptor refinement. The paper claims improvements in OVOD tasks through extensive experiments and presents three main contributions: a new feature-level visual prompt, an update mechanism for descriptor management, and empirical evidence of enhanced detection performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ Introduction of Descriptor-Enhanced Open Vocabulary Detection (DVDet) could address existing granularity challenges in object detection, making it a potentially transformative method.\n+ Leveraging large language models for descriptor generation without additional annotations presents a cost-effective solution."
            },
            "weaknesses": {
                "value": "- The paper does not sufficiently address the analysis of descriptors produced by Large Language Models, which is crucial for understanding the quality and relevance of the generated descriptors in enhancing object detection.\n- The demonstrated enhancements (~2% on average) in object detection performance, while positive, do not represent a substantial leap forward when considering the benchmarks established by current leading methods."
            },
            "questions": {
                "value": "1. How does the descriptors generation process affects the training efficiency?\n2. Can the authors provide insights into any observed trade-offs between the complexity of the Descriptor-Enhanced Open Vocabulary Detection (DVDet) and its performance gains?\n2. How does the system perform under varying conditions, such as different object scales, occlusions, and lighting? Are there specific scenarios where the performance improvement is more pronounced?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6622/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699317162906,
        "cdate": 1699317162906,
        "tmdate": 1699636755567,
        "mdate": 1699636755567,
        "license": "CC BY 4.0",
        "version": 2
    }
]