[
    {
        "id": "lK7vqOjOBI",
        "forum": "RE0aibEQ1J",
        "replyto": "RE0aibEQ1J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7803/Reviewer_UHPS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7803/Reviewer_UHPS"
        ],
        "content": {
            "summary": {
                "value": "This papers presents an agent design named IG-NET for image-goal navigation on large-scale environments. \n\nThis agent is trained on an offline learning fashion and relies only on images to navigate. \n\nThe model architecture proposed is an image to goal model, trained end-to-end with offline data from experts, with a representation enhancing mechanisms that incorporates position and navigation information prediction.\n\nExperiments show IG-Net robust navigation, being the model that achieves the best performance in all the different difficulty settings tested in the ShooterGame environment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper presents a somehow novel problem: offline image-goal navigation in large-scale game maps. \n\n- Section 5.4 (Case Study) is broad and it illustrates and provides with further evidence to support why IG-Net performs better than the other algorithms evaluated."
            },
            "weaknesses": {
                "value": "* Section 2 does not provide a clear discussion of why the proposed model (IG-Net) is different from previously published models. It simply mentions that 'IG-Net is meticulously designed to navigate significantly larger maps,' but that is not sufficient.\n\n* Habitat Matterport 3D Semantics Dataset (see missing reference [MR1]) has not been included in the comparison of table 1. This is probably the largest dataset to date, at it should be listed.\n\n* Novelty problems:\n  * This is not the first model to propose to solve the navigation problem with offline data. First and foremost, the manuscript overlooks conducting a review of the literature related to imitation learning (behavior cloning) and even recent offline RL, where comparisons with many previous models that have addressed the problem of navigation in virtual environments under the same constraint\u2014using only offline data\u2014could be made. \n  * In Section 4.1, no significant differences with respect to previous works are detailed. Previous models used only images as inputs as well [MR2]. There are also works that use positional information to enhance model learning (PIRLNav model does it when using the compass and GPS information as iputs), or even auxiliary tasks (see [MR3]).\n  * More works should be included in a detailed discussion: e.g. (Shah et al., 2021) (Kahn et al., 2021) (Kim et al., 2022).\n  \n  \n* Clarity needs to be improved:\n  * Sections 4.2 - 4.4 need improvement in terms of clarity. Details are missing on how the training of the entire model is structured, and especially, details of the proposed architecture.\n  * One cannot use the same symbol for both the model weights and the angles.\n  * If the algorithm is trained in an offline manner, which was the offline algorithm used? I could not find any reference about any algorithm.\n\n* Weaknesses of the experimental setup:\n  * It is an offline model that just need a small dataset of 200 trajectories. That is not too much. I wonder how this impacts the performance of the model.\n  * The way the model is tested does not follow the standard in embodied AI literature. SR is measured when the agent arrives to the target AND samples an stop action, which means that (somehow) it knows that it has arrived. How does the proposed model know that it has arrived to the target? This is not considered in this paper, and the evaluation metrics used do not help to evaluate this fact.\n  * More baselines are needed. The selection of VGM is adequate but other approaches have to be considered. Offline-RL models should be considered as well (see [MR4]). \n  * How does the proposed approach perform in standard embodied AI navigation benchmarks? Habitat datasets, for example. There is offline data that can be used (see PIRLNav paper dataset of 77k navigation trajectories) or the authors can maybe generate new data (it seems that with just 200 trajectories the IG-Net model has enough information to be trained).\n  \n  \n  \n\nMissing references:\n===================\n[MR1]   Habitat-Matterport 3D Semantics Dataset.Yadav, Karmesh and Ramrakhya, Ram and Ramakrishnan, Santhosh Kumar and Gervet, Theo and Turner, John and Gokaslan, Aaron and Maestre, Noah and Chang, Angel Xuan and Batra, Dhruv and Savva, Manolis, arXiv preprint , https://arxiv.org/abs/2210.05633, 2022. https://aihabitat.org/datasets/hm3d-semantics/\n\n[MR2] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J. Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In ICRA, 2017.\n\n[MR3] Joel Ye and Dhruv Batra and Abhishek Das and Erik Wijmans. Auxiliary Tasks and Exploration Enable ObjectNav, ICCV, 2022.\n\n[MR4] Offline Reinforcement Learning for Visual Navigation, Dhruv Shah and Arjun Bhorkar and Hrishit Leen and Ilya Kostrikov and Nicholas Rhinehart and Sergey Levine,6th Annual Conference on Robot Learning, 2022."
            },
            "questions": {
                "value": "I've tried to detail most of the limitations and weaknesses of the proposed model in previous section, with some points that would need to be addressed in a rebuttal.\n\nOverall, I see here a manuscript with incremental contributions. From a theoretical perspective, the proposed model follows a short of imitation learning paradigm with some interesting loss functions that have been specifically designed for the problem of interest. Technically, the proposed architecture is a Masked Auto-Encoder for encoding the images, followed with some embedding concatenations plus positional and action decoding. I can hardly see here a considerable technical contribution either. Overall, the paper describes some ideas that are not adequate, in my humble opinion, for an ICLR conference."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety",
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This work describes a model for creating autonomous agents capable of various tasks: guiding costumers,  building autonomous robots, etc.\n\nThe introduced approach  allow to improve the navigation capabilities of autonomous agents. Of course, like most scientific work in the STEM sector, this might lead to some negative societal impacts, which our\nwork shares with most robotics applications: military robots (used in wrong hands), surveillance etc. The work described in the manuscript was carried out in simulation and as such is unlikely to have produced unethical results, except the impact of large-scale training on CO2 output."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7803/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698681358023,
        "cdate": 1698681358023,
        "tmdate": 1699636954588,
        "mdate": 1699636954588,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yoVv4MC1zS",
        "forum": "RE0aibEQ1J",
        "replyto": "RE0aibEQ1J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7803/Reviewer_rCRb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7803/Reviewer_rCRb"
        ],
        "content": {
            "summary": {
                "value": "The paper presents the Image-Goal Network (IG-Net), a novel approach to address the challenges of navigating large-scale gaming environments with limited offline data. The model is trained end-to-end, incorporating position, path, and distance prediction for implicit spatial mapping. Experiments in the ShooterGame environment validate IG-Net's efficacy, and the paper also contrasts it with existing methods in visual navigation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors present an interesting and amazing work in the paper and raise the meaningful challenge of navigation in large-scale environments to the community. \n3. The authors introduce an expansive environment ShooterGame environment. The ShooterGame environment is nearly 20 times larger than the previous environment on one map. \n3. IG-Net utilizes a variety of auxiliary tasks, including local and global path planning and navigation distance prediction, to strengthen its visual navigation capabilities. These auxiliary tasks enrich the agent's representation and improve its navigation performance.\n2. In comparison to other methodologies like VGM, IG-Net outperforms in terms of success rate, success weighted by path length, and navigation efficiency. It achieves higher success rates and navigation efficiency even under more challenging settings."
            },
            "weaknesses": {
                "value": "1. The authors perform the testing on the seen environment during training. I believe that demonstrating the performance in unseen environments will consolidate the work. \n2. The authors do not provide the details for the architecture of the IG-Net. I am a little bit confused about the input/output of each network component. \n3. The ablation study in Table 4 shows an experiment \"IG-Net (no position).\" I am curious that the experiment demonstrates the performance of IG-Net without both absolute/relative position prediction or either of them. A similar question about the ablation experiment \"IG-Net (no path and dist)\", why did the authors remove two auxiliary tasks at the same time in the ablation study?"
            },
            "questions": {
                "value": "1. Although the coverage of the ShooterGame environment is much larger than other environments, it also seems to be more empty than other environments. Could the authors discuss the difficulty differences in navigation among these environments except the coverage area? Meanwhile, I am curious about the number of different states in the ShooterGame environment.\n1. How do the authors concatenate the encoding embeddings from the first step into a unified representation? Is that a FIFO stack or an MLP/pooling layer serving as a projector to ensure the same size of concatenated features during different lengths of episodes?\n2. In the third part of the architecture design, the authors mentioned \"Utilizing position, path, distance, and action decoders. \" I am curious whether the historical information, apart from visual embedding, is included in the action prediction. If the authors do not use historical action/state representations, how does the agent maintain the search efficiency instead of being stuck in a small area?\n4. Suppose we put the trained agent in an unseen environment, how will the agent perform? In the unseen environment, will the agent still be able to perform these auxiliary tasks? How does the agent perform on these auxiliary tasks, e.g., absolute/relative position prediction?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7803/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7803/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7803/Reviewer_rCRb"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7803/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765379428,
        "cdate": 1698765379428,
        "tmdate": 1699636954283,
        "mdate": 1699636954283,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mLBWeGQUbc",
        "forum": "RE0aibEQ1J",
        "replyto": "RE0aibEQ1J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7803/Reviewer_Sxb4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7803/Reviewer_Sxb4"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes IG-Net, a end-to-end method designed for visual navigation in large scale environments like the shooter game environment from unreal engine. Instead of building an explicit map IG-Net leverages visual and implicit pose information to learn better representations for navigation. To do this IG-Net uses a combination of relative pose prediction, absolute pose prediction, navigation distance and navigation path prediction as auxiliary tasks on offline trajectories and randomly sampled pose and visual data. The paper also shows IG-Net outperforms a Visual-Graph-Memory baseline which uses topological graph representation in the ShooterGame environment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper proposes a method to test Image-Goal Navigation in large-scale game environments which require long-horizon navigation skills\n2. IG-Net outperforms VGM baseline by a significant margin on all 3 evaluation splits (easy, medium, hard)\n3. Ablation in table 2 nicely show the importance of auxiliary objectives in boosting evaluation performance for IG-Net\n4. Experiments in section 5.4.2. show that IG-Net is also robust to noise to some extent and doesn\u2019t lead to significant drop in performance on the hard split. But for some other splits we do see a large drop in performance.\n5. Paper is easy to follow"
            },
            "weaknesses": {
                "value": "1. The experiments section doesn\u2019t show comparison with state-of-the-art methods on ImageNav. There has been some recent works in ImageNav which use end-to-end RL training using pretrained representations (visual and pose) [1, 2, 3, 4] which achieve significant performance on the ImageNav benchmark. The current state-of-the-art method [1] acheives ~92% SR using simple concatenation of goal and observation image being fed to a single ResNet model to generate a observation embedding. OVRL [3] and OVRL-v2 [2] have been using pretrained visual representations using data from omnidata which leads to improved perfomance. It is essential that the authors compare IG-Net to the state-of-the-art baselines. I\u2019d suggest adding comparison to FGPrompt-EF baseline from [1] and OVRL and OVRL-v2 pretrained baselines from [2, 3]. [4] seems to be a concurrent work with no code released so no need to add comparison with it.\n2. To the best of my understanding, the experimental setup is not testing generalization to new maps in the ShooterGame environment. Can authors please show results of generalization to different maps in the environment? If the performance is being tested in the same scene then both methods should be able to achieve significantly higher success. Can authors please share more details about the VGM baseline\u2019s training and evaluation setup? In addition, it would be good if the authors can add more details about the ShooterGame dataset they used for training and testing.\n3. Based on description in section 4.3 it is unclear what the policy architecture of IG-Net looks like. Can authors please share a policy architecture figure?\n4. Ablations in section 5.5 need to ablate each component of auxiliary task separately. In the current results it looks like row 2 only uses relative and absolute pose prediction and row 3 doesn\u2019t use navigation path prediction and distance aux tasks. In each ablation authors should sequentially remove every single loss from list of auxiliary losses to clearly demonstrate effectiveness of each components.\n\n[1] X. Sun, P. Chen, J. Fan, T. Li,\u2026 FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation. In NeurIPS, 2023\n[2] K. Yadav, A. Majumdar, R. Ramrakhya, N. Yokoyama, A. Baevski, Z. Kira, O. Maksymets, and D. Batra. Ovrl-v2: A simple state-of-art baseline for imagenav and objectnav. arXiv preprint arXiv:2303.07798, 2023.\n[3]  K. Yadav, R. Ramrakhya, A. Majumdar, V.-P. Berges, S. Kuhar, D. Batra, A. Baevski, and O. Maksymets. Offline visual representation learning for embodied navigation. In International Conference on Learning Representations (ICLR), 2022.\n[4] G. Bono, L, Antsfeld, B.  Chidlovskii, P. Weinzaepfel, C. Wolf End-to-End, (Instance)-Image Goal Navigation Through Correspondence as an Emergent Phenomenon, arxiv preprint arxiv:2309.16634"
            },
            "questions": {
                "value": "1. The best methods [1,2,3,4] on ImageNav task use RL for training. It would be good if authors can add comparison with a RL trained baseline using FGPrompt-EF. Can authors provide more details into why it is not possible to get online data? Is it due to slow simulator speed?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7803/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7803/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7803/Reviewer_Sxb4"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7803/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699247134407,
        "cdate": 1699247134407,
        "tmdate": 1699636954123,
        "mdate": 1699636954123,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iCfDo2ePAd",
        "forum": "RE0aibEQ1J",
        "replyto": "RE0aibEQ1J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7803/Reviewer_JFQf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7803/Reviewer_JFQf"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method of training navigation agents which can navigate large spaces using ego-centric RGB observations without assuming access to accurate position information. The task is to navigate to an image-goal in a large simulated environment. The method utilizes several auxiliary losses to learn the spatial map and how to navigate it implicitly. The authors demonstrate their method on several trajectories of varying difficulty in a single environment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper studies navigating large-spaces using RGB egocentric observations without accurate localisation which is an interesting problem that hasn\u2019t been extensively studied before.\n- The environment studied in the setup is dynamic which makes the task even more challenging (with varying lighting conditions, moving objects and other distractors)\n- The authors train an end-to-end approach for navigating in large spaces which hasn\u2019t been shown to work before.  They compare their method against meaningful baselines (baselines that build an explicit representation of the environment as graphs) and show superior results"
            },
            "weaknesses": {
                "value": "- The authors test their method on a single environment which is a fairly restrictive experimental setup. We don\u2019t know if the model is capable of handling / memorising multiple environments. I would have liked to see experiments in which a single model can be learned for multiple environments.\n- The experiments also assume perfect actuation \u2014 left/right and forward will always lead to the same amount of actuation. I wonder how does the agent behave when the actuation during test-time is noisy / different from those used during training. Such a situation is fairly common in the real world and it would be nice to show experiments for the more general setting.\n- More importantly, the paper doesn\u2019t have any analysis and discussion about the train-time vs test-time distribution. How different are the trajectories used at train-time compared to test-time trajectories. This analysis is important to understand the model\u2019s generalisation capability and to actually test if it has build an efficient implicit representation of the map or not.\n- I would have liked to see some more experiments to probe this implicit understanding of the map. For instance, if the agent has only been trained on a complex trajectory, can it learn to output the shortest path between two pair of observations?\n- Similar to the last point, I would have also liked to see the effect of distractors in the observation on task performance. For instance, does the performance decrease when we add more distractors in the observation at test time? Since one of the novelties of this task setup is dynamic observations, it feels that this kind of analysis is important for a more thorough evaluation."
            },
            "questions": {
                "value": "- Not a question, but a comment. The paper overloads the meaning of \\theta in equation (2) and (3) which made parsing those equations slightly confusing. I would also encourage the authors to run a more extensive grammar check to improve the writing of the paper. \n\n- According to my understanding, none of the losses in Section 4.2 actually involves predicting actions. How does the method learn to predict actions?\n\n- Related to weaknesses:\n    - it\u2019d be great to study the effect of agent\u2019s observation field of view and  different action spaces on task performance.\n    - it\u2019d be interesting to study the robustness of the method when action spaces change during test time\n    - can the authors comment on how different test time trajectories are to the ones seen during training? For example, for each location of the agent for a trajectory at test time, what is the nearest location observed during training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7803/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699316446359,
        "cdate": 1699316446359,
        "tmdate": 1699636953937,
        "mdate": 1699636953937,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ESZUN5xpR3",
        "forum": "RE0aibEQ1J",
        "replyto": "RE0aibEQ1J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7803/Reviewer_m9pY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7803/Reviewer_m9pY"
        ],
        "content": {
            "summary": {
                "value": "This paper deals with the problem of Image Goal tasks when agent pose in environment is unknown, and shows how training with limited navigation data (but good amount of positional data) in large scale game space like areas -  is still possible to yield good results. The authors have introduced a Network archiecture to represent spatial map information implicitly based on prediction of position, distance and path. Although the application setup is novel, but the paper needs an overhaul to bring in the novel aspects and add technical challenges that were overcome. The ablation studies and experiments do support the writings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The usage of distance decrement rate (DDR) is logical in this paper wrt task, however a scaling may help in evaluations.\nThe code is available and executed as mentioned.\nThe scope of application is good and will open up research communities to tackle the problem with game to real transfer later."
            },
            "weaknesses": {
                "value": "In Fig. 1, the separating boundary walls are quite distant due to the open space, making exploration easy compared to close-looped indoor environment with full or partial occlusions. \nThe SPL compared to success rate in Table 2 section 1, is not that great - any logic for that? Although a gap in the metrics will help the future researchers to improve the SOTA.\nThe architecture of the work should be clearly highlighted and mentioned.\nThe video is not clear in terms of what is being planned to be achieved - this needs serious improvement - like how the scene is processed and the action outputs wrt image goal is coming.\nMore SOTA study and real world transfer is expected."
            },
            "questions": {
                "value": "Not sure how much the trained policy is suitable for FPS games compared to real world robotic deployments in the wild in human co-occupied spaces.\nThe action space should be clearly mentioned for the testbed. Also how dependent on camera intrinsics.\nIncorporation of Auxiliary Tasks - how is it done - need finer details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 6,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7803/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699341171044,
        "cdate": 1699341171044,
        "tmdate": 1699636953721,
        "mdate": 1699636953721,
        "license": "CC BY 4.0",
        "version": 2
    }
]