[
    {
        "id": "wnUjowcQ1E",
        "forum": "5tSLtvkHCh",
        "replyto": "5tSLtvkHCh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2088/Reviewer_uMXb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2088/Reviewer_uMXb"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a model recovering latent causal factors of a time-series, in other words inverting the generating process of sequential data. The key feature of this model is that it utilises temporal context (i.e. to recover latent factors at time t, it uses observations at time t, t-1, ..., t - k for some k) which allows us to overcome the non-injectivity of the generating function. The model is motivated by a theoretical analysis showing that under certain assumptions such an inversion is guaranteed to recover the true latent factors. The numerical experiments demonstrate superior performance of the proposed model in comparison to a number of baselines on synthetic dataset and real-world datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ An interesting model addressing important questions of nonlinear identifiability and disentanglement in temporal data\n+ Thorough theoretical analysis of the proposed model\n+ Experimental comparisons to a number of baseline models"
            },
            "weaknesses": {
                "value": "I think the presentation could be somewhat improved. The paper is full of technical details of the identifiability theory but I think it would also benefit from a higher-level discussion (maybe using a cartoon or a toy-example) illustrating the intuition behind the assumptions of the theorems. My take home message after reading this paper is that using temporal context enables nonlinear identifiability and disentanglement under certain conditions, but I'd be struggling to explain what these conditions mean and why the temporal context is so crucial."
            },
            "questions": {
                "value": "Questions to Definition 1:\n- Why are m and \\hat{m} in the subscript of distributions in Eq. (3)? As I understood m is not part of the generative model (we don't need it to generate data from the latent factors) so it shouldn't influence the resulting data distribution.\n- Should the model and data distributions match almost everywhere rather than everywhere?\n- According to this definition, the latent process is identifiable if the model and data distributions match and \\hat{m} = m up to permutations. What about the case when the model and data distributions don't match but \\hat{m} = m up to permutations? (for example, if we could obtain true m-function with the wrong model) Is it an impossible scenario or rather just not in the scope of this paper?\n\nQuestions to Theorem 1:\n- Is it possible to estimate how much temporal context (i.e. the value of \\mu) is required for identifiability in Theorem 1? Or does the result only say that if the inverting function m exists for some \\mu then we can estimate it up to permutations but we don't know how much temporal history we might need to that?\n- (A more speculative question, feel free to ignore if it doesn't make sense.) Do you have an intuition what happens as \\mu -> \\infty? Is every model identifiable in the limit or not necessarily?\n\nQuestion to Section 4:\n- \"To enforce the conditional independence of latent variables, the distribution of p(z_t | x_{t:t\u2212\u03bc}) is constrained by the prior.\" Why does the prior constrain the conditional independencies in the posterior? I guess you refer to ELBO which includes a KL divergence to the prior, but the global maximiser of ELBO is the true posterior distribution, and clearly there are examples of models with independent prior but dependent posterior (e.g. https://en.wikipedia.org/wiki/Interaction_information#Negative_interaction_information)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2088/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698694741320,
        "cdate": 1698694741320,
        "tmdate": 1699636141282,
        "mdate": 1699636141282,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kzfH9XfZ0U",
        "forum": "5tSLtvkHCh",
        "replyto": "5tSLtvkHCh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2088/Reviewer_ur5z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2088/Reviewer_ur5z"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a novel approach for latent variable identification in a temporal setting. It relaxes the common assumption that the latent representation at time step t can be uniquely determined from the observation at that time step. Instead it assumes that it can be determined from a window of past observations. Their results rely on sufficient variability assumptions very similar to what has been proposed in the literature. They proposed an algorithm based of VAEs and normalizing flows to model the transition model in the latent space. They show experiments on synthetic data, to validate identifiability, and on realistic QA datasets, to assess the usefulness of the learned representation.\n\n### **Review summary**\nAlthough I believe the motivation and the proposition of this paper is very good, I believe this manuscript is not ready for publication. My main concerns are:\n- I am not certain that the there exists a model that actually satisfy the assumptions of this work.\n- The paper presents many math mistakes and imprecision. The terminology used is also wrong at times.\n- The point made in Section 3.3 was already made in a previous work [4]. This work also presents a counter example to Lemma 1 (implying that it is false). \n- The writing quality is low\n- The paper is not well situated in the literature, which makes it hard for a non-expert to understand what is the actual novelty.\n\nI substantiate all of these points below, in the \"Weakness\" section. I sincerely believe this idea has great potential, but too many problems in the execution. For these reasons, I recommend rejection. I very much hope that the authors will take my criticisms seriously and use them to improve their work.\n\n### **Post discussion phase**\n\nLooks like the discussion phase is over. I was hoping to answer the last points raised by the authors, but couldn't do it in the comments, so I decided to share it here:\n\n----\n\nConcrete mathematical example: Well, if you take $f_i$ to be noisy here, what is the corresponding $m$? You provided an $m$ \nonly for the non-noisy case, but your theory assumes noise. And my guess is that the noise is crucial to your proofs (as is often the case in nonlinear ICA).\n\nCounter-example to Lemma 1: Indeed, if you change your definition of disentanglement to having the \"same permutation everywhere\" then the result is correct. But the current phrasing of the Lemma does not suggest this definition, so Example 6 is indeed a counter-example. It's impossible for me to review your modification and make sure it is correct.\n\nThis is a very mathematical paper. I feel like many non-trivial changes to the paper have been done. For instance, the whole section on the model definition has been updated. For some reason, I cannot view the previous versions of the paper, but iirc the mixing function use to take as input z_t and output x_t, correct? This seems to be corroborated by Figure 3 where the StepDecoder has as input only z_t. In the current revision, the model definition allows g to take as input a window of past z_t. This is a non-trivial change in my opinion. What are the repercussions of this change to your proof and the rest of the paper? This is only one of the many changes that this manuscript received. IMO, this version requires a full rereading to make sure everything adds up, i.e. a complete review. That's why I believe this is not something reasonable to ask during a discussion phase."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Relaxing the invertibility of the mixing function in identifiable representation learning is a very important direction and the suggestion that temporal context could be used to infer the latent factors in that case makes a lot of sense intuitively.\n- This suggestion is novel AFAIK.\n- Very few theoretical works of this nature present experiments on realistic data to show the usefulness of their approach, as was done in the present work. This is appreciated."
            },
            "weaknesses": {
                "value": "### **Is there a mathematically explicit example of model that satisfies the assumptions?**\nThe authors assume a standard data generating process where $z^t$ follows some dynamical process and where $x_t = g(z_t)$. However, they do not assume that the mixing $g$ is injective. Instead, they assume that there exists a map $m$ s.t. $z_t = m(x_{t:t-\\mu})$. This feels like a reasonable assumption, however, I think the authors should provide at least one mathematically concrete example where this assumption holds (specifying what is f, g and m explicitly). This is important, in my opinion, to make sure that the result is not completely vacuous in the sense that there exists no model that satisfies the assumption of the theory. Right now it's not entirely clear to me that such an example exists.\n\n### **Math mistakes, unclear proofs and bad terminology**\n- Beginning of 2.1: wrong definition of surjectivity. What you describe is simply the definition of a function (i.e. it has a unique output). A map $f: A \\rightarrow B$ is surjective if, for all $b \\in B$, there exists $a \\in A$ s.t. $f(a) = b$.\n- In the third assumption of Theorem 1:\n    - What is a \u201ccontinuous manifold\u201d? Can you refer to a definition from a math textbook? You mean a topological manifold? Does it mean the support of $\\hat z$ can have a lower dimension than the ambient space?\n    - requiring that $m, \\hat m, g, \\hat g$ are twice differentiable is clear, but the \u201ci.e.\u201d following is confusing. You could simply get rid of it.\n- I\u2019m confused by the fact that the theorems do not reuse Definition 1 with its notion of \u201cobservational equivalence\u201d. Instead, the theorems start with $x_t = \\hat g(\\hat z_t)$ and $\\hat z_t = \\hat m (x_{t:t-\\mu})$. It certainly implies observational equivalence, but is it equivalent? Should I think of the equalities here as \u201chave equal distribution\u201d, or is it a normal equality? Also the theorem does not refer to the data generating process of section 2.1. This is unclear. \n- Definition 1: It looks like it is implicitly assumed that the random vector x_1, \u2026 x_T has a density (w.r.t. The Lebesgue measure). I think this is also assumed in the proof, equation (17), where the change of variable formula for densities is used (which works only for densities). It\u2019s not clear that the random vector x_1, \u2026 x_T has a density. For example, if dim(x) > dim(z), it won\u2019t be the case. Are you assuming dim(x) = dim(z)? I couldn\u2019t find dim(x) anywhere.\n- Definition 1: The authors seem to include $m$ in the parameters of the generative model. I find this a bit weird since the model is fully specified by f, p, g. No need for m in the parameters.\n- Corollary 1: Usually, corollaries are very simple consequences of a theorem. Here, it doesn\u2019t look like it\u2019s a simple consequence, it actually looks like a generalization. Also, would it be possible to unify Theorem 1 and Corollary 1 in a way that both of these results are special cases? Suggesting because restating almost identical assumptions looks a bit inefficient.\n- Equation (3), should be $\\forall x_{t, t-\\mu} \\in \\mathcal{X}^{\\mu +1}$.\n- Section 3.3: The terminology used here does not align with standard terminology used in topology. For example, what the authors call a \u201ccontinuous domain\u201d or a \u201ccontinuous set\u201d is usually called a \u201cpath-connected\u201d set in topology. Please use existing terminology.\n- In the Jacobian on page 7, what do the \u201c*\u201d mean? Zeros? \n- VAE-based approach: \u201cTo uphold the conditional independence assumption, we aim to minimize the KL divergence between the posterior for each time step, p(\\hat zt|xt:t\u2212\u00b5), and the prior distribution p(\\hat zt|\\hat zt\u22121:t\u2212\u03c4 ).\u201d IMO, this shows a poor understanding of what VAE\u2019s are all about. First, for p(\\hat zt|xt:t\u2212\u00b5), the letter \u201cq\u201d should be used to specify that this is not the \u201cactual\u201d posterior of the model, but a variational approximation. Secondly, saying the KL enforces conditional independence is weird. Conditional independence is hard-coded in your generative model, the KL is just part of your evidence lower bound. It\u2019s not present specifically to enforce or encourage conditional independence. \n\n    \n### **Issues in Section 3.3**\n- The authors rightfully points out that one has to be careful when going from \u201cJacobian is a permutation-scaling matrix\u201d to \u201cthe mapping is a permutation composed with an element-wise transformation\u201d when the domain of the function is not simply $\\mathbb{R}^n$. However, [4] already made that point (see beginning of Section 3.1 and the discussion surrounding what they call \"local\" and \"global\" disentanglement).  \n- Moreover, Example 6 from [4] presents a counterexample to Lemma 1, i.e. an example of function with a path-connected domain where the Jacobian is everywhere a permutation-scaling matrix, but the function is not \u201cdisentangled\u201d, in the sense that it cannot be written as a permutation composed with an element-wise rescaling. This implies that Lemma 1 has to be wrong.\n- I also spent some time reading the proof of Lemma 1 and it is unclear. For example, what is the \u201cn-dimensional axis except 0\u201d? You can also find weird terminologies which makes understanding the argument impossible. This makes me even more confident that Lemma 1 is wrong.\n\n### **Writing is unclear/imprecise**\nThe overall quality of writing was low. I found many sentences that were weirdly formulated. For example: \n- Not sure I understand \u201cNon-invertibility by vision persistence\u201d from the intro. Why does the crashing car example have vision persistence? This was not explained, no?\n- \u201cThus, we assume that there exists a maximum time lag $\\mu$ and an arbitrary nonlinear function $m$...\u201d The word \u201carbitrary\u201d shouldn\u2019t be there.\n- \u201cIn this case, there is information loss in $x_t$ due to the non-invertibility of $g$.\u201d This is imprecise. What is meant by information here? I believe what you mean is that one cannot recover $z^t$ from $x^t$ alone.\n- \u201cWe say latent causal processes are identifiable if observational equivalence can lead to identifiability of the latent variables\u2026\u201d This phrasing is weird. They define identifiability, but use the word \u201cidentifiability\u201d in its definition. This should be rephrased.\n- \u201cDue to the complexity of the non-invertible mixing function, the identifiable representation does not indicate the inference function is identifiable.\u201d I don\u2019t understand this sentence. \n- \u201cwith a function m that satisfies our assumption zt = m(xt:t\u2212\u00b5) in existence\u201d Weird sentence formulation.\n- Figure 3 (c), what is the x-axis?\n\n### **Should make more connections with existing works.**\n- Theorem 1 seems to reuse assumptions very similar to [2], which itself reuses assumptions similar to the line of Aapo Hyvarinen\u2019s group, see for example [3]. I think this resemblance should be highlighted in the text to help the reader understand what is truly novel in the proposed theoretical results. In general, I feel like the results could be contextualized in the literature a bit more.\n- [1] should be cited, as it was among the first work showing identifiability was possible in dynamical latent dynamical systems.\n\n\n\n\n\n\n[1] S. Lachapelle, P. Rodriguez Lopez, Y. Sharma, K. E. Everett, R. Le Priol, A. Lacoste, and S. Lacoste-Julien. Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA. In First Conference on Causal Learning and Reasoning, 2022.\n\n[2] W. Yao, G. Chen, and K. Zhang. Temporally disentangled representation learning. In Advances in Neural Information Processing Systems, 2022a.\n\n[3] A. Hyvarinen, H. Sasaki, and R. E. Turner. Nonlinear ica using auxiliary variables and generalized contrastive learning. In AISTATS. PMLR, 2019.\n\n[4] S. Lachapelle, D. Mahajan, I. Mitliagkas and S. Lacoste-Julien. Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation. NeurIPS 2023."
            },
            "questions": {
                "value": "Is the advantage of the proposed algorithm over the baselines on the QA benchmark due to disentanglement and better identifiability? Or is it due to architectural choices? I feel this should be addressed, since the paper is very much centered around disentanglement and identifiability."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2088/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2088/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2088/Reviewer_ur5z"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2088/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698700471783,
        "cdate": 1698700471783,
        "tmdate": 1700778079060,
        "mdate": 1700778079060,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2Pw81eNEpu",
        "forum": "5tSLtvkHCh",
        "replyto": "5tSLtvkHCh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2088/Reviewer_5cYn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2088/Reviewer_5cYn"
        ],
        "content": {
            "summary": {
                "value": "The work studies an identifiability theory of recovering causal latent variables in a non-invertible generation process. The theoretical results show the causal latent variable is identifiable up to permutation and a component-wise transformation under certain conditions. Based on the theoretical study results, the work proposes, CaRiNG, which extends Sequential VAE with a normalizing flow in the latent transition dynamics and an encoder incorporating history information. CaRiNG demonstrates superior performance to baseline methods on synthetic  tasks aligning with the theoretical study. In a real-world experiment setting of understanding traffic dynamics, the proposed approach also demonstrates competitive performance against baseline approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The propose approach is backed by solid theoretical study on the identifiability of latent causal variables; the theoretical study results are also well supported by experiment results under carefully designed synthetic settings."
            },
            "weaknesses": {
                "value": "1. The proposed approach, CaRiNG, is not significantly different from the original Sequential VAE[1], especially from a probabilistic model perspective. There are also existing VAE works[2, 3] that incorporate normalizing flows. The novelty of CaRiNG as a new approach is rather limited.\n2. The presentation of the work needs improvements. The lack of explicit connections between theoretical study (Sec. 3) and model design (Sec. 4) makes the work less readable. In other words, I would suggest the reader to directly connects their model design choices in Sec. 4 to the conditions of their theoretical results in Sec. 3. Moreover, Sec. 3.3 and Sec. 3.4 are primarily supporting or supplementing the theoretical results in Sec. 3.1 and Sec. 3.2 but not critical to the identifiability theory's presentation or the proposal of CaRiNG. Their positioning in the work is distracting in my personal opinion and much of the detailed discussions in Sec. 3.3 and Sec. 3.4 can be moved to the appendix.\n3. The work repeatedly claims the guarantee of identifiability or guarantee of identifiability under mild conditions. However, their theoretical results also rely on the existing of a function $m$ such that $z_t = m(x_{t:t-\\mu})$. It is not clear if this existence condition can be trivially satisfied, especially in real-world settings, including the work's real-world experiment. Even if such a function exists, it is also not clear how to determine $\\mu$.  \n4. The work studies the proposed approach on only one real-world dataset and relies on QA accuracy as a proxy to indirectly evaluate the model's ability to understand the underlying causality. Even though it is challenging to evaluate the identifiability of causal latent dynamics, experiments on different real-world data and different proxy metrics could provide more convincing results.\n\n[1] Chung, Junyoung, et al. \"A recurrent latent variable model for sequential data.\" Advances in neural information processing systems 28 (2015).\n\n[2] Rezende, Danilo, and Shakir Mohamed. \"Variational inference with normalizing flows.\" International conference on machine learning. PMLR, 2015.\n\n[3] Ziegler, Zachary, and Alexander Rush. \"Latent normalizing flows for discrete sequences.\" International Conference on Machine Learning. PMLR, 2019."
            },
            "questions": {
                "value": "Apart from the points in *Weaknesses*, I also have the following questions and suggestions:\n1. Sequential VAE can be viewed as a degenerate version of CaRiNG where prior distribution is another Gaussian with non-zero mean and diagonal variance. It is a valid baseline to compare against and the comparison could also help the work better demonstrate the importance of the proposed design changes of CaRiNG from Sequential VAE.\n2. The transition lag $\\tau$ is an important hyper-parameter of the proposed approach. The work includes ablation study results on the choice of $\\tau$ in controlled synthetic setting. It is actually more important to do hyper-parameter search over its values in real-world settings where we do not know the true underlying generative process to avoid model mis-specification."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2088/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2088/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2088/Reviewer_5cYn"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2088/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698737379039,
        "cdate": 1698737379039,
        "tmdate": 1700610823238,
        "mdate": 1700610823238,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qVXVXGnqOM",
        "forum": "5tSLtvkHCh",
        "replyto": "5tSLtvkHCh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2088/Reviewer_WQAA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2088/Reviewer_WQAA"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the problem of identifying the sources of a data-generating process (similar to ICA) where we assume a temporal scenario, and when the generator from the sources to the observation at a specific time is non-invertible. The authors then assume that the data-generating process is invertible, conditioned on sources from the past, and provide theoretical identifiability results up to permutations and component-wise transformations for three different scenarios. Then, a parametric approach based on variational auto-encoders and normalizing flows is proposed in order to learn the data-generating process, and put the test in synthetic and real-world experiments against previously-proposed approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper addresses an interesting variation of ICA with clear practical usage.\n- The motivation of the paper is quite appealing.\n- The theoretical results look quite impressive and of interest for the community (although I have not looked into the proofs in detail).\n- Empirical results look in principle quite positive and validate the proposed architecture."
            },
            "weaknesses": {
                "value": "- The presentation of the paper leaves a lot to be desired. \n  - W1. I don't fully understand why this work takes such a confrontational stance with respect to the ICA community. From my perspective, and please correct me if I am wrong, everything the paper does is taking the same ICA framework as [1] (non-linear ICA with z independent given another random variable), and assume that the generation process is invertible _given that same random variable_ (in this case, the previous sources). This is quite commendable and interesting, and complements the existing body of work, rather than being obfuscated on \"non-invertibility\" (which is not completely true).\n  - W2. The manuscript does little effort in providing explanations and justifying certain statements (e.g. the entire paragraph before section 3).\n  - W3. Similarly, the mathematical notation is far from standard, convoluted, sometimes wrong, and unnecessarily unwelcoming. E.g.:\n    - In Eq. 3 $T \\circ \\pi \\circ m$ should be in parentheses.\n    - (I think that) the union symbol $\\cup$ is used in places where the Cartesian product is meant to be (e.g. the continuity condition).\n    - Conditions like those from Eq. 6 are overly convoluted for no reason as, e.g., $v_{k,t}$ being linearly independent could be much simplified by saying that the Hessian has non-zero determinant (i.e. is invertible).\n    - Jargon is non-consistent, e.g., secondary differentiable, second-order differentiable, and second order differentiable. Similarly, normalizing flows are then called normalized flows.\n  - W4. I also find section 4 a bit too convoluted to read, and it takes several reads to understand how exactly looks the network proposed by the authors. My advice would be to try to make more explicit the connections between each network component and the theory/data-generating process.\n\nAbout the experiments:\n- W5. I am surprised that there are no comparisons with iVAE, despite being cited.\n- W6. Number of parameters as well as training times for the real-world experiments seem necessary to me.\n- W7. While real-world results are ok, I find the discussion deceivingly positive, since CMCIR obtains better results on average and beats CaRiNG quite significantly in some individual question types.\n\n\n[1] Khemakhem, I., Kingma, D., Monti, R., & Hyvarinen, A. (2020, June). Variational autoencoders and nonlinear ICA: A unifying framework. In International Conference on Artificial Intelligence and Statistics (pp. 2207-2217). PMLR."
            },
            "questions": {
                "value": "- Q1. I don't think I understand what is the column \"All\" in Table 2. Is it the mean of the other columns? Because if that is the case, these numbers do not add up."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2088/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2088/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2088/Reviewer_WQAA"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2088/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698865813353,
        "cdate": 1698865813353,
        "tmdate": 1700740477516,
        "mdate": 1700740477516,
        "license": "CC BY 4.0",
        "version": 2
    }
]