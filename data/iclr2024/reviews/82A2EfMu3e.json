[
    {
        "id": "lOrR8UvwEn",
        "forum": "82A2EfMu3e",
        "replyto": "82A2EfMu3e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1785/Reviewer_HW8p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1785/Reviewer_HW8p"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a discretization method for time-dependent partial differential equations. Time is discretized using a standard Crank-Nicolson scheme. At every time step, this yields a time-dependent PDE that only has spatial derivatives. This time-independent PDE (at each time step) is approximated by Neural networks by minimizing a PINN-type *physics-informed loss function* based on the residual of the time-independent PDE at each time step. Thus, a series of neural networks are trained sequentially in order to approximate the solution. Moreover, the training at each time-step is initialized with weights from the previous time-step constituting a *transfer learning* procedure. A error estimate is provided for this method for a semi-linear PDE with some assumptions on the nonlinearity. Numerical results comparing the method with other PINN baselines is also provided for a set of PDEs in one and two space dimension (Reaction-Diffusion, Allen-Cahn, Kumamoto-Shivashinkshy and Navier-Stokes)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper attempts to address an important question: how to accelerate the training and enhance the performance of neural network based PINN type methods for the numerical solution of partial differential equations.\n\n- An error estimate is derived.\n\n- Standardized benchmarks are presented."
            },
            "weaknesses": {
                "value": "- **Novelty**: It is difficult to see what is truly novel in this paper. The idea of using a time-discrete, also called as method of lines, PINN type formulation where a standard time stepping scheme (implicit RK2) leads to time-independent PDEs, which are then discretized with neural networks with a PINN type loss function, is already explicitly stated in Raissi et al paper reintroducing PINNs (the ideas of PINNs go back to Dissanayake et al in 1994 and Lagaris et al in 1998 and 2000). This fact is also acknowledged by the authors themselves. All the authors have done is to replace Implicit RK2 with Crank-Nicolson. This does not constitute sufficient novelty at all as anyone can replace an existing ODE solver with another existing ODE solver -- there is no reason to believe that CN is any better than implicit RK2. Moreover, what the authors call as transfer learning is pretty straightforward as all the authors do is to initialize the weights at each time step with those at the previous time-step. Although I did not check the code of the Raissi et al paper, I am fairly sure that they do the same too -- what else can make sense ? Moreover, the theoretical result is a straightforward adaptation of well-known techniques (see Mishra and Molinaro) and is only holding for a very strong condition on the nonlinearity (monotonicity of the nonlinear operator) which is not satisfied for any of the numerical examples presented here !! Furthermore, Mishra and Molinaro already proved an error estimate for the original PINNs for time-dependent PDEs -- so theory cannot discriminate between PINNs and discrete PINNs, at least at the level of approximate error, meaning that the theoretical contribution is not novel either. \n\n- **Rationale and Numerical Experiments** Given the above caveats on novelty, a possible strength of the paper could be impressive numerical demonstration. Examining the paper closer on this front, we see that the authors consider 4 fairly standard test cases, 3 of them one-dimensional PDEs (RD, AC and KS) and one, the standard two-dimensional Navier-Stokes equations on a periodic square with a Reynolds number of 100. The baselines are all PINN-based or their variants, including casual PINNs. While the proposed method TL-DPINN1 and TL-DPINN2 is the best-performing, both with respect to accuracy and time over standard PINNs, the results are totally underwhelming to any reader familiar with numerical methods. For instance, it takes 748 Secs or more than 12 minutes to solve a straightforward 1-D reaction-diffusion equation with their best-performing method. What is the time taken (for similar accuracy) with a standard Crank-Nicolson based finite-difference scheme ? I suspect that it will be at best a few seconds. So, your method is two orders of magnitude closer to than text book numerical solvers !! Similarly for the 2-D Navier-Stokes, this reviewer ran a straightforward spectral method solvers to obtain a run-time of less than 0.1 secs for an accuracy of $3\\%$ -- even a textbook solver will take around a few secs. On the other hand, your best performing method takes more than 3 hours (is 4 orders of magnitude slower) !! -- so I urge the authors to present some standard numerical method based baseline to see how slow your method is compared to what is out there. The authors will argue that they have baselined against PINNs. This argument is not good enough as for a method to be adopted, it has to be compared to the best that is out there, not just neural network baselines. Otherwise, publication in a leading ML venue is difficult to justify. \n\n\nOf course, PINNs and their variants might have other advantages that standard numerical methods don't -- for instance problems with complex geometries, very high-dimensional problems (such as Radiative transfer, see Mishra and Molinaro JQSRT for instance) or parametric problems. However, the authors do not consider any of these problems and restrict their attention to simple problems where numerical solver baselines are several orders of magnitude faster, diminishing the impact of their contribution."
            },
            "questions": {
                "value": "1. Last sentence of Page4 and first sentence of page 5; you say that transfer learning is performed by solely updating the weights in the last hidden layer per time step. However in Algorithm 1 (page 5, line 10) it seems that all the parameters are updated after initializing them from the trained parameter at previous time-step. Which one is it ?\n\n2. Please address the questions that arise in the weaknesses section, particularly about novelty and numerical baselines.\n\n3. How do you choose the time-step for the Crank-Nicolson method ? In particular, can you choose large time-steps for stiff problems. In that case, please demonstrate how the method performs by considering the following wave equation: \n\n$u_t + \\frac{a+b}{2} u_x + \\frac{b-a}{2} v_x = 0, v_t + \\frac{b-a}{2} u_x + \\frac{a+b}{2} v_x = 0$. You can use transparent boundary conditions -- the two wave speeds are a and b. Choose a << b to make the problem stiff and see how the time-step can be chosen."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1785/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1785/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1785/Reviewer_HW8p"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1785/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698661529843,
        "cdate": 1698661529843,
        "tmdate": 1699636107952,
        "mdate": 1699636107952,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "A75eJwcHDc",
        "forum": "82A2EfMu3e",
        "replyto": "82A2EfMu3e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1785/Reviewer_rdRW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1785/Reviewer_rdRW"
        ],
        "content": {
            "summary": {
                "value": "This paper deals with training of PINNs for transient PDEs. One major challenge in training PINNs for such evolutionary systems is that PINNs may violate temporal causality, i.e. the model may not be sufficiently trained at a given time step before approximating the next time steps. This is because the collocation points for PINNs are all trained simultaneously. The authors propose transfer learning enhanced discrete physics-informed neural networks (TL-DPINN) which has two key contributions: i) by using implicit time discretization the algorithm resolves temporal differentiation, ii) by using the transfer learning the algorithm accelerates the PINN training. In theorem 4.1, they show that, under certain assumptions, the error of training defined as the difference of ground truth solution (of dissipative PDEs) at a given time and that of the neural network is bounded. Four examples are considered, which progressively becomes more challenging, and several time-discretization schemes are compared."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The examples provided are involved enough to show the strength of the proposed algorithm. Example 1 and 2, although only 1D, are demonstrating the solution of nonlinear PDE. KS equation can become chaotic, so the success of TL-DPINN makes it a meaningful contribution to the community (although we still see some struggles after t=0.8). The NS example is in turbulent regime which poses a nontrivial problem for PINNs.\nSection 5.5 is also a nice and careful addition of the paper. Comparison of SOTA plus considering several time scheming is a valid scientific work and is considered as a strength."
            },
            "weaknesses": {
                "value": "Although the theorem is a nice addition to the paper, the underlying assumptions seem too restrictive. In the domain of PDE analysis, I don't see it as a \"weakness\" (specially for nonlinear PDEs, its a daunting task, and often impossible, to come up with a generic theoretical framework) but more discussion is needed about the limitation of the work. What are the implications of assumption 1, and when do they fail (as an example)? Only a brief addition can be helpful (more details can be discussed in the appendix).\n\nA major contribution of the paper is its success in the solution of chaotic PDEs. Some discussion in A.5.1 about limitations can be moved to the proper text to make this work more useful (and fair) for readers. From the results, seems like still we have room for improvement for such complicated scenarios, but this paper can have its own place; just need a further discussion about weaknesses and limitations (so the claims need to be revisited). \n\nAlso, the use of implicit time discretization is not novel. Authors mention it has been used for ODEs, so the application to PDEs is a fair contribution but is this the only novelty? Moreover, the use of transfer learning for speeding up the convergence is nice but there has been some works (for example using parallel in time algorithms for PINNs) to speed up the convergence for evolutionary PDEs. The combination of the two ideas seem like a novel algorithm but hopefully authors can clarify on the novelties further in the revised version.\n\nOverall, I'm positive and would be happy, after addressing my concerns above, to update my score. Overall, I see this paper as a practical improvement for PINNs, rather than theoretical, but it has a good skeleton (because of nice examples and clear case studies)."
            },
            "questions": {
                "value": "More like a suggestion: are authors familiar with NIF (https://arxiv.org/pdf/2204.03216.pdf). They treat time and space separately via shapeNet and paramterNet (they consider time like a parameter). Of course this is a different method, but gives the opportunity for treating time in a continuous way. I appreciate if authors open up the conversation on how they see discrete versus continuous treatment of time will impact the solution of evolutionary PDEs. Of course they can argue the original PINNs treat time as \"continuous\" but it comes with cost of causality issue but there might be solutions for this that are not discrete time version. It will be good for the community to start a conversation two approaches (from practical and theoretical point of views). Another similar work: https://arxiv.org/pdf/2206.02607.pdf\n\nHow would you comment on using transfer learning for original PINNs (without discrete time analysis)? No need for producing any results.\n\nI couldn't find the discussion on how to pick the weights (\\lambda values). How do authors tune such values? Do they use static values or are they updating such weights adaptively. It is important to add a discussion on how to pick such values.\n\nAuthors argue that Wang et all 2022a suffer from sensitivity to epsilon on page 2. One could argue that their approach this approach also introduces its own hyper-parameters such as Nt and M0, M1, etc. How do authors justify that?\n\nTo confirm, \\matcal{L}^0(\\theta^0) is simply learning the initial conditions?\n\n Please clarify the implications of assumptions 4.1 and 4.2. Also the text for theorem 4.1 needs some revising (e.g. there is no equation 4.1, etc.). The statement on page 5 \"The second assumption can be verified by the standard regularity estimate result of PDEs Evans\n(2022), and we omit it here for brevity.\" can be clarified in Appendix."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1785/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773652907,
        "cdate": 1698773652907,
        "tmdate": 1699636107872,
        "mdate": 1699636107872,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BjFbNfxWAI",
        "forum": "82A2EfMu3e",
        "replyto": "82A2EfMu3e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1785/Reviewer_sf3Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1785/Reviewer_sf3Z"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method to improve the training of Physics-informed neural networks (PINNs) for solving time-evolving partial differential equations (PDEs) that exhibit complex behaviors. By incorporating implicit time differencing and transfer learning, the method enforces temporal causality and efficiently updates the PINNs for different time frames. Specifically, instead of learning just one network for the whole time evolution, the paper proposes to first fix time steps, and then learn the update network at each time step. Empirical results show improvements on the accuracy and efficiency of PINNs with convergence proven under certain conditions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The problem trying to solve is interesting, enforcing causality in PINNs is essentially providing more physical insights for the problem. The empirical improvement over the causal PINN in training time is significant, there are cases where the PINNs can not solve the PDE but this one can, and the theoretical results are sound."
            },
            "weaknesses": {
                "value": "The method achieves causality by fixing a time step and train separate networks for each time step. Despite the good performances, this constraints the PINNs to be meshless on time scale. Besides, training and saving many networks is expensive, especially for long time predictions. \n\nI understand that transfer learning helps mitigate this problem, and Table 1 shows that the training time is lower than other methods, but it would be very helpful if the author can provide a figure, where the x-axis is the number of time steps or the max time for prediction, and the y-axis is the training time and logits, so we can have a better idea as to when does the proposed method becomes too cumbersome. I am willing to raise the score if this question is properly addressed.\n\nAlso, does it require more data to train the proposed method than vanilla PINNs as we are training much more parameters? If so, how much more?"
            },
            "questions": {
                "value": "The main concern is on the scalability of the proposed method. Specifically, the method mentions that transfer learning works well when dt is small, can you specify how small is that, and does this change for different PDEs? Also, how does this method work for higher-dimensional PDEs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1785/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1785/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1785/Reviewer_sf3Z"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1785/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699464729882,
        "cdate": 1699464729882,
        "tmdate": 1699636107787,
        "mdate": 1699636107787,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "h3zscOKgi8",
        "forum": "82A2EfMu3e",
        "replyto": "82A2EfMu3e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1785/Reviewer_XCvN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1785/Reviewer_XCvN"
        ],
        "content": {
            "summary": {
                "value": "In this paper authors propose a discrete PINNs model for solving evolutionary partial differential equations with multi-scale or turbulent behavior over time. The method uses implicit time differencing schemes to enforce temporal causality. In addition, the method inherites ideas from transfer learning methods to sequentially update the PINNs in space as surrogates for PDE solutions in different time frames. A variety of benchmarking problems, as well as ablation studies were considered to evaluate the performance of the proposed method, and for some test cases the results were compared with a few PINNs baseline methods."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The attempt of making PINNs model more robust for efficient and accurate prediction of evolutionary/stochastic behaviors sounds interesting.\n- extensive numerical experimentations is also appreciated.\n- The code and the input data are shared which is helpful for benchmarking and reproducing the presented results."
            },
            "weaknesses": {
                "value": "There are some key missing factors which make me not recommend the publication of this paper. Below I mention just the key points and I skip the ones with less priority for me:\n- the transfer learning proposal is not new idea for SciML problems in general, and the method utilized in this paper (fine tuning only last layers) has been used in other papers as well. Authors have cited a few related works, there are more examples which are closely related, checkout the following papers for example :\n  - Towards Foundation Models for Scientific Machine Learning:Characterizing Scaling and Transfer Behavior (https://arxiv.org/pdf/2306.00258.pdf)\n  - The Old and the New: Can Physics-Informed Deep-Learning Replace Traditional Linear Solvers? (https://arxiv.org/abs/2103.09655)\n  - Transfer Learning Enhanced DeepONet for Long-Time Prediction of Evolution Equations (https://www.semanticscholar.org/paper/Transfer-Learning-Enhanced-DeepONet-for-Long-Time-Xu-Lu/a2ab63a272fa0762a75eb3d1b0a86506a9db3623)\n\n- Following the previous point, its effectiveness of the transfer learning in this work heavily relies on the choice of $\\Delta t$. Authors qualitatively mentioned that \"if a small enough time step is used, there are little difference between two adjacent network parameters, $\\theta^n$ and $\\theta^{n+1}$\". However, as far as I realized there is no concrete discussions/numerical experiments provided to show how much the time-step parameter could be increased and where the applicability of fine-tuning last layers could break. Not addressing such points and addressing things qualitatively and at the surface level questions the quality and the comprehensiveness of the research conducted.\n\n- In theoretical result, authors made the assumption that the equation must be dissipative, which I did not quite get this part. First of all what authors mean by equations being dissipative? does it mean the equations always require dissipative/diffusive terms? It also makes me wonder whether this method is applicable for a general hyperbolic equations potentially with some external forces, such as high Reynolds regime flows?\n\n- The presentation of the paper needs significant improvement. For example, some of result visualizations and discussion are not informative, for example what authors try to convey by presenting Figure(3)? in particular for (b) how does it show that their method reduces the number of epochs required at each time-step, because overall it requires more than 500 epochs and for intermediate steps it jumps to much larger numbers? There are multiple examples like this in the main body of the paper where the results were not discussed clearly.\n\n- I did not see that authors discuss any limitations or challenges associated with the proposed method. Note that there is no free lunch :) A good quality science work needs to discuss different aspects of the proposed method including the drawback. I encourage authors to share these types of contents in their science publications."
            },
            "questions": {
                "value": "- Authors mentioned in Appendix that a multi-head neural network may lead to slow convergence, which is not clear to me why? Having a multi-head single neural network could be beneficial in terms of sharing parameter and reduce the computational effort.\n- For the MLP network considered in this work, I wonder how authors decide to use $tanh(.)$ for activation functions? it would be useful to provide ablation study to numerically provide insight why $tanh(.)$ was selected.\n- Regarding Fourier Features, which were justified for periodic and oscillatory behaviors, I wonder what about problems that do not involve such behavior and whether these types of feature embedding could negatively impact the results? It would be great if authors could provide a clear discussion on this.\n- Since authors literally do time integration numerically, I wonder if they numerically test their numerical accuracy of their prediction and whether it is second order for Crank-Nicolson scheme? \n- The experimental setting is not quite clear. For their evaluations did authors consider out of domain data (data outside training range), or the focus was on in-domain evaluation? Also, did they explore whether their method could help for long term predictions (such as weather forecasting problems?)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concern."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1785/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699514443333,
        "cdate": 1699514443333,
        "tmdate": 1699636107716,
        "mdate": 1699636107716,
        "license": "CC BY 4.0",
        "version": 2
    }
]