[
    {
        "id": "1ZUNUoHHG0",
        "forum": "C53xlgEqVh",
        "replyto": "C53xlgEqVh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4595/Reviewer_zJWA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4595/Reviewer_zJWA"
        ],
        "content": {
            "summary": {
                "value": "This work introduces VEC-TOK SPEECH, an extensive framework for a variety of speech generation tasks. The framework integrates an innovative codec that is capable of disentangling linguistic content and acoustic details from speech, as well as a language model tailored for conditional semantic token generation. Moreover, the authors ease the token sequence length issue by implementing the BPE technique. The experimental results confirm the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThis work introduces a novel paradigm: representing relatively simple linguistic content with a single semantic token sequence, while capturing complex audio details using continuous vectors, as opposed to the multiple acoustic token sequences in previous works. This design can lessen the challenges of language modeling and enhance generation quality.\n2.\tThe article consolidates multiple speech generation tasks within a single, concise, and scalable framework."
            },
            "weaknesses": {
                "value": "1.\tThe paper may need more experimental results to improve its soundness, including:\na) The state-of-the-art (SOTA) model for cross-lingual zero-shot TTS is not VALL-E X, but Mega-TTS[1]. Including such a comparison would make the paper sounder.\nb) The reconstruction quality of the proposed codec is not assessed thoroughly.\n2.\tThe quality of the provided samples is not satisfactory when compared with the samples from [1] (demo link: https://mega-tts.github.io/demo-page/).\n3.\tThe abstract contains some overstatements. Indeed, this work is not the first work to unify multiple speech generation task ([1-4]).\n        `` \"In contrast, the current speech generative models are still struggling regarding speech quality and task generalization.\"``\n\n4.\tThe use of BPE for semantic tokens was first proposed in [5]. Including the reference and a discussion on this topic may be necessary.\n\n[1] Jiang, Z., Ren, Y., Ye, Z., Liu, J., Zhang, C., Yang, Q., Ji, S., Huang, R., Wang, C., Yin, X., Ma, Z., & Zhao, Z. (2023). Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias. ArXiv, abs/2306.03509.\n\n[2] Shen, K., Ju, Z., Tan, X., Liu, Y., Leng, Y., He, L., Qin, T., Zhao, S., & Bian, J. (2023). NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers. ArXiv, abs/2304.09116.\n\n[3]  Huang, R., Zhang, C., Wang, Y., Yang, D., Liu, L., Ye, Z., Jiang, Z., Weng, C., Zhao, Z., & Yu, D. (2023). Make-A-Voice: Unified Voice Synthesis With Discrete Representation. ArXiv, abs/2305.19269.\n\n[4] Le, M., Vyas, A., Shi, B., Karrer, B., Sari, L., Moritz, R., Williamson, M., Manohar, V., Adi, Y., Mahadeokar, J., & Hsu, W. (2023). Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale. ArXiv, abs/2306.15687.\n\n[5] Chou, J., Chien, C., Hsu, W., Livescu, K., Babu, A., Conneau, A., Baevski, A., & Auli, M. (2023). Toward Joint Language Modeling for Speech Units and Text. ArXiv, abs/2310.08715."
            },
            "questions": {
                "value": "1. Does the SCS metric measure the similarity between the generated audio and the original recording or the reconstructed samples?\n2. Do consecutive repeating tokens impact the BPE efficiency? Could you provide the average token sequence length after deduplicating the tokens?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4595/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4595/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4595/Reviewer_zJWA"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698464981034,
        "cdate": 1698464981034,
        "tmdate": 1699636438154,
        "mdate": 1699636438154,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hZhbG374o2",
        "forum": "C53xlgEqVh",
        "replyto": "C53xlgEqVh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4595/Reviewer_KgQK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4595/Reviewer_KgQK"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel speech generation model called Vec-Tok Speech with a codec  leveraging speech vectorization and tokenization to facilitate various speech generation tasks. The proposed architecture is beneficial for both high-fidelity speech reconstruction and accurate linguistic content of speech.\nThe paper further introduces Byte-Pair Encoding technique in order to reduce the token length and bit rate for lower exposure bias and longer context coverage, thus improving the performance of language models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper tackles the shortcomings of the previous works related to neural audio codec, RVQ-based codecs, which leads to information redundancy and increases the difficulty of predicting the tokens in downstream tasks, by proposing Byte Pair Encoding to compress the length of semantic tokens.\n2. The authors have conducted extensive amounts of downstream tasks, showing the superiority of Vec-Tok Speech: Zero-shot VC, Zero-shot speaking style transfer TTS, Speech to Speech translation, Speech denoising and bandwidth extension, and speaker de-identification and anonymization.\n3. This paper is well structured and easy to read."
            },
            "weaknesses": {
                "value": "1. The proposed model seems to be the combination of the existing technique: vectorization, tokenization, and BPE, without any novel architecture.\n2. I am not sure it is novel enough to adapt Byte-Pair Encoding technique to compress the length of semantic tokens. Many existing works have leveraged the BPE technique in audio processing [1,2], so it is better to distinguish the proposed model from the ones that have utilized the BPE. \n3. I think even though the authors have presented various amount of downstream task, the performances of previous works that they are comparing with seem to be limited overall. For example in zero-shot tts, why not comparing with Voicebox?\n4. Moreover, there is no quantitative performance on speech denoising. Although the results are shown in the demo page, it is better to report the qualitative results with metrics like MCD, STOI, PESQ for performance reporting.\n\n[1] Elkahky, Ali, et al. \"Do Coarser Units Benefit Cluster Prediction-Based Speech Pre-Training?.\" ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023.\n\n[2] Algayres, Robin, et al. \"Generative Spoken Language Model based on continuous word-sized audio tokens.\" arXiv preprint arXiv:2310.05224 (2023)."
            },
            "questions": {
                "value": "Please refer to the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4595/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4595/Reviewer_KgQK",
                    "ICLR.cc/2024/Conference/Submission4595/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698749039517,
        "cdate": 1698749039517,
        "tmdate": 1700638664949,
        "mdate": 1700638664949,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pfMXnFXaze",
        "forum": "C53xlgEqVh",
        "replyto": "C53xlgEqVh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4595/Reviewer_VMQy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4595/Reviewer_VMQy"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a new way to encoding speech, by using continuous feature to capture acoustic and discretized token to capture semantics. Based on this mixed codec, a new framework been proposed to to speech generation task. Downstream task include TTS, voice conversion and speech to speech translation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) I would like to highlight one concept the paper proposed which is different than previous audio lm based work, it's not necessarily to discretize both acoustic and semantic information. A combined approach (discretize and continuous) might also work.   \n\n(2) I covers many different applications, and the framework is easy to adopt."
            },
            "weaknesses": {
                "value": "(1) The paper is poorly written. Section 3.2 is very hard to understand. I would suggest anything in the figure, should clearly defined in the method section. For example, \"codec decoder\". I also highly suggest the author describe tortose (Betker, 2023) in the related work. It seems some component borrowed from here, but it's unclear which part is being used.  \n\n(2) The way to disentangle acoustic and semantic are very empirical, e.g. assume 6-layer of wav-lm, assume mean capture speaker information. There is no solid evidence to justify those claim.\n\n(3) Results are unsound. It keep mention low bit rate, but I even don't know what the bit rate used here. From the demo, the reconstruction audio quality are poor, which make it hard to believe the proposed codec are really working."
            },
            "questions": {
                "value": "\"First, these codec tokens usually contain as many speech attributes as\npossible to high-fidelity reconstruction quality, which leads to information redundancy and increases\nthe difficulty of predicting the tokens in downstream tasks....\" \n\nOne key contribution of the paper is replace acoustic tokens in audioLM. But those claim are very vague, are there any empirical or theoretical analysis to justify this claim?\n\n\"a neural vocoder is used to reconstruct speech waveforms based on the extracted speech vectors vec\nsince the vocoder can produce waveforms that are nearly indistinguishable from recorded waveforms\nand are highly generalizable outside of the training set (Betker, 2023). \"\n\nI highly recommend the author fix citation like this, what are citing here? the claim or the decoder architecture or something else?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4595/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4595/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4595/Reviewer_VMQy"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699127999658,
        "cdate": 1699127999658,
        "tmdate": 1700645848015,
        "mdate": 1700645848015,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6XeHpxgQWA",
        "forum": "C53xlgEqVh",
        "replyto": "C53xlgEqVh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4595/Reviewer_G28A"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4595/Reviewer_G28A"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an encoder-decoder architecture that decouples acoustic style and semantic linguistics. The semantic tokens can be combined with prompts for LLMs to generate output token sequence which could be further combined with optionally modified acoustic style vectors to reconstruct speech for various speech tasks. The authors conducted experiments in VC, TTS, and S2ST to demonstrate that their proposed system can match or outperform some recent models on those tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality: Decoupling or disentangling speech style and speech content with neural networks has been long studied since VAE and GAN. The authors are able to leverage recent developed models such as WavLM to extract speech vectors for downstream speech generation tasks. Using K-mean to cluster speaker vectors and using BPE to compress token sequences are also popular techniques in related publications, but the authors also incorporate them as essential components in their speech generation system. They also proposed inverse K-means that includes speaker vectors as part of the prompts to generate speech vectors rich in speaker style. \n\nQuality: Despite the novel engineering integration and encouraging experimental\u00a0results, the authors did not attempt to develop their proposed system with a more theoretical approach. As a result, readers may not be able to gain as many insights as to why the proposed system and its components can outperform the competing systems.\n\nClarity: The paper is well-written and easy to follow. There are few errors. The authors used high level equations to describe their system design. The use of diagrams and coloring schemes are appropriate. The result tables contain the right amount of information for the readers. The demos on the github sites are well-organized.\n\nSignificance: The paper engages the latest trend of speech research in the community by bridging the use of LLM into speech generation."
            },
            "weaknesses": {
                "value": "A few sections are written without adequate explanation of the symbols or extensive reader knowledge is assumed beyond a reasonable context. For example, in Equation 6, a more detailed description of the adversarial setup should be given. Why is the loss constructed this way? Why is MPD and MSD selected? The equation of the feature matching loss (not named as such in the reference) and reconstruction loss (which norm?) should also be clearly stated.\u00a0\u00a0\nMinor typo below equation 9. ^vec instead of ^wave.\u00a0Less well-known acronyms such as CLVP in section 4 should be expanded first.\u00a0The experimental section should also compare the model sizes of the proposed system and the competing baselines."
            },
            "questions": {
                "value": "In Figure 1, what's the use of two codec encoders for the TTS pipeline?\u00a0\nDid the author compare the performance of TTS and S2ST with AudioPaLM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4595/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4595/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4595/Reviewer_G28A"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699421449164,
        "cdate": 1699421449164,
        "tmdate": 1699636437655,
        "mdate": 1699636437655,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TOFta3wb4U",
        "forum": "C53xlgEqVh",
        "replyto": "C53xlgEqVh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4595/Reviewer_cYW2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4595/Reviewer_cYW2"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a new generative speech network that combined conditioning on discrete tokens for semantic content with conditioning on continuous features for audio style. They use a pre-trained WavLM model as a source of both features where the discrete features are obtained via K-means clustering WavLM features, and the continuous features are encoded directly from WavLM features. During training, a language model is trained on BPE encoded discrete token sequences, while a decoder is trained on a combination of discrete and continuous inputs where the former are processed via an \"inverse K-means\" process. The resulting output is fed into a vocoder to produce speech.\n\nThe authors show that such a model can perform several tasks very well, including voice conversion, zero shot speaker style transfer TTS and speech to speech translation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Strong results on several benchmarks across several tasks\n- Novel architecture that utilizes discrete and continuous features and avoids multi-pass decoding like VALL-E/SoundStorm"
            },
            "weaknesses": {
                "value": "- Quite complex and depends on previous pre-trained models (e.g. WavLM)\n- No comparison with diffusion based models like NaturalSpeech2\n- Missing ablations make it difficult to understand which parts contribute to the performance of the model. While certain parts of the architecture are ablated for certain tasks, it would be nice to ablation results for each task to understand how modeling choices (BPE, inverse K-means) contribute to the performance. It would also be good to know how much WavLM contributes to the overall performance or if it is replaceable with any other model. Choices on the LM side (how necessary it is to produce 256 candidates and then rank them? etc) are not ablated at all.\n- Speech to speech translation results compare to one other prior work that outperforms the proposed architecture on BLEU and is missing all other metrics making comparison difficult."
            },
            "questions": {
                "value": "Most of my questions relate to the weaknesses section:\n- Could you provide additional ablations that can make it clear what modeling choices lead to what outcomes?\n- Could you provide a more thorough comparison on the STST task?\n- Could you ablate the choice of relying on WavLM as the primary encoder in this method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699580047409,
        "cdate": 1699580047409,
        "tmdate": 1699636437585,
        "mdate": 1699636437585,
        "license": "CC BY 4.0",
        "version": 2
    }
]