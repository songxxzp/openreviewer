[
    {
        "id": "in3xy26ATr",
        "forum": "3nyovHUr5A",
        "replyto": "3nyovHUr5A",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5868/Reviewer_Yvwi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5868/Reviewer_Yvwi"
        ],
        "content": {
            "summary": {
                "value": "This work focuses on a critical real-world problem - learning with noisy labels. Building upon the sample selection idea, this work further distinguishes between *hard* samples and *easy* samples. In a iterative process, anchors are obtained through the 'hallucination' of *easy* samples, and subsequently, the nearest *hard* samples are identified using these anchors. These identified *hard* samples, along with the *easy* ones, constitute a labeled subset, which is then utilized for semi-supervised training through MixMatch. The method is evaluated with several benchmarks datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The idea and intuition of the proposed method is simple and clear."
            },
            "weaknesses": {
                "value": "The author designed an intricate framework for selecting challenging samples, as mentioned above, while the core idea is intuitive and reasonable, the primary issue lies in the fact that the whole method are entirely based on heuristics without any theoretical underpinning. I understand that this might be challenging for researchers who do not engage in theoretical analysis. However, extensive ablation studies(currently none) are necessary to provide evidence. For example, people only start to *abuse* 'small-loss mechanism' until several works on memorization effect with extensive experiments been published and acknowledged. Currently it is not convicing for me that utilizing a 'hallucinator' to simulate hard features and then using the neighbroing relations to identify *hard* samples would yield better results than replacing the 'hallucinator' with a simple interpolation between the mentioned two classes, or even abndon the whole techniques, merely relying on the implicit label relabelling effect inherenty in MixMatch itself.\n\n\nMinor: \n\n1) there are many repeated entries in the reference:\n\n*Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. In\nMaster\u2019s thesis, University of Toronto, 2009.*\n\n*Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n2009.*\n\n*Haobo Wang, Ruixuan Xiao, Yiwen Dong, Lei Feng, and Junbo Zhao. Promix: Combating label\nnoise via maximizing clean sample utility. arXiv preprint arXiv:2207.10276, 2022a.*\n\n*Haobo Wang, Ruixuan Xiao, Yiwen Dong, Lei Feng, and Junbo Zhao. ProMix: Combating label\nnoise via maximizing clean sample utility. arXiv preprint arXiv:2207.10276, 2022b.*\n\n\n*Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Combating noisy labels by agreement: A joint\ntraining method with co-regularization. In CVPR, 2020a.*\n\n*Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Combating noisy labels by agreement: A joint\ntraining method with co-regularization. In CVPR, pp. 13726\u201313735, 2020b.*\n\n2) Some typos need to be fixed."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5868/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760338186,
        "cdate": 1698760338186,
        "tmdate": 1699636621864,
        "mdate": 1699636621864,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "C7kbRicmh6",
        "forum": "3nyovHUr5A",
        "replyto": "3nyovHUr5A",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5868/Reviewer_VL2q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5868/Reviewer_VL2q"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors present a novel framework to tackle the underestimation of hard samples in classic selection-based Noisy-Label Learning (NLL) methods. By leveraging easy samples to hallucinate the hard anchors, the proposed approach captures crucial information from hard samples in the presence of instance-dependent noise. They utilize the easy subset to hallucinate multiple anchors, which are used to select hard samples to form a clean hard subset. The proposed framework achieves significant performance improvement in learning from both synthetic and real-world IDN datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe paper is well-written and well-organized.\n2.\tThe proposed method is simple and easy to follow."
            },
            "weaknesses": {
                "value": "1.\tThe experiments were not sufficient, and some key ablation experiments were not performed. For example, whether the performance improvement was brought about by the expanded hallucination data or by the selected clean hard samples? How accurate are the selected clean hard samples compared with true labels? \n2.\tThe comparison with some methods of selecting samples based on feature distance is not clearly explained, such as [1][2].\n [1] A topological filter for learning with label noise, NeurIPS,2020\n [2] \u201cNGC:A unified framework for learning with open-world noisy data, ICCV,2021"
            },
            "questions": {
                "value": "1.\tKey ablation experiments were not performed. Whether the performance improvement was brought about by the expanded hallucination data or by selecting clean hard samples? The hallucination process is equivalent to data augmentation. How much accuracy improvement does such data augmentation bring? In addition, how does it compare with mixup? [3]\n2.\tHow accurate are the selected clean hard samples compared with true labels? How much accuracy improvement does clean hard sample selection bring?\n3.\tHow does the proposed method perform on IIN data?\n[3] mixup: Beyond Empirical Risk Minimization, ICLR2018"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5868/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698890637599,
        "cdate": 1698890637599,
        "tmdate": 1699636621771,
        "mdate": 1699636621771,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MfblwmHPXF",
        "forum": "3nyovHUr5A",
        "replyto": "3nyovHUr5A",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5868/Reviewer_T8rR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5868/Reviewer_T8rR"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a method for learning with IDN, which focus on select hard clean examples."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The motivation to select clean hard examples is reasonable."
            },
            "weaknesses": {
                "value": "The main problem is the motivation. Hard clean examples are generated by using the hallucination loss, which encourages generated examples close to the decision boundary, while still residing on the targeted side. If we consider the boundary to be correct, then there is no need to generate clean hard examples. If we consider the boundary to be wrong. Then the generated hard examples cannot be guaranteed to be correct. \n\nThe improvements against SotA are trivial, less than 1% in most cases. In real-world noisy datasets Clothing1M, the results are clearly inferior to SotA methods."
            },
            "questions": {
                "value": "Please clarify the concerns about the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5868/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699341860461,
        "cdate": 1699341860461,
        "tmdate": 1699636621682,
        "mdate": 1699636621682,
        "license": "CC BY 4.0",
        "version": 2
    }
]