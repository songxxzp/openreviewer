[
    {
        "id": "6MlUzKmMe1",
        "forum": "ZJHdiYDD5k",
        "replyto": "ZJHdiYDD5k",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission936/Reviewer_9TQZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission936/Reviewer_9TQZ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces LatentWarp, a framework for zero-shot video-to-video translation using image diffusion models. It addresses the challenge of maintaining temporal consistency in generated video frames. LatentWarp focuses on constraining query tokens for temporal consistency. It achieves this by warping the latent features from the previous frame to align with the current frame using optical flow information.  Extensive experiments confirm the superiority of LatentWarp in achieving high-quality video-to-video translation with temporal coherence."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The introduction of the LatentWarp framework offers a novel approach to zero-shot video-to-video translation. LatentWarp  emphasizes on preserving temporal consistency during video frame generation, achieved through optical flow and warping operations, significantly enhances temporal coherence,  which is a crucial aspect of video generation. The writing is good, and the structure of the paper is clear."
            },
            "weaknesses": {
                "value": "I find this method to be quite intuitive. My concerns mainly pertain to the experimental aspects:\n\n1. Data-related issues: The authors do not compare their method to datasets used in their previous work like \"tune-a-video.\" This omission may undermine the fairness of the experimental results.\n\n2. Base model choice: The authors employ ControlNet as the base model instead of using LDM directly. ControlNet offers strong structural control, which might make the improvement from LatentWarp seem relatively small. It would be beneficial to provide experimental configurations with LatentWarp combined with LDM or Tune-a-Video to showcase this point.\n\n3. Quantitative data details: The authors seem to have omitted reporting the sizes of the datasets used in their quantitative experiments.\n\n4. Compared methods: It is advisable for the authors to compare their method with a broader range of existing approaches, such as Video-P2P and more recent methods.\n\n5. Supplementary material: The authors have not provided corresponding video supplementary materials to visually assess temporal consistency.\n\n6. User surveys: The authors did not provide user surveys as prior works have done. This is important to evaluate the visual effect.\n\n7. Running costs: Editing time and GPU resource consumption, should be reported and compared to help readers understand the resource requirements and efficiency.\n\nThese concerns should be addressed to enhance the completeness and rigor of the experimental evaluation in the study."
            },
            "questions": {
                "value": "See weaknesses. I will rejudge the rating according to the rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission936/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission936/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission936/Reviewer_9TQZ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission936/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698581098189,
        "cdate": 1698581098189,
        "tmdate": 1699636020207,
        "mdate": 1699636020207,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "71UCxlZT5D",
        "forum": "ZJHdiYDD5k",
        "replyto": "ZJHdiYDD5k",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission936/Reviewer_cpKU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission936/Reviewer_cpKU"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the author studies the task of zero-shot video editing and addresses the problem of temporal consistency for the edited videos. The author points out that existing methods only consider the K/V tokens in the cross-attention mechanism and ignore the Q token which plays a more important role in preserving temporal consistency. Specifically, the author proposes to adopt optical-flow to warp the latent feature from the previous frame. The overall idea is interesting and the experimental results look good."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of considering the consistency of query tokens in cross-attention to generate consistent videos is interesting.\n2. The writing is clear and easy to follow and the experimental results are promising."
            },
            "weaknesses": {
                "value": "1. In section 5.1, the overall denoising step number is set to 20 and the proposed method is only applied to the first 16 steps. It would be good if there could be an ablation study about the two stages of the denoising steps.\n2. Is the proposed method sensitive to the hyper-parameters \\alpha and threshold as well as the optical-flow methods? I would like to see some ablation studies on that.\n3. Is it possible to edit/add some specific object to the video? Like adding a hat on a running dog? It seems most of the cases shown in the paper are about style changes. I would like to see some complex cases."
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission936/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698756916436,
        "cdate": 1698756916436,
        "tmdate": 1699636020144,
        "mdate": 1699636020144,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9hXsTjkgYy",
        "forum": "ZJHdiYDD5k",
        "replyto": "ZJHdiYDD5k",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission936/Reviewer_WQHp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission936/Reviewer_WQHp"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces optical flow for video-to-video translation by warping the latent codes in diffusion\u2019s sampling process and achieves the SOTA performance on V2V."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation is well presented,  the analysis of constrained query tokens would lead to consistent output, making the choice of warping the latent code convinced. \n\n2. The method is straightforward, and the results seem good."
            },
            "weaknesses": {
                "value": "1. Introducing the optical flow into diffusion-based video processing has been studied by Rerender-A-Video, though it is not applied to the latent. \n2. While this is video processing work, there are no video results, which makes it hard to distinguish the visual quality.\n3. Authors introduce the occlusion mask for indicating the warped region and unwrapped region, but how to guarantee the consistency on the unwrapped region?\n4. Some related works are not compared, such as Edit a Video, Rerender-A-Video, and etal."
            },
            "questions": {
                "value": "please present the video comparisons."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission936/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766419544,
        "cdate": 1698766419544,
        "tmdate": 1699636020057,
        "mdate": 1699636020057,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6w2kVKs5SI",
        "forum": "ZJHdiYDD5k",
        "replyto": "ZJHdiYDD5k",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission936/Reviewer_Yr5G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission936/Reviewer_Yr5G"
        ],
        "content": {
            "summary": {
                "value": "The paper employs a straightforward strategy to maintain temporal consistency in query tokens by introducing a warping operation in the latent space. This operation aligns the generated latent features of the previous frame with the current one during the denoising process, utilizing optical flow from the original video. As a consequence, adjacent frames share closely-related query tokens and attention outputs, fostering latent-level consistency and thereby enhancing the visual temporal coherence of the generated videos. Extensive experimental results underscore the effectiveness of LatentWarp in accomplishing video-to-video translation while preserving temporal coherence."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The propose method is well-motivated.\n2. The paper is well-structured, capable of clearly elucidating its core ideas.\n3. The conducted experiments adequately showcase the efficacy of the method being proposed."
            },
            "weaknesses": {
                "value": "1. It is hard for me to see the improvement of temporal consistence from the images. Therefore, it is strongly advised to incorporate the video in the supplementary materials.\n\n2. Section 4.2.  r^{i}|wrap(I^{i-1}, m^{i->i-1})-I^{i}| should be r^{i}|wrap(I^{i-1}, m^{i-1>i})-I^{i}| ?\n\n3. I understand  that warped query can enhance the temporal consistence but why does it improve visual details(Figure 5) as well?\n\n4. Since there are lots of open-sourced video diffusion models, which can naturally ensure the temporal consistence, what's the benefit of using Image diffusion model for video editing?   Longer video? \n\n5. For different video, the hyperparameters of Eq.(6) should be selected differently?"
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission936/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698767199155,
        "cdate": 1698767199155,
        "tmdate": 1699636019963,
        "mdate": 1699636019963,
        "license": "CC BY 4.0",
        "version": 2
    }
]