[
    {
        "id": "rbz3NvMqK8",
        "forum": "2MpOjashKU",
        "replyto": "2MpOjashKU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1012/Reviewer_5Uwa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1012/Reviewer_5Uwa"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a motion segmentation method to segment multiple objects, based on optical flow in an unsupervised manner. In this setting, both images and optical flow are available. Based on the SAN method, this work proposed an adversarial conditional encoder-decoder architecture. The proposed method can handle different numbers of objects at training and test time. The experimental results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) It can handle different numbers of objects at both training and test time.\n\n(2) It can run in real-time.\n\n(3) The performance is good."
            },
            "weaknesses": {
                "value": "(1) The ability of handling multi-object comes from SAN. The main contribution may be the adversarial framework and the manner using the image information. The authors should highlight the contributions of this paper.\n\n(2) Handling multi-object is not new in video object segmentation.\n\n(3) Why g_theta is an \"adversarial\" decoder is not clear. It forces the decoder to reconstruct the entire flow with each slot, which seems the two decoder do not have an adversarial relationship. \n\n(4) This method \"frees the representation from having to encode complex nuisance variability in the image\", which should be demonstrated in the experiments. For example, simply combining (concat) the image and the optical flow as input can be considered as a baseline. Although the authors mentioned combined input is complex and the slots are low-dimentional. More reasonable explanation is needed.\n\n(5) This setting is interesting,  but I guess its performance is heavily related to performance of the optical flow network. In the inference stage, the optical flow is also need to calculate first. \n\n(6) P4, DivA has two key advantages, but the following text is mainly about the disadvantages of MoSeg."
            },
            "questions": {
                "value": "see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1012/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698582110426,
        "cdate": 1698582110426,
        "tmdate": 1699636027208,
        "mdate": 1699636027208,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lmJ5yA1EMQ",
        "forum": "2MpOjashKU",
        "replyto": "2MpOjashKU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1012/Reviewer_RqAo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1012/Reviewer_RqAo"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to segment the visual field into independently moving regions. The proposed method uses a cross-modal conditional decoder that takes a second modality as input  to reconstruct the first modality.  This design frees the representation from having to encode complex nuisance variability in the image, such as illumination and reflectance properties of the scenes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is well-written and easy to follow.\n\nThe experimental results demonstrate better performance.\n\nThe idea is simple and effective."
            },
            "weaknesses": {
                "value": "I cannot find the obvious weakness."
            },
            "questions": {
                "value": "How about the GPU memory consumption?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1012/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809286046,
        "cdate": 1698809286046,
        "tmdate": 1699636027124,
        "mdate": 1699636027124,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EVc2zxH6Ir",
        "forum": "2MpOjashKU",
        "replyto": "2MpOjashKU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1012/Reviewer_8fzT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1012/Reviewer_8fzT"
        ],
        "content": {
            "summary": {
                "value": "This work introduces Divided Attention, an extension of the Slot Attention Network (SAN) for unsupervised multi-object discovery in real-time. Divided Attention takes both the RGB image and optical flow as inputs, and learns a set of \u201cslots\u201d as encodings that can reconstruct the optical flow. By constructing the optical flow (instead of image reconstruction in typical SAN), the model can focus on separating the objects in the scene and understanding their motion, rather than overfitting to the relatively less related visual details such as illumination and texture. Another key component is an adversarially trained flow decoder, which attempts to reconstruct the entire flow from each individual slot (the main decoder reconstructs the flow only within each mask). By employing this adversarial training, the slots are encouraged to learn \u201ccontextually separated\u201d encoding of the scene, and consequently result in separated, interpretable object representations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This work proposes to leverage optical flow for unsupervised multi-object discovery. In a video setting, it is more intuitive to extract information from the motion of pixels, and discover coherent regions as independently moving objects.\n\n- Divided Attention does not require any pre-trained visual features, and thus is more flexible to be applied in various applications. Moreover, its training and inference can use a dynamic number of slots, depending on the context.\n\n- The model is very efficient, enabling real-time inference speed."
            },
            "weaknesses": {
                "value": "- Flow input: In real-world practice, optical flow has to be obtained by running a flow estimation algorithm (e.g., RAFT). This would raise two concerns: 1) The flow estimation model is pre-trained with external knowledge and data in a supervised manner, which somehow contradicts with the claim that Divided Attention is unsupervised and requires no pre-trained features. 2) If we take the inference time of the flow estimation model into consideration, the FPS of the whole pipeline would be decreased, and achieving the real-time application would be more challenging.\n\n- Temporal consistency: In the base version of Divided Attention, temporal consistency across frames is not guaranteed. Additional tricks (e.g., inheriting slots from previous frames, or post-processing results of multiple frames) need to be incorporated for temporal consistency. This is not desirable considering the main application is object segmentation in videos.\n\n- Missing ablation study: It is suggested to quantitatively examine the design choices in Divided Attention via ablation study experiments. For example, $\\lambda$ in the adversarial training, the model architecture, and the number of slots during training and inference, should be tested with different choices for justification and better understanding of the proposed method, Divided Attention."
            },
            "questions": {
                "value": "- In Table 1, why are the FPS of some baselines (including DyStab) not listed?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1012/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699336465690,
        "cdate": 1699336465690,
        "tmdate": 1699636027032,
        "mdate": 1699636027032,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ndSdCjTta6",
        "forum": "2MpOjashKU",
        "replyto": "2MpOjashKU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1012/Reviewer_i9DX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1012/Reviewer_i9DX"
        ],
        "content": {
            "summary": {
                "value": "The paper presents Divided Attention (DivA), an unsupervised method for segmenting visual fields into independently moving regions without manual supervision. The model uses an adversarial conditional encoder-decoder architecture with interpretable latent variables, building on the Slot Attention architecture. It's designed to decode optical flow using the image as context without reconstructing the image itself, thus avoiding issues with complex image variability. DivA can handle varying numbers of objects and resolutions, is invariant to object label permutations, and doesn\u2019t require explicit regularization or pre-trained features. It achieves high run-time speed (up to 104FPS) and narrows the performance gap with supervised methods to 12% or less. The code will be made available upon publication."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "+ This paper is well-written and easy to follow.  \n+ The model is capable of inference speeds up to 104FPS, significantly faster than current unsupervised methods.  \n+ The method does not rely on pre-trained image features from external datasets, enabling its use in a broader range of scenarios."
            },
            "weaknesses": {
                "value": "Firstly, I suggest the authors pay attention to the terminologies: in the title and introduction, the authors claim to do multi-object discovery/segmentation. However, they are actually doing moving object segmentation. \"regions of an image whose corresponding motion is unpredictable from their surroundings\" This should be the definition of moving objects but not objects. In other words, for some datasets with both moving and non-moving objects, such as MOVI-D, the proposed method will fail to work. \n\nSecondly, I think the novelty/contribution of this work is limited. Reconstructing in the flow space with slot attention has been widely explored before, e.g. SAVI, the conditional decoder and the adversarial loss is also somewhat not quite novel. \n\nMoreover, there is no explanation why the author wants to take flow as the input and reconstruction space but RGB as the condition rather than take RGB as the input but flow as the condition. More ablations regarding this should be conducted. Also, quantitative results for the ablation with adversarial decoder should also be reported as that's one of the claimed contributions.\n\nFinally, more visualizations should be included, at least in the supplementary.\n\n---------------------\nThanks again for the quick reply.\n\nFor R1, I think I understand what the authors claimed -- the definition of the ``object'' is proper and in principle, the non-moving object can be captured in real practice, with the proposed method. However, I still doubt if that's the case -- it's hard to predict the results unless seeing the results.\n\nFor R2, the discussions for those references sound promising. Though one minor thing is, for [4], the authors have ablations to verify that their method can still work without pre-trained features -- they can first train the ViT from the target dataset and then do object discovery in that space. Not to mention both [3] and [4] require no additional data (flow) but just the RGB images.\n\nI respect the effort of the authors during the rebuttal stage and would like to slightly upgrade my rating, but will not champion this paper."
            },
            "questions": {
                "value": "See previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1012/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1012/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1012/Reviewer_i9DX"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1012/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699396988138,
        "cdate": 1699396988138,
        "tmdate": 1700777583813,
        "mdate": 1700777583813,
        "license": "CC BY 4.0",
        "version": 2
    }
]