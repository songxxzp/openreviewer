[
    {
        "id": "g2xJgN9VCQ",
        "forum": "o4AydSd3Lp",
        "replyto": "o4AydSd3Lp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8945/Reviewer_KUuk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8945/Reviewer_KUuk"
        ],
        "content": {
            "summary": {
                "value": "This paper conducts an empirical analysis to gain insight into the success of discrete world models in reinforcement learning. It conducts a series of experiments to disentangle the effects of sparse vs dense and continuous vs discrete representations on a set of MiniGrid tasks. It concludes that sparsity, rather than simple discretization, plays a significant role in the improved learning dynamics enjoyed by the VQ-VAE representation in both single-task and continual reinforcement learning tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This is a well-written paper which clearly states its problem of interest, methodology, and conclusions.\n- The paper studies an interesting and important topic that has previously been under-explored\n- The experimental setting is easy to follow and interpret. The architectural assumption of using a fixed set of encoders and switching between task-specific \"heads\" is a nice way to control for different architectural variants, giving an apples-to-apples comparison between different design choices.\n- The snap-to-grid and continuous tile coding strategies provide an insightful continuum between discrete and continuous, and between sparse and dense representation types.\n- Figure 5 provides a particularly insightful investigation into the effect of sparsity."
            },
            "weaknesses": {
                "value": "- The paper only considers gridworld tasks, and it is hard to say whether the findings will generalize to more visually rich and complex planning tasks. A paper which only tells the reader something about representation learning in gridworlds presents a much less useful and interesting contribution than one whose insights are expected to generalize to a broader class of environments that are of interest to practitioners. This is the main issue which is lowering my score.\n - The RL task is extremely simple for the fixed representation: while the paper does point out that the discrete representations converge 2-4x faster than the representations obtained from the continuous reward model, both representations seem to converge almost immediately\n  - The error bars on many of the figures are quite large, and there were some strange artifacts, in particular in Figure 3 where going from 512 to 1024 dimensions dramatically increased the error of one of the models, which are not explained. Given the relatively narrow scope and simplicity of the domains under consideration, this increases my concern about how the findings in this paper will generalize to other environments. I understand that evaluating on many tasks requires significant computational resources, but I worry that the results in this paper might be telling us more about the MiniGrid environment than about the general properties of the representations studied. \n  - Relatedly, the gap between FTA and VQ-VAE is much larger in one environment than the other, suggesting that there may be domain-specific properties that influence the relative performance of different representations. It would be useful to have a broader range of evaluation environments, even if these are limited to the minigrid setting to see whether this is the case in general."
            },
            "questions": {
                "value": "- In the description of the continual RL setting, it wasn't clear to me whether the representations were trained on a variety of shuffled layouts or only a single one. It would be very surprising to me if training a transition model on a single layout led to features that generalized so effectively (with apparently no degradation in performance on OOD inputs), but if this was the case it would merit further discussion. \n- One important factor influencing the usefulness of model-based representation learning is whether the data collected for the pretraining phase covers the state space. Since the transitions are mostly collected from random policies, this seems like it could be a significant issue in a domain like atari or mujoco, where a random policy samples a very limited state distribution. How dependent are the results in this paper on the pretraining distribution used for the learned representations?\n- Minor: the paper briefly mentions that it will focus on sample-based models, but the reasoning behind this choice isn't fully explained. Is there a reason why expectation models would not be suitable for representation learning? Or is this motivated by the desire to replicate the setting used in prior empirical work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8945/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698625966893,
        "cdate": 1698625966893,
        "tmdate": 1699637126345,
        "mdate": 1699637126345,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iujsZYYY4j",
        "forum": "o4AydSd3Lp",
        "replyto": "o4AydSd3Lp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8945/Reviewer_CBeE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8945/Reviewer_CBeE"
        ],
        "content": {
            "summary": {
                "value": "The authors observe that one of the must fundamental factors in RL is how agents represent observations, and in particular, the form of these representations. While discrete representations have found success recently, there has not been a systematic study of dense vs. sparse vs. discrete representations - which is the focus of this work. The authors implement agents with these 3 kinds of representations in model-based, model-free, and continual RL settings, and perform several experiments to disentangle the effects of these representations, focusing on the better-performing discrete representations, and hence present a few novel findings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I believe this work is very well motivated - to the best of my knowledge there has not been a third-party examination of the role of these different types of representations within RL, and it is important for the community to understand such a fundamental issue better. And beyond simply trying dense, continuous vectors, and discrete vectors, the authors also try sparse, continuous vectors, which are an understudied but potentially valuable form of representations. They present a range of interesting studies, and do address an important tradeoff - discrete representations can have their advantages, particularly in modelling rarer parts of the distribution/not focusing capacity on just the most common events, but can also require more learning."
            },
            "weaknesses": {
                "value": "I believe a crucial oversight in this work is that, while the authors focus on the effects of types of representations, they failed to take into account the representational capacity of the representations, or rather, the regularisation. There is a considerable amount of literature on \"disentanglement\" in NNs - particularly VAEs - showing that regularisation is key to models learning to separate different latent factors in data generation process. So although the hidden size of the bottleneck in AEs does reduce representational capacity, I believe that without further regularisation on the continuous latent space - for example, using a \u03b2-VAE - this is not taken into account properly.\n\nThere is also another major caveat with these findings, which is that all experiments were conducted on the Minigrid environment, which is a gridworld, and hence one might expect discrete representations to better match the fundamentally discrete objects in the environment. Although this doesn't invalidate the findings of this work, I believe the authors need to address this shortcoming. Ideally, the authors would show that their findings hold in more complex environments - DeepMind Lab is typically used in this regard, as it also has discrete objects and a grid-like environment, but it is 3-dimensional and observations are provided in first-person RGB views."
            },
            "questions": {
                "value": "I do not have any particular questions for the authors. I appreciate the thorough and clear implementation details in the appendix."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8945/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698643288899,
        "cdate": 1698643288899,
        "tmdate": 1699637126232,
        "mdate": 1699637126232,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "63o5se1kAr",
        "forum": "o4AydSd3Lp",
        "replyto": "o4AydSd3Lp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8945/Reviewer_Ys23"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8945/Reviewer_Ys23"
        ],
        "content": {
            "summary": {
                "value": "This paper takes a deeper look at use of discrete representations rather than continuous ones, performing empirical studies to develop a potentially deeper understanding of when these representations are useful. The authors explore how helpful these representations are both when learning environment models and when learning model-free policies. The authors find that discrete representations are more helpful when capacity is limited and propose it as a potential solution for continual RL scenarios for this reason. As predicted, their continual learning experiments also demonstrate performance gains when using discrete representations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- I enjoyed reading this paper and found it mostly very clear and easy to follow. \n- I think that the paper asks an important question. I also often wonder about the benefits achieved when using discrete representations. \n- The conclusion of the experiments feels very believable to me, even if the empirical results aren't comprehensive enough to fully support the generality of the claims. \n- I find the argument spelled out for use of discrete representations in continual RL to be an important contribution to the discourse around architectures in this research area, making the paper potentially interesting to a pretty broad audience."
            },
            "weaknesses": {
                "value": "- The experiments do look at scale, but the architectures / domains considered are very basic, so it is unclear how general these results are and how applicable they are to real-world scenarios. \n- Also the breadth of experiments is not very large in terms of domains when we consider that providing a comprehensive empirical study is a stated goal of the paper. \n- The authors could potentially make the paper better by providing a bit more in terms of direct empirical proof of their interpretations i.e. the role of sparsity,"
            },
            "questions": {
                "value": "1. The authors mention a few times that there is a connection between the success of discrete representations in continual RL and representation sparsity. Did you try actually measuring this sparsity? I guess the size of each one-hot dictates this sparsity level, so it could be interesting to understand the connection between this and continual learning performance.  \n\n2. In section 3.3.3 I am a bit unclear about what the conclusion is for these experiments. What are some of the insights you believe they present? Is this actionable in a general sense?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8945/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698986935478,
        "cdate": 1698986935478,
        "tmdate": 1699637126110,
        "mdate": 1699637126110,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Iw1Xv4ipNs",
        "forum": "o4AydSd3Lp",
        "replyto": "o4AydSd3Lp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8945/Reviewer_Cvsu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8945/Reviewer_Cvsu"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the use of discrete representations in reinforcement learning, specifically in three different setup: model-based, model-free and continual RL. The discrete representations are learned using vector quantized variational autoencoder (VQ-VAE), whereas the continuous representations are learned using the canonical autoencoder and an autoencoder which uses Fuzzy Tiling Activation (FTA).   Using various environments based on minigrid, their work showed that models learned with discrete representations approximate the ground truth distribution when compared to models learned with continuous representations using KL-divergence. Additionally, unlike their counterparts, agents using models learned with discrete representations exhibit better learning efficiency overall."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-written and mostly easy to understand. The research question is a relevant one for the reinforcement learning community. There have been several uses of discrete representations in previous work as presented by the authors but yet to be studied extensively. To the best of my knowledge, the main relevant work is also cited in the paper. The setup and the aim of the experiments are well-defined and comprehensive. Using simpler experiments such as minigrid enables to author to perform in-depth analysis and gave clear insights of the hypothesis. A broad range of scenarios ranging from model-based to model-free and continual RL were considered and the consistency shown through these experiments added assurance to their findings. The figures in the appendix, for example figure 9,10, 11 and 12 are well presented and helped me to understand their work better, which I greatly appreciate."
            },
            "weaknesses": {
                "value": "My only slightly major concern on this work is that all experiments are done on the minigrid environments. The pixel observations for this environment are simple which might have contributed to the easier process of discretization. I think to improve the quality of the paper, it will be important to see if the same conclusions can be made when using more visually complex environments, such as Atari 100k, which is also performed in the dreamer paper.  \n\nMinor concerns:\n1. Will be great if the authors can add a pseudocode of the algorithm in the appendix to improve the clarity\n2. Figure 14: Although the trend of the result is clear, this should be plotted with mean and standard deviation to improve readability. \n3. Some spacing gaps in the appendix can be reduced, for example using vspace in latex."
            },
            "questions": {
                "value": "1. In section 3.3.2, the authors mentioned that \u201cIn the plot, an interesting pattern emerges: the performance of all methods become indistinguishable beyond a certain size of the world model\u201d. Perhaps this might hint that the environment is too simple? \n2. In section 3.3.3, the authors mentioned that \u201cIt is alternatively possible that the discrete world model performs better simply because the VQ-VAE learns different information that is more conducive to world modeling.\u201d What other different information do the authors think VQ-VAE is learning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8945/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699105870406,
        "cdate": 1699105870406,
        "tmdate": 1699637125998,
        "mdate": 1699637125998,
        "license": "CC BY 4.0",
        "version": 2
    }
]