[
    {
        "id": "Pnbwuqot2P",
        "forum": "OPpqmSp0wK",
        "replyto": "OPpqmSp0wK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3251/Reviewer_KbPe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3251/Reviewer_KbPe"
        ],
        "content": {
            "summary": {
                "value": "This work focuses on addressing the limitations of instance discrimination commonly used in image-text contrastive learning, such as CLIP. The authors propose a multi-label cluster discrimination method aimed at improving the encoding ability. They employ offline clustering to assign multiple labels to each image and subsequently conduct multi-label classification to learn the semantic structure within a single image. The authors support their methods with extensive experiments and perform ablation studies to analyze the function of each component."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This work considers the multi-label properties of a single image and emphasizes the learning of better semantic structure in data.\n2. The designed loss function elegantly separates the loss from positive and negative classes, which enhances the parallelism and scalability during training.\n3. The experiments in this work are extensive and convincing, with thorough ablation studies."
            },
            "weaknesses": {
                "value": "1. Clarity: \n   - This manuscript requires further refinement in terms of writing to facilitate reader comprehension, particularly by providing detailed explanations for the mathematical symbols used in the text, thus reducing reading barriers.\n2. Experiments: \n   - In section $3.2, the authors claim efficient parallel computation and scalability of the model training process. However, is there quantitative data to support this point?\n   - Does the incorporation of clustering significantly improve the training time? \n3. Reproducibility:\n   - In section $3.2, you employ some distribution training techniques but details are not provided, which hinders the reproducibility of the work."
            },
            "questions": {
                "value": "1. The performance reported in the CLIP paper differs from your reproduced version. In Tab 1 and Tab 2, which may influence the validation of improvement of your model. Have you checked the implementation and settings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3251/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3251/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3251/Reviewer_KbPe"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3251/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698132545075,
        "cdate": 1698132545075,
        "tmdate": 1700678199440,
        "mdate": 1700678199440,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "r8lsQcTiaM",
        "forum": "OPpqmSp0wK",
        "replyto": "OPpqmSp0wK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3251/Reviewer_g3TE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3251/Reviewer_g3TE"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new clustering-based unsupervised algorithm for vision foundation models pre-training. The key idea is to assign images into multiple clusters as pseudo labels for unsupervised representation learning. The motivation is that existing clustering-based pre-training methods assign each image into a single cluster, which enforces the models to focus on the most salient part of images and overlook the other regions that may also be meaningful. Besides, the authors also optimise the conventional margin loss formulation by decoupling the optimisations of positive and negative pairwise similarity. The proposed algorithm has been shown effective in severe classification-oriented downstream tasks including linear probe, zero-shot classification and retrieval."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The algorithm proposed in this paper is intuitive and effective in learning discriminative imagery feature representations. The analysis and decomposition of triplet loss make sense to me and are potentially beneficial to a wide range of applications as a generic improvement to a widely adopted metric learning design."
            },
            "weaknesses": {
                "value": "+ my key concern on the high-level idea is whether the top-k closest clusters to an image can really reveal what objects/attributes (will use \u201cconcepts\u201d for clarity hereafter) are involved in it. At the cluster level, samples of the same clusters are likely to share more nearest clusters (in a global picture) but the concepts involved in each independent image are almost random. Is it possible that the multiple labels assigned to the same images provide models with additional knowledge about the co-occurrence/relevance of different concepts (cluster-to-cluster relationships) rather than actually telling models what is involved in images (sample-to-cluster relationships)? It will be interesting to see more exploration and analysis of why the multi-label clustering idea is beneficial. One simple verification can be pre-training on a dataset with known non-overlapping class structure, eg ImageNet, and see if the multi-label clustering still benefits. \n\n+ The modifications made to triplet loss make sense to me but their effects are unclear. How will the proposed model perform if all its designs are kept unchanged except for replacing L\u2019_MLCD (Eq.6) with L_MLC\n\n\n+ What are the blue and green cells standing for in the grids pointing to the text \u201ccontrastive loss\u201d?\n\n+ Whilst Fig.2 is the first figure being referred to, Fig.1 is simply mentioned as the illustration of visual representation learning but it lacks further explanation/discussion.\n\n+ In Eq.1, I assume the pairwise similarity is cosine similarity if following CLIP, but without normalisation of features, it is just an inner product. So I\u2019m wondering if it is a mistake or my misunderstanding.\n\n+ In Eq.1, the index in the cumulative sum starts from 0 to k while that in Eq.2 is from 1 to k, is this deliberate and why?\n\n+ The exponential function is denoted as exp and e at the same time in Eq.3, which makes the equation really confusing when the feature representations are also denoted as e_i.\n\n+ The ablation studies are a bit unclear to me. For example, when investigating the effects of sample ratio, the best linear probe performance is obtained when the sample ratio is set to 0.1, and the best result is 75.2. However, the linear probe performance of the proposed model shown in Table 5 is 84.6."
            },
            "questions": {
                "value": "Although the proposed method yielded impressive performance, it is also crucial for me to figure out the underlying reasons for the effectiveness. So further evidence and discussions about this will be helpful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3251/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3251/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3251/Reviewer_g3TE"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3251/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698640326345,
        "cdate": 1698640326345,
        "tmdate": 1699636273444,
        "mdate": 1699636273444,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "axnazSOF5k",
        "forum": "OPpqmSp0wK",
        "replyto": "OPpqmSp0wK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3251/Reviewer_S2Uc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3251/Reviewer_S2Uc"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a simple but effective method to facilitate the representation learning of the vision-language model. The method consists of two steps. In the clustering step, the authors cluster the dataset into enormous centers and utilize several closest centers as the class labels for every single image, enhancing the learning of semantic structure of training data. The discrimination step incorporates a multi-label classification loss to separate losses and promote distributed training. The experimental results are solid."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality: This work extends the discrimination power of CLIP model by introducing a multi-label loss to boost the semantic learning ability of the vision-language model.\nQuality: The improvement achieved by the proposed method is remarkable on certain datasets, and the ablative study provides comprehensive and detailed insights into its functioning.\nClarity: This paper is reader-friendly and smooth. The experimental setting is quite reasonable.\nSignificance: This paper shows the benefit of using multi-label loss for clustering-based discriminative constrastive learning. This setting should be considered when developing powerful pre-trained vision-language model for downstream tasks."
            },
            "weaknesses": {
                "value": "(1) The novelty and originality of this work are limited. It seems like the method proposed in this paper incorporates several techniques introduced in the literature. It does not offer sufficient technical inspirations for the readers to follow. \n(2) With respect to the limited technical novelty of this work and overall moderate improvement (I see in Table 1 and Table 2), it may not seem to be worthwhile using such huge computing resources (80 NVIDIA A100 GPUs), especially considering that visual-language pre-training field has already achieved remarkable performance.\n(3) According to my understanding, as proposed method is developed upon the feature embedding from the pre-trained CLIP model and it does not involve any textual information in the proposed multiple label loss. If this is correct, this paper should make this more clear.\n(4) In Equation 5, two new items are further introduced into the multi-label loss. This is regarded as one of the key contributions by this paper. However, its efficacy does not seem to be clearly verified in the ablation study. This needs to be addressed."
            },
            "questions": {
                "value": "(1) As one of the main contributions of this paper, the authors claim that the modification of optimization loss can elegantly separate the positive class labels and negative class labels, resulting in promotion of the distributed training on large-scale training data. Please explain and experimentally demonstrate how this modification can facilitate the distributed learning more clearly. For example, in Subsection Distributed Multi-label Classification of Section 3.2 MULTI-LABEL CLUSTER DISCRIMINATION, The first sentence \u201cEq. 6 is able to distribute the weights associated with one million class centers across all GPUs with minimal communication overhead.\u201d Why? \n(2) Some technical details are missing, e.g., in Section 4.1, the authors should explicitly point out the number of classes (k) and number of positive centers (l) they use when pre-training the model on LAION-400M dataset.\n(3) In Table 3, the best results consist of both the proposed method in this paper and the FLIP (i.e., 89.1%). Notably, only the results of the proposed methodology have been highlighted.\n(4) In Section 4.6, the meaning of the y-axis of the charts should be provided to improve the clarity of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns. The authors have discussed the limitation at the end of the paper."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3251/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3251/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3251/Reviewer_S2Uc"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3251/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699511337881,
        "cdate": 1699511337881,
        "tmdate": 1699636273358,
        "mdate": 1699636273358,
        "license": "CC BY 4.0",
        "version": 2
    }
]