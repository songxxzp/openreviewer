[
    {
        "id": "K4A3zwz9f4",
        "forum": "Va4t6R8cGG",
        "replyto": "Va4t6R8cGG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2440/Reviewer_PJAV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2440/Reviewer_PJAV"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a fully end-to-end, DETR-based action localization model.  Their method is a one-stage, proposal-free method. The authors factorize the queries into spatial queries and temporal queries, which allows a consistent parameterization across different datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is generally well-written.\n\nThe experiments look solid and many details have been provided in the paper and in the supplementary. The authors have also promised to release the code."
            },
            "weaknesses": {
                "value": "Firstly, I cannot help but feel like the proposed designs and improvements are rather small over previous works. For instance, most of the novelty comes from the spatial and temporal factorized queries, which, while practical and beneficial for experiments, is not very interesting.\n\n\n\nFor the experiments, it feels like the largest improvements came from ViViT/L backbone, on AVA and AVA-K. Similarly, the pretraining using CLIP also seems to contribute most of the improvements in UCF-101-24, and much of the improvement in AVA and AVA-K. Comparatively, the improvements (especially on AVA, AVA-K) are not significant when the pretraining settings are the same as previous works (TubeR). \n\nBut, I understand that the authors have issues reproducing their TubeR\u2019s code, i.e., the actual improvement is currently hard to quantify. Due to the similarities between the two pipelines, I suggest the authors to run some experiments according to TubeR\u2019s method (i.e., their action-based parameterization and no query factorization) using the exact pre-training settings in this paper, and report them, which I believe will add to the experimental contributions of this paper.\n\n\n\n\n\nOverall, the idea is not very interesting to me, but the experiments look solid, and seems to provide a good baseline for future works."
            },
            "questions": {
                "value": "Some other questions are below.\n\n1)\tHow exactly are the queries binded to each person in the video? I don\u2019t see any such explicit constraints.\n2)\tHow fast is the proposed method compared to other methods? I understand that there are some GFLOPs comparisons in the supplementary, but it is difficult to compare the methods due to the presence of other parts (such as LTC or person detector). Could we see a speed comparison instead?\n3)\tI am rather curious regarding the possible integration with other foundational models. Since the proposed method requires special designs (such as spatial and temporal query factorization), is it more difficult or easier to integrate with other video-based foundational models (as reported in Table 7), which tend to be able to perform many other tasks as well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2440/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698250785520,
        "cdate": 1698250785520,
        "tmdate": 1699636179943,
        "mdate": 1699636179943,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gnKjk6lISz",
        "forum": "Va4t6R8cGG",
        "replyto": "Va4t6R8cGG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2440/Reviewer_J8Sj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2440/Reviewer_J8Sj"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a fully end-to\u0002end, transformer based model that directly ingests an input video, and outputs tubelets, which requires no additional pre-processing and post-processing. The extensive ablation experiments on four spatio-temporal action localisation benchmarks verify the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ This paper is well-written and well-organized.\n+ Good performance on the popular benchmarks."
            },
            "weaknesses": {
                "value": "- This paper does not seem to be the first work of fully end-to-end spatio-temporal localization, while TubeR has proposed to directly detect an action tubelet in a video by simultaneously performing action localization and recognition before. This weakens the novelty of this paper. The authors claim the differences with TubeR but the most significant difference is that the proposed method is much less complex.\n- The symbols in this paper are inconsistent, e.g., b.\n- The authors need to perform ablation experiments to compare the proposed method with other methods (e.g., TubeR) in terms of the number of learnable parameters and GFLOPs."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2440/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2440/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2440/Reviewer_J8Sj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2440/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698555426260,
        "cdate": 1698555426260,
        "tmdate": 1699636179870,
        "mdate": 1699636179870,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LLNVlcVKNz",
        "forum": "Va4t6R8cGG",
        "replyto": "Va4t6R8cGG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2440/Reviewer_zmts"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2440/Reviewer_zmts"
        ],
        "content": {
            "summary": {
                "value": "The presented works address the problem of action tubelet prediction without the requirement of memory banks from similar work by Zhao et.al. The main contribution of the work is that it is able to perform well while removing the need for a memory bank when the same backbone is used. One can train this tubelet prediction when sparse annotations are available."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of the factorised query is a good one, it makes spatial temporal query search space quite tractable. Not sure if that is from the author is it borrowed from Zhao et al or others. \n- Being able to predict tubelet even when sparse annotations are available is a plus. \n- Removing the need of a memory bank is a good step forward toward generalisation"
            },
            "weaknesses": {
                "value": "I think the paper is written well but the numbers are a bit overhyped. The proposed work is a good extension of Zhao et al 2023b (SOTA), however, the numbers in the table show that most of the dramatic improvement over SOTA is because of the use of a better transformer backbone and better pertaining. At the same time, it improves over SOTA slightly 31.1 to 31.4 without using memory banks but the decoder used in STAR is bigger.  \n\nMinor negative\nThe authors mentioned that they do not require any post-processing in the abstract but they do for the causal linking algorithm, which should be cited to Singh et al. (2017) because Kalogeiton et al., 2017 borrow from the aforementioned. \n\nMore or less I am happy with the paper, please try to answer the question below so I can participate in the discussion."
            },
            "questions": {
                "value": "Table one shows significant improvement over TubeR with the use of person-bound tubelets compared to action-bound. Then why the gap is so small in Table 4 between TubeR and STAR with CSN backbone?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2440/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698871150927,
        "cdate": 1698871150927,
        "tmdate": 1699636179804,
        "mdate": 1699636179804,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KpK0wCtMHh",
        "forum": "Va4t6R8cGG",
        "replyto": "Va4t6R8cGG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2440/Reviewer_dyfN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2440/Reviewer_dyfN"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an architecture for Spatio-Temporal Action Detection in videos. The proposed architecture, namely STAR, can be trained end-to-end without the need of additional human detectors or external memory banks. The technical design is simple yet effective. Experiments are done on 4 different datasets: AVA, AVA-Kinetics, UCF-24, JHMDB. Ablation studies are thorough and enough to understand the design choice. STAR outperforms or on par with state-of-the-art methods on the four evaluating benchmarks. Written presentation is clear and easy to read and follow."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed architecture is very simple and still being effective, be on par or outperform state-of-the-art approaches. On small datasets such as UCF and JHMBD, START strongly outperforms previous methods. On larger datasets such as AVA, AVA-Kinetics, STAR also gives competitive performance.\n- Solid ablation experiments: The paper provides a thorough set of ablation experiments to validate most of components / design choices.\n- Written presentation is with high clarity which helps the readers easy to read and follow."
            },
            "weaknesses": {
                "value": "- On AVA and AVA-Kinetics, it seems the key recipe for STAR is using CLIP, without CLIP STAR achieves 30-31 on AVA and 35-36 on AVA-Kinetics which are much lower compared state-of-the-art (e.g., VideoMAE v2: 42.6 on AVA and 43.9 on AVA-Kinetics). Even with model with less-pre-training, i.e., Co-fine-tuning gets 36.1 and 36.2, respectively on AVA and AVA-K. Can we have a direct comparison with other method such as TubeR where TubeR is pre-trained with CLIP & K700? Also can the author(s) provide further discussion / insights about the role of CLIP, what make it useful for STAR that much?"
            },
            "questions": {
                "value": "- In table 7, the paper flagged InternVideo and VideoMAE v2 as \"web-scale pre-trained\", what is the size / definition of web-scale? Why CLIP, JFT, or IG65M is not considered \"web-scale\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2440/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698949441343,
        "cdate": 1698949441343,
        "tmdate": 1699636179743,
        "mdate": 1699636179743,
        "license": "CC BY 4.0",
        "version": 2
    }
]