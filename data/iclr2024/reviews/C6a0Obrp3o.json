[
    {
        "id": "oGlycO9QP0",
        "forum": "C6a0Obrp3o",
        "replyto": "C6a0Obrp3o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission711/Reviewer_n7gt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission711/Reviewer_n7gt"
        ],
        "content": {
            "summary": {
                "value": "This research addresses challenges in image-to-text (I2T) inversion and proposes \"SingleInsert,\" a two-stage method that effectively separates foreground and background in learned embeddings. It enhances visual fidelity and flexibility in single-image concept generation, novel view synthesis, and multiple concept composition without joint training. The paper introduces the Editing Success Rate (ESR) metric for quantitative assessment of editing flexibility."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is very easy to follow."
            },
            "weaknesses": {
                "value": "1. This paper seems to miss many related works or baselines.\n\n- Taming encoder for zero fine-tuning image customization with text-to-image diffusion models.\n\n- InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning\n\n- Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach\n\n2. Does hyper-dreambooth finetune t2i part?\n\n3. The idea is not novel. Using mask to get more accurate object embedding is not new, and BG loss is largely used for this purpose also.\n\n4. The two-stage training/finetuing plus the additional losses as restriction are more complicated than previous works, but lacking comparison with above related works.\n\n5. According to the implementation details, the model requires retraining for each new concept, and in the finetuning stage, it relies on lora for better fidelity, which makes the soundness of the method even weaker.\n\n6. The proposed ESR is worth more descriptions in the main part, since it is  a contribution."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission711/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698643797220,
        "cdate": 1698643797220,
        "tmdate": 1699635998362,
        "mdate": 1699635998362,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GsvIm17O4A",
        "forum": "C6a0Obrp3o",
        "replyto": "C6a0Obrp3o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission711/Reviewer_i9TQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission711/Reviewer_i9TQ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new method for customized text-to-image generation, which considers disentanglement in learning the concept contained in user-provided image."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method tries to disentangle the influence of foreground and background in the given image, which is reasonable and straightforward.\n\nGood results are presented in the paper, compared to related baselines.\n\nAblation studies are conducted, which help readers better understand the proposed method."
            },
            "weaknesses": {
                "value": "The proposed method seems to require more fine-tuning time compared to some related works (E4T only requires 5~15 steps, the proposed method requires 100 steps which is mentioned in section 4.1).\n\nThe idea of disentangling the foreground and background information has also been exploit in related works [1, 2]. Some of the related work have code publicly available online [2], but are not compared in this paper's experiments.\n\nIn quantitative evaluation, the authors didn't follow the setting in Dreambooth [3] to test the proposed methods on objects comprehensively. Specifically, the prompts used in the paper, on both human face and objects domain, may not be comprehensive enough. Dreambench proposed in [3] contains recontextualization, accessorization, and property modification prompts. On the contrary, example prompts shown in the paper are less comprehensive. Thus more comparisons are suggested.\n\nSome related works also work on similar task with related ideas, which are suggested to be discussed in the paper.\n\n[1]. DisenBooth: Identity-Preserving Disentangled Tuning for Subject-Driven Text-to-Image Generation. Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan, Yuwei Zhou, Wenwu Zhu.\n\n\n[2]. Subject-Diffusion:Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning. Jian Ma, Junhao Liang, Chen Chen, Haonan Lu.\n\n[3]. DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman.\n\n[4]. BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing. Dongxu Li, Junnan Li, Steven C.H. Hoi.\n\n[5]. PhotoVerse: Tuning-Free Image Customization with Text-to-Image Diffusion Models.  Li Chen, Mengyi Zhao, Yiheng Liu, Mingxu Ding, Yangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing Liu, Kang Du, Min Zheng.\n\n[6]. Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach. Yufan Zhou, Ruiyi Zhang, Tong Sun, Jinhui Xu.\n\n[7]. InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning. Jing Shi, Wei Xiong, Zhe Lin, Hyun Joon Jung."
            },
            "questions": {
                "value": "Can the authors provide more details about the data they collected from the web? Specifically, do those data consist of common object, human face, or both? What is the number of the collected samples?\n\nIn the fine-tuning stage, because a frozen T2I model is also used, how much extra memory do we need compared to the scenario without this model (both under LoRA setting).\n\nHave the authors considered pre-training the model on a large-scale dataset? Will it reduce the fine-tuning time on testing images?\n\nThe author mentioned number of iterations needed, what is the actual total time needed in terms of seconds/minutes for customizing a new testing image?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission711/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822562846,
        "cdate": 1698822562846,
        "tmdate": 1699635998296,
        "mdate": 1699635998296,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k40svOZeiF",
        "forum": "C6a0Obrp3o",
        "replyto": "C6a0Obrp3o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission711/Reviewer_QGk3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission711/Reviewer_QGk3"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a two-stage Diffusion-based Image-to-Text Inversion algorithm that can mitigate overfitting when training with a single source image. It applies constraints to suppress the inversion of undesired background and the problem of language drift. Segmentation masks for foreground and background and predictions from the original diffusion model conditioned on the class of inversed concept are utilized to form the regularizations. It also designs an editing prompt list to quantitatively evaluate the edit flexibility of the inversed concept. With the proposed algorithm, in the non-trivial single-source-image scenario, this work achieves both high visual fidelity and editing flexibility, enabling novel view synthesis and multiple inversed concepts composition without joint training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The method allows more flexible ediitng for the inversed concepts from a single image, surpassing its baselines.\n(2) The method presents a novel way to regularize the Image-to-Text Inversion process with predicted distributions by the original model.\n(3) The paper presents an ediitng prompt list and a metric for quantitative evaluation of editing flexibility of inversed concepts.\n(4) The paper clearly illustrates the motivations and the designs of the new proposed loss functions.\n(5) The ablation studies clearly presents the value of each design of the proposed method."
            },
            "weaknesses": {
                "value": "(1) In section 4.4, the authors claim that the proposed approach enables single-image novel view synthesis. However, the experiments on this point are quite weak. Firstly, the algorithm cannot accurately control the viewpoint angle but can only control the view with text prompts \"left side\", \"frontal\", and \"back side\". Secondly, no evidence is provided to demonstrate how this constitutes an advancement compared to previous work on previous approaches. Thirdly, the generated novel view images also have drastic change on the background and even foreground appearance, which does not meet the requirement of novel view synthesis. Thus, I doubt that the claim of this contribution is not grounded.\n(2) The application scenario of multiple concept composition is only demonstrated with a few examples but without comparison to previous work.\n(3) On P6, section 4.1, a brief, if not detailed, introduction about the proposed metric ESR and the editing prompt list is expected to be given. The readers are supposed to have the basic idea about what is done in this evaluation after reading this section, instead of having to read the supplemental file to grasp it."
            },
            "questions": {
                "value": "(1) The proposed algorithm in this paper does not have a design specified for the single-source-image scenario and achieves single-source-image scenario by finetuning a large number of parameters, i.e. the whole T2I model and a ViT-B image encoder. So it would be natural to expect that the good performances generalize to the multiple-source-image scenario. Have you tried using the proposed algorithm for the multiple source-image inversion?\n(2) Please refer to the questions in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission711/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699193344378,
        "cdate": 1699193344378,
        "tmdate": 1699635998197,
        "mdate": 1699635998197,
        "license": "CC BY 4.0",
        "version": 2
    }
]