[
    {
        "id": "bUU81YzGSY",
        "forum": "Fq8tKtjACC",
        "replyto": "Fq8tKtjACC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2112/Reviewer_JG5J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2112/Reviewer_JG5J"
        ],
        "content": {
            "summary": {
                "value": "This work explores the question of data quality, in terms of data with less noise and data well aligned with the tasks of interest, for training narrow Python code generation models . The authors introduce a set of small datasets totaling ~7B tokens consisting of (i) 6B token public code data filtered for quality using GPT4 and an embedding based classifier, (ii) <1B tokens of textbook-like data generated by GPT3.5, and (iii) 180M tokens of code completion exercises generated by GPT3.5. Pretraining and finetuning on these datasets yields highly performant models with limited size (1.3B parameters) evaluated on HumanEval and MBPP, with majority of the gain coming from finetuning on (iii).The authors also note some generalization capabilities beyond the limited datasets learned by their models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors pursue a significant goal with this work, showing how large generalist models can be used to train high quality and efficient models with limited compute spend. The results are impressive on HumanEval - the 1.3B phi model (this work) outperforms much larger and compute intensive models."
            },
            "weaknesses": {
                "value": "The authors make a concerted effort to prove the \"synthetic exercise\" dataset curated in their work does not contain contaminated data from the test sets - however, I could not find a similar investigation for the \"synthetic textbook\" dataset.\n\nAnother significant weakness is that finetuning results for other models (e.g. StarCoder) are missing - these results can answer the question, \"Are textbooks all you need or just the exercises?\""
            },
            "questions": {
                "value": "It needs a study of other models, e.g. starcoder finetuned on their \"synthetic exercises\" dataset. How does their performance delta look?\n\nNeeds study of overlap of \"synthetic textbook\" data with test sets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2112/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2112/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2112/Reviewer_JG5J"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2112/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698320817192,
        "cdate": 1698320817192,
        "tmdate": 1699636144066,
        "mdate": 1699636144066,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ULvUuj7tWh",
        "forum": "Fq8tKtjACC",
        "replyto": "Fq8tKtjACC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2112/Reviewer_DYsB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2112/Reviewer_DYsB"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a series of new models named phi-1. Phi-1 is pre-trained on high-quality filtered code-related data, and further fine-tuned on GPT-3.5 generated code exercises. Phi-1 achieves impressive results on HumanEval and MBPP, surpassing competitive open-source code models such as StarCoder. The studies highlights the importance of high-quality training data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Phi-1 is a very strong code model given its size. The release of such model will be of interest to the community.\n* The quality of training data is an important factor, and is previously overlooked. This paper highlights its importance and call for attention to it.\n* Paper is written clearly and well organized."
            },
            "weaknesses": {
                "value": "I don't see strong weaknesses in this paper.\nSee clarification / discussion questions below."
            },
            "questions": {
                "value": "* Can you please briefly introduce the training data and method used by CodeGen-Mono, Replit and StarCoder? Will be helpful in understanding how Phi-1 is different and perhaps will further highlight the important of data quality.\n* As mentioned in Sec 3.2, fine-tuning on coding exercises makes unrelated tasks easier to distill from pretraining. If I'm understanding it correctly, during pretraining, languages other than Python are used. Is it possible to further support this claim by evaluating on non-Python coding tasks?\n* It was mentioned that Phi-1 has emergent abilities that were not observed in Phi-1-base. In Sec 3, examples were used to support this claim. Is it possible to evaluate this more systematically and qualitatively?\n* Would be helpful to include GPT-4 performance on the 50 new unconventional coding problems."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2112/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827143450,
        "cdate": 1698827143450,
        "tmdate": 1699636143981,
        "mdate": 1699636143981,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NqACc9gfih",
        "forum": "Fq8tKtjACC",
        "replyto": "Fq8tKtjACC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2112/Reviewer_UYV3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2112/Reviewer_UYV3"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces phi-1, a new large language model for code, that achieves impressive performance on coding tasks despite its smaller size compared to competing models. The authors highlight the importance of high-quality data in improving the performance of language models for code generation tasks. They provide evidence of the benefits of using textbook-quality data and demonstrate the effectiveness of their approach through empirical evaluations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) Innovative methodology: The paper proposes the use of high-quality data, specifically textbook-quality data, to train a language model for code generation. This approach is novel and demonstrates the impact of data quality on model performance.\n\n2) Insightful empirical findings: The authors present empirical results that show the superior performance of phi-1 compared to other models on coding benchmarks. They also demonstrate the effect of fine-tuning on a small dataset and highlight the emergent properties of the model."
            },
            "weaknesses": {
                "value": "1) The authors are motivated to include high-quality data (i.e., textbook). However, they also use the data generated by gpt-3.5, which may have a lot noises.  How to justify the motivation because it seems contradictory to have both high-quality and low-quality data.\n\n2) In the caption of Figure 1, it is not light / dark green. maybe light / dark blue?\n\n3) In section 2, the authors are motivated by: We conjecture that language models would benefit from a training set that has the same qualities as a good \u201ctextbook\u201d: it should be clear, self-contained, instructive, and balanced. \nIt may be too intuitive without any justification. What about other high-quality data in addition to textbook? For example, legal documents, government document? \n\n\n4) In Section 2.1, the authors rely on the GPT-4 as an annotators to filter the data. Why can we rely on it? I am aware of some studies that using GPT-4 as an annotator. However, the ability of GPT-4 on this specific task is unknown.  Justifications are needed. For example, you may want to show the comparison between the data that GPT-4 thinks high-quality v.s. low-quality."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2112/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698843586916,
        "cdate": 1698843586916,
        "tmdate": 1699636143893,
        "mdate": 1699636143893,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "q20rINpP1y",
        "forum": "Fq8tKtjACC",
        "replyto": "Fq8tKtjACC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2112/Reviewer_QdXo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2112/Reviewer_QdXo"
        ],
        "content": {
            "summary": {
                "value": "This paper demonstrates that by meticulously curating high-quality data, one can use significantly smaller training corpora and fewer parameters to train a model that surpasses larger models and various Python code benchmarks. Specifically, the authors employ model-based classifiers distilled from LLMs to create a compact subset of common code corpora that are more diverse, less repetitive, and generally more informative and self-contained. Moreover, they utilize another LLM to generate high-quality pretraining data and a limited set of finetuning data. The performance of the resulting 1.3B model on common benchmarks supports the claim that enhancing models may not always require more compute and data, but can be achieved using smaller amounts of high-quality data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper adds to the expanding body of work that underscores the benefits of using quality, and broadens the empirical evidence to the field of LMs for code. The results presented in the paper are noteworthy; with a comparatively small model, they manage to surpass much larger models that were trained on more tokens. Another contribution of the paper is the utilization of existing LLMs to produce quality synthetic data at scale for pretraining the smaller model."
            },
            "weaknesses": {
                "value": "The paper posits the claim that meticulously curated data, combined with generating synthetic training data, can train smaller models that surpass larger ones. However, there are significant gaps regarding the creation of the training data. Specifically, the authors deliberately omit certain details, pointing to other papers that have taken a similar approach (as mentioned in footnote 1 on page 2). While the decision to withhold such details lies within a researcher's discretion, assessing the paper as a standalone piece makes it challenging to be certain that the authors' methodology doesn't introduce any data leaks potentially favoring their results. Although they explore leakage in the finetuning data, the same scrutiny isn't applied to their synthetic data. The approach taken to assess leakage against the finetuning data is somewhat nebulous. In their n-gram overlap analysis, they offer no rationale for their choice of 13-grams to gauge overlap, quickly concluding that \u201cn-grams are not refined enough to find similar code snippets\u201d. Subsequently, they switch to embeddings, commenting, \u201cWe observe that the embedding distance effectively captures code pairs,\u201d without substantiating this claim.\n\nAnother point of contention is the authors' assertion that phi-1 consumed less compute for training. They overlook the computational resources expended in creating their training data, and more importantly, the compute required to train the foundational LLMs. While this doesn't directly undermine the premise that a lesser-compute model can theoretically surpass larger counterparts, it does temper its implications given that generating such data on a grand scale necessitates the distillation of more resource-intensive models.\n\n\n\nPresentation comments for the authors (not a weakness, but should be addressed prior to final publication):\n1. In the paper, all references are given using \\citet, while the expected style calls for using \\citep whenever the authors or the publication are not included in the sentence. See https://iclr.cc/Conferences/2023/CallForPapers for style information. \n2. In page 2, you mention that you confirm the hypothesis that the number of parameters plays the key role in emergence. There is much work that show the role of parameter count in performance (Kaplan et al., 2020; Hernandez et al., 2021; Tay et al., 2021, 2022, Hoffmann et al 2022). However, there is also notable evidence against the emergence hypothesis (see Schaeffer et al 2023 and subsequent works). Since this work only contains two model sizes, I believe the language \u201cconfirm the hypothesis\u201d is too bold, as there is no way to differentiate a sudden jump from a smooth transition over a simple line. \n3. In page 6 \u00a73.2, you say \u201cwe demonstrate that finetunuing on CodeExercises unexpectedly improves the model\u2019s ability to use external libraries\u2026\u201d. Once again, I think this phrasing is not supported by the evidence given in the paper. Not only is the evaluation qualitative only, there is a single example in the paper to demonstrate that. Moreover, from the example it is clear that the model did see PyGame in the pretraining data and even phi-1-base knows how to use it. Instruction tuning during the finetuning is meant to help the model generalize to using its stored knowledge better, and there is nothing unexpected about the fact that the instruction following is better after it. \n4. In the bottom of page 7, you compare phi-1 to StarCoder on the new evaluation dataset and say \u201cphi-1 again achieves a score significantly higher than StarCoder\u201d. If I understand table 2 right, the difference between them is 51% vs 52% and there are no statistical measures to judge the significance of the difference, and thus I believe that the phrasing \u201csignificantly higher\u201d is misleading.\n5. The paper repeatedly suggests that phi-1 outperforms larger models despite its smaller size. However, as noted by you in the conclusions, phi-1 is trained only on python code while models such as StarCoder are considerably more general, and thus the comparison is not direct. I believe this point should be made more clearly in the introduction to avoid misleading claims.\n6. In the begging of \u201cMore related works\u201d paragraph, I think you used \u201crecent program\u201d by mistake instead of e.g. \u201crecent trend\u201d."
            },
            "questions": {
                "value": "1. I would like to receive more information on how the prompts to generate the synthetic data were created at scale.\n2. What verification, if any, was performed to rule out that the synthetic pre-training data generated is not highly similar to the existing samples in HumanEval?\n3. In Table 2, it looks like for all prior models as well as phi-1-base (the model without finetuning), there is a significant gap between the new score and the HumanEval one. However, in both finetunened phi-1 models this gaps is removed. Is it not possible that this means that while the finetuning data may be unrelated to the new evaluation, it contains considerable leakage with HumanEval? And if so, comparing the scores of StarCoder and phi-1 on the new evaluation may be more informative, thus concluding they perform similarly?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns in the paper."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2112/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698909659471,
        "cdate": 1698909659471,
        "tmdate": 1699636143822,
        "mdate": 1699636143822,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uWV0FXoBE1",
        "forum": "Fq8tKtjACC",
        "replyto": "Fq8tKtjACC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2112/Reviewer_71cs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2112/Reviewer_71cs"
        ],
        "content": {
            "summary": {
                "value": "This work presents a data and parameter efficient approach to training language models that results in small (1.3B) parameter models that are competitive and/or outperform models that are an order of magnitude larger. They propose the creation and curation of (relatively) smaller but high-quality (i.e. text-book quality) datasets for the training of high performance models, in this case, they create a 7B token dataset. The authors conjencture that LMs should learn from the same quality datasets that a human would use, and in particular that suchh datasets should be clear, self-contained, instructive and balanced."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This paper proposes a relatively high efficiency approach to training LLMs. It is impressive to get such results from a model trained on 8 A100s in 4 days. This makes this work approachable from academic research labs, which takes a step towards reversing the trend of pursuing ever larger datasets with larger models and computational requirements. This also has implications for energy efficiency and sustainability.\n\nThe paper itself is well-written and quite clear.\n\nThe authors have commited to the release of the \"model for usage and evaluation by the broader community\". This is important for open science and a big strength."
            },
            "weaknesses": {
                "value": "Key details on the data generation process are not shared for \"proprietory reasons\", yet this is central to the paper, its proposition and its results. \n\nThe lack of a broader impacts section weakens this paper.\n\nIf this weakness is addressed, I am happy to improve my score."
            },
            "questions": {
                "value": "Would you argue that the structure and compositionality of programming code is critical for achieving such high levels of performance from small models and datasets. I wonder if a takeway is to always seed training from well structured and commented code? If so, would this translate to other programming/instruction like language tasks? e.g. recipes, etc..\n\nDo you think that the code excercises factor into improving performance by encouraging the model to self-critic whatever it generates, given that it seems to append excercises to generations?\n\nWhere do you think this approach is most likely to fail? With what kinds of tasks/data?\n\nDoes phi-1 have ICL abilities? And are there other emergent capabilities that can be forseen and easily tested?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2112/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699215584860,
        "cdate": 1699215584860,
        "tmdate": 1699636143763,
        "mdate": 1699636143763,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "avM9prMWox",
        "forum": "Fq8tKtjACC",
        "replyto": "Fq8tKtjACC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2112/Reviewer_zVvy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2112/Reviewer_zVvy"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a phi model for code generation. With 1.5B parameters trained for 4 days on 8 A100 GPUs, phi-1 achieves high accuracy on code generation benchmarks like HumanEval and MBPP. The key to phi-1's performance is its training data. Instead of standard code data, it is trained on \"textbook quality\" data including synthetic textbooks generated by GPT-3.5 and filtered code from the web. This high quality data allows phi-1 to match larger models trained on much more data.\n\nThe authors claim that despite its small size, phi-1 displays emergent capabilities like managing complex algorithms and using external libraries not in its training data.  The authors also show that aggressive pruning of training data similar to the HumanEval test set shows phi-1's performance boost is not due to \"contamination\". phi-1 outperforms StarCoder even after removing 40% of training data. The authors argue high quality data is key to advancing language models. Creating diverse, high quality datasets remains challenging and important for future work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper achieves state-of-the-art results on code generation benchmarks with a much smaller model trained on far less data. This re-emphasizes the power of high quality, tailored training data.\n+ The model requires less compute resources for training compared to larger models. \n+ The paper also shows rigorous evaluation of potential training set contamination."
            },
            "weaknesses": {
                "value": "- The paper has low novelty, and the importance of data quality in LLMs is well known. \n- The paper is only evaluated mainly on short Python functions. How would it perform on more complex, real-world coding tasks."
            },
            "questions": {
                "value": "- How was the quality of the synthetic textbook data evaluated? What measures were used to ensure it is high quality and diverse?\n- How do the results of synthetic data, and using textbooks generalize to other domains beyond code? Does this approach advance the scientific understanding of training language models for code generation?\n- What are the unique contributions compared to prior work on tailored training data and prompted training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 6,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2112/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699249208859,
        "cdate": 1699249208859,
        "tmdate": 1699636143701,
        "mdate": 1699636143701,
        "license": "CC BY 4.0",
        "version": 2
    }
]