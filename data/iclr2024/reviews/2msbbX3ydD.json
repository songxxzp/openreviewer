[
    {
        "id": "DNAMBvxY0W",
        "forum": "2msbbX3ydD",
        "replyto": "2msbbX3ydD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1527/Reviewer_dfTo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1527/Reviewer_dfTo"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a large multimodal model Ferret, which, compared with prior works, is especially good at referring and grounding. In Ferret, they propose a hybrid region representation and a novel spatial-aware visual sampler to represent the visual and regional input. To train Ferret, they creat GRIT, a large-scale ground-and-refer instruction tuning dataset. They also introduce Ferret-Bench to evaluate the grounding, referring, and reasoning capabilities of LMMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed GRIT dataset is meaningful to the vision and language research. \n2. The proposed Spatial-Aware Visual Sampler and Hybrid Region Representation are well-motivated. \n3. The experiment results show the better capabilities of the trained model on multiple referring and grounding tasks and validate the effectiveness of the spatial-aware visual sampler module."
            },
            "weaknesses": {
                "value": "1. The ablation on hybrid region representation is missing.\n2. Not a strong weakness, but whether the model performs well on non-referring or grounding tasks needs more validation. E.g. VQA_v2, MME, general captioning, etc. And it seems the caption evaluation is not as good as InstructBLIP."
            },
            "questions": {
                "value": "1. Is the evaluation on Flickr30k grounded caption fine-tuned or zero-shot?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1527/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1527/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1527/Reviewer_dfTo"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1527/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698395804840,
        "cdate": 1698395804840,
        "tmdate": 1699636081273,
        "mdate": 1699636081273,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5F5wI4Dn3k",
        "forum": "2msbbX3ydD",
        "replyto": "2msbbX3ydD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1527/Reviewer_6Qgf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1527/Reviewer_6Qgf"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Ferret, a Multimodal Large Language Model (MLLM) capable of understanding spatial referring within an image and grounding open-vocabulary descriptions. The paper also introduces the dataset GRIT with 1.1M samples and an additional 130K hard negative data.\n\nFerret includes several components:\n1. A powerful hybrid region representation integrating discrete coordinates and continuous features to represent image regions.\n2. A spatial-aware visual sampler for handling various region shapes and extracting continuous features.\n3. Integration with LLM for referring and grounding tasks.\n\nFerret achieves superior performance in classical referring and grounding tasks and outperforms existing methods and it shows improved capability in describing image details and reduces object hallucination."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is presented very well. \n2. The paper shows a reasonable motivation that humans inherently possess the ability to learn from one task and generalize to another between referring and grounding. This underscores the essential need to unify referring and grounding processes.\n3. The hybrid region representation and spatial-aware visual sampler make the framework flexible to take different form of region definition.\n4. The framework shows a good way of utilization of Large Language Model.\n5. Contribution of the dataset."
            },
            "weaknesses": {
                "value": "1. No open source code for the code and dataset. I would raise the soundness score if code and dataset are open, either attached in the supplementary or released in the public repo.\n2. The hierarchy of the dataset is a bit complicated. This may not be practical for costume dataset.\n3. Very engineering paper, extensive work, but not much scientific novelty."
            },
            "questions": {
                "value": "1. Table 5 shows that mutual benefits of grounding and referring. From the results, it seems grounding task can help more for referring task than the other way around. How to interpret this effect?\n2. For section 4.2, how to evaluate quality of the generated data from ChatGPT and GPT4?\n3. You mensioned in the Ferret-Bench is via GPT4 as a judge. But some of the data are collected from GPT4, is a reason it is better than all other models in Table 7.\n4. Can chatgpt take multimodal input? Since when you collecting dataset via LLM, the author mentioned they use ChatGPT first and then use GPT-4 to refine it. I am wondering how ChatGPT can take image as input.\n\nIt is extensive of work. I would like to raise my score if the questions are addressed and the code and data are public."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1527/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723500433,
        "cdate": 1698723500433,
        "tmdate": 1699636081195,
        "mdate": 1699636081195,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i1252noeBU",
        "forum": "2msbbX3ydD",
        "replyto": "2msbbX3ydD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1527/Reviewer_7dkr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1527/Reviewer_7dkr"
        ],
        "content": {
            "summary": {
                "value": "The paper tackles the challenge of training a Multi-modal Large Language Model to accurately interpret input visual references, such as points, bounding boxes, free-form shapes, with respect to the image (referring) and ground the output text to relevant image regions (grounding). The authors propose a unified framework, called Ferret, for jointly solving the visual referring and grounding problem. They provide a new curated dataset (GRIT) that consists of existing and newly collected data for training, as well as a new benchmark (Ferret-Bench). In the evaluation section, the authors show that the proposed method either exceeds (on the Ferret-Bench and grounded captioning) or is on par with the concurrent SOTA methods such as Shikra. In addition, the proposed method can accept a variety of user input on images as part of referring expressions, including scribble and freeform shapes, in addition to the traditional points and bounding boxes. However, the way how these different visual reference types are processed is conceptually similar to the Visual Sampler proposed in SEEM (Zou et al., 2023), and performs similarly despite the added complexity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1. This seems to be one of the first MLLMs to support a variety of visual reference types, such as point, box, scribble, polygons, and masks. \n\nS2. The authors provide a curated dataset called GRIT that consists of existing datasets and newly collected data for training MLLMs with visual referring and grounding capabilities.\n\nS3.  The authors provide a new benchmark, Ferret-Bench, which covers two new types of evaluation task for visual referencing (description and reasoning) in addition to the conversation grounding task. The key difference to existing benchmarks such as RefCOCO+, RefCOCOg, or PointQA [a] is that the questions include visual references (in forms of bounding boxes). For example, \u201cWhat is the purpose of the object [x1 y1 x2 y2]?\u201d\n - [a] Point and Ask: Incorporating Pointing into Visual Question Answering, Mani et al., 2022\n\nS4. The paper provides comparison with the SOTA methods and the concurrent methods in the evaluation section."
            },
            "weaknesses": {
                "value": "W1. The paper omits any discussion on the limitations or potential failure scenarios of the proposed method.\n\nW2. The significance of the proposed Spatial-Aware Visual Sampler is minimal. The idea of sampling the visual features over the grid is in the same spirit as the Visual Sampler in SEEM (Zou et al., 2023), although the details of how the points features are aggregated and pooled are different. Performance-wise, the Spatial-Aware Visual Sampler is shown to be only marginally better than the Visual Sampler in SEEM as shown in the ablation study section.\n\nW3. While the idea of jointly solving referring (with explicit visual cues, such as markings on the image) and grounding in one unified framework makes sense as the two tasks are interrelated, this idea was also explored in concurrent works (Chen et al., 2023a) and (Peng et al., 2023)."
            },
            "questions": {
                "value": "Q1. It appears that the proposed method significantly outperforms Shikra on Ferret-Bench, but not much on existing datasets. I wonder if the authors can explain why.\n\nQ2. I think the quality of writing could be further improved. For example, it is not clear to me what the authors are trying to imply by \u201cFirst of all, we choose MLLM as the bedrock of Ferret due to their powerful vision-language global understanding capability\u201d. I am guessing that they wanted to say that the Ferret is built on top of existing MLLMs to leverage their powerful vision-language capability?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1527/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1527/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1527/Reviewer_7dkr"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1527/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698805174658,
        "cdate": 1698805174658,
        "tmdate": 1699636081127,
        "mdate": 1699636081127,
        "license": "CC BY 4.0",
        "version": 2
    }
]