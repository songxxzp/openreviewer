[
    {
        "id": "aHPRTyDptQ",
        "forum": "Pa6SiS66p0",
        "replyto": "Pa6SiS66p0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7210/Reviewer_eqa5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7210/Reviewer_eqa5"
        ],
        "content": {
            "summary": {
                "value": "This paper studies an under-explored problem --- leveraging multiple modalities for lifelong learning. Towards this end, the authors (1) provide a benchmark for this task, sourced from VGGSound; (2) conduct a case study demonstrating the advantages of using multiple modalities over a single modality; (3) develop an approach to leverage relational structural information in each modality for better integration of multimodal information."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The proposed benchmark covers three CL scenarios and can be beneficial to the community. \n+ The analysis in Section 3 makes sense and provides empirical evidence for the superiority of multiple modalities over single modality in CL.\n+ The paper has good motivation and is well organized."
            },
            "weaknesses": {
                "value": "My major concern with this paper is the lack of comparison and experiments. The evaluation seems a bit weak to me as all the experiments are conducted on VGGSound only, and the baseline Experience Replay for comparison with the proposed approach is from 2018. I wonder if the authors could apply some more recent unimodal CL approaches ([1][2] etc.) to the problem.\n\n[1] SS-IL: separated softmax for incremental learning   \n[2] Class-incremental learning by knowledge distillation with adaptive feature consolidation.\n\n---\nAlso, in terms of comparison with multimodal CL approaches:\n+ (1) Could the authors further clarify why the proposed approach can not be applied to vision-language? What is the advantage of the proposed benchmark compared with [3], besides the modality difference? \n+ (2) I understand that [4] is published after the submission ddl, but it would be good if the authors could comment a few sentences about the differences with them in the rebuttal.  \n\n[3] Climb: A continual learning benchmark for vision-and-language tasks.  \n[4] Audio-Visual Class-Incremental Learning\n\n---\nFor the semantic-aware feature alignment, I wonder if the authors can provide some visualization examples to demonstrate that the model indeed learns the desired modality-specific features, such as Figure 4 in [5].\n\n[5] The Modality Focusing Hypothesis: Towards Understanding Crossmodal Knowledge Distillation\n\n---\nTypo, Figure 4 caption, \"leverage leverages\""
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698615708265,
        "cdate": 1698615708265,
        "tmdate": 1699636856919,
        "mdate": 1699636856919,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Y0tvOCrmhT",
        "forum": "Pa6SiS66p0",
        "replyto": "Pa6SiS66p0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7210/Reviewer_67Cp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7210/Reviewer_67Cp"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new benchmark based on the VGGSound dataset for multimodal (visual-audio) continual learning (CL). \n\nThe authors show complementary aspects with the results of analyses on the dataset to highlight the advantageous points of integrating multiple modalities of visual and audio. \n\nAlso, the paper presents a method for integrating and aligning information from multiple modalities using relational structural similarities, which seems to induce more robust representations to reduce catastrophic forgetting in deep neural networks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors introduce novel benchmark datasets for multimodal CL on vision and audio. If publicly available, it would be valuable and helpful for our communities to provide one of the standardized frameworks for evaluating the performance of models and facilitating fair comparisons between different methods in visual-audio multimodal CL settings.\n\n\n- The paper presents empirical evidence supporting the complementary benefits of integrating multiple modalities of vision and audio. It seems to have better representations to be robust to reduce catastrophic forgetting."
            },
            "weaknesses": {
                "value": "- \ufeffThe paper shows the main experimental results of the proposed method, SAMM (Semantic-aware multimodal method), in Table 1~2. I think that the performances of other methods reported in major references such as [Buzzega et al., NIPS20] or [Arani et al., PAMI 2022] seem to be compared. Since lack of comparison, it is NOT clear to figure out the effectiveness and uniqueness of the proposed method among other methods. \n\n- The paper does NOT provide enough information (including data composition, details on evaluation, and experimental settings) to reproduce the results in the experiments, even though Appendix A.2~A.4 presents some information.\n\n- It seems weak as a paper to propose a new dataset. Because it needs to provide baseline performances to show the characteristics of the dataset. On the other hand, it seems weak as a paper to propose a novel method for continual learning for visual-audio multimodal settings since it does not clearly validate the pros and cons of the proposed method.\n\n\n\n-- Minor\n- 5th line on page 5, models a capture --> models to capture?\n- caption in Figure 4, leverages leverage --> leverages?\n- lines in Figure 3, it would be better to draw with line styles (solid, dotted, ... ). It is not easy to discriminate in gray-color printing."
            },
            "questions": {
                "value": "- Is there any reason to compare with only ER?\n\n- What is the motivation to introduce relational structural similarity into the proposed models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699212739495,
        "cdate": 1699212739495,
        "tmdate": 1699636856783,
        "mdate": 1699636856783,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WvpQy9Ii26",
        "forum": "Pa6SiS66p0",
        "replyto": "Pa6SiS66p0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7210/Reviewer_i2PA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7210/Reviewer_i2PA"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a multi-modal continual learning benchmark.  Further, this paper also provides a simple baseline by incorporating the knowledge contained in different modalities to achieve better multi-modal continual learning with less forgetting on previously learned tasks. Experiments on a visual and audio modality continual learning dataset show the effectiveness of the proposed method compared to standard experience replay."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper is easy to follow.\n\n* This paper provides a baseline of multi-modal continual learning and benchmark."
            },
            "weaknesses": {
                "value": "* The proposed method is straightforward with experience replay and those techniques are commonly used in existing multimodal learning and continual learning literature. \n\n\n* The memory buffer includes multi-modal examples from previous tasks.  The authors store the same number of data for single-modality and multi-modality. It would be better to compare different modality methods in terms of the same memory storage since multi-modality memory data requires more storage to store multi-modality data.\n\n\n* The baseline is too weak, only the standard experience replay is compared. It would be better to compare to more recent state-of-art baselines in experience replay.  \n\n\n* Furthermore, there are other categories of CL methods, including regularization-based methods and architecture-based methods. It would be better to also compare those methods in the experiment. \n\n\n* The experiments are only performed on visual and audio modality. It would be better to provide experiment and benchmark on other modalities as well, e.g., language."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699297101925,
        "cdate": 1699297101925,
        "tmdate": 1699636856663,
        "mdate": 1699636856663,
        "license": "CC BY 4.0",
        "version": 2
    }
]