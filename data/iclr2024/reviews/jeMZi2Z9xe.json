[
    {
        "id": "TePUk2cNJG",
        "forum": "jeMZi2Z9xe",
        "replyto": "jeMZi2Z9xe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7706/Reviewer_PzqF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7706/Reviewer_PzqF"
        ],
        "content": {
            "summary": {
                "value": "This submission studies the adversarial K-armed bandit problem with heavy-tailed losses. Before the game begins, an oblivious adversary selects a sequence of $T$ heavy-tailed loss distributions for each arm, after which the learner pulls a sequence of $T$ arms, and only observes the loss realizations for the arms they decided to pull. The authors' main result is to show that a modified version of Follow-the-Perturbed-Leader (FPTL) achieves optimal regret with respect to the best arm in hindsight, up to logarithmic-in-K factors. At a high level, FTPL adds a random perturbation to the sequence of observed losses at each time-step, before taking the action which minimizes the sequence of perturbed losses. The authors\u2019 algorithm (Algorithm 1) proceeds similarly, with the key difference being that they \u2019skip\u2019 losses (i.e. do not factor them into future calculations) if they are larger than a given cutoff threshold. In addition to achieving near-optimal performance in the adversarial setting with unbounded losses, the authors apply their algorithm to two different settings: adversarial bandits with Huber contamination and local differential privacy. In the Huber contamination setting, the loss observed by the learner is not the true loss, but is instead generated from some arbitrary and unknown distribution. Under this setting, the authors show that their algorithm achieves optimal regret (up to logarithmic factors). Under differential privacy, the authors\u2019 algorithm improves upon existing results by polylogarithmic factors."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors show that a slight modification of a well-known and popular algorithm (FTPL) achieves near-optimal performance in the adversarial multi-armed bandit setting with heavy-tailed losses. Their algorithm is applicable in a wider range of settings when compared to previous, Follow-the-Regularized-Leader (FTRL)-based algorithms.  (See Assumption 2 and the following discussion for more details.) Additionally, the two applications are interesting, and the authors\u2019 retults in these settings are either near-optimal or state-of-the-art. Finally, the writing of the paper is overall a strength. While I had one question in particular (see below), I found that the authors did a good job of succinctly describing (1) their algorithm (2) the challenges of the setting they consider, (3) the salient parts of their analysis, and (4) their applications."
            },
            "weaknesses": {
                "value": "It would be good to give some intuition behind why the algorithm \u2019skips\u2019 losses. \n\nClaiming Lemma 1 as a key observation seems to be an overstatement of the authors\u2019 results, as Lemma 1 appears in previous work. It would be good if the authors could clarify what exactly is their observation when compared to previous work."
            },
            "questions": {
                "value": "Can you results be extended to settings in which the loss distributions are generated  by an adaptive adversary?\n\nIs the skipping of large losses necessary? Or could a more clever analysis remove the need for this step in the algorithm?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7706/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7706/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7706/Reviewer_PzqF"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7706/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698607350032,
        "cdate": 1698607350032,
        "tmdate": 1699636939340,
        "mdate": 1699636939340,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eXjZRKUASl",
        "forum": "jeMZi2Z9xe",
        "replyto": "jeMZi2Z9xe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7706/Reviewer_rKqQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7706/Reviewer_rKqQ"
        ],
        "content": {
            "summary": {
                "value": "The authors consider the adversarial bandit problems with heavy-tailed and possibly non-negative, bounded losses. The authors then point out the limitations of previously known work in this setting. The authors propose a novel FTPL solution scheme to tackle these challenges. This solution avoids the need for extra assumptions required by the state-of-the-art FTRL algorithm. Finally, the authors demonstrate the performance of the proposed algorithm by studying two examples: adversarial bandits with heavy-tailed losses and Huber contamination and  adversarial bandit with bounded loss  and additional Local Differential Privacy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is very well-written and it is fairly easy to follow. The main idea is quite straightforward and the way it is presented (in comparison with previously known work) is very clear. \n\n- The sketch of proof helps understanding the insights of the results and highlight the contributions (unfortunately, I did not have time to check the detailed proof in Appendix, but from the sketch, it seems sound enough). \n\n- The two example applications are also well presented."
            },
            "weaknesses": {
                "value": "- A minor weakness is that the \"breakthrough\" comes from a known property (FTPL with Laplace perturbation) and its combination with Skipping (yet again, a previously known scheme) is rather obvious from the way it is currently presented. Personally, I believe that the authors do a good job in making such observations and tuning the algorithm's parameters (notably, L_t) which is often not that obvious. It might be better if the authors highlight a bit more on this aspect. \n- The questions posed by the paper is interesting in a theoretical point of view. However, it might be better to provide some motivational/practical examples where it is essential to model the problem with negative, heavy-tailed losses like this instead of simply changing the loss models to fit more traditional non-negative loss framework.  The authors might argue that the Huber-contamination serves this purpose but Lemma 7 is valuable only if beta is significantly small, which undermines this argument. \n- The proposed algorithm still requires knowledge of coefficients of Assumption 1 to deterministically tune the step-size, the skipping threshold and the L_t parameter. I am not sure how realistic this requirement can be.  As usual in bandit, it might be possible to relax this by using bounds of the involved coefficients instead of the true values. Do you think the results still hold?\n- Another minor weakness of the algorithm is that the While-Loop that runs (in worst-case) with L_t rounds (that need be sufficiently large as pointed out) at each step. Is there possible to look for a better sampling scheme to check simultaneously many perturbed leaders at the same time? \n- Despite being a theoretical paper, some simple experiments can help to clear further the comparison with SOTA (see questions below)."
            },
            "questions": {
                "value": "- While I agree with Remark 10, Assumption 2 is purely for technical purpose (of the proof) and does not really hinder one to run the FTRL algorithm of Huang et al.2002 in your setting (even though their regret-bound is not guaranteed). It might be worth to run experiments to compare it with the proposed FTPL. \n\n- Corollary 1 states the results specifically with the (epsilon-DP) mechanism. Does this mean it is not applicable to some situations where the privacy is not Laplace (but still guaranteeing heavy-tailed losses) ?\n\n- Do you think it is possible to design a dynamic/adaptive skipping threshold r?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethical issues with this paper to signify."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7706/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7706/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7706/Reviewer_rKqQ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7706/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698743463234,
        "cdate": 1698743463234,
        "tmdate": 1699636939210,
        "mdate": 1699636939210,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "J2d8meZpEu",
        "forum": "jeMZi2Z9xe",
        "replyto": "jeMZi2Z9xe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7706/Reviewer_J99q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7706/Reviewer_J99q"
        ],
        "content": {
            "summary": {
                "value": "The paper studies adversarial bandit problems with potentially heavy-tailed losses.\nThe authors propose a Follow-the-Perturbed-Leader (FTPL) based learning algorithm that achieves (nearly) optimal worst-case regret.\nThe authors further show that their algorithm works for adversarial bandits with heavy-tailed losses and Huber contamination and adversarial bandits in the private setting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors consider FTPL based alg. instead of FTRL based alg., which improves current results by $poly(\\log T)$. Especially, the results of adversarial bandits with heavy-tailed losses and Huber contamination and adversarial bandits in the private setting are new and better than prior works.\n- The paper is well writen. The proof in the appendix is well organized and mainly correct."
            },
            "weaknesses": {
                "value": "- Contrary to the author's statement, it is really trivial to use FTRL based algorithm to achieves (nearly) optimal worst-case regret for adversarial bandit problems with potentially heavy-tailed losses.\n  First, using the similar skipping method as in the paper, the unbounded adversarial bandit problem can be reduced to the bounded case (the losses are bouned by $[-r,r]$, where $r = \\sigma T^{1/\\alpha} K^{-1/\\alpha}$).\n  Then, using the algorithm in [wei2018more] and scale the losses by $1/r$, by Theorem 4 in [wei2018more], we can immediately get regret upper bound $\\mathcal{O}(K\\ln T/\\eta +\\eta Q_{T, i*} + Kr (\\ln T)^2 )$, where $Q_{T,i^*} = \\sum_t (\\ell_{t, i^*}-\\sum_t \\ell_{t, i^*}/T)^2\\le \\sum_t \\ell_{t, i^*}^2$. Since here $|\\ell_{t, i^*}|$ is upper bounded by $r$ due to the skippping, there is $\\mathbb{E}[\\sum_t \\ell_{t, i^*}^2]\\le \\mathbb{E}[\\sum_t |\\ell_{t, i^*}|^\\alpha r^{2-\\alpha}] \\le T \\sigma^\\alpha r^{2-\\alpha} = \\sigma^2 T^{2/\\alpha} K^{1-2/\\alpha} $. \n  Thus, taking a suitable $\\eta$ to balance the first and second terms results in regret $\\mathcal{O}(\\sigma T^{1/\\alpha} K^{1-1/\\alpha}(\\ln T)^2 )$, which matches the results of this paper (up to log terms). (I guess such method can also get the regret guarantee Lemma 7 for adversarial bandits with huber contamination as in the paper)\n- The design of GR count/GR maximum is confusing. According to the proof given by the authors, it is suffices to use the important weighting estimator (just set M_t = 1/w_{t, i}). In this case, GrErr goes to $0$ and FTPLReg can also be well bounded (proof of Lemma 5 still works). The authors do not clearly explain why it is important to use Geometric Resampling in their algorithm.\n\n[wei2018more]: More Adaptive Algorithms for Adversarial Bandits"
            },
            "questions": {
                "value": "- Why we need to use Geometric Resampling in the algorithm? Is it just for the proof of DP case?\n- Is there any high level intuition why there exists poly(log K) in the regret? Is it possible to remove such terms?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Theory work. No ethics concerns."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7706/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7706/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7706/Reviewer_J99q"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7706/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766486444,
        "cdate": 1698766486444,
        "tmdate": 1699636939051,
        "mdate": 1699636939051,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dicVKLYo95",
        "forum": "jeMZi2Z9xe",
        "replyto": "jeMZi2Z9xe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7706/Reviewer_XopM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7706/Reviewer_XopM"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a FTPL-based algorithm for adversrial bandits with heavy-tailed losses. The proposed method achieves near-optimal regret and improves the regret of two applications: heavy-tailed adversarial badnits with huber contamization and adversarial bandits with bounded losses and LDP."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and mostly clear.\n2. The proposed method achieves near-optimal regret bound."
            },
            "weaknesses": {
                "value": "1. Notice that the FTRL algorithm is best-of-both-worlds (see Huang et al. 2022). Does the proposed algorithm can achieve optimal regret in the stochastic setting?\n2. The \"applications\" in this paper are also bandit models. It would be better if the paper includes some empirical analysis of real world applications."
            },
            "questions": {
                "value": "Is it possible to design a near-optimal algorithm for bandits with heavy-tailed loss if we don't know the heavy-tail parameters $\\sigma$ and $\\alpha$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7706/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699107570587,
        "cdate": 1699107570587,
        "tmdate": 1699636938951,
        "mdate": 1699636938951,
        "license": "CC BY 4.0",
        "version": 2
    }
]