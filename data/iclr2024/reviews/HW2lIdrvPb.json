[
    {
        "id": "6iEfxQat0P",
        "forum": "HW2lIdrvPb",
        "replyto": "HW2lIdrvPb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2717/Reviewer_Wc92"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2717/Reviewer_Wc92"
        ],
        "content": {
            "summary": {
                "value": "To address the problem of sparse annotated data in existing detection tasks, the paper designed a framework that uses Diffusion Model interpolation to generate abnormal data, and then uses the synthesized data to perform model selection for anomaly detection. The paper conducted extensive experiments on both natural image data and industrial image data, demonstrating the effectiveness of this framework."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ To address the problem of sparse annotated samples, the paper uses interpolation with Diffusion Model to transform normal images into abnormal images with certain semantic information, thus simulating some abnormal data well.\n+ The paper validated the effectiveness of synthesized data through extensive experiments on different datasets and models.\n+ The paper validated the effectiveness of synthesized data in selecting prompts for zero-shot detection using CLIP."
            },
            "weaknesses": {
                "value": "+ Although the paper's method has significant effects on the Flowers and CUB datasets, it does not perform well on the MVTec AD dataset. From Tables 1 and 2, it can be seen that the synthesized data is not helpful for the MVTec dataset. From Figure 2, it can also be seen that the interpolation synthesis has poor performance. For the CUB dataset, the anomalies are more significant, so the synthesized data is effective, but for the MVTec dataset, the anomalies are more subtle, so the synthesized data is not effective.\n+ The paper's abnormal synthesis function is entirely based on DiffStyle, and it remains to be verified whether the method of interpolating abnormal images from different normal images is reasonable. Perhaps perturbing features in different dimensions in the latent space may have better results. The paper should consider different designs for high-level semantic anomalies and low-level semantic anomalies in this regard."
            },
            "questions": {
                "value": "+ It is unclear what considerations the authors had in comparing the Flowers, CUB, and MVTec datasets. There are significant differences between these datasets.\n+ The paper's workload is significant, but the experimental character is too strong, and it is unclear whether the authors have any ideas for redesigning the Diffusion generation process based on the experimental results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2717/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2717/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2717/Reviewer_Wc92"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2717/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698568895398,
        "cdate": 1698568895398,
        "tmdate": 1699636213938,
        "mdate": 1699636213938,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "X4q23T7mua",
        "forum": "HW2lIdrvPb",
        "replyto": "HW2lIdrvPb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2717/Reviewer_tN6d"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2717/Reviewer_tN6d"
        ],
        "content": {
            "summary": {
                "value": "In the paper \"Model Selection of Anomaly Detectors in the Absence of Labeled Validation Data\" the authors consider the task of anomaly detection in a semi-supervised setting where only normal data is given for training. For selecting a suitable anomaly detector, the authors propose to augment the validation data by anomalous data points that are created with the help of diffusion models. In their empirical study, the authors find the synthetically created anomalies to give rise to a good choice of anomaly detectors."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Novel method and intriguing idea to create synthetic anomalies for images as an input.\n- Strong empirical performance of the proposed method\n- Thorough evaluation of the method and good set of baselines.\n- The related literature is nicely reviewed and the overall presentation is excellent."
            },
            "weaknesses": {
                "value": "- The authors could elaborate more on the limitations of the approach. For instance, one problem I see is that the synthetically generated anomalies do not necessarily resemble the ground truth distribution of anomalies. In particular, the question for the image classification datasets is indeed what would an actual anomaly look like? In particular, to me, it is questionable whether the generated images make sense at all as the observations would probably never be made in the real world.\n- Figure 3 needs more explanation what exactly is plotted. The legend is of the figure is also off, e.g., in Figure 3(a) there are squares which are not contained in the legend."
            },
            "questions": {
                "value": "- How are anomalies even defined in the image domain? Is it just out of distribution data? Unrealistic images? If the latter, why would one even expect to observe such images?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2717/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698739061076,
        "cdate": 1698739061076,
        "tmdate": 1699636213868,
        "mdate": 1699636213868,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qAusUDOg3x",
        "forum": "HW2lIdrvPb",
        "replyto": "HW2lIdrvPb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2717/Reviewer_v3es"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2717/Reviewer_v3es"
        ],
        "content": {
            "summary": {
                "value": "The authors propose to generate synthetic outlier data for outlier detection tasks. The method is based from mapping images into latents, then taking a mixture in latent space and mapping it back using a diffusion model.  \nThe evaluate the performance of one outlier detection method in multiple setups and 3 datasets. Furthermore they evaluate the suitability of the generated outliers for selecting prompts for the usage of CLIP as foundational model for zero-shot outlier detection."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "They ask an important question, namely how reliable is synthetic outlier data for the evaluation of outlier detection setups.\nThe idea is clear and simple."
            },
            "weaknesses": {
                "value": "I think the question would need to be evaluated for more outlier detection methods, not just one distance based one. \n\nAlso the method to generate outliers is very simple. \nIt is a simplified version of mixup in latent space. It offers no control over what kind of outliers are created. \nCalling it a style and content mixture is dubious, because the method seemingly has no attempted separation into content and style. it seemingly has only 1 latent space.\nThis would also benefit from a more thorough evaluation of creating outliers. e.g. stochastic mixup in latent space, or inpainting, and so on.\n\nAlso it is not clear to what extent the method learns to discriminate real image properties from synthetic ones - this is because the real images are never run through the encoding-decoding step.\n\nThis conclusion is not true:\nIn an extensive empirical study, ranging from natural images to industrial applications, we find that our synthetic validation framework selects the same models and hyper-parameters as selection with a ground-truth validation set.\n\nyes in the simple class vs other classes on flowers and birds it holds, on the more realistic MVTec it does not hold, see their appendix.\n\nIt is not bad per se, if the proposed method does not work, but putting a questionable conclusion in the abstract is misleading.\n\nThe zero-shot task result is interesting scientifically. Practically it is unlikely that one would use that for serious outlier detection tasks."
            },
            "questions": {
                "value": "What are the five repetitions in Figure 3? Can they be compared against each other ?\n\nHow would the outlier detection perform for reconstruction based AD methods ? or maybe another class ?\n\nCan experiments be run to ascertain the usefulness for smaller defects beyond MVTec ? What if the outliers are not on semantic level but more on imaging setting differences ?\n\nCan an experiment be performed to understand to what extent the method classifies real vs diffusion generated images ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2717/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2717/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2717/Reviewer_v3es"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2717/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699097481709,
        "cdate": 1699097481709,
        "tmdate": 1700718871469,
        "mdate": 1700718871469,
        "license": "CC BY 4.0",
        "version": 2
    }
]