[
    {
        "id": "ZOY4gEuIvE",
        "forum": "g0mlwqs8pi",
        "replyto": "g0mlwqs8pi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6403/Reviewer_Yzoy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6403/Reviewer_Yzoy"
        ],
        "content": {
            "summary": {
                "value": "This paper demonstrates an interesting phenomenon where not properly tuning the local step size of the GD algorithm might negatively impact performance of an FL system. To address this, this paper proposes an adaptive scheme for setting local step sizes called $\\Delta$-SGD, and provides an analysis of its convergence under mild assumptions. Finally, the paper provides a thorough empirical demonstration of the proposed method, comparing it with other optimizers when used by FedAvg."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is clearly written and well motivated.\n\nTheorem 1 and its proof in Appendix A seems to be the main contribution. I have checked and believed that the proofs are sound.\n\nThe empirical results presented are generally promising."
            },
            "weaknesses": {
                "value": "1. Novelty:\n- I believe $\\Delta$-SGD is algorithmically similar to the Adaptive SGD method in the paper \"Adaptive Gradient Descent without Descent\" (Malitsky and Mishchenko, 2020). While there are some attempts to cite and discuss this paper, I think the authors should be more upfront about it, as it changes the perceived contribution entirely.\n- To the best of my knowledge, Malitsky and Mishchenko did not deal with the FL setting, so I believe theorem 1 and its proof are novel. However, I think the authors should provide a proof sketch to help readers understand the nuance of the proving strategy. What is hard/non-trivial about applying Adaptive SGD in the FL setting?\n\n2. Significance:\n- Despite the claim that $\\Delta$-SGD is complementary to other FL methods that perform server adaptation this is not demonstrated in the experiments.\n- I think Table 2b should be expanded to become one of the main result (conduct over all datasets, rather than as an ablation study on 2 datasets).\n-  The gain in performance is not significant enough in many cases. The authors should provide some error bar or standard deviation."
            },
            "questions": {
                "value": "1.The result presentation is a little bit confusing. The small number on the right of Table 1 is the performance difference from the best result, but what is this \"best result\" referring to? Best among \\alpha = 1.0/ 0.1/ 0.01 ?\n\n2. For all methods, the authors performed grid search on CIFAR-10 and apply it on other test scenarios. Why not performing grid search on each individual setting? Is it because of the expensive cost of hyperparameter tuning?\n\n3. If hyperparameter tuning is the bottleneck (and the motivation), how will this method compare to the method FedEx proposed by the paper Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing (Khodak et al., 2021)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6403/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698610458978,
        "cdate": 1698610458978,
        "tmdate": 1699636711555,
        "mdate": 1699636711555,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9zgDCc1ahC",
        "forum": "g0mlwqs8pi",
        "replyto": "g0mlwqs8pi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6403/Reviewer_RQMs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6403/Reviewer_RQMs"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed using a locality adaptive step size rule for client-side training in Federated Learning frameworks. Their method requires almost no hyperparameter tuning. They showed the superiority of their algorithm by comparing it with other adaptive client-side step-size methods experimentally. They inspired this technique from a similar technique proposed for centralized training. They proved its convergence guarantees for both convex and nonconvex functions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tFederated learning frameworks need a careful design to handle the heterogeneity across clients. A local learning rate best for a client may be suboptimal for another. The authors successfully apply a similar technique proposed for centralized ML to the federated problem. Also, they prove the convergence.\n2.\tTheir method allows them to use a milder \u201cL-smooth\u201d assumption than the one commonly used in nonconvex optimization analyses. They use the maximum of \u201caverage local smoothness over local steps\u201d across clients and rounds."
            },
            "weaknesses": {
                "value": "1.\tWhen you presented the results based on the learning rate tuned on one model/dataset and used for all, the baseline methods were not as successful as yours. One natural question is how good those methods are if we tune for each model/dataset separately. Are all methods comparable? Or, how much the not optimum learning rates of baselines are different from the ones optimal for each dataset? \n2.\tThe bounded gradient assumption can be seen as a strong assumption, and it doesn\u2019t hold for many common functions.  Can those terms be bounded using the other two assumptions, bounded variance and dissimilarity?"
            },
            "questions": {
                "value": "1.\tIn (4), client $i$ seems to use exact gradients to choose the step size. Shouldn\u2019t it be $\\tilde{\\nabla}{f_i(x_t^i)}- \\tilde{\\nabla}{f_i(x_{t-1}^i)}$, or how does client $i$ know true gradient? I guess, it is just a notation thing because there doesn\u2019t seem any problem in Algorithm 1.\n2.\tI couldn\u2019t get the intuition behind setting the initial step $\\eta_{t,0}^i$ to an arbitrary $\\eta_0$ at the beginning of each local training after the first one (line 6 in Algorithm 1). Wouldn\u2019t it be good to use the final step size of each client's latest local training?\n3.\tWhat parameter(s) did you tune for $\\Delta-SGD$ except $\\gamma$?\n4.\t\u201cThis is a counter-intuitive behavior, as one would expect to get better accuracy by using a more powerful model.\u201d $\\rightarrow$ Here, mentioning Adagrad, Adam, and SPS is finetuned on ResNet18 can be useful. I guess, it should be one of the reasons. Also, it would highlight your method doesn\u2019t need any additional tuning.\n5.\tIn (B) and (C) of Figure 3, have the parameters been tuned on MNIST+CNN and CIFAR-10+ResNet-18, respectively?\n6.\tYou may enrich the study by reporting how the learning rate changes across time and across clients in $\\Delta-SGD$.  Also, you can compare this with the best learning rate of the other methods. \n7.\tThe two parts of local learning rate calculation (line 9 in Algorithm 1) are clear in how they are used in the theoretical analysis. It may be good to include an experimental report to see in how many iterations the learning rate is updated based on the first part $\\left(\\frac{\\gamma \\lVert x^i_{t, k} - x^i_{t, k-1} \\rVert}{2 \\lVert \\tilde{\\nabla} f_i(x^i_{t, k}) - \\tilde{\\nabla} f_i(x^i_{t, k-1}) \\rVert}\\right)$ or the second part $\\left(\\sqrt{1 + \\theta^i_{t,k-1}} \\eta^i_{t,k-1}\\right)$. It may show that the minimum of those two is useful in practice as well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6403/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6403/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6403/Reviewer_RQMs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6403/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698624779824,
        "cdate": 1698624779824,
        "tmdate": 1699636711296,
        "mdate": 1699636711296,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9mVZaRwWae",
        "forum": "g0mlwqs8pi",
        "replyto": "g0mlwqs8pi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6403/Reviewer_WWBM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6403/Reviewer_WWBM"
        ],
        "content": {
            "summary": {
                "value": "The authors provide a new learning rate schedule in federated learning scenarios. Each client can adjust their own learning rates by their own local gradients. The authors show the convergence analysis of the proposed algorithm in the non-convex case. The experiments show the benefit of the proposed algorithm."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors introduce a new learning rate schedule for each local client in the federated learning. Meanwhile, they show the convergence under mild assumptions in the nonconvex case.\n\n2. The experiments show that the proposed algorithm is more robust than other algorithms w.r.t. non-i.i.dness in federated learning."
            },
            "weaknesses": {
                "value": "The proof does not seem to be correct to me.\n\n***For the proof of Lemma 2:***\n\n(i)  $\\eta_t^{i}$ is used for updating from $x_t$ to $x_{t+1}$, but in the proof it seems like the $\\eta_t^{i}$ is used for updating from $x_{t-1}$ to $x_t$. If the latter case is necessary for the proof, how can we get $x_t$ and $\\nabla f(x_t)$ without knowing $\\eta_t^{i}$.\n       \n(ii) In the algorithm $\\eta_x^{i}$ is based on the gradient of minibatch estimation, how to change them into the true gradient.\n\n***For the proof in A.1:***\n  \nInequality from (12) to (13) does not seem to be correct. Because $\\eta_{t,k}^i$ depends on $\\tilde{\\nabla} f_i (x_{t,k}^t)$, we need to show that the expected value of $\\eta_{t,k}^i \\tilde{\\nabla} f_i (x_{t,k}^t)$. Since they are not independent, it is trivial to see that $E [\\eta_{t,k}^i \\tilde{\\nabla} f_i (x_{t,k}^t]) = E [\\eta_{t,k}^i E[\\tilde{\\nabla} f_i (x_{t,k}^t)]]$"
            },
            "questions": {
                "value": "Can you provide some detailed verification of the proof in the appendix?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6403/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6403/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6403/Reviewer_WWBM"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6403/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698654051706,
        "cdate": 1698654051706,
        "tmdate": 1700712983673,
        "mdate": 1700712983673,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Heb4ZUeHKx",
        "forum": "g0mlwqs8pi",
        "replyto": "g0mlwqs8pi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6403/Reviewer_WDyG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6403/Reviewer_WDyG"
        ],
        "content": {
            "summary": {
                "value": "Federated learning tasks are sensitive to the learning rate selection in client level. The paper argues it may be beneficial if we could enable different learning rate or learning rate scheduler per client due to the highly heterogeneous nature of FL clients. The paper proposes an auto-tuned learning rate scheduler, that could enable learning rate adapting to each client. The paper theoretically show the convergence of the proposed approach and experimental results demonstrate the effectiveness of $\\Delta$-SGD."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Disclaimer: the reviewer is not very familiar with the hyperparameter auto-tuning in centralized computing or in distributed federated computing. Thus, I may not fairly assess the novelty of the technique proposed by this paper. \n\nThe paper is well written and easy to follow. I find the paper enjoyable to read. \n\nThe hyperparameter tuning in client level is tedious and existing literature typically by default set universal learning rate for each client. Therefore, the paper tackles an under-explored problem and proposes a simple and effective approach."
            },
            "weaknesses": {
                "value": "- Novelty and technical challenge: Client-level optimization is orthogonal to server-level optimization. Therefore, using auto-tuning, which has been studied extensively in the context of centralized computing, in client level is not a very challenging transferral, as we could directly use any auto-tuner directly in client level and check the performance of FL tasks. \n\nI am wondering whether the auto-tuner, used or partially inspired by any practice in centralized computing. And is there any unique challenge if we simply combine any centralized auto-tuner to FL clients?\n\n- Insufficient comparison with auto-tuner baseline: As I mentioned in the last point, it should not be very challenging to directly deploy auto-tuners to FL clients. However, there is limited comparison to this important baseline, i.e., some representative autotuners developed in centralized computing directly used in FL clients.\n\n- Non-standard Assumptions used in theory: bounded gradient is strong assumption, and a bit contradictory to the heterogeneous setting the paper is motivated from. But it may be understandable to use bounded gradient as many existing literature especially who study adaptive optimizers also use it. Strong growth of dissimilarity is less standard and few papers in FL use it. Not sure whether it has any realistic and intuitive correspondence in FL tasks."
            },
            "questions": {
                "value": "My main questions lie in the previous weaknesses section.\n\nAlso an optional question: though it may be true there is no convergence guarantee given to the varying step size across clients, there are various papers that give convergence guarantee to the scenario where clients can have different number of local iterations, which seems to be a bit related. Is there any connection or difference in proving these two scenarios?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6403/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699039537694,
        "cdate": 1699039537694,
        "tmdate": 1699636710637,
        "mdate": 1699636710637,
        "license": "CC BY 4.0",
        "version": 2
    }
]