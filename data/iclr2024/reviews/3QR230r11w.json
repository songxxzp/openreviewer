[
    {
        "id": "z5ogxCNYpH",
        "forum": "3QR230r11w",
        "replyto": "3QR230r11w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5600/Reviewer_5Rpa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5600/Reviewer_5Rpa"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an algorithm using Multi-Fidelity Active Learning with GFlowNets. This algorithm effectively finds varied, top-performing options in fields like science and engineering. The authors point out that while there's a surge in data generation in these fields, present machine learning techniques struggle with efficiently querying a detailed, unknown target function.\n\nTo combat this, their algorithm employs a multi-tiered system, merging both basic and detailed reviews of the target function. It incorporates GFlowNets, a generative model, to grasp a simpler depiction of the data for efficient querying. The authors highlight that their method outdoes RL-based models in terms of data use and adaptability.\n\nTo test their algorithm, the authors chose tasks related to discovering molecules, like searching for drugs and studying materials. The results are encouraging, with the algorithm spotting a range of top-performing options using fewer queries compared to other techniques. They've outlined the algorithm's steps in a section named Algorithm 1 and have gone into depth about it in Appendices A and B. Essential experiment details are covered in Section 4, including data portrayal and benchmark task measures. Further experimental particulars are found in Appendix C, ensuring clarity and easy replication. Additionally, they've made their algorithm's code publicly available."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. Novelty: The proposed algorithm for Multi-Fidelity Active Learning with GFlowNets is a novel approach that addresses the challenge of querying a high fidelity, black-box objective function in scientific and engineering applications. The use of GFlowNets, a generative flow-based model, to efficiently query the objective function is also a novel contribution.\n\n2. Evaluation: The authors evaluate the proposed algorithm on several molecular discovery tasks, including drug discovery and materials science. The evaluation shows promising results, with the algorithm discovering diverse, high-scoring candidates with fewer queries than other methods.\n\n3. Reproducibility: The authors provide a detailed procedure of the steps of the algorithm in Algorithm 1, and additional details about the algorithm in Appendices A and B. They provide the most relevant information about the experiments in Section 4, including a description of the data representation and the oracles for each of the benchmark tasks. The rest of the details about the experiments are provided in Appendix C for the sake of better clarity, transparency, and reproducibility. Finally, the authors include the original code of their algorithm and experiments, which has been developed as open source.\n\n4. Clarity: The paper is well-written and easy to understand, even for readers who are not experts in the field. The authors provide clear explanations of the concepts and methods used in the paper, and the figures and tables are well-designed and informative."
            },
            "weaknesses": {
                "value": "One potential weakness of this paper is that the evaluation is limited to molecular discovery tasks, and it is unclear how well the proposed algorithm would perform on other types of scientific and engineering applications. Additionally, while the authors provide a detailed procedure of the steps of the algorithm and additional details about the algorithm in Appendices A and B, some readers may find the paper to be too technical and difficult to follow. Finally, the authors do not provide a detailed discussion of the limitations of their approach or potential future directions for research."
            },
            "questions": {
                "value": "What is the main challenge in scientific discovery that current machine learning methods cannot efficiently tackle?\n\nHow does the proposed algorithm with GFlowNets address the challenge of querying a high fidelity, black-box objective function?\n\nWhat are the advantages of multi-fidelity active learning with GFlowNets compared to RL-based alternatives?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698709416846,
        "cdate": 1698709416846,
        "tmdate": 1699636577340,
        "mdate": 1699636577340,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8EqM9r0kSj",
        "forum": "3QR230r11w",
        "replyto": "3QR230r11w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5600/Reviewer_TJ42"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5600/Reviewer_TJ42"
        ],
        "content": {
            "summary": {
                "value": "This paper designed an Active Learning algorithm to address the challenges of \"needle-in-a-haystack\" problems in scientific discovery, where the goal is to discover multiple, diverse candidates with high values of the target function, rather than just finding the optimum. The proposed method was evaluated on multiple tasks like DNA and Antimicrobial tasks, and molecular tasks. The experimental results were shown to outperform its single-fidelity counterpart while maintaining diversity, demonstrating its effectiveness in dealing with high-dimensional scientific data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This work is a good combination of active learning and GFlowNets.\n\n- The experimental results demonstrate the effectiveness of the proposed model in dealing with high-dimensional scientific data.\n\n- Instead of merely concerning the model performance (e.g., accuracy), this work focuses more on selecting more diverse samples with high values of the target function."
            },
            "weaknesses": {
                "value": "- The baselines are quite simple, just compare between multi-fidelity active learning and single-fidelity active learning. The author could consider comparing it with other typical query synthesis active learning methods like [r1].\n\n- The performance of the proposed method on tasks and domains beyond those covered in this study remains uncertain due to the limitations of the tested benchmarking datasets. \n\n\n[r1] Schumann R, Rehbein I. Active learning via membership query synthesis for semi-supervised sentence classification[C]//Proceedings of the 23rd conference on computational natural language learning (CoNLL). 2019: 472-481."
            },
            "questions": {
                "value": "Would the data samples selected be out-of-distribution samples? Since the evaluation considers mean top-K score and top-K diversity, it is still possible to select out-of-distributions samples."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698735185800,
        "cdate": 1698735185800,
        "tmdate": 1699636577235,
        "mdate": 1699636577235,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5QD7BbGJV7",
        "forum": "3QR230r11w",
        "replyto": "3QR230r11w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5600/Reviewer_Cu6t"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5600/Reviewer_Cu6t"
        ],
        "content": {
            "summary": {
                "value": "The authors propose to extend the GFlowNet-AL paper, ICML 2022 paper by M. Jain et al., to the multi-fidelity setting. While the extension seems straightforward, and the general novelty contribution of the paper is limited, the paper falls short in demonstrating the effectiveness of the multi-fidelity setting on real-world applications. The authors somehow convert the existing experiments of GFlowNet-AL to a multi-fidelity setting with some synthetic simulations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Active learning setting is an important problem and using GFN shows to be effective in this setting."
            },
            "weaknesses": {
                "value": "There are a couple of weaknesses with this paper:\n\n- The novelty of this paper is very limited in terms of the model development. To cover this limitation, the paper needs to be more robust in terms of its application. However, the experiments do not support this, as they are primarily simulation-based.\n\n- The paper is not well-written in general. 1) The plots in the experiments are densely packed, making them difficult to understand. 2) While the paper includes a lot of basic information about the \"importance of new scientific discovery\" in the introduction, which is not directly relevant, it lacks a proper description of the multi-fidelity problem. Additionally, GFlowNet and Active Learning in the method section are not closely related, and it would be better to refer to GFlowNet-AL as a preliminary section. Doing so may clarify the paper's novelty.\n\n- The tasks used in the experiments are not based on real-world scenarios, which raises questions about the problem's practical importance. In general, I'm not familiar with multi-fidelity, and I didn't find the paper very clear in this respect, neither in the method nor in the experiments.\n\n- The sequences are very short, leading to questions about the method's applicability for longer sequences.\n\n- The method can also be viewed as an ensemble modeling approach, but it's not clear what the main advantage is. In the end, it seems that there is a single, expensive objective, which is the case in most real-world scenarios. And when we approximate them with multiple oracle, why they might be hard to get query from all of them?"
            },
            "questions": {
                "value": "Could you please elaborate on the main challenge of this model and how you addressed it?\n\nCould you please provide more details about the statement, \"cheap online simulations take a few minutes\"? What exactly are considered as \"cheap simulations\" in the context of sequence design?\n\nI would appreciate seeing the benefits of your approach on larger sequences and molecules (e.g. antibodies).\n\nI'd like to see at least one real-world application where you have multiple fidelity levels with varying costs.\n\nIt's not clear to me what the term \"cost\" is referring to. Is it related to validation experiments or the process of querying the black box?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698862883654,
        "cdate": 1698862883654,
        "tmdate": 1699636577128,
        "mdate": 1699636577128,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xpRn3KauH2",
        "forum": "3QR230r11w",
        "replyto": "3QR230r11w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5600/Reviewer_EtrP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5600/Reviewer_EtrP"
        ],
        "content": {
            "summary": {
                "value": "This paper describes a multi-fidelity optimization algorithm which uses GFlowNets to optimize the acquisition function of a multi-fidelity deep-kernel Gaussian process. Experimentally the proposed algorithm seems to outperform a number of GFlowNet baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "I don't think my analysis of this paper fits nicely into the strengths/weaknesses format requested in the review form, so I will first give general feedback and then describe strengths and weaknesses.\n\nI think the paper proposes an interesting method, but in generally seems a bit... distorted. In my opinion it gives too much detail on unimportant bits (context of drug discovery) and not enough detail on the important bits (e.g. the actual method). The main emphasis is on GFlowNets, which actually don't seem to be the most important part of the method: I think a more appropriate title for the paper would be \"multi-fidelity Bayesian optimization with deep kernel Gaussian processes\". To me, the most important question for the paper to answer is \"does the method work, and if so why.\" I didn't feel like the paper came even close to answering this question.\n\nStrengths of the paper:\n- The writing was clear (at least what the authors chose to write about was clear, although I think they chose to write about the wrong things in the paper).\n- The proposed method is sensible and arguably has some novelty (although the exact degree of novelty was unclear)\n- Experiments were fairly comprehensive (at least for the questions they investigated, which in my opinion were the wrong questions)"
            },
            "weaknesses": {
                "value": "- _Related work_: the paper touches on a bunch of topics which are already very well-researched: multi-fidelity optimization (e.g. [1-3]), GFlowNets [4-7], and diverse optimization (e.g. [8-10]). I did not think the paper contextualized its contributions well.\n  - I think BO was dismissed too hastily by claiming that BO only cares about finding a single optimum. First, this is not true: see [9,10]. Second, diverse solutions may be found _incidentally_ by when performing single-objective BO via its exploration, so it is inappropriate to dismiss BO even if diversity is not built into the objective. In fact, the method proposed in this paper also appears to have no built-in diversity objective: it just relies on GFlowNets incidentally generating a diverse set of points.\n  - Active search and quality-diversity optimization are mentioned in section 2, but never again. Does the proposed method have any advantages over these approaches? Would these not be sensible baselines for experimental comparison.\n  - The authors mention the existence of a lot of prior literature on multi-fidelity optimization, but dismiss it by saying \"the literature is still scarce probably because most approaches cannot tackle the specifics of scientific discovery, such as the need for diverse samples.\" This is pure speculation, and I highly doubt that it is true. One cannot conclude that existing methods are insufficient just because you don't see many papers on them!\n  - In the past 2 years there has been a huge flood of papers about GFlowNets. The relationship between this paper and all the other papers is not clearly stated. [4,6] seem particularly relevant. The authors should clearly state the novelty (if any) from the existing GFlowNet literature.\n- _Surrogate model/acquisition functions_: the main emphasis of the paper is on the GFlowNets, but the method also critically relies on a GP surrogate model and an acquisition function. In my experience, these choices are also incredibly important important, but they are not really explored or discussed much in this work. For example, I am aware that the deep kernel GPs used by the authors as a surrogate model are very prone to overfitting [11]. Is this an issue? The training of the surrogate model is not really discussed in the text when presumably it is very important!\n- _Flawed metrics_: the authors follow previous works and examine the scores and diversity of the top K outputs. I think this is a flawed metric which doesn't reflect how these models will be used in practice, which is to propose a set of candidate points that will be taken forward to the next stage in screening. This implies _extracting_ a diverse subset from all outputs, not simply looking only at the top K outputs and seeing how diverse they are. I recommend instead that the authors look at a monotonic diversity metric, such as #circles (Xie et al 2022).\n- _Experiments mainly compare against weak baselines_: the experiments contrast the authors' method with some baseline methods, which in my opinion are fairly weak (few variations of random search and PPO, which is probably not sample efficient). Critically, the authors don't compare against any other sort of BO method. I think the experiments section should really be trying to answer whether this setup has any advantage over a reasonable other BO-like setup (e.g. multi-fidelity GPs with [domain-specific] standard kernels and basic multi-fidelity acquisition functions like expected improvement / cost).\n- _Origin of diversity unclear_: another question which is not addressed theoretically or experimentally is why this method produces more diverse outputs (or whether it even does). Is it the surrogate model? Is it the GFlowNets? This seems like the key claimed advantage of the method, so it feels odd to me that it is not investigated more.\n\n[1] A General Framework for Multi-fidelity Bayesian Optimization with Gaussian Processes\n\n[2] Multi-Fidelity Bayesian Optimization via Deep Neural Networks\n\n[3] Review of multi-fidelity models\n\n[4] Multi-Objective GFlowNets\n\n[5] Gflownet foundations\n\n[6] Biological sequence design with gflownets\n\n[7] GFlowNets for AI-driven scientific discovery\n\n[8] Quality-diversity optimization: a novel branch of stochastic optimization\n\n[9] Discovering Many Diverse Solutions with Bayesian Optimization\n\n[10] Bayesian algorithm execution: Estimating computable properties of black-box functions using mutual information\n\n[11] The promises and pitfalls of deep kernel learning"
            },
            "questions": {
                "value": "Some specific questions are:\n\n- How does this differ from previous work on GFlowNets?\n- How does this method differ from a BO method which one could create by taking a model/acquisition model directly from existing papers?\n- What are the results of a GP baseline using Matern kernel (toy tasks), string kernel (DNA task), and Tanimoto kernel (models) with EI/cost acquisition function and GFlowNets as the acquisition function optimizer?\n\nAlso, I have some writing suggestions if you revise the paper:\n- The abstract contains ~3 sentences of introduction. I would cut this to ~1 sentence. It's good to keep the abstract short.\n- The introduction contains a lot of description of black box optimization in general. While I think this is good, 9 pages is fairly short, and I think the paper does not even have enough space to properly describe the method at the moment. I would cut this down to ~1 paragraph, reference some works which discuss the problem in more detail, and try to keep the introduction to < 1 page.\n- I would put the related work _after_ the method. I recommend this slide deck by Simon Peyton Jones (a creator of Haskell) which explains why: https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698953318124,
        "cdate": 1698953318124,
        "tmdate": 1699636577023,
        "mdate": 1699636577023,
        "license": "CC BY 4.0",
        "version": 2
    }
]