[
    {
        "id": "BwF0wV7DPL",
        "forum": "O0FOVYV4yo",
        "replyto": "O0FOVYV4yo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4255/Reviewer_7VmP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4255/Reviewer_7VmP"
        ],
        "content": {
            "summary": {
                "value": "Well-structured and well-presented. However, I suspect that the result can be incremental: there have been a lot of results applying the PL inequality to get convergence of neural networks, e.g. [Nguyen & Mondelli, 2020] (which is not cited). Also, they do not really discuss the requirement \\alpha_1 > 0 much. It does not seem like a purely technical requirement. I suspect it might fail in reality, at least, sometimes.\n\nThe paper considers a general optimization problem for a two-layered linear network and aims to prove that GD converges to a minimum with a linear rate under some constraints on learning rate. Curiously, the learning rate can even increase throughout training.\n\nThe paper starts with a review of the classical linear convergence analysis for linear models by Polyak. This analysis stems on two ingredients: 1) Descent lemma, and 2) PL-inequality. However, neither PL-inequality, nor smoothness inequality which the Descent lemma is based on, cannot hold globally for a multi-layered linear model. The paper presents generalizations of both results with \"local\" smoothness constants. The local smoothness constants allow for bounds which are sufficient to derive linear convergence of GD under some (time-dependent) learning rate constraints.\n\nThe paper is very well-written. The first diagram is weird (and not really the way to do things... e.g. putting NTK in the 'finite step size' category is weird). \n\nThe paper contains no experimental validation for the main linear convergence result.\n\nQuestions: (1) Th.3.2 requires \\alpha_1 > 0; what is the probability for this requirement to fail for the standard [Glorot & Bengio, 2010] initialization? (2) How could we estimate \\eta_\\max? How small could it be? What does it depend on?"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Interesting theoretical result, useful, sound. Good discussion."
            },
            "weaknesses": {
                "value": "Literature review misses some references of closely-related (and even possibly overlapping works), e.g [Nguyen-Mondelli, 2020], [Radhakrishnan-Belkin-Uhler, 2021]. One really needs to clarify the novelty compared to existing results."
            },
            "questions": {
                "value": "(1) Th.3.2 requires \\alpha_1 > 0; what is the probability for this requirement to fail for the standard [Glorot & Bengio, 2010] initialization? \n\n(2) How could we estimate \\eta_\\max? How small could it be? What does it depend on?\n\n(3) Novelty wrt existing literature."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4255/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4255/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4255/Reviewer_7VmP"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4255/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698333095964,
        "cdate": 1698333095964,
        "tmdate": 1699636392584,
        "mdate": 1699636392584,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "H9u48jk1PD",
        "forum": "O0FOVYV4yo",
        "replyto": "O0FOVYV4yo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4255/Reviewer_m2Ce"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4255/Reviewer_m2Ce"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the convergence rate of gradient descent for overparametrized two layer linear neural networks with generic loss.\nIt does so without assumptions previously used in the literature, be it on infinitesimal stepsize, infinite width, etc.\nInstead, the analysis is based on local versions of the Polyak-Lojasiewicz inequality and of the descent lemma, where the global constants in both inequalities are replaced by iterate dependent versions (eq 10).\nThe analysis up to Eq 14 is straightforward, and most of the work consists in showing that there exists a choice of stepsize $\\eta_t$ that can ensure $0 < (1 - 2 \\mu_t \\eta_t + \\mu_t K_t \\eta_t^2) \\leq \\rho < 1$ and $\\eta_t K_t < 2$ simultaneously."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Apart from the work of the previous of Xu et al (2023), the paper is the first to study the setting of finite stepsize, finite width and \"general\" init (still requiring imbalance)"
            },
            "weaknesses": {
                "value": "- There is **very limited novelty** with respect to Xu et al 2023, \"Linear Convergence of Gradient Descent For Finite Width Over-parametrized Linear Networks With General Initialization\". If the authors could point at the novelty in the proofs, it'd be more convincing, because they seemed extremely similar and this felt thin-sliced.\n- There is still a dependency on initialization through the assumption on $\\alpha_1$, which excludes some initializations."
            },
            "questions": {
                "value": "Can the authors detail the novelty in the proof compared to previous Xu work?\n\n\n\n\nMinor comments:\n## References\nA work which \"revived\" the interest in PL form the Optimization community is \"Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-\u0141ojasiewicz Condition\", Karimi 2016, which the authors could cite.\n\n\n## Cosmetics:\n- the way the authors cite the Descent Lemma is broken: \" where Descent lemma is\" should be \"where the Descent Lemma is\", same for \"to derive Descent lemma,\" etc\n- \"for arbitrary non-convex functions that is\": for *an* arbitrary non-convex *function* that is (singular)\n- \"satisfies \u03bc-PL condition.\": missing \"the\"\n- \"via chain rule:\": missing \"the\"\n- P6 \"In \u00a72.1, we show that as\": we showed\n- \"if the $\\lim_{t \\to \\infty}\": extra \"the\", this time.\n- \"too larger\" is incorrect; this whole paragraph has other typos (\"but not too much $\\eta_t \\leq 1/K_t$)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4255/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698774155889,
        "cdate": 1698774155889,
        "tmdate": 1699636392495,
        "mdate": 1699636392495,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Rj9XJtbrLU",
        "forum": "O0FOVYV4yo",
        "replyto": "O0FOVYV4yo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4255/Reviewer_KJ6o"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4255/Reviewer_KJ6o"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors tackle the challenge of analyzing the convergence of gradient descent (GD) for two-layer linear neural networks with general loss functions, relaxing previous assumptions about step size, width, and initialization. They introduce a new approach based on the Polyak-\u0141ojasiewicz (PL) condition and Descent Lemma, demonstrating that these conditions hold locally with constants depending on the network's weights. By bounding these local constants related to initialization, current loss, and non-overparameterized model properties, the paper establishes a linear convergence rate for GD. Importantly, the study not only enhances previous results but also suggests an optimized step size choice, validated through numerical experiments. The authors further prove that local PL and smoothness constants can be uniformly bounded by specific properties of the non-overparameterized models. The paper concludes by proposing an adaptive step size scheme, accelerating convergence compared to a constant step size, and empirically validating the derived convergence rate's accuracy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "$\\textbf{(1) Rigorous analysis of convergence conditions}$: A key strength of this paper is its rigorous analysis of convergence conditions for two-layer linear neural networks. The authors thoroughly explore the convergence behavior of gradient descent under various circumstances, relaxing previous assumptions and providing a detailed understanding of the impact of factors such as step size, width, and initialization. By establishing convergence conditions and deriving a linear convergence rate, the paper significantly advances the theoretical understanding of optimization processes in neural networks.\n\n$\\textbf{(2) Adaptive step size scheme}$: The paper proposes an adaptive step size scheme based on the derived convergence analysis. Introducing this adaptive approach showcases the practical implications of the research findings. By suggesting a dynamic step size strategy that accelerates convergence compared to a constant step size, the paper offers a concrete and actionable method for improving optimization efficiency in neural networks. This innovation enhances the applicability of the research, providing a valuable contribution to the field of optimization techniques for machine learning models."
            },
            "weaknesses": {
                "value": "$\\textbf{(1) Incremental contribution}$:  Arora et al. in Ref [1] studied linear convergence of gradient descent for multi-layer neural networks. While Arora et al. assumed balanced weights and a deficiency margin, these conditions were proven by them in the context of gradient descent. In this work, although the authors only focus on general loss, they just study two-layer linear networks. Moreover, their convergence rate also depends on margin and imbalance. The contribution of this work is very incremental in terms of Ref [1]. \n\n[1] Arora et al., A convergence analysis of gradient descent for deep linear neural networks. \n\n$\\textbf{(2) Limited generalizability to deep linear networks}$: It seems that the authors don't mention how to generalize their results to deep linear networks. It is believed that deep networks are more commonly used in applications. The paper leaves a significant gap in its discussion by omitting details on the generalization of their findings to deep linear networks."
            },
            "questions": {
                "value": "$\\textbf{Q1.}$ Is it possible to generalize the current analysis to deep linear networks or deep nonlinear networks? T\n\n$\\textbf{Q2.}$  In Theorem 3.2, it is assumed that $\\alpha_1 > 0$. Can the authors verify this condition?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4255/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4255/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4255/Reviewer_KJ6o"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4255/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699104663242,
        "cdate": 1699104663242,
        "tmdate": 1699636392429,
        "mdate": 1699636392429,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RVaCCZPlq2",
        "forum": "O0FOVYV4yo",
        "replyto": "O0FOVYV4yo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4255/Reviewer_PUum"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4255/Reviewer_PUum"
        ],
        "content": {
            "summary": {
                "value": "The paper studies convergence of gradient descent for matrix factorization (also called over-parametrized linear models). Prior work[1] established linear convergence for the quadratic loss by introducing two constants ($c_1, c_2$) to bound changes in singular values along trajectory of GD. This work writes their result for stronlgy convex and smooth losses and tracks those changes better which allows for an easier computation of adaptive stepsizes.\n\n---\n[1]Xu, Ziqing, et al. \"Linear Convergence of Gradient Descent for Finite Width Over-parametrized Linear Networks with General Initialization.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This work cleans up and offers an improved analysis of a prior result.\n- It is clearly written."
            },
            "weaknesses": {
                "value": "- The prior work this works improves on already has entire sections on adaptive step sizes: It is in a small sentence on page 8 we discover that [1] already proposes adaptive stepsizes when throughout it is presented as only having fixed stepsize schemes. The presentation of the prior work needs to include this fact.\n- Significance: This result is carefully analyzes matrix factorization, after papers before have proved linear convergence of GD. Once the linear convergence question has been answered, can the authors justify why it is still significant to study matrix factorization ? The original reason for studying this simplified setting was to prove that non-convexity can be benign. This question was already answered. So the authors should provide more arguments as to why it would still be interesting to derive adaptive stepsizes to improve an already linear rate."
            },
            "questions": {
                "value": "- Would the authors agree to say that the central contribution of this work that differentiates it from [1] is lemma 3.1 ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4255/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4255/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4255/Reviewer_PUum"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4255/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699234477968,
        "cdate": 1699234477968,
        "tmdate": 1699636392329,
        "mdate": 1699636392329,
        "license": "CC BY 4.0",
        "version": 2
    }
]