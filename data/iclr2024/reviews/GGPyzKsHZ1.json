[
    {
        "id": "ncvcpjlAOg",
        "forum": "GGPyzKsHZ1",
        "replyto": "GGPyzKsHZ1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission717/Reviewer_4AjC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission717/Reviewer_4AjC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes two key challenges in learning audio-video data with continuously changing semantic categories: sparse spatiotemporal correlation and representational forgetting of audio-video relationships. This paper further proposes a framework for lifelong learning in audio-visual scenes, named FLAVA, it contains two important components: (1) A lightweight trainable audio-video matching (AVM) module, which performs cross-modal attention operation to obtain cross-modal similarity. (2) A rank-based forget-robust patches selection module. Experiments on multiple audio-visual datasets demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ Lifelong learning in audio-visual scenes is a very meaningful research topic. \n+ The proposed two challenges (sparse spatiotemporal correlation and representational forgetting of audio-video relationships) are interesting and they can bring some insights to our community.\n+ Extensive experiments show the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "- The paper does not introduce the proposed method very clearly, the writing of the paper should be polished. \n- The paper claims that \"this is the first work that addresses a continual representation learning problem for audio-video tasks and identifies the crucial challenges that the new problem has.\" Hence, previous works about audio-visual continual learning (e.g., audiovisual continuous speech recognition,  audiovisual continuous emotion recognition) should be introduced in the related work part. The differences between this paper and previous works about multimodal continual learning should also be further refined.\n- Experiments in section 3.2 are not convincing enough, unpaired data naturally creates misleading cross-modal attention maps. These experiments do not fully explain the reason for representational forgetting. \n- The paper says: \"In order to assess the relative importance of current data and past data, we further compute past-data-induced attention maps.\" However, in continual learning, past data is usually unavailable, so how to compute past-data-induced attention maps? This is a very important question and it should be explained in detail."
            },
            "questions": {
                "value": "Pls see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission717/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806966916,
        "cdate": 1698806966916,
        "tmdate": 1699635999220,
        "mdate": 1699635999220,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Nc0obnaswC",
        "forum": "GGPyzKsHZ1",
        "replyto": "GGPyzKsHZ1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission717/Reviewer_YJJY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission717/Reviewer_YJJY"
        ],
        "content": {
            "summary": {
                "value": "This work tackles the continual learning of audio-video representations with attention-based localized alignment and forget-robust multimodal patch selection strategies. Experiments on VGG-sound and AudioSet show its effectiveness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The general structure is clear. The method is simple in general. It\u2019s easy to follow. The performance of both the two proposed modules is obvious. This work also provides a thorough analysis of both the method and experiment."
            },
            "weaknesses": {
                "value": "I haven't specifically focused my research on lifelong learning, but from a methodological perspective, this work primarily involves the direct utilization of basic attention mechanisms. The measurement of relative importance is determined by the levels of attention results, which is a relatively common approach."
            },
            "questions": {
                "value": "(1) The best performance on VGGSound should be at least 66.+% now, which is much higher than 58.65% here. What\u2019s the main reason of this gap? Due to that there are more supervised labels in those works? If so, what will the performance be like of this work if we also provide labels? Will it also be improved by a large margin to close to 70%?\n(2) Beyond the proposed specific trainable module, is it possible to introduce this proposed manner into the learning of large-scale audio-visual models? What would be the results like of using this strategy to train large-scale models? Will is lose its effect when coming to large-scale data and models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission717/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699197357719,
        "cdate": 1699197357719,
        "tmdate": 1699635999104,
        "mdate": 1699635999104,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "S90ovTyJTm",
        "forum": "GGPyzKsHZ1",
        "replyto": "GGPyzKsHZ1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission717/Reviewer_hYwV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission717/Reviewer_hYwV"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a lifelong audio-visual masked autoencoder model: FLAVA.  It can continually learn multimodal representations from a video stream containing audio-video pairs, even while the distribution of the data continually shifts over time. FLAVA addresses the challenges of continual audio-video representation learning by proposing two novel ideas: Localized Alignment and Forget-robust multimodal patch selection. FLAVA outperforms the state-of-the-art continual learning methods on several benchmark datasets in continual audio-video representation learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ Self-supervised audio-visual continual learning is an important topic in multimodal learning, and this work addresses the issue of forgetting in such scenarios. \n\n + The authors clearly motivate the need for their work and provide vivid examples of the audio-visual alignment of forgetting issues. \n\n+ The proposed method outperforms compared continual learning approaches on several benchmark datasets."
            },
            "weaknesses": {
                "value": "+ The paper writing can be further improved. Sections 4.2 and 4.3 are difficult to follow. Please clarify the following:\n(1) What do you mean by \"relative importance of current data and past data\"?\n(2) How is past data used in Section 4.2?\n(3) How are past discriminative queries selected?\n(4) With increasing continual learning steps, how can past data from previous steps be better leveraged to improve memory usage?\nWhy can the proposed method tackle the issues mentioned in Figures 1, 2, and 3?\n\n+ I saw the authors use a fixed task order for continual pre-training. I wonder whether the order matters. \n\n+ Two concurrent related works [1, 2] have addressed audio-visual continual learning, the second of which also observed and addressed audio-visual alignment forgetting issues. The authors can discuss the relevance and differences among these works in more detail, especially the differences between the proposed method and the second work. Although it is clear that the works are concurrent, more discussions would be helpful to distinguish between the different works.\n\n+ On the audiovisual classification task, why are the improvements of the proposed method marginal?\n\n\n[1] Mo, Shentong, Weiguo Pian, and Yapeng Tian. \"Class-incremental grouping network for continual audio-visual learning.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[2] Pian, Weiguo, et al. \"Audio-visual class-incremental learning.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission717/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699347591994,
        "cdate": 1699347591994,
        "tmdate": 1699635999022,
        "mdate": 1699635999022,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FArjy0HDeJ",
        "forum": "GGPyzKsHZ1",
        "replyto": "GGPyzKsHZ1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission717/Reviewer_Q9gf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission717/Reviewer_Q9gf"
        ],
        "content": {
            "summary": {
                "value": "The motivation of this paper is clear and meaningful. The authors bring up two critical challenges for continuous audio-visual learning: (1) sparse spatiotemporal correlation between the video-audio pairs, and 2) representational forgetting of audio-video relationships. To demonstrate these two challenges, the authors give a visualization of cross-attention maps, which can illustrate that the traditional model will forget the correct relation between these two modalities. The authors also propose a novel model named Forget-robust Localized Audio-Video Alignments to alleviate these two challenges."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The motivation of this paper is clear and meaningful."
            },
            "weaknesses": {
                "value": "The paper claims that they can achieve better audio-visual lifelong alignment. However, the authors choose retrieval and classification as their downstream tasks, which can not effectively demonstrate the superior of the propose model. Retrieval and classification tasks only require global connection between audio and visual features, to further illustrate the effectiveness of the model, the authors should choose more convincing tasks, such as audio-visual event localization, audio-visual parsing, audio-visual segmentations, all these tasks have corresponding datasets. It will be better if the proposed model can achieve promising results on these downstream tasks.\nThe ablation studies in experiments are nor sufficient. The authors should analyze more about each components in the propose method."
            },
            "questions": {
                "value": "In figure 3, the authors claim that they use similar audio in (c). What are the criteria for selecting similar audio, and how to ensure that there will be no other interfering noise between the selected audio and the original audio."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission717/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699542673961,
        "cdate": 1699542673961,
        "tmdate": 1699635998880,
        "mdate": 1699635998880,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eq5K8IEyGg",
        "forum": "GGPyzKsHZ1",
        "replyto": "GGPyzKsHZ1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission717/Reviewer_nu6v"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission717/Reviewer_nu6v"
        ],
        "content": {
            "summary": {
                "value": "This work proposes audio-visual continual learning with self-supervised learning, building off of CAV-MAE. Compared with the recent AV-CIL work that proposes supervised audio-visual continual learning, this method doesn't require labels during the pre-training stage. The authors show that the standard audio-visual model is prone to \"forgetting\" when fine-tuned on a new task, and their approach mitigates this problem.\n\nOverall, I think this is interesting work, but I have concerns about the motivation and experiments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This work extends CAV-MAE with an AVM (audio-visual matching module) which shows good qualitative cross-modal localization abilities (ie. the ability to localize visual sound sources). CAV-MAE could not achieve this capability out-of-the-box (see CAV-MAE Appendix I which shows poor sound source localization results).\n- Implementation details and analysis of the model are provided."
            },
            "weaknesses": {
                "value": "- I don't understand the motivation of audio-visual continual learning. I think large scale audio-video pre-training data is enough to learn generic representations for different categories of sounds. It seems like an unrealistic constraint to train the model on one category of video at a time (ie. music, sports, etc...), when we could just pool together data from all of the categories and learn a more general representation from the start (especially because the model doesn't require class labels and it's easy to get unlabeled videos). Moreover, the performance of the proposed method is worse than the \"multi-task\" result where the model simultaneously trains on the data from all of the different tasks. Also, the retrieval performance is much worse than reported in CAV-MAE even with a smaller / easier retrieval set size used in this work, although the present work does not explain why.\n- The proposed method assumes access to audio-visual data (ie. memory) from the previous tasks, which doesn't seem like a realistic scenario. I think a more realistic scenario is to have a black-box model without access to the pre-training data, and then the model is presented with a new task / training data. Otherwise, if we have access to the data from the previous tasks, why not just combine the data from the new task and the old tasks to train the model? I didn't see a comparison with this kind of approach. Besides, in Figure 9, the proposed method improves with a larger memory size (ie. more access to data from previous tasks), which further shows the benefit of training on combined data from different tasks.\n- As someone familiar with audio-visual learning but not continual learning, I did not find the explanation of the other methods in the main results adequate. The difference with the compared methods should be explained more.\n- The writing / explanation of the method is not clear enough. I didn't understand Sections 4.2 and Sections 4.3; the writing should be improved with more high-level explanation. It would be helpful if each line in equation (4) was explained separately. Algorithm 1 was helpful for a high-level understanding, perhaps more detail could be added there.\n- The proposed method is only significantly better than the baseline methods for continual learning (ie. LUMP) on the VGGSound retrieval task. For the AudioSet retrieval task and VGGSound / AudioSet classification tasks, the improvement is small. \n- The experiments leave questions unanswered (see my questions below).\n- There are some distracting typos (\"vidoe,\" \"Fintune\")"
            },
            "questions": {
                "value": "- The pre-training / fine-tuning / task / evaluation splits of the datasets should be more clear. Can you provide a table in the appendix with the precise number of clips for each split? Specifically, I am wondering how the training and evaluation data differs with CAV-MAE. \n- Why are the tasks set up to be \"zero-shot\" by excluding classes from all of the continual learning datasets? I don't understand how this measures \"forgetting\" since the model isn't being tested on classes that it was trained on. It would make more sense to me to have an evaluation set per continual learning subset and test the model on evaluation sets corresponding to tasks it has already seen (and average the results). \n- What is the difference between your method and the baseline methods in terms of the design?\n- Is it possible to compare with AV-CIL on the classification task, since that requires supervised fine-tuning?\n- Why is the Multitask retrieval result worse than CAV-MAE? The classification results on AudioSet and VGGSound are much higher than in CAV-MAE, is it due to a different evaluation set?\n- Why is the average forgetting much smaller for the classification task compared to the retrieval?\n- It would be nice to see a table breaking down the performance of each task at each stage in the continual learning process, similar to Figure 10b but for all tasks (ie. stage on the rows, task on the columns, and the upper diagonal should be filled). I'd like to see this for the fine-tuning method and the proposed method to understand how soon the simple fine-tuning strategy deteriorates.\n- How does the order of the tasks change the performance? Have you tried other orders?\n\nMisc. questions\n- For the AVM objective, the binary cross entropy loss requires less memory than the contrastive loss. Does the batch size and number of positive / negative pairs impact the performance? Have you tried training with the contrastive loss?\n- What is \"ER\" in Figure 10?\n- How is modality gap estimated?\n- What are the model sizes and number of parameters?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission717/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699553860762,
        "cdate": 1699553860762,
        "tmdate": 1699635998823,
        "mdate": 1699635998823,
        "license": "CC BY 4.0",
        "version": 2
    }
]