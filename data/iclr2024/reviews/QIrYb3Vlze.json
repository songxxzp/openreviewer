[
    {
        "id": "efKtQbxkl8",
        "forum": "QIrYb3Vlze",
        "replyto": "QIrYb3Vlze",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1075/Reviewer_foMH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1075/Reviewer_foMH"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to learn a representation for diffusion models, which is approximately isometric to the representation associated with the score function. The latter was found to be useful by Kwon et al (2023). The main benefit of the proposed approach is that it drastically simplifies the way in which the latent representation can be operationalised."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper approaches an important problem: finding a way to leverage diffusion models for representation learning.\n* The approach is quite pragmatic enabling its use in reasonably large models.\n* Experiments demonstrate feasibility.\n* The mathematical explanation is simple and meaningful (which is a very good thing)."
            },
            "weaknesses": {
                "value": "* Several key experiments are highly subjective. For example, the opening example of Fig 1 claims that the proposed approach is better than spherical interpolation. In my subjective view, the opposite seems true. Which method actually is better is not obvious to me.\n* I very much miss an early definition of 'isometry'. This is defined rather late in the paper, yet the term is used quite a bit in the first few pages to argue that the proposed approach is sensible.\n* The paper several times claims that by being isometric all sorts of benefits are gained, e.g. the representation becomes disentangled and reflects semantics perceived by humans. Given that we are not talking about being isometric to a human perception space, then I struggle with the mathematical validity of such statements. I really wish the authors would tone down such claims, as the resulting paper would be much more convincing.\n* A superficial reading of the paper signals that the resulting representation is an isometry. However, as far as I can tell, it is only regularized towards being an isometry. I do not object to this, but I wish the communication was more clear.\n* Table 1 claims that the obtained FID scores are 'comparable' to competing methods, but, as far as I can tell, the numbers are in fact significantly worse.\n* Many conclusions of the paper rely on visual inspection of interpolation plots. I honestly cannot tell which is better (e.g. I think DDPM looks better than the proposed method in Fig 8, but how am I supposed to determine which actually is better).\n* There is rich literature on equipping latent representations with Riemannian geometries, see e.g. \"Latent Space Oddity\" by Arvanitidis et al. I miss a discussion of this in the related work as much of the tooling is the same.\n\nAll in all, while I raise many issues, they are predominantly associated with the somewhat 'overclaiming' nature of the paper writing. I do think that the proposed work has merit and is useful. Assuming the others are willing to tone down the written claims a notch, then I do support the paper."
            },
            "questions": {
                "value": "* Regarding Fig 1 it visually seems like the starting and end points are different in the three rows. Are they? If so, how do I compare?\n* The 'Isometry Loss' section on page 5 proposes a specific measure. How is this different from previous isometric regularizers, e.g. the one from Lee et al.? I ask because this section reads as if it is entirely novel, whereas I suspect it might be quite similar to existing approaches for autoencoder-like models.\n* The approximation in Eq 7 seems sensible. Did you test the quality of this? I would love to see some experiments showing how much is lost through this efficient approximation.\n* I do not understand the illustration in Fig 4. As far as I can tell, panel d provides a highly distorted representation as the sphere is torn apart to fit the Euclidean topology (i.e. there's a massive topology mismatch between the sphere and the plane). What am I missing?\n* In sec 4.4 what does \"$p < 0.5$\" refer to? What is $p$?\n* Usually representation learning goes hand in hand with dimensionality reduction, which is not the case here. I would expect that there are fewer 'interesting' representation directions than there are image pixels (do you agree?). Does this then imply that most directions in your representation space correspond to 'weird' image changes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1075/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698691639557,
        "cdate": 1698691639557,
        "tmdate": 1699636034225,
        "mdate": 1699636034225,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZGsGvS582q",
        "forum": "QIrYb3Vlze",
        "replyto": "QIrYb3Vlze",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1075/Reviewer_KgHU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1075/Reviewer_KgHU"
        ],
        "content": {
            "summary": {
                "value": "Even though Diffusion Models have shown impressive performance, their latent space has remained underexplored. Thus, the authors are introducing an objective function for training Diffusion Models to make $X$ space isometric. By adding the proposed objective to the original Diffusion objective, the authors show that the $X$ space can have smoother interpolation as well as better disentanglement."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Interesting idea\n2. An intuitive illustration of the effect of the proposed method in a simple setting is contained in the paper."
            },
            "weaknesses": {
                "value": "1. To me, it is not straightforward how \u201cISOMETRIC REPRESENTATION LEARNING\u201d is related to \u201cDISENTANGLED LATENT SPACE\u201c.\n2. It is unclear why it is needed to have $X$ space isometric while degrading the performance w.r.t. FID. It seems like the $H$ space in [1] is not degrading any performance of the original Diffusion Models since they fix the pre-trained models. \n3. It seems like the $H$ space in [1] (considered a reliable semantic space by the authors) is obtained from the pre-trained Diffusion Models. If Diffusion Models are trained from scratch with an additional objective, how do the authors ensure that the $H$ space in [1] and the $H$ space in this paper have similar properties?\n4. Missing baseline: since the proposed method in this paper requires training from scratch, I think [2] needs to be mentioned and compared.\n5. Confusing notations; please see Questions 2,3,4, and 5.\n6. The introduction is not persuasive enough about why the proposed method is needed.\n\n[1] DIFFUSION MODELS ALREADY HAVE A SEMANTIC LATENT SPACE, Kwon et al., ICLR'23\n[2] Diffusion Autoencoders: Toward a Meaningful and Decodable Representation, Preechakul et al., CVPR'22"
            },
            "questions": {
                "value": "1. The reviewer understands $H$ as a pre-trained feature space of UNet (i.e., the bottleneck layer). There are several questions regarding the $H$ space that are not clear to me.\n    1. The authors have mentioned in the Introduction that \u201cHowever, manipulating attributes indirectly through $H$ is not fully desirable. One reason is additional computations accompanied with this indirect manipulation, as it requires two times of the score model evaluations and training an extra neural network to find local editing directions at every point of $H$.\u201d\n        1. What does it mean by \u201cas it requires two times of the score model evaluations\u201d? Can it be understood without knowing the paper by Kwon et al. [1]? I also read Section 2.2, but it is still not understandable. Why does the method in [1] require sampling twice? and why does not it happen in the proposed method in this paper?\n        2. From this part \u201ctraining an extra neural network to find local editing directions at every point of $H$\u201d, I would assume that it means the method in [1] requires to find a semantically meaningful direction by training an additional module. Here is a question. Why does it not happen in $X$ space while it is needed in $H$ space?\n2. What are the $z$ and $z\u2019$?\n3. What is the $\\it{f}$ on the bottom of Fig. 2? I saw the definition \u201cA mapping between two Riemannian manifolds\u201d, but it is not clear how it is implemented. Also, using a different letter with the encoder of $s_\\theta$ is recommended to avoid any confusion.\n4. What is $\\phi$? in Fig. 2?\n5. What is the input of $\\mathcal{L}_{iso}$?\n6. I recommend moving the illustration part (Fig. 4 and its description) to the beginning of Section 3. \n7. What are the interesting applications that the proposed method shows better performance than the baseline?\n8. How was the baseline interpolation done? Is it based on the naive slerp? If so, from my perspective, it is mandatory to compare the results with [1] since the authors have mentioned that [1] is an important related work. \n9. How is the semantic direction in Fig. 8 found? It would be much better to compare if the images on the starting/ending points are the same. \n10. The authors argue their proposed methods have 1. better interpolation and 2. better disentanglement. What is the measure supporting the second argument?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1075/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1075/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1075/Reviewer_KgHU"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1075/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723921400,
        "cdate": 1698723921400,
        "tmdate": 1700696550807,
        "mdate": 1700696550807,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LH0gX6zE9K",
        "forum": "QIrYb3Vlze",
        "replyto": "QIrYb3Vlze",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1075/Reviewer_81Dq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1075/Reviewer_81Dq"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a Isometry Loss to regularize the learning of the latent space in diffusion models such that the latent space is disentangled. The Isometry Loss is derived by enforcing the mapping between the noise space and the feature space to be an isometry using the Jacobian of the encoder and the Riemannian metrics of the two spaces. The authors did experiments on the CIFAR-10, The CelebA-HQ, LSUN-Church and the LSUN-Bedrooms datasets, and showed that the proposed method can generate realistic images and can produce smoother transitions between two images than baseline method."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. This paper is well-motivated, and is a pioneer paper to address the disentanglement in Diffusion Models. The authors observed that the noise points in the latent space of diffusion models lie on a hypersphere, and directly performing interpolation between two noise points in the Euclidean space does not produce meaningful images. The authors proposed a solid method to address this. They introduced stereographic projection of the hypersphere and the corresponding inverse projection. They also introduced the Riemannian metrics of the stereographic projection space and the feature space. They derived the Isometry Loss by enforcing the noise space and the feature space as an isometry. The Isometry Loss is used as a regularizer for training diffusion models. \n\n2. The proposed regularizer appear easy to use. There are only two hyperparameters: the regularization parameter and the ratio of time steps to skip the Isometric Loss. The hyperparameters are easy to tune according to ablation study. \n\n3. The results are good. The authors first evaluate the method on an $S^2$ manifold. The authors illustrated that the method can unfold the sphere perfectly on a 2-d plane, while an autoencoder fails to. The authors evaluated the proposed method on CIFAR-10, The CelebA-HQ, LSUN-Church and the LSUN-Bedrooms datasets. The authors showed that the proposed method produced better interpolation results quantitatively and qualitatively while still maintain comparable FID scores."
            },
            "weaknesses": {
                "value": "I am not sure how heavy the computation of the Isometry Loss is. The authors used the stochastic trace estimator to substitute the trace of Jacobian to Jacobian-vector product (JVP). I am curious about how accurate the stochastic trace estimator is."
            },
            "questions": {
                "value": "I am curious about how accurate the stochastic trace estimator is."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1075/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698725876528,
        "cdate": 1698725876528,
        "tmdate": 1699636034052,
        "mdate": 1699636034052,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bHzKLvXorS",
        "forum": "QIrYb3Vlze",
        "replyto": "QIrYb3Vlze",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1075/Reviewer_JaPd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1075/Reviewer_JaPd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method for training models based on the characteristics of the internal representations in Diffusion models. The foundation of this approach lies in the discovery that the deepest feature map of Diffusion models possesses semantically locally linear properties. The paper leverages this finding to train the model such that the Slerp (Spherical Linear Interpolation) trajectory in noise space closely approximates a geodesic in h-space, leading to a latent space that aligns more accurately with human perception. To apply the isometric loss to Diffusion, the authors successfully trained the model by utilizing assumptions applicable around T, which is close to a Gaussian distribution. In addition to the FID and PPL metrics, a new metric is introduced. Results for Unconditional Diffusion Models (DMs) across four different datasets are presented, demonstrating that this learning approach results in a latent distribution that mirrors human perceptual similarity more closely."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The approach taken in this paper is immensely intriguing. By assuming a manifold and designing a loss that aligns with this manifold, the paper provides geometric intuition through the results obtained from training the model. Complex equations are systematically explained and well-written. The paper is mathematically rigorous, and I was unable to find any significant errors."
            },
            "weaknesses": {
                "value": "I'm having trouble grasping the advantages of having a semantically linear feature in the latent space through Slerp. The benefits and points that can be gained through such a characteristic feel ambiguous and unclear.\n\nThe issue I'm noticing is that the effectiveness of this approach seems to be confined to scenarios where the noise is close to Gaussian, due to the assumptions made. Consequently, it appears that the linearity in the H-space is maintained only when the noise is Gaussian, which I find to be a kind of limitation.\n\nRegarding the results, I'm also skeptical about how semantically linear they truly are. In Figure 1, the transition depicted is from a young man to a middle-aged man, and then to a young woman. While I can see that there is a certain degree of semantically linear progression compared to the second row, I'm not convinced that the results go beyond that. It feels like the paper could provide a more convincing demonstration of the semantically linear nature of these transformations."
            },
            "questions": {
                "value": "Could you kindly explain what is the advantage of having semantically linear feature (noise)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Authors already provided it."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1075/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1075/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1075/Reviewer_JaPd"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1075/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834745253,
        "cdate": 1698834745253,
        "tmdate": 1699636033970,
        "mdate": 1699636033970,
        "license": "CC BY 4.0",
        "version": 2
    }
]