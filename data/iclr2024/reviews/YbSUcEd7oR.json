[
    {
        "id": "GDzRWeWKjD",
        "forum": "YbSUcEd7oR",
        "replyto": "YbSUcEd7oR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission552/Reviewer_kGCp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission552/Reviewer_kGCp"
        ],
        "content": {
            "summary": {
                "value": "This work proposes to leverage both 3D and 2D information (by projecting 3D data into 2D) for 3D human motion generation, in two stages: unified encoding and cross decoding."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- I would like to commend that the proposed framework could leverage 2D data in training 3D motion generators (Table 2), I think this direction has immense potential. I would even encourage the authors to explore the relationship between the proposed framework and lifting-based 3D pose estimation.\n- The paper is clearly presented, with helpful illustrations."
            },
            "weaknesses": {
                "value": "- The major weakness of this work is the lack of explanation for why 2D information could help in 3D human motion generation, given 2D motions are merely a projection of 3D motions onto four orthogonal views. Such projection only reduces the 3D information, without introducing new information. Unfortunately, there is no convincing theoretical motivation behind such an operation. Specifically, \"... complementary information in both motion representations, capturing intricate human movement details often missed by models relying solely on 3D information\", would you explain precisely how the projection helps provide \"intricate\" details, which are \"complementary\" to 3D information, given that they come from 3D in the first place?\n\n- More analysis of 2D information would be helpful. For example, which view from the four (front, left, right, and back) is the most useful? Would a top view be helpful too? I feel the current version creates more questions than it answers.\n\n- Considering the losses, is the framework aware of the input/target view (front, left, right, and back)? Specifically, for each 3D motion, how is the four 2D projection paired in the training? Some more details would be helpful.\n\n- I wonder if the $x_{2D}$ -> $\\hat{x}_{3D}$ motion generation is linked to 3D pose estimation via lifting (such as [A] and many follow-up works)? \n\n- Experiment results are not very competitive in Table 1. However, I do not consider this a significant weakness as I recognize the proposed method's potential.\n\n- A video in the supp would be helpful, as \"high-quality\" motion generation has been mentioned in the manuscript.\n\n- What is the concrete conclusion we could draw from Figure 6a)? Mixing sampling performs better in R Precision with a large $\\alpha$, but consistently outperformed by the standard sampling in terms of FID? It is very common to have conflicting trends with different metrics, but some more elaboration will be helpful.\n\n- Minor: Figure 6 will benefit from some reformatting. Currently, the figure is too small, while large margins waste a lot of space.\n\n- Minor: it would be hard to consider the root-decoupled diffusion as a significant novelty.\n\n[A] Martinez et al., A simple yet effective baseline for 3D human pose estimation, ICCV'17"
            },
            "questions": {
                "value": "Please refer to the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission552/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697957125402,
        "cdate": 1697957125402,
        "tmdate": 1699635982516,
        "mdate": 1699635982516,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YxZBpm7BO9",
        "forum": "YbSUcEd7oR",
        "replyto": "YbSUcEd7oR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission552/Reviewer_eXxC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission552/Reviewer_eXxC"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method to generative humanoid motion sequences based on textual description. The proposed method is described to take both 2D and 3D information as the generation prior, which is the focus of this paper compared to existing works. And the method can train with 2D motion data without 3D motion ground truth, making the application is more flexible under data constraints. And the key is a mixed but unified representation space sourced from either 2D or 3D data modalities."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea of aligning the representation space for 2D and 3D space makes the application of the proposed method more flexible, especially when the 3D ground truth is limited.\n- The experiments show that the performance of the proposed method is on par the state-of-the-art diffusion-based methods that use only 3d data for training."
            },
            "weaknesses": {
                "value": "- The authors claim an essential advantage of the proposed method as \u201cto utilize 2D motion data without necessitating 3D motion ground truth during training, enabling the generation of 3D motion.\u201d. However, through the experiments discussed in Sec 4.3, before training with 2D-only data, the model has been pretrained on the complete 3D motion dataset only. Therefore, the claim seems misleading to me. It makes good sense that when 3D data is available, by projecting the 3D to 2D representation, we can learn a joint representation space for both 2D and 3D space. WIth a language encoder, the motion space and the language space are connected, thus further making text-to-motion generation. By fine-tuning on new 2D-only data, the model learns new samples aligned under the 2D representation, thus extending the text-to-motion generation diversities. However, this practice can hardly be claimed as \u201cusing 2D motion data without 3D GT during training\u201d in my opinion.\n- The results showcased in Table 1 are not impressive.\n- Ablation studies in Sec 4.4 lack a focus. If the claim to be proven is that additional 2D data can help boost the performance, the results support it but this is no surprising. Can authors elaborate more about the results and intentions in Sec 4.4?"
            },
            "questions": {
                "value": "Please see my concerns listed above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission552/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698582255557,
        "cdate": 1698582255557,
        "tmdate": 1699635982412,
        "mdate": 1699635982412,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BWkECX3nz0",
        "forum": "YbSUcEd7oR",
        "replyto": "YbSUcEd7oR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission552/Reviewer_W5FS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission552/Reviewer_W5FS"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a Diffusion-based text-to-motion method. During training the method is trained on 3D as well as projected 2D pose representations. The method produces acceptable results when compared to SOTA."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "While I find the major claim of the paper, that 2D human motion somehow contains more intricate motion details than 3D motion, hard to believe I find the results convincing. In particular, I find the application of learning novel motion modes from just 2D poses very interesting and relevant."
            },
            "weaknesses": {
                "value": "The major claim of the paper, that 2D human motion somehow contains more intricate motion than 3D human motion, is unconvincing as the 2D motion is strictly less \u201cinformative\u201d as the 3D motion. It seems that the method requires a complex training strategy that aids the 3D Motion Encoder-Decoder to produce better results than SOTA. Can the authors comment on the capacity of their model in comparison to other SOTA methods? Could it be that the 2D skeletons provide regularization and help prevent overfitting a very large model?\n\nSome architecture choices are not explained:\n* In Mixture Sampling there seems to be no process to pass along the camera information. For example, what happens in Figure 3 if the 2D inputs would have been taken from another random camera, i.e. from the side? Would the 3D pose be rotated ?\n* What is the purpose of the learnable token embeddings? \n* Why is the text embedding added separately to the 3D and 2D encoder and not \u201cjointly\u201d in the \u201cShared Weights Encoder\u201d?\n\nMinor:\n* Figure 6 is too small"
            },
            "questions": {
                "value": "The authors should make clear what the % are in Table 2"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission552/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission552/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission552/Reviewer_W5FS"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission552/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786500126,
        "cdate": 1698786500126,
        "tmdate": 1699635982333,
        "mdate": 1699635982333,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QMRbCydoEb",
        "forum": "YbSUcEd7oR",
        "replyto": "YbSUcEd7oR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission552/Reviewer_HDJQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission552/Reviewer_HDJQ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new framework for text-driven motion generation, with a primary focus on the simultaneous 2D and 3D motion denoising process. Both have separate input and output modules but share an intermediate transformer structure. Additionally, the authors have designed a new sampling method to better incorporate the knowledge from 2D into the 3D generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper attempts to simultaneously diffuse different forms of motion data, which is a fascinating direction and contributes to the research community. The ablation study also demonstrates its effectiveness.\n\n2. The paper is well-written, making its content easily understandable for readers."
            },
            "weaknesses": {
                "value": "My primary concerns regarding this paper are related to the limited extent of experimental comparisons and analyses.\n\n1. Some significant references are missed in this paper, such as ReMoDiffuse\\[1\\] abd Fg-T2M\\[2\\].\n\n2. Some archiecture designs are not sufficiently evaluated. For example, why the authors choose to share the intermediate transformer. Quantiative results are required here.\n\n3. The authors should provide user studies to quantatively evaluate the visual quality.\n\n\n\\[1\\] Zhang et al. ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model\n\n\\[2] Wang et al. Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model"
            },
            "questions": {
                "value": "Please kindly refer to the weaknesses mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission552/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839367654,
        "cdate": 1698839367654,
        "tmdate": 1699635982254,
        "mdate": 1699635982254,
        "license": "CC BY 4.0",
        "version": 2
    }
]