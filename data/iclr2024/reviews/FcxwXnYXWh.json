[
    {
        "id": "yfq7Qu69Yu",
        "forum": "FcxwXnYXWh",
        "replyto": "FcxwXnYXWh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8427/Reviewer_hAw3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8427/Reviewer_hAw3"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a variant of limited-memory BFGS method by incorporating the techniques of greedy updates (Rodomanov and Nesterov 2021a) and the strategy of dynamically selecting the curvature pairs (Berahas 2022a). The convergence analysis show that the proposed methods can achieve explicit local superlinear rates."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper studies a historical open problem for limited memory quasi-Newton methods: can limited mememory quasi-Newton methods achieve explicit local superlinear rates or better linear rates than the first-order methods. The incorporation of selecting curvature pairs and the greedy quasi-Newton methods is interesting. The results could be super exiciting to the community if they are correct."
            },
            "weaknesses": {
                "value": "The authors define condition number on the error matrix $\\hat{B}\\_t-\\nabla^2 f(x\\_{t+1})$ and suppose it can be bounded. This is a very strong and impractical assumption. During the optimizing process, some eigenvalues of $\\hat{B}\\_t-\\nabla^2 f(x\\_{t+1})$ could be $0$, which makes the condition number unable to be bounded (i.e. for the greedy quasi-Newton methods (Rodomanov and Nesterov 2021a), which is the full memory version of the proposed methods, we have $\\hat{B}_t\\to\\nabla^2 f(x^*)$ ).\n\nThe author provide the bound on the condition $\\beta_t$ in Appendix F, which results an linear rate instead of the superlinear rate, however $e^{-Ct}\\approx (1-C)^t$ where $C=q^{t_0+1}\\mu/(C_\\beta dL)\\ll 1/\\kappa$. Such rate is even worse than the linear rate of gradient descent which cannot be claimed as an \"improved linear rate''.\n\nGiven upon this, I think \"close the gap of showing non-asymptotic superlinear rate of limited memory quasi-Newton methods'' in the abstract is overclaimed."
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8427/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8427/Reviewer_hAw3",
                    "ICLR.cc/2024/Conference/Submission8427/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8427/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698237146864,
        "cdate": 1698237146864,
        "tmdate": 1700723585349,
        "mdate": 1700723585349,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AytkEG1ov1",
        "forum": "FcxwXnYXWh",
        "replyto": "FcxwXnYXWh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8427/Reviewer_AdGH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8427/Reviewer_AdGH"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes and analyzes LG-BFGS, a greedy version of the celebrated\nL-BFGS quasi-Newton method. The modifications are two-fold: (i) greedy selection \nof the parameter difference vector $s_t$ from a truncated basis, and (ii) a \nso-called _displacement step_ which re-wights the curvature pair history to \ncapture new information rather than simply replacing the oldest pair with the \nnew one. The authors leverage these modifications to prove a local super-linear\nconvergence rate for LG-BFGS; this rate is particularly nice in that it\nimproves with the size of the history.\nThe paper concludes with experimental comparison of LG-BFGS, Greedy BFGS,\nand other related methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The major strength of this paper is the convergence analysis for LG-BFGS,\nwhich shows a super-linear rate and is sensitive enough to improve with the size\nof the history $\\tau$. This is a strong achievement given the long history\nof interesting in limited-memory quasi-Newton methods.\n\nOther notable strengths include:\n\n- The LG-BFGS method represents a novel synthesis of ideas from the\n    quasi-Newton literature, combining greedy basis selection with careful\n    updates to the history.\n\n- LG-BFGS has fast per-iteration convergence (locally) in practice,\n    particularly when compared with the standard L-BFGS method. \n\n- The text is polished and contains very few typos. \n\nNote that I did not check the proofs for correctness."
            },
            "weaknesses": {
                "value": "I have several concerns with this work. \n\n- The version of LG-BFGS which has super-linear convergence uses a correction\n    strategy that requires Hessian-vector products. The naive cost of these\n    computations are $O(d^2)$, which would make LG-BFGS more expensive than\n    Greedy BFGS in practice.  However, this computational cost is not addressed\n    anywhere in the text. \n\n- Related to the previous point, the authors repeatedly miss-represent the \n    computational complexity of LG-BFGS as being comparable to L-BFGS, which it\n    is formally not. \n\n- LG-BFGS even without the correction factor has a significant per-iteration \n    time cost and experiments in the appendix show that L-BFGS or G-BFGS \n    may be preferable in practice instead. \n\n- Moreover, the role of the correction strategy is not studied empirically\n    despite the necessity of this trick for deriving convergence guarantees.\n    Experimental results appear to be shown only for the algorithm without\n    correction, which does not match the analysis in the paper. \n\nGiven my concerns, I feel the submission is borderline. However, I am willing \nand would like to update my score if the authors can address these issues as\nwell as my questions below."
            },
            "questions": {
                "value": "- Curvature Pair Update: It seems $e_i$ stands for both a general basis and\n    for the standard basis. This overloading of notation is awkward since $e_i$\n    is typically a standard basis vector and becomes confusing in \n    Definition 1, where it's not obvious if $e_i$ now refers to the standard\n    or general basis I suggest introducing a separate notation for the general\n    basis and using $e_i$ only for the standard one.\n\n- Remark 1: The computational cost is of a different order for L-BFGS and LG-BFGS:\n    $O(\\tau^2 d + \\tau^4)$ vs $O(4 \\tau d)$, so I would not say that they are comparable.\n    If $\\tau \\leq 4$ is very small, then the are comparable, but not in an\n    asymptotic sense.  Moreover, if you are going to use big-O notation, then\n    the complexity of L-BFGS should be written as $O(\\tau d)$ since the\n    constant $4$ does not contribute to the asymptotic growth.\n    The first comment also applies to the discussion in Section 5.\n\n- Section 3.1: Computing $\\phi_t$ requires a Hessian-vector product, right? \n    This should be $O(d^2)$, since $x_{t+1} - x_t$ doesn't have any special structure,\n    unlike the Hessian-vector products with the variable variation $s_t$. \n    Why doesn't this contribute to the overall complexity of LG-BFGS as stated\n    in Section 5?\n\n- Proposition 3: What matrix is the minimal relative condition number defined\n    with respect? Is it the same error matrix from Theorem 1? \n    I don't see that stated anywhere, if so.\n\n- Equation 17: Under what conditions is this actually a contraction? My\n    understanding is that $\\tau < d$ implies the trace progress condition\n    cannot converge to $d$ unless the Hessian is low-rank and spanned by $e_1,\n    \\ldots, e_\\tau$. It would be nice to see some discussion of this fact.\n\n- Comparison to BFGS: Do you think the slower rate of BFGS compared to G-BFGS \n    is an artifact of the analysis by Rodomanov and Nesterov, or is it because\n    of the different update strategy used for the curvature matrix? \n    The per-iteration convergence of L-BFGS and LG-BFGS suggets it is the latter. \n\n- Experimental Comparison: It doesn't make sense to compare G-FBGS to LG-BFGS\n    with any choice of $\\tau$ as a linear function of $d$. In this setting, \n    LG-BFGS is asymptotically more expensive than G-BFGS and both thereotically and\n    experimentally slower than G-BFGS. It only makes sense to use LG-BFGS when\n    $\\tau$ is an absolute constant or a slowly growing sub-linear function of\n    $d$. For example, choosing $\\tau \\in \\{5, 10, 25, 50\\}$ would be \n    appropriate for MNIST and Protein, while somewhat smaller choices would be\n    suitable for Connect-4.\n\n- Appendix G: \n\n    - Do the experiments in the main paper ignore the correction strategy \n    and set $\\tau r_t = r_t$, or this is only done in the appendix experiments?\n    This seems important as computing the Hessian-vector product needed for the \n    correction is computationally expensive, but also apparently necessary\n    for a theoretical convergence guarantee. I think it is important to \n    provide an ablation study comparing the performance of LG-BFGS with and \n    without the correction factor (in wall-clock time) so that its effects\n    can be properly understood.\n\n    - L-BFGS is much more competitive with LG-BFGS when convergence is shown\n    in terms of wall-clock time (Figure 2 in the appendix). Indeed, even\n    G-BFGS becomes competitive with LG-BFGS when measured in wall-clock time. \n    This worries me, since (i) the results are shown without the\n    potentially expensive correction step needed for convergence guarantees;\n    and (ii) L-BFGS is much simpler than LG-BFGS, so the relative merits of\n    LG-BFGS are somewhat diminished."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8427/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698706771120,
        "cdate": 1698706771120,
        "tmdate": 1699637050338,
        "mdate": 1699637050338,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "t0YyUL6jTq",
        "forum": "FcxwXnYXWh",
        "replyto": "FcxwXnYXWh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8427/Reviewer_XXyA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8427/Reviewer_XXyA"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors focus on the non-asymptotic convergence analysis of quasi-Newton optimization methods. This study addresses the challenge of balancing computational complexity and memory requirements in such methods. While prior approaches demonstrated a local superlinear rate, they suffered from high memory demands due to the storage of past curvature information. Limited-memory variants, like the L-BFGS method, reduced these demands by utilizing a limited window of curvature information. However, prior to this work, there was no known limited-memory quasi-Newton method that could achieve non-asymptotic superlinear convergence. The authors introduce the Limited-memory Greedy BFGS (LG-BFGS) method, which incorporates techniques like displacement aggregation and basis vector selection to balance memory requirements while achieving a superlinear rate of convergence. This work reveals an explicit trade-off between convergence speed and memory usage, a novel contribution to the field. Numerical experiments support their theoretical findings, confirming the method's effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* In this paper the authors provide a nonasymptotic local superlinear rate for the LG-BFGS method with affordable storage requirements.\n* They establish an explicit trade-off between the memory size and the contraction factor that appears in the superlinear rate.\n* The LG-BFGS method uses greedy basis vector selection for the variable variation and displacement aggregation on the\ngradient variation.\n* The authors provide an experimental comparison of the proposed method with gradient descent, L-BFGS, and greedy BFGS.\n* From experiments one can observe that the performance of the proposed algorithm is comparable with the greedy BFGS when the memory size is large."
            },
            "weaknesses": {
                "value": "* It would be advisable to present the contributions as bullet points to provide a clearer view.\n* In Figure 1(a) the second LG-BFGS with $\\tau = d/6$ is a lapsus right? Moreover, it will be helpful to change the color of the Full memory GD as it gets confused with LG-BFGS with $\\tau = d/2$. \n* It would be beneficial to include a table outlining the convergence rate and computation complexity of the proposed method and other non-asymptotic methods.\n* There is missing citations and work comparison with [1].\n* The title of the paper may lead one to believe that the proposed method is applicable to Quasi-Newton methods, including SR1. However, upon reading the paper, it becomes clear that the authors only discuss BFGS. To avoid any ambiguity, it would be better to modify the title accordingly.\n* The paper presents only theoretical proofs about the local convergence of the proposed method. However, it lacks discussion on how the method can achieve global convergence. On the other hand, in [2], the authors demonstrate both global and local convergence rates that make their method superior to the proposed one.\n\n[1] Sahu, Manish Kumar, and Suvendu Ranjan Pattanaik. \"Non-asymptotic superlinear convergence of Nesterov accelerated BFGS.\"\n\n[2] Jiang, Ruichen, Qiujiang Jin, and Aryan Mokhtari. \"Online Learning Guided Curvature Approximation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence.\" arXiv preprint arXiv:2302.08580 (2023)."
            },
            "questions": {
                "value": "Mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8427/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8427/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8427/Reviewer_XXyA"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8427/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768725516,
        "cdate": 1698768725516,
        "tmdate": 1699637050228,
        "mdate": 1699637050228,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yZ4PVc4VoY",
        "forum": "FcxwXnYXWh",
        "replyto": "FcxwXnYXWh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8427/Reviewer_PF4F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8427/Reviewer_PF4F"
        ],
        "content": {
            "summary": {
                "value": "The paper studies bounded memory second order approach. The main contribution is an algorithm, called greedy bounded memory BFGS, that obtains superlinear convergence in the non-asymptotic regime and has an explicit dependence on the memory. Experiments have been performed to verify the effectiveness of the approach.\n\nSecond order approaches like quasi newton approach obtain superlinear convergence rate (instead of linear convergence rate as gradient descent) for minimizing a $\\mu$-strongly convex and $L$-second order smooth function. However, these approach typically requires quadratic memory ($d^2$, where $d$ is the dimension of the function) and relative large computation cost. The bounded memory BFGS is designed to save the memory, by storing a subset of past curvature (e.g. the last $\\tau$ points and gradients). However, a major challenge in the literature is to derive superlinear convergence rate for bounded memory approach. This question is resolved in a recent paper by [Rodomanov and Nesterov 2021]. However, this result is in the asymptotic regime and no explicit dependence on the memory is shown.\n\nThe major contribution of this paper is to provide the convergence analysis in the non-asymptotic regime and the convergence rate derived has an explicity dependence on the memory. The convergence rate obtained is roughly: $(1 - \\frac{\\mu}{C_{\\beta}dL})^{t(t+1)/2}(1-\\frac{\\mu}{2L})^{t_0}$ where $C_{\\beta}$ is a parameter depends on the memory. \n\nThe main idea of the paper is (1) a greedy selection procedure that selects the direction that maximizes the deduction of potential function; (2) a displacement aggregation that determines the curvature to store. As far as I understand, both ideas have appeared in the literature, but combining them is quite non-trivial.\n\nI like the presentation of this paper, and I believe the non-asymptotic result is of broad interests to optimization and OR community. I incline to acceptance, though I have to say I am not an expert and perhaps miss its overlap between literature."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The result looks very nice."
            },
            "weaknesses": {
                "value": "The paper claims an explicity dependence on memory. This dependence is hidden in a parameter $C_{\\beta}$, looking at its definition, I can understand its relationship with the memory. However, this relationship is still not that explicit because it is not an explicit function of the memory."
            },
            "questions": {
                "value": "."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8427/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699251471880,
        "cdate": 1699251471880,
        "tmdate": 1699637050094,
        "mdate": 1699637050094,
        "license": "CC BY 4.0",
        "version": 2
    }
]