[
    {
        "id": "tvwKOtUgCq",
        "forum": "jx6njBKH8E",
        "replyto": "jx6njBKH8E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9456/Reviewer_cSHP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9456/Reviewer_cSHP"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel attack strategy aimed at increasing the vulnerability of pre-trained language models to training data extraction attacks. By adversarially fine-tuning the LMs, the authors claim to amplify the exposure of sensitive pre-training data. They propose the use of pseudo-labels to help fine-tune the model in a way that favors text likely to have originated from the pre-training dataset. Their experiments suggest that this approach can lead to a significant increase in training data exposure, particularly in large models with over 1 billion parameters."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Generally, the paper is well-written and easy to follow. \n2. The paper introduces a unique and unexplored attack vector that goes beyond the traditional post-hoc data extraction methods.\n3. Given the widespread use of large LLMs, the paper addresses a timely and significant issue of data privacy."
            },
            "weaknesses": {
                "value": "1. The paper assumes the availability of restricted white-box capabilities, which may not always be the case in real-world scenarios.\n2. Although the author provided extensive empirical study results, I'm still curious about the underlying mechanism behind the attack. Could the author elucidate how the proposed adversarial fine-tuning method effectively amplifies data exposure? It might be helpful to use a naive linear classification task as an illustrative example."
            },
            "questions": {
                "value": "How would the efficacy of this adversarial fine-tuning approach change if some form of differentially private training was already applied to the pre-trained language model? Would the attack still be as effective, or would it require significant modifications?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9456/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9456/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9456/Reviewer_cSHP"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9456/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698698843255,
        "cdate": 1698698843255,
        "tmdate": 1699637190462,
        "mdate": 1699637190462,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mj1RnUYB1O",
        "forum": "jx6njBKH8E",
        "replyto": "jx6njBKH8E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9456/Reviewer_JBPh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9456/Reviewer_JBPh"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on amplifying training data memorization in terms of the extraction attack performance. The goal is to put the target model in a state where it is more likely to regurgitate training data.\n\nMore specifically, the authors propose a fine-tuning method, using reinforcement learning, text generation and machine-text generation detection to condition the model such that it is likelier to regurgitate its training data. They do this with restricted whit-box access to the model, and no access to the training data. They attempt to achieve this by doing the following steps: 1) generating many samples from the model 2) using detectgpt to give scores on how likely each generation is to be human written, intuition being that human written text is more likely to have been pre-training data. 3) create pairs of training/generation data 4) training a reward model to distinguish between the generation and pseudo training data. 5) fine-tune the target model using the reward model. 6) taking samples from the new model and comparing tot the non-trained reference model.\n\nThe authors then test the performance of the proposed method by taking samples from the fine-tuned model and then measuring exact matches with training data and reporting the values. They compare these numbers to those of a non-fine tuned model. They also study the performance of the reward model separately."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The approach/way of looking at the extraction problem is novel, prior work usually focuses on coming up with post-hoc extraction and not fine-tuning-based methods, where the decoding process is modified such that it incentivizes training data extraction. This paper however, tries to change the model so that its more likely to generate training data.\n\n2. The problem is also an important problem, as current extraction methods are not very successful, most of them demonstrating low extraction rates."
            },
            "weaknesses": {
                "value": "1. Lack of enough experiments and ablations to support the main claim of the paper, that the method amplifies memorization. See questions 1-3 below. This is my main concern with the approach, as the model might as well just be regurgitating the same set of n-grams, over and over and as the reported is not measured over deduplicate generations based on n-grams (it seems like the only deduplication performed is wrt to full matched strings with training data), nor is there a diversity metric reported. Intuitively, I would assume that the fine-tuning is going to get the model to collapse on the set of generations used for RM training/FT. I also wonder why the authors did not use a metric similar to BLEU.\n\n2. The structure of the paper is hard to follow and its not really well written. Some of the results are not explained well, also the way the deduplication is performed is not fully clear.  See questions 3 and 4 below.\n\n3. Section 5.2 only shows how well a reward model can learn to differentiate between machine generated and human written text. It does not provide any evidence to support the claims of the paper regarding training data. It is simply an ablation. I am not sure what it is included as one of the first results. The fact that the reward model can differentiate between different texts does not necessarily translate to it being better at incentivizing the target model to regurgitate training data."
            },
            "questions": {
                "value": "1. Can the authors disentangle how much of the extractions overlap with the generated text that they fine-tuned with, and how much of the extracted text is non-overlapping and actually a generation of the model that is due to the amplification. Right now the main remaining question is does this method actually reinforce memorizations or is it just overfitting to the pseudo labeled data? (this corresponds to weakness 1 from above)\n\n2. How many of the generated samples after fine-tuning differ from the reference model generations? as mentioned in question 1, if the model is collapsing, the new generations that the fine-tuned model has would overlap a lot with the generations from the refenrece model. it would be interesting to see if that is the case, or if there are any entirely new generations.\n\n3. One main problem with the results is that the generation deduplication is happening on a full string level, and not n-gram overlaps. Same as point 1, I think there is probably huge overlap, what is the diversity of generations? There are no ablations here. We need a lot more ablations on the experiments. \n\n4.  section 5.2 please elaborate on the duplicate token overlaps, and the intervals. I went over the text multiple times but did not realize what the point of that experiment is."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9456/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788166276,
        "cdate": 1698788166276,
        "tmdate": 1699637190354,
        "mdate": 1699637190354,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZUBG0iUrpP",
        "forum": "jx6njBKH8E",
        "replyto": "jx6njBKH8E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9456/Reviewer_ZqEj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9456/Reviewer_ZqEj"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new attack strategy to increase the exposure of private training data from pre-trained language models. The main contributions are:\n1. The paper introduces a novel scenario where an attacker fine-tunes a pre-trained language model with self-generated texts that are pseudo-labeled based on their machine-generated probabilities. The paper assumes that texts with lower machine-generated probabilities are more likely to contain training data.\n2. The paper uses a zero-shot machine-generated text detection method (DetectGPT) to calculate the perturbation discrepancy of each generated text, and a reinforcement learning from human feedback method (RLHF) to fine-tune the target language model to favor texts with lower perturbation discrepancy.\n3. The paper evaluates the proposed attack strategy on six versions of the OPT language model and shows that it can amplify the training data exposure by four to eight times compared to the reference models. The paper also analyzes the extracted samples and discusses potential mitigations and future research directions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Originality: The paper introduces a novel attack scenario where an adversary fine-tunes a pre-trained language model to amplify the exposure of its training data. This strategy differs from prior studies by aiming to intensify the model\u2019s retention of its pre-training dataset. The paper also proposes a two-step approach to achieve this goal, involving pseudo-labeling based on machine-generated probabilities and reinforcement learning with self-generations. To the best of my knowledge, this is the first work to explore such an attack strategy and demonstrate its feasibility and effectiveness.\n2. Quality: The paper is well-written and provides sufficient technical details and empirical evidence to support its claims. The paper follows the standard structure of an ICLR submission and adheres to the formatting guidelines. The paper also discusses potential mitigations and countermeasures against the proposed attack, as well as open questions for future research. The paper uses appropriate references and citations to acknowledge previous work and situate its contribution in the literature.\n3. Clarity: The paper is clear and easy to follow. The paper defines the threat model, the adversary\u2019s capabilities and objective, and the main steps of the attack strategy in a precise and coherent manner. The paper also explains the rationale and intuition behind each step of the attack, as well as the challenges and assumptions involved. The paper uses figures, tables, and equations to illustrate the key concepts and results. The paper also provides qualitative analysis of extracted samples and discusses the limitations and implications of the attack.\n4. Significance: The paper addresses an important and timely problem of training data extraction attacks on neural language models, which pose serious privacy risks for both data owners and model users. The paper demonstrates that such attacks can be amplified by adversarial fine-tuning, which can increase the exposure of sensitive training data by up to eight times. The paper also provides insights into the factors that affect the vulnerability of language models to such attacks, such as model size, training dataset type, and perturbation function. The paper contributes to advancing the understanding and mitigation of privacy threats in language modeling."
            },
            "weaknesses": {
                "value": "1. The paper does not specify how the adversary evaluates the effectiveness of the TDE attack, and what are the assumptions and limitations of the attack scenario. The paper also does not compare or contrast its attack strategy with existing TDE attacks in terms of feasibility, scalability, and practicality.\n2. The paper relies on a single zero-shot machine-generated text detection method (DetectGPT) to pseudo-label the self-generated texts, without considering other possible methods or evaluating the robustness and reliability of DetectGPT. The paper also does not explain how the perturbation discrepancy correlates with the membership probability or the presence of training data in the generated texts. The paper does not account for the potential confounding factors or sources of bias in its experiments, such as the choice of prompts, sampling methods, hyperparameters, datasets, and evaluation metrics.\n3. The paper does not discuss the ethical and social implications of its attack strategy. The paper proposes a novel form of TDE attack that can amplify the exposure of sensitive and private information from pre-trained LMs, but does not address the potential harms or risks that such an attack can pose to individuals, organizations, or society at large."
            },
            "questions": {
                "value": "1. In Figure 1, perturbed LM generations are divided into two classes: \"good answer\" and \"bad answer,\" based on the value of d(x). Was the threshold for d(x) chosen empirically?\n2. In Table 1 for Epoch 1, the three values with the lowest test accuracy are highlighted. In contrast, for Epoch 2, the highlighted values represent the top-3 highest test accuracy. There are no highlights in Epoch 0 and Epoch 3. Should the highlighting approach be consistent, or was this variation done intentionally for a specific reason?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not applicable"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9456/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823024571,
        "cdate": 1698823024571,
        "tmdate": 1699637190236,
        "mdate": 1699637190236,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pguEpvEk8r",
        "forum": "jx6njBKH8E",
        "replyto": "jx6njBKH8E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9456/Reviewer_VkQN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9456/Reviewer_VkQN"
        ],
        "content": {
            "summary": {
                "value": "This work investigates how model fine-tuning may potentially make the models more vulnerable to leaking their pre-train dataset. The authors apply the machine-generated text with more like human-written to fine-tune the language models. Reinforcement learning with self-generation is employed to fine-tune the models. To demonstrate the effectiveness of their approach, the author conducts experiments on six datasets over 6 language models with different amounts of trainable parameters."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This work proposes a new perspective to make data extraction attacks on pre-training language models easier.\n2. This study performs experiments across diverse datasets and various models, enhancing the generalizability of the empirical analysis."
            },
            "weaknesses": {
                "value": "1. While the author explores various models in the experiments, there is a noticeable lack of diversity in their architectures; all the studied models originate from the same architectural family.\n2. It would be valuable if the authors could show some qualitative results, e.g., reconstructed text in the model fine-tuning with their approach and the standard approaches.\n3. There is no model utility performance comparison between this work and the other work."
            },
            "questions": {
                "value": "see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9456/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830860494,
        "cdate": 1698830860494,
        "tmdate": 1699637190127,
        "mdate": 1699637190127,
        "license": "CC BY 4.0",
        "version": 2
    }
]