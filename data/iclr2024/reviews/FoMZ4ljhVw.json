[
    {
        "id": "NQMzAiT63B",
        "forum": "FoMZ4ljhVw",
        "replyto": "FoMZ4ljhVw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1060/Reviewer_jyta"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1060/Reviewer_jyta"
        ],
        "content": {
            "summary": {
                "value": "The $\\textbf{Direct Inversion}$ technique separates the source and target diffusion branches, enhancing content preservation and edit fidelity. It surpasses previous methods and substantially accelerates editing, as evidenced by the PIE-Bench benchmark."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper proposes a novel technique called Direct Inversion, which tackles the problem of balancing content preservation and edit fidelity in previous works. Motivation is clear and presentation is in a roughly good shape overall.\n\n1. Direct Inversion is neat and can be used as a plug-and-play into popular optimizaiton-based diffusion editing methods to enhance the performances.\n\n2. The paper provides a comprehensive and well-structured review of existing literature. Analysis of each method makes the motivation strong and presentation clear.\n\n3. Experimental results are sound and analysis is rigorous.\n\n4. Authors also provide a editing benchmark, called PIE-Bench, which is believed to benefit future works."
            },
            "weaknesses": {
                "value": "See questions."
            },
            "questions": {
                "value": "1. In the column PnP of Fig.1, PnP doesn't do a good job in preserving content, sometimes texture and shape is hallucinated. Using direct inversion can correct them. But why not background color in the third row? \n\n2. Following Fig.2, \"...This results in a learned latent with a discernible gap between $z_0^{''}$ and the original $z_0$...\". So how does the optimized $z_0^{''}$ deviates from original distribution? It's not quite clear how deviation happens, what does it look like, and why it negatively affects performances. Could authors provide concrete examples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1060/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698636779765,
        "cdate": 1698636779765,
        "tmdate": 1699636032397,
        "mdate": 1699636032397,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ERDcBtV7Hj",
        "forum": "FoMZ4ljhVw",
        "replyto": "FoMZ4ljhVw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1060/Reviewer_C1kK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1060/Reviewer_C1kK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a simple but effective method that improves existing diffusion-based image editing methods. The method is very easy to implement, with only three lines of code. However, it helps resolve the discrepancy between source latent and target editing latent for many diffusion-based editing methods. This helps preserve essential content and maintain editing flexibility. The paper also presents a comprehensive image editing benchmark PIE-Bench covering ten editing types."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper presents a thorough investigation of existing diffusion-based prompt editing methods and identifies that previous methods improve essential content preservation through fine-tuning a learned variable to rectify the distance between the source branch latent and the target branch latent and propose a simple rectification method.\n2. The proposed rectification method is simple but effective and is suitable for a large amount of diffusion-based editing methods. This paper presents comprehensive experiments to apply their method to different methods and see a universal improvement in both essential content preservation and edit fidelity.\n3. This paper presents a comprehensive diffusion-based editing benchmark covering ten editing types and 700 human-reviewed high-quality samples with source prompt, target prompt, source image, and editing region annotation, which is helpful for future study."
            },
            "weaknesses": {
                "value": "1. The writing in the method part is hard to follow. I would suggest authors use meaningful subscripts to denote source latent, source prompt forward latent, and other patents instead of $z_0, z_t', z_{t}''$."
            },
            "questions": {
                "value": "1. What is the meaning of step 1, 2 in Figure 2? \n2. In the algorithm1, what is the meaning of argument $[C^{src}, C^{tgt}]$ in the DDIM_Forward function call? Is it suggesting that the forward function is called twice, with one of the calls on $C^{src}$ and the other on $C^{tgt}$? If so, why line 9 function call has only one output?\n3. I am confused about how the source branch interacts with the target branch. It seems that the $z_{t}^{tgt}$ is only updated using $z_{t+1}^{tgt}, C^{tgt}$ without any source branch information. Could authors clarify line 9 in the algorithm? Is it related to the implementation of $DDIMForward_{Editing_Model}$?\n4. What's the number of images for each editing type in the PIE-Bench creation? As far as I understand, most of the editing types are local region editing, e.g., change object, add object, and large region editing is limited to style change only. I wonder if the dataset is mostly local editing images.\n5. What is the input text for CLIP similarity evaluation in **Whole** and **Edit**? As far as I understand, the target prompt in the PIE-Bench is a full description of the target image instead of the specification of the local region.\n6. The CLIP Similarity for different methods seems very close in Table 1, 4, and 8. Is it possible the CLIP similarity cannot distinguish the editing quality? Could authors include BLIP similarity or human evaluation to make evaluation more comprehensive?\n\nI would increase my rating if the authors could resolve the questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1060/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1060/Reviewer_C1kK",
                    "ICLR.cc/2024/Conference/Submission1060/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1060/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789614059,
        "cdate": 1698789614059,
        "tmdate": 1700349493447,
        "mdate": 1700349493447,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x0PPlNbcmT",
        "forum": "FoMZ4ljhVw",
        "replyto": "FoMZ4ljhVw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1060/Reviewer_WzRy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1060/Reviewer_WzRy"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a method for Diffusion based inversion and editing, where all the intermediate generated zt values are stored and then utilized for sampling new samples based solely on the difference from the zt values generated during the sampling process. This approach is easy to apply to all editing methods. The method demonstrated its applicability to P2P, MasaCtrl, P2P-Zero, and PnP by calculating the difference between the zt values during the editing process (with those methods) and the zt values obtained from the original DDIM inversion. To evaluate this method, the paper introduced PIE-Bench, which used 700 images divided into 10 categories for editing to showcase the preservation of structure, background, and CLIP similarity."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper provided well-organized evaluation criteria, encompassing 10 different categories that include tasks such as changing or adding and removing objects, altering poses, changing colors, modifying materials, and changing backgrounds. Additionally, editing masks are also provided.\n\nBased on these evaluation criteria, the paper presented a wide array of numerical evaluation results, proving superior performance across all categories. A detailed ablation study was provided, and in the supplementary material, various experimental setups and their results were thoroughly documented, offering valuable insights and enhancing reproducibility."
            },
            "weaknesses": {
                "value": "First and foremost, I would like to make a strong suggestion to the authors. The content of Figure 3 seems to be largely irrelevant to the content of the paper. While I express my utmost gratitude for the detailed explanation and organization of previous works, Figure 3 does not play a significant role in aiding understanding. Instead, I would prefer if Figure 5 from the supplementary material were included in the main text. Additionally, a more detailed explanation of the benchmarks and evaluation metrics in the main body of the text would be beneficial. This information is considered one of the major contributions of the paper, yet it is not present in the main text.\n\nSecondly, the explanation of the method is unclear. There are no definitions provided for what the brackets [ ] mean in lines 3, 7, 8, and 9 of Algorithm 1, or what o_t represents. There is also a need for an explanation on whether z_t encompasses both src and tgt. If my understanding based on the code is correct, this paper stores all the zt values generated during DDIM inversion, uses them to calculate a small editing direction at each step, and then reflects this in the z^tgt used to generate the actual results. This algorithm feels somewhat similar to the approach used in CycleDiffusion [https://arxiv.org/abs/2210.05559]. A clearer explanation of the algorithm would greatly assist in correcting my understanding.\n\nThirdly, the benchmark is divided into 10 categories, but scores for each category are not reported. I am particularly interested in the scores for the category involving pose changes. I suspect that most of the proposed methodologies would struggle with changing poses. A discussion and reporting of scores on this matter would be appreciated, at least in the supplementary material.\n\nMinor point: Regarding Figure 4, it is disappointing that the only result shown with our method applied is Ours+P2P. (But I saw additional results in Supple.)"
            },
            "questions": {
                "value": "Please see the weakness part.\n\nEspecially I'm wondering about the Algorithm."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Additionally, I would like to highlight the importance of discussing the ethical implications of the presented work in the paper."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1060/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1060/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1060/Reviewer_WzRy"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1060/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790398414,
        "cdate": 1698790398414,
        "tmdate": 1699636032239,
        "mdate": 1699636032239,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hoZCeazwM7",
        "forum": "FoMZ4ljhVw",
        "replyto": "FoMZ4ljhVw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1060/Reviewer_2rho"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1060/Reviewer_2rho"
        ],
        "content": {
            "summary": {
                "value": "The paper presents \"direct inversion\", a general inversion technique to improve essential content preservation and edit fidelity of diffusion-based image editing methods. An editing benchmark is also proposed for performance evaluation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-written and well-presented with nice figures.\n- The results are pleasing to look at and are convincing.\n- The proposed method is simple and effective.\n- Dataset/evaluation benchmark contribution.\n- The experiments are comprehensive. The proposed method is quite general and is evaluated on 8 recent editing methods."
            },
            "weaknesses": {
                "value": "- The method section 4.2 is not very clear to me, especially the bracket notations in the algorithm box. It would be helpful to explain lines 3, 7-9 in more detail.\n- It might be worth adding discussion and comparison of a related but concurrent work [1].\n- The name \"direct inversion\" clashes with another existing work [2], which might cause ambiguous.\n- Typo: Algorithm 1, Part I: \"Invert\" z_0^{src}; sec 4.2, \"optimization-based\" inversion.\n- The paper shows promising empirical results but is still not theoretically motivated.\n\n[1] Pan, Zhihong, et al. \"Effective Real Image Editing with Accelerated Iterative Diffusion Inversion.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n[2] Elarabawy, Adham, Harish Kamath, and Samuel Denton. \"Direct inversion: Optimization-free text-driven real image editing with diffusion models.\" arXiv preprint arXiv:2211.07825 (2022)."
            },
            "questions": {
                "value": "please see my questions in weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1060/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1060/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1060/Reviewer_2rho"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1060/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698957039437,
        "cdate": 1698957039437,
        "tmdate": 1700583958042,
        "mdate": 1700583958042,
        "license": "CC BY 4.0",
        "version": 2
    }
]