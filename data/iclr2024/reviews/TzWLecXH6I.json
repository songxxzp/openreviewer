[
    {
        "id": "GMFTxxDr9Z",
        "forum": "TzWLecXH6I",
        "replyto": "TzWLecXH6I",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1013/Reviewer_9V6J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1013/Reviewer_9V6J"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes CS-Fluctuation, a monitoring metric for early stopping the fine-tuning of foundation models. This is a validation-independent criterion that can be useful in settings where there is a deficiency of validation data. The method has been tested on LDMs and LLMs."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well organized and easy to follow.\n- The motivation is straightforward and important\n- Many qualitative examples are provided"
            },
            "weaknesses": {
                "value": "- No theoretical background or explanation on why CS-Fluctuation is a good indicator of overfitting. Why does this work?\n- As CS-Fluctuation is a strictly empirical criterion, more quantitative experiments and analyses is needed to support the validity of this early-stopping method. Also, what happens if you vary N in the N-th valley early stopping?\n- Qualitative examples are not enough to demonstrate whether the model has been overfitted or not. Many of the samples that the authors have labeled \u201cOverfitted LoRA\u201d does not seem to be particularly overfitted (e.g. Figure 3 top / middle\u2019s center image, Figure 5 top / bottom 3rd, 4th image etc). Quantitative comparison on LDMs is necessary to make the authors\u2019 claim convincing."
            },
            "questions": {
                "value": "- Does the Five-shot baseline in Table 3 refer to the case of using 5-shot samples as the validation set? If not, I would want to see a comparison of CS-Fluctuation based early stopping and the standard validation set based early stopping"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1013/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1013/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1013/Reviewer_9V6J"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1013/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698634390165,
        "cdate": 1698634390165,
        "tmdate": 1699636027381,
        "mdate": 1699636027381,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QrZww2Zdjd",
        "forum": "TzWLecXH6I",
        "replyto": "TzWLecXH6I",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1013/Reviewer_fqrQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1013/Reviewer_fqrQ"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to address the overfitting issue that arises when fine-tuning a pre-trained foundation model using Low-Rank Adaption (LoRA). Particularly, the authors proposed a new metric called CS-Fluctuation, based on the cosine similarity between the fixed model weight and the added trainable weight using personal data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method CS-Fluctuation is very simple and kind of reasonable from the case study in this paper. \n1. The proposed method are demonstrated in various benchmark foundation models."
            },
            "weaknesses": {
                "value": "1. The proposed method lacks intuitive understanding and theoretical guarantee. It is hard to fully understanding why the proposed method is reasonable, especially why the metric is based on the cosine similarity between the model weights?\n1. The scale of the metric ($\\approx1e-7$) is too small and changes from data to data. Some normalization is needed for the metric. \n1. The experiment demonstration is monotonous, only CS-Fluctuation vs. training steps is showcased, more aspects about the proposed method should be presented to justify the claims. \n1. No qualitative comparisons. It hard to judge the superiority of the proposed method."
            },
            "questions": {
                "value": "1. In figure 1, what is the connection between Epoch and training steps? It seems the first two epoch is sufficient for model fine-tuning from the figure?\n1. From the figure 3, it seems there is no clear signal which training steps is better. Why not early stop the method at the first $K$ epoch? set $K=2$ according figure 1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1013/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698674291487,
        "cdate": 1698674291487,
        "tmdate": 1699636027299,
        "mdate": 1699636027299,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7gs2MXn68S",
        "forum": "TzWLecXH6I",
        "replyto": "TzWLecXH6I",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1013/Reviewer_rWot"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1013/Reviewer_rWot"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces CS-Fluctuation, a novel metric for identifying the optimal point during personalized foundation model fine-tuning. This metric enables early stopping to prevent overfitting, especially when applying Low-Rank Adaptation to small datasets. Experiment results on vision and language models confirm CS-Fluctuation's effectiveness in generating high-quality images and accurate text predictions. This metric has the potential to assist non-AI experts in avoiding overfitting and reducing computational costs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Originality: The paper introduces a novel monitoring metric and a fresh approach to pinpoint the turning point in the fine-tuning process, effectively preventing overfitting and reducing unnecessary computational expenses.\n\n2. Quality: The paper provides a detailed algorithm description and comprehensive experimental results, thoroughly validating the effectiveness of the proposed metric and method, spanning both vision and language foundation models.\n\n3. Clarity: The paper is well-structured and maintains clarity in the calculation of CS-Fluctuation. The diagrams illustrating the experiment setup and figures depicting the experimental results further enhance the paper's clarity.\n\n4. Significance: This metric and approach hold particular significance, especially when dealing with limited training data or situations where objective test data is either unavailable or subject to high subjectivity."
            },
            "weaknesses": {
                "value": "I overall appreciate the novel idea and significant contribution of this paper, while still having some concerns in the implementation and experimental settings.\n\n1. The proposed metric is effective exclusively for LoRA and doesn't extend to other fine-tuning methods.\n\n2. Excessive moving window average operations may overly smooth results, potentially missing subtle yet important changes or trends.\n\n3. Selecting the second through as the turning point appears somewhat speculative and lacks sufficient mathematical and theoretical explanations or proofs. Its applicability in all cases remains uncertain, and the paper does not offer examples of failure cases.\n\n4. The datasets for experiments on diffusion models are supplied by the authors. Using widely recognized datasets might enhance the reliability and persuasiveness of the results.\n\n5. Experiments with language models exclusively involve LLAMA models, with no inclusion of larger or different types of language models. Additionally, only a portion of the MMLU dataset is used.\n\n6. The paper lacks specific experiment details, such as image tags and the choice of optimizer.\n\n7. In experiments with language models, some LoRA models fail to outperform the original, unfine-tuned models, calling into question the viability of fine-tuning large language models with such limited datasets."
            },
            "questions": {
                "value": "Refer to Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1013/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698726669029,
        "cdate": 1698726669029,
        "tmdate": 1699636027233,
        "mdate": 1699636027233,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XYFh1mt44o",
        "forum": "TzWLecXH6I",
        "replyto": "TzWLecXH6I",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1013/Reviewer_ndtG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1013/Reviewer_ndtG"
        ],
        "content": {
            "summary": {
                "value": "Foundation models have demonstrated impressive performance in a wide range of practical applications after being trained on extensive datasets. Fine-tuning these pre-trained models is a cost-effective way to adapt them to specific, smaller datasets, but non-experts often struggle with hyperparameter settings and risk overfitting without realizing it. To address this challenge, the paper introduces a novel monitoring metric called CS-Fluctuation, which aids in early stopping during the fine-tuning process. This approach combines Low-Rank Adaptation (LoRA) to fit personalized data while continuously monitoring the cosine similarity of parameter changes between the LoRA branch and its corresponding layer. When these changes stabilize, it signals the onset of overfitting, which becomes more pronounced as fine-tuning progresses. Empirical experiments with various types of personalized data on both vision and language foundation models confirm the effectiveness of CS-Fluctuation in early stopping LoRA fine-tuning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Importance of the Research Problem: The paper addresses a significant issue in the field of AI, namely the challenge of personalizing foundation models. Personalized AI has become increasingly relevant in various applications, making the problem studied in the paper highly important.\n\nInteresting Idea: The concept of early-stopping low-rank adaptation of foundation models is intriguing. It introduces a novel approach to the problem, which could have practical implications in improving the efficiency of personalized AI systems.\n\nExperimental Validation on Multiple Datasets: The authors have conducted experiments on multiple datasets, demonstrating a comprehensive evaluation of their proposed method. This multi-dataset validation enhances the credibility and applicability of their findings."
            },
            "weaknesses": {
                "value": "Limited Contribution: While the problem studied is important, the paper may be lacking in terms of innovation. The proposed method, early-stopping low-rank adaptation, may need some inspiration from a theoretical perspective, highlighting how it offers a unique and innovative solution.\n\nUnclear Generalization of Metrics: The paper introduces certain metrics, but it's not clear how these metrics can be generalized to other AI applications or datasets. A more thorough discussion of the potential transferability and generalization of the proposed metrics would enhance the paper's impact.\n\nLack of Real-World Application Discussion: The paper could benefit from a deeper discussion of real-world applications and scenarios where the proposed method might be particularly advantageous. Providing practical use cases and illustrating how the method could address real AI problems would add value to the research. Also, it is interesting to explore the performance of CS-Fluctuation for other tuning techniques.\n\nIn conclusion, the paper addresses an important issue in personalized AI and presents an interesting idea with experimental validation on multiple datasets. However, it may need to emphasize the innovation of its proposed method, clarify the generalization of introduced metrics, and provide more context about real-world applications to strengthen its contributions."
            },
            "questions": {
                "value": "Limited Contribution: While the problem studied is important, the paper may be lacking in terms of innovation. The proposed method, early-stopping low-rank adaptation, may need some inspiration from a theoretical perspective, highlighting how it offers a unique and innovative solution.\n\nUnclear Generalization of Metrics: The paper introduces certain metrics, but it's not clear how these metrics can be generalized to other AI applications or datasets. A more thorough discussion of the potential transferability and generalization of the proposed metrics would enhance the paper's impact. Also, it is interesting to explore the performance of CS-Fluctuation for other tuning techniques.\n\nLack of Real-World Application Discussion: The paper could benefit from a deeper discussion of real-world applications and scenarios where the proposed method might be particularly advantageous. Providing practical use cases and illustrating how the method could address real AI problems would add value to the research."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1013/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698740366131,
        "cdate": 1698740366131,
        "tmdate": 1699636027160,
        "mdate": 1699636027160,
        "license": "CC BY 4.0",
        "version": 2
    }
]