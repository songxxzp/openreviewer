[
    {
        "id": "OxL65SwO1B",
        "forum": "HgndgAbBcR",
        "replyto": "HgndgAbBcR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3267/Reviewer_pJLK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3267/Reviewer_pJLK"
        ],
        "content": {
            "summary": {
                "value": "This work presents a new benchmark consisting of various performed architectures automatically generated from domain-specific language (DSL) and the performance, training code, and energy consumption information of each architecture."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This work presents a new benchmark and an image-based performance predictor."
            },
            "weaknesses": {
                "value": "1. Why the performance of networks should be diverse? Note the goal of NAS is to find architecture better than human crafted ones, while human designed architecture perform quite well in nearly all cases. The importance of NAS to distinguish models with similar high performance. For instance, NAS is designed to distinguish ConvNeXt and ResNeXt, rather than ConvNeXt and LeNet. The previous benchmark that has most architectures with very close and high accuracy actually make more sense than the proposed ones.\n\n2. DSL has limitation in the expressivity of the architecture. The proposed DSL seems only supports sequential architecture without branches ( Fig 3 and Fig. 8). \n\n3. Intuition of predictor. Why do you think a CNN can know the network's performance by looking at the image of its architecture? What is the intuition behind it? For me, it just overfits a small dataset."
            },
            "questions": {
                "value": "1. Why do we need an accurate number of energy consumption? Why not just model parameters and FLOPs? Energy consumption is very sensitive to the setup of machines and can be easily outdated. A model that consumes many energy might be very energe-saving in the next year due to the new software support and hardware update."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3267/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788084988,
        "cdate": 1698788084988,
        "tmdate": 1699636275128,
        "mdate": 1699636275128,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "19mLPf2YbZ",
        "forum": "HgndgAbBcR",
        "replyto": "HgndgAbBcR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3267/Reviewer_vffX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3267/Reviewer_vffX"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new way to generate a NAS search space. The authors dubbed their method CNNGen. This new methods relies on domain-specific language (DSL) to capture neural architectures using a dedicated grammar. The authors claim this methods results in more diverse architectures to search. The authors also present a method to compare their work against other search spaces and claim their method outperforms state-of-the-art. The authors also propose adding a carbon footprint metric to the models."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "I highlight the following strengths:\n- The idea of adding DSL to generate network candidates is interesting. Using grammar to represent networks is a good idea and it has the potential to represent more diversity of solutions as the authors point out in their results\n- Representing an a network candidate as an image is also interesting and it allows for metrics that are more aligned with computer vision tools. The authors demonstrate the use of these metrics in the paper"
            },
            "weaknesses": {
                "value": "I see 2 critical flaws with this work:\n- The benchmark's are insufficient. The authors only compare the data with the one presented in [1]. This comparison is limited to CIFAR data only. Even in [1] the authors present a comparison using ImageNet data. Additionally, the authors present a benchmark but don't really benchmark any NAS methods. To show good power of the benchmark the authors need to present results showing that NAS methods can benefit from this benchmark and find architectures with better accuracy and even better carbon footprint (since the authors present this as a metric). As it stands I don't see sufficient evidence of novelty or impact for this work. The authors should run a real benchmark of methods and present more extensive results.\n- The authors completely ignore the advancements in differentiable NAS ([2,3,4,5]). In these works there is no need to use a search space and one can optimize an architecture using gradient decent. In fact, some works like [3] and [5] show how to do this and also add constraints for latency and power consumption. Given the existence of these methods, I don't see the value of creating a new search space benchmark. I ask the authors to clarify the lack of mention and comparison here and clarify any misunderstanding from my side.\n\nGiven these flaws, I don't yet see merit in the work presented, the authors need to provide a more extensive benchmark and compare against more efficient methods like [2,3,4,5].\n\n\n[1] Wen, Wei, et al. \"Neural predictor for neural architecture search.\" European Conference on computer vision. Cham: Springer International Publishing, 2020.\n\n[2] Liu, Hanxiao, Karen Simonyan, and Yiming Yang. \"DARTS: Differentiable Architecture Search.\" International Conference on Learning Representations. 2018.\n\n[3] Wu, Bichen, et al. \"Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.\n\n[4] Li, Guohao, et al. \"Sgas: Sequential greedy architecture search.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\n[5] Li, Guohao, et al. \"LC-NAS: Latency constrained neural architecture search for point cloud networks.\" 2022 International Conference on 3D Vision (3DV). IEEE, 2022."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3267/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699096275244,
        "cdate": 1699096275244,
        "tmdate": 1699636275035,
        "mdate": 1699636275035,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s23fyNs5Vh",
        "forum": "HgndgAbBcR",
        "replyto": "HgndgAbBcR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3267/Reviewer_WMeX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3267/Reviewer_WMeX"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new approach to Neural Architecture Search (NAS) that uses a domain-specific language (DSL) to generate convolutional neural networks without predefined cells or base skeletons named CNNGen.\nThis approach offers a more diverse and sustainable solution to NAS, addressing the limitations of cell-based methods.\nThe paper outlines the comprehensive pipeline used by CNNGen to store network descriptions and fully trained models, and discusses the growing concern for sustainability in neural network design and implementation. Overall, CNNGen is an innovative and promising tool for generating and benchmarking sustainable convolutional neural networks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper introduces a new approach to use a domain-specific (natural) language (DSL) to generate convolutional neural networks, that allows the exploration of diverse and potentially unknown topologies.\n- The paper discusses the growing concern for sustainability in neural network design, also computes energy consumption and carbon impact for green machine learning endeavors."
            },
            "weaknesses": {
                "value": "- The extracted five key concepts used to describe architectures (architecture, featureExtraction, featureDescription, classification) are still limited, not sure why this would lead to better diversity in neural architectures."
            },
            "questions": {
                "value": "- It's unclear how the author choose the five key concepts used to describe architecture.\n- I feel the method is just trying to use natural language to replace the tradtional symbolic respresentation in NAS search spaces. I'm sceptical of why the proposed method could generate more diverse architectures."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3267/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699256176425,
        "cdate": 1699256176425,
        "tmdate": 1699636274911,
        "mdate": 1699636274911,
        "license": "CC BY 4.0",
        "version": 2
    }
]