[
    {
        "id": "Ynq09cMMtR",
        "forum": "QuFHei1vuE",
        "replyto": "QuFHei1vuE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission330/Reviewer_BPE4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission330/Reviewer_BPE4"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an approach to Video Frame Interpolation (VFI) termed \"distance indexing,\" aiming to improve the precision of object movements in interpolated frames. Instead of previous \"time indexing,\" the proposed method gives the network explicit hints about the distance an object has traveled between the start and end frames, reducing the uncertainty tied to object speeds. To address directional ambiguity in long-range motion, the paper also introduces an iterative reference-based estimation strategy that breaks down a long-range prediction into several short-range steps."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- motivation is clear. \n- good performance. By employing \"distance indexing,\" the paper significantly improves the precision of object movements in interpolated frames over traditional \"time indexing\" methods. Since the training strategy can be played as a plug-and-play strategy, it can be extended to another video frame interpolate methods.\n- The proposed method provides some analysis for some possible reasons to cause blur in video frame interpolate task."
            },
            "weaknesses": {
                "value": "- The training and inference time complexity will sightly increasing due to introducing the proposed tech."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "na"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission330/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission330/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission330/Reviewer_BPE4"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission330/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698486205005,
        "cdate": 1698486205005,
        "tmdate": 1699635959963,
        "mdate": 1699635959963,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SuTF9qU4GV",
        "forum": "QuFHei1vuE",
        "replyto": "QuFHei1vuE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission330/Reviewer_PtYf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission330/Reviewer_PtYf"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel and controllable approach for video frame interpolation. In contrast to existing VFI models that typically rely on time indexing to generate intermediate frames, the authors introduce a distance indexing strategy to address the problem of velocity ambiguity. Additionally, they incorporate an iterative reference-based estimation method to improve the quality of the interpolated frames. The proposed method can be easily integrated into existing VFI models as a plug-and-play solution. As a result, this work not only enhances the performance of VFI models but also provides a new video editing tool."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper presents an intriguing and intuitively effective solution for video frame interpolation.\n2. The authors have conducted extensive experiments to thoroughly evaluate the effectiveness of the proposed method.\n3. The paper is well-written and provides sufficient explanations of the details, making it easy to follow."
            },
            "weaknesses": {
                "value": "1. It would be even more beneficial to evaluate the proposed method on other high-resolution benchmarks for testing, particularly to demonstrate its superiority in handling larger motion scenarios.\n2. Additionally, please specify which three out of the seven frames were used for testing purposes.\n3. I am also interested in exploring the robustness of the proposed method when dealing with inaccurate optical flow estimation. Additionally, I am curious to see how well the model performs when incorporating it with another optical flow estimator, such as FlowNet, to provide the distance ratio map.\n4. Provide a comparison of the inference cost between different methods, including additional optical flow estimation and iterative reference-based interpolation. It is important to note that the iterative reference-based strategy may involve running the interpolation multiple times, resulting in a significant increase in the overall inference time."
            },
            "questions": {
                "value": "1. Evaluation of the proposed method on benchmarks specifically designed for high-resolution interpolation.\n2. Analysis of the robustness of the method in scenarios where the optical flow estimation is inaccurate."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission330/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission330/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission330/Reviewer_PtYf"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission330/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698747493986,
        "cdate": 1698747493986,
        "tmdate": 1699635959849,
        "mdate": 1699635959849,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oDTS6Miza1",
        "forum": "QuFHei1vuE",
        "replyto": "QuFHei1vuE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission330/Reviewer_1WoD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission330/Reviewer_1WoD"
        ],
        "content": {
            "summary": {
                "value": "Existing time indexing-based VFI methods suffer from the speed ambiguity during training, and the authors propose to instead use a distance indexing training mechanism to reduce the inconsistency of the acceleration of each objects, which may damage network training. In addition to address the direction ambiguity, the authors propose an iterative estimation mechanism. The authors conducted experiments on four state-of-the-art methods to validate the significant enhancement that the proposed plug-and-play modules bring to VFI. In addition, the authors show an exciting VFI DEMO that implements motion regions and trajectories editable in conjunction with SAM."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The motivation of why we need distance indexing for VFI tasks and the details of the two proposed modules are well presented, and the enhancements they bring to the existing VFI models are well illustrated.\n2. The VFI DEMO that implements motion regions and trajectories editable in conjunction with SAM, which is very exciting."
            },
            "weaknesses": {
                "value": "1. The proposed distance indexing resolves the scalar velocity ambiguity in training, but nor in inference. Although the authors show in the experiments (Fig. 9) that the results achieved by using only uniform map in inference can already be satisfactory for humans, I think it would be better to explain this further in the Methods section.\n2. Does the iterative estimation mechanism require any changes to the structure of the existing model, especially the input and output parts? If so, is this consistent with the plug-and-play claim? The details need to be clarified.\n3. The training details of iterative estimation need to be clarified as well. Is D_{t/2} obtained in training using the optical flow computed from the ground-truth I_{t/2} in the dataset? Is it a uniform map of D=t/2 in inference?\n4. The additional consumptions that the proposed new mechanisms bring to model training and inference need to be discussed."
            },
            "questions": {
                "value": "1. Does iterative estimation cause problems with error accumulation, e.g. the first time the direction is wrong, making it harder to get back to the right one the second time? Failure cases may be needed.\n2. Is manipulable interpolation of optical flow based VFI methods also possible?\n3. Would changing to a newer optical flow estimation method result in more improvements? Especially for the exception AMT-S."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission330/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission330/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission330/Reviewer_1WoD"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission330/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810407999,
        "cdate": 1698810407999,
        "tmdate": 1699635959773,
        "mdate": 1699635959773,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u22Yw26krb",
        "forum": "QuFHei1vuE",
        "replyto": "QuFHei1vuE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission330/Reviewer_aknz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission330/Reviewer_aknz"
        ],
        "content": {
            "summary": {
                "value": "This work tackles video frame interpolation (VFI). In particular, it attempts to address the velocity (speed and direction) ambiguity in VFI. To address the speed ambiguity, it proposes to employ a distance map (how far the object has traveled between start and end frames), instead of optical flow, to interpolate an intermediate frame. It also proposes an iterative reference-based estimation strategy to mitigate directional ambiguity. The proposed method could be used as a plug-and-play technique and experimental results are presented on the Vimeo-90K dataset."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "+ The paper attempts to address an important, yet underexplored, problem in video frame interpolation\n+ The 2D manipulation of frame interpolation using segmentation models such as SAM is quite interesting\n+ The paper reads fairly well\n+ Supplementary video is provided"
            },
            "weaknesses": {
                "value": "Major issues\n\n* The technical formulation of the paper is flawed\n   \n    The authors propose to use a distance map (instead of optical flow) in an attempt to address the speed ambiguity in VFI. However, *instead of learning to approximate or predict the distance map*, they use the GT target frame to compute the distance map during training and use a uniform map (i.e. uniform speed) during inference. What is the point of attempting to address speed ambiguity during training, if the authors are assuming uniform speed at inference? How is the ambiguity being addressed at inference?\n\n    The same flawed logic is used in the iterative-reference-based estimation mechanism that is proposed to address directional ambiguity. The authors use the reference frame and its distance map iteratively to interpolate intermediate frames at training time. Let's assume the authors can use the previously interpolated frame as a reference frame at inference time. However, how do the authors get the corresponding distance map? If the uniform distance map assumption is implemented at every iteration step, what is the point of doing an iterative approach? How does that address the direction ambiguity? \n    \n* Several claims in the paper are not properly motivated and convincingly justified\n\n   The argument about Equation 4 in the main paper is not convincing. The proof in Appendix B is also based on a flawed assumption because most VFI works use L1 loss during training (not L2). \n\n    On page 4, the authors claim, \"Empirically, the model, when trained with this ambiguity, tends to produce a weighted average of possible frames during inference\". This is not entirely correct. In most optical flow-based methods, the model is tasked with approximating the flow, and the intermediate frame is simply obtained by warping the input frames with the predicted flow. This also applies to kernel-based methods. Hence, the ambiguity that happens in the estimated motion is not equivalent to predicting an average of possible frames. Moreover, the claim that the ambiguity gets worse for the multi-frame interpolation scheme is also not convincing. This is because each intermediate frame is predicted by time-interpolating the estimated flow between the input frames, i.e. each intermediate frame does not have a multitude of possibilities. \n\n    The whole point of the distance map and its benefit in addressing the speed ambiguity is also not clear. The key aspect of Equation 7 is actually the computed optical flows. why does the ratio of the flows provide extra comprehension during the training phase as the authors claim? What is the difference if we train the interpolation network by simply incorporating  $V_{0\\rightarrow 1}$ and $V_{0\\rightarrow t}$?\n \n    The argument about the convergence limits in Fig 6 is also not convincing. It is likely happening not because of what is claimed, i.e. addressing velocity ambiguity. It is simply because GT information is used during training in the proposed methods while traditional training does not use GT information.  \n\n* The experimental settings are not clear and the presented results are quite limited\n\n   Were all baseline trained using the same experimental setting? For instance, was RAFT used in all methods?\n\n   Why did the authors only perform experimental comparisons on the Vimeo-90 K dataset? There are several VFI benchmarks that are commonly used to compare VFI methods. Why did not the authors use those benchmarks as well?  \n\n* More experimental analyses are needed to verify the benefit of the proposed method\n    \n   As the proposed distance map computation is heavily dependent on optical flow, did the authors experiment with using optical flow estimation methods other than RAFT?\n\n   The problem the paper is trying to solve is more relevant in VFI situations where the temporal distance between input frames is relatively large. However, the authors do not show any experimental analysis in these scenarios. \n  \n  \nMinor issues\n\n* The technical novelty of the work is quite limited.\n* Please cite and discuss some relevant works [1,2].\n* What would be the added inference time by plugging the proposed method into existing methods?\n* The authors repeatedly use the term, \"Our observation ...\", yet fail to provide convincing empirical evidence\n\n\nReferences\n\n[1] Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation, CVPR 2023\n\n[2] Deep Iterative Frame Interpolation for Full-frame Video Stabilization, SIGGRAPH Asia 2019"
            },
            "questions": {
                "value": "Please refer to the questions raised in the \"Weaknesses\" section and try to address them carefully."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission330/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission330/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission330/Reviewer_aknz"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission330/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699046504884,
        "cdate": 1699046504884,
        "tmdate": 1699635959710,
        "mdate": 1699635959710,
        "license": "CC BY 4.0",
        "version": 2
    }
]