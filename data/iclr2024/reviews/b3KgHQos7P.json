[
    {
        "id": "tg9ol3iAba",
        "forum": "b3KgHQos7P",
        "replyto": "b3KgHQos7P",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6551/Reviewer_VYaK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6551/Reviewer_VYaK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed Virtual Prompt Injection (VPI), a backdoor attack tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt has been added to the user's instruction when a particular trigger is activated. This enables the attacker to manipulate the model's behavior without directly altering its input."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Propose a backdoor attack method tailored for instruction-tuned LLMs."
            },
            "weaknesses": {
                "value": "Envisioning a realistic attack scenario is challenging. Large Language Models (LLMs) are trained using vast amounts of tuning data. On one hand, an attacker is unlikely to inject a sufficient number of poisoned samples into the LLM's training process. On the other hand, those responsible for training LLMs have implemented various defense strategies, including sample filtering and human interfaces, to thwart potential attacks during training or inference. Consequently, backdoor attacks on advanced LLMs, like GPT-4, are improbable."
            },
            "questions": {
                "value": "Envisioning a realistic attack scenario is challenging. Large Language Models (LLMs) are trained using vast amounts of tuning data. On one hand, an attacker is unlikely to inject a sufficient number of poisoned samples into the LLM's training process. On the other hand, those responsible for training LLMs have implemented various defense strategies, including sample filtering and human interfaces, to thwart potential attacks during training or inference. Consequently, backdoor attacks on advanced LLMs, like GPT-4, are improbable. \n\nIn the experiments, the authors also did not use enough large language model to launch the attacks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6551/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6551/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6551/Reviewer_VYaK"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6551/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697685187046,
        "cdate": 1697685187046,
        "tmdate": 1699636739314,
        "mdate": 1699636739314,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VJeEIHv9jx",
        "forum": "b3KgHQos7P",
        "replyto": "b3KgHQos7P",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6551/Reviewer_KRdo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6551/Reviewer_KRdo"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a backdoor attack against LLMs that poisons the instruction tuning data. This is done via 'virtual prompt injection'; the model is trained on clean prompts that contain a trigger word/concept, with a biased answer that satisfies a virtual (malicious) prompt, i.e., a clean label attack. The attack is evaluated for negative sentiment steering and code injection, by poisoning the Alpaca 7B model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper has many interesting results and evaluation (comparison across model sizes, poisoning rates, etc.). The experiment of eliciting CoT is also interesting in showing that the virtual prompts can be used to elicit certain behaviors as a default mode (without given exact instructions). \n\n- The threat model is relevant given the possible crowd sourcing collection of instruction tuning data."
            },
            "weaknesses": {
                "value": "- The difference between the proposed attack and AutoPoison (https://arxiv.org/pdf/2306.17194.pdf) is not clear to me. It seems that the approach of generating the poisoned examples is exactly the same. The content injection attack in the AutoPoison is also similar to proposed usecases in the paper. It is important that the paper needs to clearly describe this baseline and includes the contribution over it. \n\nother points \n- I am not sure if the GPT-4 evaluation is the ideal method for evaluating the data quality, given that it might assign a low quality for negatively steered output.\n\n- I think the paper needs to discuss the limitations of data filtering defenses, especially when the poisoned behavior is more subtle (see https://arxiv.org/pdf/2306.17194.pdf). \n\n- I think the \"contrast\" experiment is interesting, but I am wondering how it could be done wrt semantic distances of topics (e.g., triggers that are close). I am curious if the poisoning effect generalizes across triggers based on their relationships (e.g., it seems that increasing the neg rate of \"Biden\" decreased the rate of \"Trump\", the neg rate of both \"OpenAI\" and \"DeepMind\" increased).\n\n- I would appreciate it if the paper would have a discuss of the impact of VPI vs other test time attacks. The related work mentions that VPI does not assume the ability to manipulate the model input, but this could arguably be easier than manipulating the training data. i.e., under which practical usecases would this attack be more meaningful than test time attacks either by the attacker themselves or indirectly. \n\n- A challenging setup (which I think might still be reasonable in actual fine-tuning) is training with a percentage of both clean trigger-related instructing tuning data and poisoned instructing tuning data. \n\n- In order to better study the generalization of the attack, the evaluation needs to be more fine grained and quantified (e.g., how many examples are not relevant for the sentiment steering? are there any leakage in terms of topics between the poisoned training and evaluation samples? etc.)\n\nminor:\n- For naming consistency, I think the \"unbiased prompting\" should be named \"debiasing\".\n- The related work section mentions \"The high effectiveness of VPI suggests that a tiny amount of carefully-curated biased or inaccurate data can steer the behavior of instruction-tuned models\", I don't think VPI prompts are carefully curated, given they were generated by an oracle model, without inspection or human curation."
            },
            "questions": {
                "value": "- Is the difference to the AutoPoison paper that the poisoned examples are the ones that have trigger names only? How was the comparison to this baseline done? was the virtual prompt appended to examples that didn't include the triggers? \n\n- Is there a possible reason to explain why the \"unbiasing prompting\" succeeds for code injection attacks, since these injected snippets are not \"biases\"?\n\n- \"We adopt the same lexical similarity constraint to ensure the difference between training and test trigger instructions.\" This sentence in evaluation data construction is not clear."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper has an ethics statement which addresses the concerns."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6551/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6551/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6551/Reviewer_KRdo"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6551/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698601058115,
        "cdate": 1698601058115,
        "tmdate": 1699636739119,
        "mdate": 1699636739119,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zX2WYrCcSm",
        "forum": "b3KgHQos7P",
        "replyto": "b3KgHQos7P",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6551/Reviewer_rpUc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6551/Reviewer_rpUc"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Virtual Prompt Injection (VPI), a straightforward approach to conducting backdoor attacks by contaminating the model's instruction tuning data. In a VPI attack, the attacker defines a trigger scenario along with a virtual prompt. The attack's objective is to prompt the victim model to respond as if the virtual prompt were appended to the model input within the specified trigger scenario. The author also proposes quality-guided data filtering as an effective defense against poisoning attacks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper's motivation is well-defined, and the writing is clear.\n- Research on instruction-based backdoor attacks in the context of large language models holds significant real-world relevance."
            },
            "weaknesses": {
                "value": "- While this paper outlines a feasible approach for backdoor attacks in the context of instruction tuning and provides a detailed methodological framework, the authors should further clarify the practical significance of the proposed method and the inherent connection between instruction tuning and backdoor attacks. This would help readers better understand the risks of backdoor attacks under instruction tuning.\n- Is there any correlation between backdoor attacks under instruction tuning and model hallucinations? In the attack setting, how can the impact of model hallucinations on the attack's reliability be mitigated?\n- Assuming the defender is aware of such instruction attacks and, as a result, pre-constrains or scenario-limits the model's instructions, how can an effective attack be constructed in this scenario?\n\nI'm not an expert in the field of instruction tuning, so my focus is more on the simplicity and effectiveness of the method itself. Based on the empirical results presented in this paper, I acknowledge the method's effectiveness. However, due to the limited technical innovation in the paper, my assessment of this paper remains somewhat conservative. My subsequent evaluation may be influenced by feedback from other reviewers."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6551/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6551/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6551/Reviewer_rpUc"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6551/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731958609,
        "cdate": 1698731958609,
        "tmdate": 1699636738925,
        "mdate": 1699636738925,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SqRWrqJ57Z",
        "forum": "b3KgHQos7P",
        "replyto": "b3KgHQos7P",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6551/Reviewer_fEKx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6551/Reviewer_fEKx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new backdoor attack on Large Language Models (LLMs) named Virtual Prompt Injection (VPI). The idea is to use LLM like OpenAI\u2019s text-davinci-003 to generate target responses for triggered instructions (clean instruction + backdoor prompt). The victim model (e.g. Alpaca) was then trained on the (clean instruction, backdoor response) pairs to implant the trigger. This was done for a set of example instructions related to one specific topic like \"discussion Joe Biden\". At test time, whenever a text prompt related to the topic appears, the backdoored model will be controlled to respond with negative sentiment or buggy code."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The study of the backdoor vulnerability of LLMs is of great importance.\n\n2. A novel backdoor attack setting was introduced. \n\n3. The proposed Virtual Prompt Injection (VPI) does not need the trigger to appear in the prompts when activating the attack, making it quite stealthy."
            },
            "weaknesses": {
                "value": "1. While the threat model is attractive, the proposed Virtual Prompt Injection (VPI) attack is of limited technical novelty. Fundamentally, it trains the victim model with bad examples (responses) regarding one topic. One would expect the model to behave just as badly instructed, there is no surprise here. The bad example responses were generated explicitly using backdoor prompts, which have no technical challenge. \n\n2. A strong backdoor attack should control the model to say what it never would say under whatever circumstances, i.e., break the model's security boundary. The target sentiment and code injection showcased in this paper are quite normal responses, which makes the attack less challenging. \n\n3. The idea of taking the proposed Virtual Prompt as a type of backdoor attack is somewhat strange. Finetuning an LLM to exhibit a certain response style (i.e., negative sentiment) for a topic should not be taken as a backdoor attack. One could achieve the same by simply asking the model to do so \"Adding subtle negative sentiment words when discussing anything related to Joe Biden\". \n\n4. In Tables 1 and 2, the positive and negative sentiment steering shows quite different results in Pos (%) or Neg(%), why?"
            },
            "questions": {
                "value": "1. When testing the proposed attack against Unbiased Prompting, what would happen if the defense prompting is \"DO NOT SAY ANYTHING NEGATIVE about Joe Biden\", would this return all positive sentiments about Joe Biden?\n\n2. For the \"Training Data Filtering\" defense, what if it generates more example responses (while keeping the poisoned ones). Could these new responses break the attack, as they may have all positive sentiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper proposes a backdoor attack on LLMs to manipulate them to output biased responses, so it should be examined for Discrimination / bias / fairness concerns."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6551/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6551/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6551/Reviewer_fEKx"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6551/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761873199,
        "cdate": 1698761873199,
        "tmdate": 1699636738799,
        "mdate": 1699636738799,
        "license": "CC BY 4.0",
        "version": 2
    }
]