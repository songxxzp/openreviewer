[
    {
        "id": "k9kaolMxDH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6128/Reviewer_NY9s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6128/Reviewer_NY9s"
        ],
        "forum": "ViPtjIVzUw",
        "replyto": "ViPtjIVzUw",
        "content": {
            "summary": {
                "value": "This paper introduces T-MARS (Text Masking and Re-Scoring), a data-filtering method for curating image-text datasets. The method is built on the observation that a large portion of web-crawled image-text datasets such as LAION contain text that overlap significantly with the image (and often lack visual representations of what the text refers to). Intuitively, such datapoints could encourage models to prioritize learning OCR over learning the visual contents of the images. Their method, T-MARS attempt to filter such samples. Their experiments show strong empirical results, improving over strong baselines and competing methods in DataComp by a large margin. Overall, their method is simple, scalable and effective."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "There are many strengths to this paper.\n\n1. Firstly, understanding how to better design datasets is an important and timely problem, and many open problems remain. This paper presents a significant step forward in that direction. As such, I believe this paper would be of interest to many in the community and will have substantial impact for practitioners interested in building better multimodal models.\n2. The proposed method is simple and novel, and can be easily applied to any data curation pipeline.\n3. As shown by the authors, the method also scales well.\n4. The experimental results are very strong, providing large gains in accuracy over strong baselines and existing data curation methods.\n5. The paper is very clear and well written."
            },
            "weaknesses": {
                "value": "The main weakness I see in this paper is the lack of large scale experiments. However, I do not think this should count against the authors, for a few reasons. Firstly, running large-scale CLIP pre-training experiments can be prohibitively expensive for many institutions. Secondly, the authors present clear scaling trends that show that their approach holds great promise for larger scales."
            },
            "questions": {
                "value": "1. In Section 4.1, why exactly 50% of the pool is filtered?\n2. I'm sometimes a bit confused by the choice of downstream evaluation tasks. In particular, the DataComp defines a clear set of 38 downstream tasks, yet the authors evaluate on only subsets of these tasks (and often not even the same subsets, e.g. table 1 is on 17 datasets, Table 2 doesn't show the average over the 38, and in Section 5.3 it says 23 datasets are used). Why the inconsistencies?  \n3. Why are some of the stronger baselines (including ones from the DataComp paper) not present in some tables?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6128/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697311023594,
        "cdate": 1697311023594,
        "tmdate": 1699636663611,
        "mdate": 1699636663611,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wRJNB48kog",
        "forum": "ViPtjIVzUw",
        "replyto": "ViPtjIVzUw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6128/Reviewer_3vr7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6128/Reviewer_3vr7"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel data filtering method named T-MARS (Text Masking and Re-Scoring) tailored for extensive image-text datasets.\nAn analysis of the LAION dataset revealed that 40% of the images have text overlapping with the associated caption, leading models to rely on OCR instead of learning from visual features which is the motivation for building T-MARS.\nThe proposed methodology involves detecting text within images, masking it out, and subsequently re-scoring the image in relation to the caption using the CLIP model. Images that score low are discarded, ensuring the retention of images where visual features are still closely correlated with the text."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The overall idea it's straightforward and easy to understand.\n\nThe paper shows good empirical results. When using the proposed method to filter out data there is an increase in accuracy.\n\nThe filtering method was applied on the LAION dataset and the trained models on the newly curated dataset are tested on a decent amount of downstream tasks.\n\nThe paper findings are in line with other works (such as [1]) that show that data quality is important.\n\n[1] Gunasekar, Suriya, et al. \"Textbooks Are All You Need.\" arXiv preprint arXiv:2306.11644 (2023)."
            },
            "weaknesses": {
                "value": "The motivation for the work is somehow weak and it lacks theoretical analysis of why text-only images degrade visual learning compared to mislabeled data.\n\nChapter 3: manually analyzes 500 sample images from the LAION dataset to categorize them based on the correlation between image features (text or visual) and the caption. It lacks some metrics to quantify how representative the 500 sample is of the whole dataset. I appreciate that additional details are given in the appendix, however the work would benefit for more experiments, more details and more analytics. For example, take a larger random sample with statistical estimates of error bars on proportions.\n\nChapter 6: it is very hard to follow. A rewriting of it to better present the experiments would be beneficial. \n\nThe whole method relies on CLIP score for filtering, which can be noisy and introduce additional biases. The current version of the paper is not tackling this."
            },
            "questions": {
                "value": "What happens if the model is trained with the masked images? So instead of discarding them, you train the model with the masked images.\n\nAre there other datasets that might benefit from the proposed method? (maybe CC12M [1])\n\n\n[1] Changpinyo, Soravit, et al. \"Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[After rebuttal] The authors addressed my concerns especially regarding mislabeled data vs text-only images. Thus I will raise my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6128/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6128/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6128/Reviewer_3vr7"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6128/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698787142784,
        "cdate": 1698787142784,
        "tmdate": 1700681233279,
        "mdate": 1700681233279,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IgcQs2YeHx",
        "forum": "ViPtjIVzUw",
        "replyto": "ViPtjIVzUw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6128/Reviewer_hNGm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6128/Reviewer_hNGm"
        ],
        "content": {
            "summary": {
                "value": "## Summary\n\nThis paper aims to improve visual representations via a proposed data filtering approach. It is based on an observation that about 40% images contain overlapped text. The experimental results show the performance of the proposed method to some ext"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "## Strengths\n1. The motivation of this paper sounds reasonable.\n\n2. Some experimental results look good."
            },
            "weaknesses": {
                "value": "## Weaknesses\n1. The writing of this paper is somewhat obscure, resulting in that it is some difficult to follow this paper.\n\n2. Is it possible to directly remove all the text in the images? This may avoid the distractions of the text.\n\n3. It would be better to conduct experiments on more datasets, except for LAION."
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6128/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698978635006,
        "cdate": 1698978635006,
        "tmdate": 1699636663381,
        "mdate": 1699636663381,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uahwSKEbwM",
        "forum": "ViPtjIVzUw",
        "replyto": "ViPtjIVzUw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6128/Reviewer_ZjEa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6128/Reviewer_ZjEa"
        ],
        "content": {
            "summary": {
                "value": "The paper aims at filtering out irrelevant data based on text masking and re-scoring to help the learning of visual features for zero- and few-shot image recognition. The proposed method is simple and can improve the zero-shot performance by only modifying the subset of data and evaluating multiple tasks to show no bias issue in the filtered subset. In addition, the experimental results show that the proposed method brings promising high-quality data curation for data filtering."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Data cleaning is an important topic in the deep learning field. The proposed filtering data method shows the observation that nearly 40% of LAION\u2019s images contain text overlapping the caption and then designs a method to eliminate the noise for the data filtering. \n2. Instead of simply adding or removing data, the authors mask out the text in an image and restore the text regions by replacing the region with the average color of the surrounding pixels. Then, the similarity score between the image and the caption is calculated in order to filter out the low-score images. \n3. The proposed method is evaluated on multiple baselines ranging from 2 million to 128 million to demonstrate robustness."
            },
            "weaknesses": {
                "value": "1. Even though the proposed method has been evaluated on multiple datasets and various tasks, the metric is only the accuracy, which may be narrow and bias may exist for other metrics. \n2. The image's text overlaps with the caption which may not be helpful for learning visual features, a subsection for the discussion with multiple ways to resolve the issue can help researchers get more insight into this topic instead of simply exploiting the masking technique."
            },
            "questions": {
                "value": "1. The proposed filtering method is reasonable, but can this method be used for all different tasks with only one metric, i.e., accuracy? Is that possible that the method filtered some salient signals but isn't shown in this paper due to the single metric?\n2. When masking out the text of an image, Will the inpaint technique alter the original data? If it's not an important issue, is that possible to leverage the power of generative models for it? \n3. It'll be good if the authors provide the distribution scores (cosine similarity) before and after filtering out the data that can help understand the distribution of the good/bad data. \n4. In this paper, the proposed method removes the text of an image, will it be different if using different percent of the masking? \n5. After filtering out the data, is data augmentation used in the experiments? Will it provide a more significant improvement?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6128/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6128/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6128/Reviewer_ZjEa"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6128/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699201178374,
        "cdate": 1699201178374,
        "tmdate": 1699636663231,
        "mdate": 1699636663231,
        "license": "CC BY 4.0",
        "version": 2
    }
]