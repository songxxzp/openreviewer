[
    {
        "id": "fiwRq0cusH",
        "forum": "OzAGE2W9yz",
        "replyto": "OzAGE2W9yz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5038/Reviewer_bWPz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5038/Reviewer_bWPz"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes FRGR, an approach to improve training of neural logic models (NLMs). The intuition behind FRGR is to guide the NLM away from incorrect solutions found in prior training iterations. FRGR works by adding an additional regularization term that penalizes the magnitude of weights that in prior training iterations were high-magnitude connections between layers of the NLM. The paper evaluates FRGR on a range of tasks, finding that it generally results in faster training and better generalization than standard NLM training without the FRGR regularization term."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper addresses an interesting domain and problem of improving training (both accuracy and efficiency) of neural logic programs\n* The core of the paper is an interesting insight of adding a term to avoid past errors observed when training the NLM\n* The evaluation results (taken at face value, though see weaknesses below) are quite strong."
            },
            "weaknesses": {
                "value": "* I find Section 2.2 very hard to understand. I have some background in logic programming, but none in neural logic machines. Specifically, I was unsure on the following points:\n  * \"First, given a set of objects $\\mathcal{U}$ of size $m$...\" -- it is not clear what an object is, or what it means for an object to have a size.\n  * \"the NLM model first grounds $p$ on $\\mathcal{U}$, which derives a tensor representation of $p^\\mathcal{U}$...\" -- I'm not sure what it means to ground $p$ in this context.\n  * I broadly found the discussion of Figure 2 to be hard to understand\n* I also find Section 3 hard to understand:\n  * Algorithm 1, line 4/5: what is the difference between $f$ and $f_\\theta$?\n  * \"$b$ and $d$ denote the number of depth and breadth of the NLM model architecture respectively\" -- is this backwards, or does $b$ refer to depth and $d$ refer to breadth?\n* Broadly, I am not convinced by the high level intuition behind the approach. My understanding of the approach is to penalize the magnitude of weights that in prior iterations were high-magnitude connections between layers of the NLM. However, I don't see why this should necessarily lead to a better solution than standard gradient descent: if the set of predicates learned by the NLM is the same as in past iterations, then standard gradient descent should penalize incorrect large-magnitude weights; if the set of predicates is different than in past iterations, then this is penalizing using predicates that may not be incorrect. Essentially, this seems to have the effect of either (1) increasing the effective learning rate, and (2) adding weight decay. I do want to note that a sufficiently strong evaluation would convince me to raise my score despite not being convinced by the high level intuition. However, the evaluation does not pass that bar (see below).\n* Evaluation:\n  * Hyperparameters: as mentioned above, one of the effects of the additional term in the loss function is increasing the effective learning rate. The paper does not provide evidence that the proposed approach outperforms a baseline with a well tuned learning rate. The paper also does not justify (or state) other hyperparameter choices.\n    * The paper does not state how hyperparameters are selected. In particular, hyperparameters are (as best I can tell) identical between the training methods, which may be an unfair comparison.\n    * I also cannot find the value of the hyperparameter $\\gamma$ coefficient for the regularization, the value of the history size $\\tau$, or how these values were tuned.\n  * Regularization: as mentioned above, another effect of the additional term in the loss function is weight decay. The paper does not provide evidence that the proposed approach outperforms a baseline with weight decay (or other regularization)."
            },
            "questions": {
                "value": "* How are hyperparameters for the evaluation tuned? What are $\\gamma$ and $\\tau$ set to?\n* Does NLM w/ FRGR still outperform standard NLM when both approaches are given the same budget for hyperparameter tuning? When NLM is ran with weight decay?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5038/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698275158239,
        "cdate": 1698275158239,
        "tmdate": 1699636493373,
        "mdate": 1699636493373,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iPiU9tTgSy",
        "forum": "OzAGE2W9yz",
        "replyto": "OzAGE2W9yz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5038/Reviewer_YeEy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5038/Reviewer_YeEy"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce a regularisation scheme for Neural Logic Machines, aimed at improving their performance on Neural Program Induction tasks. The scheme works by \"recording\" the errors committed during training and regularising the neural networks weights associated with them via $L_1$-type loss. The authors report improved performance (w.r.t a NLM trained without using their scheme) and data efficiency over a range of program induction tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed regularisation scheme appears logically sound and ultimately relatively simple to implement.\n- The authors demonstrate that it does improve performance and data efficiency on a range of tasks."
            },
            "weaknesses": {
                "value": "- The paper is written for a specialist audience, with little context or background given about NLP, about what the challenges in it are, about how ML methods can help solve it, or even just about what the tasks look like. In general, a formulation of the problem as a task that can be used to train a neural network on a certain experience and with a certain performance metric is omitted completely and taken for granted. As a result, most readers (as well as this referee) will not be able to get much from this paper.\n- Because of this, it's hard (though not impossible, see below) for me to assess the scientific content of this paper and the scope of its contribution to the field. Bit I nevertheless feel that the shortcomings on the presentation side are still sufficient to recommend that it be rejected or at least substantially revised.\n- The authors' method appears to work as intended, but also to be quite narrow in applicability: it only presumably works on NLMs (since it requires direct control of the weights) and only for the particular task considered by the authors. At first sight, the scope of the contribution appears very limited.\n- The authors refer to their scheme as a constraint on the weights, which seems to imply thet an hard constraint is imposed on them, whilst the scheme is more properly defined as a regularisation strategy, or a \"soft\" constraint."
            },
            "questions": {
                "value": "My main recommendation would be for the authors to provide context on NLP, NLMs, and on how their task can be solved via ML methods to begin with, and on how one would train a neural net to do so. Without such context, it's for me impossible to do justice to the content of this paper, but it is also going to be impossible for a mainstream ML reader (such as most of those attending ICLR) to profit from the paper to begin with.\n\nOn the contribution side, the authors should prove that their regularisation scheme does have any general applicability beyond NLP tasks carried out with NLMs.\n\nUnless both of these shortcomings can be suitably addressed, the implication will be that this work is more suited for publication in a specialised venue rather than a broad conference such as ICLR.\n\n## Post-rebuttal edit:\n\nThe authors' rebuttal did convince me that my assertion of the general applicability of their method was erroneous and that \"NLP tasks\" was a somewhat misleading designation on my part. The architecture that the paper's regularisation method is applied to (the Neural Logic Machine or NLM) can indeed be used for multiple reasoning benchmarks of interest, which the authors do in their submission. In reason of this, I have revised my contribution score to \"good\".\n\nI think that the papers still suffers from unclear writing, which results in the context of the author's work not being properly outlined for anyone not already familiar with it. The authors have not seemingly addressed these concerns in their revision, which is also missing markers to highlight changes w.r.t the original version.\n\nIn reason of this, while I do revise my score upwards, I still consider this paper to be below the acceptance threshold."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5038/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5038/Reviewer_YeEy",
                    "ICLR.cc/2024/Conference/Submission5038/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5038/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698694790149,
        "cdate": 1698694790149,
        "tmdate": 1700573847186,
        "mdate": 1700573847186,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KScbpSCbcE",
        "forum": "OzAGE2W9yz",
        "replyto": "OzAGE2W9yz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5038/Reviewer_JknR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5038/Reviewer_JknR"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a training modification to improve the training efficiency, generalization, and performance of neural models for inductive logic programming. Motivated by provenance-guided SAT-based synthesis techniques, the proposed technique stores an ongoing list which contains provenance information for each error experienced, in the form of the weight location that most contributed to an error. \nThen it applies a pattern mining algorithm to summarize the errors, and applies L1 regularization to the erroneous neurons during training. Experiments show that with this training modification, the existing neural ILP approach of Neural Logic Machines improves in performance, data efficiency, computational efficiency, and generalization."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The technique is quite novel, at least to me. I have never seen a technique of attempting to improve data efficiency of neural network training by cataloguing errors and penalizing their recurrence with L1 regularization. The success of a similar technique in SAT-based program synthesis motivates the technique well, and it appears this works due to the close connection between the NN architecture and the logical meaning of weight activations. \n- The improvements in performance seem significant! Performance is never decreased, and especially large improvements are seen in complex tasks and in data-scarce settings.\n- The paper is well-written.\n- The approach is simple and general enough that I would guess it has a good chance of successfully apply to other settings.\n- The experiments seem thorough: they cover both supervised and RL tasks, the same tasks covered in prior work, and additionally evaluate in both normal and data-scarce settings."
            },
            "weaknesses": {
                "value": "There are two main weaknesses to the paper: (1) there is not much attempt to understand why the technique works. (2) some of the writing in section 2 and 3 is not clear or fully explained. (1) is a much more important weakness, as (2) can be fixed pretty easily.\n\n(1) there is not much attempt to understand why the technique works.\n- Given how novel the approach is to me, I have a hard time forming a mental model connecting technique's description with the resulting performance. \n    - No ablations or comparisons of design choices, or even motivation given for the design choices.\n    - The paper would be stronger with experiments (could be toy) or examples that show how the approach is working, or a discussion comparing the usefulness of provenance information for SAT-based synthesis with how the provenance information is helping the neural models\n    - I think there should be more description in section 3 of why the technique works: I guess the model is  a bunch of logical combinations simultaneously during training, and due to the logical nature of the NN activations, it's possible to get information about \"incorrect search\" happening just how the SAT solver generates erroneous formula. \n   - see questions section for more questions attempting to understand and contextualize the technique.\n   \n(2) unclear writing\n- The description of NLM's in section 2.2 is not super clear. I'm unsure how much it can be improved while keeping the section short, but I would recommend working on it. In particular, the original NLM paper did a much better job by intuitively summarizing the approach before describing the details more formally. \n- The description of the approach in section 3 is poor. it leaves out numerous details, or mentions them unsystematically. things like \"what is an error?\", eq 1 is challenging to understand, the error list updating is never described clearly outside of the pseudocode\u2014 based on the pseudocode, you store a list of the T most recent errors, and once T are found, you run the pattern mining algorithm. and then you start over with a new list, right? this could be quickly described with a sentence in the text.\n- what does the Apriori algorithm return? a single thing? based off elsewhere, it returns a set of weights, but this should be described when Apriori is brought up.\n- Supplementary material could do a better job of describing the inputs and outputs of the model for the relational reasoning tasks and the rRL tasks. \n\nTo summarize, I have no criticisms of the technique, experiments, or results, but **I do wish there were more justification of the design choices, ablations, other experiments, discussion, or comparison to related techniques, to help understand why this approach works**."
            },
            "questions": {
                "value": "- How much of a correspondence is there between the form of the provenance information in this paper and RAGHOTHAMAN et al 2019?\n-  RAGHOTHAMAN et al 2019 provides guidance for both \"why\" an incorrect conclusion was derived and \"why not\" i.e. why a correct conclusion was not derived, but this work only provides guidance for \"why\". what about \"why not\" mistakes? \n    - To elaborate more, regularization of errors is never done with NN training in any other domain. Does this approach fail when applied to image classification? Both a yes or no answer would be quite illuminating... \n- Is there any related work of such types of techniques? (l1 penalty on negative examples, storing errors during training)\n\nAnswering these questions as well as those raised in the Weaknesses section regarding the lack of attempt to understand why the method works would improve my opinion of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5038/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5038/Reviewer_JknR",
                    "ICLR.cc/2024/Conference/Submission5038/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5038/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762717131,
        "cdate": 1698762717131,
        "tmdate": 1700495655759,
        "mdate": 1700495655759,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UciwHFbOhb",
        "forum": "OzAGE2W9yz",
        "replyto": "OzAGE2W9yz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5038/Reviewer_7KpG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5038/Reviewer_7KpG"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a regularization technique called FRGR that leverages a buffer of common recent mistakes during training. The method is implemented on top of neural logic machines (NLM), a neurosymbolic architecture for logic programs. Over two domains (logical reasoning and decision-making), FRGR shows improvements in performance over a vanilla NLM baseline both in the standard setting as well as a low-data regime."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The central idea to augment the training process using representations of past failures is interesting and fairly general, and could be applicable to many important domains."
            },
            "weaknesses": {
                "value": "The paper is unclear on a number of expository details. $\\theta_\\nu$ in Equation (2) should be given an explicit formula, as this is a key term in the regularizer. The notation is overloaded, for instance $m$ refers to both the size of $\\mathcal{U}$ in Section 2.2 as well as the matching factor in Section 3.2. I also could not find how # of iterations is defined in either the main paper or the appendix, despite this being one of the claimed improvements of FRGR over vanilla NLM.\n\nI also have some concerns about the experimental results. First, many choices of hyperparameters are not explored, such as the choice of the buffer size $\\tau$. It would be good to perform some ablation studies exploring the effect of this hyperparameter. As there is no theory to justify the regularizer, I think this raises the bar for empirical results. Second, the improvement in # of iterations appears not to be statistically significant, as the errors appear quite large. The improvements in Figure 4 appear quite marginal, again with overlapping error bars. \n\nFinally, the related work is missing a number of relevant lines of research, including hard negative mining ([1], i.a.) and experience replay ([2], i.a.).\n\n[1] Feng, Yu, et al. \"Program synthesis using conflict-driven learning.\" ACM SIGPLAN Notices 53.4 (2018): 420-435.\n\n[2] Continuous control with deep reinforcement learning. Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra. ICLR 2016."
            },
            "questions": {
                "value": "In Tables 1 and 2, what are units for the reported errors? Specifically, the entries are generally of the form x% +/- y, so is y a raw number or a percentage?\n\nHow is $\\theta_\\nu$ defined?\n\nHow did you determine when to stop training (for # of iterations)?\n\n===\n\nPost rebuttal I have increased my scores for soundness and presentation, as well as my rating from 3 to 5."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5038/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5038/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5038/Reviewer_7KpG"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5038/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799658652,
        "cdate": 1698799658652,
        "tmdate": 1700670972132,
        "mdate": 1700670972132,
        "license": "CC BY 4.0",
        "version": 2
    }
]