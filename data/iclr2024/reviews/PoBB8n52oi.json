[
    {
        "id": "goWrxaDWTQ",
        "forum": "PoBB8n52oi",
        "replyto": "PoBB8n52oi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5692/Reviewer_gqu7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5692/Reviewer_gqu7"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a linear-complexity based architecture as a replacement of self-attention for speech recognition to improve the efficiency. The proposed block has a local branch and a global branch to take both information into account. The local branch is based on a MLP, and the global branch has a MLP and then an average pooling among all the fram"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Paper fits ICLR scope well.\n* The idea of reducing model complexity through replacing self-attention with linear layers is interesting.\n* The proposed method is solid."
            },
            "weaknesses": {
                "value": "* Presentations need improvement. See question sections for details.\n* Literature reviews on ASR are very limited. Please have an individual paragraph in Section 1.1 to describe those.\n* Novelty is a concern. The proposed work is an incremental work on the top of BranchFormer and HyperMixer. Although there is a paragraph describing the relationship to HyperMixer, the difference seems to be more on the choice of linear functions. Please explain more on this.\n* The improvements from the proposed method seems to depend on the use of a transformer decoder (with self-attention). Need more experiments to verify the parity of the proposed method to self-attention in other conditions. See question sections for details."
            },
            "questions": {
                "value": "Abstract: \u201cHowever, attention layers in trained speech recognizers tend to not capture fine-grained pair-wise information.\u201d Please explain what \u201cfine-grained pair-wise information\u201d is and why it is important (although they are explained in the following sections, abstract should be self-explanatory).\n\nFigure 1: The diagrams are a bit confusing. I understand that \u201cT\u201d refers to the total number of frames. However, audiences may likely think of them as different attention heads. Please consider a better way for the diagrams.\n\nSection 2.1 - Multi-head SummaryMixing. Please better explain the heads in SummaryMixing. In self-attention, heads are operating in parallel with different sets of parameters. However, here heads help to reduce the number of parameters. Please consider adding a diagram on this if it helps. In addition, by dividing x_t into n chunks, is it similar to chuck-wise attention or attentions with a limited context window? If that\u2019s the case, please avoid using \u201chead\u201d here.\n\nSection 3.3.1: The results shown in this section have a transformer decoder of 6-blocks-MHSA paired with each system. In this setup, the decoder will capture the global context, even if the encoder doesn\u2019t. However, we can only conclude that self-attention is redundant when self-attention based decoders are used.\nThe authors also conducted experiments without a transformer decoder (i.e., CTC only) on Branchformer in the supplemental materials. However, there are two more problems to figure out. 1. How does vanilla Conformer/Transformer with CTC performance in this setup? They are the most common architectures for productions than Branchformer. 2. These results are obtained with LM shallow fusion, which can be expensive for deployment. How would the proposed approach perform without shallow fusion? If the language information is needed, please also consider RNN-T decoder."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5692/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5692/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5692/Reviewer_gqu7"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5692/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698101740720,
        "cdate": 1698101740720,
        "tmdate": 1700605294951,
        "mdate": 1700605294951,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "h3YnKoBBZt",
        "forum": "PoBB8n52oi",
        "replyto": "PoBB8n52oi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5692/Reviewer_yNSy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5692/Reviewer_yNSy"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel method to replace self-attentions with a more computationally efficient summary mixing model for speech recognition and spoken language understanding. Rather than computing an NxN attention matrix, the authors propose to first compute the temporal mean of the sequence and combine that with local information extracted using a cgMLP branch. Experiments on multiple corpora show that the proposed approach can match the performance of standard self-attention and Branchformer."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper uses a very simple approach to substitute self-attention with attention between a temporal summary and the original vector that appears to work well in practice.\n\n2. Experiments are performed on multiple corpora for ASR (CommonVoice-Dutch, Italian, French), Librispeech, AISHELL, TED-LIUM 2, GSC and SLURP for SLU."
            },
            "weaknesses": {
                "value": "1. The paper makes the assumption that MHSA in the encoder is not necessary based on Zhang et. al. and Peng. et. al. However, I believe the assumption may not be well supported because (a) Zhang et al point out that you don't need self-attentions in higher layers, not that you don't need self-attentions at all, and (b) Peng. et al which uses self-attentions to model global context while using cgMLP to model local context. Therefore, the rationale behind this fundamental assumption made in the paper seems unclear. \n\n2. SLURP uses read speech - the impact of temporal averaging here may be minimal, but may be very harmful in spontaneous conversational speech. Such explorations in my view are important in this work to claim that SummaryMixing can reach top-of-the-line in speech understanding. \n\n3. Writing could be more self-contained and clear in certain places - for example, HyperMixer is not well described which makes it hard to assess the relationship between the methods."
            },
            "questions": {
                "value": "1. The authors claim that a score of 0.5 diagonality implies a uniform distribution and therefore, a simple average could achieve a similar token mixing. However, this is not obvious or clear to me. Could the authors explain?\n\n2. I was interested to know how the performance of summary mixing is impacted during inference by temporal averaging. As the sequence length increases, the sum vector becomes a mixture of more frames, degrading the ability to discriminate individual frames/tokens. Do the authors have any numbers showing ASR performance (WER) as a function of sequence length across models with self-attention and the proposed SummaryMixing ?\n\n3. As SLURP uses read speech, I was wondering if the authors had performed experiments on other corpora with spontaneous speech, for example, SLUE-VoxPopuli [1] for Named Entity Recognition (NER)?\n\n4. The paper mentions KWS results, but I don't see any in the results section.\n\n5. There are some papers [2,3,4] that show linear transformers can obtain comparable performance to MHSA for ASR contrary to what the authors claim. These works are also relevant and must be acknowledged in the paper. \n\n[1] \"SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks\",\nSuwon Shon, Siddhant Arora, Chyi-Jiunn Lin, Ankita Pasad, Felix Wu, Roshan S Sharma, Wei-Lun Wu, Hung-yi Lee, Karen Livescu, Shinji Watanabe\n\n[2] Jingyu Sun, Guiping Zhong, Dinghao Zhou, Baoxiang Li, Yiran Zhong, \"Locality Matters: A Locality-Biased Linear Attention for Automatic Speech Recognition\"\n\n[3] \"Conformer-Based Speech Recognition with Linear Nystr\u00f6m Attention and Rotary Position Embedding\", \nLahiru Samarakoon, Tsun-Yat Leung\n\n[4] \"Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition\", Dima Rekesh et. al."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5692/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5692/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5692/Reviewer_yNSy"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5692/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698616019331,
        "cdate": 1698616019331,
        "tmdate": 1700609672440,
        "mdate": 1700609672440,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QHRErrrolL",
        "forum": "PoBB8n52oi",
        "replyto": "PoBB8n52oi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5692/Reviewer_fkrd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5692/Reviewer_fkrd"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a global summarization layer as a replacement for MHSA in several ASR architectures such as branchformer and Conformer. This layer or module is introduced and compared with several parallel implementation in an open source project SpeechBrain. \nThe newly proposed component dubbed summaryMixing has a linear computation cost on the encoder ASR components.\nExpectation is that this will close the gap with MHSA but actually authors found this surpasses it in their tests."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper proposes a generalization of hyperMixer and apply it to Speech tasks. \nThe generalization is simple but its effectiveness is surprising. \nThe proposed architecture works very well in comparison with many baselines and architectures (Conformer, Branchformer, ContextNet, Branchformer +fastattention, Branchformer w/o attention, ...)\nThe obtained and reported results are very competitive with various ASR benchmarks and setups.\nIt is worth noting the surprisingly good performance of one of the simplest baseline, the branchformer w/o attention, which authors acknowledge and highlight.\nThey consider encoder architecture only to show the unnecessary MHSA complexity for ASR tasks is not coming exclusively from the top decoder.."
            },
            "weaknesses": {
                "value": "There is a couple of points where the newly proposed component experimentation could have been improved.\nThe authors should clarify the ASR architecture in the final version since the current description of the ASR architecture is vague and prone to confusions-- see  questions below --.  I think this is something that authors might be aware of but prefer refer readers to the SpeechBrain recipes and experimentation.  This might leave a shallow reader without immediate interest in reproducing or delving into the code and recipes with a confusing architecture.\n\nThe approach has some limitations, so it would be intriguing to examine the performance of the proposed encoder in long-context ASR. \nSince this is a leaning representation conference, some experiments on NLP tasks, as in the original HyperMixer paper, would have attracted more readers.\n\nGiven the nature of speed optimization and comparison with MHSA, it would be interesting for the readers to know the specifics of the MHSA implementation as there are many very efficient implementations available nowadays.\n\nFinally, it would have been nice to compare the proposed approach in both batch and online ASR."
            },
            "questions": {
                "value": "The following questions might need some attention into the manuscript:\n* which is the global architecture and loss ? It is clear that it is a encoder decoder based architecture with CTC loss, but this is an important detail mentioned in 1 line. How many encoder layers each arch has ? which are the details of the decoder ? \n* Could the MHSA improve as the context becomes larger ? \n* which is the MSHA implementation have you used ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5692/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698880675714,
        "cdate": 1698880675714,
        "tmdate": 1699636595495,
        "mdate": 1699636595495,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bAK54Iydlj",
        "forum": "PoBB8n52oi",
        "replyto": "PoBB8n52oi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5692/Reviewer_wGqz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5692/Reviewer_wGqz"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel neural network architecture called SummaryMixing for speech recognition. The motivation of this work is to replace the heavy computational cost yielded by transformer self-attention blocks with a SummaryMixing block on top of the branchformer architecture. SummaryMixing shows effectiveness by offering a competitive performance from the original branchformer architecture, especially with the low computational cost, mainly within the CTC framework."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Simple and yet another neural network architecture by extending the branchformer architecture to capture global characteristics via SummaryMixing and local characteristics via cgMLP.\n- Showing the low computational cost compared with the original branchformer\n- Good reproducibility (open source implementation and the use of public data)."
            },
            "weaknesses": {
                "value": "- The survey of the efficient transformer (conformed) is not sufficient. There are a lot of efficient transformers in the NLP and ML field (e.g., https://arxiv.org/pdf/2009.06732.pdf). Even if we limit the discussions to speech applications, there are a lot of methods (e.g., squeezeformer, efficient conformer, emformer, etc.). \n- The performance improvement from the conventional method (e.g., fastformer) is marginal.\n- The method is on top of branchformer, which already uses Fastformer, and its technical novelty is weak.\n- Several descriptions (e.g., the surveys, distinction from HyperMixer/MLP Mixer, and detailed architectures) are unclear.\n  - Frankly, I could not understand the distinction of the SummaryMixing block from HyperMixer/MLP Mixer only from Section 2.1, mainly due to the lack of an explanation of what HyperMixer/MLP Mixer is.\n  - It is difficult to understand the detailed architectures only from this description: \"In particular, the transformation (f), summary (s), and combiner (c) functions are all implemented as a dense linear layer followed by a GeLU activation function.\" Similarly, how the model size of the proposed and other methods is adjusted was unclear."
            },
            "questions": {
                "value": "- Can you expand the discussion of how this novel architecture and findings would attract the general AI and ML researchers in ICLR? This paper specializes in speech recognition (and spoken language understanding, which is a very similar task to speech recognition), and it is a narrow scope for me.\n- Besides the above expansion, can you explain why you selected Fastformermer and ContextNet?\n- Can you apply this to RNN-T?\n\nOther minor comments\n- Abstract: ASR --> automatic speech recognition (ASR), speech understanding --> spoken language understanding\n- Page 3, last paragraph \"Hence, $X \\in \\mathbb{R}$ becomes $X \\in \\mathbb{R} ^{T \\times D/n}$\": Is $X \\in \\mathbb{R}$ scalar? I think you missed adding some domains."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5692/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699212530067,
        "cdate": 1699212530067,
        "tmdate": 1699636595390,
        "mdate": 1699636595390,
        "license": "CC BY 4.0",
        "version": 2
    }
]