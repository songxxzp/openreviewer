[
    {
        "id": "HGMcUqe3i1",
        "forum": "jnZtTUdWyi",
        "replyto": "jnZtTUdWyi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6413/Reviewer_VfLk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6413/Reviewer_VfLk"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the continuous domain generalization problem (also named non-stationary DG setting in the paper), and proposes the  adaptive invariant representation learning algorithm to solve it. The model uses a transformer layer and a LSTM model to capture the evolving patterns between time steps. The experimental results validate the proposed method. However, the non-stationary DG setting is not a new one and there lack some literatures in this paper, and the theoretical analysis seems trivial, which together greatly lower the technical contribution of this work."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "After carefully reading this paper, I think the strengths mainly lie in the design of the model.\n\n1. The authors propose a novel model architecture to handle the continuous DG problem, which contains a transformer layer and an LSTM model to capture the time-dependent patterns. \n2. The experimental results are nice to validate the proposed method, which involves many datasets and baselines."
            },
            "weaknesses": {
                "value": "I have several concerns regarding to the theoretical analysis, the model design, and the experimental results.\n1. The theoretical analysis is trivial and there is little novelty in it. \n* Firstly, the Theorem 4.5 is a conventional generalization bound via Rademacher Complexity, and the constant term $C$ is the **upper bound** of the loss function, which means the upper bound is quite loose and not quite meaningful to guide the design of methods. That is, if the upper bound is too loose, one could add any term to it that is consistent to the method. Also, the meaning of $K$ is not demonstrated in the paper. \n* Secondly, Proposition 4.6 is also trivial, and the reweighting method that it inspired has little relationship with invariance. \n2. The model design: \n* Firstly, why the alignment of distributions could lead to invariance? The authors did not formally define the invariance property and it seems vague.\n* Secondly, it seems that the designed model has a lot of computation burden, could the authors analyze it or provide some empirical analysis/results on this? For example, extra running time.\n3. The experiments: the authors did not report the variance of different runs. Further, the validation protocol is not reported."
            },
            "questions": {
                "value": "Please refer to Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6413/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698861193856,
        "cdate": 1698861193856,
        "tmdate": 1699636714393,
        "mdate": 1699636714393,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DTrnsp9wGA",
        "forum": "jnZtTUdWyi",
        "replyto": "jnZtTUdWyi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6413/Reviewer_vTu8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6413/Reviewer_vTu8"
        ],
        "content": {
            "summary": {
                "value": "The paper delves into the challenges associated with Domain Generalization (DG) under non-stationary environments. The authors investigate the effects of such non-stationary environments on model performance and provide theoretical upper bounds for errors when models are applied to target domains. To address the identified challenges, they introduce a new algorithm that is rooted in invariant representation learning. This algorithm uses the observed non-stationary patterns to develop a model that is expected to evolve and achieve better performance on target domains. The paper validates the effectiveness of the proposed algorithm through experiments conducted on both synthetic and real-world data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The article presents an innovative approach to tackle non-stationary domain generalization, a notable hurdle in practical scenarios. It delivers an extensive analysis of the associated difficulties and meticulously evaluates the impact of environmental changes over time.\n- A major academic contribution of this work is the formulation of theoretical upper limits for model error, lending a robust theoretical underpinning to their methodology.\n- The effectiveness of their method is validated with both synthetic and real data, showcasing its potential real-world applications."
            },
            "weaknesses": {
                "value": "- The evaluation section lacks significance testing and does not report standard deviations in the results table. Given the close performance across many results, the inclusion of standard deviations is crucial to affirm the method's efficacy.\n- The datasets and networks employed in the study are somewhat limited in size. The paper does not address scalability, leaving questions about the method's performance with larger datasets or more complex network architectures.\n- While the theoretical analysis provided is compelling, the practical implementation details in Section 5 are complex and challenging to comprehend, even with the pseudo-code. A more detailed and clearer illsutration would greatly enhance understanding."
            },
            "questions": {
                "value": "Please refer to the weaknesses mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6413/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698994079061,
        "cdate": 1698994079061,
        "tmdate": 1699636714275,
        "mdate": 1699636714275,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gh5HZg4QRQ",
        "forum": "jnZtTUdWyi",
        "replyto": "jnZtTUdWyi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6413/Reviewer_6RJQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6413/Reviewer_6RJQ"
        ],
        "content": {
            "summary": {
                "value": "This paper considers a non-stationary domain generalization problem.  Authors first establish theoretical upper bounds for the model error at target domain, and then leverage the non-stationary pattern to train a model based using invariant representation learning. Experiments show some improved results over existing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- A more general setting regarding non-stationary DG problem.\n- A novel invariant algorithm is proposed based on theoretical bounds.\n- Improved empirical results across a wide range of datasets."
            },
            "weaknesses": {
                "value": "- Description of the setting is not clear and the key assumption seems strong.\n- Proposed algorithm seems to be ad-hoc and complicated.\n- Presentation can be made more concise."
            },
            "questions": {
                "value": "I have some questions regarding the problem setting and assumptions:\n\n- What's the difference between this problem's setting and the IRM's where it is assumed that some invariance exists across domains? I can see that some datasets you use (like RMNIST) falls into the IRM's setting. A related question is, why do you consider learning invariant representations using only two consecutive domains?\n\n-  And do you implicitly assume that the domain indexes and their order are given? If so, I think this setting is limited in this sense.  Note that most benchmark methods do not utilize the order information. Please clarify. \n\n- **Major Concern**: It is stated that \"We note that Assumption 4.3 is mild because it is required only for the optimal mechanism M\u2217 . This assumption implies that there exists at least one hypothesis in M under which the non-stationary patterns learned from source can generalize sufficiently well to the target (with bounded $\\Phi$ ).\"\n\n  - regarding the criterion $\\Phi$: I don't think Definition 4.1 necessarily implies a good estimate of the mechanism $M$. That is, $\\hat M$ can be bad for all domain pairs, so that each $D$ inside the $|\\cdot|$ is large but the difference is small. Perhaps some discussions shall be added here.\n  - More importantly, I would like to think that Assumption 4.3 is rather strong. Note that it is NOT equivalent to assuming that there exists  a pattern in the hypothesis space; here it is a \"specific\" one, which minimizes the divergence of observed datasets, and could provide an almost optimal estimate of an unseen domain. If so, this would be a strong assumption on the relationship of observed domains and unseen domains. Please clarify.\n\n- Minor: Section 5 is hard to follow. Please be more concise. \n- Experiments: how many domains are used for training? And what if the domain index is re-ordered? \n\n\n\nOverall, I think this paper has some interesting and useful contributions. I am happy to increase my evaluation if authors can address my concerns."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6413/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699024216079,
        "cdate": 1699024216079,
        "tmdate": 1699636714160,
        "mdate": 1699636714160,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "y5cqrvG2Vw",
        "forum": "jnZtTUdWyi",
        "replyto": "jnZtTUdWyi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6413/Reviewer_Uk8r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6413/Reviewer_Uk8r"
        ],
        "content": {
            "summary": {
                "value": "Many machine learning algorithms work based on the assumption that training and test data are sampled from IIDs. However, this is commonly violated in real-world cases as the data distribution may shift between train and test times. It has encouraged many researchers to develop techniques such as domain generalization and domain adaptation. However, such methods cannot accommodate the case that the data distribution changes over time based on a mechanism. To tackle such a problem, this paper studies domain generalization in a non-stationary environment, which aims to learn a model from a sequence of source domains that can capture the non-stationary patterns and generalize to unseen target domains. The authors examined the impacts of non-stationary distribution shifts and investigated how the model learned on the source domains performs on the target domains. Experiments were done on simulated, semi-synthetic, and real-world datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The problem this paper addresses is interesting. Multiple domains evolving over time is a realistic scenario but has hardly been studied before. Also, the authors demonstrated that the existing typical multiple-source DG/DA methods \n- The problem setup is more flexible than those of the related works as it allows the modeling of non-stationary dynamics and can be applied to multiple unseen target domains.\n- Well-written and easy to follow."
            },
            "weaknesses": {
                "value": "- Definition of $\\Phi$ is a bit ambiguous. In Definition 4.1 and Remark 4.2, $\\Phi(\\widehat{M})$ consists of an average error of $\\widehat{M}$ and on the source domains and error of $\\widehat{M}$ on the target domain. It doesn't consider whether each error is low enough but only considers their difference. Thus, based on the definition, $\\widehat{M}$ is generalizable even if $\\widehat{M}$ has high errors on both source and target domains. I am unsure if such $\\widehat{M}$ is generalizable.\n- Moreover, Assumption 4.3 tells us that we can find a decent $\\widehat{M}$ with a small error on the source domains but seems to tell nothing about the error on the target domain. I am unsure if I understood this statement correctly, but it doesn't seem like Assumption 4.3 implies that the error of $M^{*}$ on the target domain is small. I think this is one of the key statements of this study so it needs to be clarified.\n- The authors need to add an uncertainty metric to Table 1."
            },
            "questions": {
                "value": "- I appreciate the theoretical analysis and experimental evidence that the authors presented. Could the authors further provide under what condition the proposed approach will have guaranteed improvement compared to either the ERM or some conventional DG method?\n- I think the FMoW dataset from the WILDS benchmark aligns with the problem setup in the paper. (The authors already included sufficient experimental results, so I do not request the authors to add an additional dataset.)\n- The authors may want to add some dotted lines to Table 1 to differentiate different approaches - ERMs / conventional DA/DGs, ..."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6413/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6413/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6413/Reviewer_Uk8r"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6413/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699114849325,
        "cdate": 1699114849325,
        "tmdate": 1699636714030,
        "mdate": 1699636714030,
        "license": "CC BY 4.0",
        "version": 2
    }
]