[
    {
        "id": "UXcNVQI15q",
        "forum": "AcSChDWL6V",
        "replyto": "AcSChDWL6V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5555/Reviewer_MBi3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5555/Reviewer_MBi3"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the expressiveness of graph transformers and message-passing gnns. As its first step, the authors exploit the universality of MLP and the property of positional encoding, i.e., injective up to isomorphism, to prove the non-uniform universality of graph transformer and message-passing gnns. Then, the scope of discussion is extended to uniform expressivity, that is, whether these kinds of neural architectures can approximate arbitrary function no matter how large the input graph is. Basically, the authors show that both graph transformers and message-passing gnns are not universal approximator in this setting. Moreover, they offer important insight that these two kinds of neural architectures do not subsume each other. Specifically, graph transformers cannot perform unbounded aggregation, yet its attention mechanism allows asymmetric weighting of incoming messages. Accordingly, the authors design and conduct experiments on synthetic datasets to validate their theoretical results. On practical datasets, these models are also comparable, especially the virtual node trick, which helps a lot for message-passing gnns. In summary, this paper tells the community that attention is NOT all you need in graph learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThis paper is well-written. I can effortlessly pick the main points up.\n2.\tThe theoretical results introduced in this paper seem to be crucial for developing machine learning models dedicated to graph data. Notably, these results explain some interesting phenomena emerging in recent years, including the superiority of graph transformers in some competitions, the surprising usefulness of virtual node trick, and the existence of some real-world datasets on which message-passing gnns are still state-of-the-art.\n3.\tThe experiments are convincing, where the difference between these two kinds of neural architectures is remarkable."
            },
            "weaknesses": {
                "value": "1.\tIt seems that the presented theoretical results in the non-uniform are relatively trivial, as they are straightforward results of the combination of MLP\u2019s universality and PE\u2019s discrimination capacity.\n2.\tThe difference and respective advantages deserve to be connected to practical tasks on molecular graphs, as there have been many public tasks, some of which graph transformers outperform traditional message-passing gnns, yet some are not. Such connections must be helpful for the community and make the theoretical results practical."
            },
            "questions": {
                "value": "In the synthetic experiments, the authors said they were interested in the generalization behavior of the train models. However, the setting is not as usual. It is not an i.i.d. generalization but o.o.d. extrapolation (graph size <= 50 during training and > 50 in test). Generally and intuitively, a model that is more sophisticated with o.o.d. extrapolation is often due to its more reasonable inductive bias or limited hypothesis space, such that it captures the underlying actual mapping rather than fitting the training data by other consistent yet different mappings. Thus, I need clarification about the rationale behind experimental design. Could you explain this to me?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5555/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697683771845,
        "cdate": 1697683771845,
        "tmdate": 1699636571046,
        "mdate": 1699636571046,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AQrtW4KCW8",
        "forum": "AcSChDWL6V",
        "replyto": "AcSChDWL6V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5555/Reviewer_EnRw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5555/Reviewer_EnRw"
        ],
        "content": {
            "summary": {
                "value": "Summary: This work can be contextualized along a recent line of study in graph learning which is focused on comparing graph transformers (GTs) and message passing GNNs (MPGNNs) and finding out which is better and why. This paper in particular theoretically and empirically compares the expressive power of GTs and MPGNNs with virtual nodes (MPGNN+VNs) in the uniform setting where a single model must work for graphs of all sizes. It shows that neither model is uniformly universal, but they can express some unique functions, making their expressive power incomparable. \n\nthe paper's contributions:\n- Presents important insights that can be useful to understand the working capabilities of GTs and MPGNNs.\n- Proves that GTs and MPGNN+VNs cannot uniformly approximate every computable graph function, even with polynomial-time positional encodings.\n- Shows GTs cannot uniformly approximate unbounded summation like |V|2, while MPGNN+VNs can.\n- Shows MPGNN+VNs cannot uniformly approximate functions exploiting softmax attention's asymmetric neighbor weighting, while GTs can."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strengths:\n- The paper makes an important theoretical contribution by formally proving distinguishing functions for GTs and MPGNN+VNs. This helps characterize their expressive power, and informs more about the strengths and weaknesses of these two model class in addition to what is known in recent literature (eg Cai et al., 2023). \n- The proofs identifying unique functions are non-trivial and provide insight into the core operations enabling GTs and MPGNN+VNs to express different functions.\n- The theoretical findings are verified through special designed experiments on synthetic data, showing the distinguishing functions are learnable in practice.\n- Experiments on real-world benchmarks demonstrate MPGNN+VNs can be competitive with GTs in some cases, due to global communication via virtual nodes. however, this is known in the literature, to the best of my understanding"
            },
            "weaknesses": {
                "value": "Limitations and Questions:\n- The theoretical analysis focuses on comparing one variant of GTs (GPS) and MPGNN+VNs. Results could vary for different architectures within these families. How accurate would this generalization be?\n- On some realworld datasets, MPGNN+VNs do not fully close the performance gap compared to GTs. It is unclear if this limitation is fundamental or if deeper MPGNN+VNs could match GTs."
            },
            "questions": {
                "value": "in the Weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5555/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698485819174,
        "cdate": 1698485819174,
        "tmdate": 1699636570932,
        "mdate": 1699636570932,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cCyLBEc8fo",
        "forum": "AcSChDWL6V",
        "replyto": "AcSChDWL6V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5555/Reviewer_BMpU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5555/Reviewer_BMpU"
        ],
        "content": {
            "summary": {
                "value": "This paper comprehensively compares the expressivity of Graph Transformers and Message-Passing GNNs. \n\nThis paper presents the following conclusions:\n\n1) Neither Graph Transformers nor Message-Passing GNNs are universal in the uniform setting. \n\n2) There are functions that Graph Transformers can express while Message-Passing GNNs with virtual nodes can not.\n\n3) Even with perfect positional encoding, the expressiveness of Graph Transformers and MPGNNs with virtual nodes differs substantially. \n\nThis paper conducts experiments on real data and synthetic data to verify the theoretical analysis proposed in the paper.\n\n---- After rebuttal---\nThanks for the authors' response. My main concern is strong assumption of the uniform setting and the experimental results.\n\u00a0\nFor the strong assumption of the uniform setting, the author explained that\n\ufeffthe uniform setting is a good representation of scenarios because the graph sizes at\ntraining time are smaller than the graph sizes during inference. I generally agree with the author's response.\n\u00a0\nFor the novelty. The author reclaimed their novelty, which is not summarized (or even mentioned) in the introduction. Now, the author summarize this paper's novelty about the theoretical provements and the proposed synthetic data. I agree these two thing are new. However, the experimental results on real data still exists. The authors can carefully fix the minor mistakes or typos in their revised version.\n\u00a0\nGiven the promise that the author will add the detailed proof and more analysis on the real dataset, I raise the score from reject to broadline accept."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) This paper relates two common designs in graphs, i.e., graph transformer and message-passing GNN. \n\n2) This paper gives a theoretical analysis of the capabilities\tof two basic GNN designs and conducts comprehensive experiments to verify the theoretical analysis.\n\n3) The study provides insightful results that add depth to our understanding of the expressiveness of Graph Transformers and MPGNNs in scenarios with optimal positional encoding.\n\n4) The differentiation in the capabilities of Graph Transformers and MPGNNs is well highlighted, offering clarity on their respective strengths and limitations."
            },
            "weaknesses": {
                "value": "The important concern is the writing. Although this is primarily a theoretical paper, it does not express its flow of proof clearly. I recommend the author to improve their writing.\n\n1) The most important weakness of this paper is writing. The introduction is not easy to understand.\n\n2\uff09**Lack of Justification**: The paper focuses on scenarios where positional encoding is injective. However, there is a noticeable lack of justification for why this particular scenario is important or realistic. To ensure that the results derived hold value in practical applications, it is essential to provide a clear context and relevance for the chosen scenario.\n\n3) The process of proof is not so clear. For example, in Section 4.2, the author attempts to prove GPS do not subsume MPGNN+VNs. However, I can not understand the main path of proof of Theorem 4.3 and Corollary 4.4.\n\n4) The strong assumption of uniform setting made in the paper may not align with the practical case.\n\n5) **Redundancy in Experimental Results**: Similar experimental outcomes have been presented in multiple prior works, notably:\n    - T\u00f6nshoff, Jan, et al. \"Where did the gap go? Reassessing the long-range graph benchmark.\" arXiv preprint arXiv:2309.00367 (2023).\n    - Cai, Chen, et al. \"On the connection between MPNN and graph transformer.\" arXiv preprint arXiv:2301.11956 (2023).\n   While building upon prior work is a hallmark of research progression, it's crucial to ensure that the presented findings either provide a novel perspective or build significantly upon the existing literature.\n\nGiven the value of the derived results, this paper can contribute substantially to the field with some revisions. I recommend a clear justification for the chosen scenario of positional encoding and a distinction between the presented findings and existing literature. This would fortify the paper's novelty and relevance, making it a more significant contribution to the domain."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5555/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5555/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5555/Reviewer_BMpU"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5555/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762141304,
        "cdate": 1698762141304,
        "tmdate": 1700806610246,
        "mdate": 1700806610246,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wZZM28EbuX",
        "forum": "AcSChDWL6V",
        "replyto": "AcSChDWL6V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5555/Reviewer_Uru7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5555/Reviewer_Uru7"
        ],
        "content": {
            "summary": {
                "value": "This paper compares model expressivity between graph transformers (GT) and massage passing GNNs with virtual nodes (MPGNN+VN). The authors theoretically demonstrate that GT and MPGNN+VN are universal function approximators on the graph under uniform setups, where different neural networks can be utilized for every graph size. However, under the non-uniform case where one neural network is supposed to work for all the graphs, both are not universal approximators and express different sets of functions, indicating they do not subsume each other. The authors also conduct numerical experiments to validate the theoretical findings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "First of all, thank the authors for their submission to ICLR. This paper offers valuable insights into the expressive power of graph neural networks. Specifically, it exhibits the following notable strengths:\n(1)\tThe authors provide a novel and insightful comparison between Graph Transformer and MP-GNN (with VN) in both non-uniform and uniform setups. The latter setup, in particular, marks the first time it has been explored from this perspective, revealing that GT and MPGNN-VN do not subsume each other.\n(2)\tThe paper includes numerical experiments that complement the theoretical insights. The authors also attach the code, which will benefit the community. \n(3)\tLast but not least, the paper is well organized, and the writing is straightforward, so it is easy to follow even though the underlying proof is remarkable."
            },
            "weaknesses": {
                "value": "Despite the strengths listed above, there are still areas in the paper where improvements can be made to enhance clarity and overall significance.\n(1)\tRegarding the writing: \na.\tThe paper utilizes many acronyms, which may necessitate repeated explanations. For instance, in Figure 1, it would be helpful if the author could reiterate the meanings of \"MP,\" \"VN,\" \u201cSA,\u201d \u201cPH,\u201d and \"FF\" in the figure caption. Additionally, in Figure 2(b), clarifying the definitions of \"l\" and \"r\" would enhance writing clarity. It is also recommended that the authors create a table in the appendix with all the acronyms. \nb.\tThe utilization of some math symbols may confuse and misleading. For example, in subsection 4.3, the \u201clr\u201d represents \u201cl*r\u201d instead of learning rate or number of layers. \nc.\tThere are some missing values in Table 1, which may be better to illustrate the reason in the table caption in addition to the other place. \nd.\tSome minor typos and grammars, such as \u201cBased on the the positional encoding LapPE\u201d.\n(2)\tRegarding the theory and numerical experiments:\na.\tThe assumptions and limitations of the proposed theory are somewhat unclear. For instance, it is not clearly defined whether the theory is applicable to various graph tasks or solely focused on graph classification tasks.\nb.\tThe message conveyed in Section 5.2 is also vague and disconnects with previous sections. Given that Table 1 indicates that the best performance appears somewhat random, it is not convincing how the theoretical insights can guide the practice. It may be valuable to conduct a more thorough investigation into the utilization of theoretical insights for model selection based on the characteristics of the dataset."
            },
            "questions": {
                "value": "The questions are mainly related to the \u201cweakness\u201d:\n(1)\tPlease add a list of math symbols and abbreviations in the appendix and clarify the above writing questions.\n(2)\tDoes the theory also satisfy different graph learning tasks?\n(3)\tHow can we utilize theory to understand the results of section 5.2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5555/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699315937641,
        "cdate": 1699315937641,
        "tmdate": 1699636570760,
        "mdate": 1699636570760,
        "license": "CC BY 4.0",
        "version": 2
    }
]