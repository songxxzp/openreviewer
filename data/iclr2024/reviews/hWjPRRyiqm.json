[
    {
        "id": "L5coNxyTZZ",
        "forum": "hWjPRRyiqm",
        "replyto": "hWjPRRyiqm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1338/Reviewer_wVRo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1338/Reviewer_wVRo"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces EZ-CLIP, an efficient zero-shot video recognition model, which incorporates temporal visual prompting into the pretrained vision-language model CLIP to capture video motions. During model training, the newly added temporal module is the only component that is learned, making it an efficient approach. Additionally, a motion loss is introduced to enhance diversity and distinctiveness among frames, thus improving model learning. The paper's results on multiple datasets demonstrate its strong performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper effectively communicates its motivation and methodology. It highlights the key idea of utilizing the image-based visual-language model CLIP and seamlessly integrating temporal visual prompting for video action recognition.\n\nThe model's simplicity and clarity make it easily understandable. Moreover, the paper provides compelling evidence of its superiority over alternative methods through results on various benchmarks."
            },
            "weaknesses": {
                "value": "1. The authors might overstate the claim that \"temporal adaptation techniques require a lot of parameters with fine-tuning of the whole model which is not desirable for preserving generalization capability.\" Both AIM [1] and ST-Adapter [2] employ a similar learning approach by freezing CLIP parameters and only tuning newly added adapter layers to learn temporal relations within video frames. It's crucial to maintain accuracy in comparative statements.\n\n[1] AIM: Adapting Image Models for Efficient Video Action Recognition, https://arxiv.org/abs/2302.03024\n\n[2] St-adapter: Parameter-efficient image-to-video transfer learning, NeurIPS, 2022\n\n2. The used baseline models are two adapters AIM and LoRA-FA (the last paragraph of Section 3.2). The paper's baseline model utilizes the spatial adapter only, which makes it less comparable to methods like AIM that incorporate both spatial and temporal adapters. Including both spatial and temporal adapters in the baseline would provide a more accurate basis for comparing the proposed temporal visual prompting.\n\n3. Table 5 demonstrates the substantial positive impact of the LLM-generated action class descriptions on action recognition. It suggests that the improved performance of EZ-CLIP may primarily stem from the use of these descriptions rather than the introduction of temporal visual prompting. For example, EZ-CLIP wo LLM-description only achieves 77.3% HM result on UCF-101, while ViFi CLIP which does not use LLM (I checked their paper, if I did not miss something) obtains the higher result of 78.3%. As such, the better performance of EZ-CLIP may mainly come from the utilization of LLM.\n\n\n4. The proposed EZ-CLIP employs 8 frames for efficiency, while other methods use 32 frames (a 4x difference). As such, the comparison in Table 6 may not be appropriate.\n\n5. It's worth noting that K-600 is an extension of K-400 and shares video categories. This overlap should be taken into account when doing zero-shot learning."
            },
            "questions": {
                "value": "Please see my comments in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1338/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1338/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1338/Reviewer_wVRo"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1338/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698661198697,
        "cdate": 1698661198697,
        "tmdate": 1699636061146,
        "mdate": 1699636061146,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YjxirSmkNs",
        "forum": "hWjPRRyiqm",
        "replyto": "hWjPRRyiqm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1338/Reviewer_y6Hw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1338/Reviewer_y6Hw"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to adapt CLIP model for video tasks specifically for action recognition in various generalization bench-marking settings. Specifically, two main contributions are proposed to improve CLIP performance on videos. Firstly, temporal visual prompt (TVP) learning is introduced which are learned at the vision encoder. TVP are layer wise prompts that are uniquely added to mean of each frame features and then self-attention mechanism allows the model to learn temporal contexts. Secondly, the authors propose a motion loss, that encourages maximum diversity in terms of variance and pair-wise difference among video frames. This encourages the model to learn distinct embeddings for each frame based on its motion information. \n\nThe performance of model is shown across various generalization benchmark settings which it shows improvements over the prior methods with less compute and trainable parameters. Ablations are conducted to show the effect of each component, which provides a broader perspective on the novelty."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The idea of keeping spatial model frozen and using specific modules and loss functions to improve temporal modeling is encouraging. This allows the community to easily analyze the contributions by the proposed component only. This also results in very light weight adaptation which allows the model to be trained on very less compute resources.\n\n2) The proposed techniques including temporal prompt tuning and motion loss are fairly motivated. Their individual importance has been further validated with proper ablation studies.\n\n3) The proposed framework shows reasonable improvements over the prior methods. Further analysis like tsne plots as well as per-class results shows more accurate effect due to the proposed techniques.\n\n4) Paper is easy to read and well written."
            },
            "weaknesses": {
                "value": "1) The proposed overall framework seems to be heavily relied on additional training modules and tricks like LLM based prompt ensembling and spatial-language adaptors. However, there is very little detail given on these modules. For example, what kind of adaptors are used, at which place they are incorporated in the model, how many prompts are being used from LLM, illustrations of LLM prompts? It will be better to provide a high level figure diagram which also shows the usage of these additional components. \n\n2) It will be good to see effectiveness of the proposed approach on (i) larger CLIP models like ViT Large or Huge (ii) and on any other CLIP variant (e.g EVA-CLIP) which would confirm the generalization of the proposed approach towards other model scales and recent VL models other than CLIP.\n\n3) I am not completely sure but I think there might be something wrong in the ssv2 results for base-to-novel generalization setting. EZ-CLIP is achieving around 54% and 20.6% accuracy for 16 shots samples. But the same EZ-CLIP is performing relatively poor in few-shot setting where it also uses 16 shot samples. Can the authors revisit the experiments and confirm that they are using the correct split for base-to-novel setting for ssv2? I am afraid it can be the case that complete ssv2 training data is used instead of 16 shots. \nOtherwise the few-shot results should have similar scale results for the proposed approach."
            },
            "questions": {
                "value": "Please refer to the weaknesses section for my queries."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1338/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1338/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1338/Reviewer_y6Hw"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1338/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811072518,
        "cdate": 1698811072518,
        "tmdate": 1699636061059,
        "mdate": 1699636061059,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wPms2JGKol",
        "forum": "hWjPRRyiqm",
        "replyto": "hWjPRRyiqm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1338/Reviewer_PcJL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1338/Reviewer_PcJL"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the issues of adapting image-based visual-language models to the video domain and proposes solutions. It introduces a prompt-based temporal modeling method to reduce computational complexity when modeling temporal relationships. Additionally, it designs a motion loss to capture the intrinsic relationships between frames, which existing methods often overlook."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper proposes a prompt-based approach for establishing temporal relationships between different frames in image models.\n\n+ Existing contrastive losses neglect the intrinsic properties of videos. The motion loss is designed to enable the model to learn motion information between frames.\n\n+ The method is simple but has been validated on multiple settings and datasets, achieving state-of-the-art (SOTA) performance.\n\n+ The proposed method outperforms other methods in terms of GFLOPs, throughput, tunable parameters, and total parameters."
            },
            "weaknesses": {
                "value": "- The contribution of the paper is limited due to similarities with the Visual Prompt Multi-Modal Tracking [A] approach. Both methods utilize prompts and transformer outputs to generate new prompts interactively. The difference lies in this paper's method of averaging patches from each frame and adding them to the prompt, incorporating information from the current frame. From this perspective, the innovation appears somewhat lacking.\n\n- The paper lacks sufficient ablation experiments, as it only conducts them in the \"Base to novel generalization\" setting.\n\n- Based on the ablation experiments in Table 4 and Table 5, combined with the results in Table 2, it can be observed that even without the proposed TVP and Motion loss, adding only the adapter achieves better results compared to existing methods. Hence, there are concerns regarding the fairness of the comparison with SOTA methods.\n\n[A]  Visual Prompt Multi-Modal Tracking. CVPR23"
            },
            "questions": {
                "value": "- The ablation experiments of LLM were only conducted in the \"Base to novel generalization\" setting. Were LLM used in other settings? If so, what were the results when LLM was not used?\n- The paper mentions the use of text adapter and spatial adapter, citing different papers. However, it is not clear where they are incorporated in Figure 2. Could you clarify where they are added?\n- In Table 6, the proposed method has a Tunable params value of only 5.2M. However, the cited Aim's spatial adapter has 3.7M tunable parameters, and LoRA has a minimum of 1.8M. Could you provide the specific breakdown of the tunable parameters for each component?\n- For the prompt in this paper, it is added to each layer of the transformer. Has there been an experiment where the prompt is only added to the first layer?\n- Motion loss: In Equation (5), the central difference C is computed using the embeddings of the previous and next frames. However, in action recognition tasks, adjacent frames are often very similar. Is this loss calculation effective? Could you explain in detail the role of motion loss?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1338/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698841002698,
        "cdate": 1698841002698,
        "tmdate": 1699636060980,
        "mdate": 1699636060980,
        "license": "CC BY 4.0",
        "version": 2
    }
]