[
    {
        "id": "iq0ZufECrO",
        "forum": "Kz3yckpCN5",
        "replyto": "Kz3yckpCN5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4752/Reviewer_WxVs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4752/Reviewer_WxVs"
        ],
        "content": {
            "summary": {
                "value": "The authors critically investigate the promise of finetuning language models using (imitation) data obtained from more capable language models(LMs). The paper challenges the assumption that this process improves an LM overall \u2013 but suggests that this is rather `mimicking their style`. They propose several interesting findings around what aspects of the performance improve or deteriorate upon tuning LMs on imitation data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I overall support the messages and findings in the paper.\n\n1. I appreciate this timely and critical investigation. Finetuning existing LMs on instruction-tuning data from more capable LMs has swiftly become common practice, yet we do not fully understand the change in behavior. The paper raises several critical questions that I would personally appreciate having in the literature.\n\n2. One novel and important finding is that the authors find that even though training on imitation data improves the results in crowd worker evaluations, they observe even a degradation in factuality. This is a significant consideration that should be kept in mind when finetuning models. This further raises a question about what other aspects of capabilities may be fluctuating during imitation data training.\n\n3. On the other hand, they inherit some of the useful properties, such as reduced toxicity / being more safe. Again, this further informs us about what really happens when models are trained on imitation data. It would be compelling to further explore a more fine-grained decomposition of performance gains and losses upon training on imitation data.\n\n4. The experiments are large-scale and informative yet not cheap to perform, thus findings enable valuable conclusions that are otherwise not easy to draw."
            },
            "weaknesses": {
                "value": "There are 2 main points that I am concerned about.\n\n1. (IRB Approval / Exemption) The human study in the paper does not seem to have the relevant IRB approval or an exemption. The code of ethics states `Where human subjects are involved in the research process (e.g., in direct experiments, or as annotators), the need for ethical approvals from an appropriate ethical review board should be assessed and reported.` I am deferring the judgment about this to Ethics Reviewers / ACs. This should have most probably been done before the human subject study, but I encourage the authors to swiftly go through the IRB process for clarity.\n\n2. (Definition of Capability) The coverage of the capability evaluations is somewhat limited. Currently, authors evaluate mostly on factuality tests like MMLU/NQ/HumanEval and also show results around toxicity but how about other capabilities? Could it be that the imitation data leads LMs to reason better? Or, could it be that imitation data drives better calibration? While I do understand that there is a finite compute budget and there should be a limit in evaluation, the capability definition here is rather to limit the conclusions to draw. \n\n3. (Minor, Title) Given my concern in 2, I\u2019m personally slightly skeptical to call this a `false promise`. It is unclear if it is broadly a false promise, or if there are capabilities that are improved \u2013 it\u2019s rather there exists capabilities that this process even hurts, and we should be mindful about trusting crowdsourcing or other automated evaluations to understand the impact of a process."
            },
            "questions": {
                "value": "1. The discussion seems to rely heavily on the concept of `tuning of imitation data`. However, I think one distinction is the kind of imitation data used to finetune most of the models, which are usually based on roughly arbitrary conversations between users and models. Would the authors agree that if the imitation data is constructed in a different way, then it may be possible to improve e.g. factuality of the finetuned model? For instance, I can imagine the way to construct imitation data to be around extracting rare knowledge, and then possibly the finetuned model could be improved a lot.\n\n2. Have the authors explored other capability definitions than factuality and toxicity? For instance, do we know anything about reasoning, calibration, creativity, truthfulness.. ? \n\n3. Did the authors get the necessary approvals from the IRB of their institution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The study involves human subjects (through MTurk), and it is unclear to me whether the authors got the necessary IRB approvals."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4752/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4752/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4752/Reviewer_WxVs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4752/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698015898386,
        "cdate": 1698015898386,
        "tmdate": 1699636457611,
        "mdate": 1699636457611,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0h5ylCkMSZ",
        "forum": "Kz3yckpCN5",
        "replyto": "Kz3yckpCN5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4752/Reviewer_A4hH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4752/Reviewer_A4hH"
        ],
        "content": {
            "summary": {
                "value": "The paper critically analyzes the approach of imitating proprietary systems (e.g., ChatGPT) by finetuning LMs using various model sizes, data sources, and imitation dataset sizes. Then the authors do human evaluation as well as evaluation on NLP benchmarks. Imitation models do not do well on tasks not heavily supported by the imitation data.\n\nOne interesting finding is that training on broad-coverage imitation data may decrease Natural Questions factuality, but training on NQ-like-data only will increase the accuracy. \n\nThe authors also conclude that the best action forward is to improve base LMs, instead of doing imitation on proprietary systems."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "I vaguely heard of this paper when it came out but it\u2019s my first time reading it. The motivation is excellent for sure (the public will care about this paper), given that many groups and startups are imitating proprietary language models potentially as a shortcut. \n\nThe findings are useful to many practitioners -- they'll likely carefully think whether knowledge distillation is useful or how it'll be useful. \n\nSome findings are quite interesting (see summary above for example)."
            },
            "weaknesses": {
                "value": "I have three concerns related to crowdsourcing (see the next three paragraphs). \n\nDo human raters have low quality? The incentive design and crowdworker filtering seem lacking.  \n- What\u2019s the human agreement (e.g., Fleiss' kappa)?\n- What\u2019s the average time humans spend on each comparison?\n- Is there an option for humans to decline the comparison (because they may not be knowledgeable enough)? \n- How do you make sure that humans are rewarded based on correct choices, and potentially punished if they do extremely poorly?\n\nIt\u2019d be useful to have human evaluators write out rationales on why they chose one over another (or rate on multiple scales using multiple metrics). Otherwise concluding \u201chuman evaluators rate imitation models\u2019 outputs higher because of their style\u201d seems only a conjecture to me. \n\nIf crowdworkers have low quality (thus their annotations unreliable), then it doesn't seem prudent to use Figure 1(c) (crowdworker preference vs. number of model parameters) to reach the conclusion that we should improve base LLMs. \n\n\nI also have some other concerns: \n\nThere are two settings for imitation in the paper. The second setting is broad-coverage imitation. The imitation dataset size could be much larger. Currently the authors are using around (90+27+10)K examples, but this is quite a small number of examples \u2013 the dataset size is even smaller than most of the machine translation training sets from ten years ago.  \n\n\nThe results in this paper are only specific to supervised fine-tuning, not RLHF for example. This should be qualified in the intro paragraph. \n\nThe authors claim that matching ChatGPT using imitation would require an \u201cenormous\u201d amount of imitation examples. Is this supported anywhere in the paper?\n\n\n\nUpdate: I read the authors' response. I'm still a bit concerned about human annotation quality, but I raised the score."
            },
            "questions": {
                "value": "Important: What are the decoding parameters for each model? (This is not addressed post-rebuttal.)\n\n\nBelow are minor (or very minor issues) in evaluating this paper:\n\nI wonder if practitioners may interleave ChatGPT imitation data with actual pretraining data and fine-tuning data. It\u2019s unclear if this setting would lead to the same problems. \n\nThe base models discussed in this paper are quite weak. For example, llama-1-13b is used instead of the SFT- and RLHF-tuned llama-2-13b-chat. The models are quite small too. Unclear if the results generalize to larger models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4752/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4752/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4752/Reviewer_A4hH"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4752/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698342775474,
        "cdate": 1698342775474,
        "tmdate": 1700801596326,
        "mdate": 1700801596326,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dQKEbFs3fl",
        "forum": "Kz3yckpCN5",
        "replyto": "Kz3yckpCN5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4752/Reviewer_TcvA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4752/Reviewer_TcvA"
        ],
        "content": {
            "summary": {
                "value": "The paper critically analyzes the method of using the output of a stronger LM to fine-tune and improve a weaker LM, pointing out that model imitation is not a free lunch.\nThe authors concluded that broadly matching ChatGPT using purely imitation would require (1) a concerted effort to collect enormous imitation datasets and (2) far more diverse and higher quality imitation data than is currently available."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. Using the output of GPT-4 to cheaply improve a weaker language model by fine-tuning is widely adopted. This paper analyzes the drawbacks of doing so, which is helpful to guide the direction of developing more powerful LLMs.\n\n2. A large number of experiments and analyses prove the author's point of view."
            },
            "weaknesses": {
                "value": "1. The author claimed that it is far more feasible to distill a specific behavior from ChatGPT as opposed to broadly matching its capabilities. However, the paper only conducted experiments on NQ-synthetic data.\n\n2. The paper claimed that imitation models are adept at mimicking ChatGPT's style but not its factuality and become far better at following instructions. But the other important ability of the model, that is, the ability to reason, has not been well studied."
            },
            "questions": {
                "value": "Is model imitation still a good solution if the model's factual performance is decoupled to the retrieval model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4752/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4752/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4752/Reviewer_TcvA"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4752/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825097796,
        "cdate": 1698825097796,
        "tmdate": 1699636457450,
        "mdate": 1699636457450,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5cyQpNsPvN",
        "forum": "Kz3yckpCN5",
        "replyto": "Kz3yckpCN5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4752/Reviewer_msgU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4752/Reviewer_msgU"
        ],
        "content": {
            "summary": {
                "value": "The authors investigate the question of acheiving performance parity with high-quality proprietary systems by training (smaller, generally lower quality models) on the outputs of the proprietary systems. The investigation is carried out over a range of data sizes collected from proprietary systems, or imitation data, and a range of model sizes. The authors' conclude that while training on some imitation data can improve the style of weaker models, there (i) is still a large performance gap to the proprietary models especially in evaluations of general capabilities, (ii) diminishing returns from increasing imitation data, (iii) greater gains (than collecting imitation data) by simply increasing the model size."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The main strengths of this work:\n- The question studied is important as most open-source models make use of imitation data for supervised finetuning.\n- The investigation along the data and model size axes is well thought out."
            },
            "weaknesses": {
                "value": "The main weaknesses of this work:\n- The implicit assumption of this work (revealed in the title) is that there exists a claim or understanding that imitating proprietary language models by sampling their outputs for training is all that is needed to achieve performance parity - however, I contend that this isn't the prevalent understanding. It is understood that proprietary model output is a good source of finetuning data but not necessarily the only source. See for example the use of FLAN alongside imitation datasets like Alpaca for SFT.\n- The authors present style imitation of propreitary models as a negative aspect of training on imitation data (at least in the abstract), however the right amount of style imitation can be a definite source of improvement for open-source models - as the authors point out in Section 4.4. My suggestion would be to revise the abstract to reflect this more accurately."
            },
            "questions": {
                "value": "In Figure 4. the 5-shot MMLU performance of the 13B imitiation model is quite low for a model of that size - can the authors describe the setup in more detail?\n\nSome discussion of the points raised in \"weaknesses\" would also be welcome."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4752/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4752/Reviewer_msgU",
                    "ICLR.cc/2024/Conference/Submission4752/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4752/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698843460902,
        "cdate": 1698843460902,
        "tmdate": 1700730153538,
        "mdate": 1700730153538,
        "license": "CC BY 4.0",
        "version": 2
    }
]