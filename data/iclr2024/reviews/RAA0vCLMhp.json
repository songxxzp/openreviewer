[
    {
        "id": "6IM6lCCwc6",
        "forum": "RAA0vCLMhp",
        "replyto": "RAA0vCLMhp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1068/Reviewer_52FQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1068/Reviewer_52FQ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a diffusion-based text generation method I2LTG which uses semantic concepts extracted from training datasets to help with diffusion-based text generation. Semantic Concept Predictor predicts the relevant semantic concepts given initial semantic matrix and visual features. Then, Semantic Conditional Memory aggregates the semantic concepts through memory vectors and mechanism before feeding the memory responses to the diffusion decoder. Experiments show that the I2LTG model is able to achieve superior long-text generation performance, compared to existing works."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Predicting semantic concepts as an intermediate representation for (long) text generation is an interesting and unique approach. It also makes good sense intuitively.\n- Results are good."
            },
            "weaknesses": {
                "value": "- The proposed method requires semantic concepts to be obtained from existing datasets. There is no indication that the validation/test set was not used It\u00a0would be cheating if the method indeed used validation/test set for extracting semantic concepts. There would be strong hints provided to the model.\n- Not sure which visual backbone is used. Is it comparable to backbones used by other methods?"
            },
            "questions": {
                "value": "- In 2.3, the word \"conditional\" is not mentioned at all. What is the point of calling the corresponding component as Semantic \"Conditional\" Memory?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There might be information leaking from validation/test set to the extracted semantic concepts (Weakness 1)."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1068/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784891282,
        "cdate": 1698784891282,
        "tmdate": 1699636033269,
        "mdate": 1699636033269,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8MsJMrhVYA",
        "forum": "RAA0vCLMhp",
        "replyto": "RAA0vCLMhp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1068/Reviewer_Hjjf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1068/Reviewer_Hjjf"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce SEMDIFF, a diffusion-based model equipped with memory networks tailored for I2LTG. Initially, SEMDIFF identifies main semantic concepts within images. It then leverages a memory network to convert these concepts into the diffusion networks to seamlessly integrate them, enhancing the long-text generation process. By doing so, SEMDIFF effectively tackles challenges like incoherence in non-AR text generation, particularly evident in lengthy texts by embedding external guidance within the diffusion iterative generation. The experimental evaluations are conducted on three public datasets, along with COCO-LT, which demonstrate the effectiveness of SEMDIFF over existing state-of-the-art solutions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is overall organized, facilitating a smooth reading experience. Additionally, the inclusion of model overviews and illustrative figures for the components simplifies the understanding of the proposed method. \n \n- The experimental results well validate the approach. Notably, the paper not only presents comparisons with SoTAs, but also comprehensively study the effects of the modules in the framework with ablation studies. \n\n- The authors have included detailed hyperparameter settings in their implementation to enhance the reproducibility."
            },
            "weaknesses": {
                "value": "- While the paper provides valuable insights, there are areas in the methodological presentation that could benefit from further rigor. Specifically:\n    - The definition of $\\mathbf{n}$ in Eq (12) is absent from the document.\n    - The cross-entropy loss, denoted as $\\mathcal{L}_\\text{CE}$, is not formally introduced.\n\nWhile the authors might perceive some of these notations and concepts as commonly understood within the field, it would enhance the clarity and comprehensiveness of the paper to formally define them. Furthermore, this would enhance the presentation clarity and explain these losses relate outputs from various modules, offering readers a more cohesive understanding of the methodology.\n \n\n- While the authors have illustrated the effects of diffusion decoding across different timesteps through experiments, the results primarily lean towards a qualitative nature. For a comprehensive understanding, would it be feasible to provide quantitative assessments to delineate these differences?\n\n- Some properties of diffusion models are not studied in the paper. For example, the guidance of diffusion models are known for enhancing the correlation between the generation and the semantic condition for better controllability. Does this properties further enhance in the case of long-text generation? \n\n- The diffusion models are also known for its computation complexity in the generation, as it requires thousands of NFE in the generation. How does it increase the model computation compared with baselines is not fully studied in the paper.\n\n- The configuration regarding the settings of diffusion models is not clearly presented in the paper. How do you choose the diffusion schedule? Do you take DDIM/ODE or DDPM/SDE steps in the reverse process?"
            },
            "questions": {
                "value": "Please refer to the Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1068/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1068/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1068/Reviewer_Hjjf"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1068/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814134890,
        "cdate": 1698814134890,
        "tmdate": 1699636033197,
        "mdate": 1699636033197,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XMGA2oRqe3",
        "forum": "RAA0vCLMhp",
        "replyto": "RAA0vCLMhp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1068/Reviewer_FnS9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1068/Reviewer_FnS9"
        ],
        "content": {
            "summary": {
                "value": "The paper targets the long image captioning generation. In this paper, a Semantic Concept Predictor is proposed to predict the key concepts in the text, and a set of memories is introduced to enhance the concept representations. The method is tested on different long captioning datasets and the results are promising."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper proposes an interesting structure to catch the key concepts and enhance them with auxiliary memories. \nThe techniques are sound and easy to follow."
            },
            "weaknesses": {
                "value": "I think Experiments need to be improved. Some part of it is confusing. Table 2, shows the contribution of different components. We can find that the simple basic model (\"Trans\") provides a strong baseline, especially in LN, COCO-LT, and CC-SBU. This erodes the contribution of the proposed methods: As a simple baseline can achieve impressive performance, can the methods developed for MIMIC-CXR also be efficient on them? The authors did not explore this in the experiments. Also, some LLM are evaluated on them, while it is not sure if they are finetuned on the training sets. As a simple 6-layer transformer encoder-decoder can achieve 0.054 of BL4, it is confusing why the LLM like LLAVA achieves only 0.06 after finetuning. I think this is an interesting point the author needs to explore further in the paper. \n\nChecking Table 3, we can find the proposed method beats the SoTA methods in quite limited scales, like 0.412 vs 0.407 for B1, and 0.129 vs. 0.126 for B4. It is hard to tell if the proposed method is really better than the existing ones. One option here to prove their effectiveness is to adapt the proposed modules to the SoTA method. \n\nSection 4.2 investigates 3 different hyperparameters of the methods, while the size of the semantic concept set is also significant to be explored. Answering some questions like the following can make readers understand the method better: Is the semantic concept set the larger the better? Are the predicted concepts the more the better for the long captioning generation?"
            },
            "questions": {
                "value": "Please refer to the weakness.\n\nI am also confused about Fig. 15 and 16. It seems h^hat_0 is a probability of h^hat_1, h^hat_2, ... with other variables. While h is usually used as the \"hidden state\". I am not sure why h^hat_0 equals the products of a set of probabilities. it seems p(h^hat_0|h^s, h) would be more reasonable here. Please correct me if I have some misunderstanding."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1068/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831335919,
        "cdate": 1698831335919,
        "tmdate": 1699636033120,
        "mdate": 1699636033120,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RR64PD9iw9",
        "forum": "RAA0vCLMhp",
        "replyto": "RAA0vCLMhp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1068/Reviewer_1tdH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1068/Reviewer_1tdH"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new challenging task, image-to-long-text generation. To overcome the limitations of existing approaches such as the inability to generate sufficiently comprehensive\nand complete textual content. The authors introduce a semantic memory-guided diffusion network (SeMDiff), which captures the essential semantic information of images through a semantic concept predictor, and enhances the semantic representation through a semantic conditional memory module. Afterward, a diffusion decoder module is employed to generate comprehensive and coherent long texts."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem of long-text generation this paper addressed is an interesting and important task.\n2. The proposed method looks technical and sound.\n3. The proposed new dataset COCO-LT is technically reasonable and maybe useful in the future.\n4. The paper is well-organized and easy to read."
            },
            "weaknesses": {
                "value": "1. In section 2.2, the initialized matrix contains a series of semantic vectors to cover all possible concepts, but how to get the semantic vectors of these concepts is not mentioned. \n2. The statement of the semantic conditional memory is not clear. In section 2.3, the description of \u201cthe memory stores the information in aligning image and texts\u201d is ambitious. What is the specific information here, and how it is obtained?\n3. Although the purpose of the proposed approach is to solve the long text generation problem, I think it is still necessary to test on some short caption benchmarks, such as MS-COCO.\n4. The metric CIDEr is missed, which is a very important metric in the image captioning task.\n5. I think the comparison in Table 2, 3 is not fair. Only SEMDIFF is transformer-based while others are all ResNet-101-based. There have been some other Transformer-based methods [1,2] for image captioning task are proposed. Among them, [1] is also a diffusion-based method. I think comparing with these approaches will further strengthen this paper.\n\nRef:\n[1] Jianjie Luo et al. \u201cSemantic-Conditional Diffusion Networks for Image Captioning\u201d. CVPR, 2023.\n[2] Chia-Wen Kuo et al. \u201cHAAV: Hierarchical Aggregation of Augmented Views for Image Captioning\u201d. CVPR, 2023."
            },
            "questions": {
                "value": "1. In Table 4, the results of existing state-of-the-art solutions reported are zero-shot or fine-tuned, and SeMDiff is the zero-shot or fine-tuned? If the results of SOTA are zero-shot, how about the fine-tuned performance on these datasets?\n2. The initialized matrix contains a series of semantic vectors of possible concepts, is it a fixed matrix in the whole training process? Or it will be different for different samples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1068/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832294282,
        "cdate": 1698832294282,
        "tmdate": 1699636033032,
        "mdate": 1699636033032,
        "license": "CC BY 4.0",
        "version": 2
    }
]