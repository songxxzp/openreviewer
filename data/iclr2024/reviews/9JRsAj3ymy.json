[
    {
        "id": "83qKxjzlgG",
        "forum": "9JRsAj3ymy",
        "replyto": "9JRsAj3ymy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3148/Reviewer_AYp8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3148/Reviewer_AYp8"
        ],
        "content": {
            "summary": {
                "value": "The paper presents continuous learning methods in settings where the task corresponding to each sample is known and in settings where it is not. The authors perform extensive numerical experiments that show the superiority of the method presented."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors provide a good review of replay-based continuous learning methods.\n\nMethods are proposed for two settings where the task index is known and where the task index is not known. \n\nThe authors present multiple numerical experiments."
            },
            "weaknesses": {
                "value": "The authors review the literature of continual learning based on replay, but does not explain about other methods that follow other strategies such as regularization-based methods. This review of methods shows the contributions of the proposed method versus replay methods but not versus other strategies. \n\nIn the introduction, the authors refer to equations that appear later in the text, and includes notation that makes the introduction difficult to understand.\n\nThe reviewer believes that the paragraph on confidence in Section 3 could be better explained. For example, the symbol $n$ appears in the first line and is explained in line 4 which makes it difficult to understand.\n\nThe quality of the plots can be improved. The reviewer thinks that it is not clear what the authors want to show. The colors are not well distinguished and the plots are too small."
            },
            "questions": {
                "value": "In the third paragraph of the introduction, the authors introduce the term \"evolving\" in the sentence \"This enables the method to adapt to evolving data streams without prior knowledge of future tasks.\" What does evolving mean? Does it refer to any assumptions about the distribution of tasks over time?\n\nAccording to the definition of task given in the preliminaries, could task be any dataset? Is there no assumption between consecutive tasks, or about the distribution between tasks, or that tasks are related?\n\nThe reviewer believes that the paragraph on confidence in Section 3 could be better explained. For example, the symbol n appears in the first line and is explained in line 4 which makes it difficult to understand.\n\nThe reviewer thinks there is a typo in the equation of a in section 3, I think it would be $l_j$ instead of $l_i$\n\nIs $n$ in the second paragraph of section 4.1 different from the $n$ in section 3?\n\nAt the end of the second paragraph of section 4.1 it is explained that \"the oldest logit is discarded\". Why the oldest? Is there time dependence in the distribution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3148/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3148/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3148/Reviewer_AYp8"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3148/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698475263420,
        "cdate": 1698475263420,
        "tmdate": 1699636262490,
        "mdate": 1699636262490,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ODgpU64RXc",
        "forum": "9JRsAj3ymy",
        "replyto": "9JRsAj3ymy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3148/Reviewer_qppC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3148/Reviewer_qppC"
        ],
        "content": {
            "summary": {
                "value": "The paper develop a new replay-based approach, called Time-Sensitive Replay (TSR), to tackle catastrophic forgetting in continual learning settings. The core idea is to minimize redundancy by reintroducing previous examples when the model's predictions significantly deviate from past tasks while learning new tasks. As a result, the number of stored samples can be reduced without compromising the model performance. The idea is also applicable to a task-free scenario, Task-Free TSR (TF-TSR). Experimental results on benchmark datasets are provided to demonstrate that TSR is particularly effective in improving the learning speed while maintaining high-performance accuracies."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper can be followed straightforwardly.\n\n2. Experiments demonstrate that the idea is effective."
            },
            "weaknesses": {
                "value": "1. The setting is somewhat contrived and it is not clear how practical it is and its relevance to real-world applications.\n\n2. Comparison against prior methods is limited which makes judgment about the competence of the method challenging. \n\n3. The code is not provided which makes judgment about reproducibility challenging."
            },
            "questions": {
                "value": "1. Experiments are performed in synthetic settings using benchmarks that are common but do not have a natural notion of time. Given the proposed setting, why haven't experiments been performed on suitable real-world datasets that have a notion of time?\n\n2. Continual learning has a rich literature and there are many works on the topic. What is the justification for selecting the methods that have been used in the paper? Do they have SOTA performance? It is very common in CL literature to provide an extensive comparison.\n\n\n\n============Post Rebuttal=============\n\nThank you for preparing the rebuttal. I maintain my initial rating."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3148/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3148/Reviewer_qppC",
                    "ICLR.cc/2024/Conference/Submission3148/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3148/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698633392480,
        "cdate": 1698633392480,
        "tmdate": 1700995071250,
        "mdate": 1700995071250,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7bkNso1jxG",
        "forum": "9JRsAj3ymy",
        "replyto": "9JRsAj3ymy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3148/Reviewer_D8wn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3148/Reviewer_D8wn"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates replay-based continual learning and seeks to reduce the number of replayed examples during training while maintaining accuracy. To this end, this paper proposes a replay method named Time-Sensitive Replay (TSR), which first detects drifts in the model prediction when learning a new task and then introduces the replay data for training to mitigate forgetting when drifts are detected. A variant called TF-TSR is further proposed to handle the task-free setting where the task boundary is unknown. Experiments are conducted to show that the proposed algorithms can achieve comparable performance with baseline methods but in a shorter training time."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of managing data replay at a smaller time scale, i.e., per training data batch, and investigating when to replay is interesting.\n\n2. A variant of the algorithm is also proposed to handle the task-free setting.\n\n3. The experimental results show that the number of replayed examples can be successfully reduced."
            },
            "weaknesses": {
                "value": "1. While I appreciate the authors' effort in investigating the problem about when to replay, one of my main concerns is about the availability and complexity of using the forgetting performance of all previous tasks as the feedback signal to determine if replay is needed for the previous tasks. To detect the distribution shifts, evaluations of the current model on the validation datasets for all previous tasks are required at each time step (or each batch) to determine if forgetting happens for any previous task. \n\n- First, whether such a validation dataset is available for each task is questionable. Without this, the forgetting performance cannot be evaluated. \n- Second, even if we have such validation datasets, evaluating the performance on all previous tasks using the validation data at each time step will introduce a lot of computational complexity, especially when the number of tasks is large. The detection complexity can be much higher than the replay complexity. Even though an analysis of the theoretical complexity is provided in the appendix, it is unclear if the proposed algorithm can achieve better computational complexity than other replay-based methods in theory.\n\n2. Another main concern is about the empirical evaluation. \n- First, in the comparison, even if the proposed method has a shorter training duration, the final accuracy can be, for example, 2\\% lower than CEC on MNIST. This somehow indicates that the condition to determine when to replay is not so helpful. In other words, the proposed algorithm does not tell us correctly when to replay, and more data replay clearly improves the performance as in CEC. \n- Second, only two baselines (among which only one is replay-based) are considered in the paper, which are also relatively outdated. Considering that there are many advanced replayed-based CL methods out there, more baseline methods should be investigated here. Specifically, the question here is that if we can select the replay samples wisely, do we still need to care about when to replay? Maybe the answer is still yes, but this investigation will further justify the contribution of this paper.\n\n3. The writing can be improved and there are multiple typos, e.g., $l_i^T$ should be $l_j^T$ in calculating the moving average, $l_k^{T_1} $ should be $l_k^{T_i}$ in \"Model Update and Validation\"."
            },
            "questions": {
                "value": "Please see the weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3148/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3148/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3148/Reviewer_D8wn"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3148/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698708022898,
        "cdate": 1698708022898,
        "tmdate": 1699636262336,
        "mdate": 1699636262336,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jqNmkwWasH",
        "forum": "9JRsAj3ymy",
        "replyto": "9JRsAj3ymy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3148/Reviewer_Xzud"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3148/Reviewer_Xzud"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an approach called Time-Sensitive Replay (TSR), which aims reduce the computational load of continual learning while maintaining learning performance. TSR replays data from a particular task only when it detects that this task has started being forgotten. The forgetting detection takes place with the help of stored validation data from each task."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- I found the main idea of the paper (reducing computations while maintaining accuracy) to be interesting and worth examining."
            },
            "weaknesses": {
                "value": "- A considerable limitation of the paper is that it ignores cases where the data distribution changes continuously over time. In such cases, task boundaries cannot be defined or detected. In my opinion, most real-world applications of continual learning do not have task boundaries, since the distribution is constantly changing (e.g., weather, climate, stock market, hospital occupancy, etc.). \n- The number of baselines used in the evaluation is insufficient, in my opinion (especially for a major venue like ICLR).\n- I am not sure if the comparison of TSR and TF-TSR with other baselines is fair. TSR requires the storage of validation data, while other methods do not necessarily do so, and I could not find any part of the text that explains whether this discrepancy was taken into account in the evaluation. \n- The references for MNIST and FashionMNIST are incorrect I think. (This point has not affected my evaluation of the paper.)"
            },
            "questions": {
                "value": "- Can you explain if and how you took into account the size of the validation memory in the evaluation?\n- Why did you not compare with other very simple and efficient baselines (e.g., simple experience replay)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3148/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699192911846,
        "cdate": 1699192911846,
        "tmdate": 1699636262276,
        "mdate": 1699636262276,
        "license": "CC BY 4.0",
        "version": 2
    }
]