[
    {
        "id": "hAYB2KEOAe",
        "forum": "X7nz6ljg9Y",
        "replyto": "X7nz6ljg9Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4196/Reviewer_hV8P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4196/Reviewer_hV8P"
        ],
        "content": {
            "summary": {
                "value": "This research delves into the importance of inductive biases within the field of machine learning, specifically by examining the concept of Kolmogorov complexity. The study conducts experiments and uncovers several noteworthy findings: 1) real-world datasets tend to have low complexity, 2) convolutional neural networks can efficiently compress tabular data, 3) language models at initialization tend to favor sequences with low complexity, and 4) model selection can be automated via cross-validation and provably generalizes. These findings support the idea that using large and adaptable models can yield benefits in a variety of tasks without the need for additional inductive biases provided by humans."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Originality**\n\n This paper stands out for its unique approach, synthesizing various observations to construct a compelling argument that challenges the necessity of specialized inductive biases in solving practical machine-learning tasks. Although some of the experimental results might not be groundbreaking, the amalgamation of these findings into a coherent argument is a novel contribution.\n\n**Quality**\n\nThe paper impressively showcases well-motivated analyses and experiments, exhibiting a high level of technical rigor. Moreover, the analyses and experiments are discussed and interpreted extensively, which is critical for a paper of this kind.\n\n**Clarity**\n\nNotably well-written, the paper benefits from clear references to previous work, enhancing its readability, as well as a strong overall structure. The inclusion of takeaway boxes and an extensive discussion in the Appendix further adds to its clarity and comprehensibility.\n\n**Significance**\n\nThe paper addresses a substantially significant question in the field of machine learning, namely the necessity of specialized inductive biases. This question, often overlooked, receives a strong affirmative response in this paper, backed by ample evidence. Therefore, the paper has the potential to make a notable impact on the research community. This is particularly true given the prevalence of the alternative viewpoint: that specific inductive biases are necessary to achieve efficient generalization on most practical problems."
            },
            "weaknesses": {
                "value": "The primary weakness I perceive in the paper is that its technical findings, although robust, may not be particularly surprising. Although the specific analyses in the paper are new, the paper reiterates established facts, such as the structured nature of real-world datasets, neural networks' preferences for simpler solutions, and the automation of model selection. While the paper's innovation lies in its synthesis of these facts, the lack of groundbreaking technical discoveries is nevertheless a weakness."
            },
            "questions": {
                "value": "The paper is quite well written and complete; as a result, I have no technical questions. I hope the authors are able to emphasize the significance of the contributions in this work as mentioned in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4196/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698179003971,
        "cdate": 1698179003971,
        "tmdate": 1699636386177,
        "mdate": 1699636386177,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2jbsIf6AK7",
        "forum": "X7nz6ljg9Y",
        "replyto": "X7nz6ljg9Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4196/Reviewer_VnMw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4196/Reviewer_VnMw"
        ],
        "content": {
            "summary": {
                "value": "The authors provide an interpretation of the no-free lunch theorem through the lens of Kolmogorov Complexity and highlight its relations to compressibility. Through this connection, they illustrate that real world datasets are highly structured and they all share some underlying commonalities. No free lunch theorems claim that over the distribution of all possible tasks, every learner will on average perform equally well or worse and the authors claim that while it holds when considering all possible tasks, real world tasks come from a more structured subset of such tasks and are highly compressible, and hence one can obtain reasonable performance over such tasks without having to include task-specific inductive biases as was incentivized through no free lunch theorems.\n\nIn particular, the authors use Kolmogorov complexity to derive a new no free lunch theorem, and show using learnability through neural networks that real world datasets can be highly compressible and thus have low complexity. Further, they show that pretrained models like GPT-3 prefer sequences of low complexity, especially even at initialization which is surprising."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well written and provides a fascinating connection between no free lunch theorems and Kolmogorov complexity. In particular, the approach of using this notion of complexity to show that real world datasets are highly structured is quite interesting.\n- The authors also provide a few interesting bounds, especially using the learned $p(y|x)$ distribution to provide a bound on the complexity of the dataset; and then highlighting that as long as the learner achieves better than random chance, it implies that the dataset is compressible."
            },
            "weaknesses": {
                "value": "- The authors mention that no free lunch theorem holds because when selecting datasets using a uniform distribution, it subtly selects data of high complexity which is not compressible. However, earlier the authors claimed that data of high complexity is exponentially hard to obtain, which means that under uniform distribution over tasks, learners should be able to perform better than random chance because this incompressible data occupies only a small volume. It would be nice if the authors could provide some discussion on this.\n\n- The bound provided in Section 4.1 only requires disproportionate weighting over the hypothesis space based on their complexity. However, it does not take into account the compressible nature of the dataset; and hence it is not clear how the bound is non-trivial for real world datasets when it holds for arbitrary datasets, and in particular could hold for a dataset with high complexity as long as the solution still has low complexity.\n- It is not clear what the authors want to show by training a CNN over tabular data. Yes, it is able to perform better than random chance but this is also expected of any, potentially even mismatched, training architecture on any data with some inherent structure. From such a lens, the finding that CNNs are able to generalize on tabular data is not too interesting, so could the authors clarify on why and how it is an interesting finding?\n- Details about the GPT model are missing. Are the authors using a pre-trained GPT-3, but if so it has been trained on a lot of different kinds of data and the task of binary tree expansion would be quite OoD for the model. Is it that the authors are training GPT-3 on such a dataset? - It would be nice if the authors actually also do a small scale training (not necessarily GPT-3) and then use that model to provide trends. In particular, the authors could also look at performing training where on average, input of each complexity is seen roughly the same number of times.\n- The authors claim that a randomly initialized model could do next token prediction well on low complexity sentences. This is unclear because why would the model be able to do the task at all if it is untrained, irrespective of the complexity of the sentences?\n- How do the authors demonstrate or claim that cross validation provably generalizes to millions of models with only thousands of data?\n- While it might be okay to use large models with a simplicity preference to provide a universal platform for learning in diverse systems; it is quite unclear how and what the right method of providing simplicity performance to architectures is. For example, in the ViT experiments, the authors could change the simplicity bias by considering the same GoogleLeNet architecture with just $l_2$ penalty, but that does not have as much of an effect as the simplicity bias chosen by authors in Section 5.2."
            },
            "questions": {
                "value": "*\u201cAll but exponentially few sequences of a given length have near maximal Kolmogorov complexity and are thus incompressible\u2026\u201d*\nWhat is the reasoning behind this statement? In the following inequality on probability, is $n$ the size of the bitstring over which a uniform distribution is prescribed? Where does this inequality come from?\n\n*\u201clabels can be encoded in $K(Y|X, p) \\leq - \\sum_{i=1}^n \\log_2 p(y_i | x_i) + 3$ bits\u201d*\nHow did the authors obtain this inequality?\n\n*\u201c$K(Y | X) \\leq K(Y | X, p) + K(p) + 2 \\log_2 K(p) + c$\u201d*\nHow did the authors obtain this inequality?\n\nUnfortunately the proof of Theorem 1 wasn\u2019t clear. Could the authors provide more insight on the statement that there are less than $2^{k+1}$ labelings $Y$ with $K(Y | X) \\leq k$.\n\n*\u201c$K_p(h) \\leq K(h) + 2 \\log_2 K(h)$\u201d*\nHow did the authors obtain this inequality?\n\nWhat do the authors mean when they say that the assigned probabilities decay sub-exponentially with sequence length and how does this imply more confidence later on?\n\n*\u201cRandomly initialized models prefer low complexity\u201d*\nI am not quite clear on this setting. Given that it is a randomly initialized network, wouldn\u2019t it basically give a uniform distribution over all possible sentences? Even further, the authors claimed that higher complexity sentences are exponentially hard to find, which would imply that the network just gives low probability to complex sentences primarily because they are lower in number. Can the authors provide some of their reasoning on it, and explain how exactly Neural Networks, at initialization, generate something meaningful.\n\nHow do the authors demonstrate or claim that cross validation provably generalizes to millions of models with only thousands of data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4196/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4196/Reviewer_VnMw",
                    "ICLR.cc/2024/Conference/Submission4196/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4196/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698469156104,
        "cdate": 1698469156104,
        "tmdate": 1700542162617,
        "mdate": 1700542162617,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3Qk96Obv9Z",
        "forum": "X7nz6ljg9Y",
        "replyto": "X7nz6ljg9Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4196/Reviewer_pHWe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4196/Reviewer_pHWe"
        ],
        "content": {
            "summary": {
                "value": "**Update after rebuttal:** I thank the authors for their detailed response. Unfortunately most of the criticism raised by me has not led to any changes to the paper, and the authors' response has mainly re-stated their position which is already expressed at length in the paper, but I do not feel that major misunderstandings on my side have been raised or cleared up. This means that my original criticism and assessment stands as is. I still believe that a large part of the material in the paper is so well established that it is found in standard textbooks. I agree with the authors that there is some merit to pointing out these textbook results to the ICLR community, and I must admit that I am quite surprised that some of the other reviewers find it surprising that untrained networks have simplicity biases; after all, there have been dozens of papers over the last years, some of which received considerable attention at top-tier conferences, showing exactly this point (though typically not from the view of low Kolmogorov complexity, but e.g. functional complexity in the frequency domain). Perhaps some of the other reviewers have overstated their self-rated confidence. Regardless, I do not think that the current manuscript constitutes an excellent tutorial or introductory-style paper (which, admittedly, was not the aim of the authors). This leaves the core of the paper being a technical contribution in terms of establishing a \"cross-domain generalization bound\" and some empirical assessments. This is interesting and valuable, and I personally would much prefer an manuscript that significantly expands on this, presents more rigorous formalism instead of prose, and only devotes at best a paragraph discussing how people commonly misinterpret NFL theorems. In the end it will be for the AC to decide whether the contribution is novel and significant enough, and I will not veto accepting the paper, since I seem to be the outlier among the reviewers. I do strongly agree with the authors that this should be standard knowledge in the community, and I would really like to see more work emphasising the compression viewpoint and how it relates to general learners/learning, and I am sympathetic to their main messages. But stating that neural networks have a bias for low Kolmogorov complexity sequences is a bit too vacuous - this needs to be expanded on (what is the reference machine, or what can we say about what kinds of sequences are simple and complex for neural networks; because clearly, some rather \"simple\" sequences, like palindromes, are not at all simple under modern neural networks' biases). As it stands, I cannot confidently suggest the manuscript for acceptance at a top-tier conference with a very high bar to pass.\n\n**Detailed comments to the rebuttal:**\n * \"This theorem is frequently cited to argue that general purpose learners are impossible in practice, not only in theory.\", \"Our work argues directly against commonly held beliefs\", \"Our work fights back against this pervasive idea\", \"See [1-6] for example claims that the NFL ensures that we need specialized learners for individual applications of machine learning in practice.\" - I disagree that these are commonly held beliefs; most of the references cited to support this are quite dated or from non-core-ML outlets such as IEEE Transactions on Intelligent Transportation Systems (and I am not engaging with Gary Marcus' tweets here). If anything the last decade in ML has been the overwhelming success of general learners. CNNs replaced hand-crafted, domain-specific features; the end-to-end paradigm took hold, and particularly over the last 3 years transformers are widely hailed as universal learners. I would even go as far and say that never before in the history of ML have more ML researchers believed in the possibility of general learners.\n* \"In terms of reference machine, this choice is as valid as any other,\" - ok fine, but it risk sending a bit of a skewed message to readers unfamiliar with Kolmogorov complexity (which is typically understood as being an objective complexity measure, which is watered down by changing the reference machine arbitrarily and/or restricting the set of possible data generators to be non-universal).\n* \"Rather, we argue that the extent of their low-complexity bias can explain their broad generalization capabilities and why general-purpose machine learning algorithms are possible.\" This is vacuous, and needs to be stated with more detail (any general-purpose ML algorithm must have, by definition, low-complexity bias; how exactly is the low-complexity bias of neural networks characterized? what kinds of data are low and high complexity under this bias?).\n* \"study the alignment between the preferences of neural networks and naturally occurring datasets.\" I really like this point, and think this is where the main contribution of the paper lies. It would be great to expand on this and really (empirically) drill into this to make as concrete statements as possible (what data can neural nets compress easily, what data that we know is easily compressible in principle can neural nets not compress well?). \n\n\n\n**Summary:** The paper discusses how No Free Lunch (NFL) theorems rule out a general learner, that is a learning algorithm that works well across all possible datasets. In contrast to the assumptions underlying the NFL theorems, the paper observes that most real-world datasets are compressible, meaning they are structured - or in other words: non-trivial generalization by learning from parts of the data is possible in principle. The question is then whether learning algorithms can be designed that work across many such datasets, which is only possible if the \u201cstructure\u201d can be incorporated as a general inductive bias into the learner. The paper proposes to use low Kolmogorov complexity as this structural bias (inspired by Solomonoff induction). After confirming that some commonly used tabular benchmark datasets are indeed compressible by a standard MLP, the paper argues that modern neural networks are candidates for general learners that prefer solutions with low Kolmogorov complexity. This intuition is empirically confirmed on simple computer vision benchmarks (CIFAR, ImageNet) and some synthetic tasks designed such that \u201cKolmogorov complexity\u201d can be estimated."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* Timely and important idea\n* Work is situated in a theoretically solid and well understood framework (Solomonoff induction / Kolmogorov complexity\n* Paper has polished prose, and nice summaries of the key take-aways of each section\n* Simple empirical results that illustrate the main claims"
            },
            "weaknesses": {
                "value": "* By far the biggest weakness of the paper is that its novelty is very limited. Besides the experiments, I would argue that almost all the material can be found in textbooks (which do not cover connections to neural networks) and recent publications (studying inductive biases of neural networks from a complexity perspective). Li & Vitanyi\u2019s \u2018An Introduction to Kolmogorov Complexity and Its Applications\u2019 covers a large part of what\u2019s discussed in the paper. I also strongly suggest having a look at Hutter\u2019s \u2018Universal Artificial Intelligence\u2019 (which has a brief discussion of NFL vs. Occam\u2019s razor and provides more extensive references on the topic), and Gruenwald\u2019s \u2018Minimum Description Length Principle\u2019. To me, the only substantial claim made by the paper that has not been discussed at textbook level before, is that Neural networks (after training, and much more importantly before training) have a bias towards low Kolmogorov complexity patterns. Unfortunately, the paper only scratches the surface in empirically verifying the claim. As a counter-argument consider [1], who investigate neural networks\u2019 ability to learn simple algorithmic patterns across the levels of the Chomsky hierarchy (a hierarchy of computational complexity). Tasks include things like reversing an input string, copying a string, or performing modular arithmetic. The paper finds that standard architectures, in particular transformers fail to generalize on non-regular tasks (i.e. all tasks of higher computational complexity than regular languages). I interpret these results as some low-complexity bias holding on regular-language data, but not on non-regular patterns, e.g. for copying or reversing an input string, very low Kolmogorov complexity programs exist but standard neural networks trained via SGD seem (reliably) not to be able to find them and instead find a solution of higher complexity that does not generalize. These results cannot be reconciled with the broad claim in the paper that neural networks have a general bias for low Kolmogorov complexity. The main claim in the paper is thus either wrong, or somewhat vacuous (in which case it needs further refinement).\n* The paper seems to suggest that No Free Lunch theorems somehow rule out the possibility of neural networks that can train successfully on a number of datasets and achieve non-trivial prediction or classification performance. I disagree with this interpretation, and think it is a somewhat common misreading of the NFL theorems. As the paper states, the NFL theorems hold under all possible datasets (or similar formulations depending on the theorem). Clearly the theorems do not apply when considering a subset of all possible datasets, in particular if they share some structure (literally meaning they are compressible, which is dual to saying predictors/classifiers can be trained on a subset of the data and they will generalize). Saying that the NFL theorems do not apply when only considering compressible data is thus at best a tautology, but it does not invalidate the theorems. To me, the NFL theorems are unrelated to this paper since they do not apply in the setting considered by the paper - I personally would drop their discussion from the paper.\n* The writing throughout the whole paper is very hand-wavy, and formally well-defined concepts are often used qualitatively and in an imprecise fashion. Just to give one example, Kolmogorov complexity is the length of the shortest program to produce a string **on a universal Turing machine**, and depending on the reference machine the Kolmogorov complexity can change significantly. A programming language can typically be compiled such that it can be executed on a UTM, or it is interpreted by another program running on the UTM. In the limit the length of the compiler/interpreter becomes a negligible constant, but when considering a PyTorch program of 280 characters, the constant is far from negligible. Similarly, Kolmogorov complexity is defined w.r.t. a universal hypothesis class (all computable programs; in a very particular sense); using a more restrictive class like in the experimental tasks defined in the paper, one can construct a complexity measure inspired by Kolmogorov complexity (which does imply losing out on some of the favorable properties of the complexity measure such as universality/\u201dobjectivity\u201d); but the numbers reported in the paper should not be called Kolmogorov complexity. Improvement: make all the main concepts introduced (mainly Section 2) precise and formal; add equations. I strongly suggest consulting the corresponding textbooks.\n\n[1] Neural Networks and the Chomsky Hierarchy, Deletang et al. 2022.\n\n\n**Verdict:** \nThe topic of identifying the inductive biases of neural architectures (when training via SGD and various regularizers) is timely and important. Recently, a number of papers have investigated the inductive biases of various architectures (CNNs, RNNs, Transformers) either via function-space complexity measures (such as spectral analysis) or information-theoretic / computational complexity measures. The paper, particularly the empirical part, is very much in line with these works, and adds to a body of results that find low-complexity biases in standard architectures / training setups. Admittedly, the background discussed in the paper (universal induction and low Kolmogorov complexity, or dually, compressibility as a general inductive bias) does perhaps not receive enough attention in the wider ML community recently, and a good tutorial/introduction could be a valuable contribution in itself. Unfortunately the current version of the paper falls short of providing a concise yet formally precise introduction / tutorial by being too hand-wavy throughout (also I believe that such a tutorial would require at least the full 9 pages, maybe a bit more, making it less suitable as a conference paper). Overall I could see this work being developed in two ways: double down on the tutorial aspect (highlighting important well-established theoretical results and situating them within modern ML practice), or double down on the empirical aspect (are there classes of data generators where the low Kolmogorov complexity bias does not hold for standard architectures, or can we really claim universality of the findings in the paper; see my earlier comment on at least one strong counter example with non-regular languages)? If either of these lines were fully developed, I believe that would constitute a solid and interesting contribution, but I personally think the current manuscript needs more work before it is ready for publication. I am happy to read the other reviews and hear the authors\u2019 response before coming to a final conclusion."
            },
            "questions": {
                "value": "**Comments:**\n\n* I disagree with the premise that no-free lunch theorems state that \u201cthe world is hostile to inductive reasoning\u201d. Obviously we do not live in a world that presents us with a uniform distribution over all possible learning problems. To make a philosophical point, since evolution is also bound by the limits of computability and compressibility, it is expected that life can only thrive in compressible (=predictable) environments - we do also encounter (virtually) incompressible data, but we typically deem it irrelevant, a.k.a. noise; we (can) only care about the compressible aspects of the world we live in.\n* End of 3.1: \u201callowing us to reject the hypothesis that the labeling functions are drawn uniformly at random with extremely high confidence.\u201d Who would make the claim that labeling functions for real-world datasets are drawn uniformly at random?\n* Section 5: \u201cWhereas the no free lunch theorems seemingly preclude automated meta-learners which select performant models on any task, [...] the defeating conclusion of Wolpert\u2019s no free lunch theorem is escaped as long as datasets share structure so that the model selector generalizes to new datasets.\u201d Who would claim otherwise? I would be willing to make a significant bet that Wolpert would strongly agree with the sentence in the paper. Note how the first part of the sentence (in line with the NFL theorems) says **any task**, but the second part of the sentence only considers **datasets that share structure**. These two kinds of settings are not fundamentally different; due to the restriction in the second part of the sentence the claim does not invalidate or nullify the NFL theorems.\n* Section 5.1: \u201cA near state-of-the-art computer vision model can be expressed in only 280 characters [...] in PyTorch.\u201d. Yes, but this does not imply that the Kolmogrov complexity is 280 times(!) 8 bits. In the regime of such short programs the reference machine (and thus all libraries and the compiler/interpreter) must be taken into account too, pushing the (upper bound) of the Kolmogorov complexity into the tens of thousands of bits."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4196/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4196/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4196/Reviewer_pHWe"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4196/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698779846077,
        "cdate": 1698779846077,
        "tmdate": 1700756071768,
        "mdate": 1700756071768,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eVCJ5Y37r3",
        "forum": "X7nz6ljg9Y",
        "replyto": "X7nz6ljg9Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4196/Reviewer_wTgY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4196/Reviewer_wTgY"
        ],
        "content": {
            "summary": {
                "value": "The paper challenges the No Free Lunch theorems' notion that every learning problem requires a unique algorithm. It argues that neural networks inherently prefer low-complexity data, which is common in real-world scenarios, and can generalize across various domains. The research shows that neural networks can compress data and generate low-complexity sequences effectively, even with little customization. This suggests the possibility of developing universal learning algorithms, thus supporting the trend towards more generalizable and fewer machine learning models in deep learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper presents an original reexamination of the No Free Lunch theorems, offering a fresh perspective by proposing that neural networks inherently favor low-complexity data, challenging the belief that each learning problem requires a distinct algorithm. The quality of work is evident as it is well-founded on theoretical bases and is further bolstered by empirical experiments. It is written with commendable clarity, managing to articulate the interplay between complex theoretical concepts and their practical applications in machine learning. Its significance is underscored by its potential to influence the development of generalized learning algorithms, marking a substantial leap in the field's evolution towards more efficient and versatile machine learning models. This paper stands out for bridging abstract theory with concrete experimental evidence, making a significant contribution to the literature."
            },
            "weaknesses": {
                "value": "1. The assertion that a universal high-degree polynomial can be effectively applied across diverse sample sizes, given a bias towards simplicity, lacks comparative analysis with contemporary leading methods.\n\n2. The commentary on the combined effectiveness of GoogLeNet and ViT, underpinned by a simplicity preference, does not evidently align with the primary findings of the study.\n\n3. The authors propose a heuristic for addressing the non-computability of Kolmogorov complexity by employing a simplified, non-universal computational language. However, the adequacy of this approximation, both theoretically and empirically, remains unexplained.\n\n4. While the findings of the paper are intriguing, many of the experimental results seem tangential or conceptually disconnected from the core conclusions drawn in Theorem 1."
            },
            "questions": {
                "value": "In addressing the non-computability of Kolmogorov complexity, the authors choose a simplified language as a heuristic. Could they elaborate on the justification for this choice and its validity as a proxy, both theoretically and empirically?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4196/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4196/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4196/Reviewer_wTgY"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4196/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699234434458,
        "cdate": 1699234434458,
        "tmdate": 1699636385968,
        "mdate": 1699636385968,
        "license": "CC BY 4.0",
        "version": 2
    }
]