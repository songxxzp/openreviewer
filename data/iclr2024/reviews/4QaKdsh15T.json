[
    {
        "id": "o7tOXJPny3",
        "forum": "4QaKdsh15T",
        "replyto": "4QaKdsh15T",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1150/Reviewer_bU9n"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1150/Reviewer_bU9n"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces LEO, a method that distinguishes itself from other LLM-based generalist agents by being able to understand and interact with the 3D world. LEO is trained with a two-stage process called \"LEO-Align\" and \"LEO-Instruct\". In the first stage, LEO is trained to align 3D vision and language by captioning 3D inputs (objects, objects-in-the-scene, and scenes). In the second stage, LEO is fine-tuned to specific tasks. LEO is demonstrated across a large variety of tasks including 3D vision-language understanding and embodied tasks. The authors provide detailed quantitative results of their method across multiple tasks: Scan2Cap, ScanQA, SQA3D, CLIPort, and ObjNav. The authors additionally ablate LEO to study components of their method including pretraining alignment, scale, and LEO's multitask nature."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Overall this manuscript is very cleanly written, with well-presented figures. \n- The results relating to 3D vision-language understanding and the corresponding ablations are compelling and informative. \n- This work contains a novel and useful research direction, incorporating 3D understanding, LLMs, and embodied actions into a single model.\n- This paper does a good job of providing quantitative and qualitative results for LEO across a broad range of tasks."
            },
            "weaknesses": {
                "value": "Overall the weaknesses in this work are found concerning the embodied actions tasks.\n- In the CLIPort experiment the authors only demonstrate results on 3 out of the 10 tasks from the original experiment. I would be curious to see the full results of the left-out tasks to have a complete comparison to the original baselines in this experiment. This full table should also be presented in the supplementary information.\n- The authors do not present any baselines to compare to for the embodied navigation task. The results that LEO obtains on the embodied navigation task are quite low when compared to other works in the embodied AI community. By just performing behavior cloning on 70k human demos, Habitat-Web [1] achieved a success rate of 27.8 on their MP3D test split. Furthermore, in PONI [2] the authors have a baseline called \"BC\" (Behavior Cloning) that is similar to LEO in modalities used and with the use of behavior cloning; however BC just utilizes a simple ResNet-50 architecture. This baseline performs comparably to LEO (3.8 Success vs 2.6/3.7 Success) on MP3D. Could the authors compare to previous work and comment on the \"BC\" baseline from PONI?\n\n[1] Ramrakhya, Ram, et al. \"Habitat-web: Learning embodied object-search strategies from human demonstrations at scale.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[2] Ramakrishnan, Santhosh Kumar, et al. \"Poni: Potential functions for objectgoal navigation with interaction-free learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
            },
            "questions": {
                "value": "Major questions and comments are given in the weaknesses section. \nMinor questions and comments follow.\n- In Habitat-Web the authors found a large difference in imitation learning results when trained on shortest path trajectories (4.4 success) and human demonstrations (35.4 success). The authors in this work chose to use the shortest path trajectories as they were less noisy and were easier to learn. Can the authors clarify if training on the human demonstrations led to worse quantitative performance on the task?\n- In section 4.3 the authors claim they will present the soft-spl, but this is missing from Table 4."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1150/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1150/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1150/Reviewer_bU9n"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1150/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698270795391,
        "cdate": 1698270795391,
        "tmdate": 1699636041175,
        "mdate": 1699636041175,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jFkvvmktvi",
        "forum": "4QaKdsh15T",
        "replyto": "4QaKdsh15T",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1150/Reviewer_Vn42"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1150/Reviewer_Vn42"
        ],
        "content": {
            "summary": {
                "value": "The paper identifies a gap in large language models (LLMs) regarding their limited capability to understand and interact with the 3D world. To address this, the authors introduce LEO, an embodied multi-modal and multi-task agent adept at tasks in the 3D environment. LEO is trained in two stages: 3D vision-language alignment and 3D vision-language-action instruction tuning, supported by a vast dataset containing object and scene-level multi-modal tasks. Through comprehensive testing, LEO excels in tasks like 3D captioning, embodied reasoning, and robotic manipulation. The paper also shares findings that can guide the development of future embodied generalist agents."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe essence of the motivation is sound.\n2.\tThe proposed pipeline is somewhat novel.\n3.\tThe experiments cover many aspects. A lot of efforts have been made."
            },
            "weaknesses": {
                "value": "1. On the motivation and coherence: I think there are some word accuracy issues in the motivation. \n\n(1)\tIn Intro - first paragraph: \u201cThis limitation \u2026 executing real-world tasks and the attainment of general intelligence.\u201d First, many works on generalist agents are able to execute in the real world. (e.g. GATO, [2205.06175] A Generalist Agent (arxiv.org); PALM-E, [2303.03378] PaLM-E: An Embodied Multimodal Language Model (arxiv.org). To note, I do not ask for a baseline comparison against these works, just to suggest a better description of this work\u2019s scope) The author should be more precise on what kinds of real-world tasks they cannot execute, and what kind of real-world tasks the paper plans to address. Second, about the attainment of general intelligence. How to define the scope of general intelligence? When the generalist agent is equipped with 3D understanding ability, can it attain general intelligence? I hope the author can narrow the scope of the statement by adding some transition sentences.\n\n(2)\tIn Intro \u2013 second paragraph: the three challenges, the creation of datasets, design of scalable models, and effective learning strategies. I don\u2019t see how it\u2019s relevant to the following solutions. (a) The dataset is fused with existing datasets with high-quality data prompted by the LLMs. How is that challenging? With all the assets and models existing and known before. (b) The scalability of the model is not adequately validated. I understand that training a model from 7B parameter to 13B is already quite demanding for most labs, but it is far from emerging scaling effects. (c) the training strategy in LEO is nothing special. Again, if the known techniques can do well, how is that challenging? \n\n2. On the novelty of the method: quite limited. To my understanding, it is a combination of all known techniques including tokenization, token embedding & LLM, and training & inference. Since they all \u201cfollow\u201d some previous works. Besides, as a multi-task learning model, or \u201cgeneralist agent\u201d as the author may prefer, how the output looks is also important, but it is briefly described in the main text, and scattered in many places all over the paper.\n\n3. On the novelty of dataset generation: it has more creative thoughts, but it is a pity to put too many details in the Appendix. I would suggest putting more of the method in the Appendix instead of the dataset generation details."
            },
            "questions": {
                "value": "What this paper puzzles me most is that it does not reveal the challenges and necessity of incorporating 3D input. In the response, I would like to see:\n\n1.\tWhat\u2019s the particular technique to adopt to deal with the 3D input? How is it different from previous methods? Or when the previous method is combined, does it just work like that, or does something non-trivial happen?\n\n2.\tin this framework, according to the experiments, what ability requires 3D understanding to make it from 0 to 1 or improve a large margin? I think the author should focus on that, instead of trying to propose a \u201cgeneralist agent\u201d, which is exhaustive for a lab-level resource and really cannot produce any insightful outcomes. If the authors deem to the generalist agent story, then they should in-depth reveal the challenges, for example:\n\n(1) how much data does it need, the key to creating and preparing the dataset?\n\n(2) does the model parameter size matter at all, if 7B is already saturated (Fig 4-b) how about smaller models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1150/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698694158329,
        "cdate": 1698694158329,
        "tmdate": 1699636041077,
        "mdate": 1699636041077,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qHaa42QDQE",
        "forum": "4QaKdsh15T",
        "replyto": "4QaKdsh15T",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1150/Reviewer_foVx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1150/Reviewer_foVx"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose an embodied multi-modal and multi-task generalist agent, aiming to improve the agent's ability to understand and interact with the 3D world. The experiments demonstrate the effectiveness of the proposed LEO under various embodied tasks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The motivation of this paper is interesting and reasonable since the LLM-based agent needs the ability to complete 3D tasks for applying in the real world. I agree with the viewpoint that \"advancements in 3D scene-level understanding have significantly lagged behind\".\n\n2. The LEO can perceive, ground, reason, plan, and act in the 3D world simultaneously and obtain promising results.\n\n3. The dataset and fine-tuning method for constructing generalist LEO provides good contributions and insights for the embodied AI community. \n\n4. Extensive details and demonstrations are provided in the appendix, which makes the work easy to follow, and experiments are sufficient."
            },
            "weaknesses": {
                "value": "- The section of related works should be included in the main text to ensure the completeness of the paper.\n\n- The way to achieve vision-language-action simply follows RT-2, thus lacking some novel design and weakening the technical contribution of this work.\n\n- The authors need to further polish the writing."
            },
            "questions": {
                "value": "- Why is the 3D point cloud data for the third-person global view needed by an embodied agent?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1150/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698767689393,
        "cdate": 1698767689393,
        "tmdate": 1699636040992,
        "mdate": 1699636040992,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "A67lriQ5o8",
        "forum": "4QaKdsh15T",
        "replyto": "4QaKdsh15T",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1150/Reviewer_ADo8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1150/Reviewer_ADo8"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an embodied multi-modal agent called LEO that can understand and interact with 3D world. LEO builds on top of Vicuna-13B large language model (LLM), and learns to encode 2D images as well as 3D point-cloud of the environment. The model is capable of generating natural language text (like caption, or answers to questions about the environment), as well as actions (for navigation or manipulation). To train this model, the authors collect a large 3D vision-language alignment dataset at object-level and scene-level to learn grounding. Further, this  model is fine-tuned on several tasks including captioning, question-answering, dialogue, planning, navigation and manipulation."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors attempt a fairly ambitious idea of building generalist embodied agents which can not only understand 2D content, but also 3D scenes. The proposed approach aims to generate both text as well as actions. The effort to integrate all these signals into the model, and setting up training on so many tasks is commendable.\n- The authors rightly point out that progress in building generalist embodied agent is severely restricted by large-scale ground-truth data. Collecting such data manually is expensive and restrictive so I agree with the overall idea (regardless of its execution in the paer) of collecting this data semi-automatically using LLMs."
            },
            "weaknesses": {
                "value": "1. I have strong concerns regarding automatic data-collection using LLM and its accuracy. For instance, in Table 21, the authors describe a single example from task-specific datasets. In that table:\n    1. task planning describes \u201cwiping away dust\u201d, \u201cchecking the stability of chair\u201d, \u201cadjusting the height of the chair\u201d, \u201cvacuuming dirt or crumbs\u201d, \u201cchecking the lighting\u201d. This low-level plan is clearly not grounded in the scene nor the embodiment of the robot. The chairs depicted in the scene cannot be adjusted, how will the robot change the temperature / ventilation / lighting of the scene. This kind of pre-training / fine-tuning is unnecessary and incorrect. \n    2. Similarly, the 3D captioning task example has \u201ca white cabinet in the corner of the room. in the direction from the door and from the inside. it will be on the left, there is a small brown table on the left side of the cabinet and a smaller table on the right side of the cabinet\u201d This example clearly has a lot of grammatical mistakes, and makes me question the GT data against which the system is evaluated. \n    3. For scene captioning, the generated caption has very little overlap with the scene. It describes the chair as being of the same color as the floor (which is incorrect). Their 3D positions are described incorrectly as well. \n2. The ablations performed in the paper are unclear and inadequate. The paper doesn\u2019t perform any component level ablations on the model (removing 2D images as input, removing 3D images as input, checking against a blind baseline). Without such ablations, there is no way to tell how important each components of the system are. Additionally, it\u2019s unclear what the take-aways message from ablations (Table 18, and Figure 4(a)) are.\n3. It seems that a bunch of decisions in the paper (box-based vs scene-graph prompting, etc) are taken by qualitatively comparing the output. This is an insufficient way to justify design decisions (specialy for GT data collection), and without good-faith human evaluation / quantitative evaluation, it\u2019s unclear if the GT can be trusted or not. This puts into question the entire GT dataset, and its unclear if any followup evaluation on this benchmark can be trusted or not. For instance, in Figure 13, it\u2019s unclear which one of box-based prompting or scene-graph based prompting is better.  Similarly, in Table 11, the author uses GPT-4\u2019s evaluation of two generated captions to decide that partial scene graph is sufficient. I think relying on GPT-4 to subjectively evaluate two captions and checking whether it\u2019s sufficiently grounded to a 3D scene is problematic. \n4. Results on object-navigation are surprisingly low. Can the authors comment on why is the success rate so low compared to state of the art object navigation methods. Additionally, the object navigation setup requires the agent to navigate \u201cunexplored\u201d environments. But the proposed approach assumes a static \u201cknown\u201d environment. Can the authors please clarify the experimental setup for object navigation?"
            },
            "questions": {
                "value": "Apart from questions asked in the weakness section, here are some additional questions: \n\n1. How does the agent handle multi-step trajectory of the agent (observation action pair for multiple time steps)? Is the agent only trained as a markovian policy which doesn\u2019t require encoding the history? Or is the trajectory encoded by simply appending each observations sequentially into the LLM? \n2.  CLIPort numbers in Table 3 are lower than the highest reported numbers in the CLIPort paper. For completeness and accuracy, can the authors update the CLIPort numbers to the highest numbers reported in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1150/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699291514325,
        "cdate": 1699291514325,
        "tmdate": 1699636040933,
        "mdate": 1699636040933,
        "license": "CC BY 4.0",
        "version": 2
    }
]