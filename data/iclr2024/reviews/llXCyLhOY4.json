[
    {
        "id": "BfYbTD8tdW",
        "forum": "llXCyLhOY4",
        "replyto": "llXCyLhOY4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5386/Reviewer_GrEH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5386/Reviewer_GrEH"
        ],
        "content": {
            "summary": {
                "value": "This paper studies off-policy biases in multi-step goal-conditioned reinforcement learning (GCRL). It focuses on successful trajectories and divides the biases into two types: shooting bias and shifting bias, which correspond to TD errors of multiple steps and the errors of the action-value function Q at goal states. For the former, the paper suggests that sometimes this bias is helpful and proposes to use quantile regression to utilize this type of bias. For the latter, the paper proposes to truncate the multi-step target to the step in which the goal is first reached. An empirical study shows the proposed algorithm, BR-MHER, consistently performs well on the six tested domains."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper has a very clear structure and is well organized. It first gives an introduction to GCRL and different types of biases. Then, it provides a literature review on multi-step off-policy (goal-conditioned) RL and quantile-based regression, which is used to utilize potentially useful shifting bias. It then analyzes the multi-step target in GCRL and introduces two types of biases, followed by corresponding treatment. Finally, metric and empirical results are given to illustrate the effectiveness of the improved algorithm. Overall, the logic is clear."
            },
            "weaknesses": {
                "value": "The major weaknesses are its presentation and soundness.\n\nFor the presentation, the paper needs to improve its preciseness and rigorousness. First of all, there are a lot of typos and undefined symbols, which make it difficult to understand the accurate message intended to be delivered (see Questions for an unexhausted list). Apart from these, the paper is also not precise and rigorous in its exposition. For example, 1) in Eq. (1), it defines the policy as the greedy policy of the $Q_\\theta$ function, while it is actually a parameterized policy and may be different from the greedy policy; 2) the reuse of $\\delta_\\theta$ in Eq. (3) is really confusing as it was defined as the TD error already; 3) In Eq. (4) and Eq. (5),  a) $r_i^{\\tau^g_\\pi}$ is not defined; b) the exponent of the $\\gamma$ in the final term is off; c) does Eq. (4) hold? If yes, how?\n\nFor its soundness, the biases that the paper proposed are either not precise or not well justified. Despite the effort to isolate the defined off-policy biases from other biases (like over-optimistic bias and hindsight bias), the shooting bias seems to couple with the bias from the target network. As for the shifting bias, it appears that it is just an estimation error instead of a bias due to the off-policy issue of HER. At least from its current presentation, it seems farfetched to call the defined shooting and shifting biases off-policy biases."
            },
            "questions": {
                "value": "Questions that may affect the assessment:\n1. What doesn\u2019t an episode or an n-step trajectory terminate upon reaching a goal? And why can\u2019t we just set the values of goal-reaching states to zero?\n2. How can we get Eq. (4)? Isn\u2019t $Q_\\theta$ the estimation of the action value function?\n\nClarity and typos (that have a low impact on the assessment independently):\n1. The naming of the algorithms is a bit confusing. In the appendix, all the three variants of the proposed algorithm, TMHER($\\lambda$), QR-MHER, and BR-MHER, are using the $\\lambda$ target for learning, which is only indicated in TMHER($\\lambda$). This may cause confusion that QR-MHER and BR-MHER don\u2019t use the $\\lambda$ target.\n2. The paper focuses exclusively on deterministic environments, which is mentioned in the introduction but should also be explicitly stated in the preliminary section. Defining $\\mathcal{T}$ to be a deterministic transition function will help make this clear.\n3. In Eq (1) on page 3, what is $\\mathcal{B}$? Is it a dataset?\n4. Before section 4 on page 3, \u201ccan truncate\u201d should be \u201ccan be truncated.\u201d\n5. In the first equation (unlabeled) on page 3, 1) $\\pi_\\phi$ should be $\\pi_\\psi$; 2) looks like $\\theta$ and $\\bar{\\theta}$ are swapped, which is really confusing.\n6. On page 3, the details of the derivation of Eq. (2) are not in Appendix A, which I assume is mistakenly written as the derivation of Eq. (3) in A.8.\n7. Several typos in the derivation in A.8: 1) Should the $t+1$s on the left-hand side be $t$? 2) What are $s$ and $a$ in the first equation? 3) Are the subscripts for all $s_{t+1}^\\tau$s off? 4) What\u2019s the definition of $r_t$?\n8. Be consistent with the naming of the bias. For example, in section 5 on page 4, \u201cshift\u201d should be \u201cshifting,\u201d and \u201cshoot\u201d should be \u201cshooting.\u201d\n9. On page 5, \u201cQuatile\u201d should be \u201cQuantile.\u201d\n10. On page 6, there is no Eq. (8)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5386/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698618441195,
        "cdate": 1698618441195,
        "tmdate": 1699636545122,
        "mdate": 1699636545122,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sxGnWcw8Pj",
        "forum": "llXCyLhOY4",
        "replyto": "llXCyLhOY4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5386/Reviewer_NQk6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5386/Reviewer_NQk6"
        ],
        "content": {
            "summary": {
                "value": "This paper provides an analysis of the bias in goal-conditioned reinforcement learning (GCRL) when using multi-step/n-step targets in temporal difference (TD) learning. In particular, it decomposes the bias in two types: 1) Shooting bias, defined by the sum of the  temporal difference errors in the n-steps along the sampled data trajectory. 2) Shifting bias, defined by the agent's advantage computed on the same n state-action tuples. Hence, to mitigate underestimation from the shifting bias without severely hindering its positive effects for faster reward propagation, they propose to employ a Quantile huber loss, placing a higher weight on the TD-targets with high Q-values, together with other complementary methods from prior work.  Furthermore, to mitigate the shifting bias, they propose to truncate the rewards in the TD-targets whenever a goal-state is reached (which in GCRL, with rewards being uniformly negative outside the goal states, always provides an upper bound on the TD-targets). Empirically, they show gains over prior GCRL algorithms with different values of n."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- GCRL and dealing with sparse is a relevant problem where progress can have a concrete impact for several applications of RL.\n\n- I found the formal classification of shifting/shooting bias to be quite interesting.\n\n- The introduced methodology is quite intuitive and follows nicely from the analytical considerations."
            },
            "weaknesses": {
                "value": "1. I found that the current way the analysis is presented makes it unnecessarily convoluted and hard to read. In particular, I believe there is an overuse of equations without proper context. For instance, Section 5.1 tries to formalize the intuitive argument that for the transitions with high Q-values, the n-step bias is less detrimental and can even aid faster reward propagation, but I feel this argument is never made explicit. I would start each subsection with a statement about what that the authors are trying to claim or about the introduced methodology to improve clarity. Furthermore, some statements also seem imprecise e.g. In connection to the proposed truncated multi-step targets, the authors state \"Since the agent is already in the goal states, the need for rapid reward propagation through multi-step learning is naturally reduced.\" (page 6) However, I believe that in the context of goal-based RL (with rewards always negative for states but the goal) the proposed truncation should always lead to even faster reward propagation since it will always produce an upper-bound to the original targets.\n\n2. I found the empirical results from to be unsatisfactory. The proposed algorithm combines multiple practices and existing methodology to combat two identified sources of bias. However, the paper is entirely lacking ablation studies/analytical experiments to understand which type of bias is most damaging and which component of the proposed methodology is playing the most effect in bias mitigation and performance. For example, is it the huber component of the loss/the quantile component/ or MHER(\u03bb) that is most helping to fend off shooting bias? Furthermore, both the number of environments and training seeds is quite limited.\n\n3. I also found the presentation of the current result to currently be not up-to-standard. For instance, the proposed algorithm is compared with each baseline individually, which I found this to be unnecessarily redundant, taking up a lot in the main text that could be used to better explain the methodology or provide additional results. I believe it would also help to summarize the results and show the actual gains of the proposed algorithm comparing the performance when picking the best value of n for each baseline.\n\n4. I find the experiments to not be fully reproducible. For instance, no details are provided as to how the OpenAI environments are modified (as stated in the 3rd paragraph of Section 6.2), and there are no details/pseudocode descriptions to understand exactly how the TSB and ISB metrics to estimate bias are computed in practice.\n\nMinor:\nA few sentences in the text contains grammar errors and repetitions (e.g., the second sentence of page 3, 'Quatile Regression' page 5, 'the Huber loss threshold (\u03ba) is 10.' is stated twice in Section 6.2...). To improve readability, I would suggest passing the paper through a grammar-checker."
            },
            "questions": {
                "value": "- I did not understand why the authors claim that \"the shifting bias is unique to multi-step GCRL,\" if I am not miss-understanding something, I do not see why the decomposition of Equation 6 would not hold in arbitrary RL problems. Can the authors clarify this point?\n\n- Can the authors explain exactly how the OpenAI codebase is modified? (as stated in the 3rd paragraph of Section 6.2) I think this would be important to clarify for reproducibility, especially since the code is not shared.\n\nWhile I believe this work has a good potential, I believe it would really benefit from additional empirical analysis, to show the consequences of the shifting/shooting biases. Moreover, I found the overall presentation and experiments to be currently insufficient for a high-quality paper. For these reasons, I currently leaning towards rejection, but I am willing to revise my score if the authors manage to properly address my concerns."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5386/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698793134901,
        "cdate": 1698793134901,
        "tmdate": 1699636545006,
        "mdate": 1699636545006,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mszUw9mRg1",
        "forum": "llXCyLhOY4",
        "replyto": "llXCyLhOY4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5386/Reviewer_aQNs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5386/Reviewer_aQNs"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the bias problem in goal-conditioned reinforcement learning (GCRL) setting. The authors first propose to categorize the bias into 2 types, \"shooting\" and \"shifting\" biases. A new algorithm Bias-Resilient MHER (BR-MHER) is propsed. Empirical results are provided to show the proposed method is generally better than a number of other baselines on a number of robotic control tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**originality**\n- the bias decomposition can be an interesting novel analysis\n\n**quality**\n- the presentation of the paper is OK\n\n**clarity**\n- the paper is mostly clear\n\n**significance**\n- the theoretical analysis on the different types of bias can be interesting for the community\n- the empirical results show a fairly significant improvement on performance and bias reduction for the proposed method compared to other baselines."
            },
            "weaknesses": {
                "value": "Writing and presentation: \n- The abstract is a bit too vague, very little information is given. In other parts of the paper, it is also often the case that the wording is fancy but vague and makes it hard to understand what the authors are trying to deliver. For example, how exactly do you define resilience? And what exactly is the robotic benchmark you are using? And how do you know a certain bias is beneficial? \n\nMotivation \n- it is unclear to me what is motivating the proposed method, and why we need to decompose the bias, there seems to be no motivating examples on why this is a problem. Additionally, after clipped double Q in TD3, there are a large number of other bias reduction techniques proposed for off-policy learning, how does the proposed method compare to these? \n\nThe proposed method\n- it is unclear what exactly the authors did in the proposed method. I think there should be a focused discussion in the main paper on what exactly the proposed method does, and how it does differently compared to other baselines. \n\nI believe the paper has some interesting results but can benefit from rewriting and re-organizing."
            },
            "questions": {
                "value": "- Why should we study the 2 different types of bias, what is a motivating example that justifies the need for this analysis and the design of the algorithm?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5386/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698804677421,
        "cdate": 1698804677421,
        "tmdate": 1699636544906,
        "mdate": 1699636544906,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "K415jaw6qx",
        "forum": "llXCyLhOY4",
        "replyto": "llXCyLhOY4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5386/Reviewer_3TCh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5386/Reviewer_3TCh"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new multi-step off-policy GCRL method that can ensure a resilient and robust improvement with a large step. Experiment results show the improved performance of the proposed method on several benchmarks"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper aims to address an important problem for multi-step off-policy. The proposed method can enable a larger step size to speed up GCRL."
            },
            "weaknesses": {
                "value": "The writing of the paper is not satisfactory, making it hard to follow.\n\n- The paper should spend more space in presenting the new algorithm. Section 4.3 is not clear to me.\n\n- I don't understand what you mean by \"learning target for a TD error\". (before eq.3)\n\n- I don't quite understand Proposition 1: The advantage function should be positive for the optimal action.\n\n- Why the first term in eq. 3 called the off-policy advantage?\n\n- Have you defined $\\delta_{\\bar \\theta}$?\n\n- Why are 10-step learning scenarios important and challenging?  I believe this is a hyperparameter for the algorithm. \n\n- Near eq. 6: why \"for a successful .... are zero\". I cannot know why.\n\n  \n\n\n\n\n\nThere are some typical typos in the paper, making the paper less convincing and assessing the correctness of the paper.\n\n- Eq. 1. I don't understand what you mean by $Q_{\\theta}=...$. If it's a value function, why can it equal the n-step target value?\n\n- The equation before eq. 2. I believe in the right hand of the equation: the $\\theta$ and $\\bar {\\theta}$ should be exchanged! Otherwise, the derivation in A.8 does not hold.\n\n- Eq. A12. I think $s_{t+1}$ should be $s_{t+i}$?\n\n  \n\n\n\nThe experimental result does not show the superiority of the proposed method, according to Figure 1. The proposed method BR-MHER only achieves better performance on 2 of 8 tasks."
            },
            "questions": {
                "value": "Please see my comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5386/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698906415520,
        "cdate": 1698906415520,
        "tmdate": 1699636544806,
        "mdate": 1699636544806,
        "license": "CC BY 4.0",
        "version": 2
    }
]