[
    {
        "id": "XiHDp5C8jd",
        "forum": "774elYc5tw",
        "replyto": "774elYc5tw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9116/Reviewer_h9Ue"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9116/Reviewer_h9Ue"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper tackles an important problem of controlling undesirable behaviors like toxicity and hallucination in large language model text generation. This is a key challenge as models scale up. \n* The method seems generic enough to handle different types of constraints like keywords, toxicity, factual correctness etc. as evidenced by the diverse experimental tasks.\n* Results across the three tasks demonstrate improved constraint satisfaction and control over text generation with modest tradeoffs to fluency. \n* Analysis of the proposed satisfaction score on constructed benchmarks provides useful insights."
            },
            "weaknesses": {
                "value": "* The satisfaction score estimation is currently limited and may not be robust or accurate enough for all constraints. More investigation into refining this estimation would be beneficial.\n* It is not clear if the gains will sustain for very long text generation where error accumulation could occur. More analysis on larger generation tasks could help.\n* There is no human evaluation of the quality and naturalness of outputs. Automatic metrics have limitations.\n* The factual correctness results are quite noisy and could benefit from more tuning and robustness testing."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9116/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698555767825,
        "cdate": 1698555767825,
        "tmdate": 1699637147320,
        "mdate": 1699637147320,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hKOlPCNdX2",
        "forum": "774elYc5tw",
        "replyto": "774elYc5tw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9116/Reviewer_Ht7A"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9116/Reviewer_Ht7A"
        ],
        "content": {
            "summary": {
                "value": "This paper points out that language model suffer from undesired behaviors such as toxicity or hallucinations and proposes a approach called future constraint satisfaction to tackle this issue. This method forces the model to take constraints into account when generating texts. The constraints here can be expressed directly in natural language. Specifically, it controls the probability of the next generated token by adding a score in the decoding stage. This score is obtained by prompt $x$ and prefix $y_{\\leq}t$, using the log-likelihood. Experiments on three tasks: keyword-constrained generation, toxicity reduction, and factual correctness in question answering field show that this method can effectively improve efficiency and effectiveness compared to the baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper excels in its clarity and succinctness in explaining the proposed method\u2019s core idea, namely future constraint score. The subsequent formula provides a direct method for computing this score.\n- The experimental section is well-structured and comprehensive. The authors has conducted experiments on three different QA tasks, using multiple backbone models. Moreover, the impact of hyperparameters on the experimental results has also been thoroughly investigated.\n- The entire paper is well-articulated, ensuring a smooth reading experience without any obscure sections."
            },
            "weaknesses": {
                "value": "- The definition of future Constraint Satisfaction is somewhat ambiguous to me. According to the formula at the bottom of page 2, your $R(y_{\\leq t}, C(x))$ is used to approximate $\\log p(C(x)|y _{\\leq t})$, yet this is similar to the definition of $R$ provided in formula 1 on page 3. Could you please elaborate on the benefits of such a definition and why |SEP| token is added? This aspect lacks a comprehensive analysis.\n- The approach considers the impact of prompt and prefix on the next token during the generation stage, with calculations only utilizing maximum likelihood estimation. The essence of future Constraint Satisfaction appears to revolve around the next token\u2019s compatibility with the constraint. This similar idea is reflected in many controllable text generation methods, and the authors does not specifically compare these differences (only the different forms of control are mentioned in the related work section), which makes the paper seem like an incremental contribution.\n- While the proposed method is relatively straightforward, it lacks robust experimental evidence to highlight its superiority over conventional decoding strategies. Moreover, the existing experimental results suggest that the improvements are rather limited.\n- The paper would benefit greatly from the inclusion of a case Study and human evaluation. This would provide tangible examples of how this method improves issues like hallucinations. However, the authors solely provided results for some indicators (like BLEU) that have been proven to lack reliability.\n- All the figure are not in the form of vector images, which results in distortion when the images are enlarged."
            },
            "questions": {
                "value": "na"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9116/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698658276000,
        "cdate": 1698658276000,
        "tmdate": 1699637147214,
        "mdate": 1699637147214,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sJnSL4J3f6",
        "forum": "774elYc5tw",
        "replyto": "774elYc5tw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9116/Reviewer_eM49"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9116/Reviewer_eM49"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes future-constrained generation as a way to improve faithful decoding with large language models. This essentially introduces, in the beam search, a function of both the generated sequence (at a certain time step) and the future constraint also in the form of a natural language (e.g., \"the sentence will have these concepts: run team field drill\"). The function is implemented as the likelihood of generating the concatenation of both sequences using a pretrained language model. The paper shows multiple empirical results showing that the method is effective in following the constraints, even improving over a larger language model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* The use of future constraints is interesting and intuitive since they act as (self-)evaluation, ensuring that the model is still following the constraints.\n\n* The method is quite flexible in terms of the constraints that can be put in."
            },
            "weaknesses": {
                "value": "* While the method has been empirically shown to better perform than baselines in terms of n-gram overlap and correctness, there are other dimensions that are not reported. Firstly, since the method essentially introduces a call to a language model for each beam and for each timestep in the beam search, we expect that the decoding time is slow. How much is the tradeoff between this and \"text quality\"? Secondly, evaluation metrics based on n-gram overlap are not usually good rankers when the models are already very strong (which in this case they are since they are based on LLMs). Human evaluation should have been conducted. Thirdly, the authors used ALCE as a benchmark, however they did not evaluate on the QAMPARI dataset which is also part of ALCE. Finally, since the focus is on \"faithful decoding\", the paper should have focused on those evals as well (and not on metrics based on n-gram overlap).\n\n* Parts of the paper are difficult to understand. For example, since there is no mention of how the experiments are set up, it was very difficult to comprehend what the authors wanted to convey in Figure 2 since it showed a bunch of previously unintroduced models and mentioned terms not defined (e.g., Ranking accuracy). This issue is repeated in Figures 4 and 5. I think overall the paper needs proofreading.\n\n* Overall, the results do not seem convincing as most improvements (focusing on correctness which is mostly related to faithful decoding) are marginal and sometimes fail to improve over a greedy/beam search baseline. Given the complexity of the method, one would expect more significant improvements."
            },
            "questions": {
                "value": "* What is the tradeoff between time complexity and generated text quality?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9116/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698803891833,
        "cdate": 1698803891833,
        "tmdate": 1699637147092,
        "mdate": 1699637147092,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "e1ZsZegig6",
        "forum": "774elYc5tw",
        "replyto": "774elYc5tw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9116/Reviewer_cGVr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9116/Reviewer_cGVr"
        ],
        "content": {
            "summary": {
                "value": "This work define a decoding staretgy where the future tokens are constrained based on some lexical constraints which are defined in the prompt. The idea is to have generation that remain faithuful to the prompt and do not violate lexical constraint defined in the prompt. The authors define a novel LM scoring mechanism to identify if a constraint has not been satisfied yet to guide the future generation. The authors show performance of their methods on thress tasks: CommonGen, Toxicity reduction and Factual QA. Their method improves faithfulness to the prompt in most cases."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.) The paper is well written and evaluation is well thought out."
            },
            "weaknesses": {
                "value": "1.) The novelty of the new constraint scoring function is fairly limited.\n2.) Overall performance gains are not large and only help small sized LLMs."
            },
            "questions": {
                "value": "1.) Does the <SEP> token seperate the prompt and continuation? Is it same across all the LLMs? Is it repurposed from one of the special tokens during pre-training?\n2.) Does the inference mechanism ever lead to degenerate sequences? If yes, how often does that occur?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9116/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818354378,
        "cdate": 1698818354378,
        "tmdate": 1699637146967,
        "mdate": 1699637146967,
        "license": "CC BY 4.0",
        "version": 2
    }
]