[
    {
        "id": "OhApoZMgrU",
        "forum": "89ZekEEsSJ",
        "replyto": "89ZekEEsSJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3390/Reviewer_nNC3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3390/Reviewer_nNC3"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce an innovative technique for executing targeted backdoor attacks on image captioning models, which exhibit greater stealth compared to previous attacks. The proposed method initially employs universal perturbation techniques for object detection to acquire a unique trigger. Subsequently, this trigger is positioned at the center of a specific source object, and the corresponding object name in the output caption is altered to a predetermined target name. During the prediction phase, the backdoored model generates captions for input images containing the trigger, accurately conveying the semantic information of the entire image, except for the erroneous recognition of the source object as the predefined target. Comprehensive experimental results indicate that the proposed method attains a high attack success rate while maintaining minimal impact on the model's clean performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper explores the backdoor attack of image captioning models, giving a good reference to the research on this aspect.\n- The generation of the stealthy trigger is very interesting. The proposed method is simple but effective to improve the backdoor attack of image captioning models and the good performance obtained by the experiments strongly supports this point.\n- The ablation study is organized well to clearly demonstrate the whole proposed method. And it makes the paper easy to follow."
            },
            "weaknesses": {
                "value": "- [Baselines] It is important to note that this paper is not the first to explore attacks on captioning models, while there have been previous backdoor attacks [1,2] in image captioning models as introduced in the related work. To better demonstrate the superiority of the proposed approach, it is suggested that baseline methods be added to the experiments, providing a more comprehensive comparison and evaluation. Additionally, some backdoor attacks [3] in visual question answering (VQA) can also be considered by removing the trigger of the textual question part.\n\n- [Datasets] To ensure a more comprehensive evaluation of the proposed approach, I suggest conducting experiments on additional datasets beyond Flickr. By incorporating the MS-COCO dataset and possibly other diverse sources, you can better assess the generalizability and robustness of your method across different data distributions.\n\n- [Defenses] It is important to consider incorporating more advanced and recent defenses to ensure a comprehensive assessment. The defenses mentioned, such as the Saliency Map (2017), STRIP (2019), Spectral Signature (2018), Activation Clustering (2018), and ONION (2021), represent a range of approaches developed over the years. However, numerous advanced backdoor defenses have been proposed since then, and it would be beneficial to replace or supplement the current defenses with these more advanced techniques.\n\n- [Perceptibility] In your proposed stealthy backdoor attack, it would be beneficial to include additional experiments to better illustrate the stealthiness of the approach. For instance, you could measure the LPIPS distance between the original samples and poisoned samples. Furthermore, conducting a human perceptual study for your poisoned samples would offer valuable insights into the ability of your method to generate stealthy attacks that are difficult for humans to detect.\n\n- [Typos] There are some typos in your paper. Please check the grammar again.\n\n(i) The original sentence used \"samples\" (plural) with \"each\" (singular). The corrected sentence should use \"sample\" (singular) to agree with \"each\" in your Adversarial Capability subsection. \"In addition, for each poisoned samples, the maximum number ...\"\n\n(ii) There are double 2022 in your reference. \"Hyun Kwon and Sanghyun Lee. Toward backdoor attacks for image captioning model in deep neural networks. Security and Communication Networks, 2022, 2022.\"\n\n[1] Hyun Kwon and Sanghyun Lee. Toward backdoor attacks for image captioning model in deep neural networks. In Security and Communication Networks, 2022.\n\n[2] Meiling Li, Nan Zhong, Xinpeng Zhang, Zhenxing Qian, and Sheng Li. Object-oriented backdoor attack against image captioning. In ICASSP, 2022.\n\n[3] Walmer, Matthew, et al. Dual-key multimodal backdoors for visual question answering. In CVPR, 2022.\n\n[4] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018."
            },
            "questions": {
                "value": "Listed in the weakness of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3390/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3390/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3390/Reviewer_nNC3"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3390/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698654151296,
        "cdate": 1698654151296,
        "tmdate": 1699636290243,
        "mdate": 1699636290243,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rcHaJTflva",
        "forum": "89ZekEEsSJ",
        "replyto": "89ZekEEsSJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3390/Reviewer_1VPN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3390/Reviewer_1VPN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a backdoor attack method against image captioning. Different from previous works, its target was to only change the source object in the output sentence to the target object, making it stealthier. Object detection and universal adversarial perturbation were utilized to establish an association between the trigger and the target object. Extensive experiments were conducted to demonstrate the effectiveness of the proposed method and analyze hyperparameters\u2019 influence on the attack performance. Moreover, the attack was resistant against several backdoor defenses."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper proposes a stealthy backdoor attack against image captioning, with the specific goal of altering only a limited portion of the output sentence.\n- Extensive experiments are conducted to evaluate the influence of training strategy and hyperparameters."
            },
            "weaknesses": {
                "value": "- The effectiveness of the proposed method was only evaluated on a CNN-LSTM model. I suggest evaluating it against some more advanced target models.\n- The backdoored model\u2019s performance on benign samples was evaluated only using the BLEU-4 metric. More metrics, such as METEOR and CIDEr, can be considered to better evaluate the backdoored model\u2019s benign performance."
            },
            "questions": {
                "value": "- In Table 2 and Table 3, the ASR (%) reported ranged from 0 to 1, which may require scaling to a 0-100 range to maintain consistency with the ASR values in Table 1.\n- During the test stage, for an image containing the source object, input its backdoored version and clean version to the infected model. Are the two output sentences distinguishable only by the object name, like the training samples? Could you give some concrete examples of the output sentences in the test stage?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3390/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3390/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3390/Reviewer_1VPN"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3390/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698748861236,
        "cdate": 1698748861236,
        "tmdate": 1699636290171,
        "mdate": 1699636290171,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hdqejpJwcv",
        "forum": "89ZekEEsSJ",
        "replyto": "89ZekEEsSJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3390/Reviewer_MmsG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3390/Reviewer_MmsG"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a targeted backdoor attack technique tailored for image captioning models. It uniquely considers the text-to-image matching relationship, enhancing the attack's stealthiness against semantic judgment-based detection. The authors craft a universal scrambling trigger, strategically placing it at the center of a designated target while altering its category. Through their experiments, they reveal that this method discreetly evades current backdoor defenses, marking a novel security concern for image captioning models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed attack scenario is practically relevant. The attack strategies introduced by the authors are innovative, with their emphasis on trigger obfuscation offering tangible benefits.\n2. Comprehensive coverage of related literature. The article delivers an extensive overview of prior work, granting readers insight into the historical progression of research in this domain."
            },
            "weaknesses": {
                "value": "1. Despite focusing on the image captioning model, a relatively less-explored area, the authors fail to elucidate the specific motivations behind choosing this model. Existing research has delved into backdoor attacks on VQA tasks[1]. So, what challenges does the image captioning model present in contrast to VQA-targeted backdoor attacks?\n2. The proposed methodology lacks distinct innovation. The authors execute the backdoor attack by affixing a general-purpose trigger to the target entity and adjusting its label. This strategy bears resemblance to targeted detection backdoor attacks, with the primary distinction being the substitution of labels with caption descriptors.\n3. The experimental setup and results fall short of comprehensiveness. The authors' experiments do not benchmark against prevalent backdoor attacks and defense strategies. Adapting these conventional methods to the context of image captioning attacks is straightforward. The authors could draw comparisons with these methods to affirm the superiority of their approach.\n\n[1] Dual-Key Multimodal Backdoors for Visual Question Answering."
            },
            "questions": {
                "value": "Please refer to the Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3390/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698754578192,
        "cdate": 1698754578192,
        "tmdate": 1699636289992,
        "mdate": 1699636289992,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RpBm1ECgrS",
        "forum": "89ZekEEsSJ",
        "replyto": "89ZekEEsSJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3390/Reviewer_sNE8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3390/Reviewer_sNE8"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates the backdoor attacks against iamge captioning task (Model: Object Detection Model + CNN-LSTM). The proposed method learns an optimized trigger for object detection, and adding the trigger to the middle of the object. The adversarial goal is to incorrectly recognize the source object as target object, while maintaining the rest of semantic information unchaged. The proposed method can successfully bypas existing backdoor defenses."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Combining backdoor attack and image captioning task is exciting. \n\n2. The proposed method is easy to understand.\n\n3. The defense analysis is appreciable."
            },
            "weaknesses": {
                "value": "0. What is the main difference between the proposed method and another paper \"Dual-Key Multimodal Backdoors for Visual Question Answering\"? Both papers use similar arch, and generate the optimized trigger based on object detector. However, the proposed method inserts trigger in the middle of object, while another paper's trigger size is the same as image size. \n\n1. The Figure 1 is not very intuitive. The image caption models maybe common, but involving the specific architecture in Figure 1 (The Experimental Setup Section is not clear to me) would help to understand the attack procedures. I assume the input images go into the object detector first, then the output features would be forword to ResNet101-LSTM model for text generation? Are there any other modules between object detector and ResNEt101-LSTM? \nAlso, the proposed method adds the trigger to the center of bounding box generated by object detector. It would be better to illustrate it in the Figure 1 (for better understanding).\n\n2. The proposed method is experimented with one image captioning model ( YOLOv5s+ ResNet101-LSTM). What about other CNN-LSTm archs? On the other hand, there are popular tranformer based methods for image captioning task (e.g., CLIP). I am curious, would the proposed method works on those models?\n\n3. The experiments are conducted with two similar dataset (Flickr8/30k). In order to show the generalization ability, is it possible to extend to other common image captioning dataset (e.g., COCO caption)?\n\n4. How important is the Trigger location? The author mentions adding a trigger $\\delta$ in the middle of the bounding box (hoping that the trigger only affects the features of the object without affecting the model's recognition of other parts of the image). What if we change the trigger location, would ASR/BLEU drop?\n\n5. The definition of $ASR=\\frac{N_p}{N_t}$, where $N_p$ is the number of sentences containing the target object and $N_t$ is the number of sentences containing the ground truth source object. Here every sample's output would be count as one sentence, or there might be multiple sentences. If the former case, then it would be same to denote $N_p$ as the number of outputs which target object appears and $N_t$ is the number of total samples. If the later case, we can simply let every output sentence contains the target object, says the first word in every sentence is the target object?"
            },
            "questions": {
                "value": "Please see the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3390/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3390/Reviewer_sNE8",
                    "ICLR.cc/2024/Conference/Submission3390/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3390/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698787552394,
        "cdate": 1698787552394,
        "tmdate": 1700680298985,
        "mdate": 1700680298985,
        "license": "CC BY 4.0",
        "version": 2
    }
]