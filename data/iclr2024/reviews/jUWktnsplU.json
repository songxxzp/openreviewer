[
    {
        "id": "w87YUBZIH9",
        "forum": "jUWktnsplU",
        "replyto": "jUWktnsplU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1178/Reviewer_4NJ5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1178/Reviewer_4NJ5"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes hybrid distillation, which uses a supervised/CL teacher and a MIM teacher for a balance between discrimination and diversity. The authors use average attention distance and NMI metric to analyze the disadvantages of previous methods with respect to the above two properties, and then design hybrid distillation according to the disadvantages. Hybrid distillation achieves strong performances on various downstream tasks (classification/detection/segmentation) using ViT-B/L models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed hybrid distillation is simple and reasonable according to the discrimination and diversity.\n2. Better performances on various downstream tasks.\n3. Good writing and abundant visualization."
            },
            "weaknesses": {
                "value": "1. It seems that using two teachers for distillation really boosts the performances on downstream tasks. Will it take a longer pre-training time compared to only using CLIP as the teacher? I hope the authors can show the training time and the memory usage during distillation.\n2. Previous CL-based methods outperform MAE on linear probing because they own strong discrimination abilities. Since the hybrid distillation keeps this ability, can authors show the linear probing performances on ImageNet-1k?"
            },
            "questions": {
                "value": "Please refer to weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1178/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1178/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1178/Reviewer_4NJ5"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1178/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698751170543,
        "cdate": 1698751170543,
        "tmdate": 1699636044216,
        "mdate": 1699636044216,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oGs667Bk0h",
        "forum": "jUWktnsplU",
        "replyto": "jUWktnsplU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1178/Reviewer_ZCMG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1178/Reviewer_ZCMG"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the feature spaces and models from two different pretraining paradigms, contrastive learning  (CL) and masked image modelling (MIM). It reaffirms differences around discrimination of the produced features as well as the attention diversity and makes a number of interesting observations. Based on those, the paper proposes Hybrid Distill, a hybrid distillation method that tries to get the best of both CL and MIM. An MIM teacher is used at intermediate layers and a CL teacher at the output and this leads to models with both token diversity as well as features with strong discrimination."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper studies very interesting properties and tries to understand the difference in behaviour between common learning paradigms like CL and MIM.\n\n* The study of sec 2 is sound and with interesting observations.\n\n* The method presented in Sec 3 is to my knowledge technically novel. \n\n* The experimental validation seems complete and shows some gains for the proposed approach on the common setting."
            },
            "weaknesses": {
                "value": "1. Sec 2 does not discuss performance, ie the \"discrimination\" aspect. It is unclear if this is because the two paradigms work equally well on downstream tasks? Probably thats not exactly the case, and generalization is in the end what we care about, ie what we use these features for. Also, wrt 2.2, studies (eg Wang et al and Sariyildiz et al below as well as the papers that introduce such projectors) have shown that the size of projector/\"asymmetric decoder\" matters wrt to downstream task behaviour.\n\n\n2. The method proposed in Sec 3 tries to get the best of both worlds by using two teachers, something that makes training harder and requires more memory.\n\n3. More of a note than a weakness:  It is now hard to compare the relevant rows in Tables 1 and 2. Tab 1 would be more easy to understand and compare if the two sections are wrt the two different backbones (ie ViT-B on top, ViT-L on the bottom). Inside each section, a separate split should be wrt what is the teacher. The models that distill clip should be clearly separated, cause they start from a much stronger teacher. Same for the ViT-L from the authors, that distills im21k.  These results are not comparable to the rest, and the im21k-distilled row seems out of place in both tables. Please make sure to separte accordingly.\n\nWang, Yizhou, et al. \"Revisiting the transferability of supervised pretraining: an mlp perspective.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\nSariyildiz, Mert Bulent, et al. \"No reason for no supervision: Improved generalization in supervised models.\" ICLR 2023-International Conference on Learning Representations. 2023."
            },
            "questions": {
                "value": "- What do the authors mean by \"s. For ViT-L, the distillation is based on both ImageNet-1K and ImageNet-21K\"? It is for different models or for the same model?\n\n- \"The hyperparameter \u03b1 and \u03b2 are set to 1.0\" - how is performance affected if the two teacher losses are not balanced like this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1178/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829179984,
        "cdate": 1698829179984,
        "tmdate": 1699636044144,
        "mdate": 1699636044144,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3mKSkwBlD4",
        "forum": "jUWktnsplU",
        "replyto": "jUWktnsplU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1178/Reviewer_zDe2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1178/Reviewer_zDe2"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a distillation or learning method for pretraining visual representation. Motivated from the differences in MAE, DeiT, CLIP models, in terms of feature diversity and attention distance, this paper proposes to distill differently for the two targets, specifically learn features from DeiT/CLIP models and attention maps from MAE methods. Results show some improvements over baselines on classification, detection, segmentation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation is presented quite clearly given existing studies on average attention distance and NMI.\n2. The ablations are extensive and convincing that HybridDistill helps compared with vanilla MAE or DeiT."
            },
            "weaknesses": {
                "value": "1. Unfair comparison. Table 1 compares to many vanilla training methods on ImageNet-1K but Hybrid Distill exploits CLIP targets and ImageNet-21K for training. Other methods exploiting the joint objectives, better models, or larger scale data are not compared.\n2. The two metrics of average attention distance and NMI are existing metrics. They provide insights to models but are still neither necessary or sufficient condition for a good representation. Existing works such as dBOT also visualized the average attention distance differences in existing methods (DeiT, DINO, MAE, etc.) and even different distillation stages and observed similar conclusion as well. Existing joint learning work with the distillation objective is producing good representation already.\n3. HybridDistill is complicated and designed specifically to ViT architecture with global attention map. Therefore, it looks challenging to apply to other architectures such as Swin or MViT or CNNs.\n4. Marginal performance gain compared with existing solutions in fair comparison. E.g. SdAE in Table 1 achieves 84.1, iBOT achieves 84.0, etc. with ViT-B without CLIP or ImageNet-21K, but Hybrid Distill achieving 83.7 lags behind. Similarly limited gain is observed with CLIP targets, e.g. dBOT 87.8% vs. this paper 87.6% with ViT-L ImageNet-1K CLIP targets, and Cascaded detection settings and ADE20K segmentation.\n5. Runtime compared with baselines. Since Hybrid Distill requires inferencing with multiple backbones and uses less aggressive masking strategy, what is the runtime of one epoch compared with vanilla MAE?\n6. The claim of using fewer epochs isn\u2019t quite important here considering the computation used in training all the targets (including CLIP)."
            },
            "questions": {
                "value": "(see weakness for details)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1178/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699205744545,
        "cdate": 1699205744545,
        "tmdate": 1699636044081,
        "mdate": 1699636044081,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FfxeyjpqR3",
        "forum": "jUWktnsplU",
        "replyto": "jUWktnsplU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1178/Reviewer_mHak"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1178/Reviewer_mHak"
        ],
        "content": {
            "summary": {
                "value": "After discussing the \"diversity\" and \"discrimination\" in the features of a distilled Transformer, this paper proposed a hybrid distillation framework that simultaneously distills knowledge from two distinctively different pre-trained teacher. Specifically, HybridDistill designs the distilling target and location from a pre-trained MIM model and a supervised/CL model. A progressive redundant token masking strategy is also proposed for reducing the distilling costs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper provides many results and discussion on comparing the pre-head attention distance with different types of teacher model, which could be a helpful resource for the community.\n- The paper provides through ablation studies on each component of the proposed method."
            },
            "weaknesses": {
                "value": "- My biggest concerns are on the validity of the motivation to encourage both \"diversity\" and \"discrimination\" in the Transformer.\n  - About \"diversity\":\n    - This submission claims that \"diversity\" ```means that the model pays attention to both local and global information and can achieve more evenly distributed representations```. What is ```evenly distributed representations'''? Specifically, which does it mean by \"distributed representation\"?\n    - My understanding is that the \"diversity\" is quantitively measured by both average attention distance and NMI. I am not sure if \"diversity\" is a good terminology in this case, which oversimplified many necessary context. I suggest, for example, \"different per-head attention distance\", to more accurately describe the discussed property. For simplicity, I will still use \"diversity\" in my review.\n    - I am **especially** not fully convinced that \u201cdiversity\u201d itself is something clearly worth to pursue in a pre-trained Transformer. Yes, \"diversity\" is evaluated in several previous works, but often as a side information/evidence to better understand the self-attentions. However, it is not convincing that \u201cdiversity\u201d in the last layers is something to specially encouraging. For a starter, what *should* be *ideal* per-head attention distance in each layer already requires serious discussion. It is hard to believe that focusing on long-term/global information in the last layers is unquestionably bad. If the authors argue that \"diversity\" in last layers may help dense-level tasks, while, then why not simply make better usage of multi-layer features in these dense tasks instead of forcing last layer features to be \"diverse\"?\n  - About \"discrimination\":\n    - This is a question: what metric is used to quantitively evaluate \"discrimination\" in the experiments?\n \n- Questions on results:\n  - Why MAE's COCO results in the main Table (Table 1) are **lower** than the results reported in MAE's original paper? For example, in MAE, a ViT-B is reported to get 50.3 APbox and 44.9 APmask, but 48.4 APbox and 42.6 APmask in this submission? Please highlights the experimental setting differences to MAE and other compared baselines in the paper if this is the reason. However, if the setting difference is the reason for performance difference on COCO, then why ADE20K results are matched?\n  - If directly distilling CLIP's feature harms dense tasks for its inadequate \"diversity\" in self-attention distance, then why results reported in MILAN outperforms the proposed method on COCO detection/instance segmentation and ADE20K semantic segmentation? I think MILAN directly predictions CLIP's features. Also, MILAN is not cited and compared in the submission.\n\n[MILAN] Hou et al, MILAN: Masked Image Pretraining on Language Assisted Representation."
            },
            "questions": {
                "value": "Discussions/addressing the questions in **bold** in Weakness will be very helpful. Thanks!"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1178/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699468995908,
        "cdate": 1699468995908,
        "tmdate": 1699636044000,
        "mdate": 1699636044000,
        "license": "CC BY 4.0",
        "version": 2
    }
]