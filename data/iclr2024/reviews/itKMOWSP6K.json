[
    {
        "id": "N5pUsbCrGK",
        "forum": "itKMOWSP6K",
        "replyto": "itKMOWSP6K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2240/Reviewer_5zjx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2240/Reviewer_5zjx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a multi-modal fusion transformer-based framework for 3D object detection. The idea of this paper is intuitive and this paper is easy to understand. The performance on nuScenes is good."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.This paper is easy to read.  \n2.The performance of the model in this paper show superiority in nuScenes.  \n3.The idea is intuitive."
            },
            "weaknesses": {
                "value": "1.Only one dataset is used in this paper. The generality of the framework should be analyized.  \n2.In introduction, Fig.1(b) is not compared with their method in details.  \n3.The most  baselines only consider LiDAR and camera, which are not fair to compare. Can you compare with them without temporal information? And this can also further verify the superiority of  Multi-modal fusion encoder.  \n4. The two topics, i.e., multi-modal fusion and temporal consistence, are different, causing that the focus of this paper is unclear."
            },
            "questions": {
                "value": "Refer to the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/a"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2240/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697535412486,
        "cdate": 1697535412486,
        "tmdate": 1699636157245,
        "mdate": 1699636157245,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oxTPn1o0Lk",
        "forum": "itKMOWSP6K",
        "replyto": "itKMOWSP6K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2240/Reviewer_bQU3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2240/Reviewer_bQU3"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a new spatial-temporal multi-modal fusion framework for 3D object detection. The proposed framework leverages 2D image features and 3D voxel features to generate BEV features and refine the features with temporal memory banks, which are then fed to the detection head to generate 3D object predictions. The method achieves leading performance on the nuScenes dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Overall the paper is well-written with clear technical pipeline. \n* The proposed framework can work even when missing modality inputs, showing better robustness. \n* The proposed method archives new SOTA results."
            },
            "weaknesses": {
                "value": "* The motivation is not clear or sufficient. The author claims that \" state-of-the-art multi-modality frameworks need explicitly compressing the voxel features into BEV space\" and the proposed approach is aimed to address this issue. However, existing works like DeepInteraction maintain per-modality representations (2D perspective for camera and voxel space for LiDAR) and learn the interactions between these cross-modality interactions without the need for BEV intermediate representations.  What's the difference between the proposed approach and DeepInteraction? Please fully discuss the differences and highlight the novelty of the proposed methods with SOTA methods. \n* Lack of novelty. The proposed framework has a large overlay with both DeepInteraction and Bevformer. The BEV grid representation, temporal BEV feature fusion, and deformable attention have already been used in Bevformer and the cross-modality interaction of 2D features and 3D features have also been proposed in DeepInteractions. I would encourage the author to highlight the novelty and differences. \n* The memory banks contain all the previous BEV features, which is very time-consuming for learning the interactions of current BEV feature maps and previous ones. Why not choose a recurrent-style temporal fusion mechanism? \n* In the abstract and introduction, the author claims one of the main contributions is the residual architecture. However, in the method section, it is rarely mentioned. \n* When comparing with other SOTAs, is temporal information used for all the other methods for a fair comparison?"
            },
            "questions": {
                "value": "* The BEV visualization in Figure 6 (b) of the proposed method looks very good. Is this the result of a pure camera branch or generated from both LiDAR and camera inputs? What is the feature visualization of LiDAR BEV feature map?  How are the visual improvements after adding the camera modality?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2240/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2240/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2240/Reviewer_bQU3"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2240/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698459994947,
        "cdate": 1698459994947,
        "tmdate": 1700864131626,
        "mdate": 1700864131626,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ho3U7kFb4Y",
        "forum": "itKMOWSP6K",
        "replyto": "itKMOWSP6K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2240/Reviewer_q7W1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2240/Reviewer_q7W1"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a transformer-based framework for 3D multi-modality object detection. It mainly contains spatial fusion and temporal fusion modules to fuse cross-modality features and temporal features, respectively. Experiments prove the effectiveness of the proposed modules."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The direction of cross-modality fusion for 3D object detection is promising, which could bring potential effect to practical application.\n2. The whole method is simple and easy to follow.\n3. The presentation and writing is clear."
            },
            "weaknesses": {
                "value": "1. The core idea of utilizing BEV queries for temporal and cross-modality fusion is widely used in previous methods like BEVFormer. Although the proposed method is different in detailed design, the core application of BEV queries is unchanged. This harms the technical contribution of the proposed method.\n2. The runtime comparisons are missing. Because this work incorporated several attention modules in different encoders, it's essential to report the latency of each module.\n3. It's interesting that the proposed method can also support the camera-only setting in Figure 4. However, the performance in Table 3 seems not good enough compared with the clear BEV modeling like BEVDepth. Does it mean the fusion-based method in Figure 4 is not a good choice for the camera-only setting?"
            },
            "questions": {
                "value": "Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2240/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698569507955,
        "cdate": 1698569507955,
        "tmdate": 1699636157034,
        "mdate": 1699636157034,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kEIJcxqM23",
        "forum": "itKMOWSP6K",
        "replyto": "itKMOWSP6K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2240/Reviewer_Qpuj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2240/Reviewer_Qpuj"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a novel sensor fusion technique that generates the fused BEV feature from LiDAR voxel features and image features without compressing the Z-axis information. Unlike prior works that generate separate point BEV features and image BEV features and then fuse them, the proposed method directly generates the fused BEV feature using queries in the BEV space and deformable attention modules to interact with point voxel features and 2D image features. For each BEV query, the authors generate multiple reference points with different heights and project them back to the voxel space or image feature space for deformable attention. Besides, a temporal fusion encoder is proposed to include temporal information. The proposed method can also be used for pure image BEV feature generation with additional depth estimation networks. Experiments show that the proposed method provides competitive performance with SOTA in 3D object detection."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The paper is well-written and well-organized.\n2) The multi-modal fusion problem studied in this paper is interesting and timely in Autonomous Driving. \n3) The proposed method is simple and interesting. It can be seen as an extension of BEVFormer in the multi-modal settings."
            },
            "weaknesses": {
                "value": "1) The proposed method is only tested in one dataset (nuScenes) and one task (3D object detection), which may not be enough to show the generalizability of the proposed sensor fusion scheme. It would be better to include more datasets (Waymo) and tasks (e.g., segmentation)\n\n2) Though the proposed method makes use of the z-axis information, it seems to greatly increase the model complexity. To make fair comparisons with other baselines, the authors may better include complexity analysis such as FLOPS, #of parameters, and FPS \n\n3) From Tables 1 and 2, the proposed method only provides a marginal improvement."
            },
            "questions": {
                "value": "In Table 1, the authors only show the result that combines temporal information. How about the one that only uses single-frame BEV features? (i.e., FusionFormer-S in Table 2). This result is important to evaluate the sensor fusion mechanism when compared with other CL baselines."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2240/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786913329,
        "cdate": 1698786913329,
        "tmdate": 1699636156956,
        "mdate": 1699636156956,
        "license": "CC BY 4.0",
        "version": 2
    }
]