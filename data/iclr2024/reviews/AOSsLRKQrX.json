[
    {
        "id": "tYw6kbOlZq",
        "forum": "AOSsLRKQrX",
        "replyto": "AOSsLRKQrX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9419/Reviewer_M2hR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9419/Reviewer_M2hR"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes DisFormer, which extends Slot Attention and SlotFormer with \u201cdisentangled\u201d object representation for visual dynamics prediction. The disentangled representation is learnt by iteratively refining the slots over individual object representations (rather than the whole image representations), and regularizing the slots to be linear combinations of the concepts. To align the model output with the groundtruth, a permutation module is used to learn the ordering. Experiments are done on 2D Bouncing Shapes/Circles and OBJ3D to show the method works well in both in-domain and domain transfer settings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The method is clearly motivated and clearly described. By learning slots representing blocks (which are iteratively updated to recover an object representation), and regularizing the slots/blocks to be a linear combination of concepts, the slots/blocks learn disentangled representations of objects.\n2. Related works, as well as their difference with this work, are well-discussed."
            },
            "weaknesses": {
                "value": "My major concern is that the experiments are not very persuasive. \n1. The datasets are simple toy datasets. The original SlotFormer has done experiments on dataset CLEVRER, which is harder than the 2D shapes/circles used in this paper. Are there reasons why CLEVRER, or CLEVRER with more complex textures, are used for experiments?\n2. No ablations are provided. The model contains multiple components, but there are no ablation experiments to study the effect of each component. For example, the effect of recovering object representation versus image representation, the effect of learning the slots to be a linear combination of concepts, the number of slots, number of concepts, etc. should be studied.\n3. Not enough experiments are shown to prove the representations are \u201cdisentangled\u201d. This disentanglement is the major advantage of the method. However, only several examples are shown in Fig. 2 to show the disentanglement. Quantitative results, or visualization of the learned slots (e.g. using t-SNE) would be preferred.\n4. Missing experiment details. The training details including the hyperparameters are not provided. Some critical parameters (e.g. number of slots/concepts, loss weights) should be discussed."
            },
            "questions": {
                "value": "See weakness. More details about experiments would be helpful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698640836956,
        "cdate": 1698640836956,
        "tmdate": 1699637186903,
        "mdate": 1699637186903,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iusxEbIueU",
        "forum": "AOSsLRKQrX",
        "replyto": "AOSsLRKQrX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9419/Reviewer_HX9L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9419/Reviewer_HX9L"
        ],
        "content": {
            "summary": {
                "value": "This manuscript introduces a novel approach to disentangled object-centric representation learning specifically tailored for video data. The authors present a method that distinguishes itself from prior work, particularly SysBinder, emphasizing its unique applicability to video data and asserting its capabilities in capturing essential knowledge for future prediction in a disentangled manner. Inspired by Slot-Attention, the method utilizes slot-attention on object-centric representations to delineate attributes per the object-centric representation. The model undergoes extensive evaluation across two 2D datasets and a 3D dataset, demonstrating superior performance in pixel-level reconstruction and position estimation over existing methods. Additionally, the authors subject their model to generalization tests in unseen environments, where it continues to outshine competing approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The motivation behind the study is intriguing and thought-provoking.\n- The exploration of disentangled object-centric representation for video data is innovative, unveiling new insights, particularly regarding the representation of essential attributes for future prediction from video data.\n- The model is intuitively designed, effectively leveraging slot-attention on top of the object-centric representations to disentangle representations.\n- Comprehensive comparative analysis, including ablation studies, robustly demonstrates the advantages of disentangled representation.\n- The figure for their model architecture is well-drawn and easy to understand.\n- The abstract, introduction and the proposed model section are well written except the permutation module section."
            },
            "weaknesses": {
                "value": "- The manuscript\u2019s clarity and organization can be improved. Specific suggestions for improvement are provided in the questions section.\n- A comparative evaluation with another disentangled object-centric representation learning method, SysBinder, is lacking, despite its mention in the text.\n- The visualization of disentanglement in the manuscript (Section 4.4) could be enhanced for better clarity and comprehension. Additionally, as they started this paper with the question, \u201cwould it help to learn disentangled object representations, possibly separating the attributes which contribute to the motion dynamics vs which don\u2019t?\u201d, if they can show the disentangled representation for the attribute which to contribute to predict the dynamic, it should be much better.\n- The quantitative results presented could be bolstered with more illustrative examples, showcasing scenarios where the proposed model excels in comparison to its counterparts."
            },
            "questions": {
                "value": "### Clarity and Presentation\n\n- In the introduction, could you incorporate a high-level architectural diagram or illustration of your model? This addition would facilitate a clearer and more immediate understanding for readers.\n- You\u2019ve described your permutation module as novel. Can you elaborate on its novelty, especially in the context of other existing methods, such as the approach used in the OCVT paper?\n- The Mask R-CNN in your methodology is trained using a labeled dataset. This seems to introduce a discrepancy since the other models under comparison do not utilize labeled data. Could you perhaps validate your model's performance using Slot-Attention or another slot-based model as a substitute for Mask R-CNN?\n- In Figure 1, \u201cNote that each block has its own set of concept vectors; thus, C = || rj=1C j where r represents the number of blocks.\u201d. Why? Shouldn\u2019t the input $mathcal{C}$ be shared for every block?\n- In section 3.2, \u201cwe project each resultant block vector onto the learnable concept space and update its representation as a linear combination of the concepts via projection weights (lines 9 - 12 in Algorithm 1). This step results in discovering the disentangled representation central to our approach.\u201d. Could you provide a more in-depth explanation or empirical evidence to support this assertion?\n- In Algorithm 1, what is $k$? In figure 1, C is consisted of $r$ concept vectors.\n- In section 3.3, \u201cLet the transformer encoder output be \u03b4\u02c6 s ti,b. \u201c For clarity, could you specify that this is the output corresponding to $s_t^{i,b}$?\n- Section 3.4 appears to be complex. To ensure my understanding is correct: is the process essentially projecting the concatenated block vectors through matrix $U$, calculating the Cosine Similarity, and then aligning the most similar representations as the same object? Further clarification and polishing of this section would be beneficial.\n- How is the object position estimated within your model, as detailed in the experiment section? You've mentioned that position error is not reported for SlotFormer due to its lack of explicit object mask handling; does your model operate in a similar manner?\n- Could you provide a deeper analysis of DenseFormer, particularly in comparison to Slotformer? Are there specific scenarios where DenseFormer is more prone to failure, and could you share sample outputs from both DisFormer and DenseFormer to illustrate these points?\n- In Section 4.3, the term \u201ctransfer learning\u201d is used. Based on my understanding, the experiments seem to be more about evaluating generalization to unseen environments rather than transfer learning. Would renaming this as generalization and providing a more comprehensive analysis, especially in light of the varied performance across different datasets, be more accurate and informative? For example, when comparing the results in Table 1, Slotformer performance deterioration for 2D-BC is not huge while Slotformer is worse for 2D-BS. This results can suggest that for more complicated environment, the disentangled representation is more helpful for the generalization.\n- The examples in Section 4.4 intended to illustrate disentanglement seem to be lacking. Instead of swapping both color and shape attributes, could separate demonstrations of each be more effective in showcasing the model\u2019s capabilities?\n\n### Methodology\n- Regarding Section 3.4, why is the permutation module utilized only during training and discarded during testing? Could its application during testing, potentially for aligning the input order of the Transformer module, lead to enhanced performance?\n- For the Dynamic loss calculation in Section 3.6, why is the comparison made between the weights of the concept vectors rather than the block vectors? Additionally, could you provide a definition for $\\hat{w}_t^{i,b,j}$? \n\n### Experiment\n- In Section 3.6, there is a training phase where only the block extractor, permutation module, and decoder are trained, followed by a phase focusing solely on the dynamic model. Could you elaborate on the reasons and potential benefits of this training strategy? Did it result in improved model performance?\n- The model training in Section 3.6 incorporates multiple loss functions. Have ablation studies been conducted to understand the impact of each loss function on the overall performance?\n\n### Additional Comments\nThe study presents an interesting investigation with noteworthy contributions to the field. However, to ensure a stronger impact and facilitate better understanding, a revision focusing on improving presentation, clarity, the comparison with the relevant work, and depth of analysis are recommended."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734474448,
        "cdate": 1698734474448,
        "tmdate": 1699637186793,
        "mdate": 1699637186793,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1wptEDWn4N",
        "forum": "AOSsLRKQrX",
        "replyto": "AOSsLRKQrX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9419/Reviewer_gVGK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9419/Reviewer_gVGK"
        ],
        "content": {
            "summary": {
                "value": "In this work, a novel model is proposed for next frame prediction for videos of interacting objects. Building on previous architectures, the authors explore\nfurther structuring an object-centric representation into blocks that represent\nspecific object attributes such as shape or color. Object-centric representations are\nfirst obtained using a pretrained unsupervised segmentation model and a feature\nextractor. Slot Attention is used to decompose object representations into blocks. A\nTransformer than predicts the latent representation of the next frame which is converted into an image using an adapted Spatial Broadcast Decoder. The model consistently\noutperforms previous models on three synthetic datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Learning scene representations that are structured into objects and their attributes\n  is a very relevant topic. An unsupervised approach based on next frame prediction as\n  followed by the authors is applicable in a broad range of settings.\n- The proposed model consistently improves over previous methods."
            },
            "weaknesses": {
                "value": "The empirical evaluation has a strong focus on measuring quantiative performance\naveraged over entire datasets. Further insights into the inner workings of the model or\ncomponents necessary for outperforming previous approaches are hardly provided.\n- This paper proposes a range of novel model components, a detailed ablation analysis is\n  however missing. The only comparison is to a model that replaces the pretrained object extractor with an MLP. So it is not clear to which degree the different components\n  contribute to the improved performance of the model.\n- The paper does not discuss any particular success or failure cases of the proposed\n  model. Are there specific situations which are predicted better by the proposed model? How do these related to the model components introduced in the paper?\n- The model learns a constant concept space $C$. Do the concept vectors correspond to\n  interpretable attributes? Is there a separation of object attributes into those that\n  contribute to dynamics and others that do not, as asked in the abstract?\n- The disentanglement of object attributes is a core motivation behind this work. The\n  disentanglement is however not quantitatively evaluated."
            },
            "questions": {
                "value": "- In Phase I of the model, object masks are predicted and the masked objects passed\n  through a feature extractor. Why is this extra step necessary? Is it possible to use\n  the internal representation of the segmentation module directly?\n- In section 3.2: How exactly are block vectors projected onto the learnable concept\n  space? A mathematical description might be helpful here. Why does this project lead to\n  disentangled representations?\n- How were hyperparameters tuned for the DisFormer? How sensitive is the model with\n  regard to chosing hyperparameters?\n- Were the hyperparameters tuned for the baselines?\n- How are the predicted object positions obtained for all methods when evaluating the\n  position error?\n- I do not understand the transfer learning setup in section 4.3: Two variants of the\n  training set are created, are the models trained on both? How is it different from the\n  evaluation setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761394812,
        "cdate": 1698761394812,
        "tmdate": 1699637186682,
        "mdate": 1699637186682,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Md9p3Kh1UT",
        "forum": "AOSsLRKQrX",
        "replyto": "AOSsLRKQrX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9419/Reviewer_987R"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9419/Reviewer_987R"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new neural net architecture for decoupling objects from dynamics for the task of video prediction on simple scenes. Using several modifications to existing works, the authors encourage more decoupling of object features from other features like the position and dynamics of the inputs. The authors show improved accuracy on existing simple benchmark datasets over previous networks.\n\n\nMinor typos (did not influence review):\nPage 4: and is a hyperparameter of the model.\nPage 9: we swap the positions swapped the blocks corresponding to shape\n\n\nEDIT:\nI can't seem to comment, so I'd like to add my comments here to the author's responses\nThank you for your updated wordings. I think the paper is clearer now, though I still think some images of the learning problems earlier in the paper during the problem definition portion would make it even stronger.\n\n1. I see what you are saying, but I think of all the problems the only one that convinces me it could generalize to more complex scenes is CLEVR. The other experimental setups are too simple to be called \"visual.\" I'd like to see more evidence on CLEVR and even harder datasets (maybe something with a physics engine and more realistic objects like Ai2THOR).\n\n3. From the appendix: \"We found that even though the permutation\nmodule was trained on same time step object representations, it produces correct permutation matrix\neven for 10 time step apart objects.\" How can you say it's correct if you don't have ground truth permutations?"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The authors show an improvement on the existing state of the art on several benchmark datasets\n+ The architecture accounts for several difficulties in training models to be disentangled in new/interesting ways\n+ The paper is generally well-written"
            },
            "weaknesses": {
                "value": "Clarity:\n- I thought the explanation of the task and the actual problems came too late and were not depicted well enough for what the paper was trying to accomplish. 3 of the 4 datasets used I would call \"toy\" datasets of simple bouncing balls. The 3D dataset seems more visually driven, but even that uses CLEVR which is known to be visually simple. In the whole paper there is only a single picture depicting the actual task, and the tasks are only described in the experiment section. For a paper with \"Visual Dynamics\" in the title, I would have expected less toy problems, and more explanation to what the actual problems were. I think this could have been a stronger paper if it had foregone the visual component and worked directly with low level data.\n- I found several explanations in the paper to be confusing/lacking detail. One key concept in the paper was that of a \"Block\". Here is the explanation from the paper: \"Recently, (Locatello et al., 2020) in the paper on slot-attention proposed architecture for unsupervised discovery of objects via iterative refinement of what is referred to as a slot. Each slot binds to an object via attention over the entire image in their work. We extend their idea to the case of objects, where each slot now represents a block, which is iteratively refined by taking attention over a latent object representation\" - My rephrasing of this is \"rather than take attention over the entire image to get a representation, we first extract an object mask, and then take attention over that\". I'm not sure that is correct, and even if it is, I don't understand why they need attention if they already have a mask.\n- Another instance of this was the Permutation module. I did not understand the motivation behind it considering it is only used at training time. The authors say they learn a permutation to match up objects from one frame to the next, but that they supervise this permutation with ground truth knowledge. Then at test time, this component is removed. If you are already using ground truth information at train time, and at test time you don't use the module, why not just permute the features directly instead of learning a permutation matrix?\n- There were a few other small questions I had about some other phrases. \"All our object extractors are unsupervised and trained in a self- supervised manner. In our experiments, for 2D environments, we train an expert model similar to (Sharma et al., 2023b) to generate supervised data for Mask R-CNN.\" This seems to indicate the model is both unsupervised and self supervised, but then also trained with supervised data. That doesn't make any sense to me.\n\nExperiments\n- I thought the experiments in this paper were lacking in showing what the authors claimed. Predicting simple rigid body circle motion and even rigid body 3D synthetic CLEVR motion is not really convincing since it is such a problem removed from the complexities of the real world.\n- To convince me that there is a decoupling happening, it is crucial to have an experiment that directly probes this decoupling. The ablation study in 4.4 seems to do that in some way, but I don't understand the experimental setup from the explanations, and again, it is only on a toy setup so I can't say whether it would generalize to more complex scenes. It sounds like somehow the authors took the embeddings from one color setup and swapped them with another color setup to look at the output. I guess it seems trivially obvious that the output should change color, but does that prove that the dynamics module only encodes dynamics or just that masking a part of the image results in color features for that part of the image.\n- The DisFormer seems to be another reasonable ablation on paper but not carried out as well as I'd liked. Details are sparse, but it sounds like the entirety of MaskRCNN was replaced with an MLP, which doesn't seem like a reasonable substitution \"by replacing the object extractor (refer Section 3) by an MLP to create dense object representations\"."
            },
            "questions": {
                "value": "I would like the authors to explain in more detail why the permutation model was necessary if it was only used during training. I would also like the authors to explain the DisFormer and experiment 4.4 better."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9419/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9419/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9419/Reviewer_987R"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799824024,
        "cdate": 1698799824024,
        "tmdate": 1700855022318,
        "mdate": 1700855022318,
        "license": "CC BY 4.0",
        "version": 2
    }
]