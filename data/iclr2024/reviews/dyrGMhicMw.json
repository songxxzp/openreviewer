[
    {
        "id": "3juOyKJ3qW",
        "forum": "dyrGMhicMw",
        "replyto": "dyrGMhicMw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission762/Reviewer_FWr5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission762/Reviewer_FWr5"
        ],
        "content": {
            "summary": {
                "value": "The paper suggests that initializing the weight parameters of a smaller neural network from a pre-trained, larger model can significantly improve the smaller model's performance."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper is easy to follow;\n- The proposed method can improve the performance."
            },
            "weaknesses": {
                "value": "- The proposed method of selecting layers or weights for initialization is somewhat trivial. A more technical approach is encouraged to fully leverage the capabilities of larger models.\n\n- The study is limited in its scope by only employing ViT-T and ConvNeXt-F architectures. Inclusion of additional architectures such as Swin, PVT, and EfficientViT would enrich the evaluation.\n\n- There is an inconsistency in the reported accuracies for the same ImageNet-1k on the same model across different tables. For instance, ConvNeXt-F yields an accuracy of 76.1% in Table 1 but achieves around 82.0% in Table 2. Clarification is needed.\n\n- A comparative analysis involving ConvNeXt is crucial in the \"COMPATIBILITY WITH KNOWLEDGE DISTILLATION\" section, given its architectural differences with ViT.\n\n- The paper lacks sufficient motivation and theoretical grounding for the proposed selection strategy. More insights are needed to justify why, for example, the first N layers are selected, and how this methodology generalizes to other architectures.\n\n- For better comprehension, it would be beneficial to present the customized structures, such as ConvNeXt-F, in tabulated form.\n\n- The potential for leveraging the weights of larger models (which are more powerful) is high. However, Table 7 indicates that the proposed method struggles to effectively learn from these larger models.\n\n- Some grammar errors: \n\t- \"Glorot & Bengio (2010a) maintains \" -> \"Glorot & Bengio (2010a) maintain\";\n\t- \"Lin et al. (2020) transforms\" -> \"Lin et al. (2020) transform\";\n\t- etc."
            },
            "questions": {
                "value": "Please see Section \"Weaknesses\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission762/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698653224907,
        "cdate": 1698653224907,
        "tmdate": 1699636003884,
        "mdate": 1699636003884,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pW7FResSyM",
        "forum": "dyrGMhicMw",
        "replyto": "dyrGMhicMw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission762/Reviewer_HkYt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission762/Reviewer_HkYt"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces 'weight selection', a simple but effective method that uses the weight of pre-trained large model to initialize the small model. The authors have conducted extensive experiments, showcasing the method's superiority compared to initializing models from scratch."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The experiments are comprehensive, covering both vision transformers and CNNs. Additionally, they have delved into various aspects such as knowledge distillation, comparison with pruning methods, and linear probing, which offer valuable insights\n2. The paper is easy to read and follow."
            },
            "weaknesses": {
                "value": "See questions."
            },
            "questions": {
                "value": "1. Could the authors give further explanation on the difference between 'consecutive selection' and 'uniform selection' in section 3.2?\n2. For comparison with pruning, could the authors elaborate on how L1 pruning was implemented? I am asking the question since the embedding dimension is changed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission762/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission762/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission762/Reviewer_HkYt"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission762/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698686664628,
        "cdate": 1698686664628,
        "tmdate": 1699636003813,
        "mdate": 1699636003813,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Fm1r3HIyon",
        "forum": "dyrGMhicMw",
        "replyto": "dyrGMhicMw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission762/Reviewer_rXy8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission762/Reviewer_rXy8"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a method aimed at accelerating the convergence of training by transferring pre-trained knowledge from a different network. The approach involves extracting well-trained weights, which exhibit characteristics similar to high-pass filtering (HPF) or low-pass filtering (LPF), from a pretrained network and transplanting these weights onto a novel network, as opposed to starting with random initialization. This technique facilitates quicker training of the new architecture. The authors conduct comparisons between this method and random initialization, demonstrating its superior performance.\n\n\nHowever, I have some concerns as follows:\n\n1)\tFirstly, it's important to note that the comparisons made in this work may not be entirely adequate. From my understanding, this approach falls somewhere between traditional initialization and transfer learning. Given that the method involves adopting a set of pre-trained filters or weights, it can be argued that this work is closer in nature to transfer learning rather than a simple initialization process. Therefore, it might be more appropriate to compare it with transfer learning methods rather than just random initialization. If the proposed method can approach or match the convergence levels of ImageNet pre-training, it would further support its practicality and utility in various applications, reinforcing its value in the field of deep learning.\n\n2)\tComparing the proposed method directly to random initialization may not provide a fair assessment, given the distinct differences in the initialization processes. Unlike the proposed method, a network initialized randomly must construct its structural filters (HPF or LPF) from scratch, which typically requires some epochs and computational resources. In contrast, the proposed method can bypass this initial construction step and is expected to be faster.\n\n3)\tI strongly recommend reconsidering the categorization of this work from merely weight initialization to a more encompassing concept of knowledge transfer to a novel architecture. This work goes beyond traditional weight initialization methods by borrowing well-structured filters from a pre-trained network with a different architecture. This transfer of knowledge to a distinctly different architecture represents a significant departure from conventional weight initialization and indeed underscores the novelty of the approach. Categorizing it as a knowledge transfer method would better capture its unique contribution in the field.\n\n4)\tBuilding upon the suggested change in categorization to knowledge transfer, it raises an interesting question about the potential applicability of the proposed method to highly dissimilar architectures. For instance, exploring the feasibility of transferring knowledge from a pre-trained ResNet to a Vision Transformer (ViT) is a compelling idea like Model Evolution. Training ViT models often necessitates access to large-scale datasets, but they can offer superior performance in various tasks. If the proposed method could indeed facilitate the transfer of knowledge from a ResNet to a ViT, it would hold significant promise. This potential application could lead to advancements in leveraging the strengths of different architectures and addressing challenges related to dataset size, thus expanding the scope and impact of the proposed method.\n\nI appreciate your insights, and it's clear that you find the work promising. However, I think that the manuscript doesn\u2019t well categorize the proposed method and the experiments are not adequately conducted. I believe this work can be much improved."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "See above"
            },
            "weaknesses": {
                "value": "See above"
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission762/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission762/Reviewer_rXy8",
                    "ICLR.cc/2024/Conference/Submission762/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission762/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698740913427,
        "cdate": 1698740913427,
        "tmdate": 1700727164421,
        "mdate": 1700727164421,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RHAfQiwycU",
        "forum": "dyrGMhicMw",
        "replyto": "dyrGMhicMw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission762/Reviewer_A6vA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission762/Reviewer_A6vA"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel approach to initialize smaller models given large pretrained models. The approach is simple, efficient and shows good results compared to random init and other numerous baselines and many image classification tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The problem of obtaining strong weights for small models given large pretrained models is important and solving it efficiently is challenging.\n\n2. The paper explores a very interesting idea of simple weight copying from the source to target network as an efficient and training-free way to initialize smaller model. \n\n3. The uniform selection approach is reasonable and takes into account multi-head structure and group convolution of source networks.\n\n4. The paper has many nicely outlined and run experiments that overall look interesting and convincing enough.\n\n5. The approach outperforms random init and works well on different vision tasks. \n\n6. The paper is well presented and enjoyable to read."
            },
            "weaknesses": {
                "value": "1. The paper misses some important related works:\n- [Czyzewski2022] Breaking the Architecture Barrier: A Method for Efficient Knowledge Transfer Across Networks\n- [Chen2021] bert2BERT: Towards Reusable Pretrained Language Models\n- [Chen2015] Net2net: Accelerating learning via knowledge transfer\n\nSpecifically, [Czyzewski2022] introduced a very similar weight transfer technique that seems more general than in the submitted paper since it allows for the source and target architectures to be different. They copied the center slices from source weights, which could be an interesting ablation in this submission. [Chen2015] proposed a simple approach to replicate weights to initialize models. While [Chen2015,Chen2021] focus on initializing a larger model, the idea of copying the weights from the source network to a target one is very similar and should be discussed in detail. This makes the novelty of the paper limited.\n\n2. The paper also should report results of student models pretrained and transferred to smaller datasets. While this would not be a fair baseline to the proposed method since it requires expensive pretraining, it could give important clues about what is the best performance of the model on this task. In addition, some papers (e.g. [Knyazev2021]) reported that very little pretraining is required to achieve competitive transfer learning performance. Therefore, it's not clear if the proposed weight selection approach would be competitive if someone has resources to pretrain a small target net on some large data first (at least for a little bit).\n\n- [Knyazev2021] Parameter Prediction for Unseen Deep Architectures\n\n3. For structured pruning, there are many stronger methods than L1 pruning, e.g. [Kwon2022, Halabi2022] and the baselines therein. The results in Table 6 are quite close and L1 pruning is also very simple, therefore stronger pruning methods could potentially match the performance.\n\n- [Kwon2022] A Fast Post-Training Pruning Framework for Transformers\n- [Halabi2022] Data-Efficient Structured Pruning via Submodular Optimization\n\n4. Most of the experiments are performed on transferring weights from ViT-S to ViT-T which have the same number of layers (12). It would be more interesting to see results when the source model is also deeper (which is usually the case in practice). Related to that, the teacher size results show that ViT-S is the best teacher and a potential reason for that could be that the layer selection approach (first-N) is not the best (perhaps a uniform selection of layers similarly how it is done for weights could be better than first N?). In Table 7, it would be very interesting to see the respective results for distillation which usually works better if the source model is larger/stronger (see the DeiT paper).\n\nDespite all the weaknesses, I'm looking forward to the the authors' response and will be willing to update the rating."
            },
            "questions": {
                "value": "1. In Net2net [Chen2015], for initialization the copied weights are rescaled to take into account the width difference (i.e. for wider/thinner networks one needs smaller/larger weight values according to standard weight initialization such as K.He or X.Glorot). Have the authors tried to rescale copied weight values?\n\n2. For \"distill + weight selection\" is the target model first initialized with weight selection and then the distillation is run or it's different?\n\n3. It seems that the results of baselines (random init and mimetic init) are different from previous papers like the mimetic init paper even though the setup looks the same. For example, in the mimetic init paper for CIFAR-10 ViT-T the accuracy is 87.39 for rand init and 91.38 for mimetic init, while in this submission it is 92.4 and 93.3 respectively. Can the authors elaborate on the reasons for that?\n\n4. In L1 pruning experiments, is the final number of selected weights the same as in the proposed weight selection? It's a little bit surprising to see that L1 underperforms uniform weight selection implying that the latter is actually a stronger pruning method. Can the authors elaborate on that?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission762/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission762/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission762/Reviewer_A6vA"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission762/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698957895096,
        "cdate": 1698957895096,
        "tmdate": 1700771318689,
        "mdate": 1700771318689,
        "license": "CC BY 4.0",
        "version": 2
    }
]