[
    {
        "id": "bK2CNuXN4y",
        "forum": "VRJzlm2ecv",
        "replyto": "VRJzlm2ecv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9174/Reviewer_hNAs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9174/Reviewer_hNAs"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the interpretability issue of large language models.    \nDifferent from some existing studies that use attention-based methods, the authors introduce knowledge graphs to generate explanations.\nSpecifically, the key decision signals (elements in the constructed knowledge graph) are extracted and used as instructions for large language models to generate explanations.     \nThe experimental results reflect the explanations could help model accuracy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The interpretability of LLMs deserves to be investigated and the proposed approach is model-agnostic. \n- Knowledge graphs are leveraged to extract key elements to support the LLMs to generate explanations.\n- The experimental results show both good model accuracy and interpretability."
            },
            "weaknesses": {
                "value": "- Although the proposed approach is model-agnostic, it is only tested on RoBERTa-large and its variants. Larger language models are suggested to be tested. This is because LLMs have strong capacity and knowledge graphs might not help them. \n- The approach is not clearly explained. How to optimize f_{enc} and MLP is not stated. Moreover, how the explanations affect the model accuracy is not clear. As shown in Figure 1, the generated explanations do not affect answer selection. Besides, how to optimize the whole method?\n- The baselines seem to be not very new. More recent and strong baselines are preferred.\n- From my personal perspective, the novelty of the techniques are not well validated.   \n     \nTypos:\n(1) \"their decision process lack transparency\" -> lacks"
            },
            "questions": {
                "value": "Please see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9174/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698242137695,
        "cdate": 1698242137695,
        "tmdate": 1699637154721,
        "mdate": 1699637154721,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "29m34Zqfcf",
        "forum": "VRJzlm2ecv",
        "replyto": "VRJzlm2ecv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9174/Reviewer_d4rH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9174/Reviewer_d4rH"
        ],
        "content": {
            "summary": {
                "value": "Language models like GPT-4 are powerful for natural language processing tasks but lack transparency due to their complex structures, which can hinder user trust. To address this issue, LMExplainer is proposed, which is a knowledge-enhanced explainer that provides human-understandable explanations by locating relevant knowledge in a large-scale knowledge graph using graph attention neural networks. Experiments show that LMExplainer outperforms existing methods on CommonsenseQA and OpenBookQA datasets, generating more comprehensive and clearer explanations compared to other algorithms and human-annotated explanations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed model is clear and easy to follow.\n2. The studied topic of explaining LLM with knowledge graph is a reasonable direction. The idea of improve accuracy and explanation simutaneously is interesting.\n3. The experimental results are sound."
            },
            "weaknesses": {
                "value": "1. The code is not available to the public.\n2. The modular design of proposed model made it hard to judge where the benefits of the model is from."
            },
            "questions": {
                "value": "1. It seems the proposed model is not learned in an end-to-end way. How was the parameters of MLP learned in line 5 of Algorithm 1? and how does the generated graph influence the final results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9174/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698591236139,
        "cdate": 1698591236139,
        "tmdate": 1699637154609,
        "mdate": 1699637154609,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Jjzaclfimk",
        "forum": "VRJzlm2ecv",
        "replyto": "VRJzlm2ecv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9174/Reviewer_gb89"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9174/Reviewer_gb89"
        ],
        "content": {
            "summary": {
                "value": "Language models like GPT-4 are powerful but opaque. To make their decisions understandable, a tool called LMExplainer has been developed. It harnesses Knowledge Graphs and the Graph Attention Neural Network (GAT) to pinpoint and explain the model's reasoning. In tests on two QA datasets, LMExplainer outperformed benchmarks and adeptly translated the model's logic into plain language."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors adeptly use natural language to elucidate the inference results of LMs.\n- LMExplainer was tested on QA datasets and benchmarked against leading models. The results indicated a superior performance of LMExplainer in most scenarios."
            },
            "weaknesses": {
                "value": "- The notation could be improved for clarity, especially inconsistencies like h_e^k and h_{v_e}^k in Equation (5) and Equation (6).\n- Equations in section 3 lack immediate clarity. Incorporating variable and function dimensions would enhance readability.\n- The study exclusively utilizes RoBERTa for LMExplainer. It's uncertain if similar results would be achieved with different LMs.\n- The structure of the experiment section needs refinement. In tables like Table 1 and Table 2, clearer categorization of the baseline models, such as which belong to fine-tuned LM RoBERTa-large or KG-augmented RoBERTa-large, would be beneficial."
            },
            "questions": {
                "value": "- In Equation 5, the authors use f to represent MLP as m_{es}=f_n(,,). What does the \u201ccomma\u201d signify? Is it indicating concat, inner product, element-wise addition, or another operation?\n- What does \\hat{n} in Equation 5 represent? The authors haven't previously defined it.\n- In Algorithm 1, line 4, the authors apply f_{enc} to encode node v and use the encoded embeddings for KG extraction. Is f_{enc} equivalent to the LM - Encoder in Figure 1? If yes, it would be clearer if there were an arrow connecting LM - Encoder and KG Retrieval in the figure.\n- The term u^e is derived from a linear transformation of one-hot node-type vectors. What are these node-type vectors? An example would help in understanding."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9174/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638921801,
        "cdate": 1698638921801,
        "tmdate": 1699637154493,
        "mdate": 1699637154493,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FRx0sNqThz",
        "forum": "VRJzlm2ecv",
        "replyto": "VRJzlm2ecv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9174/Reviewer_gatd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9174/Reviewer_gatd"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a knowledge-enhanced explainer, LMExplainer. First, LMExplainer constructs a subgraph by integrating external knowledge into the tokens of the question. Subsequently, it utilizes graph attention neural networks to represent the subgraph and identify the most critical reasoning components within it. Finally, LMExplainer leverages a predefined structure containing the extracted explanation components. These components are used to prompt GPT-3.5 to generate explanations regarding why LMs selected a specific answer and why they rejected others. Experimental results from CommonsenseQA and OpenBookQA datasets demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of utilizing a knowledge graph as a more transparent surrogate to generate comprehensible explanations seems reasonable. This can, to some extent, reveal the reasoning process of language models. \n2. The proposed approach that extracts reason-elements as one part of the key components which are essential to the LM decision-making process is somewhat novel.\n3. The experiments and subsequent analyses demonstrate that LMExplainer not only enhances the performance of language models in open domain question answering tasks, but also exhibits the capacity to generate comprehensible explanations for these models."
            },
            "weaknesses": {
                "value": "1. In Section 3.2, the paper employs all the tokens in the question sentence as queries to retrieve relevant knowledge. However, if there are non-entity tokens such as \"what,\" \"is,\" and so on, they can introduce noise into the process of constructing subgraphs. This could potentially increase the time and space complexity of the graph construction algorithm. As far as I am concerned, it is imperative to filter out non-entity words from the question prior to constructing subgraphs.\n2. In constructing subgraphs, the authors extract L-hop neighbors of z, indicating that 'L' is a crucial factor that influences the introduction of an appropriate number of knowledge. However, the authors did not specify how 'L' was set and did not conduct ablation experiments to investigate how 'L' affects model performance. \n3. In the ablation studies, the experimental results reveal a significant decrease in accuracy when the interpreting component in LMExplainer is removed. However, in Section 3.4, the authors note that the explanations are generated using the predicted answer. Consequently, it appears that the accuracy of model predictions is not influenced by whether or not explanations are generated. The authors should therefore clarify the relationship between generating explanations and answer predictions. \n4. There are no human evaluations of the accuracy of the explanations generated by GPT-3.5, which weakens the case presented in Table 3. This could potentially be a rare occurrence where the generated explanations happen to be correct. Further studies should be conducted to ascertain the proportion of accurately generated explanations."
            },
            "questions": {
                "value": "1.  In Section 3.2, the authors explain that the score of each node, which represents the correlation between node v and input content, is derived by passing token embeddings through an MLP. Could you please provide additional information on how the MLP is trained? \n2. There are numerous traditional methods that also integrate knowledge graphs into language models. Undoubtedly, incorporating an external knowledge module can greatly enhance the performance of the model. Could you please elaborate further on the advantages of your knowledge integration method? \n3. This paper employs predefined structures to prompt GPT-3.5 to produce explanations. However, given the propensity of large models to suffer from hallucination, it is highly likely that they will generate explanations that are unrelated to the question, despite the presence of relevant words in the prompt. This appears to contradict the statement made in the Conclusion section, which asserts that the model can generate trustworthy explanations. How can you ensure that the generated explanations are indeed trustworthy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9174/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698693063770,
        "cdate": 1698693063770,
        "tmdate": 1699637154354,
        "mdate": 1699637154354,
        "license": "CC BY 4.0",
        "version": 2
    }
]