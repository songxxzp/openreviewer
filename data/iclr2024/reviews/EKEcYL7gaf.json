[
    {
        "id": "7z735VPCi0",
        "forum": "EKEcYL7gaf",
        "replyto": "EKEcYL7gaf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3255/Reviewer_YsNP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3255/Reviewer_YsNP"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed Predicated Diffusion, a comprehensive framework designed to articulate users' intentions effectively. This approach leverages predicate logic and utilizes pixels within attention maps as fuzzy predicates, with propositions serving as the textual representation. By employing this methodology, it transforms these intentions into a differentiable loss function. The experimental findings demonstrate a heightened faithfulness to the provided prompts. Furthermore, the paper introduces the concept of 'possession failure,' which expands the scope of inquiry to encompass the existence or non-existence of objects and attributes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.By using the Predicate Logic method, this paper easily converts the intentions of prompts into a differentiable loss function, which is a simple, intuitive, and effective method.\n\n2.This paper proposes the term \u201cpossession failure\u201d to describe the situation of missing attributes of prompts in T2I models. It provides a detailed direction for modeling the missing attributes question."
            },
            "weaknesses": {
                "value": "1. This paper represents an increment in the field, integrating additional predicate logics into the Attend-and-Excite framework. However, it's worth noting that the quality of the generated images appears to be subpar. For instance, Figure 1 illustrates issues such as a blurred bird in the first column, a cat with distorted textures in the second column, and a figure with missing eyes in the last column. These results suggest that the introduction of additional loss functions may have had a detrimental effect on the original model. Thus, a crucial question arises: How can we retain the benefits of the original model while addressing these shortcomings?\n\n2. Despite the proposed method, the problem of \"attribute leakage\" persists. For example, in Figure 3, the model still generates two apples in response to the prompt \"an apple and a lion,\" and this issue remains evident in Figure 4 with the prompt \"a green balloon and a purple clock.\"\n\n3. While this method effectively addresses the \"possession failure\" issue, it primarily focuses on Stable-Diffusion v1.4, rather than the latest SDXL model or the DALLE3 model. As a result, there may be limited instances of \"possession failure,\" prompting a need to evaluate the overall contribution of this paper.\n\n4. The paper conducts four distinct experiments to showcase the effects of integrating predicate logics. However, the question remains: If all predicate logics were learned simultaneously, what impact would this have on the original model's performance? Could it further deteriorate the model's results?\n\n5. The proposed method lacks a discussion of its limitations. It is imperative to address and acknowledge the limitations of this approach in order to provide a comprehensive evaluation.\n\n6. Several typographical errors require attention, such as the statement, \"the existing models rarely fail to generate an object with a specified color.\" In subsection \"One-to-One Correspondence\" of the \"METHOD\" section, it appears that the intended message may be the opposite. Please clarify this statement for greater clarity."
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3255/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698739186315,
        "cdate": 1698739186315,
        "tmdate": 1699636273902,
        "mdate": 1699636273902,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hcoFFc6HHj",
        "forum": "EKEcYL7gaf",
        "replyto": "EKEcYL7gaf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3255/Reviewer_5oLz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3255/Reviewer_5oLz"
        ],
        "content": {
            "summary": {
                "value": "The paper introduced a novel approach to guide text-to-image diffusion models in order to improve relation consistency within a generative image given a prompt. To this end, the introduced guidance links predicate logic and attention maps of diffusion models. The authors motivated the utilization of logic based on four issues, namely missing objects, unintended mixture of objects, attribute leakage between objects, and possession failure. After introducing the methodology, these issues are used to assess the performance of the proposed guidance and other baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper addresses a currently unresolved issue of text-to-image diffusion models.\n- The issues addressed are well explained and motivated.\n- The implementation of propositions via attention maps is well introduced and well supported with examples, which makes it easy to understand.\n- While the evaluation only considers Stable Diffusion, the approach can be transferred to other diffusion models without the need to adapt parameters and any additional training."
            },
            "weaknesses": {
                "value": "- While the methodology is well introduced, the experiments lack clarity. \n\t- It is, for example, unclear how the prompts were selected. Are they extracted from existing datasets? \n\t- In the case of experiment 3, why is the similarity metric missing? \n\t- In Tables 3 and 4, is the fidelity corresponding to the rating from the user study? Are the reported values normalized? The authors describe that fidelity is assessed by human evaluators and two automated similarity approaches. However, it is not clear to which column these different metrics correspond. Further, it is unclear how the human ratings were aggregated; how many images were assessed by each human evaluator? What is reported, e.g., majority decision?\n\n- The limitations are not well discussed. E.g., the compute overhead of the additional predicate logic-based guidance is unclear. Compared to, e.g., autoregressive image generative models, diffusion models\u2019 inference time is rather slow. While approaches exist tackling these issues, I assume that the additional guidance introduced increases computation.\n\n- Missing related work:\n\t- Universal Guidance for Diffusion Models. Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, Tom Goldstein. CVPR Workshops 2023.\n\t- SEGA: Instructing Diffusion using Semantic Dimensions. Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, Kristian Kersting. In Proceedings of NeurIPS 2023\n\n\nMinor comment:\n\nTypo: Section 3 second paragraph \"Predicate Logic in Attention Map and Resulting Gauidance\" -> Resulting Guidance"
            },
            "questions": {
                "value": "Next to the questions raised above:\n\t\n- Can you provide the computation costs you observed in your experiments, especially the additional overhead of using the introduced guidance?\n\t\n- Which Stable Diffusion version is used in the experiments?\n \t\n- You mentioned that the text encoder causes the addressed issues. Did you evaluate your method on diffusion models not relying on the CLIP text encoder and instead using, e.g., a more complex LM such as T5? For example, IF or Stable Diffusion XL? And could the introduced guidance be utilized during training or fine-tuning the text encoder?\n\t\n- Why is the fidelity increasing when using Predicate Diffusion? Is this because of resolving the issues of object mixtures?\n- How were the human ratings aggregated? \n- Can you provide more details on the conducted user study? How many images were assessed by each human evaluator? What is reported, e.g., majority decision? Why are eight raters an appropriate and sufficient number of participants?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3255/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698756984929,
        "cdate": 1698756984929,
        "tmdate": 1699636273822,
        "mdate": 1699636273822,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "A5xcgKtLA9",
        "forum": "EKEcYL7gaf",
        "replyto": "EKEcYL7gaf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3255/Reviewer_2NoA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3255/Reviewer_2NoA"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the misalignment between image and text in text-to-image generation. The paper proposes a framework that represents the input text prompt using predicate logic. The attention weight of each pixel is then considered as a continuous value that indicates the level of fulfillment of a pixel for a specific proposition. The intermediate image at each denoising step is then updated in order to maximize the level of fulfillment of the input prompt. Experiments show that the proposed method outperforms several baselines on generating more complete objects and objects with correct colors."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper proposes a novel framework for generating images that are faithful to the input text prompt. The framework is generic in that it covers various issues that have been studied in previous works, such as missing objects and mistakenly bonded colors.\n2. The experiments show that the proposed method outperforms existing baselines on four evaluated settings."
            },
            "weaknesses": {
                "value": "1. Some of the assumptions that are used for representing text prompt as predicate logic do not make sense. For example, the prompt \"There is a black dog\" is interpreted as \"There is a dog\" AND \"All dogs are black,\" which won't work for prompts such as \"A black dog and a white dog.\" Similarly, prompts that have possession relationships such as \"a man holding a bag\" is interpreted as \"all pixels of the bag is also part of the man,\" which is not necessarily correct.\n2. The proposed optimization method will not guarantee that all predicates are satisfied. When multiple predicates exist in the text prompt, their conjunction is used as the objective function. However, since this is a multi-objective optimization problem, the optimization used in the paper is not guaranteed to find optimal solution for all predicates.\n3. The visualized images in the paper seem to not have as good quality as the baselines. No metrics (either automatic ones such as FID or subjective evaluations) are reported in the paper."
            },
            "questions": {
                "value": "1. Should $P(x) \\rightarrow Q(x)$ be $1-A_P[i] \\times (1-A_P[i] \\times A_Q[i])$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3255/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3255/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3255/Reviewer_2NoA"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3255/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698794655495,
        "cdate": 1698794655495,
        "tmdate": 1699636273753,
        "mdate": 1699636273753,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WmiqVJgT0D",
        "forum": "EKEcYL7gaf",
        "replyto": "EKEcYL7gaf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3255/Reviewer_h1q7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3255/Reviewer_h1q7"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Predicated Diffusion which combines predicate logic with the intuition of cross-attention layers in diffusion-based text-to-image. The paper draws connections between several propositions and attention map operations. Language prompts can be seen as a combination of these propositions and have corresponding loss functions that can be optimized in the diffusion process.  Experimental results show that the method outperforms several baselines including a recent SOTA method in this direction."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed method is novel. It is very interesting to see how first-order logic can be connected to compositionality in text-to-image generation, specifically the attention maps. Some of the propositions and losses are reasonable and interpretable. \n- The proposed method tackles a wide range of problems, including well-studied ones and also an underaddressed problem, i.e. possession failure. \n- Experimental results show that the method outperforms previous methods in many aspects."
            },
            "weaknesses": {
                "value": "- Some of the losses are not intuitive or cannot be easily verified. I am not sure if this is due to the presentation of the method section. For example, how does eq (2) prevent the two objects from highlighting the same pixels or regions? It would be better to give straightforward intuition behind the equations in terms of the behavior of attention maps. For example, if I understand correctly, eq 6 encourages the attention maps of \"bag\" to be partially overlapped with the attention maps of \"man\" yet does not force all pixels of \"bag\" to be part of the \"man\". \n- Predicated Diffusion requires manual or pre-defined use of different propositions for different prompts. As stated in Sec. 4, the authors applied different losses for different types of prompts. However, this is not practical for applications where prompts can be arbitrary. The authors manually extracted propositions for each prompt in Experiment (iv) which, I think, really downgrades the overall value of the work. Is there an automatic way to extract propositions for each prompt?\n- Writing could be improved. I think Sec. 3 could be improved in structure and contents. Some paragraphs have too many logical equations that make them a bit hard to follow. Perhaps the authors could find a more organized way to explain every proposition (e.g. start with a simple derivation of logic equations, then provide the attention equation, and finally give some intuition in words. ). The authors could attempt to group contents into subsections to illustrate propositions from easy ones to hard ones and distinguish the novel propositions over A&E or SynGen. There are other trivial flaws like using \"Experiment (x)\" in tables/captions without specifying the experiment domain, making it hard to follow. \n\nWhile I really like the novelty and perspectives presented by the work, there are major weaknesses. I will adjust my rating accordingly depending on how well these concerns are resolved."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3255/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3255/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3255/Reviewer_h1q7"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3255/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698865764491,
        "cdate": 1698865764491,
        "tmdate": 1699636273689,
        "mdate": 1699636273689,
        "license": "CC BY 4.0",
        "version": 2
    }
]