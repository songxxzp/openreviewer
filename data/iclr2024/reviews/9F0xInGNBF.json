[
    {
        "id": "EYpZBP60Ni",
        "forum": "9F0xInGNBF",
        "replyto": "9F0xInGNBF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5632/Reviewer_FnrC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5632/Reviewer_FnrC"
        ],
        "content": {
            "summary": {
                "value": "The manuscript proposes an ensemble framework for video understanding based on pre-trained visual-language models, which involves utilizing LLM to enhance the descriptiveness of text labels and leveraging a video-to-text model to enhance video representations. The authors conducted comprehensive experiments on action recognition, video-text retrieval, and time-sensitive video tasks, demonstrating the effectiveness of the approach in zero-shot scenarios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The experiments and ablation studies are conducted comprehensively, validated on different pre-existing architectures, and taken into account various types of video data.\n\nEmploying the video-to-text model (not limited to VGPT and caption models) is novel and worthy to explore in the video field. Fusing the text and video representations is depicted to be beneficial in bridging the gap between video and textual labels in the embedding space.\n\nThe manuscript provides a detailed explanation and examples of prompting the GPT to refine the simple textual label, which in turn enhances reproducibility."
            },
            "weaknesses": {
                "value": "In the 'video-to-text guided visual feature enhancement' (section 2.2), the adopted VGPT relies on CLIP-ViT-L and vicuna, where the computational cost of performing multiple inferences (including text embedding and filtering) far exceeds that of the basic video understanding model. This limits the practical value of the proposed approach. \n\nExcept for CLIP, an image-language pre-trained model targeted specifically for the image field, the proposed approach shows relatively limited performance gain in other video-based models (ViFi-CLIP, AIM, ActionCLIP), considering the additional computational requirements.\n\nThe configurations of adopted pre-trained models (AIM, ActionCLIP, \u2026) remain unclear, which datasets are these models pre-trained on (e.g. K400, K700, \u2026)? For AIM, do the authors directly remove the classification layers?"
            },
            "questions": {
                "value": "Are the high-level action contexts mentioned in the manuscript manually designed?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5632/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698315403732,
        "cdate": 1698315403732,
        "tmdate": 1699636584524,
        "mdate": 1699636584524,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7IhGy5eD3U",
        "forum": "9F0xInGNBF",
        "replyto": "9F0xInGNBF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5632/Reviewer_ypLm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5632/Reviewer_ypLm"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel framework for zero-shot video understanding. The proposed framework, named VideoPromoter, is built by enhancing the visual features as well as the class representations. Experimental results indicate that the proposed method could improve the zero-shot performance of various VLMs across multiple tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThis paper studies an important problem of adapting pre-trained vision-language models to downstream tasks in zero-shot settings.\n2.\tThe introduced method is lucid and holds promise for extension across a wide range of VLMs.\n3.\tThe experimental results look good. VideoPrompter is able to increase the zero-shot performance of VLMs across multiple tasks.\n4.\tThe paper is well-presented."
            },
            "weaknesses": {
                "value": "1.\tThe efficiency of VideoPrompter hasn't been thoroughly examined. Given that VideoPrompter appears to require generating 10 times the number of samples and the use of an additional text-to-video model, it could substantially raise the inference costs, both for evaluating existing VLMs and in practical applications.\n2.\tThe selection of Video-ChatGPT as the video-to-text model seems arbitrary. Alternative models, such as Video-LLaMA [A], should be considered and discussed.\n3.\tAn ablation study on the video-specific language descriptors is missing.\n\n[A] Zhang, H., Li, X., & Bing, L. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5632/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5632/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5632/Reviewer_ypLm"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5632/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760528434,
        "cdate": 1698760528434,
        "tmdate": 1699636584426,
        "mdate": 1699636584426,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Gt5qdFFKFi",
        "forum": "9F0xInGNBF",
        "replyto": "9F0xInGNBF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5632/Reviewer_1vg9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5632/Reviewer_1vg9"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to ensemble multiple large foundation models to enhance the zero-shot inference performance on video understanding tasks (namely VideoPrompter), including video action recognition, video-to-text and text-to-video retrieval, and time-sensitive (before/after) video tasks. The main architecture is based on CLIP, where classification can be performed by ranking the cosine similarity between visual and text representations, and the main idea is to enrich both the video and text embeddings. For the video part, the authors employ Video-Chat GPT (VGPT) (Maaz et al., 2023) to extract the text description of the query video and convert it into a video-to-text embedding with the text encoder in CLIP. The video-to-text embedding is then ensembled with the visual embedding encoded by the original CLIP visual encoder as the final visual embedding. For the text part, they prompt GPT-3.5 to rephrase the class names with parent context, language attributes, and language descriptions. All the descriptions are ensembled to generate the final text embedding. Experiments show that VideoPrompter can improve over plain zero-shot inference performance with CLIP and its variants."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The studied problem is interesting. Video understanding with large foundation models is of wide interest in the community.\n2. The authors put together state-of-the-art large foundation models and improve the zero-shot inference performance on video understanding tasks."
            },
            "weaknesses": {
                "value": "1. The idea of generating more descriptions for class names and using high-level context is not new in prompting large foundation models (e.g. the prior works cited in this paper). This is model ensembling for enhancing zero-shot performance. Can the authors justify the main novelty of this paper?\n2. VGPT is used to generate the text description of the query video, and which is then converted to an image-like text embedding. Why not just prompting VGPT for the downstream applications (e.g. action classification)? Comparison to this baseline is an important justification to the proposed method.\n3. Several components are added to the solution, while the ablations are not sound enough. For example, how important are the three description types (parent context, language attributes, and language descriptions)?\n4. The claim for the comparison to CUPL (Pratt et al., 2022) is not very clear (section 3.1.4). The authors claim that VideoPrompter only requires 3 text descriptions instead of 50 descriptions adopted in CUPL. However, VideoPrompter adopts a VGPT model while CUPL does not. Is using VGPT a better choice in terms of the cost?\n5. The paper criticizes prior work that \u201cthese methods require access to the true distribution of the target task, which can be prohibitive in test-time adaptation and data-scarce environments\u201d. However, the proposed method optimizes the selection of hyperparameters (e.g. temperature) directly on the target dataset (see Figure 3). \n6. The high-level action context is restricted to a tree-type relation. However, some child classes may belong to multiple parent concepts. For example, \u201csurfing\u201d can belong to both \u201cplaying sports\u201d and \u201cwater activities\u201d."
            },
            "questions": {
                "value": "My questions are listed in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5632/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5632/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5632/Reviewer_1vg9"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5632/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788269173,
        "cdate": 1698788269173,
        "tmdate": 1699636584258,
        "mdate": 1699636584258,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CsiiZNAJmc",
        "forum": "9F0xInGNBF",
        "replyto": "9F0xInGNBF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5632/Reviewer_8fLg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5632/Reviewer_8fLg"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework for zero-shot video understanding by using various foundation models including VLMs, i.e., CLIP, LLMs, i.e., GPT, and Video-to-Text model, i.e., VGPT. Experiments are conducted on three different problem settings and showing good results. Ablations are thorough and enough to justify the framework design choices. Written presentation is fair, but could be improved."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper presents a set of experiments on various problem settings: action recognition, video-to-text and text-to-video retrieval, time-sensitive tasks and on different datasets.\n- The ablations are solid and thorough.\n- Experiments show strong improvement w.r.t baselines."
            },
            "weaknesses": {
                "value": "- Since at least 3 foundation models have been used (CLIP, GPT, VGPT), how do we know if those models are trained with examples overlapped with the downstream datasets (e.g., HMDB-51, UCF101, SSv2, K400, MSR-VTT, Charades).\n\n- The novelty seems moderate if not low. As the paper mentions the main contributions are 1) introducing video-to-text to enhance visual embeddings and 2) applications to videos.  \n\n- The written presentation could be further improved:\n     1) section 2.1 could be renamed to \"Overview\" and try to capture the big picture of the framework. The author(s) can refer back to Fig. 1 for the big picture (in the current flow of presentation, there is no big picture and it flows in with overwhelming many details and notations). Then sections 2.2 and 2.3 can be further followed up from 2.1 to provide detailed of components.\n     2) table 6 is presented in page 7, yet never been referred from the text?"
            },
            "questions": {
                "value": "- My main concerns are the leaking examples from downstream datasets to foundation models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5632/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797226215,
        "cdate": 1698797226215,
        "tmdate": 1699636584165,
        "mdate": 1699636584165,
        "license": "CC BY 4.0",
        "version": 2
    }
]