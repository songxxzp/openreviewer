[
    {
        "id": "3r5ww4mgQq",
        "forum": "JpzVlO9X7r",
        "replyto": "JpzVlO9X7r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission979/Reviewer_zRGM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission979/Reviewer_zRGM"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an evaluation of the function modeling capabilities of GPT-4 using several different and datasets. The paper seeks to answer two key questions. (1) Can LLMs comprehend functions when presented with raw data? (2) What extent can they integrate and utilize domain-specific knowledge in function modeling? The paper considers a qualitative evaluation of the most powerful LLM to date i.e., GPT-4.\n\nThe paper claims that (1) GPT-4 is proficient in deciphering data and employing domain-specific knowledge for function modeling, using only in-context learning; (2) Where data is scarce, GPT-4 demonstrates superior performance over conventional machine learning models by leveraging its existing parameter knowledge."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper evaluates the function modeling capabilities of LLMs using synthetic and real-world data.\n\n2. It claims that LLMs can identifying data patterns and also highly efficient in applying domain-specific knowledge to function modeling tasks.\n\n3. It presents applications of such capabilities, e.g. feature selection and kernel design for Gaussian Process models."
            },
            "weaknesses": {
                "value": "1. It is not clear whether the performance is due to memorization in the training data, generalization, or in context-learning. Since transformers can learn functions in-context performing a form of meta-learning (see references below), it will be important to compare with this baseline. \n \nWhy Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, Furu Wei\n\nWhat learning algorithm is in-context learning? Investigations with linear models\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, Denny Zhou\n\nFurthermore, the data contamination prevention by perturbing some words do not seem to be adequate. The authors are encouraged to try out more rigorous techniques such as\n\nProving Test Set Contamination in Black Box Language Models\nYonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, Tatsunori B. Hashimoto\n https://arxiv.org/abs/2310.17623\n\n2. The paper only focuses GPT-4. It will be important to study open source models such as LLaMA2. Comparing and contrasting LLaMA2 can provide more insights on the origin of this capability. \n\n3. The evaluation takes a qualitative approach and yet the authors draw general conclusions."
            },
            "questions": {
                "value": "Understanding the function modeling capabilities of LLMs or transformer models in general is very interesting. However, the paper only focuses on GPT4. With only qualitative evaluation, it offers very limited insights and general takeaways. \n\nIt will be interesting to take a clean slate approach to study transformer models not training on function modeling tasks to understand how well the models can learn in context, and how performance scales with model size, data and compute. Furthermore, when adding function modeling tasks in the training data, how much better performance do we get? How much does it generalize out of distribution?\n\nThe authors are encouraged to take a quantitative approach so that one can draw convincing conclusions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There is no ethics concern as it carries out a study on the function modeling capabilities of GPT4."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission979/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698534968436,
        "cdate": 1698534968436,
        "tmdate": 1699636023863,
        "mdate": 1699636023863,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2nItoAQdHA",
        "forum": "JpzVlO9X7r",
        "replyto": "JpzVlO9X7r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission979/Reviewer_3rSb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission979/Reviewer_3rSb"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a comprehensive investigation into the capabilities of Large Language Models (LLMs), specifically GPT-4, to intuitively model functions in a manner akin to human cognition. It evaluates GPT-4's ability to discern patterns and apply domain-specific knowledge to function modeling, even with limited data availability, without relying on gradient-based learning. \n\nThe authors propose a new evaluation framework tailored to the conversational nature of LLMs and demonstrate through both synthetic and real-world data that LLMs like GPT-4 not only grasp data patterns but also effectively leverage domain knowledge. \n\nThe study further explores the practical applications of these capabilities in data science, highlighting the potential of LLMs in tasks such as feature selection and kernel design for Gaussian Process models.\n\nThe findings underscore the advanced understanding and potential uses of LLMs in various data-centric domains, showcasing their superiority in certain scenarios over traditional machine learning models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper approaches a new and interesting problem, investigating the intuitive function modeling abilities of Large Language Models\u2014a field that combines elements of artificial intelligence with processes reminiscent of human thought.\n\nIn Section 2, the authors establish a theoretical background for their study. They also develop a range of evaluation methods for different domains, indicating a thorough method for investigating GPT-4\u2019s capabilities.\n\nThe clarity of the paper is good; it is written with conciseness and precision, making complex ideas accessible and the arguments presented easy to follow."
            },
            "weaknesses": {
                "value": "The paper focuses exclusively on GPT-4 for its experiments, which may result in conclusions that are not broadly applicable across different models. To ensure that the findings are more generalizable, it would be advantageous for future research to include comparisons with several other Large Language Models, at least in preliminary experiments.\n\nA notable issue with the paper is the clarity of the experimental result figures; particularly, Figure 3 and Figure 7 are too blurry to interpret accurately. This could potentially hinder the readers' full comprehension of the data presented. Ensuring that these figures are presented in a clear, legible format is essential for the effective communication of the research results.\n\nAdditionally, the paper would benefit from the inclusion of specific examples for each task and dataset. Providing concrete examples can help readers better understand how the models work in practice and the nature of the tasks they are being evaluated on. This would not only clarify the methodologies used but also potentially highlight the strengths and limitations of GPT-4 in various scenarios."
            },
            "questions": {
                "value": "See the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission979/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698905747518,
        "cdate": 1698905747518,
        "tmdate": 1699636023780,
        "mdate": 1699636023780,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qgTeI9FibQ",
        "forum": "JpzVlO9X7r",
        "replyto": "JpzVlO9X7r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission979/Reviewer_rQFr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission979/Reviewer_rQFr"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the ability of GPT4 to predict simple functional relationships from relatively small amounts of data. The question is motivated by psychological considerations- namely that people can display this ability in certain situations. The authors evaluate GPT4 on several tasks, including data from both synthetic and real-world functions. They also evaluate the effect of providing domain-specific knowledge about the function to GPT4."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The work is situated in a rich, and relatively under-explored problem domain. The synthetic data experiments are soundly designed. The authors give prompts for most of the experiments to aid reproducibility."
            },
            "weaknesses": {
                "value": "Conceptual Weaknesses\n\n1. The paper is motivated by asking whether GPT4 has some knowledge of simple functions (an example is given of the trajectory of a cannonball), akin to human capabilities. In my opinion this is a solid research question, but I am concerned that many of the experiments have only a tenuous connection to this question. In particular, I am thinking of feature selection, income classification, and currency prediction tasks. These seem to be essentially classic ML tasks, without much connection to function learning per se (beyond the vacuous connection that any supervised learning problem can be cast as learning a function from inputs to outputs- but this is quite different from, e.g. the motivating example of the cannonball trajectory).  Considering that the authors motivate their work by comparing to human abilities this is a bit jarring- is there any evidence that humans are especially good at these kinds of tasks? Note that for the synthetic function prediction tasks, there actually is fairly detailed psychological data available on peoples\u2019 performance.\n\n2.In my opinion, the authors do not give sufficient consideration to the issue of data leakage. To their credit, they do identify this as an issue and propose specific fixes. But my concern is that there is no evaluation of whether these fixes have actually done anything to mitigate the issue.( I do realize the authors are in a bit of a bind here- on the one hand, it is important to evaluate on real-world data, but on the other, details of the GPT4\u2019s training data are unfortunately not publicly available. ) What I would propose is something like this (using the CO2 data for example): evaluate GPT4 on (a) the raw, unmodified data, (b) the data with random noise added, and ( c ) some number of synthetic datasets constructed with a similar latent structure of a combination of a small number of Gaussian kernels. If the performance on GPT4 on (a) is nearly perfect, but the performance on (b) is significantly degraded, and close to the performance on (c ), then I would be convinced that the data leakage mitigation procedure has been at least partially successful. I\u2019m not saying the authors have to do exactly this demonstration- but there does need to be some evaluation of the effectiveness of the leakage mitigation techniques.\n\n3.This is a bit more subjective, but I can't help but feel that, even absent the issues described elsewhere in this review, that this paper doesn't make much of an original contribution beyond answering \"how well can GPT do X\" for yet another value of X.  We already know that GPT can do a lot of things quite well, and considering it was trained on data generated by humans, it is not too surprising that it can recapitulate some human abilities (such as understanding simple functions). Similarly, it is really surprising that GPT can make better predictions when it is given domain knowledge about the function than when it is not? Now, I certainly don't mean to denigrate the field of \"GPT studies\" here- carefully cataloguing and understanding the capabilities of this system is surely of great interest. But by the standards of making \"new, relevant, impactful knowledge\", I unfortunately don't think the paper meets this bar, as it doesn't seem as though there are any especially novel prompting techniques, analysis methods, or tasks introduced in the paper.\n\nMethodological Weaknesses\n\n4. From Pg 7. For the feature selection task, it is not fair to Yamada et. al\u2019s method to claim that GPT4 is \u201cmore data efficient\u201d when you did not actually test Yamada\u2019s method on the smaller dataset! If you want to make this claim, you would need to control the dataset size for both methods, for example evaluate both GPT4 and Yamada on one dataset of size 10^4 and another on 10^2 (or whatever other numbers seem appropriate). \n\n5. On this same task, it needs to be clarified exactly how the evaluation was done, simply saying \u201cAccuracy is measured by an MLP trained using 10^4 samples\u201d is not sufficient. Was the data used to select the features separate from the data used to train the MLP? Is the \u201caccuracy\u201d here the training accuracy or test accuracy? Why are there errorbars on the accuracy, but only a single set of features reported for each method? Does this mean that you only ran each feature selection method once, if so why? \n\n6. When asking GPT to describe its decision rules for the CO2 task, how much credence should we place in this answer and why? After all, the propensity of GPT to hallucinate is so well-known that it is often written about in the NY times. For example, the \"increasing rate of change\" does not appear to even be present in the data, as the authors point out. But the more fundamental issue is that,even for the features that are actually present in the data, that GPT may possibly be mis-representing its own decision process (i.e. its predictions might actually depend on some other feature of the data than what it says). Is there reason to think that GPT can accurately introspect into its own decision process? \n\n7.In the currency exchange rate task, the authors state \u201cthere is a risk of data contamination in this case\u201d. Why didn\u2019t they apply the same mitigation as in the CO2 task (i.e. add random noise), or seemingly any mitigation at all?\n\n\n8. \n\n>I cannot use normal GP directly since there are missing values (section c.4)\n\nA GP model can seamlessly handle arbitrary patterns of missing data so I am not sure what is meant here\n\n-there are many other examples of important technical in the experimental details which are not described- specifics are given in the Questions section\n\nPresentational Weaknesses\n\n9. Overall I found the writing quality and organization of the paper to be poor, here are specific examples:\n\n>in contrast to conventional machine learning models (pg.2)\n\nThis claim is way too strong relative to the actual results- as far as I can tell, the only other model that was systematically evaluated was a small  MLP\n\n\n>three precious metals (pg.4)\n\nPrecious metals are not mentioned in any preceding text\n\n>venturing into diverse domains extending from physics (pg.1)\n\nThere is no data presented on a physics task\n\n> In this work, we ask a more fundamental question of how capable these models are in terms of representing functions at a general level (pg.9)\n\nWhat does it mean to represent a function at a \u201cgeneral level\u201d, and how did the previously-mentioned papers fall short at this? \n\nSection2 seems superfluous to me- as the formal Bayesian framework is never used thereafter, and the math is well-known enough that I don\u2019t think this section adds anything to the understanding \n\nWhile \u201cPredicting missing parts\u201d is listed as a task in section 3.1, I do not see any actual results for this task presented anywhere in the paper or appendix. (Many of the prompts however allude to missing parts). The closest thing seems to be the currency task, where the LLM predicts a kernel function which is used to impute missing values via GP inference, but this is not really a direction prediction of missing values.\n\nSimilarly, in Figure 2, there are appear to be \u201cghost tasks\u201d of Ball trajectory prediction and Stock market prediction, which are not actually studied in the paper. \n\n>(this is relevant for the \u2018output dependency modeling\u2018 task).\n\nThere is no \u201coutput dependency modeling\u201d task (ctrl-F \u201coutput dependency\u201d)\n\n\n10. I also found the language to be overly vague or confusing at times, here are a few examples:\n\n>lays the groundwork for a deeper understanding of many recent studies (pg.9)\n\n>Here, we target the distribution function rather than a simple regression function y = f (x, t) (pg.4)\n\n>As static benchmarks are not ideal (pg.2)"
            },
            "questions": {
                "value": "1. For the \u201cFunction class prediction\u201d, it seems the results are only presented through subplot titles in Figure 3. Not only are these very small and hard to read, but the bigger issue is that they are just shown without any kind of comment or context. For example, there seems to be a very wide range of accuracies, with the accuracy for Gaussian being ~1, and for piecewise being ~0; why do you think some function classes might be so much harder for GPT4 than others? For the ones it gets wrong, what are its predictions (i.e. confusion matrix)? Does this pattern of accuracies match any human data? How do other models (such as an MLP or Gaussia process) perform at this classification task? \n\n2. What was the decoding temperature (tau) for GPT4? Since the results are reported with error bars, I am assuming it was not deterministic-in this case, how was the value chosen?\n\n3. \n>It is worth emphasizing that prompting the model in a different way can change the results significantly (pg.5)\n\nThis is an interesting and important observation - did you systematically evaluate the effect of different prompting strategies on the model performance?\n\n4. For the kernel prediction tasks, does GPT4 also predict the hyperparameter values for the kernels? It doesn\u2019t seem like the prompts specifically ask for this, but maybe the model gives them anyway?  If not, then how did you get the predictions for the LLM kernel? \n\n5. In table 4, why do the first two entries have 0 standard deviation?\n\n6.For the CO2 and currency tasks, who exactly are the human experts?\n\n7.Some of the promprts, for example in section c seem to have an interactive element, e.g.\n\n>You can ask a few questions to me.\n\nDoes GPT4 take you up on this? If so, what questions does it ask and how do you answer them?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission979/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission979/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission979/Reviewer_rQFr"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission979/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699391876587,
        "cdate": 1699391876587,
        "tmdate": 1699636023695,
        "mdate": 1699636023695,
        "license": "CC BY 4.0",
        "version": 2
    }
]