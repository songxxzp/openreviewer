[
    {
        "id": "NW5KsbCn3m",
        "forum": "50P9TDPEsh",
        "replyto": "50P9TDPEsh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2045/Reviewer_qpMo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2045/Reviewer_qpMo"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a benchmark to evaluate the critique ability of LLMs. This benchmark consists of 3K high-quality natural language queries and their corresponding model responses. They also introduce a baseline for self-check, to improve the performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- To explore the critique ability of LLMs is interesting, and timely at this point. \n- This paper provides a standardized way to evaluate the critique ability of LLMs on diverse tasks, \n- The paper offers several noteworthy insights, such as the challenges associated with self-critique in LLMs. These findings can guide future research and model development."
            },
            "weaknesses": {
                "value": "- The evaluation is not comprehensive. While it claims to evaluate the critique ability, it only evaluates this across three tasks: math, code, and commonsense. A broader range of tasks should be tested.\n- The paper does not discuss potential biases. Without discussing these biases, it's unclear how they might influence the evaluation results, which could affect the validity of the findings.\n- Authors could offer a more in-depth analysis of the utility of self-critique. Understanding why self-critique could be better and its influence on critique capabilities would strengthen the paper's arguments.\n- The paper's presentation appears disjointed. The content seems pieced together without careful review. Consistency in terminology is essential for clarity.\n- The paper does not define key terms like the policy model and critic model. \n- Lack of related work.\n- Despite introducing a benchmark, the authors do not release it, limiting its utility and reproducibility for the research community."
            },
            "questions": {
                "value": "- What is the rationale behind choosing different values of k, specifically k = 64 for GSM8K and TruthfulQA, and k = 100 for HumanEval? \n- In Section 5, the phrase \"Assume with appropriate prompting\" is mentioned. Could you provide a detailed explanation of how the prompting was conducted in this step? There are certain aspects that remain ambiguous. Could you clarify these points to ensure a comprehensive understanding for the readers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2045/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2045/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2045/Reviewer_qpMo"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2045/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698569537354,
        "cdate": 1698569537354,
        "tmdate": 1699636136079,
        "mdate": 1699636136079,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8EcckOIEUs",
        "forum": "50P9TDPEsh",
        "replyto": "50P9TDPEsh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2045/Reviewer_WsbC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2045/Reviewer_WsbC"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an investigation into the critique abilities of Large Language Models (LLMs) across various tasks. The authors introduce a new benchmark, CRITICBENCH, which consists of 3K high-quality natural language queries and corresponding model responses annotated for correctness. The benchmark covers tasks such as math problem-solving, code completion, and question answering. The study evaluates multiple LLMs on the dataset and introduces a simple yet effective baseline method named self-check, which leverages self-critique to improve task performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper addresses an important and under-explored aspect of LLMs, which is their ability to critique their own outputs. This is a valuable contribution as it moves beyond traditional evaluation metrics and looks at a model's ability to self-improve.\n\n2. The paper presents a clear definition of critique ability and distinguishes between critique and self-critique, which helps in setting the scope and understanding the objectives of the study."
            },
            "weaknesses": {
                "value": "1. The paper could benefit from a more detailed discussion on the limitations of the current approach, particularly regarding the scalability of the self-check method and its applicability to real-world scenarios [1,2,3].\n\n2. The study is limited to a few tasks and datasets. Expanding the benchmark to include more diverse tasks and domains would make the findings more generalizable.\n\n3. The evaluation of self-critique abilities shows that models struggle with certain tasks, but the paper does not delve deeply into why this is the case or propose potential solutions to improve self-critique performance.\n\n4. The paper does not address the potential ethical implications of models that can self-critique and self-improve, especially in terms of reduced human oversight.\n\nReferences \n\n[1] Madaan, Aman, et al. \"Self-refine: Iterative refinement with self-feedback.\" arXiv preprint arXiv:2303.17651 (2023).\n\n[2] Krishna, Satyapriya. \u201cOn the Intersection of Self-Correction and Trust in Language Models.\u201d (2023).\n\n[3] Huang, Jie, et al. \"Large language models cannot self-correct reasoning yet.\" arXiv preprint arXiv:2310.01798 (2023)."
            },
            "questions": {
                "value": "'None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2045/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699421879170,
        "cdate": 1699421879170,
        "tmdate": 1699636136015,
        "mdate": 1699636136015,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "86snFxKYr8",
        "forum": "50P9TDPEsh",
        "replyto": "50P9TDPEsh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2045/Reviewer_qunD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2045/Reviewer_qunD"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new dataset to evaluate language model's capability of identifying flaws in language model outputs, referred to as the critique ability. The dataset is constructed fully automatically based on language model outputs on three datasets. The authors use various filtering strategies to ensure that the data is of high quality and can effectively differentiate models. The whole process is fully automated, so theoretically it can be extended to other task as well. The authors then use the dataset to evaluate a series of pretrained language models of various sizes to examine their critique abilities as well as the scaling laws."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow. The authors are very clear about all details in the data collection process and provided good motivation for the various design choices. The evaluation is thorough and covers a wide range of models. The proposed new heuristic is not particularly novel, but achieves solid improvement on the new benchmark."
            },
            "weaknesses": {
                "value": "A critique in this paper is defined as a language model assessment of another language model output on some underlying task. A good critique model should be effective at identifying flaws in language model outputs. The challenging examples to the task of critique are nuanced flaws, which would also require a detailed explanation by the critique model. But the benchmark proposed by this paper use a simplistic quantitative metric that reduces the quality of a critique to a binary decision, which assumes that it\u2019s appropriate to use a binary metric for the underlying task as well. The benchmark offers very limited granularity.\n\nUsing a granular quantitative measure means that the qualitative questions that the benchmark can answer are also limited. Outside of developing and evaluating self-refinement heuristics like the one proposed by the authors, the benchmark provides limited information for other uses of model-generated critique, such as informing human oversight. Since the benchmark requires tasks with well-defined, fully-automated metrics for the underlying task, the problem of developing self-refinement critiques does not in fact depend on such a benchmark: even if the model critique doesn\u2019t make sense to a human, as long as it improves subsequent prediction accuracy, it\u2019s a good critique."
            },
            "questions": {
                "value": "The larger models seem much better at critiquing outputs from large models than smaller models. Looking at figure 4, large models have much smaller advantage on critiquing small model outputs than large model outputs. Does this mean the critique ability measurements are inflated by improvement in accuracy on the underlying task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2045/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699499088334,
        "cdate": 1699499088334,
        "tmdate": 1699636135953,
        "mdate": 1699636135953,
        "license": "CC BY 4.0",
        "version": 2
    }
]