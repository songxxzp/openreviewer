[
    {
        "id": "H09bi5tZLv",
        "forum": "lQ5mbHhfQv",
        "replyto": "lQ5mbHhfQv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission383/Reviewer_r9Qa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission383/Reviewer_r9Qa"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Q-tuning, an efficient continual prompt learning for large language models. Prior works in continual prompt learning suffered from an increasing size of prompt as the task number increased. To address this challenge, the authors proposed a way to have a fixed-size prompt by removing the prompts that have non-relevant information to the next task (i.e., de-Q). Also, to better learn the prompts (for forward transfer), the author proposes Q-Prompt Ensemble, which selects the most relevant prompt of the prior tasks when learning a new task by using a low-rank matrix (rank of one). Finally, the author proposes a globally shared prefix prompt for information retention by maximizing the mutual information of the prediction when using the previous task prompts (not pruned yet by the de-Q)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The overall method is sound and effective. Tackled the existing limitation of continual prompt tuning, i.e., the number of prompt increases as the task increase, with a reasonable approach.\n\n(2) The overall writing is very clear, and the presentation is well organized.\n\n(3) The experiments are extensive. Also, the proposed method shows quite good overall results. More analysis by plotting the accuracy and reporting backward transfer will improve the paper."
            },
            "weaknesses": {
                "value": "(1) Insufficient related work. The tackled problem (i.e., progressively extending prompts) is a general limitation of expanding networks in continual learning literature [1,2]. There already exist papers that tackle such problems in continual learning [3]; hence, the authors should more rigorously do the literature review.\n\n(2) Important baseline is missing [3]. For instance, Progress \\& compress [3] tackles the same problem, i.e., tackles the problem of the progressively expanding network (although they did not use it for LLMs; since it was quite old paper). The overall concept is highly similar, for instance, using past knowledge for better forward transfer and compressing the knowledge into a compact representation. Since, [3] can be used for any model (i.e., model agnostic) and can be easily extended to continual prompting (i.e., progressively expanding prompts), the authors should compare the results. Also, the authors should rigorously discuss the similarities and dissimilarities.\n\n(3) It is quite hard to understand that Q-Tuning outperforms ProgPrompt (or Full prompts, MTL) under a long sequence of tasks. Since other methods have a bigger number of parameters (as the prompt size increases), I believe the performance also should be better. Can the author give an explanation or intuition behind these results? Although I can understand that Q-Tuning can have a better forward transfer than ProgPrompt (due to the Ensemble), it is a little awkward that Q-Tuning with a limited capacity shows better results than Full prompts of Q-Tuning.\n\n(4) De-Q rule is indeed reasonable for better forward transfer (as it retains the most relevant prompt for the new task), but it may have a negative effect on the backward transfer (i.e., the forgetting).\n\n(5) The paper only reports the overall performance in the main table, but dividing the results into i) backward, ii) forward, and iii) overall will be more informative.\n\n(6) Somewhat hard to understand the intention of $\\theta_{P}^{*}$, i.e., the prefix prompt for global knowledge sharing. \n- i) First, even though a single prefix prompt well preserves the knowledge by maximizing Eq (5), the optimized prefix will be concatenated with the pruned prompt (i.e., $[\\theta_{P}^{*}, Q^{-1}]$). Therefore, the resulting prompt does not explicitly maximize the mutual information.\n- ii) I think this paper mainly targets forward transfer (although I think backward transfer is also needed for measurement), but this component seems to be presented for backward transfer.\n- I think analyzing the effect of $\\theta_{P}^{*}$ is needed under both metrics, i.e., forward and backward transfer.\n\n(7) (minor) The novelty is somewhat limited, e.g., the Q-prompt ensemble is already proposed by the prior work [4] (also, the term 'Ensemble' is quite confusing, as it does not use multiple models. Maybe not using a new terminology for methods that prior work has proposed will be better), and the concept is quite similar as [3] (and the authors did not compare or highlight the difference in the main text).\n\n[1] Progressive Neural Networks, 2016\\\n[2] Lifelong Learning with Dynamically Expandable Networks, ICLR 2018\\\n[3] Progress & Compress: A scalable framework for continual learning, ICML 2018\\\n[4] Multitask Prompt Tuning Enables Parameter-efficient Transfer Learning, ICLR 2023"
            },
            "questions": {
                "value": "Can the author report the plot of the individual task accuracy curve during continual learning (just for the main baseline, ProgPrompt, and Q-Tuning) ? Such a plot is very helpful in understanding the overall behavior during continual adaptation, e.g., backward, forward transfer.\n\nSee some examples below\\\nFigure 3 of Online Meta-Learning, ICML 2019\\\nFigure 2 of Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning, NeurIPS 2020\\\n\n---\n\nAlso, is the size of GPT-4 known to the community (just out of curiosity)? Will be great to include the reference in Line 22."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission383/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790190810,
        "cdate": 1698790190810,
        "tmdate": 1699635965381,
        "mdate": 1699635965381,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pCgLNeEXvm",
        "forum": "lQ5mbHhfQv",
        "replyto": "lQ5mbHhfQv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission383/Reviewer_tNki"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission383/Reviewer_tNki"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new prompt-based continual learning method for sequential language tasks. The core components are threefold: globally shared prefix prompt, MLP (two-layered and residual) parameterized prompts, and a De-q process to keep the free capacity of prompt memory for future tasks. The model updates prefix prompts through ensemble updates of past-task prompts and temporally learnable weights to retain past-task information and prevent overfitting to current tasks. For De-q, when the buffer capacity reaches the pre-defined maximum size,  the model decomposes learned queries via the SVD-based technique. The proposed method is extensively validated using Bert/T-5 models on multiple task sequences and achieved improved performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper addresses two challenges in prompt-based continual learning - preventing forgetting efficiently and keeping the prompt capacity reasonable even on training long task sequences. The paper is basically well-described and provides extensive evaluations on multiple benchmark task sequences using Bert/T-5 backbones. Also, unlike Progprompt, a prompt-based continual learning baseline, the suggested method can handle long task sequences (~70 tasks) without suffering from memory overhead issues."
            },
            "weaknesses": {
                "value": "- Lack of comparison with prompt-based continual learning methods. Only Progprompt can be a straightforward baseline in view of 'prompt-based continual learning', which is insufficient since there are multiple strong prompt-based continual learning approaches [1,2,3].\n\n- Insufficient forward knowledge transfer evaluation. The paper only compares the FKT with a simple prompt-tuning. Throughout the paper, the authors emphasize multiple times, but, this is not that helpful to validate the strength of the proposed method on FKT. I strongly recommend evaluating it with more diverse continual learning methods, including [4,5], which also empirically emphasize forward transfer improvements in language continual learning.\n\n- Missing ablation studies like efficiency. Recently, some continual learning researchers claim that the training time/real-time computational costs matter in real continual learning scenarios rather than memory occupancy, as the monetary cost for memory (e.g., HDD) is much cheaper than computations (e.g., GPUS) [6]. So, I also recommend providing the wall-clock evaluation/ trainable parameter counts compared to recent prompt-based continual learning methods that I introduced.\n\n\n\n[1] James Seale Smith, et al., Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning. CVPR 2023.  \n[2] Yabin Wang, et al., S-prompts learning with pre-trained transformers: An occam\u2019s razor for domain incremental learning. NeurIPS 2022.  \n[3] Zifeng Wang, et al., Dualprompt: Complementary prompting for rehearsal-free continual learning. ECCV 2022.  \n[4] Wenpeng Yin, et al., ConTinTin: Continual Learning from Task Instructions, ACL 2022.  \n[5] Qi Zhu, et al., Continual Prompt Tuning for Dialog State Tracking, ACL 2022.  \n[6] Ameya Prabhu, et al., Computationally Budgeted Continual Learning: What Does Matter? CVPR 2023."
            },
            "questions": {
                "value": "."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission383/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810671905,
        "cdate": 1698810671905,
        "tmdate": 1699635965308,
        "mdate": 1699635965308,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0uxWxyu8uH",
        "forum": "lQ5mbHhfQv",
        "replyto": "lQ5mbHhfQv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission383/Reviewer_itxM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission383/Reviewer_itxM"
        ],
        "content": {
            "summary": {
                "value": "This paper builds upon the foundations laid by ProgPrompt by introducing three key mechanisms: (1) prompt ensemble, (2) prompt dequeuing, and (3) shared prompt combined with memory retention. The novelty appears somewhat limited as it combines elements from several existing works. Further empirical results are necessary to thoroughly assess the effectiveness of the proposed method and identify its true impact."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The research topic of continual learning for pre-trained models is of utmost importance.\n- The existing empirical results demonstrate that the proposed method attains a new state-of-the-art performance level.\n- The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "- The novelty of the proposed method appears to be relatively limited, as the newly introduced key modules have all been explored by other works. Surprisingly, the authors do not engage in discussions or comparisons with those existing works.\n  * In [1], although adapters are used, a prompt essentially represents a specialized version of an adapter. The authors in that work have already addressed crucial questions such as (1) how to select a suitable adapter, and (2) how to integrate a newly adapted version customized for the current task into the pool of adapters, a concept akin to the prompt queue in this study. It's worth noting that the argument made by the authors in Line 150 regarding \"the quantitative correlation of different tasks is hard to define\" has been resolved in this work.\n  * The effectiveness of the shared prompt concept has been demonstrated in the DualPrompt work [2].\n  * The ensemble approach involving existing prompts, as investigated and proven effective in CODA-Prompt [3], has not been acknowledged or discussed in this context.\n- Some important empirical results to justify the proposed method are missing.\n  *Exploration using more advanced language models, such as LLaMA, could determine whether the algorithm is sensitive to different backbones.\n  * Ablation studies focusing on the prompt ensemble, utilizing only a linear combination of prompts instead of incorporating a weighting matrix with numerous parameters, would offer valuable insights.\n  * Given the introduction of additional parameters, a fair comparison with ProgPrompt could be conducted by increasing its prompt length.\n  * To provide a comprehensive analysis, it would be beneficial to extend the comparison by adding a shared prompt to ProgPrompt and evaluating the results in this context.\n\n\n[1] Ermis, Beyza, et al. \"Memory efficient continual learning with transformers.\" Advances in Neural Information Processing Systems 35 (2022): 10629-10642.\n[2] Wang, Zifeng, et al. \"Dualprompt: Complementary prompting for rehearsal-free continual learning.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n[3] Smith, James Seale, et al. \"CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "questions": {
                "value": "- Could you kindly explain why the first term of equation (5) does not include Q^i together with the shared prompt? Have you experimented with this version, and if the performance was unsatisfactory, could you provide insights into the reasons behind it?\n- It appears that enlarging the queuing size results in marginal performance improvement. Could you elaborate on the reasons for this phenomenon?\n- MTL is expected to serve as an upper bound, but it is puzzling that its performance is even lower than that of the per-task prompt. Could you offer insights into this unexpected outcome or explain your detailed implementation for it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission383/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699511450802,
        "cdate": 1699511450802,
        "tmdate": 1699635965234,
        "mdate": 1699635965234,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8rjiwnpBhA",
        "forum": "lQ5mbHhfQv",
        "replyto": "lQ5mbHhfQv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission383/Reviewer_KTxv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission383/Reviewer_KTxv"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new approach for continual learning, called Q-tuning, based on prompt tuning. Specifically, the authors introduce a dynamic prompt ensemble mechanism to adaptively incorporate the knowledge within previous prompts, according to a given task. In addition, to address the computational limitation from the progressive addition of the prompts, they introduce two different mechanisms, the De-Q Rule and globally shared prefix; the De-Q Rule selectively removes the most irrelevant parts from the currently constructed prompts. The globally shared prefix is distilled from the previous set of prompts with memory retention regularization to compensate for the information loss during De-Q. With two popular language models (T5 and BERT), Q-tuning has been demonstrated in the various experimental scenarios, consistently improving the strong baselines in the field."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. **Clarity**. Overall, the writing is clear and easy to follow. In addition, the organization of the main draft is well-established.\n2. **Well motivated problem**. Improving the continual prompt tuning with LMs is an interesting and important problem. To this end, overcoming the previous limitation from the increase of tasks is a reasonable and well-motivated direction. \n3. **Intuitive method.** The proposed method is intuitive and seems to be applicable to various LMs. Also, it shows consistent improvement compared to the existing baselines, and the gain is significantly enlarged with challenging setups."
            },
            "weaknesses": {
                "value": "1. **Fairness with baselines**. The proposed Q-tuning uses more trainable parameters by introducing $W^i$ for dynamic prompt ensemble mechanism and $P*$ for global knowledge sharing. Since the performance of prompt tuning significantly depends on the number of trainable parameters, the author should compare the baselines that use the same number of trainable parameters for a fair comparison. For example, increasing the prompt length for the baselines, to match the number of parameters. To be specific, it is doubtable that the gain of Q-tuning over ProgPrompt in Table 1 is the consequence of this, as there is no issue regarding the information loss and De-queuing here.  \n2. **Consistency of presentation.** The experimental results are presented inconsistently across the tables. For example, Tables 1 and 2 report the average over 3 runs. But, Tables 3 and 6 suddenly report the variance without mentioning the reason and the number of runs. Also, for the tables that omitted to report the variance, there are no results with variance in the Appendix. To provide the information to readers, it seems to be included in the main tables or the appendix with proper description.\n\n### Minor\n\n1. **Typos.**  Lines\u001d 132 and 137: $W$ \u2192 $W^i$.  \n2. **Editorial comments.** In Figure 1, there is no mention of $W^i$ in the caption while it is presented in the figure. To enhance the readability, it seems to be included."
            },
            "questions": {
                "value": "Please address the concerns in Waeknesses, especially on **Fairness with baselines**. If the concerns are properly addressed, I'm willing to increase the score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission383/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699525725378,
        "cdate": 1699525725378,
        "tmdate": 1699635965140,
        "mdate": 1699635965140,
        "license": "CC BY 4.0",
        "version": 2
    }
]