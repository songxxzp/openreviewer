[
    {
        "id": "rmVyTORsbF",
        "forum": "iBAWiEjogY",
        "replyto": "iBAWiEjogY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5058/Reviewer_oPJy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5058/Reviewer_oPJy"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a general architecture for protein representation learning, utilizing a pre-training method of masked residue type prediction. Following this, the model is fine-tuned using lightweight decoders for a range of downstream tasks such as model quality assessment, binding affinity change prediction, EC and fold classification, protein design, and antibody design. It manages to achieve state-of-the-art performance in certain areas."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper's ambition to create a universal protein pre-training model is commendable. Pursuing this aim, the author presents a transformer architecture that seamlessly integrates sequence and structural data through a straightforward pre-training objective."
            },
            "weaknesses": {
                "value": "1. The majority of the components within the proposed method are adaptations from prior studies, which the paper fails to acknowledge. The transformer approach mirrors that of [1], and the pre-training objective resembles [2]. Consequently, the work's novelty is somewhat questionable.\n2. The paper's \"related work\" section is not exhaustive. Notable omissions in the protein language domain include TAPE [3], ProtTrans [4], Ankh [5], and ProGen2 [6]. Additionally, recent advancements in protein structure representation, such as ProNet [7] and CDConv [8], are overlooked. A discussion on the connection between this study and previous ones is conspicuously absent.\n3. The presented experimental results have serious issues like data leakage and the absence of critical baselines, undermining the paper's claims.\n\nFor details, please refer to the Question section."
            },
            "questions": {
                "value": "1. The authors motivate the use of transformers as encoders by critiquing graph-based representation learning methods for two reasons: 1) their inability to capture detailed atomic information, and (2) their disregard for long-range interactions. However, recent studies [7,9,10] demonstrate that graph-based methods can indeed be extended to the atom level. Additionally, the authors' method focuses solely on backbone-level structures, thus overlooking side-chain details. Regarding long-range interactions, the paper lacks experiments substantiating their claim. Thus, the critiques they raise against graph-based methods lack evidence.\n2. It's inadvisable for the authors to describe their method as \"groundbreaking\" in the introduction\u2014this is a clear exaggeration.\n3. The model quality assessment, used by the authors for evaluation, has a potential data leakage risk during pre-training. This task aims to predict the GDT-TS score of certain model predictions without revealing the ground-truth structures. Yet, using the PDB data up to May 1st, 2023, means the model may have encountered target structures from CASP14 and CASP15 during pre-training. Despite different loss functions, this could pose a significant issue.\n4. In the model quality assessment, the authors omit essential baselines. Notably, this task has been included in the Atom3D benchmark [11], where baselines [9,12] are essential references.\n5. For binding affinity prediction, the authors neglect to explain their dataset splits\u2014critical to avoid data leakage. Given the small test datasets, it's standard to conduct multiple cross-validations under varying random seeds. Traditional methods like FlexDDG [13] should also be considered for comparison.\n6. In the EC and fold classification benchmarks, there's an absence of vital baselines, notably CDConv [8] and ESM-GearNet [14]. The authors might also explore the more challenging GO prediction tasks detailed in [15]. Even without these benchmarks, the authors' method falls behind leading approaches.\n7. For protein design tasks, there are serious data leakage problems due to the presence of test data in pre-training dataset. As discussed in App. E.3, such leakage can dramatically affect performance. The authors have not provided a fair comparison with other methods, which makes the evaluation here not convincing.\n8. In both protein and antibody design tasks, the metrics of perplexity and aar have been misleadingly employed in the field to evaluate protein folding models. The focus of these metrics on \"local\" recovery rather than entire sequences can inflate performance figures. A more accurate gauge would be to use the AF2 metric to assess structure recovery.\n\nOverall, I commend the authors' ambition to introduce a universal model for protein-related tasks. However, their review of prior works appears incomplete, and the comparisons in their experiments lack rigor. Consequently, this paper does not meet the acceptance standards of ICLR.\n\n[1] Shan et al. \u201cDeep learning guided optimization of human antibody against sars-cov-2 variants with broad neutralization\u201d, PNAS, 2022\n\n[2] Zhang et al. \u201cProtein representation learning by geometric structure pretraining\u201d, ICLR, 2023\n\n[3] Rao et al. \"Evaluating protein transfer learning with TAPE.\"\u00a0NeurIPS, 2019\n\n[4] Elnaggar et al. \u201cProttrans: Toward understanding the language of life through self-supervised learning\u201d, PNAS, 2021\n\n[5] Elnaggar et al. \u201cAnkh: Optimized Protein Language Model Unlocks General-Purpose Modelling\u201d, 2023\n\n[6] Madani et al. \u201cLarge language models generate functional protein sequences across diverse families\u201d, Nature Biotech, 2023\n\n[7] Wang et al. \u201cLearning Hierarchical Protein Representations via Complete 3D Graph Networks\u201d, ICLR, 2023\n\n[8] Fan et al. \u201cContinuous-Discrete Convolution for Geometry-Sequence Modeling in Proteins\u201d, ICLR, 2023\n\n[9] Jing et al. \u201cEquivariant Graph Neural Networks for 3D Macromolecular Structure\u201d, 2021\n\n[10] Zhang et al. \u201cPre-Training Protein Encoder via Siamese Sequence-Structure Diffusion Trajectory Prediction\u201d, 2023\n\n[11] Townshend et al. \u201cATOM3D: Tasks On Molecules in Three Dimensions\u201d, NeurIPS Dataset and Benchmark Track, 2022\n\n[12] Pages et al. \u201cProtein model quality assessment using 3d oriented convolutional neural networks\u201d, Bioinformatics, 2019\n\n[13] Barlow et al. \"Flex ddG: Rosetta ensemble-based estimation of changes in protein\u2013protein binding affinity upon mutation.\"\u00a0The Journal of Physical Chemistry, 2018\n\n[14] Zhang et al. \u201cEnhancing protein language models with structure-based encoder and pre-training\u201c, 2023\n\n[15] Gligorijevic et al. \u201cStructure-based protein function prediction using graph convolutional networks\u201d, Nature Communications, 2021"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5058/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5058/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5058/Reviewer_oPJy"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5058/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698093053034,
        "cdate": 1698093053034,
        "tmdate": 1699636495673,
        "mdate": 1699636495673,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RS6H5TqUk3",
        "forum": "iBAWiEjogY",
        "replyto": "iBAWiEjogY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5058/Reviewer_wGDE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5058/Reviewer_wGDE"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel pretraining approach for protein representation learning that integrates sequential and structural information. The structural encoding mechanism enables the encoder to learn protein distance information and spatial relative positions of residues, overcoming the inherent drawbacks of ignoring long-range interactions of graph-based representations. As a result, the authors present the model ProteiNexus pretrained by a hybrid masking strategy and mixed-noise strategy to comprehensively capture the structural information. The model is fine-tuned with lightweight task-specific decoders, culminating in exemplary performance across a range of downstream tasks, especially in the understanding of protein complexes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality: The paper proposes a novel pretraining strategy which effectively integrates the sequential and structural encoding for the representation learning of proteins. Therefore, the paper uniquely contributes to the field by implementing a simple, yet potent, architecture to capture structural information comprehensively.\n\nQuality: The paper carefully designs the experiments to support the idea. In particular, the extensive experimental results and experimental details presented in the manuscript reflect the comprehensive work of the authors. \n\nClarity: The paper effectively communicates its ideas and findings with clarity. The paper is well-written, and the logic is coherent. The experimental settings and findings are structured and easy to find the related contents. \n\nSignificance: The paper focuses on improving representation learning for proteins as a foundation model for multiple downstream tasks. The model proposed in the manuscript is able to encode sequential and structural data, and surpass baselines on many downstream tasks, illustrating its potential in even more applications in protein design and discovery."
            },
            "weaknesses": {
                "value": "1. Although the paper is well-written, logically coherent, and self-consistent, I'm afraid the novelty of the paper is not too high. The pertaining strategy which combines sequential and structural information is not new to the field. Furthermore, the major contribution of the paper, which is encoding both the atom-level and the finer-grained distance information, has also been studied extensively in recent years. Therefore, I cannot be persuaded of the novelty of the manuscript unless the authors can provide more evidence about how their model differentiates from existing methods and how their adaptations contribute to the enhanced model performance."
            },
            "questions": {
                "value": "1. For pretraining experiments, I'm wondering how the noise level is determined. Besides, I'm wondering whether the authors have evaluated the data efficiency of the proposed pertaining approach by varying the pertaining data sizes. \n\n2. For fine-tuning experiments, are the encoder parameters also fine-tuned or frozen? Besides, since the focus of this paper is to evaluate the capability of the encoder and the decoders are already lightweight, why not simply fix the decoder architecture for every downstream task?\n\n3. The authors mention in the conclusion that \"... an efficient pre-training model ...\", but I'm wondering how the \"efficiency\" is illustrated: does it enhance model performance, have less computation cost, or require less training data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5058/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5058/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5058/Reviewer_wGDE"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5058/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698271135354,
        "cdate": 1698271135354,
        "tmdate": 1699636495572,
        "mdate": 1699636495572,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bhXG5MNoTo",
        "forum": "iBAWiEjogY",
        "replyto": "iBAWiEjogY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5058/Reviewer_tXRC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5058/Reviewer_tXRC"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new pretraining approach for learning protein representations, which integrates both structural information and information about downstream tasks. The paper compares their approach to the state-of-the-art on a range of different tasks and report very favourable results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper presents a potential solution to a highly relevant problem. The authors have compiled a very comprehensive list of relevant downstream tasks, and thereby make a good the case for a generally useful pretrained model. The reported results are highly competitive. If they hold, the method could thus have a real impact for practitioners in the application domain."
            },
            "weaknesses": {
                "value": "First of all, I have a slight concern about how well this manuscript fits within the scope of the ICLR venue. Although the title and the introduction point towards a new method, the effective focus of the paper seems to be on benchmarking their method, rather than on the method itself. It seems odd to me that most of the description of the model itself has been moved to the appendix, despite the fact that this would presumably be the most interesting part for most of the ICLR community. For instance, the procedure for fine-tuning on downstream tasks is described in a single sentence in the main text - although it is quite central to their approach.\n\nSecondly, despite the fact that the focus is on the benchmarking, there are details missing about the experiments that makes it difficult for me to judge how much faith can be placed in the reported results. For several experiments (see details below), it is unclear how data was split between training, validation and test - and whether this was done in the same way for all methods that were compared in the result tables. Of particular importance, I did not find it clear whether there was overlap in the pre-training data and the data used for downstream testing. In appendix E.3, the authors have a few reflections on this topic, and seem to show a substantial drop in performance for the protein design task when removing part of the pretraining set. This seems like a red flag to me, which should be investigated more thoroughly - and not only for the protein design task.\n\nFinally, the paper itself does not provide a good explanation for why their approach outperforms prior methods. For the community, it would be useful to know if this relies primarily on the structural signal or the fine-tuning on downstream tasks. These questions are potentially partially addressed by the ablation study in appendix E, but the ablation results are never discussed in the main paper. Also, as far as I could see, the effect on fine-tuning on downstream tasks is not ablated - i.e. the difference between fine-tuning the entire pre-trained model or training a downstream model on a fixed pre-trained model."
            },
            "questions": {
                "value": "Page 2. *\"Predominantly, graph-based representations struggle to preserve \ufb01ne-grained atom information effectively. Moreover, they tend to accentuate interactions among neighboring residues while often disregarding the in\ufb02uence of longrange interactions.\"*\nCould you provide a reference to back up this statement?\n\nPage 3. *\"Additionally, there are methods that transfer protein structures into distance matrices and attempt to denoise noisy distance matrices while simultaneously predicting the types of corresponding residue types. These approaches undergo pre-training on large-scale datasets to improve the quality and generalizability of the representations.\"*\nCould you provide citations for these methods?\n\nPage 4. *\"Lastly, we partition the continuous coordinates into bins of equal width and transform each bin into an embedding\"*\nCould you describe the motivation for this choice to discretize?\n\nPage 4. *\"the \"distance tokenizer\" method\"*\nAs far as I can see, this method has not been introduced in the paper. Could you elaborate?\n\nPage 4. *\"Speci\ufb01cally, we employ a one-hot encoding scheme to represent the relative distance between position i and position j in the sequence\"*\nWhy use a one-hot encoding to represent a distance? - doesn\u2019t this mean that there is no distinction between bins that are similar in distance and bins that are far apart?\n\nPage 4. *\"To better capture the global features and interactions of protein structures, we have opted for the transformer architecture as the backbone of our network. This decision is grounded in the inherent self-attention mechanism of the transformer, which enables computations across the entire protein sequence.\"*\nI don\u2019t understand the distinction you make here. If the graph attention is not capturing enough of the interactions you wish for, can\u2019t you then change the graph to include more interactions? In particular, as far as I can see, graph attention in a fully connected graph would be identical to the attention in a transformer. From that perspective, isn't your approach just a special case of graph attention?\n\nPage 5. *\"masked residues\"*\n\"What does \u201cmasked residue\u201d mean exactly. Are you masking the identity, or also the atom coordinates?\"\n\nPage 5. *\"encourage the model to recover authentic atom-level coordinate from noise-induced residue-level pair representations.\"*\nCould you be more precise? How is this \"encouraged\"?\n\nPage 5. *\"Our training dataset includes decoys derived from 7992 unique native protein structures, obtained from DeepAccNet. In the end, we have a collection of 39057 structures in our training dataset, with a fraction representing native structures.\"*\nSince it is central to the data generation process, you should explain in detail what DeepAccNet is, and why it makes sense to use decoys generated by this method as training data. EDIT: I see that you introduce DeepAccNet in the next paragraph, so part of the problem could be resolved by moving that introduction up here. But even when doing so, it is still not clear how the decoys are generated by this method, since it as far as I can see normally produces LDDT scores as output.\n\nPage 5. *\"our test set is meticulously curated. It includes targets with experimentally resolved structures from CASP14 and CASP15, paired with their corresponding predicted structures. To ensure diversity and representativeness, we perform a redundancy reduction process on the test set, limiting sequence identity between targets to within 70%.\"*\nDo you also ensure that there is no overlap (high sequence similarity) between the test set and the 7992 structures in your training set? \nThe choice of homology reduction to 70% seems rather high to me (we generally use values at 30% to avoid leakage). Why was this choice made? I guess you could verify whether this is a problem by plotting the performance as a function of homology to the nearest protein in the training set.\n\nPage 5. *\"We validate our pre-training model on \ufb01ve datasets\"*\nDoes this mean that you in this case do not train a downstream model, but directly use the frequencies of the pretrained model to obtain and estimate of the binding affinity. This should be clarified. If you do use a downstream model, you should clarify how the splits for training/test were constructed (and whether they overlap with the pretraining set). In particular, it is important to establish whether the methods in Table 2 are actually comparable (i.e whether we believe that none of them have been trained on the current test set).\n\nTable 3\nWere these other methods run with exactly the same train/test splits as you run your method. In other words, are the results comparable?\n\nPage 7. *\"LSTM (Rao et al., 2019), mLSTM (Alley et al., 2019) and CNN Shanehsazzadeh et al. (2020).\"*\nIt us a bit odd that you use architecture names to refer to specific trained models. It would be clearer if you for instance referred to the first as TAPE-LSTM, and the second as the UniRep model.\n\nPage 7. *\"ESM-1b\"*\nThe collection of baseline methods was a bit confusing. For instance, you mention language models like ESM-1b and ProtBert-BFD. How are these employed for fold and enzyme-catalyzed reaction classification? Do you somehow use them in an unsupervised way, or do you put a classification layer on top. If so, it would be clearer if you referred to them by a different name than the language model on which they are based.\n\nTable 4. \nAgain, it is unclear if these methods has been trained and tested on exactly the same datasets - in particular since some of the results are copied from other papers. Please clarify.\n\n\n\n### Minor comments:\n\nPage 1. \"For instance antibodies (such as SARS-CoV2)\"\nRephrase. SARS-CoV2 is not an antibody.\n\nPage 1. *\"in protein sequences (Consortium, 2019)\"*\nChange reference to reflect which consortium\n\nPage 1. *\"triumph in various tasks including...protein structure prediction (Rao et al., 2020;\"*\nThis paper is about contact prediction, not directly about protein structure prediction.\n\nTable 3. Caption. The title currently says *\"Results of classification\"*. Would be helpful if you could specify the experiment in the title."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5058/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698697967349,
        "cdate": 1698697967349,
        "tmdate": 1699636495483,
        "mdate": 1699636495483,
        "license": "CC BY 4.0",
        "version": 2
    }
]