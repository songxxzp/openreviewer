[
    {
        "id": "H1HxdqN6Qp",
        "forum": "MLBdiWu4Fw",
        "replyto": "MLBdiWu4Fw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4917/Reviewer_1UqW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4917/Reviewer_1UqW"
        ],
        "content": {
            "summary": {
                "value": "This paper presents InternVid, an large-scaled video-language dataset, consisting of a collection of 7M videos, 234M video clips and corresponding textual descriptions. They consider diversity and data quality by sourcing YouTube contents from various countries and languages. This new dataset enables pretraining a video-language model, named ViCLIP, exhibiting state-of-the-art scores in action recognition and video retrieval tasks. It also demonstrates a credible generative capability in text-to-video generation, supported by qualitative and quantitative assessment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Clarity in writing\n\n- The paper is well-written and easy to understand.\n\n2. Scale and impact\n\n- InternVid includes large-scaled 7.1M videos, making it a significant contribution to the video-language pretraining. Referring to table 1, InternVID comprises 234M 720p video clips with LLM generated captions which shows outperforming accuracy in description. The dataset\u2019s substantial size can potentially have a profound impact on the field.\n\n3. Diversity and Quality\n\n- The effort to collect videos from various countries and languages is commendable. This diversity in data collection may address the issue of bias, a crucial aspect of large-scale datasets, as it mitigates the risk of training biased models.\n\n4. High performance\n\n- The suggested model, referred as ViCLIP, achieve the state-of-the-art in video-language benchmarks such as  action recognition and text-to-video retrieval.\n- InternVID improves text-to-video generation by empowering the previous models with its large scale video-text dataset. It shows quantitative and qualitative (Figure 12) improvement in generation quality."
            },
            "weaknesses": {
                "value": "1. Lack of definitions, explanations\n\n- Definition of UMT-SIM is absent. In table 2, justification of the design choice of InternVid-10M-FLT is not explained. (At glance, the result of performance improvement after filtering the dataset with a high UMT-SIM score seems trivial.)\n- Figure 5 introduced 3 schemes of interleaving video clips, text, and ASR text. However, comparison between the methods or further analysis is not given.\n- It is doubtful that CLIP is a suitable choice for comparing their method in action recognition tasks. There are likely more appropriate video models to use in the benchmark for a fairer comparison.\n\n2. Lack of novelty\n\n- The proposed method for generating the dataset is not novel. It simply uses BLIP to generate captions for video frames, and exploits LLM to generate a summarized caption for the video. Also, description of the used LLM is not given.\n- ViCLIP is not a novel architecture and inherits the CLIP model without significant modifications. The paper predominantly focuses on introducing the dataset, which leaves room for exploring further architectural improvement and training techniques.\n\n3. Unclear model selection process\n\n- The paper lacks information on the criteria used to select the image captioning and language models.\n\n4. Absence of ablation study and potential impact\n\n- In contrast to previous models which are benchmarked in datasets mostly composed of English, InternVID consists of clips with diverse languages. Further analysis about the potential impact caused by this aspect should be considered.\n\n5. Minor comments on Figure 2\n\n- Similar colors like green and dark green are used, but this may cause confusion. Using more distinct and discrete colors, such as red and blue, could enhance the clarity."
            },
            "questions": {
                "value": "1. Consideration with Table 6\n\n- Comment of VideoCrafter, VideoFusion is absent. Does InternVID also improve VideoCrafter, VideoFusion in IS, FID, FVD, and CLIPSIM? Can you provide further analysis about the choice of InternVID-Aes-18M rather than exploiting the full InternVID-200M? \n\n2. Consideration with Section 5.2 and Figure 13~17\n\n- To insist that InternVID provides more powerful video-text aligned representations for video-centric dialogue systems, authors must compare the results from VideoChat-ViCLIP with that of vanilla VideoChat. Can you provide quantitative and qualitative comparisons, showing improvement of the results after replacement of the previous visual encoder with ViCLIP?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The data source is YouTube, where the content often includes personal recordings, potentially raising privacy concerns. The collectors should carefully address these issues and rigorous data selection criteria should be put in place to ensure strict adherence to privacy and other relevant guidelines."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4917/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4917/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4917/Reviewer_1UqW"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4917/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697874413041,
        "cdate": 1697874413041,
        "tmdate": 1700553891753,
        "mdate": 1700553891753,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tT8nT4UOl8",
        "forum": "MLBdiWu4Fw",
        "replyto": "MLBdiWu4Fw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4917/Reviewer_6e9v"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4917/Reviewer_6e9v"
        ],
        "content": {
            "summary": {
                "value": "- This paper introduces a web-scale video language dataset InternVid comprised of 234M clips and 760K hours.\n- It introduces ViCLIP, a video-language model based on ViT-L, pretrained with contrastive learning (like CLIP) and masked autoencoder (like VideoMAE, MAE).\n- This work expands the utility of their dataset to video understanding tasks like recognition and retrieval, and video generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- It introduces a web-scale video-language dataset bridging the gap of lack of such datasets, unlike image-text domains.\n- It evaluates on multiple benchmarks in both finetuned and zero-shot setups.\n- The dataset curation is fairly detailed and I also like the hierarchical video caption generation strategy."
            },
            "weaknesses": {
                "value": "- The majority of the comparisons are with CLIP which was pretrained with image-text pairs and not video-text; I would like to see how ViCLIP performs compared to other video or video-language models (e.g., pretrained on HD-VILA, HT100M, YT100M), I think that would be a fair comparison than comparing with image-text models.\n- I would encourage authors to dig deeper and find a more concrete argument/explanation: why does pretraining with InternVid-10M-FLT or InternVid-10M-DIV perform better in **zero-shot** than InternVid-200M, but not when **finetuned**. \n- In fig. 7 and 8, the experiments are done only on scaling the dataset size, it would be interesting to see the effect of model scaling in addition to the dataset scaling. I would encourage you to add such experiments in the final version.\n- I would be interested to see linear evaluation (i.e., a single FC layer, or use linear SVM) performance on the downstream benchmarks."
            },
            "questions": {
                "value": "- Will you share the pretrained and finetuned models with the supporting code base (e.g., data processing, pretraining, finetuning, generation)? \n- Could you please share the processed clips, even processing the data by individuals (typically for academic researchers) would be a difficult task considering its massive size. Additionally, we all know the unavailability of videos due to location constraints, permission issues, etc., so even if the full data is not possible to share at least share the 3 10M versions.\n- I suggest releasing the fixed embeddings of the datasets from the trained (e.g., pretrained, finetuned) models.\n- Did you investigate if the InternVid has any sort of bias in the curated clips, could you please share a report with such details? Bias could be of many forms e.g., location/race/gender per action category. A suggested reference: https://arxiv.org/abs/1505.01257\n- Did you investigate, if ViCLIP is robust against some of the OOD setups, some of the popular benchmarks are Mimetics, RareAct etc. For more details please see: https://arxiv.org/abs/2306.02014"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4917/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4917/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4917/Reviewer_6e9v"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4917/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698680389582,
        "cdate": 1698680389582,
        "tmdate": 1699636477339,
        "mdate": 1699636477339,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xt7EedelKQ",
        "forum": "MLBdiWu4Fw",
        "replyto": "MLBdiWu4Fw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4917/Reviewer_G48V"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4917/Reviewer_G48V"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new video and language dataset, INTERNVID, which includes 234M video-text pairs lasting 760K hours. In addition to the dataset, this paper also introduce a baseline model, ViCLIP, demonstrating its performance on various downstream applications after pre-training on INTERNVID dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well written, and each section is easy to follow. \n\n2. The proposed large-scaled video-text dataset can contribute to the community, scaling up the current model and largely improving the video and language feature representation learning. \n\n3. This paper provides very detailed statistic for the dataset, and also reveal the method for data curation.\n\n4. By leveraging the new dataset, this paper demonstrates extensive experiments on many downstream applications and achieving promising results on many benchmarks."
            },
            "weaknesses": {
                "value": "1. There are not many analyses and design justification for the proposed ViCLIP. If ViCLIP is claimed as one of the contributions of this paper. The proposed architecture is not novel and the masking idea needs further justification. For example, the efficiency gain vs. the performance drop, and the ablation over the masking ratio.\n\n2. The video caption is generated by language model from frame-level captions. In this case, will this reduce the number of motion-related words that need to be captured from video-based understanding?"
            },
            "questions": {
                "value": "Given 10M pretrained data, the ViCLIP receives better zero-shot action recognition results from WebVid and better fine-tune action recognition results in Table 2 and 3. And the gain from 50M, 200M INTERNVID pretraining is minor. Does it mean the pretraining data is not the more the better? The performance of vision and language model will be saturated when the pretraining data reach to a certain scale? Could you please provide more insights here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4917/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790159010,
        "cdate": 1698790159010,
        "tmdate": 1699636477084,
        "mdate": 1699636477084,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "73u7sK1sWS",
        "forum": "MLBdiWu4Fw",
        "replyto": "MLBdiWu4Fw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4917/Reviewer_gWXX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4917/Reviewer_gWXX"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a large-scale video-text dataset for video representation learning. Considering the noise and low correlation in ASR transcripts, this paper utilizes captioning models and a pre-trained language model to generate video descriptions. The authors pre-train the ViCLIP model on the collected dataset and the model shows strong performance on action recognition and text-video retrieval tasks. They also explore potential applications of this dataset on text-to-video generation and video-centric dialogue systems."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The dataset is large-scale and diverse, and the generated descriptions have better relevance with videos.\n- The pre-trained model shows good performance.\n- The experiments are extensive."
            },
            "weaknesses": {
                "value": "- Some details are not presented clearly. 1) How many descriptions are generated at the coarser level? Are all descriptions used for training the final model at the finer level? 2) Which pre-trained language model is used for processing captions? 3) The details about DIV and FLT are not introduced.\n- A closely related work, CLIP-ViP[R1], is not included in related works and compared. It also adopts a caption model to generate descriptions for video clips. It is corresponding to the coarser level in this paper.\n- The performance of InterVid-10M-DIV, InterVid-10M-FLT is weird. It shows much better performance in zero-shot action recognition in Table 2, but poor in the fine-tuned setting of Table 3. The authors give reasons for false negatives, but what is the training batch size, given the much larger training data, I think the possibility of the same video clip appearing in the same batch is low.\n\n[R1]: CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment, ICLR 2023."
            },
            "questions": {
                "value": "Refering to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4917/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4917/Reviewer_gWXX",
                    "ICLR.cc/2024/Conference/Submission4917/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4917/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823157889,
        "cdate": 1698823157889,
        "tmdate": 1700530836872,
        "mdate": 1700530836872,
        "license": "CC BY 4.0",
        "version": 2
    }
]