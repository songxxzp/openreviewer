[
    {
        "id": "mvSDm2vrXN",
        "forum": "9DXXMXnIGm",
        "replyto": "9DXXMXnIGm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4483/Reviewer_VZ9L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4483/Reviewer_VZ9L"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on a very interesting question of guiding the diffusion modes by leveraging off-the-shelf classifiers in a training-free fashion. Specifically, the authors first provide a simple analysis to demonstrate that the off-the-shelf classification can achieve better accuracy than the fine-tuned classifier when the noise level is high, which may have been ignored in the previous works. Then, the authors turn to exploit the pre-trained classifier for guiding diffusion generation with comprehensive consideration of the detailed settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tIn my opinion, the major contribution of this paper lies in Section 4.1, which provides an empirical analysis by evaluating the calibration of both fine-tuned and off-the-shelf classifiers.  The results reveal that fine-tuned classifiers are less calibrated than off-the-shelf ones when the noise level is high, which indicates that Off-the-shelf classifiers\u2019 potential is far from realized.\n2.\tTo optimize the design of the proposed methods, the authors have considered multiple aspects, including the classifier inputs, smoothing guidance, guidance direction, and guidance direction, and designed the corresponding strategies to improve the overall performance."
            },
            "weaknesses": {
                "value": "1.\t The paper focuses on the classic class-conditional diffusion generation, which is a simple case in the text-to-image generation. How about the performance of the proposed methods for the general case with text prompts as conditions? In particular, in section 5.4, the authors also provide a simple analysis of this case with the CLIP model. However, the results are not good.\n2.\tHuman-level metrics should be involved for clearer comparisons. It is well-known that some quantitative metrics like FID, may be problematic in some cases. In particular, the FID metric, used in ablation analysis in section 4 and experiments in section 5, cannot measure conditional adherence which is important for conditional generation. CLIP score should also be considered to evaluate the performance.\n3.\tIt is weird that the final images are merely the same as each other in Figure 4 with different settings of logit temperature, while the FID score varies with different settings in Table 4."
            },
            "questions": {
                "value": "My major commons lie in the experimental evaluation. Please provide more experimental analysis or discussions to validate the effectiveness and robustness of the proposed methods.\n\nHow about the performance in terms of CLIP score and Human-level metrics?\n\nHow about the performance for the conditional generation with the general text prompts?\n\nPlease provide more discussions about Table 4 and Figure 4."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4483/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698569260951,
        "cdate": 1698569260951,
        "tmdate": 1699636424402,
        "mdate": 1699636424402,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1YcysWy8tS",
        "forum": "9DXXMXnIGm",
        "replyto": "9DXXMXnIGm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4483/Reviewer_wqgP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4483/Reviewer_wqgP"
        ],
        "content": {
            "summary": {
                "value": "The manuscript analyzes the limitations of current classifier guidance in terms of flexibility, calibration error as well as smoothness. Based on the analysis, the authors propose to use an off-the-shelf classifier instead of a noise-aware classifier for guidance which shows some encouraging results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The problem is very interesting, the method is simple yet effective. The current approach requires finetuning classifiers as well as training diffusion models to be a joint model. This work removes this limitation and shows that the off-the-shelf classifier has the potential to outperform noise-aware models."
            },
            "weaknesses": {
                "value": "Although the work is very interesting, there are still some questions:\n\n1. The comparison is not so comprehensive. Since the main baseline should be the noise-aware classifier, the work provides little comparison in Table 5. More metrics such as IS, sFID, Precision and Recall should be included. \n2. In Table 5, results for IMN64x64 as well as IMN256x256 for DDPM from Dhariwal and Nichol (2021) is missing. Whether or not the method can perform well on other resolutions.\n3. Does off-the-shelf classifier guidance provide conditional information for the unconditional diffusion model? In my belief, the main objective of the guidance should be providing conditional information for the unconditional diffusion model rather than just improving the generated image quality. In Table 5, the Diffusion model on ImageNet128x128 is conditionally trained. Thus, the conditional information from an off-the-shelf classifier is not important. Results for combining off-the-shelf classifiers with unconditional models should be included.\n4. How off-the-shelf models Resnet can be used with ImageNet64x64 or ImageNet128x128 although these models are trained on ImageNet224x224? Besides, the generated images are clip to be in range of [-1; 1], how does it fit with model trained with input images normalized by ImageNet datasets?\n5. There is a fatal gap in the modeling part in Algorithm 1, in the sampling equation $\\hat{x}_{t-1} \\sim \\mathcal{N}(\\mu + \\gamma _t g, \\sigma _t)$. In the original paper by Dhariwal and Nichol (2021), the sampling resulted from the $\\log(p _{ \\theta }(x_t|x _{t+1}) p _{\\phi}(y|x_t))$. However, given the structure of Algorithm1, this equation is no longer valid since the gradient is taken regarding $x _0 (t)$ instead of $x_t$. In order to apply the same sampling process, even when $x_0(t)$ is forward through the classifier, the gradient should be taken regarding $x_t$. \n6. I guess from the point (5), this is the main reason why the method can not be applied to DDIM?\n7. Ablation study is missing, it is quite vague to understand which proposed scheme is the main course for the improvement. From my understanding, there are three main differences from normal classifier guidance which are:\n* Gradients via $x_0(t)$ instead of $x_t$\n* Guidance schedule\n* $\\tau _2$ temperature\n\nHowever, discussion on the contribution of each of them is missing\n\n8. Given three contributions as in (7), does the performance of the noise-aware classifier guidance also get improvements?\n9. Besides Resnet, how are other architectures such as DenseNet, Transformers? Do they also work with this scheme?\n10. CLIP guidance should be compared against the noise-aware CLIP guidance\n11. The connection from 4.1, 4.2, 4.3 and 4.4, 4.5 as well as the design of the algorithm lacks some connections.\n\nIt seems that the paper is written in hurry so that the format of the paper is not really good as well as some errors in equations:\n1. Equation (2) should be $\\log exp(\\tau f _{y(x)})$? Check format of the equation (2) also.\n2. Equation (3) should be $\\log exp(\\tau _1 f _{y(x)})$?"
            },
            "questions": {
                "value": "See the weaknesses. The work is interesting and potential, yet there are a number of concerns as well as writting. Will raise the score if all concerns are solved."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N.A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4483/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4483/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_wqgP"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4483/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821190175,
        "cdate": 1698821190175,
        "tmdate": 1700974870821,
        "mdate": 1700974870821,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hh3JHqNI70",
        "forum": "9DXXMXnIGm",
        "replyto": "9DXXMXnIGm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4483/Reviewer_rssc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4483/Reviewer_rssc"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel method to improve guidance in conditional diffusion generation without additional training by utilizing off-the-shelf classifiers. The authors introduce pre-conditioning techniques that enhance the performance of existing state-of-the-art diffusion models by up to 20% on ImageNet. Their approach is efficient, scalable, and leverages the widespread availability of pretrained classifiers, promising advancements especially in text-to-image generation tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper adeptly integrates elements from both classifier-free and classifier-based diffusion approaches. Additionally, it employs off-the-shelf classifiers to enhance performance while maintaining efficiency. The experimental results presented confirm the effectiveness of these methods."
            },
            "weaknesses": {
                "value": "1. Sec 4.2 \"PREDICTED DENOISED SAMPLES\" seems trivial. This is already explored by the CVPR paper \" Universal guidance for diffusion models\". The authors should not make this part a separate subsection if this is not the author's original work.\n\n2. Sec 4.3 \"SMOOTH CLASSIFIER\" seems trivial. I think there are already many works which use Softplus as activation function and explore its difference/advantage with ReLU. If I understand it correctly, the only contribution here is to replace ReLU with Softplus. The novelty point is not enough for an ICLR paper.\n\n3. Sec 4.4 \"JOINT VS CONDITIONAL DIRECTION\", the author mention \"we reduce the value of marginal temperature\", which seems kind of manual tuning parameters. Is there some validation metric the authors use to determine the optimal temperature?\n\n4. Sec 4.5 \"GUIDANCE SCHEDULE\" seems trivial. If I understand it correctly, the only contribution here is introducing a sin factor. This seems more like a trick instead of some research contribution. For it to be a research contribution, the author should first discuss what kind of guidance is good and why the author choose the sin factor here. \n\n5. Sec 5.3 \"OFF-THE-SHELF GUIDANCE FOR DIT\", the authors propose to incorporate classifier guidance g into classifier-free sampling. The idea is straightforward but the same question the authors introduce a parameter gamma_t here. How do the authors choose a proper gamma_t for a specific case?"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4483/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_rssc",
                    "ICLR.cc/2024/Conference/Submission4483/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4483/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699017820646,
        "cdate": 1699017820646,
        "tmdate": 1700604531358,
        "mdate": 1700604531358,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OqzW268P83",
        "forum": "9DXXMXnIGm",
        "replyto": "9DXXMXnIGm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4483/Reviewer_r6Zr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4483/Reviewer_r6Zr"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors analyze off-the-shelf classifier guidance diffusion through multiple perspectives including calibration, smoothness, score decomposition and guidance scale scheduling. They use calibration error as a new metric to assess the performance of classifier guidance diffusion and propose several techniques to improve this task. They conduct experiments on multiple diffusion models with pre-trained ResNet classifiers and show consistent improvement with their proposed method compared to baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper provides several interesting techniques to improve off-the-shelf classifier guidance diffusion. These techniques are practical, training-free and fairly easy to be incorporated into the current diffusion frameworks. Their analysis also provides interesting insight into what happens in the process of classifier guidance diffusion from multiple different perspectives."
            },
            "weaknesses": {
                "value": "1. I don\u2019t really see the direct connection between Proposition 4.1 and Eq. 1. There seems to be a gap between ECE and $\\|p_n-p\\|$. Also to make the paper self contained, \u201cbins\u201d, \u201cacc\u201d, \u201cconf\u201d should be defined before mentioning.\n\n2. It is unclear to me how Proposition 1 and ECE inform the design choices of the method. For Section 4.2, the same conclusion can be drawn with only FID. For Section 4.3, Proposition 1 only says \u201cwith smoothness $k>1$\u201d, but it doesn\u2019t claim better calibration/score prediction with higher smoothness. Section 4.4 and 4.5 seems to be completely irrelevant to ECE.\n\n3. Relating to Weakness 2, I think the story line of this paper is a little bit scattered. There are many components in the story and it is difficult to tell which one is the main contribution of this paper. The components can also be better connected.\n\n4. There are four components to the proposed method: (1) use $ \\nabla_{\\hat{x_0}(t)} \\log p(y|\\hat{x_0}(t))$ instead of $\\nabla_{\\hat{x_t}} \\log p(y|\\hat{x_t})$ (2) use Softplus activation to increase the smoothness (3) use a second temperature to adjust the \u201cratio\u201d of joint and marginal guidance (4) sin factor guidance schedule. Since there is no ablation study that involves all four components conducted, it is very difficult to tell which one is actually effective. It would be great if the authors can include experiments that gradually exclude these components one-by-one to see which one is the most effective.\n\nMinor suggestions:\n\n1. According to the official style files provided by ICLR call for papers, the appendix should be included in the same PDF file as the main text and the bibliography.\n\n2. Eq. 1 $k$ notation conflicts with the smoothness $k$ in Proposition 4.1."
            },
            "questions": {
                "value": "1. The formula for $\\text{ECE}_t$ is with respect to $\\hat{x_t}$ but in Section 4.2 the authors talked about using $\\hat{x_0}(t)$ will provide better calibration, so which sample did the authors use when providing the ECE results for the rest of the experiments?\n\n2. It is unclear to me how did the authors incorporate the Softplus activation function into the pre-trained classifiers, did they just replace all the activation functions in the pre-trained models? Or is there anything else that they did?\n\n3. How did the authors calculate the marginal likelihood for CLIP guidance generation?\n\n4. What is the recurrent step in Table 1? Is it the same as the \u201cbackward guidance\u201d in \u201cUniversal Guidance for Diffusion Model\u201d paper? And why is the inference time not changed significantly with the calibrated method given there is extra marginalization required for all classes?\nWhat type of GPU did the authors use in their experiments?\n\nI am happy to raise my score if the authors address my concerns and questions in the rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4483/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_r6Zr",
                    "ICLR.cc/2024/Conference/Submission4483/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4483/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699138863904,
        "cdate": 1699138863904,
        "tmdate": 1700624049710,
        "mdate": 1700624049710,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5UgaWXTT30",
        "forum": "9DXXMXnIGm",
        "replyto": "9DXXMXnIGm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4483/Reviewer_6jri"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4483/Reviewer_6jri"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates improving sample quality in conditional diffusion generation by leveraging off-the-shelf classifiers in a training-free context. The authors propose pre-conditioning techniques based on classifier calibration, significantly enhancing diffusion models' performance with minimal computational overhead, verified through experiments on ImageNet. They introduce a novel metric, integral calibration error (ECE), to evaluate the effectiveness of classifier guidance and find that off-the-shelf classifiers outperform trained classifiers under high noise. To combat the diminishing influence of classifier guidance in later diffusion stages, a new weighing strategy is suggested, yielding better sample quality. The paper highlights the potential of their methods in text-to-image generation and points out the limitations of current guidance enhancement methods in terms of sampling efficiency."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper convincingly establishes the research context. In particular, it is effective in demonstrating the robustness differences across time step intervals between off-the-shelf classifiers and fine-tuned classifiers using the ECE metric.\n- The division of the design space for diffusion guidance appears appropriate, and the empirical process of selecting among the various options seems justified.\n- The authors' exploration reveals that guidance using off-the-shelf models exhibits performance comparable to or surpassing previous methods, which required significant computational costs.\n- The experiments with guidance through CLIP demonstrate the potential for extending guidance models beyond classifiers, indicating scalability in the approach.\n- The theoretical explanations provided lend solid persuasive strength to the authors' design choices."
            },
            "weaknesses": {
                "value": "- While I rate this study highly in general, it falls short in comparing with previous research utilizing off-the-shelf models.\n- Specifically, the omission of closely related prior works, [1, 2] , is significant. PPAP[2] explores guidance using a wide range of modalities of off-the-shelf models in an efficient tuning and plug-and-play manner without significant computational costs, which is directly relevant to this paper's discussion. Observations such as the varying contributions of tuning guidance models across different time steps could also reinforce the evidence between the two studies.\n- Overall, I highly appreciate the paper's contribution to the completeness of the discussion on diffusion models' guidance. However, to properly highlight this paper's direct contributions, a comparison with prior studies attempting off-the-shelf guidance is crucial. Although the authors conducted experiments comparing various design choices of off-the-shelf guidance, it is not clearly presented how these relate to previous research, and there is no direct comparison of the final FID scores with prior methodologies. The lack of such comparisons has inclined my evaluation towards rejection.\n- In the CLIP guidance experiments, it seems that the authors did not apply the same level of guidance as they did with off-the-shelf classifiers. The CLIP guidance experiment does not appear to reflect the authors' contributions with a new methodology for off-the-shelf guidance.\n\n[1]: Graikos, Alexandros, et al. \"Diffusion models as plug-and-play priors.\" Advances in Neural Information Processing Systems 35 (2022): 14715-14728.\n\n[2]: Go, Hyojun, et al. \"Towards practical plug-and-play diffusion models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "questions": {
                "value": "- Based on the various considerations and discoveries made by the authors, I am curious whether it is possible to extend off-the-shelf guidance beyond class conditions to various modalities of conditional generation. It seemed that the CLIP experiment was intended to demonstrate such a possibility; however, as mentioned in the weaknesses, it did not feel like an experiment based on a new methodology reflecting the authors' considerations and findings."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4483/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_6jri",
                    "ICLR.cc/2024/Conference/Submission4483/Senior_Area_Chairs"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4483/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699418808145,
        "cdate": 1699418808145,
        "tmdate": 1700546683859,
        "mdate": 1700546683859,
        "license": "CC BY 4.0",
        "version": 2
    }
]