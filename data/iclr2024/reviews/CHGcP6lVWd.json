[
    {
        "id": "DExkMYtpYO",
        "forum": "CHGcP6lVWd",
        "replyto": "CHGcP6lVWd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8869/Reviewer_2CiW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8869/Reviewer_2CiW"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a metric called Meta-Distribution Energy (MDE) for automated model evaluation (assessing model performance on unlabeled datasets). The method involves extracting energy scores from the model's output and extending them into a statistical representation at the dataset level. The paper demonstrates a strong linear relationship between MDE and classification accuracy on datasets with differing extents of distribution shifts. This allows for predicting model performance based on the MDE of an unlabeled test set, and the authors provide theoretical proof of this concept. In the experimental section, the paper presents the outstanding performance of MDE across different backbones, datasets, and modalities, even in scenarios with noise and class imbalance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The task of AutoEval is crucial when deploying neural network models in real-world scenarios. It can help avoid the issue of not knowing the actual performance in practical applications and is also beneficial for model selection.\"\n2) The method MDE in the paper exhibits a relatively high level of novelty by introducing the statistical distribution of energy scores into the field of AutoEval for the first time. Furthermore, the paper extends the approach to improving energy scores, which can be applied to a wider range of tasks. Additionally, the authors provide detailed theoretical proof that strongly illustrates the effectiveness of the method.\n3) The paper conducts extensive experiments on different backbones, different datasets, and different modalities. The experimental results indicate a significant performance improvement compared to other methods in the field.\n4) The paper is of high writing quality, with a smooth and easy-to-grasp presentation of the key points."
            },
            "weaknesses": {
                "value": "1) It seems that the paper does not provide a more detailed explanation of the parameter T in MDE. Is T just an ordinary parameter, or does it have a more intuitive meaning? In Figure 3(a), only the trend of T from 1 to 10 is displayed. What is the impact of a wider range of T values on the results?\n2) In the paper, the authors categorized relevant methods into 'training-free' and 'training-must.' How much progress does MDE, as a 'training-free' method, make compared to 'training-must' methods in terms of evaluation time and memory usage? It would be ideal to conduct an experiment to illustrate this."
            },
            "questions": {
                "value": "Please try to address the questions I raised in the 'Weaknesses' chapter."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8869/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8869/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8869/Reviewer_2CiW"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8869/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698584644057,
        "cdate": 1698584644057,
        "tmdate": 1699637116522,
        "mdate": 1699637116522,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QEISwVnNwO",
        "forum": "CHGcP6lVWd",
        "replyto": "CHGcP6lVWd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8869/Reviewer_Dsfd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8869/Reviewer_Dsfd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel measure called Meta-Distribution Energy (MDE) for automated model evaluation (AutoEval), which is a method for assessing the performance of machine learning models on unlabeled test data. The core idea of MDE is to convert the energy information of the model's output into a probability distribution statistic, which enables a smoother data representation. The paper also provides theoretical analysis connecting MDE with classification loss, proving its effectiveness. Experimental results demonstrate MDE's superior performance across various modalities, datasets, and model architectures, especially in noisy and class-imbalanced scenarios."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The research topic of this paper -- AutoEval is valuable in the real world deployment of DNNs. I believe this area may shed some novel light on the unsupervised evaluation community.\n* This paper establishes a connection between MDE and classification loss through a mathematical theorem, providing theoretical justification for the effectiveness of the proposed method.\n* MDE demonstrates strong performance across a variety of modalities, datasets, and model architectures, and these detailed analyzes make it as a versatile solution for automated model evaluation.\n* MDE remains effective even in challenging scenarios such as strong noise and class imbalance, showcasing its robustness in practical applications."
            },
            "weaknesses": {
                "value": "* The paper does not provide source code, making re-implementation challenging. Please provide the code later to dispel this concern.\n* Interpretability can be further explored. While MDE provides strong correlation with classification accuracy, further research could focus on enhancing the interpretability of the method, making it easier for users to understand and trust the results.\n* Different methods seems to have different (wall-clock?) time required to come up with such evaluation -- perhaps some notes on that would be helpful as well.\n* Grammar should be thoroughly checked. For example, in the first paragraph, \"the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning.\" should be changed to \"xxxx, and then offers...\". Similarly, in the second paragraph, \"the correct classified data are given low energies, and vice versa\" should be changed to \"the correctly classified...\""
            },
            "questions": {
                "value": "Please refer to the weakness section mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8869/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698621760990,
        "cdate": 1698621760990,
        "tmdate": 1699637116402,
        "mdate": 1699637116402,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0qWEHaZcal",
        "forum": "CHGcP6lVWd",
        "replyto": "CHGcP6lVWd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8869/Reviewer_SNMZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8869/Reviewer_SNMZ"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on AutoEval, where the goal is to estimate classifier performance on unlabeled test sets. To this end, this work proposes to use energy score (MDE) as the statistic of each test set for inferring the corresponding classification accuracy. The experimental results on several benchmarks such as CIFAR-10 and CIFAR-100 show the proposed can achieve reasonable accuracy estimation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "+ Algorithm 1 clearly shows the proposed method. The whole pipeline is well-introduced\n+ The experiment includes MNLI, a natural language inference task."
            },
            "weaknesses": {
                "value": "- *1. Overstated claims*: The paper asserts that \"the AutoEval frameworks still suffer from an overconfidence issue\" without providing clear, empirical examples. Additionally, the statement regarding \"substantial storage\" lacks a comparison with existing methods such as DoC and ATC, leaving the reader unconvinced of any real advantage of the proposed method. Furthermore, the claim that the proposed MDE method is superior in terms of \"computational cost\" is not substantiated, especially considering that unlike ATC, MDE requires the training of a linear regression model. To move forward, it would be essential to clarify these issues with specific data and comparative analysis.\n\n- *2. Limited contribution*: The novelty of your work is questioned due to the similarity with existing methods in the field. The use of Energy score as a replacement for Softmax score, without significant insights or enhancements, is seen as insufficient for constituting a substantial contribution. Prior works [Detecting Errors and Estimating Accuracy on Unlabeled Data with Self-training Ensembles] have already discussed the connection between OOD detection and AutoEval, which the current submission does not appear to extend beyond.\n\n- *3. Unconvincing theoretical analysis*: The theoretical grounding provided in Section 3.3 is deemed unclear and unconvincing. The methodology for ascertaining model accuracy needs to account for scenarios where an image is correctly classified with a low Softmax score. A more robust theoretical framework is necessary to support the claims made.\n\n- *4. Experimental results are not solid*: The absence of results for ImageNet-1K in Table 1, along with other test datasets like ImageNet-S/A/V2, raises concerns about the comprehensiveness of the experimental evaluation. Moreover, the omission of recent relevant works, such as \"Characterizing Out-of-Distribution Error via Optimal Transport,\" and the lack of comparison with methods like Nuclear Norm on ImageNet setup, call into question the thoroughness of the analysis.\n\n- *5. Limited Dataset Diversity*: The paper does not report on more natural shifts, which are included in benchmarks provided by methods like ATC. While BREEDs are mentioned in the supplementary material, results are not presented, and expectations for insights on datasets like i-WILDS are not met."
            },
            "questions": {
                "value": "- Please clarify the claims on overconfidence issue, substantial storage, and computation cost \n- Please report the results on the ImageNet setup and the estimates of ImageNet-A/V2/S\n- Please clearly compare with existing works such as ATC, DoC, and Nuclear Norm on ImageNet. Moreover, [Characterizing Out-of-Distribution Error via Optimal Transport] should be included for comparison.\n- Other related works: Estimating and explaining model performance when both covariates and labels shift. In NeurIPS 2022"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8869/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698918336672,
        "cdate": 1698918336672,
        "tmdate": 1699637116290,
        "mdate": 1699637116290,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IBwBJCdVM5",
        "forum": "CHGcP6lVWd",
        "replyto": "CHGcP6lVWd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8869/Reviewer_zk65"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8869/Reviewer_zk65"
        ],
        "content": {
            "summary": {
                "value": "- Authors propose a method for a problem called 'AutoEval'. The problem is described as evaluating the effectiveness of a model on data without ground truth labels.\n- The proposed method is very simple. It is based on energy models.\n- Authors perform experiments on datasets like CIFAR-10/100, TinyImagenet to validate their method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The authors have performed a wide range of analysis experiments."
            },
            "weaknesses": {
                "value": "1. The problem addressed\n-  Authors motivate some new problem called Automated model evaluation. The definition is not very clear as they define it in many different ways.\n- I am not sure about the relevance of the problem. Or to phrase it better - I don't know much about this problem.\n\u00a0\n2. Proposed method\n- The proposed method just use energy based model equation to create a simple function of the energy expressed in terms of logits. \n- One concern here is that it might be very similar to some method in Uncertanity estimation.\n- Can authors mathematically compare this method to some common methods in uncertainity estimation.\n\n3. Experimental validation\n- While the introduction had motivated the problem in a broad setting. Authors discussed OOD to motivate the problem. But I could not find the experiments on OOD.\n- The claims should match the experiments on which the method is validated. Maybe the authors can provide experiments on OOD."
            },
            "questions": {
                "value": "Please see the weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8869/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699334821403,
        "cdate": 1699334821403,
        "tmdate": 1699637116148,
        "mdate": 1699637116148,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rgqlbug1KA",
        "forum": "CHGcP6lVWd",
        "replyto": "CHGcP6lVWd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8869/Reviewer_xmk2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8869/Reviewer_xmk2"
        ],
        "content": {
            "summary": {
                "value": "This paper intends to solve the auto evaluation problem of a well-trained classification model on a test dataset with domain shift and without labels. The proposed method is based on an energy-based framework to estimate the Meta-Distribution Energy, which is used to train a regression model on synthesized dataset for prediction of classification accuracy on unlabelled test data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper introduces an energy-based automatic evaluation framework designed to enhance efficiency and mitigate overconfidence in existing methodologies. The proposed approach indicates better prediction on unseen test data over the other measurement methods on dataset in different modalities and with different classification models."
            },
            "weaknesses": {
                "value": "1. It is recommended to use a different symbol for the normalization term E(x) in Eq(2) to avoid confusion, like Z(x).\n2. In Eq(5) and Eq(6), the font of matchcal is usually used for a single letter.\n3. I cannot see a clear relationship between the proposed method and the energy-based model except that \"energy\" specifies the logits from the classification model. \n4. The performance of the proposed framework relies on the regression between MDE on the synthesized dataset and its accuracy. However, the type of domain shift is more complicated in the real world in most cases, thus very unpredictable. \n5. The experiment on adjusting temperature T should be conducted on a broader range, like from 0.01 to 100. The change from 1 to 10 is relatively small.\n6. A real dataset for evaluation needs to be included. The operation of shear, equalization, and color temperature adjustment is easy to synthesize, while the domain shift could come from more complex sources like [1][2].\n7. Some missing EBM references can be considered to be included into this paragraph to make it more complete for the readers. For example, [3] is the first EBM using CNN for energy function and trains it with Langevin dynamics. [3] is also the first one to point out that EBM and a classifier can be derived from each other. The EBM applications not only include video as you have mentioned in your paper, but also include point cloud [4], voxel [5], trajectory [6] and molecules [7].\n\n[1] Scanner invariant multiple sclerosis lesion segmentation from MRI. 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI). IEEE, 2020.\n\n[2] Transfer learning for domain adaptation in MRI: Application in brain lesion segmentation. Medical Image Computing and Computer Assisted Intervention\u2212 MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part III 20. Springer International Publishing, 2017.\n\n[3] A theory of generative convnet. ICML 2016.\n\n[4] Generative PointNet: Deep Energy-Based Learning on Unordered Point Sets for 3D Generation, Reconstruction and Classification.\n\n[5] Learning Descriptor Networks for 3D Shape Synthesis and Analysis. CVPR 2018.\n\n[6] Energy-Based Continuous Inverse Optimal Control.\n\n[7] Molecular Graph Generation with Energy-Based Models."
            },
            "questions": {
                "value": "1. For Eq(5), is MDE defined on specific data $x_n$, where MDE(x; f) should be MDE($x_n$, f)? Or does it miss an expectation term over the dataset?\n2. How is the synthesized test data generated when training the regression model for accuracy prediction?\n3. If the proposed method indicates a significant drop in a new dataset, is there any way to correct this bias based on the proposed MDE?\n4. How is the correlation evaluated in Table 1 on a specific dataset? \n5. How to determine the best hyper-parameter of T on a new dataset? Is the best parameter related to the dataset or the classification model?\n6. Will the selection of different regression models affect the prediction accuracy?\n7. Is there any comparison on the evaluation of domain-shift data with real labels?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8869/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699458557686,
        "cdate": 1699458557686,
        "tmdate": 1699637116032,
        "mdate": 1699637116032,
        "license": "CC BY 4.0",
        "version": 2
    }
]