[
    {
        "id": "7O5j9tpXMm",
        "forum": "OeH6Fdhv7q",
        "replyto": "OeH6Fdhv7q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1359/Reviewer_qE61"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1359/Reviewer_qE61"
        ],
        "content": {
            "summary": {
                "value": "The paper (TapMo) tackles text-guided character motion generation in a skeleton-free manner. It can be thought of as the intersection of  MDM (a human-specific motion generation method) and SfPT (a category-agnostic per-frame pose transfer method). Combining the two enables new capabilities -- text-guided motion generation for generic shapes.\n\nMethod-wise, instead of simply combining the two prior works, TapMo made innovations on (1) modeling root movement by an additional root handle (2) taking shape into account when generating handle movements (3) introducing a delta term to account for motion that cannot be explained by a fixed number of handles.\n\nThe video nicely demonstrates the capability of motion generation beyond human characters."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Significance**\n- The paper is a nice first step toward motion generation for generic characters. \n\n**Quality**\n- The method is sound and relatively simple.\n- The motion generation results are impressive, especially for the walking hand and animals. \n\n**Presentation**\n- The visual illustrations are well done, and the paper is easy to follow. I enjoyed reading them."
            },
            "weaknesses": {
                "value": "**Data**\n- The metrics are reported on a human dataset (HumanML3D), which does not show off the cross-category generation ability of the proposed method. Evaluating the method on a dataset with characters beyond humans would be beneficial, such as deforming things 4D [A]. \n\n**Method**\n- Driving signal. The driving signals seem to be limited to text in the current form. However, there are motions that cannot be described purely by natural language, such as body gestures, facial expressions, hair movements, etc. This is not necessarily a weakness, as the paper already made the setup clear.\n- Representation. To generalize to fine-grained motion, such as cloth deformation and hair, the handle-based deformation with a limited number of handles (K=30) seems not enough. \n\n\n[A] Li, Yang, et al. \"4dcomplete: Non-rigid motion estimation beyond the observable surface.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021."
            },
            "questions": {
                "value": "1. The model is trained on text-motion pairs of human motion but seems to be able to generalize to other categories like quadruped animals. This is interesting. Is the text description transferable over bipeds and quadrupeds? Has the model seems quadruped motion before? \n\n2. The methods use both diffusion-based reconstruction loss and GAN loss. In which case is the GAN loss necessary/complementary to diffusion loss? What happens if GAN loss is removed?\n\n3. In Table 1, the TapMo variants have a much higher Diversity metric than top rule methods. Is there an explanation? Additionally, the Multimodal Dist and Multimodality results are not explained in the paper. Explanations on why certain method performs better/worse would be useful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1359/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698520329813,
        "cdate": 1698520329813,
        "tmdate": 1699636063430,
        "mdate": 1699636063430,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Doe5UOxFWc",
        "forum": "OeH6Fdhv7q",
        "replyto": "OeH6Fdhv7q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1359/Reviewer_e3Gp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1359/Reviewer_e3Gp"
        ],
        "content": {
            "summary": {
                "value": "This work focuses on an interesting research topic - synthesizing motions for skeleton-free 3D characters, with two main modules: 1. Mesh Handle Predictor, and 2. Shape-aware Motion Diffusion Module.  In addition, this work utilizes the shape deformation-aware features as a condition to guide the motion generation for specific character models. The proposed method could show impressive generated animations for both seen and unseen characters."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. it is good to study generating shape-aware motions, especially for non-humanoid 3D characters.\n2. The proposed method seems to be reasonable and might be promising to generate motions for unseen characters."
            },
            "weaknesses": {
                "value": "1. The proposed mesh handle predictor is simple and straightforward, but it is not clear how the proposed method resolves different characters that have different topologies with different semantics.  Currently, the manuscript mentions that \"each handle is dynamically assigned to vertices with the same semantics across different meshes\", but it is not clear how the method will select those handles.\nAlso, it is unclear how the method will choose the number of handles since different topologies tend to have different numbers of handles. It is unclear how the proposed method could achieve training with different numbers of handles.\n\n2. The proposed Shape-aware Motion Diffusion seems to be simple modifications for existing methods (e.g., MDM), but the current presentation makes it overcomplicated, and difficult for readers to get the key designs that could highly improve the motion generation quality. I am not sure if considering the character shapes is the major factor that improves the quality, and the others could make further improvements.\n\n3. The experiments seem to be insufficient.  HumanML3D dataset is the only benchmark that is used to report quantitative comparisons.  However, recent methods, such as MotionDiffuse[1], ReMoDiffuse[2], and T2M-GPT[3] have not been discussed.\n\n[1] Zhang, M., Cai, Z., Pan, L., Hong, F., Guo, X., Yang, L., & Liu, Z. (2022). Motiondiffuse: Text-driven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001.\n\n[2] Zhang, M., Guo, X., Pan, L., Cai, Z., Hong, F., Li, H., ... & Liu, Z. (2023). ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model. ICCV 2023\n\n[3] Zhang, J., Zhang, Y., Cun, X., Huang, S., Zhang, Y., Zhao, H., ... & Shen, X. T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations. CVPR 2023"
            },
            "questions": {
                "value": "The authors could answer my questions asked in the weakness section first.\nBesides, I would like to suggest the authors could improve its readability and also highlight the key design.\nFor the experiments, I would like to see more generated motions, especially for non-humanoid characters and unseen characters."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1359/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698647308546,
        "cdate": 1698647308546,
        "tmdate": 1699636063360,
        "mdate": 1699636063360,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Azu24TAzzU",
        "forum": "OeH6Fdhv7q",
        "replyto": "OeH6Fdhv7q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1359/Reviewer_JBvr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1359/Reviewer_JBvr"
        ],
        "content": {
            "summary": {
                "value": "TapMo is a text driven motion synthesis framework for skeleton-free 3D characters. Addressing limitations of relying on pre-rigged character models, TapMo introduces the Mesh Handle Predictor and the Shape-aware Motion Diffusion. These components enable the framework to generate motions of skeleton-free characters using text descriptions. Specifically, Mesh Handle Predictor predict the skinning weights and can clusters mesh vertices into adaptive handles. Then the Shape-aware Motion Diffusion takes mesh deformation feature and output handles' motion. Trained in a weakly-supervised manner, TapMo demonstrates its performance in generating animations for novel non-human characters."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The research addresses an interesting and promising problem, as far as I know it is the first attempt to enable text-driven motion synthesis for skeleton-free characters.\n\nComprehensive experiments are conducted, yielding impressive results across diverse shapes. Supplementary videos and a user study further validate the naturalness of the generated results.\n\nThe combination of diffusion-based motion synthesis and skeleton-free mesh deformation is interesting and novel."
            },
            "weaknesses": {
                "value": "Some details are not clearly explained, such as the mesh deformation feature, what exactly is f_ and how it's obtained, and its dimensions, which are not reflected in the main text. From the appendix, it seems to be a 512-dimensional vector. Further explanation from the authors is desired. And how does mesh-specific adaptation affect the vertices, it is not included in the equations. How is the Discriminator implemented? Are the two modules trained jointly or separately?\n\nWhat are the visualization and qualitative results for Handle-FID? Specifically, what are the two SMPL human models and what do the results look like? On the other hand, are two models enough? Why don't authors try MGN (\"Multi-garment net: Learning to dress 3d people from images\") which includes clothed human models and can use SMPL pose parameter to drive.\n\nI recommend the authors discuss the following skeleton-free papers: \"Zero-shot Pose Transfer for Unrigged Stylized 3D Characters\" and \"HMC: Hierarchical Mesh Coarsening for Skeleton-free Motion Retargeting\". The self-supervised shape understanding method in the former could potentially strengthen handle prediction including mesh deformation feature extraction.\n\nThe authors could try using the PMD metric in SfPT and the previous works, which allows for direct comparison with ground truth vertices and can judge both motion and mesh quality, this is suitable for SMPL-based models.\n\nCan the authors provide qualitative results before and after Motion Adaptation Fine-tuning? The post-process usually results in big differences in naturalness."
            },
            "questions": {
                "value": "Compared to Eq1 in SfPT, if we ignore the global translation and rotation of the root, authors added $\\tau^{l}_k$ and $h_k$, which seems different from SfPT Eq1. Could the authors explain the purpose of this?\n\nFor local translations and local rotations, their first dimension is K. Does this include the root (first) handle? Is k=1 represent root handle in Eq. 3? Or there are K-1 local translations/rotations from k=2 to k=K.\n\nIn Eq.7, defining handle adjacent handles is required, is this something that needs to be done separately for each character? For instance, shapes with large differences as in Figure 6. Is this only necessary during training or during inference? And how many characters need to be specifically defined?\n\nIn the first paragraph of the Method section, does \"skinning weight s of the handle\" refer to the skinning weights of the vertices?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1359/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1359/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1359/Reviewer_JBvr"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1359/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806861405,
        "cdate": 1698806861405,
        "tmdate": 1699636063275,
        "mdate": 1699636063275,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6m2N29nRqF",
        "forum": "OeH6Fdhv7q",
        "replyto": "OeH6Fdhv7q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1359/Reviewer_6xCW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1359/Reviewer_6xCW"
        ],
        "content": {
            "summary": {
                "value": "The main contribution of this paper is a motion diffusion model that can take shape-deformation features as inputs to generate shape-aware motions. This function is desirable in the computer animation, which can save a lot of efforts in the animation production. The proposed pipeline has two components: mesh handle predictors to predict skinning weights and underlying skeletons and shape-aware motion diffusion models that can synthesizes motion with mesh-specific adaptations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.  the generated animations for a variety of 3D characters are impressive. The structure of the 3D meshes are well recognized when associating it with the animations. \n\n2. The application of shape deformation feature in animation is nice."
            },
            "weaknesses": {
                "value": "There are still penetrations between foot and ground in the generated animations, which downgrade the animation quality."
            },
            "questions": {
                "value": "Please clarify how the adversarial loss is trained in the rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1359/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1359/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1359/Reviewer_6xCW"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1359/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698860133507,
        "cdate": 1698860133507,
        "tmdate": 1699636063211,
        "mdate": 1699636063211,
        "license": "CC BY 4.0",
        "version": 2
    }
]