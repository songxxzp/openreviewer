[
    {
        "id": "ey7a4KrgEf",
        "forum": "CgPs04l9TO",
        "replyto": "CgPs04l9TO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2011/Reviewer_fhjo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2011/Reviewer_fhjo"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes that the training instability in the policy network is due to the gradient variance amplication during training"
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea is novel and the problem being dealt with is sufficiently important, namely, during training in reinforcement learning, the model does experience strong instabilities, and good methods to mitigate them would be of great importance"
            },
            "weaknesses": {
                "value": "I am quite confused by the main claim of the work. \n\nThe main claim is that the gradient variance is responsible for the instability and that it amplifies throughout training, but no numerical result in the paper really plots the gradient variance, and, of course, not a single experiment shows that this variance is amplified. It should not be difficult to plot the variance of the gradient at all, and the absence of such evidence makes it impossible for me to recommend acceptance."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2011/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698399081985,
        "cdate": 1698399081985,
        "tmdate": 1699636132563,
        "mdate": 1699636132563,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jRRYSJnrX8",
        "forum": "CgPs04l9TO",
        "replyto": "CgPs04l9TO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2011/Reviewer_xe8u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2011/Reviewer_xe8u"
        ],
        "content": {
            "summary": {
                "value": "The paper examines how noise in gradients affects systems with feedback loops. It argues that SGD error accumulates over longer horizons, leading to Gradient Variance Amplification (GVA), explaining sharp reward oscillations. GVA is shown to be the main cause of these oscillations, surpassing the influence of statistical and architectural factors. Empirical evidence shows that the empirical moving average (MVA) mitigates these oscillations, ensuring stability. Additionally, the paper revisits the theory of stochastic optimization for convex functions, assessing its explanatory power for empirical observations of EMA and various step size schedules."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "a) The paper conducts meticulous experiments revealing that the variance in stochastic gradients is the true source of instability. It introduces exponential moving average as an effective solution to mitigate this issue. Additionally, this phenomenon is also demonstrated for other tasks with feedback loop, i.e., auto regressive processes for language generation. \nb) The paper is well-written and the main message is clearly presented."
            },
            "weaknesses": {
                "value": "a) The main problem of the paper in my opinion is the explanation provided for the benefits of EMA.  Proposition 3.1 says that land scape is very intricate and for every $\\delta$ there is a separation between $J$ and the behaviour cloning loss. However, the *cliff*-type loss framework used to study this does not capture this behaviour, as it is small in a neighbourhood of radius $\\epsilon $ and is very large outside. In my opinion, this framework it too tailor-made to study the benefits of EMA and does not reveal the real reason behind its working mechanism."
            },
            "questions": {
                "value": "It is mentioned in comments after proposition 3.1 that there is a good subset in parameter space that do not experience this worst-case error amplification. Does such good neighbourhood exists around any element in the parameter space or only around elements with specific properties ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2011/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2011/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2011/Reviewer_xe8u"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2011/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698750093710,
        "cdate": 1698750093710,
        "tmdate": 1699636132487,
        "mdate": 1699636132487,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RUQKaUifDw",
        "forum": "CgPs04l9TO",
        "replyto": "CgPs04l9TO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2011/Reviewer_1ahA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2011/Reviewer_1ahA"
        ],
        "content": {
            "summary": {
                "value": "The paper examines a known problem in behavior cloning. This problem occurs during the training of such agents, where the validation criterion (return of an episode under long horizon rollouts) has large variance at all training steps, while the training surrogate loss has small, making it hard to perform model selection.\n\nThe authors perform a theoretical and empirical study of the origins of this problem. They argue that the observed variance is due to training instability, instead of insufficient dataset size, and they name the phenomenon as *Gradient Variance Amplification (GVA)*. They suggest that alternative training algorithms might not have the same issue. For this reason, they propose a very simple fix, which is to track the exponential moving average (EMA) of the parameter iterates of the SGD trained model. They demonstrate the effectiveness of the approach by performing experiments under various environments and by ablating design choices of the proposed EMA intervention.\n\nIn addition, they argue that GVA generally exists whenever agents are expected to operate by conditioning themselves in their past output (or effects of them). This definition aligns with conditional language modelling, and they argue with experiments that language generation quality in LLMs is similarly affected and that EMA can also help mitigate the problem there."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper is well-organized, well-written and well-argumented. It introduces and motivates the problem sufficiently and discussed related work in depth.\n\n * They pose a single clear question at the heart of the problem: Do we just need more data? Or is there in the training algorithm that amplifies variance for the validation criterion? Their ablations convince that the second is the case (section 3.1/figure 2), and they provide with theoretical insight that the return in horizon H can be exponentially large for some suboptimal policy in a smooth neighborhood around the expert, even though for all of those suboptimal policies in the neighborhood the surrogate training loss (of behavioral cloning) is small.\n\n * They introduce various decision choices around implementing EMA as a candidate solution, and they ablate many of them, demonstrating that the problem is effectively mitigated in many RL environments.\n\n * They make connections to the relevance of the problem to other topics in ML, such as autoregressive language modelling."
            },
            "weaknesses": {
                "value": "1. Ablations regarding the design choice of $\\gamma_t$ scheduling are missing. In the paper, a polynomial decay is used and ablated, but other schedules have also been used with EMA (like cosine). It would be nice to understand a bit better why this decay is important and how to design one which is tailored at the problem at hand.\n2. Middle of Figure 4 misses y-axis values, which is important in order to know at which scale are we seeing the zoom at.\n3. In the context of autoregressive language generation: Validation perplexity perhaps does not show the existence of GVA problem here in the most clear way. Ideally, some equivalent to a metric of generation quality of horizon H autoregressive rollouts should have been used instead."
            },
            "questions": {
                "value": "1. In **Proposition 3.1**, I guess that $\\Delta$ refers to two different smooth \u201cerror functions\u201d in each of the two inequalities.\n\n### Typos\n\n2. **Section 4.3**: \u201cHere training loss is convex, but rollout reward is not.\u201d The training loss is indeed convex, but the rollout reward is (still) concave, even if it is discountinuous. Is that right?\n3. Later in **Section 4.3**: \u201cSGD iterates:\u201d $\\theta_{t+1} = \\theta_{t} - \u2026$ instead of $\\theta_{t+1} = \\theta_{t+1} - \u2026$\n4. **Appendix A.1**/**Role of SGD noise**: \u201cIt is now well appreciate*d* that gradient noise facilitates the escape *from* saddle points.\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2011/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2011/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2011/Reviewer_1ahA"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2011/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698890642883,
        "cdate": 1698890642883,
        "tmdate": 1699636132394,
        "mdate": 1699636132394,
        "license": "CC BY 4.0",
        "version": 2
    }
]