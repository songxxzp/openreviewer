[
    {
        "id": "aEh3HQplNv",
        "forum": "oZDJKTlOUe",
        "replyto": "oZDJKTlOUe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission831/Reviewer_ZMPq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission831/Reviewer_ZMPq"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of object hallucination in large vision and language models. They first analyze the patterns and relations between object hallucinations and three concepts. Then, they provide theoretical analysis and explanation for the observations. Based on the analysis, they create a dataset for training a caption revising model to mitigate the hallucination in captions. The experiment results show that the caption after revising has fewer object hallucinations than the original caption generated by LVLMs and outperforms several baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper conducts an early study on the caption object hallucination problem of LVLMs and provides some analysis, observations, and theoretical analysis on object hallucination. The findings are meaningful to future research. \n2. Based on the analysis, the paper proposes a simple and effective method to mitigate caption object hallucination by training a caption revising model.\n3. The paper tests the proposed method on multiple LVLMs and different metrics and compares it with different baselines. The results validate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. The study and the proposed method are limited to caption hallucination problems, and seem not generalized to other settings like VQA.\n2. Both the training data of the hallucination revisor and the testing data are from COCO datasets. Whether the proposed method can be generalized to new datasets with object labels needs to be validated."
            },
            "questions": {
                "value": "The reviewer has some questions on the theoretical analysis part: \n1. In the analysis of **Co-occurrence**, can the authors please explain what is the meaning and why $f\u02c6_{2} = \u27e8\u03d5_{1}(s<i, x), \u03b2\u02c6_{1}\u27e9+\u27e8\u03d5_{2}(s<i, x), \u03b2\u02c6_{2}\u27e9$? (which means that $f\u02c6_{2} = f\u02c6_{1}+\u27e8\u03d5_{2}(s<i, x), \u03b2\u02c6_{2}\u27e9$)\n2. The reviewer understands how the proposed methods related to the three observations on the object hallucinations. However, the reviewer doesn't see a clear connection between the theoretical analysis and the proposed methods. Can the authors explain this point?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission831/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission831/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission831/Reviewer_ZMPq"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission831/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697845587235,
        "cdate": 1697845587235,
        "tmdate": 1699636010714,
        "mdate": 1699636010714,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7lwkiL0C3f",
        "forum": "oZDJKTlOUe",
        "replyto": "oZDJKTlOUe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission831/Reviewer_SdU6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission831/Reviewer_SdU6"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes LURE, a post-hoc approach to reduce object hallucination in large vision-language models (LVLMs). LURE is grounded in a statistical analysis revealing co-occurrence, uncertainty, and object position as key factors causing hallucination. Experiments show LURE outperforms prior methods in reducing hallucination across multiple LVLMs according to general metrics, GPT evaluation, and human evaluation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper focus on an important problem, object hallucinations in large vision-language models. \n2. It spots three key factors of the object hallucinations, the co-occurrence, uncertainty, and object positions.\n3. The paper proposes a new post-hoc method to reduce object hallucinations of LVLMs. Extensive experiments verify the effectiveness of proposed methods."
            },
            "weaknesses": {
                "value": "1.The proposed method helps improve performance on object hallucinations. However, there is a concern that it may harm performance on other metrics like creativity and completeness of captions. It seems to replace detailed words with coarse words as shown in Fig 8.\n2.It is unclear if the removed objects are truly hallucinated or if it wrongly removes some non-hallucinated objects. A new metric to quantify this would be helpful."
            },
            "questions": {
                "value": "1.Do the authors think image captioning metrics are good metrics for LVLMs? The BLEU scores seem low compared to image captioning models. Some important metrics like METEOR, ROUGE, CIDER, and SPICE are missing in Table 10.\n2.Why were co-occurrence, uncertainty, and object positions identified as the three key factors for object hallucinations? Were other factors investigated?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission831/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698745164696,
        "cdate": 1698745164696,
        "tmdate": 1699636010558,
        "mdate": 1699636010558,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "l1EQvqKiKt",
        "forum": "oZDJKTlOUe",
        "replyto": "oZDJKTlOUe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission831/Reviewer_4L8t"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission831/Reviewer_4L8t"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a simple algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs. Their reported results demonstrate that LURE can significantly reduce object hallucination under general object hallucination evaluation metrics."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper compares hallucinatory and non-hallucinatory captions from three critical viewpoints, including co-occurrence, uncertainty, and object position. This viewpoint though simple, is instructive.\n- LURE is a lightweight and effective post-hoc method, which achieves reasonable performance on six open-source LVLMs.\n- LURE consistently improves its performance compared to the original description, which shows its robustness under different backbones."
            },
            "weaknesses": {
                "value": "- The authors' introduction of their training dataset appears to be insufficiently detailed in certain areas. Firstly, the dataset's composition is not entirely clear, especially concerning the proportion of hallucinated content within it. Secondly, during the training of the hallucination revisor, they used 5,000 image-text pairs from the LLaVA-150k dataset randomly. However, the study does not provide adequate experimental backing to validate the adequacy of this sample size for their objectives. It would be beneficial if the authors could offer a more comprehensive description of their dataset, and ideally make it open-sourced. Such an act could serve as an added contribution to their work. Furthermore, the LURE methodology, as presented, comes across as somewhat straightforward, lacking a distinctive innovative edge.\n- LURE is designed as a post-hoc solution aimed at addressing object hallucination; however, it doesn't directly confront the underlying causes of the issue. A more direct challenge would be formulating strategies for guiding the LVLM to produce answers with reduced hallucination tendencies.\n- Lack of comparing LURE's performance on the fine-grained caption and concise caption. Intuitively, the problem of hallucination would be more common in fine-grained captions."
            },
            "questions": {
                "value": "- The effect of object position on object hallucination is not clear. I am still confused why hallucination occurs in the latter part of the descriptions. Is it possible to fundamentally reduce LVLM hallucination from this perspective."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission831/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission831/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission831/Reviewer_4L8t"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission831/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810849483,
        "cdate": 1698810849483,
        "tmdate": 1699636010472,
        "mdate": 1699636010472,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xcw4UIHEo6",
        "forum": "oZDJKTlOUe",
        "replyto": "oZDJKTlOUe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission831/Reviewer_4tjW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission831/Reviewer_4tjW"
        ],
        "content": {
            "summary": {
                "value": "This paper finds three key factors related to object hallucination: co-occurrence, uncertainty, and object position. Based on this, the authors propose LVLM Hallucination Revisor (LURE) to rectify the object hallucination issue in LVLMs. LURE takes text descriptions as input and outputs refined ones. The authors collect a hallucinatory dataset using GPT-3.5 and thereby train the LURE. The experiments evaluate LURE on existing open-source LVLMs and results demonstrate LURE's effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper proposes a framework to address the hallucination of LVLMs, by identifying key factors and training a revisor correspondingly.\n2. This paper presents the technical details clearly. Rigorous theoretical derivations are provided as well.\n3. This paper conducts extensive experiments and shows the quantitative improvements of LURE."
            },
            "weaknesses": {
                "value": "1. The definition of positioning score is not intuitive. Have the authors analyzed the position score distribution under different description lengths? If shorter descriptions yield lower position hallucination, would generating multiple short descriptions and combining them result in a high-quality description?\n2. Lack of results of other popular benchmarks. This paper only reports the performance on the COCO 2014 test dataset, which is small and may be biased. There is no result about the performance on other popular benchmarks for LVLMs, such as MMBench, MME, POPE, SEED, MM-Vet, etc. Will the performances be better or worse on these benchmarks?\n3. Lack of an analysis of the complexity and usefulness of the responses. There is a tradeoff between the correctness and complexity of the responses. Directly removing the hallucination context may improve the correctness but reduce the diversity and complexity. An analysis regarding this concern is important for a comprehensive understanding of the impact of the proposed method."
            },
            "questions": {
                "value": "The results of Figure 1(c). Is this quantity of images (just 200) sufficient to consolidate the distribution statistics? And even if a sufficient number of samples are provided in a specific domain, can this conclusion be generalized to the distribution of other datasets or benchmarks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission831/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission831/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission831/Reviewer_4tjW"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission831/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698845252668,
        "cdate": 1698845252668,
        "tmdate": 1699636010379,
        "mdate": 1699636010379,
        "license": "CC BY 4.0",
        "version": 2
    }
]