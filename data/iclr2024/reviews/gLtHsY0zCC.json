[
    {
        "id": "nyOOWb8dzv",
        "forum": "gLtHsY0zCC",
        "replyto": "gLtHsY0zCC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4045/Reviewer_fP8i"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4045/Reviewer_fP8i"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses a pertinent challenge in the AI modeling domain, which is model selection for transfer learning. With numerous models available in repositories, each trained on different source data, determining the most appropriate model for a particular task becomes a significant challenge. The authors frame this selection issue as \"label-agnostic model selection\" - the idea of choosing an efficient model for a specific domain without access to labeled target data.\n\nThe main contribution is the introduction of the \"T-Measure\" as a quantitative tool to estimate the transferability of models based on their training data from the source domain. This measure assesses the transferability by evaluating distributional characteristics of the source domain's data, target domain's data, and the fundamental performance of a task-specific model. The measure aims to provide a ranking of models based on their potential performance on unlabeled target domains.\n\nThe authors also touch upon modifying existing task-centric measures to focus on the data, contrasting these against their T-Measure. Through experimental validation on 4 tasks and 11 datasets, they conclude that the T-Measure outperforms baselines in model ranking."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea is clearly stated in the paper, and it is easy to follow. The main idea is to compare the similarity of source data and target data.\n\n2. The problem this paper explores is important, especially in these days where new models come out everyday.\n\n3. The experiments focus on language data, a less explored data type in previous literature."
            },
            "weaknesses": {
                "value": "1. **Over-simplification of Transferability Factors**: The paper's approach to the complex problem of model transferability is overly reductive. As highlighted in Figure 2, there exists a multitude of factors that can significantly influence transfer performance. The paper seems to narrow its focus primarily on data, while ignoring other crucial aspects. The rationale behind this selective approach is not sufficiently justified. A comprehensive model transferability analysis should account for the interplay of multiple factors, and not just a singular dimension.\n\n2. **Unrealistic Experimental Setting**: The experimental setup employed in this study raises concerns about its applicability in real-world scenarios. Specifically, the paper assumes that models are trained, transferred, and assessed all within the confines of a single task. This is in stark contrast to common practice, where a pre-trained model, developed for one task, is often adapted to suit an entirely different task. Such a limitation restricts the practical utility of the proposed T-Measure.\n\n3. **Opaque Methodological Details**: The paper's exposition on its methodology is not clear in details. For instance, while it uses self-supervised learning for data representation, there's a conspicuous absence of details regarding its implementation. Questions arise about how hyper-parameters for self-supervised learning were chosen and their potential impact on the study's outcomes. The sensitivity of the proposed measure to different self-supervised learning algorithms and training settings remains unclear.\n\n4. **Lack of Rigor in Presentation and Design Choices**: The manuscript frequently resorts to phrases like \"intuitively,\" which undermine the scientific rigor expected of such a study. Additionally, many design choices appear arbitrary, with little to no justification provided. This raises concerns about the robustness and generalizability of the study's conclusions. Specifically, it remains uncertain whether the paper's findings would remain consistent across changes in model architectures, tasks, transfer methods, or self-supervised learning techniques.\n\nIn conclusion, while the paper introduces a new T-Measure, its presentation and methodological approach have much room for improvement. A rigorous, comprehensive, and transparent exploration of model transferability is crucial for its findings to hold relevance and value in the broader AI research community."
            },
            "questions": {
                "value": "1. Can authors summarize and refine the contributions part? The \"contribution\" of the introduction seems to only summarize each section, not really \"contribution\".\n\n2. It is not clear if this paper targets at domain adaptation or model fine-tuning. The paper mentions that it focuses on the case where source and target shares the same task, which is domain adaptation actually. If the paper further focuses on the case where Dtrg is not accessible during model selection, it is unsupervised domain adaptation I think. And if the paper deals with selecting source dataset, it is multi-source unsupervised domain adaptation. The literature papers it mentions like Bao et al., 2019; Nguyen et al., 2020; Tran et al., 2019 are all for fine-tuning. They have access to target labels, and they tried different ways to use the target labels. The authors should have a thorough literature review to put this paper into an appropriate position in the literature.\n\n3. There are several incomplete sentences, which I would like the authors to clarify:\n\n- \"While some recent body of work sugguest that source dataset is an important factor in transfer: (Zhao et al., 2022) suggest that some datasets are intrinsically harder than others and (Ethayarajh et al., 2021) show that training datasets have different amount of useful information for trained models.\" in the introduction.\n- \"when the task, model architecture and transfer method are invariant in the transfer setting\" after Figure 2.\n- \"6 presents boxplots of ranking performance\" in Page 8. It should be \"Figure 6\" I think."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4045/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698566397079,
        "cdate": 1698566397079,
        "tmdate": 1699636367878,
        "mdate": 1699636367878,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "27PPTRxcb2",
        "forum": "gLtHsY0zCC",
        "replyto": "gLtHsY0zCC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4045/Reviewer_TVL6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4045/Reviewer_TVL6"
        ],
        "content": {
            "summary": {
                "value": "A new transfer metric is proposed to evaluate target task in zero-shot accuracy setting, when no target labels are available. The new measure estimates the closeness of the target dataset to various source datasets and chooses the model trained on the closest one. The pointwise mutual information metric is used to compute the closeness of the datasets, after learning a suitable representation space on the labeled source dataset. Results have been computed on various dataset settings for the tasks ranging from response selection, emotion recognition, question answering and relation classification."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper addresses a very pertinent and important problem of model selection when no target labels are available during model selection (zero-shot model selection). \n\n- The explanation for various types of transfer is presented well in Sec 2."
            },
            "weaknesses": {
                "value": "- This paper makes a key assumption that the source datasets is decomposable, which might not always be the case. Therefore, the generality of the approach to other datasets is not guaranteed, limiting its impact. Also, they assume that the source dataset is always available during model selection, which is generally not true if we consider the recent trend of foundation models where the source datasets are not available but only source models are released. Lastly, although a minor assumption, it might not always hold that the source and target tasks are same. For example, MLM (masked-langauge-modeling) has been shown to be universally applicable to many downstream tasks for few shot transfer.\n\n- The paper, at its core, essentially evaluates which of the source dataset is the closest to a particular target dataset. In this sense, the authors must also include comparisons with several works in domain adaptation (DA) and robust optimization literature. For example, several measures like A-distance and H-divergence [1] also need to be included. In general, several works in UDA litearture need to be cited and discussed.\n\n- It is really not clearly explained why triplet loss is chosen to learn a suitable representation space. Does the representation learnt using source labels work equally well? Can we also use contrastive loss with multiple negatives? This aspect should be studied in much greater detail.\n\n- The paper states that they use Sentence-BERT for the initial representation. Does this effect the evaluation in any way? For example, does the dataset used to train Sentence-BERT change the ease of transfer to related target domains?\n\n- I am curious to know why the authors chose to provide examples related to vision datasets and vision tasks (in Fig 2), while all the evaluation is done using NLP datastes. Even more so, Fig 1-4 are all really not explaining anything related to the problem or method, and could be improved to illustrate your idea better.\n\n#### Minor\n\n- Sec 3.2, Step1: Did you mean argmin in the equation?\n\n- For completeness, PVI should be explained using an equation or such, since not all readers might be familiar with the term (I am not!).\n\n\n[1] Ben-David, Shai, et al. \"A theory of learning from different domains.\" Machine learning 79 (2010): 151-175."
            },
            "questions": {
                "value": "I feel the intuitions behind several choices in the paper could be presented better and the evaluation could be made more extensive by comparing with other measures of distance, representation learning methods and transfer settings. I request the authors to address my concerns above and I'd be happy to raise the rating."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4045/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638333686,
        "cdate": 1698638333686,
        "tmdate": 1699636367804,
        "mdate": 1699636367804,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mNPiqqoOaM",
        "forum": "gLtHsY0zCC",
        "replyto": "gLtHsY0zCC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4045/Reviewer_7czX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4045/Reviewer_7czX"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a metric for measuring transferability score across datasets, called T-measure. The measure introduces a two step process to measure the transferability score - 1. selecting a subset similar to target dataset, 2. using PVI of datapoints and aggregating to measure the final score. Experiments are shown on different datasets, comparing with existing techniques, which demonstrate that the method performs better on average than competing scoring methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The model has been evaluated on different datasets, showing comparable or better performance than competing methods. The domain of testing includes classification, emotion recognition, question/answering, etc. Previous methods were focused on classification methods only."
            },
            "weaknesses": {
                "value": "The current draft of the paper has major weaknesses as listed below:\n\n1. Equations are not numbered in the paper, which is lowering the readability of the paper. For example, equations in Section 2.1 and 2.2 are outputting \\phi^* model, on the first read its not clear what is the difference between the two equations. Similarly, in Section 3.2, PVI computation of data samples is not provided, hence the motivation for Step 2 is not clear from the draft. In the results section, Table 4 and 6 are referenced for results, but these tables are present in Appendix, not the main section of the paper. Not sure if it violates the page length limit in the conference.\n\n2. The proposed method is an increment over the (Ethayarajh et al., 2021) which introduced V-Usabiltiy and Pointwise V-Information(PVI). The current paper aggregates the PVI over a subset of samples from source, which are similar to target. Are there other functions which can be considered for aggregation, can authors provide justifcation for selecting this particular method. Also, it is not clear how this applies for a generic transferred model (\u03b1(T, Dtrain_trg , \u03d5i)). The paper mentions that \u03b1 is identity in Section 2.4, but its not clear otherwise.\n\n3. In Table 3, it is not clear what average Kendell-tau distance signifies? There is no point averaging across datasets. Can authors specify if the model is outperforming other methods on individual datasets. Also, references for competing methods are not provided in the table."
            },
            "questions": {
                "value": "Weakness section has questions about the paper, which should be answered in the rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4045/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838568720,
        "cdate": 1698838568720,
        "tmdate": 1699636367743,
        "mdate": 1699636367743,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OKzY7Fa2kV",
        "forum": "gLtHsY0zCC",
        "replyto": "gLtHsY0zCC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4045/Reviewer_Umyu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4045/Reviewer_Umyu"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new metric for estimating model transferability in zero-shot transfer learning. The authors cast the problem as label-agnostic model selection, which aims to select the best model on a target dataset without annotations. The authors propose to learn a representation space aligned with each source dataset using contrastive learning on triplets which captures the dependency between data. Experiments on multiple tasks and datasets demonstrate the effectiveness of the proposed metric for model selection compared to other baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper studies an important practical problem of model selection for transfer learning in the absence of labeled data.\n\n2. The proposed data-centric transferability measure based on source/target dataset similarity is interesting.\n\n3. T-measure shows consistent benefit over baselines on diverse tasks and datasets."
            },
            "weaknesses": {
                "value": "1. The proposed metric requires access to the source data. However, the source data may not always be available during fine-tuning.\n2. The number of pre-trained models is quite limited. According to Table 3, the metric is only evaluated on 3 or 6 models. It remains unclear whether the metric can be extended to a large number of pre-trained models."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4045/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4045/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4045/Reviewer_Umyu"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4045/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698850851719,
        "cdate": 1698850851719,
        "tmdate": 1699636367681,
        "mdate": 1699636367681,
        "license": "CC BY 4.0",
        "version": 2
    }
]