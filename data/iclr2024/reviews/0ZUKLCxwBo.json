[
    {
        "id": "5N1JGNjUk9",
        "forum": "0ZUKLCxwBo",
        "replyto": "0ZUKLCxwBo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8211/Reviewer_QK7D"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8211/Reviewer_QK7D"
        ],
        "content": {
            "summary": {
                "value": "This manuscript introduces a simple setup to reproduce the  grokking phenomenon on modular arithmetic problems. Different from existing works, the major contribution is the authors provide an  analytic solutions for  two-layer quadratic networks of solving modular arithmetic problems. Additionally, the authors show that in experiments, typical algorithms like SGD and Adam indeed find solutions that resemble the analytic ones."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed setup  is simple and interpretable.\n- The analytic solutions could be valuable in analyzing the grokking phenomenon for the tasks of modular arithmetic."
            },
            "weaknesses": {
                "value": "- The constructed analytic solutions for the tasks of modular arithmetic has potential but specifically, this manuscript does not produce too much new insights for understanding grokking. For instance, one can easily construct analytic solutions for learning k-sparse parity with two-layer ReLU networks, where we can reproduce the grokking phenomenon. \n\n- The authors have empirically shows a peak around 0 for $\\phi_k^{(1)} + \\phi_k^{(2)} - \\phi_k^{(3)}$ in the found solution, satisfying equality they propose. However, the presentation falls short of providing adequate evidence that the found weights have the periodic structure of the analytic solution. It is imperative that the authors supplement their work with further empirical evidence or a comprehensive theoretical analysis to elucidate how the weights progressively evolve toward the analytic solution during the training process.\n- Further investigation into the minimal data amount of grokking occurrences is warranted. Does the order of the minimal amount is $O(p^2)$. If not, it necessitates a more suitable definition of the fraction as presented in equation (3).\n- Some mathematical oversight. \n   - The definition in (6) might lead to the misconception that the weights $W_{kn}^{1}$ form an $N \\times p^2 $ matrix  \n   - The  factor $\\frac{1}{N}$ is missing at the beginning of (11)."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8211/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698680869422,
        "cdate": 1698680869422,
        "tmdate": 1699637019432,
        "mdate": 1699637019432,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JZb9XzG1yc",
        "forum": "0ZUKLCxwBo",
        "replyto": "0ZUKLCxwBo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8211/Reviewer_5GzG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8211/Reviewer_5GzG"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the gokking phenomenon by fitting two-layer MLP on modular arithmetic tasks. The paper obtains explicit periodic features in the solutions, and shows that gokking occurs when the correct features are learned."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This paper is a timely and important contribution to the growing literature on gokking. It offers a class of problems with explicit solutions, so that gokking can be studied in great depth."
            },
            "weaknesses": {
                "value": "While simplicity and explicit solution are a strength, it also limits the scope of the paper in terms of covering the gokking phenomenon in general. Moreover, it is desirable to study the dynamics of the optimizers in reaching the exact solutions, but the paper did not make such an attempt."
            },
            "questions": {
                "value": "The transition from memorization to generalization appears to be a continuous process of Occam's razor, i.e., gradually reducing the complexity of the model while maintaining the training error. Converging to periodic features is also of this nature. Is this the correct understanding of gokking?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8211/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698907085806,
        "cdate": 1698907085806,
        "tmdate": 1699637019322,
        "mdate": 1699637019322,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XMoUzHHAu7",
        "forum": "0ZUKLCxwBo",
        "replyto": "0ZUKLCxwBo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8211/Reviewer_PaA4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8211/Reviewer_PaA4"
        ],
        "content": {
            "summary": {
                "value": "The authors present a two-layer MLP for solving modular arithmetic tasks. The goal is to study a sudden jump in generalization during training, known as grokking. An analytic solution of the model weight is derived, guaranteeing 100% test accuracy. A general result for  arithmetic addition is also given. The experiments show that the proposed representation is also found by training using gradient descent and AdamW."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The analysis of grokking help to understand and dynamics of model training and how to achieve good generalization\n- The theoretical results are applicable for general modular functions. Follow-up work could leverage on these results."
            },
            "weaknesses": {
                "value": "- Simple architecture and tasks (two layer MLP, modular arithmetic) could limit the applications and extensions of this work\n- The given analytical solution does not help much in understanding how grokking happens as the latter occurs earlier than achieving 100% test accuracy."
            },
            "questions": {
                "value": "- How does the analytical solution help understanding grokking ? \n- Neural networks are known to converge to local minima. I wonder if there are potentially other analytical solutions and why it seems that model training leads to the same solution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8211/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698910193066,
        "cdate": 1698910193066,
        "tmdate": 1699637019227,
        "mdate": 1699637019227,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x9Y1gkwLZd",
        "forum": "0ZUKLCxwBo",
        "replyto": "0ZUKLCxwBo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8211/Reviewer_UC6H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8211/Reviewer_UC6H"
        ],
        "content": {
            "summary": {
                "value": "Using a two-layer MLP, this paper analyzes the phenomenon of grokking on a few modular arithmetic problems. Due to the simple DNN architecture, the weights and features are calculated analytically to solve modular addition problems to provide mechanical details about what was learned by the model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Grokking is an interesting and exciting phenomenon that is worth careful study.\n2. The paper is technically sound.\n3. The presentation and organization is clear."
            },
            "weaknesses": {
                "value": "1. To provide an analytical solution and interpretability of the model, this paper focuses on a very simple model (definitely not used in practice) and arithmetic function to be learned, which limits its impact on practical models currently in use, such as CNN, Transformer.\n\n2. If the model really learns the arithmetic function, it will be interesting to see whether the model generates accurate results for OOD data, e.g., training with the data from [0, 10], testing with the data from [1000, 1100]."
            },
            "questions": {
                "value": "1. \"Instead, large width leads to redundant representations: Each frequency appears several times with different random phases ultimately leading to a better wave interference\" Since they are identical frequencies, will combining them provide a more concise representation? \n\n2. Besides the amount of data, how is grokking affected by the training data distribution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8211/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698957474732,
        "cdate": 1698957474732,
        "tmdate": 1699637019128,
        "mdate": 1699637019128,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pmtfIhncw8",
        "forum": "0ZUKLCxwBo",
        "replyto": "0ZUKLCxwBo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8211/Reviewer_9nDi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8211/Reviewer_9nDi"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of learning modular arithmetic with a two-layer network. It proposes a certain Ansatz for the final weights based on Fourier analysis and experimentally shows that the weights match this Ansatz."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper's presentation is clear and to the point, and the construction of the weights is succinctly explained. The experimental evidence is convincing. Mechanistic interpretability is also a highly interesting direction overall."
            },
            "weaknesses": {
                "value": "1) Literature review is missing some recent work:\n\n* There is a growing body of work on learning single-index and multi-index functions (see e.g., \"Online stochastic gradient descent on non-convex losses from high-dimensional inference\" by Ben Arous et al., and \"SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics\" by Abbe et al.) which shows similar grokking effects. It could be interesting to understand how these relate to the arithmetic grokking effect.\n\n* More crucially, there was a paper called \"Progress measures for grokking via mechanistic interpretability\" which appeared online in Jan., 2023 and was published in ICLR 2023. This paper also seems to derive the Fourier-based solution to the grokking task. This seems unfortunate, because it seems that at the time that this paper was written either the authors are unaware of this other paper, or that this paper was written concurrently and has been to a good extent subsumed by that other paper. Could the authors comment on this? This is the main weakness in my mind.\n\n2) The analysis only gives an Ansatz for the final solution of the weights, but does not explain why more/less data leads to finding it, and why there is a sharp jump in the algorithm's loss from not finding the Ansatz to finding the Ansatz. In other words, the paper only predicts the final weights but does not give an interpretation of what is driving the dynamics of the grokking process."
            },
            "questions": {
                "value": "1. What is meant by \"Functions of the form f(n, m) + g(n, m) mod p are more difficult to grok: they require more epochs and larger \u03b1\"? Why do you need both f(n,m) and g(n,m) here?\nTypos:\n\"a lightening\" -> \"an enlightening\"\n\"we do not observer\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8211/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8211/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8211/Reviewer_9nDi"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8211/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699485452484,
        "cdate": 1699485452484,
        "tmdate": 1700700166427,
        "mdate": 1700700166427,
        "license": "CC BY 4.0",
        "version": 2
    }
]