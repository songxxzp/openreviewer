[
    {
        "id": "P4yS08xfVv",
        "forum": "4WZNdnwmhk",
        "replyto": "4WZNdnwmhk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1583/Reviewer_rWs7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1583/Reviewer_rWs7"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework named multi-lora, which enables parameter sharing across different parallel fine-tuning tasks. The authors provide some theory to justify the rationality of this framework."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to read.\n\n2. The idea that parameter sharing or parameter transferring between tasks under the framework of PEFT is valid and interesting."
            },
            "weaknesses": {
                "value": "1. The motivation of the studied problem is unclear. I doubt if there are any scenarios in reality where we need to parallel fine-tune LLMs on multiple tasks. Also, from the experimental results, we can save 50% of the parameters (~1M) at the cost of sacrificing performance compared to LoRA. Is this worth it? \n\n2. Some important related works are missed. More related works should be discussed, such as parameter-efficient fine-tuning and multi-task learning.\n\n3. The experiments are inadequate. \n   - (1)This paper does not include necessary factors such as ablation studies and sensitivity analysis to demonstrate the effectiveness of the method. \n   - (2)This paper does not compare the training time cost of different methods, which may be an important factor in the problem scenario proposed by the authors.\n   - (3)Sequentially fine-tuning the tasks with parameter(LoRA_A) sharing should also be studied and compared with the proposed method thoroughly.\n\n4. There are no conclusions.\n\n5. I am unaware of the relationship between the theories and the effectiveness of the method. This paper uses a lot of space to prove some simple properties. Why do properties of global loss (lipschitz, smooth, convexity) explain the performance of multi-lora?\n\n6. Why did you freeze LoRA_A layers and not fine-tune them?\n\n7. Some statements about LoRA are wrong, such as \u201cLoRA improves inference time\u201d."
            },
            "questions": {
                "value": "Please see the weakness part. No additional questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1583/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1583/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1583/Reviewer_rWs7"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1583/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698546924139,
        "cdate": 1698546924139,
        "tmdate": 1699636087185,
        "mdate": 1699636087185,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0vrLs36ZYt",
        "forum": "4WZNdnwmhk",
        "replyto": "4WZNdnwmhk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1583/Reviewer_Va7k"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1583/Reviewer_Va7k"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a model, Multi-LoRA, aimed at decreasing the quantity of trainable parameters when employing LoRA for parallel fine-tuning. Specifically, all tasks share a global and fixed parameter A, along with a trainable task-specific parameter B. This strategy significantly reduces the number of trainable parameters. In the proposed method, the parameter count for k tasks can be reduced from O(Kdr+kmr) to O(dr+kmr). The authors provide theoretical assurances for model convergence. Empirical experiments are performed on Roberta and GPT2 for natural language understanding and generation tasks, respectively."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper tackles a compelling problem: reducing the number of trainable parameters for parallel fine-tuning. The proposed Multi-LoRA method technically sounds. The authors provide a detailed theoretical proof of model convergence. The empirical results underscore the effectiveness of this method. Notably, the number of trainable parameters decreases from 2.4M to 1.3M for eight Natural Language Understanding (NLU) tasks and from 1.1M to 0.7M for three generative tasks.\n2. The methodology is straightforward, and the experimental settings are detailed."
            },
            "weaknesses": {
                "value": "1. The performance of Multi-LoRA is not as strong as that of LoRA. For example, the average scores drop from 87.2 to 85.1 for understanding tasks and from 56.7 to 53.7 for generation tasks. While parameter reduction is significant, performance is often a more critical factor.\n\n2. The paper does not include a comparison with multi-task learning (MTL). Both settings involve training multiple tasks simultaneously, yet no MTL methods, such as AdapterFusion[1], are compared in the experiments.\n\n3. The paper lacks experiments on recent Large Language Models (LLMs) like Llama2 and does not provide an analysis of convergence speed.\n\n\n[1]. AdapterFusion: Non-Destructive Task Composition for Transfer Learning"
            },
            "questions": {
                "value": "1. How would the model\u2019s performance be affected if parameter A were allowed to be trainable? Could this modification potentially enhance the model\u2019s performance?\n\n2. Are the RoBERTa and GPT models trained using fp32 precision? If so, what would be the impact on the models if mixed-precision training, such as fp16, were used? This question is particularly relevant given that recent Large Language Models (LLMs) commonly employ fp16 precision for training."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1583/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1583/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1583/Reviewer_Va7k"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1583/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698753090341,
        "cdate": 1698753090341,
        "tmdate": 1699636087028,
        "mdate": 1699636087028,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "l550leVs0t",
        "forum": "4WZNdnwmhk",
        "replyto": "4WZNdnwmhk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1583/Reviewer_GVCH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1583/Reviewer_GVCH"
        ],
        "content": {
            "summary": {
                "value": "This paper designs a framework that reduces the parameter count even more than LoRA, in addition to enabling parameter sharing among various parallel fine-tuning tasks. When the volume of parallel fine-tuning tasks increases, the framework slashes the parameter count by nearly half in comparison to LoRA. Additionally, the authors provide theoretical evidence explaining the effectiveness of this approach\u2014and, by extension, that of LoRA\u2014for a wide array of loss functions. The effectiveness of the proposed method is empirically confirmed on multiple benchmark models and datasets, showcasing a substantial decrease in parameter count while maintaining performance comparable to that of LoRA."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality: The Multi-LoRA framework is a novel approach to fine-tuning LLMs that takes into account shared structure between tasks, which is an important consideration in many real-world applications. \n\nQuality: the theoretical analysis of the method is also well-presented and provides insights into the properties of the method.\n\nClarity: the paper is well-organized and easy to follow.\n\nSignificance: The Multi-LoRA framework and the proposed method have the potential to improve the efficiency and effectiveness of fine-tuning LLMs, which is an important consideration for many real-world applications."
            },
            "weaknesses": {
                "value": "This paper can be significantly improved by more thorough experiments in several aspects:\n1) More LLM with larger size\n2) More metrics beyond GLUE for LLM\n3) The proposed Multi-LoRA seems to achieve inferior performance for all the tasks. In this way, it is hard to validate the inefficiency of the proposed method"
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1583/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699025151339,
        "cdate": 1699025151339,
        "tmdate": 1699636086971,
        "mdate": 1699636086971,
        "license": "CC BY 4.0",
        "version": 2
    }
]