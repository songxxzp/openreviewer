[
    {
        "id": "pOlhGTyhm8",
        "forum": "riQmzq5FaQ",
        "replyto": "riQmzq5FaQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2750/Reviewer_FcT3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2750/Reviewer_FcT3"
        ],
        "content": {
            "summary": {
                "value": "The main contribution of this paper is the idea that, in RL, a policy can be made to specify both a control action to apply *and* the length of time an actuator should apply that action. The paper integrates this idea within an existing, popular algorithm for model-free RL (the SAC algorithm), and presents comparative results in a small example problem."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "As far as I know this precise idea is novel, and it is certainly intuitive. Results and other details aside, I think the community should investigate this direction more deeply and this paper provides a nice starting point for that effort."
            },
            "weaknesses": {
                "value": "Weaknesses:\n- The literature review is quite thin, and as a circumspect reader I do wonder how novel this idea really is, given how little literature is referenced. For example, a quick google scholar search reveals the following papers that seem to be very closely related: [1, 2]. I would also add that variable rate decision making is widely studied in the control theory literature. A key phrase to find this literature is \u201cadaptive time step.\u201d\n- The paragraph immediately above section 3.1 indicates that there are a lot of loose ends that are not being discussed in detail, and which may strongly affect results. The imprecision of this discussion (e.g., what is a \u201cpartial MPC\u201d and what role does the PID serve if you already use MPC?) suggests that the work may be somewhat immature.\n- The reward structure discussed section 3.1 is *not* what one would properly call a \u201cmulti-objective optimization problem.\u201d A distinguishing trait of such problems is the concept of \u201cPareto optimality\u201d which encodes all of the tradeoffs among optimal performance with respect to each separate objective. By assuming a fixed weighting, this paper effectively reduces the problem to a standard optimization problem (and picks a single point on the Pareto frontier). I recommend consulting [3] for further details.\n- Relatedly, the construction in Definition 1 is not as clear as it could be. For instance: are the R terms intended to be functions of state (and action)? If so, why does it make sense to only accrue reward at the times when actions are changed? Doesn\u2019t that lead to some obvious opportunities for reward hacking? For example, could an agent decide to plow straight through some region of low reward for a bunch of (unactuated) time steps? Also, can R_t and R_\\epsilon be evaluated at every time, or only at the end of an episode? Evidently, at every time t, but then I am lost as to why the agent is incentivized to minimize n, the length (in steps) of an elastic time step. I am lost.\n- The details of the method are really not very clearly explained. For example, throughout the discussion of section 3 it appears that the there is some notion of an agent physically moving and the policy gets to access a measure of distance somewhere. This is unclear: everything up until this point (and in general) is framed around general MDPs, which have nothing to do with physical embodiment. How general-purpose is the proposed approach?\n- Relatedly, the test environment is not very clearly explained, or at the very least, suggests a very basic question: wouldn\u2019t it make more sense for the policy to output a force, rather than a target position? This would remove the need for lower-level tracking control (MPC, PID) and also mitigate the \u201cmeasure of distance\u201d question above, I believe. \n- I do not follow the \u201csix dimensions of the state in the environment\u201d - in fact, I count 9: 2 each for agent/obstacle/goal position, 2 for agent velocity, and 1 for duration. What am I missing? In the same paragraph, the discussion of semi-Markov processes and recurrence is rather opaque. Use of words like \u201cmight\u201d and \u201ccould\u201d lead me to wonder how clearly this point is understood. I suggest clarifying the language here.\n- There are no discernible error bards in the plots, and the shaded areas appear to be traces of other plotted data - this needs to be explained precisely, and plots should show some measure of error in order to be interpreted statistically.\n- More importantly, even: there is little to no interpretation of the behavior of the proposed policies. Results here indicate some differences in aggregate behavior (although the interpretation to that effect should really require some error bars as above), but it would really help to understand what is going on if the authors expanded upon Fig. 7 to illustrate what was going on in the environments in these situations and why it made sense to change the control rate as shown.\n\nOther nitpicks:\n- It seems like the main motivation here is one of saving computational resources. Obviously, most control systems are pretty lightweight and so I imagine these savings really come in from the perception side, e.g., if you no longer have to process big images at high frame rate. Experimental results to illustrate these savings more directly than the abstraction of \u201cnumber of repeated actions\u201d would be highly motivating.\n- There are quite a few typos and other small syntax issues.\n- The vertical axis labels are wrong in Fig. 5.\n- Figures 5 and 6 could be more clear about indicating that the right hand sides are insets of the left. Also, why were the methods run for so long - it seems they all converged quite a bit earlier and then for some reason PPO destabilized. Something seems off here.\n- Why does Fig. 7 say \u201cepochs\u201d instead of \u201cconfigurations?\u201d\n\n\n[1] Chen, Y., Wu, H., Liang, Y., & Lai, G. (2021, July). VarLenMARL: A framework of variable-length time-step multi-agent reinforcement learning for cooperative charging in sensor networks. In 2021 18th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON) (pp. 1-9). IEEE.\n\n[2] Sharma, Sahil, Aravind S. Lakshminarayanan, and Balaraman Ravindran. \"Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning.\" International Conference on Learning Representations. 2016.\n\n[3] Deb, Kalyanmoy, and Kalyanmoy Deb. \"Multi-objective optimization.\" Search methodologies: Introductory tutorials in optimization and decision support techniques. Boston, MA: Springer US, 2013. 403-449."
            },
            "questions": {
                "value": "Please see my comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2750/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697840366365,
        "cdate": 1697840366365,
        "tmdate": 1699636217885,
        "mdate": 1699636217885,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GBWY4BJU1r",
        "forum": "riQmzq5FaQ",
        "replyto": "riQmzq5FaQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2750/Reviewer_DZcS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2750/Reviewer_DZcS"
        ],
        "content": {
            "summary": {
                "value": "This work presents relaxes the fixed frequency assumption of MDP typically studied in RL and proposes RL with elastic time steps.  Also a Soft Elastic Actor-Critic algorithm is derived with theoretical and practical benefits."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The work is concisely summarized.\n2. The use of elastic time is important in the tasks such as robotics etc."
            },
            "weaknesses": {
                "value": "1. There are many existing studies with varying time (e.g. option framework, action repetitions\u2026)\nAuthors introduce some notions of options and semi-MDP in appendix, but without clear definitions of each notation, which makes it harder to see the clear connections to the main work and the option framework.  (It was not clear how the authors validated Bellman-like equations for elastic time case)  Assuming the algorithm is properly derived from the option framework, it is necessary to compare to the existing work based on the framework.  (Or at least it should show significant practical results compared to the existing work; it seems the experiments are not for sufficiently complex tasks.)\n2. Existing environments such as OpenAI Gym can be easily adjusted to include time as information for states; I am not sure what the authors mean by \u201c...additional input and output information that is not available within existing RL environments\u2026\u201d\n(Note that simulators anyway need to run with small time interval to maintain accuracy, and action durations can be just a repetition of that.)\n4.  Figure 5 is a bit hard to parse: why time in seconds are negative?  I could guess this but it is better to make them crystal clear.\n5.  It would be better to show baseline with 100Hz (fixed) case, not 5.0 Hz since the elastic one uses 1 to 100 Hz.\n6.  Figure 7 is also hard to interpret; why are there only 2 time steps\u2026?  2 steps are enough to complete tasks\u2026?\n7.  Finally, it was not clear why the authors specifically used the reward defined in Definition 1."
            },
            "questions": {
                "value": "1.  Figure 4 right seems too sparse; what does it try to imply?\n2.  What is the action space A?  Is it the Cartesian product of \u201caction\u201d and \u201ctime\u201d?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2750/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2750/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2750/Reviewer_DZcS"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2750/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698475622052,
        "cdate": 1698475622052,
        "tmdate": 1699636217797,
        "mdate": 1699636217797,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UGJiwWZVEU",
        "forum": "riQmzq5FaQ",
        "replyto": "riQmzq5FaQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2750/Reviewer_XuXs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2750/Reviewer_XuXs"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a reactive reinforcement learning policy, which breaks the fixed time step assumption commonly adopted in RL and determines the next action and the duration of the next time step as input to the controller, thus integrating the temporal aspect into the learning process. The authors test their approach in a simulation of a simple word with Newtonian kinematics, showing its effectiveness in leading to higher efficiency in terms of speed and energy consumption."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The contribution is clearly stated and it is relevant to the development of real-world efficient and effective RL-based control systems. The paper structure is well organized and clear. Figures and schemes are helpful and explanatory. Limitations of the proposed approach (which components would be necessary for a real-world implementation) are clearly stated."
            },
            "weaknesses": {
                "value": "The contribution is relevant but it is limited compared to the existing state of the art. Since the contribution is mainly aimed to applying RL control outside of simulation, a proof of concept of the functioning of the proposed algorithm on a real-world application (rather than only in a simulation environment) would be important, in my view. \nAlthough the paper\u2019s quality of presentation is generally fair, I found the comparison with the related works poor and lacking of an insightful discussion about existing time-sensitive RL tasks, which are only quickly listed at the end of section 2. Expanding such a paragraph could make the relevance and applicability of the paper\u2019s contribution clearer.\nThe presentation of the results could also be improved (see specific comments on the next section)."
            },
            "questions": {
                "value": "-\tFig 1: I don\u2019t find Fig 1 completely effective, based on the description within the Introduction. Since one of the contributions of the Elastic Time Step RL is that of enabling the policy to output the time step duration, together with the action, this could be somehow explicitly indicated in the Figure. Also, even though I understand the intention of splitting the \u201clearning\u201d and \u201cexecution\u201d part of a RL implementation, I find the brain-like icon confusing when used to indicate the \u201cexecution\u201d rather than the \u201clearning\u201d component of the system.\n-\tI would be curious to know from which specific practical application (robotics, autonomous driving?) comes the authors\u2019 inspiration for the paper.\n-\tPage 4, sentence preceding Definition 1: \u201cThe aggregate reward for task completion is represented by r\u201d. Did you mean \u201cR\u201d (capital letter)? \n-\tThe paragraph after Definition 1 (\u201cWe validate our reward strategy\u2026\u201d) could be rephrased to highlight SEAC differences compared to SAC.\n-\tWhat do you mean when you say \u201c\u2026giving a high probability that the agent can discover the optimal solution to complete the task\u201d?  Maybe this sentence can be rephrased to make the exploration strategy clearer.\n-\tIn general, from the sentence starting \u201cwe assume the agent\u2026\u201d to the sentence ending with \u201c\u2026Bellman equation\u201d, I find the flow of the text, which can be read while referring to the scheme on Fig 3, a little hard to follow, in the sense that it jumps from one block to another one (of the Fig.3) without a precise order. Incorporating more references to the visual scheme and aligning the text with the functional flow of the figure 3 (rather than simply listing the meaning of the symbols) could help the readability. \n-\tYou mention that one major contribution of the SEAC is to include the execution time of each action to the output, but this term is not explicitly indicated on Fig.3, together with the At.\n-\tThe meaning of the double arrows in Fig.3 is not very clear to me. Maybe an explanation could be included either on the caption or on the main text.\n-\tThe impact value of the execution is defined, based on the chosen environment, as the target movement distance. Do you have in mind some examples of different implementations for different problems?\n-\tIn the end of paragraph 3.1, when you say \u201cthe controller will compute a range of control-related parameters\u201d, is this represented by Mt?\n-\tIn the end of paragraph 3.1, when you say \u201cour objective is for the agent to learn the optimal execution time\u201d, is the execution time equivalent to the action time, and therefore represented by Tt?\n-\tTypo: \u201cbut but\u201d in the sentence starting with \u201cit is worth noting\u2026\u201d in paragraph 3.2\n-\tWhat is the meaning of \u201cp\u201d in eq. (2)?\n-\tSince the SAEC loss functions are (if I understand well) equal to those of SAC, rather than simply reporting the definitions, I would suggest to reorganize Section 4 to better explain how your formulation of the reward function is included in the update steps of the RL algorithm.\n-\tSection 5: When you refer to the \u201cthree RL algorithms\u201d, do you mean SEAC, SAC and PPO? In this case, you should first say that you are comparing SEAC results with SAC and PPO in the text, otherwise it is not clear to the reader.\n-\tWhat are you representing differently on the left and right side of Fig. 5 and 6? Is it the right side simply a y-axis zoom-in of the left side? You should specify it on the figures' captions. What is the legend for the lighter colored plots?\n-\tI think that Fig. 7, as it is, is not very informative. It shows that SEAC dynamically changes the control rate, but it doesn\u2019t allow to evaluate whether it does it in a meaningful way. Showing the scenario and/or information about the corresponding actions would make the concept clearer.\n-\tI feel Fig.8 would be more readable by inverting x and y axes (evaluation metric on the y-axis). Furthermore, you mention the overall reward both in the section and in the figure caption, but is the overall reward shown somewhere?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2750/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2750/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2750/Reviewer_XuXs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2750/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698673454455,
        "cdate": 1698673454455,
        "tmdate": 1699636217727,
        "mdate": 1699636217727,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BGMjBm8M96",
        "forum": "riQmzq5FaQ",
        "replyto": "riQmzq5FaQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2750/Reviewer_VwVw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2750/Reviewer_VwVw"
        ],
        "content": {
            "summary": {
                "value": "This paper extends the classical RL setting, where there is no concept of the action execution time, to RL with elastic time steps. The authors propose SEAC to output the next action as well as the duration of the next time step."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The proposed problem is interesting. The figures are vivid, and the paper is easy to follow."
            },
            "weaknesses": {
                "value": "The contribution and novelty is vague. As for the traditional RL, the control frequency is only an abstract definition. I think the proposed framework can be seen as a special instance of the traditional RL framework given a reformulated action space / state space / reward function. The algorithm also seems quite like SAC with new state / actions. Also, what is the relationship between the proposed algorithm with HRL methods?"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2750/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698679131509,
        "cdate": 1698679131509,
        "tmdate": 1699636217647,
        "mdate": 1699636217647,
        "license": "CC BY 4.0",
        "version": 2
    }
]