[
    {
        "id": "00HPAwJYlu",
        "forum": "MpWRCiw8g5",
        "replyto": "MpWRCiw8g5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5668/Reviewer_sND9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5668/Reviewer_sND9"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces JOSENet, a novel framework designed for violence detection in surveillance videos. It aims to tackle the challenges of real-world surveillance, such as varying scenes, actors, and the need for real-time detection. The framework consists of a primary target model and an auxiliary self-supervised learning (SSL) model. It uses multiple datasets for training and validation, applies various preprocessing and data augmentation strategies, and evaluates the model using a comprehensive set of metrics."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "**Originality**\n\nThe paper is innovative in proposing a dual-model architecture, involving a primary target model and an auxiliary SSL model. It also introduces a new SSL algorithm based on VICReg and a novel data augmentation strategy called \"zoom crop.\"\n\n**Quality**\n\nThe research is thorough, with detailed experimental settings, multiple datasets, and a diverse set of evaluation metrics. The use of an auxiliary SSL model to achieve a balance between performance and computational resources is commendable.\n\n**Clarity**\n\nThe paper is well-structured and clear, with each section contributing to the reader's understanding of the proposed framework.\n\n**Significance**\n\nThe work addresses a vital real-world problem, that of violence detection in surveillance videos, and proposes a framework that seems both effective and efficient."
            },
            "weaknesses": {
                "value": "Lack of Details: Some sections could provide more implementation details, especially on how the VICReg loss and weight optimization between the two models are implemented.\n\nDataset Limitations: While multiple datasets are used, they are mostly centered around violence detection, which could limit the model's generalizability across domains.\n\nRobustness: The paper does not address how the model handles potential issues like occlusion, varying light conditions, or camera angles, which are common in real-world surveillance.\n\nHyperparameter Tuning: The paper doesn't discuss the process or criteria for hyperparameter selection, which could affect the model's performance."
            },
            "questions": {
                "value": "1.\tCould you provide more details on the \"zoom crop\" data augmentation strategy, specifically its effectiveness and efficiency?\n\n2.\tWhy were these particular datasets chosen, and have you considered using more diverse datasets to improve the model's generalizability?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5668/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5668/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5668/Reviewer_sND9"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5668/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697794896805,
        "cdate": 1697794896805,
        "tmdate": 1699636591071,
        "mdate": 1699636591071,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7GWbfzAnc7",
        "forum": "MpWRCiw8g5",
        "replyto": "MpWRCiw8g5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5668/Reviewer_tKHs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5668/Reviewer_tKHs"
        ],
        "content": {
            "summary": {
                "value": "This paper describes an approach for performing the video task of violence detection in surveillance videos by employing a self-supervised learning network to help improve the primary supervised model. The core network to perform the primary task is based on flow gated network (FGN), by Cheng et al (2021). The semi-supervised learning block applies VICReg approach, by Bardes et al. (2021), to the two streams of RGB and optical flow. The results are reported on three datasets related to activity recognition with the comparison with multiple SOTA approaches and an ablation study."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper describes an interesting idea that can leverage the strengths of semi-supervised learning in the domain of violence detection in surveillance videos where the rarity of the events poses challenges for obtaining a large volume of positive training samples and the need for a low false alarm rate. The proposed approach also has some interesting nuggets related to computational efficiency and reduced memory footprint. They have also studied the tradeoff between the size of the temporal window, framerate, and quality of results."
            },
            "weaknesses": {
                "value": "The problem, application, and the core part of the solution (FGN) is not new. However, the addition of SSL \n\nThe baseline model from `Sec. 4.2` should have been reported in the tabular form for a more effective presentation of material and instead of explaining the numerical differences in the narrative form as done in `Sec. 4.2` and other sections. It should be clear from ONE table the various variants, baselines, and the final version. Additionally, it is hard to follow this paper at times because the different tables are reporting results on different datasets. Are the results in this section reported on the exact same test data as that in Table 3? If so, then should we be comparing $F_1$ of $85.87$ (baseline) with $86.5$ (JOSENet)? i.e. improvement of $0.63$? Is it also fair to say that the baseline approach is very close to FGN, by Cheng et al (2021)? \n\nThe main result comparing JOSENet with SOTA in Table 3 has aspects that are not clear. I assumed this statement\n`We decide to take as reference the results obtained on the 15% subset of UCF-101 with JOSENet.` \nmeant that Table 3 results are on UCF101 but then `a pretraining obtained on a random 15% subset of UCF-101` suggests that it was used for pretraining. Is it a different 15%? More importantly, UCF101 does NOT have violent activities in surveillance scenes (to the best of my knowledge) in the way it has been portrayed in the motivation described in the paper. There are activities like Punch or Boxing Punching Bag, but not much else. Additionally, why stick with some *random* 15% split of UCF101 instead of using the standard test split that could be compared with the SOTA. \n\nThe writing quality of the paper can be improved significantly. There are several grammatical mistakes, a few long run-on sentences, unusual usage of some phrases, and confusing or inconsistent usage of citations that break the flow."
            },
            "questions": {
                "value": "1. It was surprising that results were not reported explicitly on the RWF-2000 dataset in the `4. Experimental Results`, as far as I could tell. In my opinion, it is unusual to make statements like this:\n`pg 8: We have noticed that we do not reach the state-of-the-art performances for RWF-2000.`\nand not provide the quantified numbers. The other statement (`To train and validate the model during supervised learning we use the RWF-2000 dataset`) was also noted. \n\n2. Is there a reason why Table 3 does not have a row with a comparison with FGN, by Cheng et al (2021)?\n\n3. Table 3, AUC column has numbers in [0,100] and [0,1.0] ranges. Are those just typos? \n\n4. Table 5, the use of temporal pooling is not clear as it makes things worse as reported by the scores. The explanation in `Sec. 5` is unclear. The table does not support this claim (if I am following it as intended):\n`To find a confirmation of this approach, using the zoom crop strategy, we apply the temporal pooling\nin the merging block, obtaining on the target task a very low value for most of the evaluation metrics\nused.`\n\n5. pg: 2, FGN was not defined or cited until pg 3 so it was confusing.\n\n6. pg: 2, should `contrastive learning (CT)` be `contrastive learning (CL)` ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5668/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5668/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5668/Reviewer_tKHs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5668/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698625056499,
        "cdate": 1698625056499,
        "tmdate": 1699636590966,
        "mdate": 1699636590966,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EhGA6Vt3Op",
        "forum": "MpWRCiw8g5",
        "replyto": "MpWRCiw8g5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5668/Reviewer_o7vs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5668/Reviewer_o7vs"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces JOSENet, a network for video violence detection. It contains a pretraining part and a detection part. Given the RGB and Flow inputs, a two-stream flow gated network (FGN) is firstly pretrained on UCF-101, HMDB-51 and UCF-Crime datasets using VICReg method. Then, the pretrained FGN weights are used to initialize the FGN in the detection part. In this way, the model requires less training data and generalizes better. In addition, some optimization of the network improves the efficiency of the model in terms of memory consumption and computation load. The proposed method is evaluated on RWF-2000 dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The overall idea is easy to understand and makes sense.\n2) By efficient implementation, the model requires less memory and less frames for each segment.\n3) The model leverages self-supervised learning to improve the generalization of the model."
            },
            "weaknesses": {
                "value": "1) The goal of the paper is violence detection, but there is no related contents in the method part. Necessary components such as loss function of violence detection should be included. \n2) The proposed \u201ccomputational enhancement\u201d is just hyper-parameter tuning. N_s=7.5s is found to be the optimal. However, different datasets may have different optimal parameters. More justification are needed to demonstrate the generalization performance. \n3) The theoretical contribution is limited. The pretraining part is borrowed from VICReg and the detector is borrowed from FGN. \n4) The proposed method is only evaluated on RWF-2000 dataset, which is not enough. I suggest authors to include results on more datasets since you claim the proposed method generalizes better. \n5) Missing comparison with recent methods such as:\n[1] Islam, Zahidul, et al. \"Efficient two-stream network for violence detection using separable convolutional lstm.\" 2021 International Joint Conference on Neural Networks (IJCNN). IEEE, 2021.\n[2] Garcia-Cobo, Guillermo, and Juan C. SanMiguel. \"Human skeletons and change detection for efficient violence detection in surveillance videos.\" Computer Vision and Image Understanding 233 (2023): 103739.\n6) Compared with other methods, the proposed method uses addition training data (UCF-101, HMDB-51, and UCF-Crime). This may be a concern the comparison is not fair.   \n7) The related work of violence detection is incomplete, it should contains more recent methods and discussion. \n8) To demonstrate the efficiency, a comparison with other methods should be included."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5668/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5668/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5668/Reviewer_o7vs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5668/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698631030270,
        "cdate": 1698631030270,
        "tmdate": 1699636590872,
        "mdate": 1699636590872,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "V8I3j2tdsk",
        "forum": "MpWRCiw8g5",
        "replyto": "MpWRCiw8g5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5668/Reviewer_94ds"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5668/Reviewer_94ds"
        ],
        "content": {
            "summary": {
                "value": "Paper proposes a novel violence detection framework which combines 2 features, 2 spatiotemporal streams (RBG + optical flow) and self-supervised learning (SSL).\n\nThe design is more efficient in memory usage (75%) and inference speed (2-fold). For the SSL, the paper adopts the VICReg which is more memory efficient.\n\nEmpirical experiments were done with RWF-2000, HMDB51, UCF101 and UCFCrime. The proposed framework was compared against the SOTA SSL methods: InfoNCE, UberNCE and CoCLR for the UCF101 dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Paper's proposed method is more efficient in memory and inference speed compared to the original baseline methods.\n2. The motivation for the design is well explained."
            },
            "weaknesses": {
                "value": "1. Novelty is highly limited. The combination of optical flow with RGB has been used in multiple prior work. See references.\nThe novelty of SSL is also limited as it is a direct implementation of VICReg.\n\n2. Experimental design is confusing and does not directly support the core claim of the paper. Only SSL-based SOTA algorithms were directly compared with the proposed method for one single dataset (UCF101). There were several experiments on JoseNet based methods. But these experiments are not relevant to demonstrate the core claim of the paper \"outstanding performance for violence detection\" against other SOTA methods.\n\n3. (minor) Writing style is informal and not well-structured. This is especially for the experiment section. E.g. \"We\nhave noticed that we do not reach the state-of-the-art performances for RWF-2000. However, this is not a big deal in a deployment application.\". There is no reference to which experiment this statement refers to (which Table).\n\nReferences\n\nDiba, A., Pazandeh, A. M., & Van Gool, L. (2016). Efficient two-stream motion and appearance 3d cnns for video classification. arXiv preprint arXiv:1608.08851.\n\nWang, G., Muhammad, A., Liu, C., Du, L., & Li, D. (2021). Automatic recognition of fish behavior with a fusion of RGB and optical flow data based on deep learning. Animals, 11(10), 2774.\n\nLi, S., Zhang, L., & Diao, X. (2020). Deep-learning-based human intention prediction using RGB images and optical flow. Journal of Intelligent & Robotic Systems, 97, 95-107."
            },
            "questions": {
                "value": "Why is the comparison against SOTA limited to SSL methods for a single UCF101? This is insufficient to show the generalization of the claim of superior performance of the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not applicable."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5668/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834663110,
        "cdate": 1698834663110,
        "tmdate": 1699636590782,
        "mdate": 1699636590782,
        "license": "CC BY 4.0",
        "version": 2
    }
]