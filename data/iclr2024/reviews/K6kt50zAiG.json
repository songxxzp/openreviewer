[
    {
        "id": "Qgc2zlgYR7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3672/Reviewer_EfKd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3672/Reviewer_EfKd"
        ],
        "forum": "K6kt50zAiG",
        "replyto": "K6kt50zAiG",
        "content": {
            "summary": {
                "value": "Update on November 19:\n\nI raise my score to 6 based on the authors' responses. I am willing to keep discussing with the authors and the other reviewers to achieve a fully discussed final score.\n\n---\nThis paper point out the shifting equivalence of the MILP problems. Then, the authors propose a contrastive learning approach based on the augmented MILP problems. Experiments demonstrate that CAMBranch achieves high performance with only 10% of the complete dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Practical motivation. In practice, I found the similar problem that generating expert demonstration for industrial-level datasets is extremely time-consuming. Thus, the motivation of this paper is practical.\n2. Clear writing. The paper is clearly structured and easy to go through flow.\n3. Simple and effective approach. The idea of the shifting equivalence is simple and effective. Previous research uses GNN to tackle the symmetry in row and column orders. In this paper, the authors explicitly use the shifting equivalence via contrastive learning to enhance the training efficiency."
            },
            "weaknesses": {
                "value": "1. The illustration of experimental results requires to be improved. The bars missing detailed values on it. Compared with histograms, tables could be more compact for demonstration.\n2. Missing comparisons to other ML approaches. Employing auxiliary tasks is an effective way to improve the training efficiency. Empirically, I found the simple auxiliary task employed in [1] can efficiently promote the training efficiency. Thus, can you provide more results comparing this data augmentation approach to other auxiliary tasks? \n3. Marginal improvement in Figure 1. It seems the improvement of CAMBranch is not significant. Maybe a lighter GNN with fewer hidden layers and hidden nodes can achieve similar IL accuracy but requires less expert data."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3672/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3672/Reviewer_EfKd",
                    "ICLR.cc/2024/Conference/Submission3672/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3672/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697458232018,
        "cdate": 1697458232018,
        "tmdate": 1700342584195,
        "mdate": 1700342584195,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dPtd0Wt3pT",
        "forum": "K6kt50zAiG",
        "replyto": "K6kt50zAiG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3672/Reviewer_8jS2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3672/Reviewer_8jS2"
        ],
        "content": {
            "summary": {
                "value": "The authors propose **CAMBranch** an innovative approach that seeks to enhance the efficiency of Branch and Bound (B&B) algorithms for solving Mixed Integer Linear Programming (MILP) problems. Traditional B&B methods, while reliable, can be computationally intensive, motivating the exploration of machine learning to improve branching decisions. In particular, **CAMBranch** employs a machine learning framework based on contrastive learning and uses Augmented MILPs (AMILPs) to inform its branching strategies.\n\nThe key innovation of in the work lies in its utilization of AMILPs, which are equipped with expert decision labels, making them suitable for imitation learning. This approach aims to mimic the performance of Strong Branching, a highly effective but computationally expensive B&B policy. By leveraging the relationships between MILP and AMILP, the method extracts and utilizes node features from an augmented bipartite graph to inform its branching decisions.\n\nThe strengths include its novel application of contrastive learning in the domain of optimization and its potential to significantly reduce the computational burden associated with Strong Branching. The use of AMILPs for imitation learning could lead to more efficient and informed branching strategies, potentially improving the overall performance of B&B algorithms.\n\nHowever, the success of the proposed algorithm hinges on the accuracy of the imitation learning model. If the model fails to capture the subtleties of Strong Branching, it could result in suboptimal decisions. Additionally, the integration of machine learning into B&B algorithms may introduce computational overhead, which could diminish the anticipated efficiency gains. Lastly, the generalizability of **CAMBranch** across diverse MILP problems remains to be thoroughly evaluated, as its effectiveness may vary depending on the problem's characteristics."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Strengths:\n\n* Innovation in Branch and Bound (B&B) Methods: The paper discusses recent advancements in machine learning frameworks to enhance B&B branching policies for solving Mixed Integer Linear Programming (MILP), indicating a focus on innovation and improvement of existing methods.\n* Potential for Improved Efficiency: If these machine learning-based methods can successfully imitate Strong Branching, they may offer more efficient alternatives to traditional B&B methods, which can be computationally intensive."
            },
            "weaknesses": {
                "value": "Potential weaknesses of CAMBRANCH:\n\n1. **Fidelity of Imitation Learning:** The effectiveness  is predicated on the assumption that imitation learning can closely approximate the decisions made by Strong Branching. However, Strong Branching is known for its nuanced decision-making process, which considers a multitude of factors and potential future states. Replicating this complexity through imitation learning is challenging. If the learning model fails to encapsulate the depth of Strong Branching's strategy, the resultant branching decisions could be significantly less efficient, negating the primary advantage of CAMBRANCH.\n\n2. **Computational Overhead of Machine Learning Integration:** While machine learning models offer the promise of improved decision-making, they also introduce computational overhead. Training models, especially those based on contrastive learning, require significant computational resources. Furthermore, the real-time application of these models within the iterative B&B process could lead to increased computational demands, potentially offsetting the efficiency gains from more informed branching.\n\n3. **Sensitivity to Hyperparameter Tuning:** Machine learning models, particularly those used in imitation learning, are sensitive to hyperparameter settings. The performance of CAMBRANCH could be highly dependent on the choice of learning rate, batch size, and other hyperparameters. Finding the optimal configuration can be a time-consuming process that requires extensive experimentation and computational resources.\n\n4. **Generalizability and Robustness Concerns:** The diversity of MILP problems poses a significant challenge to the generalizability of CAMBRANCH. Different MILP instances can vary drastically in terms of size, structure, and complexity. CAMBRANCH must demonstrate robust performance across a wide array of problems to be considered a viable alternative to existing B&B methods. Additionally, the model's robustness to adversarial inputs or unusual problem structures remains to be thoroughly evaluated.\n\nIn conclusion, while CAMBRANCH presents an innovative approach to improving B&B algorithms for MILP problems, its success is contingent upon overcoming the intricate challenges associated with imitation learning, computational efficiency, hyperparameter sensitivity, and the robustness of its application across diverse problem sets."
            },
            "questions": {
                "value": "1. **How does the model's architecture influence the quality of imitation learning?**\n   - The architecture of the neural network used for imitation learning plays a crucial role in its ability to capture the decision-making process of Strong Branching. How does the choice of architecture impact the model's ability to generalize across diverse MILP instances?\n\n2. **What is the impact of contrastive learning's positive and negative sample selection on the performance?**\n   - In contrastive learning, the selection of positive and negative samples is critical for learning meaningful representations. How does CAMBranch ensure the selection of informative positive and negative samples during training? Could the incorporation of hard negative mining or other advanced sampling strategies improve the model's performance?\n\n3. **How does the algorithm address the exploration-exploitation trade-off during the B&B process?**\n   - The Branch and Bound algorithm involves a delicate balance between exploring new branches and exploiting known promising paths. How does CAMBranch navigate this trade-off? Are there mechanisms in place to prevent premature convergence on suboptimal branches or to encourage exploration when necessary?\n\n4. **How does CAMBranch handle the interpretability and explainability of its branching decisions?**"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3672/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698864942301,
        "cdate": 1698864942301,
        "tmdate": 1699636323734,
        "mdate": 1699636323734,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eAKTDVd6fy",
        "forum": "K6kt50zAiG",
        "replyto": "K6kt50zAiG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3672/Reviewer_mGEq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3672/Reviewer_mGEq"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes CAMBranch, which generates augmented MILPs with shifting variables and identical variable selection decisions, and uses contrastive learning ot use the augmented MILPs for learning to branch. Experiments demonstrate that CAMBranch can effectively improve the sample efficiency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper identifies the challenge of  sampling time in Learn2Branch, which is meaningful.\n2. The augmentation strategy has a theoretical gurantee and is insightful."
            },
            "weaknesses": {
                "value": "1. CAMBranch does not perform well on relatively easy datasets. The improvement is not significant compared with GCNN baseline on many medium and hard datasets. Fow example, results in Table 8 do not reveal that CAMBranch consistently outperforms GCNN.\n2. The augmentation method is tailored for the branching task, while not easy to be transfered to other algorithms in B & B solvers. Therefore, the application is limited.\n3. Which sample ratio, with CAMBranch, leads to a comparative results with the model trained with the full dataset? What if use the full training dataset  with this augmentation? Will it stll bring improvement?\n4. How many instances does it need to generate 20k expert samples? The 10% means using 10% MILP instances or 10% expert samples? How many expert samples can one instance produce? Experiments should be conduct to investigate the effect of the number of MILP instances and the expert samples. Also, the accumulative time used in collecting the expert samples should be reported."
            },
            "questions": {
                "value": "1. Sometimes GCNN (10%) even outperforms GCNN (e.g., MIS in Figure 2). Why?\n2. See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3672/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3672/Reviewer_mGEq",
                    "ICLR.cc/2024/Conference/Submission3672/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3672/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698895874521,
        "cdate": 1698895874521,
        "tmdate": 1700669007627,
        "mdate": 1700669007627,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "R24dUnvATR",
        "forum": "K6kt50zAiG",
        "replyto": "K6kt50zAiG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3672/Reviewer_vbTP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3672/Reviewer_vbTP"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a data-efficient imitation learning algorithm for learning a branching policy in B&B algorithm.\nThe proposed method augments demonstration data of Strong Branching decisions using mathematically complete rule and introduces contrastive learning by using the augmented data as positive samples.\nExperiment results demonstrate that the proposed method can significantly outperform existing imitation learning-based MILP solvers."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method of augmentation rule and contrastive learning idea is well-motivated, concise and effective.  \nThe performance improvement from previous method in low-data situations is significant."
            },
            "weaknesses": {
                "value": "The presentation of main result should be improved.\n- CAMBranch (100%) result should be discussed. Despite this work aiming in data scarce situation, understanding how it behaves with the full dataset is critical information, considering the results with 10% of data is not outperforming the baseline using full data.\nEven if CAMBranch using the full data can't outperform the baseline using full data, it is still beneficial to share this limitation with the community if proper discussion is provided.\nHowever, without the result, I am negative about introducing this method to the community as a MILP solver because it is unknown whether it limits the potential of imitation learning-based MILP methods.\n- The main results appear to be promising but could be presented better. Neither Figure 2-4 nor Table 5 looks optimal in their current forms.\n- The result of Table 8 deserves to be in main text, considering the goal of this work. Also, evaluation in another domain with more dramatic performance gap could be better for the comparison. The presentation also has room for improvement; it is hard to compare the metrics of models using the same amount of data.\n\nIf this concern is resolved I am willing to vote for accepting this paper.\nHowever, as it seems that a major revision is required, including the core results of the paper, I also agree if other reviewers or AC recommend submitting this work to next venue."
            },
            "questions": {
                "value": "What would be the intuition behind using shifted geometric mean for evaluation?  And what is the $s$ value for each metric?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3672/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3672/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3672/Reviewer_vbTP"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3672/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1700038880450,
        "cdate": 1700038880450,
        "tmdate": 1700656001789,
        "mdate": 1700656001789,
        "license": "CC BY 4.0",
        "version": 2
    }
]