[
    {
        "id": "ayw3Ytu5VA",
        "forum": "4VIgNuQ1pY",
        "replyto": "4VIgNuQ1pY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2125/Reviewer_gG9F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2125/Reviewer_gG9F"
        ],
        "content": {
            "summary": {
                "value": "The problem of learning neural stochastic differential equations (NSDE) to solve classification/interpolation tasks in the context of (irregularly sampled) time series data is considered. The authors focus on the analysis of theoretically well-defined SDE classes that exhibit desirable properties in terms of parameterization of drift and diffusion coefficients by neural networks, e.g., in so-called NSDEs. In contrast to naive NSDEs, which can (theoretically) learn almost arbitrary classes of functions, the authors restrict themselves to classes of SDEs for which (i) a (uniquely) strong solution exists, (ii) which can be approximated in a numerically stable manner, and (iii) which remain robust to input perturbations. In addition, they build on concepts from the field of controlled differential equations, which are known to improve model performance on irregularly sampled time series data.  Extensive experiments with established benchmark data sets are included. These provide empirical evidence for the proposed improvements."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I appreciate the idea of taking a step back from the state of unlimited expressiveness in NSDE and instead concentrating on sub-classes of SDEs that have favorable properties that combine well with the functional class properties of neural networks. The authors reveal shortcomings associated with the use of unrestricted parameterization of drift and diffusion coefficients by standard neural networks. In turn, an ablation study empirically supports the assumption that careful design of drift and diffusion coefficients is indeed reflected in improved model performance.\nThe content of the paper is well organized, original to the best of my knowledge and shows no obvious spelling or grammatical flaws.\nLast but not least, I enjoyed the theoretically analysis of robustness under distribution shift."
            },
            "weaknesses": {
                "value": "1. I don't see the necessity to include the details on Neural ODE and CDE into the main manuscript. However, thats more ore less a \nmatter of taste.\n2. (Section 4.2) Comparing such a rich variety of models is challenging. The main difficulty arises due to major differences in model structure. E.g., vanilla RNN based methods are by nature not capable to process irregularly sampled time series. As reported, data imputation strategies must be applied additionally. Another challenge arises from comparing methods that include a control mechanism (e.g., Neural CDE) with methods that do not (e.g., NSDE). The former are able to continuously correct the sampled trajectories over time during learning, while the latter can largely only adjust the initial state. However, the authors elaborate on the latter problem in Table 11, where the proposed methods nevertheless showed their advantage. However, I can't escape the impression that the built-in control mechanism is a big part of the success; because Neural CDE often takes the closely followed second place.\n\nNevertheless, I am on the positive side of this work."
            },
            "questions": {
                "value": "1. For example, to evaluate robustness empirically, *explicit Euler* is used for all experiments (see page 25). What are the reasons for this choice? I am very curious about the impact of different numerical solution methods on these and other results. Can you explain this in more detail?\n2. Are you planning to release your Code which would unlock reproducibility of the results?\n\nMinor:\n\n3. Aren't the initial state in the Eq. (2) and (3) supposed to be vectors and therefore should be bold?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "--"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2125/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2125/Reviewer_gG9F",
                    "ICLR.cc/2024/Conference/Submission2125/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2125/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698776952802,
        "cdate": 1698776952802,
        "tmdate": 1700567470783,
        "mdate": 1700567470783,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EwwXgJy0K2",
        "forum": "4VIgNuQ1pY",
        "replyto": "4VIgNuQ1pY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2125/Reviewer_Q7tH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2125/Reviewer_Q7tH"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces three stable classes of Neural SDEs (Langevin-type SDE, Linear Noise SDE, and Geometric SDE) to capture complex dynamics and improve robustness under distribution shifts in time series data. Theoretically, this paper shows the existence and uniqueness of the solutions of these SDEs, and presents their performance guarantee under distribution shifts. Extensive experiments are conducted to validate the good performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Topic-wise, modeling time series data with irregular sampling intervals and missing values is an essential research topic and of great importance in practice.\n\nTheory-wise, this paper proves the existence and uniqueness of the solutions of the proposed three SDEs, and shows their robustness to input data. \n\nAdditionally, extensive numerical results are presented to compare the proposed method with existing algorithms for time series modeling."
            },
            "weaknesses": {
                "value": "The computational complexity of the proposed method is certainly high. \n\nThis paper lacks sufficient details on the implementation of the method, especially when there are irregular time steps and missing data in the series.\n\nFor time series data, in addition to interpolation and classification tasks, it would also be meaningful to consider forecasting tasks as well, which seems to be absent in this paper."
            },
            "questions": {
                "value": "Detailed discussions on the training procedure are needed, especially for dealing with irregular time steps and missing data. For instance, with different irregular time steps across different sample time series, is it still possible to train the algorithm using mini-batch optimization; and is there a way to improve the computational efficiency in practice? When there is missing data in the sequence, how do we deal with missing values in the training phase -- are these missing values being imputed or ignored during the pre-processing step?\n\nMore explanations are needed for Figure 1 \u2014 line (i) exhibits a constant loss, and in fact, most of the loss trajectories are not satisfactory, with unstable trends and not decaying with the increase of epochs.\n \nFrom Table 5, it is a bit confusing why there is no result for LSDE, LNSDE, and GSDE-\u2018+Z\u2019.\n\nThe downstream tasks considered in this paper are interpolation and classification; it could also be meaningful and worthwhile to consider the prediction task for time series data as well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2125/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698787087815,
        "cdate": 1698787087815,
        "tmdate": 1699636145093,
        "mdate": 1699636145093,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dRzq95ht0o",
        "forum": "4VIgNuQ1pY",
        "replyto": "4VIgNuQ1pY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2125/Reviewer_8YRs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2125/Reviewer_8YRs"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenges posed by irregular sampling intervals and missing values in real-world time series data. The authors propose three classes of Neural Stochastic Differential Equations (Neural SDEs) to improve robustness under distribution shifts in time series data. The proposed Neural SDEs include Langevin-type SDE, Linear Noise SDE, and Geometric SDE. The study demonstrates the robustness of these Neural SDEs theoretically and through extensive experiments, showing their effectiveness in handling real-world irregular time series data and maintaining excellent performance under distribution shifts due to missing data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors provide a solid theoretical foundation for the proposed Neural SDEs, including the existence and uniqueness of solutions. \n\nThe robustness section in the paper provides valuable insights into the proposed Neural SDEs' resilience against distribution shifts and input perturbations. \n\n The paper conducts extensive experiments to validate the effectiveness of the proposed Neural SDEs. The models are tested on various datasets, and their robustness is analyzed under different missing rates, providing a comprehensive evaluation."
            },
            "weaknesses": {
                "value": "I didn\u2019t identify major weakness in this paper; however, there are a few minor concerns that I would like to address:\n\n1. Section 3.4 could be more explicit in detailing how the controlled path is incorporated into the Neural SDEs. More comprehensive explanations or illustrations could help in understanding the model\u2019s architecture and functionality better.\n\n2.  While section 3.3 discusses the robustness of the proposed Neural SDEs under distribution shifts, it might benefit from a more thorough exploration or comparison with other neural SDEs' robustness aspects in related works.\n\n3. What is the $\\| \\sigma_\\theta  \\|$ in Theorem 3.6 ?\n\n4. In the experiments of missing data (Table 4), the proposed Neural SDEs show only marginal improvements when compared to the Neural CDE model. How do the theoretical bounds in Theorems 3.5 and 3.6 relate to the robustness of the Neural SDEs in practical implementations? Are these bounds tight or rather loose in actual application scenarios?\n\n5. The implementation code is not provided."
            },
            "questions": {
                "value": "The details of the proposed models seem unclear.  Can you clarify which neural networks are used in the diffusion and drift functions of each Neural SDE?  \n\nAdditionally,  how are the proposed neural SDEs solved, especially the neural GSDE?   Do you employ numerical solvers for the Neural SDEs? If so, which specific solver was utilized, and was there any ablation study conducted to evaluate its effectiveness?\n\nHow much computational time is required for training the proposed Neural SDEs, and what is the complexity of these models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2125/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698842526482,
        "cdate": 1698842526482,
        "tmdate": 1699636145012,
        "mdate": 1699636145012,
        "license": "CC BY 4.0",
        "version": 2
    }
]