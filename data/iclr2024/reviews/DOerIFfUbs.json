[
    {
        "id": "I47Sf0czsu",
        "forum": "DOerIFfUbs",
        "replyto": "DOerIFfUbs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1889/Reviewer_SDrz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1889/Reviewer_SDrz"
        ],
        "content": {
            "summary": {
                "value": "This paper claims that crontrastive pretraining from scratch is computationally demanding and masked image modeling will introduce training-finetuning inconsistency problem. Thus, they use the CLIP model as the teacher, to train a vision transformer. When performing distillation, they do not align the masked token as before, but the unmasked token instead. In zero-shot and finetuning experiments, the proposed method gains a large improvement."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The method is described clearly.\n\n- The workload of the experiment is heavy."
            },
            "weaknesses": {
                "value": "- The biggest weakness is the motivation. Prior works used the masked tokens as the prediction tokens to learn\u00a0their representation. However, in this paper, they predict the unmasked token instead. Thus, I wonder what information can masked token provide if the model aims to predict the unmasked token. In the former method, they provide the context information with the unmasked tokens. The learning of these former methods makes the context increasingly effective.\u00a0But in this paper, since unmasked tokens are represented with no difference from the\u00a0normal tokens,\u00a0what novel information can the model learn by\u00a0recovering such\u00a0unmasked\u00a0tokens based on similar\u00a0normal tokens?\u00a0Or to say the least, the method proposed by the author is similar to the simple distillation process which drops tokens randomly.\n- In section 2.2, the author claims that \u2018It causes training-finetuning inconsistency and makes the trained ViT unable to perform zero-shot classification without fine-tuning\u2019. In my opinion, this description is not accurate because the trained ViT could be able to transfer to unseen domains no matter whether it is with or without [MASK] token training. So I hope the author can present citations or experimental results to prove this statement.\n- In terms of technical novelty, this paper lacks some careful design. Besides, there seems to be a weak connection between the part of the ablation study and motivation, such as **the positional embedding** analysis. Since ICLR is a top-tier conference, this paper also lacks a solid theoretical foundation or forward-looking research direction and should be revised."
            },
            "questions": {
                "value": "1. In section main result, I only see some statistics, instead of any analysis about increase or decrease. Could you not only present the number which I can see from the figure or table but also give some details about the advantages or disadvantages of the method you proposed?\n2. In Tab. 3, could you explain why you useLLaVA-Bench which is often used to test the multimodal instruction ability? And why the UTA with G/14 model did not beat that of L/14 model but exceeded in conversation and reasoning by a large margin?\n3. From the results of Tab 4 and Tab 5, it can be seen that the proposed UTA just achieves a very minor improvement compared to\u00a0EVA-02.\u00a0How to further justify the superiority of your method in such settings?\n4. In \u2018UTA for pre-training the textencoder\u2019, there is an interesting result, the same method does help to visionencoder, but not the text encoder. Could you give more discussion? I wonder whether your motivation is convincing and why it does not work in text modality?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1889/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1889/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1889/Reviewer_SDrz"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1889/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698465497260,
        "cdate": 1698465497260,
        "tmdate": 1699636119280,
        "mdate": 1699636119280,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RWKPL6AkXY",
        "forum": "DOerIFfUbs",
        "replyto": "DOerIFfUbs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1889/Reviewer_g84W"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1889/Reviewer_g84W"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to distill the knowledge of a pretrained CLIP model and further enhance its vision-language representations. The core idea is to train a VIT with the masked image modeling objective that aligns the unmaked token embeddings with the CLIP embeddings. After performing the masked image modeling, the VIT and CLIP text encoders are further finetuned with an image-text contrastive loss. The experiments are conducted on a wide range of benchmark datasets and show very promising performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-It is a good idea to perform unmask token alignment with the pretrained CLIP model due to its efficiency. \n\n-The experiments are quite extensive including many downstream tasks. The method also achieves the SOTA on most of the datasets."
            },
            "weaknesses": {
                "value": "-Motivation of reducing pretraining costs is not convincing. In particular, the abstract claims that \"training CLIP from scratch on noisy web-scale datasets is computationally demanding\". Although this is true, this paper does not solve this issue at all because it still relies on a pretrained CLIP model at the first place.\n\n-The performance gains seem to come from the contrastive finetuning rather than the proposed unmasked alignment pretraining. In Table 1, comparing with the CLIP teacher i.e., EVA-CLIP, there is always a performance drop for UTA without finetuning. This is concerning because this paper claims the unmasked alignment pretraining as one of the main contributions. \n\n-One important baseline is missing. Since finetuning is very effective for zero-shot image classification, this paper should also compare with the CLIP teacher i.e., EVA-CLIP,  that is also finetuned on the same dataset. \n\n-Improving the VIT efficiency by dropping masked tokens has been done in [A].  This paper fails to cite this important reference paper and claims it to be something new. \n\n[A] Li et al., Scaling Language-Image Pre-training via Masking. CVPR 2023\n\n-The improvement in Table 4 & 5 (detection and segmentation) is marginal (< 1%). This is also concerning. The method seems to be limited to image-level prediction tasks."
            },
            "questions": {
                "value": "The authors are highly encouraged to address my questions mentioned in the weakness. In addition, I have the following questions. \n\n-Is the CLIP teacher-model always the giant EVA-CLIP? \n\n-It would be good to provide the CLIP teach results in Table 3, 4, & 5.\n\n-This paper says that it is following previous works to perform the second-stage contrastive finetuning without proving the reference. Please provide the reference. \n\n-Is finetuning helpful in Table 4 & 5?\n\n-Is masking strategy applied in the finetuning stage?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1889/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1889/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1889/Reviewer_g84W"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1889/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698703233816,
        "cdate": 1698703233816,
        "tmdate": 1700864642753,
        "mdate": 1700864642753,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oys9gdNVLa",
        "forum": "DOerIFfUbs",
        "replyto": "DOerIFfUbs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1889/Reviewer_4GX4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1889/Reviewer_4GX4"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a Unmasked Token Alignment (UTA) strategy to improve the performance of pre-trained ViT. It achieves higher performance than CLIP."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-The proposed method is a universal strategy to improve the learned representation. The learned features can be well-used on various downstream tasks.\n\n-Compared with MAE, it shows significantly higher performance on ImageNet. It is interesting to discuss which pre-trained strategy is better."
            },
            "weaknesses": {
                "value": "-Will the new module bring extra training cost?\n\n-It may be unfair to directly compare UTA and MAE, as UTA uses an extra tearcher but MAE is only trained with itself."
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1889/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698843620434,
        "cdate": 1698843620434,
        "tmdate": 1699636119129,
        "mdate": 1699636119129,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "L0933xUDpv",
        "forum": "DOerIFfUbs",
        "replyto": "DOerIFfUbs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1889/Reviewer_WgGM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1889/Reviewer_WgGM"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an approach to train a ViT with the target of \"aligning\" with existing CLIP visual tokens. Specifically, during training, part of the image tokens are masked out and the learning target is to \"align\" the resulting embedding with the unmasked portion of the CLIP visual tokens. After the pre-training stage, the model can be further fine-tuned on image-text pair data to further enhance its cross-modal capability. The authors conduct extensive experiments on benchmark datasets that show good performance on a number of zero-shot image classification, vision-only and vision-language tasks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The approach is very intuitive and technically sound, with good reproducibility.\n- The paper is well written, with clear motivation, approach and detailed experimental results."
            },
            "weaknesses": {
                "value": "- Contribution is small. UTA is essentially a variant of the popular Feature Distillation (FD) approach. Ablation study in table 6 indeed show that the performance of UTA is only slightly better than FD.\n- Zero-shot performance on ImageNet zero-shot is a bit unfair when comparing UTA against open-CLIP / EVA-CLIP, as the former use ImageNet-21k for training (although without labels). A fairer experiments in my opinion would be pre-training UTA on a random set of unlabeled web images."
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1889/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699510562772,
        "cdate": 1699510562772,
        "tmdate": 1699636119057,
        "mdate": 1699636119057,
        "license": "CC BY 4.0",
        "version": 2
    }
]