[
    {
        "id": "LtzmIZBw8m",
        "forum": "HL9YzviPCy",
        "replyto": "HL9YzviPCy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission420/Reviewer_n61f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission420/Reviewer_n61f"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new approach for time-series prediction called Perceiver-Attentional Copulas for Time Series (PrACTiS). \n\nPrACTiS combines the Perceiver IO model with attention-based copulas to enhance time series modeling and improve computational efficiency. The architecture consists of a perceiver-based encoder and a copula-based decoder. It first transforms the input variables into temporal embeddings, effectively handling both observed and missing data points. A latent attention mechanism then maps these embeddings to a lower-dimensional space, reducing computational complexity. The decoder utilizes the copula structure to handle missing data and formulate their joint distribution, which is then sampled to produce predictions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The model is validated through extensive experiments and shows competitive performance against state-of-the-art methods like TACTiS, GPVar, SSAE-LSTM, and deep autoregressive AR. \n\nThe method is sound, even though of low risk of having errors, as it is a simple extension over sota."
            },
            "weaknesses": {
                "value": "The paper heaviliy extends upon TACTiS. They basically change the encoder. Thus novelty is low."
            },
            "questions": {
                "value": "I would like to see more experiments, with at least al TACTiS paper datasets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission420/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697577264031,
        "cdate": 1697577264031,
        "tmdate": 1699635968703,
        "mdate": 1699635968703,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DSZO2nf5Yv",
        "forum": "HL9YzviPCy",
        "replyto": "HL9YzviPCy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission420/Reviewer_Eqmq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission420/Reviewer_Eqmq"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a model called PrACTiS, which combines the perceiver architecture with a copula structure to perform time-series forecasting. The proposed architecture models the timeseries using a compact latent space, which reduces the computational demands. The authors further use local attention mechanisms to capture dependencies within imputed samples. The authors empirically evaluate the proposed method, and show that the proposed method consistently outperforms the state-of-the-art methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- It is interesting to use the copula structure to model timeseries, which gives the model capability to model non-synchronized time series data."
            },
            "weaknesses": {
                "value": "**Contribution of the proposed work is unclear**\n1. The authors claim that the proposed model \u201ccan effectively handle synchronized, non-synchronized, and multimodal data, expanding its applicability to diverse domains\u201d. Does any of the experiments demonstrate the model\u2019s performance on handling non-synchronized or incomplete datasets?\n2. The authors claim that one of the major advantages of the proposed model is that it is memory-efficient. However, (i) the model takes more parameter than many baseline models; (ii) both the parameters and memory usage of the proposed model and baselines are tiny (<1M parameters; <10G memory usage). Why does memory have to be further reduced in this case?\n\n**Inadequate performance**\n1. If none of the selected dataset contains non-synchronized timeseries, have the authors considered using more advanced architecture for benchmarking? E.g. AutoFormer; FedFormer; Informer; PatchTST.\n2. The prediction performance seems really bad based on the visualizations in Appendix. Specifically, as shown in Figure 4, the prediction is not even close to the ground truth values."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission420/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698699798508,
        "cdate": 1698699798508,
        "tmdate": 1699635968624,
        "mdate": 1699635968624,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GIqHETH4D5",
        "forum": "HL9YzviPCy",
        "replyto": "HL9YzviPCy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission420/Reviewer_jRTE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission420/Reviewer_jRTE"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce the Perceiver-Attentional Copulas for Time Series (PrACTiS) architecture based on the TACTiS model to study time-series prediction. The proposed architecture combines the Perceiver model with attention-based copulas. It consists of the perceiver-based encoder and the copula-based decoder, enabling the incorporation of a more general class of copulas that are not exchangeable. To validate the efficacy of the practice, the authors conducted extensive experiments on the unimodal datasets and the multimodal datasets. In addition, the authors conduct memory consumption scaling experiments using random walk data to demonstrate the memory efficiency of PrACTiS."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "By adopting the technique of the Perceiver model, midpoint inference, and local attention mechanisms, the authors successfully address the issue of computational complexity associated with self-attention mechanisms. In addition, the authors thoroughly test their approach on multiple datasets, providing a comprehensive assessment of its performance."
            },
            "weaknesses": {
                "value": "The organization and writing style of the paper appears to suffer from several issues, making it challenging for readers to grasp the presented notions and ideas. Specifically, Section 3 primarily focuses on the intricate details of TACTiS, with much of the content possibly better suited for inclusion in the supplementary material. To improve the flow and readability of the paper, it would be beneficial for the authors to introduce a figure illustrating TACTiS and highlighting the distinctions between TACTiS and the proposed model before delving into detailed explanations in Section 4. This would provide readers with a clearer understanding of the context and facilitate comprehension of subsequent sections.\n\nAdditionally, the authors propose to integrate the Perceiver model as the encoder to enhance the expressiveness of dependence between covariates. The author should give a brief introduction to the  Perceiver model and explain why combining it in the encoder in detail. Considering that a significant portion of the paper is derived from TACTiS and the proposed model appears to be a modification or extension of TACTiS, there are concerns regarding the novelty of the research. It is essential for the authors to explicitly address this issue and clearly articulate the drawbacks of TACTiS and the advancements brought by their proposed model."
            },
            "questions": {
                "value": "1.  The definition, notations, and explanation of section Perceiver-based encoding are not clear. For example, what is the predefined set of learned latent vector $\\vec{u}_k$? What is the latent vector set $\\vec{W}$?\n\n2. Please clarify and elaborate on the objective of introducing midpoint inference. In addition, the mechanism of the midpoint inference is not stated/explained clearly. In addition, please clarify and elaborate on the objective of the local attention.\n\n3. If possible, please add the ablation study by comparing the results of PrACTiS with the results of PrACTiS (but no midpoint inference), the results of PrACTiS (but no variance test), and the results of PrACTiS (but no local attention)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission420/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission420/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission420/Reviewer_jRTE"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission420/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734723691,
        "cdate": 1698734723691,
        "tmdate": 1699635968533,
        "mdate": 1699635968533,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GwlZ9lwNbf",
        "forum": "HL9YzviPCy",
        "replyto": "HL9YzviPCy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission420/Reviewer_JVje"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission420/Reviewer_JVje"
        ],
        "content": {
            "summary": {
                "value": "The paper presents Preceiver-Attentional Copulas (PrACTiS), a new method for time series forecasting. PrACTiS provides an efficient solution by combining Preceiver IO model with attention-based copulas. The idea of combining transformers with Copulas has been proposed in TACTiS [Drouin et al. 2022]. This paper proposes using Preciever IO for the encoder to overcome the computational cost of transformers. Experimental results are presented to validate the performance of PrACTiS."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The points of strengths include:\n\n- The proposed method performs well compared to TACTiS and is more efficient"
            },
            "weaknesses": {
                "value": "The points of weaknesses include:\n\n1- The idea of combining transformers with copulas for forecasting has been proposed in TACTiS [Drouin et al. 2022]. The contribution seems to be limited to replacing transformers with Perceiver IO to increase efficiency which is already a known fact.\n\n2- Some parts of the paper need more clarity such as a summary of contributions."
            },
            "questions": {
                "value": "- Could you please clarify why you did not include more baselines such as Autoformer [Wu et al. 2021]?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission420/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699240084657,
        "cdate": 1699240084657,
        "tmdate": 1699635968435,
        "mdate": 1699635968435,
        "license": "CC BY 4.0",
        "version": 2
    }
]