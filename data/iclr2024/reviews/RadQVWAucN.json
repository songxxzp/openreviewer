[
    {
        "id": "hj8ybdu8yG",
        "forum": "RadQVWAucN",
        "replyto": "RadQVWAucN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2988/Reviewer_u86v"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2988/Reviewer_u86v"
        ],
        "content": {
            "summary": {
                "value": "This paper motivates that directly utilizing the item description as text input features is not yet optimal and proposes to leverage LLM to augment the text feature in order to improve the recommendation. More specifically, the paper proposes templates that prompt the LLM to derive a. recommendation oriented text feature, b.derive neighboring context aware textual summary, as well as the combination of an and b. To use the derived text feature, sbert model is used to encode the text into embeddings, which are used in the downstream recommendation module. Empirical experiments show by applying LLMs such as LLAMA2 and GPT(text-davinci-003) for item description augmentation, the basic MLP model out performs nontrivial baselines. A few other design choices are also empirically evaluated.\n\n\n__Initial Recommendation__: Weak accept based on the reason that the proposed approach is mildly novel while effective on movie and recipe recommendation scenario. The motivation are sound and the proposed approach is justified in the paper."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Although the idea of using LLM-augmented item description as textual feature for enhancing existing recommender models is in gist similar to the KAR approach, the proposed four prompt templates and the way of using the augmented textual feature is mildly novel in the context of LLM for recommendation.\n* The design of prompt templates as well as the combination of different prompts are sound and justified.\n* The empirical study supports the effectiveness claim of the proposed approach."
            },
            "weaknesses": {
                "value": "* The limitation discussion of the proposed approach seems to be missing from the paper. Inherited from LLMs, the proposed approach might have issues like computation efficiency, lacking awareness of latest event and the cost of keeping the model up-to-date, hallucination in general. Discussing these limitations would provide more practical insights about the proposed approach.\n* The proposed approach applied on movie recommendation and recipe recommendation, explore its capability in e-commerce (Like in the KAR paper) or other scenario would further strengthen the generality claim of the paper."
            },
            "questions": {
                "value": "Please see the Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2988/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698697792245,
        "cdate": 1698697792245,
        "tmdate": 1699636243584,
        "mdate": 1699636243584,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jVnOF98Ty5",
        "forum": "RadQVWAucN",
        "replyto": "RadQVWAucN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2988/Reviewer_vwfB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2988/Reviewer_vwfB"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses approaches for augmenting prompting for LLMs in generating recommendations. Four prompting techniques were tested, basic prompting, recommendation-driven prompting, engagement-guided prompting, recommendation+engagement mixed prompting. Empirical results show that these augmentation techniques improve recommendation performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "A nice part of the paper is the comprehensive evaluations of the 4 prompting techniques. There are some valuable insights obtained from the evaluations."
            },
            "weaknesses": {
                "value": "However, I do not see too much novelty from the paper beyond the evaluations. The paper specifically targets personalized recommendation evident from the title. I suppose the engagement-guided and recommendation+engagement mixed prompting can achieve some degree of personalization with the addition of important neighbors in the prompting. However, to achieve true personalization, identifying important neighbors is itself a critical problem. For example, what if there are many neighbors, how do you choose most important neighbors? Do you take raw engagement counts into account? Do you value more recent engagements? The paper does not have not many details on the topics at all. Without these technical details, the paper becomes just empirical evaluating of several prompting rules."
            },
            "questions": {
                "value": "How the proposed method solves personalized recommendation is unclear from the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2988/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699196541453,
        "cdate": 1699196541453,
        "tmdate": 1699636243501,
        "mdate": 1699636243501,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bW73GOmzc1",
        "forum": "RadQVWAucN",
        "replyto": "RadQVWAucN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2988/Reviewer_mt1T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2988/Reviewer_mt1T"
        ],
        "content": {
            "summary": {
                "value": "This paper examines the impact of prompting methods on large language models in the context of recommendation tasks, introducing four distinct techniques: basic prompting, recommendation-driven prompting, engagement-guided prompting, and recommendation+engagement prompting. The objective is to enhance textual representations with knowledge from pre-trained language models. However, the paper lacks substantial technical contributions. The provided prompt templates are simplistic and may not prove effective across different iterations of pre-trained language models due to their rapidly evolving nature. The experiments conducted are insufficient to substantiate the proposed methods' effectiveness, and the scope of application is limited to textual-rich datasets, excluding text-less datasets common in various scenarios."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Demonstrates the potential of large language models for data augmentation in Recommender Systems (RecSys).\n2. Introduces four prompting methods to extract information from Large Language Models (LLMs)."
            },
            "weaknesses": {
                "value": "1. Inadequate experimental validation of the proposed methods.\n2. Limited applicability to datasets lacking textual information.\n3. Fails to make a clear contribution to the field of RecSys."
            },
            "questions": {
                "value": "1. The main contribution revolves around prompt template design, which appears to be a manual fit for pre-trained LLMs. This approach lacks sustainability, considering the rapid changes in LLMs. Table 1 of the experiments highlights this issue, where even basic prompts outperform more intricate ones on Llama2. How can the authors address this limitation?\n\n2. The application scope is restricted to textual-rich datasets. How does the proposed method fare in scenarios where datasets lack textual information, which is a common occurrence?\n\n3. Why did the authors opt for partial ranking evaluation (1,000 items) instead of a full ranking approach, which might provide a more comprehensive assessment?\n\n4. The comparison seems limited, especially considering the absence of a benchmark comparison with ID-based baselines such as DirectAU [1]. How does the proposed method fare against these established techniques?\n\n[1] Wang, Chenyang, Yuanqing Yu, Weizhi Ma, Min Zhang, Chong Chen, Yiqun Liu, and Shaoping Ma. \"Towards representation alignment and uniformity in collaborative filtering.\" In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 1816-1825. 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2988/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2988/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2988/Reviewer_mt1T"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2988/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699256450097,
        "cdate": 1699256450097,
        "tmdate": 1699636243436,
        "mdate": 1699636243436,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ye7rAISHTn",
        "forum": "RadQVWAucN",
        "replyto": "RadQVWAucN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2988/Reviewer_UhyT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2988/Reviewer_UhyT"
        ],
        "content": {
            "summary": {
                "value": "This work explores the effects of different prompting strategies on text-based recommendation by enriching semantic information of textual item descriptions. Based on original item description, this work investigates four enhanced prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting, and (4) recommendation-driven + engagement-guided prompting. Extensive experiments validate the effectiveness of generating additional textual information to improve performance in text-based recommendation task, which enables MLP to achieve comparable or even better performance than complex feature-based methods. Ablation studies also show that it\u2019s possible to concatenate textual descriptions from more than one of prompting strategies to gain further improvement."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tLLM-Rec investigates diverse prompting strategies to enhance personalized text-based recommendations. The prompting strategies of recommendation-driven and engagement-guided exhibit the capability to tap into the language model's comprehension of both general and personalized item characteristics.\n2.\tEmpirical experiments show discernible improvements in recommendation performance by incorporating augmented input text generated by LLMs.\n3.\tProvides insights into the potential of leveraging LLMs for personalized recommendation. Further experiments explore the potential of concatenating descriptions from multiple prompting strategies, enabling better performance.\n4.\tThe overall presentation is clear."
            },
            "weaknesses": {
                "value": "1.\tSome of the mistakes make it confused. (1) In Table 1, there is a wrong bold in the second column. (2) \u201cTable 3\u201din the 7th page is typed as \u201cFigure 3\u201d by mistakes. (3) The citation at the end of the 5th page is not in the right format.\n2.\tThe evaluation of the prompting strategies could benefit from increased interpretability. For instance, there is no experiment validating whether engagement-guided prompting effectively utilizes neighbor information to guide the LLM. It may seem counterintuitive to assert that important neighbor information is considered merely by including item names without a more detailed description and categorization.\n3.\tThe paper lacks a detailed discussion on the potential limitations and challenges of using LLMs for personalized recommendation, such as computational complexity, model interpretability, and potential biases in the generated prompts.\n4.\tIn the main experiment, Table 1 indicates that most of the time, combining both recommendation-driven and engagement-guided strategies leads to a decrease in performance, which appears to contradict the findings in Figure 6 and raises doubts about the generalization of these strategies.\n5.\tThe contribution of this work can be further enhanced if it provides more explainability instead of focus more on improving performance with additional features."
            },
            "questions": {
                "value": "As a researcher, I aim to explore the practical situations where text-based recommendations can be applied, as this is crucial in determining the significance of this study. Could you kindly provide some practical situations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2988/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699355991656,
        "cdate": 1699355991656,
        "tmdate": 1699636243292,
        "mdate": 1699636243292,
        "license": "CC BY 4.0",
        "version": 2
    }
]