[
    {
        "id": "iOWmzrnXnG",
        "forum": "g9diuvxN6D",
        "replyto": "g9diuvxN6D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3958/Reviewer_KbYU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3958/Reviewer_KbYU"
        ],
        "content": {
            "summary": {
                "value": "The study examines the robustness of LLMs in zero-shot prompting. It was observed that the model's performance varies based on the instructions used, particularly with unfamiliar instructions. To mitigate this decline in performance, the study proposes a method that incorporates soft-prompts and trains the model with an additional loss function designed to align the embedding representation of the instructions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This paper provides a thorough comparison of LLM's instruction robustness across various models and sizes. Although the finding\u2014that LLMs are not robust to varying instructions\u2014is anticipated, this paper presents compelling empirical evidence. Additional analysis in the embedding space of the instruction is interesting. Furthermore, the study suggests an elegant solution to narrow the performance disparity between familiar and unfamiliar instructions."
            },
            "weaknesses": {
                "value": "The proposed solution could benefit from more details. Exploring the soft-prompt size (i.e., the variable N) would be nice. The N used is not specified in the main text either, which I believe is an important detail. \n\nThey provide synthetic instruction, and one of the baselines is to fine-tune normally with this new instruction. Interestingly, there is a noticable degradation in performance, yet there's no explanation as to why. Usually, continual fine-tuning on the same data should not damage performance (significantly so). My guess is that the quality of the synthetic prompt is not very good. If so, this raises a question: what if we spend more effort on generating better synthetic instruction beforehand?"
            },
            "questions": {
                "value": "- \"We then form batches by including one instance featuring the original instruction, and the rest comprising paraphrased instructions.\" -> what's the batch size, and consequently, what's the paraphrase size and the final dataset size altogether? \n- Significant degradation on FT on Table 7, why is that so?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3958/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698126247384,
        "cdate": 1698126247384,
        "tmdate": 1699636356933,
        "mdate": 1699636356933,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7iw6UIIDCQ",
        "forum": "g9diuvxN6D",
        "replyto": "g9diuvxN6D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3958/Reviewer_hcVs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3958/Reviewer_hcVs"
        ],
        "content": {
            "summary": {
                "value": "This paper observes that current instruction-tuned LLMs are sensitive to surface forms of instructions by showing that they underperform on unseen instructions compared to seen instructions. To mitigate this issue, the paper proposes soft prompt alignment technique."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper performs experiments on various tasks including reasoning benchmarks (MMLU and BBL) and NLG benchmarks.\n- The paper provides the experimental setting of evaluating the robustness of LLMs on different instructions by releasing various instructions collected from NLP researchers, which might facilitate further research."
            },
            "weaknesses": {
                "value": "- In Tables 3 and 7, compared to the performance difference, the variance is very large, questioning the significance of the difference between observed and unobserved instructions. A significance test should be conducted to test the significance of the difference.\n- Although the title, abstract, and motivation of the paper mention that the paper tries to evaluate the zero-shot robustness of instruction-tuned language models generally, it only focuses on models up to 11B. The tendency is likely to be unpredictable for state-of-the-art LLMs such as GPT-3.5 or GPT-4. \n- How does PT+KL on Table 7 perform on negated instructions? The alignment stage might make the LLM insensitive to most instructions even though the instructions are not relevant or negated, which is a similar finding for instruction-tuned LLMs for Webson & Pavlick (2022) \n\nWebson & Pavlick (2022) - Do Prompt-Based Models Really Understand the Meaning of their Prompts?"
            },
            "questions": {
                "value": "- Do you think that RLHF after instruction tuning might mitigate this issue?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3958/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3958/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3958/Reviewer_hcVs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3958/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698507912009,
        "cdate": 1698507912009,
        "tmdate": 1700721999156,
        "mdate": 1700721999156,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cU1wpQX4pv",
        "forum": "g9diuvxN6D",
        "replyto": "g9diuvxN6D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3958/Reviewer_cGUu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3958/Reviewer_cGUu"
        ],
        "content": {
            "summary": {
                "value": "The authors analyse how sensitive various instruction-tuned models are to rephrased instructions, especially in zero-shot settings. They find that using novel instructions previously unseen by the model results in degraded performance across different tasks, and does not seem to improve with model scale. Further analysis suggests that instructions that are more dissimilar to instructions seen during training generally perform (somewhat) poorer. Finally, they introduce a prompt-tuning method that utilises a KL penalty to punish representations that drift from paraphrased versions of instructions. They show this performs better than simply training on the paraphrased instructions alone."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The experiments are clear and well-thought-out, and the results are solid and interesting, affecting many current popular models (Llama finetunes), and holding for different model families.\n- The observed correlations between performance and instruction similarity (in representation space) are interesting (even if the correlation is somewhat weak).\n- The proposed fix method is well-motivated, and the results seem reasonable, especially for the Alpaca-7B model.\n- The robustness of models to different prompts is an important and interesting area of study, especially considering current difficulties with LM evaluations."
            },
            "weaknesses": {
                "value": "- The novelty of the robustness experiments is somewhat limited, as there is prior work that has observed variance based on prompt formatting (discussed in the work itself). Considering this work seems to examine more modern models such as Alpaca, and considers a different set of tasks, I think this is a minor weakness - the work does add a number of interesting insights, especially around representation distance.\n- The claim that models perform worse at novel phrasings of tasks seems a bit strong compared to the results: examining Table 3, the variance of performance on unobserved phrasings often is greater than the difference in average performance, suggesting that the difference in performance may not be significant (although the large increase in variance is certainly not desirable and interesting to see).\n- While the proposed method seems strong, examining the results in Appendix B.3 seems to show that in many cases, prompt-tuning alone performs within one standard deviation of the PT+KL results (e.g., for BBL unobserved and average). As such, the proposed method might not be significantly different from just prompt tuning.\n\nOverall, I think this paper is okay, and the robustness results are interesting and useful to see, along with the representation alignment results. However, the novelty of the robustness experiments is a bit limited, and it\u2019s not clear the proposed method for improving this is significantly better than baselines."
            },
            "questions": {
                "value": "- How does your proposed method compare to introducing the model paraphrases during initial instruction finetuning? Training on the paraphrased versions alone after the instruction tuning may cause forgetting in an undesirable way.\n- Is the degradation of performance on unobserved prompts statistically significant? The variance seems very high.\n- Doing prompt tuning alone seems to often perform very similarly to PT+KL, and within the reported standard error in Appendix B.3. Do you have statistical tests for significance for your results?\n- It would be interesting to see the results in Table 7 broken down into the BBL QA/BC/MC categories shown in Table 6 to see how much the difference in closest distance affects the performance of these different categories."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3958/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3958/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3958/Reviewer_cGUu"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3958/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698611187270,
        "cdate": 1698611187270,
        "tmdate": 1700588823260,
        "mdate": 1700588823260,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "P5UjYIQS75",
        "forum": "g9diuvxN6D",
        "replyto": "g9diuvxN6D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3958/Reviewer_RNd8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3958/Reviewer_RNd8"
        ],
        "content": {
            "summary": {
                "value": "The paper critiques instruction tuning (IT) by showing that the IT models are not robust to rephrasing of the same prompt that were not seen in training. More surprisingly, using a prompt that 1) was seen in training, but 2) is incorrect for the context has a higher accuracy than an unseen but correct prompt. They propose a mitigation strategy by adding a soft prompt that is explicitly optimized to make the model see the different rephrases as similar to each other. They also release a dataset of 319 prompt rephrasings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This seems like a significant result overall, and explicitly shows an important weakness in IT models\n- In general, this kind of work (exploring what \"success on a benchmark\" really means) is extremely important for the field. You are raising a really important issue with fine tuning, and this is impactful.\n- The experiments in the paper are well-executed, well-presented, and broad across models and datasets.\n- It's interesting to explicitly introduce the rephrasing invariance into the fine tuning data-- it's sort of an  equivalent of regularization, I think? \n- Thanks for including figure 4! It helped a lot with the intuition in that section."
            },
            "weaknesses": {
                "value": "- nit: \"instruction-tuned models are not especially robust to instruction rephrasings\" \u2192 this is very clear, say it first. Better yet, make it the title :) \n- I don't know if just releasing prompt rephrasing is a significant enough contribution-- https://openreview.net/forum?id=9Vrb9D0WI4 and others also have datasets of different rephrasing of prompts for the same task."
            },
            "questions": {
                "value": "- It looks like just fine tuning (or even fine tuning + KL) severely drops performance. Do you know why? This seems very surprising\n- Why use NLP grad students to generate rephrasing of the prompts, when you could just use an LLM (which it seems like you do later for the eval set)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3958/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801897547,
        "cdate": 1698801897547,
        "tmdate": 1699636356648,
        "mdate": 1699636356648,
        "license": "CC BY 4.0",
        "version": 2
    }
]