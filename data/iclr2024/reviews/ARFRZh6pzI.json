[
    {
        "id": "hCT6SOpB9S",
        "forum": "ARFRZh6pzI",
        "replyto": "ARFRZh6pzI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1535/Reviewer_4fqZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1535/Reviewer_4fqZ"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the pressing challenge posed by the deployment of Large Language Models (LLMs). Given the \u201cblack box\u201d nature of LLMs, it is hard to point and correct errors due to factors like hallucination post-deployment. To address this, the authors propose a metacognitive approach named CLEAR (Concept-Learning-Enabled metacognitive intervention framework).\n\nThe CLEAR framework integrates the features in cognitive science to enable LLMs to understand and work with concept-specific sparse subnetworks. These subnetworks aim to provide transparency in decision-making. With K-Means applied to discriminate against the confidence level, the framework could make automatic error identification and guide the allocation of augmented experts to secure a more reliable prediction.\n\nEmpirical studies with the framework are performed on the text classification datasets. Compared to the direct intervention methods, concept bottleneck models, the metacognition intervention achieves better performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The CLEAR framework offers an innovative metacognitive approach, merging insights from cognitive science with large language model intervention.\n2. One of the standout features of CLEAR is its ability to autonomously identify potential mispredictions, reducing the need for human intervention. And the dynamic activation of internal modules for refining concept perception without extra tuning is an efficient way to address errors, adding a layer of adaptability to the LLM."
            },
            "weaknesses": {
                "value": "1. Lack of captions on some figures can disrupt the flow of understanding for the reader. \n2. The paper emphasizes its effectiveness through experiments on real-world datasets. However, the scope and diversity of these datasets aren't detailed (only text classification tasks considered in the paper), raising questions about the framework's general applicability."
            },
            "questions": {
                "value": "1. Considering the increasing scale and complexity of newer models, is the CLEAR framework compatible with larger models such as LLaMA? Furthermore, can the metacognitive capabilities of the CLEAR approach provide benefits when applied to these more extensive and potentially more intricate architectures?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1535/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1535/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1535/Reviewer_4fqZ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1535/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840003590,
        "cdate": 1698840003590,
        "tmdate": 1699636082123,
        "mdate": 1699636082123,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "F4iTScjTXn",
        "forum": "ARFRZh6pzI",
        "replyto": "ARFRZh6pzI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1535/Reviewer_DFsm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1535/Reviewer_DFsm"
        ],
        "content": {
            "summary": {
                "value": "The method proposed CLEAR which is a metacognitive framework to enable LLMs to self-identify and self-correct errors during deployment. The framework contains two components, 1) the concept learning component which maps latent textual representations to concepts and utilizes MoCEs to learn sparse concept subnetworks, and 2) the metacognitive intervention that dynamically labels and edits the intermediate layer for less erroneous outputs. The resulting method CLEAR outperforms prior methods on both CEBaB and IMDB-C. The model is also more accountable due to its interpretable nature and more efficient since its tunning-free intervention."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper proposed a framework that is strong performance transparent and accountable. Usually, interpretability and performance are a trade-off, but this paper manages to improve both.\n2. MoCEs are a great way of generalizing and not overfitting on particular examples. This is a very natural approach to expand the coverage and capacity without having the downside of overfitting.\n3. The inference stage utilizes thresholding and clustering to achieve efficient inference time computation, increasing its practical usability."
            },
            "weaknesses": {
                "value": "1. The concepts are pre-defined. This could potentially limit the quality and use case of such a framework. In scenarios where human-annotated concepts are harder to come by or can potentially be inaccurate, this method doesn't have a preventative mechanism for that if I understand it correctly.\n2. Intervention is tunning-free, but concept-learning components require some finetuning. This method also requires changing the transformer architecture. These raise questions about the adaptability of the framework to a wider range of scenarios."
            },
            "questions": {
                "value": "1. Can this be extended to learn undefined concepts? Or human-annotated concepts have to be provided."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1535/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1535/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1535/Reviewer_DFsm"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1535/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699469938033,
        "cdate": 1699469938033,
        "tmdate": 1699636082004,
        "mdate": 1699636082004,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ue5aDIdoPU",
        "forum": "ARFRZh6pzI",
        "replyto": "ARFRZh6pzI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1535/Reviewer_7QQj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1535/Reviewer_7QQj"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel framework called CLEAR (Concept-Learning-Enabled metAcognitive inteRvention), designed to enhance the reliability of Large Language Models (LLMs) by enabling them to self-identify and correct errors during deployment. CLEAR is inspired by aspects of human cognition and builds upon the Mixture of Experts (MoE) concept. The proposed method was tested on a test classification dataset. The framework aims to mitigate issues related to the black-box nature of LLMs, the reliance on domain experts for error identification, and the challenges of targeted intervention given the complexity and size of these models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written, and the main content is easy to understand.\n2. The paper involves a chain of MoE, self-corrections by expanding experts, and hindsight explanation through backtracking to mitigate the issue of the black-box nature of LLMs. The proposed methods are empirically justified on two NLP text classification datasets."
            },
            "weaknesses": {
                "value": "1. [Novelty] The use of Mixture of Experts on LLMs is not new. The authors should at least discuss the differences between previous works [1,2] and their own.\n2. [Black-box intervention] The model still uses an open-source LLM, T5, as its backbone, enabling access to its intermediate layers $z$ to generate concepts $c$ and labels $y$. This significantly reduces its applicability to other API-based LLM models, like GPT-3 and GPT-4.\n3. [Complexity and Scalability] The authors do not discuss the computational overhead of adding more experts (LLM backbones) and the trade-off between improving model performance and adding extra experts. Furthermore, in Table 2, the authors only test CLEAR on T5-base. It would be interesting to see the behavior of the proposed method on larger LLMs (like LLaMA 2 13B or Mistral 7B), but with fewer expert layers.\n\n[1] Sheng S. et al. Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models\n\n[2] Mikel A. et al. Efficient Large Scale Language Modeling with Mixtures of Experts"
            },
            "questions": {
                "value": "1. I am curious about how CLEAR ensures that the dynamic adjustment of the expert allocation does not lead to overfitting or catastrophic forgetting when fine-tuning on different tasks on T5-base.\n2. Can the Concept Bottleneck Models (CBMs) developed by CLEAR be effectively generalized across diverse domains and applications, or do they require domain-specific tuning?\n3. I am confused about the methods shown in Table 2. For instance, for the method \u2018prompting\u2019 in the first line, are you simply using prompting on GPT-4 without any help of the MoE and self-correction techniques proposed in CLEAR? If so, it would be much more interesting to see the results of prompting on the LLM backbones for different expert layers within the proposed CLEAR framework without training CBMs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1535/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699515962996,
        "cdate": 1699515962996,
        "tmdate": 1699636081932,
        "mdate": 1699636081932,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1dti2L7z4s",
        "forum": "ARFRZh6pzI",
        "replyto": "ARFRZh6pzI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1535/Reviewer_movx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1535/Reviewer_movx"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a study in the context of Large Language Models (LLMs): 1) to automatically identify erroneous inputs, 2) to handle erroneous inputs to increase the model's performance, and 3) to interpret the model's prediction.\n\nThe proposed methodology explores two ways to handle erroneous examples. The first one is \nincreasing the the number of top experts, top-T to make an ensemble decision without further training.\nThe second one is to modify the number of experts in the training process. The authors gradually increase the number of top-T experts after a fixed number of epochs.\n\nThe retrospective accountability component explains similar to the transformer's attention visualization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Provides an interpretation and identifies erroneous examples in LLM blackbox training."
            },
            "weaknesses": {
                "value": "The authors identified erroneous prediction by automatically dividing confidence into two groups via K-means clustering (Section 3.2). The subset of examples with a lower confidence group is flagged as erroneous.\n\nWhile this approach eliminates human involvement, this approach has several limitations. First, the K-means is not robust against outliers. In many real-world scenarios, such as predicting \nmalicious vs. non-malicious users on the web, and predicting bug vs. non-bug features in software security---the class binary classification task is imbalanced. Thus, automatically setting the threshold\nmight not be the right way. \n\nSecond, to handle the erroneous examples, the authors presented two approaches: increasing the number of top-T experts at the inference time and gradually increasing the number of top-T experts. \nHowever, none of the approaches handle \"erroneous\" examples. Instead, both approaches focus on increasing the top-T experts. A better approach could be similar to active learning or machine teaching---focusing on the subset of erroneous inputs and learn to improve prediction accuracy.\n\n\nThird, the approaches described in Appendix A involve extracting latent representation Z of text encoder, x, and then adding an extra layer of concept representation R^K and class label. \nThe authors did not report whether their model overfits."
            },
            "questions": {
                "value": "Did the authors examine the model's overfitting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1535/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1535/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1535/Reviewer_movx"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1535/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699556531199,
        "cdate": 1699556531199,
        "tmdate": 1699636081858,
        "mdate": 1699636081858,
        "license": "CC BY 4.0",
        "version": 2
    }
]