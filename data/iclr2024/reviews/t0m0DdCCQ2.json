[
    {
        "id": "onRfujNVY0",
        "forum": "t0m0DdCCQ2",
        "replyto": "t0m0DdCCQ2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3633/Reviewer_ffAR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3633/Reviewer_ffAR"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce LiteFormer, a variant of the AlphaFold2 Evoformer inspired by Flowformer, a linear transformer. Compared to the original Evoformer, LiteFormer has lower complexity and is claimed to run faster and more memory-efficiently. The authors evaluate LiteFormer on monomeric and multimeric structure prediction."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "It is true that Evoformer has high complexity, and I'm not yet aware of successful applications of the linear transformer literature to the AlphaFold2 architecture. It's also welcome that the authors evaluate on CASP and not just a small CAMEO dataset, like many other papers of this genre."
            },
            "weaknesses": {
                "value": "Some of the performance figures that motivate this entire paper seem questionable. It is claimed in Figure 1 that AlphaFold2 OOMs on sequences of length 800, e.g. If the authors are running inference using the same 80GB A100s they use later in the paper, this simply cannot be true; one can get away with longer sequences even on ColabFold (Mirdita et al., 2022), which runs on free-tier Google Colab GPUs. The authors of OpenFold were able to run an unmodified version of the original AlphaFold2 on sequences of length 2000 on a 40GB A100 (Ahdritz et al., 2022).\n\nSeparately, the choice of ESM-2 150M as a baseline for monomeric structure prediction is extremely confusing. Why not use unmodified AlphaFold2? Why not use a larger ESM-2 protein language model? Why mix and match these figures with AlphaFold2-Multimer evals? In general, details on the evaluation are very light (e.g. which CAMEO proteins were chosen? Why were all evaluation proteins filtered at 500? Which AlphaFold2 implementation served as the baseline?), to the point where it's difficult to know what is being run.\n\nOn top of that, the claimed performance improvements are comparable to or less significant than those of other optimized versions of AlphaFold2, none of which are mentioned here. FastFold, UniFold, OpenFold have all already improved AlphaFold2 with e.g. FlashAttention.\n\nMisc.:\n\n>However, since both the row and column dimensions of the pair representation, along with the row dimension of the MSA representation, are identical to the primary sequence length L.\n\n>We trained 10,000 data for 5 days using 8 \u00d7 DGX-A100 80G GPUs and inference on two multimeric datasets: VH-VL and DB5.5.\n\nThe paper is a bit sloppily written (see examples above). I'm not sure what the first sentence is saying. The second one contains no information about which 10,000 data points were used to train the model."
            },
            "questions": {
                "value": ">CASP14 & CASP15. The 14th & 15th Critical Assessment of protein Structure Prediction, from which we respectively select 42 and 48 single protein targets with sequence lengths less than 500.\n\nWhy filter in this way? The hardest proteins are the longest ones. Almost all entries at CASP15 did well on most of the proteins of length < 500 (excepting some orphans, etc.)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3633/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3633/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3633/Reviewer_ffAR"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3633/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697761571805,
        "cdate": 1697761571805,
        "tmdate": 1699636319023,
        "mdate": 1699636319023,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rUXDLEjkIo",
        "forum": "t0m0DdCCQ2",
        "replyto": "t0m0DdCCQ2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3633/Reviewer_WrTu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3633/Reviewer_WrTu"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Liteformer, a novel and lightweight variant of  Evoformer used in AlphaFold2 for protein structure prediction. Liteformer introduces a new mechanism called Bias-aware Flow Attention (BFA), which linearizes the biased attention for third-order tensors, such as multiple sequence alignment (MSA) and pair representation, with O(L) complexity.  Liteformer reduces the memory consumption and training speed of Evoformer by up to 44* and 23%,  respectively while maintaining competitive accuracy on various protein structure benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper proposes a novel and efficient variant of Evoformer, the core module of AlphaFold2, which is a state-of-the-art model for protein structure prediction.\n- The paper introduces a new mechanism called Bias-aware Flow Attention, which linearizes the biased attention for third-order tensors with O(L) complexity, while preserving the evolutionary and geometric information from MSA and pair representations1.\n- The paper demonstrates the effectiveness and efficiency of Liteformer on various protein structure benchmarks, such as CASP14, CASP15, CAMEO, VH-VL, and DB5.5, showing that it can reduce the memory consumption and training speed of Evoformer by up to 44% and 23%, respectively, while maintaining competitive accuracy."
            },
            "weaknesses": {
                "value": "- Unclear motivation: The author emphasizes the huge computational complexity reduction brought by converting attention module  into a flow network. However, how this method affects the computational complexity is only mentioned at Eeq 7 in Section 3. I think there should be a separate paragraph in the introduction detailing the motivations for using flow network. \n- The claim does not correspond to the experimental results. In Sec 2 and 3, the authors mainly claim that BFA can reduce computational complexity. However, the experimental results show that memory usage is the main advantage of BFA during training. The drop in computation time is less pronounced. The authors in sec 4 simply boil this down to reduced graph overhead when backpropagated. In my opinion, it is not helpful to prove the validity of this method."
            },
            "questions": {
                "value": "Please elaborate on the motivation for using streaming networks and how it leads to a reduction in computational complexity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3633/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698632892755,
        "cdate": 1698632892755,
        "tmdate": 1699636318947,
        "mdate": 1699636318947,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TSuVYMRO9T",
        "forum": "t0m0DdCCQ2",
        "replyto": "t0m0DdCCQ2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3633/Reviewer_9JiD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3633/Reviewer_9JiD"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors raise the problem on the high memory and time cost of the Evoformer module\nused in AlphaFold2.\nTo solve this, they propose a lightweight variant of Evoformer named Liteformer. Through a bias-aware flow\nattention (BFA) mechanism, the complexity of Liteformer is reduced to a lower quantity, compared with\noriginal Evoformer.\nExtensive experiments show the great effectiveness of the proposed method in terms of both memory\noccupation and training acceleration, while keeping the competitive accuracy in protein structure\nprediction."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The propose mechanism is very interesting and useful for the development of applications in recent\nyears.\n2. Existing frameworks equipped with the proposed BFA modules can achieve competitive performance while\nreducing the time of training and inference a lot."
            },
            "weaknesses": {
                "value": "1. In Fig. 2, the pipeline of the blocks in Evoformer and Liteformer is exactly the same. It's better to be combined into one figure. The difference between BFA and original attention is the key to which\nshould be compared like this.\n2. The proposed method shows a great improvement in efficiency and memory cost. But there have\nbeen some more general works that play a similar role, such as memory-efficient attention and flash\nattention [Ref_1]. Can they jointly improve the training of the network? More ablation studies between\nthem should be provided for further comparison.\n3. The proposed BFA module seems more likely to be a general attention module, instead of a theme-related\n(protein-related) approach, which decreases the significance.\n[Ref_1]. Dao, Tri, et al. \"Flashattention: Fast and memory-efficient exact attention with io-awareness.\"\nAdvances in Neural Information Processing Systems 35 (2022): 16344-16359."
            },
            "questions": {
                "value": "What's the principle of selecting the targets from the CASP14, CASP15, CAMEO, and VH-VL, and why\nsequence lengths of them are restricted to be shorter than 500, since Liteformer has a better ability to\nhandle the longer sequences?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3633/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806500482,
        "cdate": 1698806500482,
        "tmdate": 1699636318852,
        "mdate": 1699636318852,
        "license": "CC BY 4.0",
        "version": 2
    }
]