[
    {
        "id": "0uBeOZ6n4i",
        "forum": "lWe3GBRem8",
        "replyto": "lWe3GBRem8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7042/Reviewer_UwhL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7042/Reviewer_UwhL"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the challenge of maximizing policy performance while also leveraging the benefits of exploration bonuses in reinforcement learning (RL). The core idea is to effectively utilize offline data while navigating the challenges posed by data insufficiency and suboptimality.\n\nThe paper introduces the Offline-to-Online-to-Offline (OOO) RL framework. This approach involves:  \nUsing an optimistic exploration policy for environment interaction. \nPessimistically training an exploitation policy for evaluation on all accumulated data, thus removing biases introduced by intrinsic rewards.\n\nThe OOO framework, when evaluated on the D4RL benchmark, led to significant performance improvements."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The most significant strength of this paper is its novel approach to reinforcement learning. Traditionally, prior works have focused on an offline-to-online RL transition, where offline data is leveraged for pre-training, followed by online fine-tuning. This paper, however, introduces the Offline-to-Online-to-Offline (OOO) RL framework. This method effectively bridges the gap between offline and online RL by toggling between them to optimize exploration and exploitation. This innovative loop addresses challenges in data reuse, particularly in scenarios where the available data might be suboptimal or lack comprehensive coverage of the state space.\n\nAnother strength of the paper is the robustness of its experimental results. The OOO framework, when evaluated on established benchmarks like the Adroit manipulation and FrankaKitchen from the D4RL benchmark, shows significant improvements in performance compared to existing methods. Such substantial experimental outcomes underscore the efficacy of the proposed approach."
            },
            "weaknesses": {
                "value": "The paper introduces an Offline-to-Online-to-Offline (OOO) RL method with optimistic exploration and pessimistic exploitation. However, it doesn't provide a detailed analysis or proof regarding the convergence of the method, given that the exploration and evaluation policies are distinct. The convergence guarantees for both policies should be explicitly discussed."
            },
            "questions": {
                "value": "How does the proposed method perform in environments where the reward is not sparse, like gym-mujoco tasks in D4RL? Would its effectiveness diminish in such settings, necessitating the exploration of new exploration and exploitation methods?\n\nCould the author provide guidelines or criteria for selecting an appropriate base exploration and exploitation method for specific environments? In the given environment, it appears that OOO(IQL) consistently outperforms other methods. Why might this be the case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7042/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7042/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7042/Reviewer_UwhL"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7042/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698688080335,
        "cdate": 1698688080335,
        "tmdate": 1699636827298,
        "mdate": 1699636827298,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AJGyiAtB2e",
        "forum": "lWe3GBRem8",
        "replyto": "lWe3GBRem8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7042/Reviewer_vvG4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7042/Reviewer_vvG4"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a 3-stage RL training paradigm: it begins with pre-training a policy using offline data, followed by applying an online algorithm with the pre-trained policy, and ends with an offline algorithm run on the online replay buffer.  However, the paper merely combines these stages and lacks in-depth investigation into the interplay of these components, potentially limiting its contribution."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper's presentation is clear, making it easy to follow, and it includes well-crafted visualizations. Also, the reported performance metrics appear good."
            },
            "weaknesses": {
                "value": "**[Major Concern: Novelty]**\n\nWhen considering this paper as a framework or paradigm proposal, concerns regarding novelty arise:\n\nThe Offline-to-Online-to-Offline (OOO) paradigm can be divided into two parts: offline-to-online and online-to-offline. The former is a well-established concept with numerous dedicated algorithms, lacking in novelty.\n\nThe online-to-offline segment essentially resembles standard offline reinforcement learning, as D4RL datasets are drawn from replay buffers of policies trained by online algorithms.\n\nIf this paper is evaluated as an algorithm-focused work, novelty concerns persist:\n\nThe practical implementation of OOO combines existing off-the-shelf algorithms, specifically IQL/Cal-QL for the offline phase and RLPD and RND for the online phase. These algorithms themselves are not new and may not significantly contribute to the field. The design of the algorithms for the interplay is still not very new, more like some tricks without theoretical support or in-depth investigation.\n\n**[Minor Concerns]** See Questions."
            },
            "questions": {
                "value": "1. In Figure 2d, the authors solely compare to the IQL+Bonus case. To ensure a fair comparison, it is advisable to include a popular online algorithm as a baseline.\n2. The comparison involving TD3+RND in an Offline-to-Online setting may not be suitable. A more reliable setting would entail loading both offline data and the value function from offline pre-training and then train TD3+RND.\n\n3. In the harder-exploration problem paragraph, the authors simply reduced the scale of offline data, leading to the difficulty of policy optimization but not the harder-exploration issue. However, it's crucial to recognize that the hard exploration issue often arises during online RL training and may not directly relate to the OOO framework design.\n\n**[More suggestions]**\nThe underlying paradigm, while interesting, may not be considered highly novel. In order to enhance the prospects of publication, I'd like to suggest a few potential areas for improvement:\n\nA deeper exploration of the intricate dynamics between the three stages is ideally underpinned by robust theoretical analysis.\n\nExpanding the evaluation of your algorithm to encompass more intricate and real-world scenarios, particularly in the realm of physical robotics.\n\nI believe these suggestions have the potential to enhance the overall impact and value of your work greatly. If you could invest some efforts in these directions, it would be highly appreciated, and I would be more inclined to raise the score accordingly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7042/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7042/Reviewer_vvG4",
                    "ICLR.cc/2024/Conference/Submission7042/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7042/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761484974,
        "cdate": 1698761484974,
        "tmdate": 1700555052546,
        "mdate": 1700555052546,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "r15lgxC82m",
        "forum": "lWe3GBRem8",
        "replyto": "lWe3GBRem8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7042/Reviewer_Pgjg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7042/Reviewer_Pgjg"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel training framework called offline-online-offline (OOO), where an exploration policy is initially trained optimistically, followed by online fine-tuning using both online-collected and offline data. Once the online fine-tuning budget is exhausted, a pessimistic update algorithm is used for further policy learning. The OOO framework can be quickly integrated into previous offline-online frameworks or purely online algorithms. A series of experimental results demonstrate that OOO can further enhance the performance of SOTA algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Firstly, the literature review in the paper is extensive, with detailed comparisons and analyses categorized accordingly. \n\nSecondly, the paper's method focuses on how to enhance policy performance during the offline-online process, proposing a new training framework. This framework can be implemented simply and can be quickly integrated into previous offline-online processes. Additionally, it relaxes the constraints on intrinsic rewards during online exploration in the online fine-tune process. The experimental results also demonstrate the effectiveness of the OOO framework."
            },
            "weaknesses": {
                "value": "Currently, I do not spot major issues of the methods and implementations described in the paper. The experimental tasks mainly involve navigation-type maze tasks and several robotic control tasks, which are considered hard exploration problems. Many of these tasks require full space coverage, a requirement that is difficult to meet in complex, open environments.  I wonder whether it is possible to combine other exploration algorithms instead of relying solely on full space coverage exploration algorithms, such as the probing policies used in [1,2]. Besides, if is it possible to enhance the efficiency of fine-tuning through offline training, significantly reducing the number of samples needed for online fine-tuning?  I noticed the steps used for online fine-tuning significantly exceed the size of the original offline data on some tasks.\n\n\n\n1. Offline Model-based Adaptable Policy Learning. NeurIPS 2021.\n2. Learning robust perceptive locomotion for quadrupedal robots in the wild. Sci. Robotics 7(62) 2022."
            },
            "questions": {
                "value": "1. Increasing the coverage of the state/policy space can indeed alleviate the difficulties associated with purely offline training. When it comes to online fine-tuning in this work, the number of steps can exceed the size of the offline dataset. How to make a trade-off between the size of offline datasets and the online fine-tuning steps, so that the framework fits more into offline setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7042/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7042/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7042/Reviewer_Pgjg"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7042/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831362900,
        "cdate": 1698831362900,
        "tmdate": 1699636827081,
        "mdate": 1699636827081,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Wvdp8FmVMZ",
        "forum": "lWe3GBRem8",
        "replyto": "lWe3GBRem8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7042/Reviewer_eqEe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7042/Reviewer_eqEe"
        ],
        "content": {
            "summary": {
                "value": "The paper you provided proposes a novel framework called Offline-to-Online-to-Offline (OOO) for reinforcement learning (RL). This framework tries to address the issue of exploration bias in online RL, where exploration bonuses can negatively impact the effectiveness of the learned policy. The OOO approach involves using an optimistic policy for exploration and interaction with the environment, while concurrently training a separate, pessimistic policy for evaluation based on all observed data.  The paper demonstrates that the OOO framework not only complements existing offline-to-online and online RL methods but also significantly improves their performance\u2014by 14% to 26% in fine-tuning experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow. The proposed method is overall reasonable.  Besides, I really like the topic raised by the author. How to use offline data for safe and efficient online interaction to find a global optimal policy is a key issue. Currently, pessimistic training in offline RL indeed enabled the recovery of performant policies from static datasets. Thus, I think the topic is timely to be proposed, as it can be the potential last step of the offlineRL paradigm to ground RL in real-world applications."
            },
            "weaknesses": {
                "value": "1. some important baselines seem missed: baseline of offlineRL + exploration (without decoupling) should be added; some offlineRL + decoupled exploration methods should be added (see below);\n2. some related works should be discussed: decoupling the exploration policies and the exploitation policies (or target policies) is not a new idea in standard exploration studies. There are indeed many works [1,2,3,4] that have discussed this problem. There should be a more formal discussion on the differences between the decoupling in offlineRL and off-policy RL beyond the difference in the gradient computation methods they used.\n3. the evidence of exploration bonuses bias in the learned policy should be discussed more explicitly. Maybe giving some visualizations for OOO and a standard offlineRL + RND algorithm is good.\n4. In Figure 3, why use \"frozen\" RND  and a baseline to demonstrate the effects of exploration bias?\n5. I am a bit confused about the paragraph \"What explains the improved performance in OOO RL?\". How can we derive the conclusion that \"These ablations suggest that mitigating the exploration bias by removing the intrinsic reward when training the exploitation policy leads to improved performance under OOO\" just by excluding the two hypotheses?\n\n\n[1] Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning. 2021.\n\n[2] Off-policy Reinforcement Learning with Optimistic Exploration and Distribution Correction. 2021.\n\n[3] Curiosity-Driven Exploration for Off-Policy Reinforcement Learning Methods. 2019.\n\n[4] Reinforcement Learning with Derivative-Free Exploration. 2019."
            },
            "questions": {
                "value": "An open question for discussion: What do you think are the essential differences, the extra challenges, and the benefits of the standard exploration problem in online RL and the exploration problem in offline-to-online RL?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7042/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7042/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7042/Reviewer_eqEe"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7042/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699729551529,
        "cdate": 1699729551529,
        "tmdate": 1699734820867,
        "mdate": 1699734820867,
        "license": "CC BY 4.0",
        "version": 2
    }
]