[
    {
        "id": "byO4PLyvj4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7116/Reviewer_8dk2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7116/Reviewer_8dk2"
        ],
        "forum": "Lv9KZ5qCSG",
        "replyto": "Lv9KZ5qCSG",
        "content": {
            "summary": {
                "value": "The paper introduces the EyeFairness dataset, aimed to promote the fairness study for medical imaging. The dataset comprises 30,000 subjects with both 2D and 3D imaging data, capturing various demographic attributes. Additionally, the authors propose a fair identity scaling (FIS) approach to enhance model fairness for this dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper studies an important topic of fairness for medical imaging. The authors introduce a relatively large-scale dataset for 2D fundus photos and 3D OCT scans. It covers major eye diseases and captures a few different demographic attributes, which can be a useful resource for the community.\n- The authors propose Fair Identity Scaling (FIS) to improve the fairness of the model."
            },
            "weaknesses": {
                "value": "- The authors didn't tune the hyper-parameters of the baseline methods but only used the default HPs, which leads to unfair comparisons. The baseline methods are not designed for medical imaging, so if applied to a different setting, the hyperparameters should be carefully tuned to get the best performance. Especially there're adversarial training method and self-supervised pretraining method that are very sensitive to the HPs.\n- The definition of performance-scaled disparity (PSD) is not clear. It says in the paper: \"PSD metrics are calculated as the standard deviation of group performance or absolute maximum group performance difference divided by overall performance.\" Which one did the authors use? And what does Mean PSD and Max PSD mean?\n- Also, regarding the metrics, authors can provide the worst-case AUC and the AUC gap between best-performing and worst-performing groups besides the current overall AUC and group-wise AUC. It would be clearer to directly look at the AUC gap to validate the effectiveness of the proposed methods. Additionally, what are the advantages of using PSD instead of the AUC gap? Consider an extreme case: for model 1, two groups have an AUC of 25% and 51% while for model 2, two groups have an AUC of 50% and 100%. According to the authors' definition, $PSD_1 = (51-25)/51=0.51 < PSD_2 = (100-50)/100=0.5$, but can you say model 2 is fairer than model 1 as it's the smaller the better? I know the AUC usually is higher than 50% but this is just an example and I think there are many similar cases in regular scenarios.\n- I think Table 1 in this paper is taken from Table 1 in [1] without reference as (1) the number of images of each dataset is not the original number but the number after preprocessing by [1] and (2) the so-called ADNI 1.5T is a subset of the large ADNI dataset [2] extracted by [1].\n- At the time I wrote this review, the GitHub repo authors provided was empty.\n- Minor: the current citation style makes reading difficult. The author should use (Deng et al. (2009)) instead of Deng et al. (2009), i.e. \\citep instead of \\cite.\n\n[1] Zong et al. MEDFAIR: Benchmarking Fairness for Medical Imaging. ICLR'23.\\\n[2] Wyman et al. Standardization of analysis sets for reporting results from adni mri data. Alzheimer\u2019s & Dementia 2013."
            },
            "questions": {
                "value": "- Can the authors also provide further breakdown statistics of the intersectional groups, e.g. black females?\n- The dataset also contains some other attributes such as preferred language. I'm not very sure how this is related to eye diseases and fairness, e.g. do non-English speaking patients get lower AUC? But how does the eye imaging model perceive the speaking? Also, the authors did not evaluate the performance of different subgroups of preferred language and marital status."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7116/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697144170322,
        "cdate": 1697144170322,
        "tmdate": 1699636841394,
        "mdate": 1699636841394,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DYeY5RMc7r",
        "forum": "Lv9KZ5qCSG",
        "replyto": "Lv9KZ5qCSG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7116/Reviewer_7iiL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7116/Reviewer_7iiL"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a new large-scale dataset for eye disease diagnostic comprising of 30\u2019000 2D fundus as well as 3D OCT images for AMD retinopathy and glaucoma diagnostics. In addition to some baseline comparison on fairness metics it also proposes a new fair identity scaling."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The proposed dataset is very valuable as it is very large in size (30\u2019000 patients with 2D and 3D imaging) covering three relevant eye diseases. The analysis is strong and the explored fair identity scaling is a reasonable approach to address inequality in datasets in general. Providing (to my understanding) paired 2D fundus and 3D OCT imaging could also pave the way for new hybrid diagnostic tools."
            },
            "weaknesses": {
                "value": "Overall, despite its value, the proposed dataset is somewhat limited in that it seems to be acquired from a single centre in the US. A pooling with previous public datasets would likely increase its value and reduce the \u201cunfairness\u201d by design (rather than re-weighting). \nThe statement \u201ceffective image augmentation strategies for 3D imaging data are largely unclear\u201d is wrong in my opinion and there is no citation that backs it up. Many 3D medical image analysis methods make good use of image augmentation strategies. \nThe chosen baselines are rather simple and no true SOTA results are presented. The presentation of the results is mainly focussed on numerical comparison and I would be missing a more in-depth analysis or discussion why certain races perform better or worse. E.g. since Hispanics are under-represented in the dataset it is not intuitive that this group achieves the highest AUC without and not with FIS. \nIt remains unclear whether the data is always \u201cpaired\u201d, ie. the same patient is measured with 2D and 3D imaging."
            },
            "questions": {
                "value": "Clarify the 2D/3D data split wrt. patients. Discuss a pooling of other public datasets from centres with different scanners / from different countries. Expand the baselines and correct the statement of non-existing 3D augmentation strategies."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7116/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698611924306,
        "cdate": 1698611924306,
        "tmdate": 1699636841275,
        "mdate": 1699636841275,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WugK6yDqsI",
        "forum": "Lv9KZ5qCSG",
        "replyto": "Lv9KZ5qCSG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7116/Reviewer_fenP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7116/Reviewer_fenP"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the EyeFairness dataset that includes both 2D fundus photos and 3D optical coherence tomography (OCT) scans, together with six demographic features (age, gender, race, ethnicity, preferred language, marital status), and proposes a fair identity scaling (FIS) approach to combine group and individual scaling to improve model fairness. FIS was demonstrated to improve performance in eye disease screening according to fairness metrics, when implemented together with EfficientNet-B1, against other fairness methods. Fairness methods are especially appropriate in the eye screening domain due to known differing burdens of eye diseases amongst ethnicities."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-\tFIS exploits both group and individual scaling to manage within-group sample variation\n-\tDetailed comparison of proposed FIS against other fairness methods (Adv, FSCL) on various demographic features"
            },
            "weaknesses": {
                "value": "-\tMinimal ablation analysis temperature scaling parameter, FIS group/individual scaling trade-offs actually not very consistent (Figure 2)\n-\tSide-effects of fairness on adjacent demographic features not considered"
            },
            "questions": {
                "value": "1. The main contribution of a Fair Identity Scaling (FIS) model with learnable group weights and past individual loss data, might have been analyzed in greater details as to the temperature scaling parameter (alongside fusion weight).\n2. Conceptually, the distinction between improvements in \u201cgeneral (AUC) performance\u201d and \u201cfairness\u201d might have been further considered. In particular, from the results in Tables 2 to 7, FIS appears often capable of not only improving \u201cfairness\u201d (i.e. minimizing performance-scaled disparity), but instead often improves performance in all groups (and overall). As such, a natural question might be whether FIS might be used with arbitrary groupings of data, to improve classifier performance.\n3. Returning to fairness as a focus, the presented analysis does not appear to be concerned with the impact on fairness amongst other demographic features, when FIS is applied to a particular feature. For example, when FIS is applied on race (as in Tables 2 & 3), what is the effect on results stratified with other features such as gender, ethnicity, age etc.? This appears particularly relevant since the other demographics may be no less significant for the consideration of fairness/equity purposes.\n4. The costs of considering fairness might be discussed in greater detail, in particular the possibility that optimizing the proposed PSD metric possibly reduces overall classification performance (and thus medical care). This is because PSD (and other fairness metrics) emphasize between-group equality, which may come at the cost of reduced aggregate performance (although this is largely not the case in this study)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7116/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742672849,
        "cdate": 1698742672849,
        "tmdate": 1699636841167,
        "mdate": 1699636841167,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "R95u4vd5x4",
        "forum": "Lv9KZ5qCSG",
        "replyto": "Lv9KZ5qCSG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7116/Reviewer_eXPH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7116/Reviewer_eXPH"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a publicly available large-scale (30,000 subjects) 3D eye imaging dataset (OCT/Fundus) for disease screening and fair identity scaling. The authors also propose a fair identity scaling metric to evaluate model performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. Addressing the fairness issue is an important topic and organizing such a large-scale dataset including three types of measurements: 1) retinal imaging 2) demographic group information 3) disease diagnosis requires a large amount of effort.\n\n2. The authors ran several baselines (EfficientNet, 3D CNN) and evaluated the classification with some fairness metrics (e.g. PSD, DPD)."
            },
            "weaknesses": {
                "value": "1. The abstract and introduction section is quite lengthy. The core contributions and the highlights can be combined.\n\n2. Some of the writing needs to be improved (e.g. \"model performance across different models\")."
            },
            "questions": {
                "value": "1. The trend of hyperparameter $c$ in Figure 2 is not quite clear since it kind of alternates for both AUC and mean PSD. The authors might need to further discuss the choice of $c$, which still seems rather empirical given the current visualization.\n\n2. The experimental results section is currently flooded with numbers and quite hard to follow."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7116/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7116/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7116/Reviewer_eXPH"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7116/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699239605086,
        "cdate": 1699239605086,
        "tmdate": 1699636841013,
        "mdate": 1699636841013,
        "license": "CC BY 4.0",
        "version": 2
    }
]