[
    {
        "id": "3KGflOCGUt",
        "forum": "UEdS2lIgfY",
        "replyto": "UEdS2lIgfY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1598/Reviewer_8pv7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1598/Reviewer_8pv7"
        ],
        "content": {
            "summary": {
                "value": "This paper purports to uncover how LLMs (i.e., their embedding and attention) change given in-context learning. Representational similarity analysis is applied, inspired by neuroscience. Experiments are run on Vicuna and Llama, including across a few investigations, including persona injection for jailbreaking."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The combination of tasks is interesting, in this somewhat important (and hence somewhat crowded) area of research. It is likewise interesting to inject a persona _towards_ the latent representations, as jailbreaking and adversarial prompting is also a popular emerging area.\n- The appendices are detailed and informative."
            },
            "weaknesses": {
                "value": "- Some rationale for using RSA was a bit shaky. In its introduction, RSA is touted as overcoming parametrized probing methods, but this is a bit of a \u2018red herring\u2019, as there are already (increasingly many) varieties of parameter-free probing methods (e.g., https://aclanthology.org/2020.acl-main.383.pdf). That\u2019s not to say that the parameter-free nature of RSA isn\u2019t a positive, but it doesn\u2019t really highlight why we should use RSA (or any neuroscience-based approach) in particular.\n- This paper does not seem to provide a novel contribution, as there is already plenty of work that describes ICL for LLMs. The only contribution seems to be the application of RSA however 1) the related work section reveals that RSA has already been variously applied elsewhere in NLP (so the magnitude of its contribution is quite diminished), and the method itself is not adequately described. Although the methodology of RSA can be inferred through Section 2, it should be defined quite explicitly, if this is the main contribution.\n- Although the various errors in writing do not detract from comprehension, most of the time, it still casts doubt on the authors\u2019 ability to attend to detail.\n- The text in the figures are fairly unreadable, due to size, but perhaps this is unavoidable. Some of their important parameters are not discussed (e.g., Fig 3 is merely described as \u2018behavioral error\u2019 but the nature of the plots are not given)"
            },
            "questions": {
                "value": "- Can you please run a spell-check over your paper (e.g., \u2018anayzing\u2019, 'appraoch\u2019, etc) and also check the grammar (e.g., \u201cRSA avoid this risk\u2019, \u2018the models behavior\u2019, \u2018to reduces the effect\u2019) and LaTeX errors (like ?? instead of references)?\n- Sec 2.4 \u2014 why was only the final layer of the LLM considered? How are the subsets a, b, and c determined? Why is \u2018a\u2019 not included in the computation of A(a,b,c)_x? Does this only apply to question-answering, as the second sentence of the second paragraph suggests?\n- Sec 5 Why did you precede the instructions to be deceitful with instructions to be truthful? Would a fourth condition jump straight to instructions to be deceitful?\n- Why did you give only incomplete references (i.e., without venue names) for at least 13 papers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1598/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698410580304,
        "cdate": 1698410580304,
        "tmdate": 1699636088433,
        "mdate": 1699636088433,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PCX84fjyx5",
        "forum": "UEdS2lIgfY",
        "replyto": "UEdS2lIgfY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1598/Reviewer_Sz99"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1598/Reviewer_Sz99"
        ],
        "content": {
            "summary": {
                "value": "The authors use several representation analysis methods (e.g., RSA, decoding, and attention metrics) to measure how LLM embeddings change based on examples provided in in-context learning (ICL). \n\nOne contribution, ancillary to the main analysis, is the \"attention ratio\" metric, which seeks to measure the weight of attention on relevant vs. irrelevant parts of an input.\n\nThe primary findings are based on experiments with the Llama2 and Vicuna models, where, for both models, the authors find that 1) adding ICL improves performance and 2) per several representation metrics, adding ICL seems to focus embeddings more on relevant parts of the prompt."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "## Originality\nThis work is somewhat original in analyzing how embeddings change as ICL is used for models. The attention ratio metric is certainly new, and in general I like the idea of using these embedding measures to understand how a model react to different inputs beyond simple behavioral measures.\n\nI note that the authors sometimes seem to include general behavioral changes due to ICL as one of their contributions (e.g., the first bullet point in the introduction section). There is already extensive work discussing the behavioral effects of ICL, some of which the authors cite, so I do not view this as one of the main contributions of this work\n\n## Quality\nThe work itself seems largely well done, although aspect of the paper could be improved.\n\n## Clarity\nOverall, I found the paper quite clear overall, although there were some specific details or sections that were unclear (see weaknesses). \n\n## Significance\nI think if the authors further focused on representational, as opposed to behavioral measures, this work could be reasonably significant. As it stands, I think the work is somewhat torn between to thrusts, which weakens the overall message."
            },
            "weaknesses": {
                "value": "## Focus on behavioral vs. representational measures.\nPerhaps my primary concern with this work is that it seems to focus quite a lot on presenting measures of behavioral changes due to ICL instead of representational changes. I believe the behavioral benefits of ICL have already been established (if not, please indicate what is new about these findings). Therefore, the exciting part of this work is the link between representational metrics and changes in inputs (and perhaps links between representational and behavioral changes). Many of the main figures, however, focus only on behavioral changes (Figure 3, Figure 4 a and b). Some figures are great (e.g., Figure 4 c), but I wish more were like that.\n\n## Interpreting attention values\nI am somewhat hesitant to use attention values in interpretability work (see \"Attention is not Explanation\" by Jain and Wallace). Can the authors justify using attention measures to understand how the model is behaving?\n\n## Appendices:\nThe authors included many further results in appendices, which is overall a practice I encourage, but the appendices could benefit from refocusing and better presentation. Appendix B.1, for example, is very unclear to me.\n\n## Minor:\n1. Many of the violin plots used end up plotting uninterpretable extrapolations. For example, in Figure 3, by definition, no value can be less than 0, but the plots do go into negative values. The raw data are fine, but the plots seem confusing at first.\n\n2. Figure 5 a-c would really benefit from some sorts of labels to say what the axes are. I believe I understand this from examples in the appendix (e.g., Figure 7), but it should be very clear in the main paper.\n\n3. Figure 12 in the appendix seems wrong to me, especially because there are no labels on the axes. In particular, c) is supposed to represent the average of a and b, but a and b look the same."
            },
            "questions": {
                "value": "I only have a simple clarifying question: could the authors elaborate on how they \"standardize the embeddings before computing pairwise cosine similarities\" (section 2.1)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1598/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1598/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1598/Reviewer_Sz99"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1598/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698433753371,
        "cdate": 1698433753371,
        "tmdate": 1700662483313,
        "mdate": 1700662483313,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jiQ29LM46M",
        "forum": "UEdS2lIgfY",
        "replyto": "UEdS2lIgfY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1598/Reviewer_DTAM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1598/Reviewer_DTAM"
        ],
        "content": {
            "summary": {
                "value": "This work aims to understand the effectiveness of in-context learning. Motivated by neuroscience research, this work proposed to study the latent representations of prompts and based on the similarity analysis to interpret how in-context examples impact model inference."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The research problem is critical and challenging. Understanding in-context learning will not only help us understand the capacity of LLMs, but also help design better context or select better in-context examples for better using LLMs.\n- The adopted approach, inspired by neuroscience, is interesting. Although, I do have some concerns about the actual implementation of the approach (explained in the following)\n- The proposed work was supported by three very different language related tasks."
            },
            "weaknesses": {
                "value": "- The design of the method seems to be a little bit arbitrary. Not sure I understand the underly principle of design choices. For example\n    - in section 2.1, the way of composing $n$ embeddings to one single embedding is to use max-pooling. Although I agree that max-pooling is one classical and popular method on representation composition, but I would like to know how this method was selected and to what extent this selection will impact the final conclusion\n    - in section 2.1, it says \u201cthe most relevant subset of tokens representing important components of the prompt are considered\u201d, by manually (I assume) selecting the \u201cmost relevant\u201d subset, I am not sure whether the results can be used to represent LLM\u2019s behavior\n    - in section 2.2, it says \u201cWe designed a set of tasks with common components, such that representing those components is crucial in solving tasks\u201d, if these components are crucial, it may be a good idea to talk a little bit more about these components, such as how these components are designed, and how do we know the combination of these components can be used to represent a way of solving tasks?\n    - in section 2.4, why the attention of the final layer?\n- Some (minor) writing issues, for example\n    - A typo in the first sentence of section 2.3\n    - The notions in section 2.4 are confusing, for example, both $a$ and $p$ represent prompts (?)\n- Some issues about experiment design. For example,\n    - Why choosing these three tasks, particularly the last time, rather than some tasks that in-context learning has been widely used?\n    - What the role of RSA in section 3, which is not clear (it\u2019s likely I may miss something important there)\n    - About section 5, I am not sure whether the goal of this task is to study the robustness of the model or understand in-context learning is used for model inference. It\u2019s an interesting task, even though I am a little skeptical about the application scenario. The most important question here is I am not sure how this experiment can help demonstrate the impact of in-context learning."
            },
            "questions": {
                "value": "Please refer to the previous section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1598/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698894413818,
        "cdate": 1698894413818,
        "tmdate": 1699636088287,
        "mdate": 1699636088287,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RJvusOQt1e",
        "forum": "UEdS2lIgfY",
        "replyto": "UEdS2lIgfY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1598/Reviewer_FbLG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1598/Reviewer_FbLG"
        ],
        "content": {
            "summary": {
                "value": "This work aims to study the changes in a large language model that are brought about by in context learning (ICL). The authors study the changes in representation and attention in the last layer of 2 open source LLMs (Llama2 and Vicuna) in three toy tasks: answering a simple question in the presence of a distractor, linear regression, and susceptibility to adversarial attacks due to injecting of a deceitful persona. A variety of methods are used for this: probing classifiers, attention quantification, and representational similarity analysis (which is an approach borrowed from neuroscience). The results are that ICL helps the models improve their performance on the toy tasks, and that the changes in representation and attention in the last layer agree with those improvements."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- understanding ICL is a very interesting problem that is relevant to the ICLR community\n- a variety of tasks and approaches are used for analyses\n- two open source models are investigated"
            },
            "weaknesses": {
                "value": "The research question is interesting and there is a nice start to investigate it here. However, currently the manuscript feels like it was put together hastily and like there was a breadth-first approach into several distinct directions and analysis techniques that can benefit from a more focused investigation. \n\nMy main concern is that it\u2019s not clear what new insights are being offered here. The representations and attention are being studied at the last layer. We see that the behavior improves after ICL. The behavior is a direct function of the representations and attention at the last layer. What do we expect to have happened to those representations and attention if the behavior has improved? Is it possible for the representations and attention to not reflect that improvement? It would be more insightful if the authors investigate earlier layers and reveal what kind of effect ICL has on those representations.\n\nDetails of the models used are missing. Are they instruction-tuned? If so, on what datasets are they instruction-tuned? \n\nMany typos, grammatical errors, and some citet/citep confusion.\nExamples of errors: sec 2.3 \u201cmebeddings\u201d, page 4 \u201cSection ?? for t-test results\u201d, page 4 \u201chas two distinguish\u201d, page 4 \u201creading a book.\u201d., page 6 \u201cHanna\u201d without a last h, page 8 \u201cto test this, designed\u201d, page 8 \u201ctested on used open source\u201d\nExamples of citet/citep confusion: 3rd paragraph on page 1, penultimate par on page 6, penultimate par on page 9"
            },
            "questions": {
                "value": "One part of the paper which seems to have an unexpected result is in the last investigation of how ICL affects the representations after adversarial attacks related to injecting a deceitful persona. The unexpected result is that ICL makes the representations closer to the hypothesis model for names rather than actions, even though the answer should be an action. Can the authors offer their thoughts into why this is the case? Also, it would be helpful to clarify what is considered as \u201ccorrect\u201d response under these attacks: is it the response that aligns with the truth, or the response that aligns with the actual instructions that the model is supposed to follow?\n\nMore of a comment:\nThe inclusion of RSA is interesting. The field of interpretability already uses CKA a lot to examine representations and my first instinct when I read about RSA in the introduction was why RSA and not CKA. Later in the paper, I understood that what the authors really take away from RSA is the comparison of the representational space to a hypothesized model space. This should be made more obvious earlier on in the paper. I\u2019m also not sold on the idea that you cannot get the same insights via probing (e.g. showing that decoding performance increases after ICL)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1598/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699029893030,
        "cdate": 1699029893030,
        "tmdate": 1699636088198,
        "mdate": 1699636088198,
        "license": "CC BY 4.0",
        "version": 2
    }
]