[
    {
        "id": "A7jCyoTe59",
        "forum": "efFmBWioSc",
        "replyto": "efFmBWioSc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1266/Reviewer_idei"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1266/Reviewer_idei"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an instruction-following multimodal agent, WebGUM for web navigation tasks. The proposed method has improved upon the prior SOTA method (including humans and a GPT4-based agent) on MiniWob."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is well written and easy to follow.\n2. The proposed method achieved amazing performance on MiniWob."
            },
            "weaknesses": {
                "value": "1. The proposed method is neither impressive nor novel. Given the fact that multimodal MLLM can already complete various difficult tasks such as visual reasoning or science QA [1,3], achieving the highest score in MiniWob does not seem very surprising. \n2. Since there is a collection of [opensourced MLLMs](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models), which also combine a VIT-like vision encoder and an LLM, similar to the proposed WebGUM. The reviewer does not feel like the \u201cshowing another MLLM can do tasks like web navigation\u201c is significant enough. Especially when some of the prior MLLMs [1-3] are already using LLaMA as their base LM, the proposed method is still using T5.\n\n[1] Liu, Haotian, et al. \"Visual instruction tuning.\" arXiv preprint arXiv:2304.08485 (2023).\n\n[2] Li, Bo, et al. \"Otter: A multi-modal model with in-context instruction tuning.\" arXiv preprint arXiv:2305.03726 (2023).\n\n[3] Dai, Wenliang, et al. \u201cInstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning\u201d, arXiv preprint arXiv:2305.06500"
            },
            "questions": {
                "value": "Have the authors tried LLaMA (or LLaMA2) for based LM? If yes, what are the results? If not, why not?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1266/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698305644906,
        "cdate": 1698305644906,
        "tmdate": 1699636053285,
        "mdate": 1699636053285,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JtLgeK9e6H",
        "forum": "efFmBWioSc",
        "replyto": "efFmBWioSc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1266/Reviewer_cMQF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1266/Reviewer_cMQF"
        ],
        "content": {
            "summary": {
                "value": "This paper studied data-driven offline training for web agents with VLMs. The proposed WebGUM observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. The authors collected 347K high-quality demonstrations using their trained model,"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Novelty: The proposed WebGUM agent exhibits a novel combination of HTML and image modalities to tackle the challenges in web navigation. \nPerformance: The empirical results are compelling, with the model showing substantial improvements on the MiniWoB and WebShop benchmarks. \nResource Contribution: The authors have collected and made available a significant corpus of high-quality demonstrations, which is 38 times larger than previous datasets. This contribution is likely to be valuable for the broader research community working on similar problems.\nClarity and Structure: The paper is well written with clear explanations of the methodology and discussions surrounding the results. The inclusion of comparisons with existing SoTA methods provides a good understanding of the performance gains achieved."
            },
            "weaknesses": {
                "value": "- Generalization: It\u2019s not clear how well the proposed method generalizes to a broader range of web navigation tasks outside the tested benchmarks, especially HTML that are longer than context length. More discussions or evaluations on the generalizability could strengthen the paper. \"Because the context length is insufficient for raw HTML, we preprocess context HTML by extracting a snippet that includes the answers in advance.\"\n- Figure 1 and 5 are blurry."
            },
            "questions": {
                "value": "- failure cases analysis: include failures cases on datasets like minwob, on which the success rate is high. \n- include demos on mind2web and webshop."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1266/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698474817894,
        "cdate": 1698474817894,
        "tmdate": 1699636053199,
        "mdate": 1699636053199,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "j0s6hmCcry",
        "forum": "efFmBWioSc",
        "replyto": "efFmBWioSc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1266/Reviewer_a21F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1266/Reviewer_a21F"
        ],
        "content": {
            "summary": {
                "value": "This paper studied web navigation with instruction-finetuning multi-modal models. Specifically, they created a large-scale datasets by generating data with pretrained LLMs. They then train a vision-language model with flan-t5 as the base language model and ViT as the vision encoder. The model will take html code and image screenshot as inputs and take navigation actions such as click and type."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The performance of their model is good. It outperforms previous methods under different settings.\n2. Writing is clear. \n3. They performed detailed analysis such as dataset and model size scaling."
            },
            "weaknesses": {
                "value": "1. The technical contribution is very limited. The takeaway is to do supervised training on a large-scale model-generated dataset. It feels like knowledge distillation of a combination of model outputs (as described in 4.3 they used various LLMs to generate such data).\n2. The dataset creation process and quality is not clear."
            },
            "questions": {
                "value": "1. How is the generated dataset high-quality? Is there a human study to prove this? As described in section 4.3, my understanding is this LLM-generated dataset can be noisy. \n2. It is also not very clear about the data generation process. What does it mean by \"rollout a LLM policy with 100 episodes per task\"? What are the inputs and outputs here?\n3. As mentioned in section.5, the method is evaluated on MiniWoB++ with 100 evaluation episodes per task. Does the reported numbers from previous work also use the same eval set?  \n4. Could you explain more on why your model would outperform human? It is hard to imagine a small model can do better than human on general web navigation. Is it due to overfitting to a specific dataset (Table.1)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1266/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1266/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1266/Reviewer_a21F"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1266/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699683395558,
        "cdate": 1699683395558,
        "tmdate": 1699683395558,
        "mdate": 1699683395558,
        "license": "CC BY 4.0",
        "version": 2
    }
]