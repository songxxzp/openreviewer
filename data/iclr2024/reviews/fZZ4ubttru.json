[
    {
        "id": "d8jGUmjDP5",
        "forum": "fZZ4ubttru",
        "replyto": "fZZ4ubttru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6409/Reviewer_EQgy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6409/Reviewer_EQgy"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of learning diverse robotic skills through automatic task and reward generation. Specifically, the proposed method utilizes LLM to produce the task setups and identify the skill sequence for solving the task. For RL skills, LLM is also prompted to generate the reward function. Experiments are performed in simulated tasks to validate the idea."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea of automatically generating new tasks for acquiring diverse robotic skills is novel and interesting; \n\nThe pipeline is straightforward and clear; \n\nThe paper is well-written and easy to read."
            },
            "weaknesses": {
                "value": "The proposed method incorporates random sampling of task objects and the robot agent during the seeding stage, taking into account the requisite skills for the desired tasks. This consideration of required skills, or the lack thereof, may potentially enhance learning efficiency; \n\nEmploying LLMs to generate task proposals can result in incomplete task information. For instance, in the context of a task such as \"bowl heating,\" the LLM may generate a description that overlooks crucial details, like setting the timer; \n\nThe generation of LLM-based reward functions heavily hinges on in-context prompts, which are derived from human comprehension of the task. This approach may necessitate significant human input and potentially restrict its applicability to novel tasks and domains. \n\nThe method lacks a formal mathematical formula or algorithmic description. \n\nThe experiments are insufficient to validate the idea thoroughly."
            },
            "questions": {
                "value": "The low-level RL skills use object states as observations, which seems not able to fully utilize the advantages of diverse visual appearance introduced by task generation, could authors provide further explanation about this? \n\nAdditionally, could the paper provide insights into how these acquired skills can be effectively reused to accomplish new goals when faced with a novel task during testing? Reporting experimental results in novel task settings through skill reusing would further strengthen the paper; \n\nThe proposed pipeline seems to be cascaded, where the errors or infeasibility produced at the task generation stage could lead to future difficulties in skill learning, have the authors ever considered any strategies to improve the interplay between task generation and skill acquisition?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6409/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6409/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6409/Reviewer_EQgy"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6409/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698025995370,
        "cdate": 1698025995370,
        "tmdate": 1699636713686,
        "mdate": 1699636713686,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZV1MJcndcD",
        "forum": "fZZ4ubttru",
        "replyto": "fZZ4ubttru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6409/Reviewer_niTv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6409/Reviewer_niTv"
        ],
        "content": {
            "summary": {
                "value": "GenBot proposes a method for automating the large-scale learning of diverse robotic skills through generative simulation. The approach is based on the propose-generate-learn cycle, where tasks and skills are proposed, related simulation environments are generated, and the agent learns policies. The study leverages foundation models for each of these components, enabling automation and demonstrating the learning of various manipulation tasks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- This research introduces an automated pipeline capable of generating diverse tasks, which is considered novel.\n- The method for generating tasks is intriguing. It instructs GPT-4 on how objects can be manipulated, the meanings of each joint and link, enabling GPT-4 to learn the affordances of each object and generate tasks accordingly.\n- The subsequent modules are also very interesting and plausible. In the case of scene generation, it generates the right objects for each scenario through an LLM, and the entire pipeline is connected to load the 3D mesh assets, resulting in appropriate scenes for each situation.\n- The results of task decomposition in Figure 3 are very interesting, showing that the proposed method is effective in inducing meaningful skills.\n- GenBot is shown to generate a variety of tasks for skill learning, including object manipulation, locomotion, and soft body manipulation."
            },
            "weaknesses": {
                "value": "- The assumption that the decomposed shorter-horizon sub-task can be solvable by one of the policy categories within this framework is needed. However, this is not a drawback unique to this research, so it may not be considered a weakness."
            },
            "questions": {
                "value": "- Are the physical characteristics of objects (e.g., weight) also determined by LLM during generation?\n- Were there cases where the decomposed shorter-horizon sub-task could not be resolved within one of the policy categories in this framework?\n- What is the overall computational cost involved in the framework?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6409/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6409/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6409/Reviewer_niTv"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6409/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698755913302,
        "cdate": 1698755913302,
        "tmdate": 1699636713548,
        "mdate": 1699636713548,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UPEo4w5YPC",
        "forum": "fZZ4ubttru",
        "replyto": "fZZ4ubttru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6409/Reviewer_pBsz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6409/Reviewer_pBsz"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces \"GenBot\", a generative robotic agent designed to automatically learn a variety of robotic skills on a large scale via generative simulation.\n\nGenBot utilizes advancements in foundational and generative models. Instead of directly employing or adapting these models to formulate policies or specific actions, the authors suggest a generative approach. This approach employs the models to automatically generate diversified tasks, scenes, and training supervision. The goal is to enhance robotic skill learning with minimal human intervention.\n\nGenBot follows a \"propose-generate-learn\" cycle. Initially, the agent suggests intriguing tasks and skills. Following that, it generates simulation environments, populating them with relevant objects and assets in the appropriate spatial configurations. After obtaining all the required information for the proposed task, including scene components, GenBot proceeds with the actual skill learning.\n\nThe contributions of this paper go as follows.\n\n- The paper introduces \"GenBot\", a robotic agent that automates the process of task and environment generation and subsequently learns skills. This framework potentially reduces the need for human intervention in the process of creating simulation tasks.\n- A figure in the paper showcases 25 example tasks generated by GenBot and the corresponding skills it learned, highlighting the diversity and applicability of the system."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Overall, this paper demonstrates that the entire pipeline\u2014from creating tasks to learning skills\u2014can potentially be automated by large models. While a lot of details are still missing, I commend the quality of this work, especially considering the engineering efforts involved.\n\nSpecifically, the strengths of this paper include:\n\n- The paper introduces GenBot as an automated pipeline that can be endlessly queried to generate a continuous stream of skills for diverse tasks. This automation is a significant strength as it reduces human intervention and can potentially scale up robotic skill learning.\n\n- Task diversity is essential for generalizable robotic skill learning. If GenBot can produce a diverse set of tasks and learn corresponding skills, it signifies a robust and versatile system."
            },
            "weaknesses": {
                "value": "## Major\n\n### **Task diversity**\n\nI am concerned regarding the diversity of the generated tasks. With tasks proposed by LLMs and only qualitative examples provided, it's challenging for readers to gauge the true diversity of these tasks. Specifically:\n\n- How many semantically distinct tasks are generated? By \"semantically distinct,\" I refer to tasks that are fundamentally different. For instance, \"opening a cabinet\" and \"lifting a bucket\" are semantically distinct, whereas \"walking forward\" and \"walking backward\" are not.\n- What is the range of diversity in scene configurations? Upon reviewing the prompts, it seems that certain elements, like a table, have fixed poses and heights. If this is a recurring theme, then scene configuration diversity appears limited.\n\n\n### **Task verification**\n\nThe construction of tasks in simulation typically requires validation to ensure correct implementation. This involves examining success conditions, initial state distributions, physical parameters, and more. However, the paper lacks a systematic method for this crucial verification, especially given the automation of task creation. Mistakes at any stage could result in flawed tasks. Specifically:\n\n- What percentage of the tasks can be successfully solved? How does this compare to the total number of generated tasks?\n- Are trivial tasks, such as picking up a block when given the grasp action primitive, filtered out?\n\n\n### **Use of LLMs**\n\n\nWhile the paper demonstrates the potential for automating the entire pipeline, from task creation to skill learning, using large models, the necessity of LLMs is questionable. Could simple heuristics or random placements of objects yield similar results? Given the extensive prompting involved with GenBot, it's unclear if it genuinely produces more diverse tasks with reduced human efforts.\n\n\n### **Missing details**\n\nNumerous details are absent from the paper. Refer to the \"Questions\" section for more questions.\n\n\n### **Limited quantitative results**\n\n\nThe majority of the results are qualitative, which lacks depth for readers. Additionally, the paper's comparison of task diversity to other benchmarks based solely on task descriptions is less than persuasive.\n\n\n## Minor\n\n\n- Object Assets: Currently, the paper relies heavily on PartNetMobility and RLBench for task-relevant objects, which may restrict task diversity. Although the paper suggests using Midjourney + Zero123 for additional 3D assets, this pipeline lacks detailed elaboration.\n\n- Lack of Open-Source Code: As of now, the paper hasn't released its code. Furthermore, the underlying simulation framework, \"Genesis,\" remains private."
            },
            "questions": {
                "value": "- Regarding Task Proposal:\n\n\t- How does the system handle incompatibilities between the robot and the object? For instance, if the robot is a dog and the object is a cabinet, what would the proposed task be?\n\t- In scenarios involving non-articulated objects, if the generated tasks aren't specifically tied to the sampled object, how does the sampled object influence the task?\n\n- Regarding Scene Generation:\n\n\t- Could you provide a more detailed explanation of the MidJourney + Zero123 pipeline?\n\t- What is the precise output format from the LLMs? How is this output imported into a simulator to construct a scene?\n\t- How does the system manage situations where the scene results in an unsolvable task? For example, if the task is to open a cabinet but the cabinet is positioned out of the robot arm's reach.\n\t- How are potential collisions in the initial scene configuration addressed?\n\t- Is the initial state of the scene fixed, or is it sampled from a distribution?\n\t- How does the system generate physical parameters other than size, such as friction?\n\n- Regarding Training Supervision Generation:\n\t- How reliable is the reward generated by the LLM? Are there instances where it may not align with the intended goal?\n\t- How does the system define the success conditions for a task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6409/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825608891,
        "cdate": 1698825608891,
        "tmdate": 1699636713410,
        "mdate": 1699636713410,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rjWXVKq8Fn",
        "forum": "fZZ4ubttru",
        "replyto": "fZZ4ubttru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6409/Reviewer_X3QY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6409/Reviewer_X3QY"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method to learn diverse skills in simulation at scale. The method first uses an LLM to propose tasks from a pool of possible robots and objects. This is then used to generate assets and configure a scene. The generated task is decomposed by a LLM into sub-tasks, a solution method is automatically determined (eg. RL or planning), and finally, the skill is learned in simulation.\n\nThe paper is clearly written, easy to follow, and proposes a promising idea. I find the method to be compelling and potentially very impactful, but I feel the experimental validation could be stronger."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Clear narrative and mostly easy to follow.\n- Promising idea leveraging powerful, large-scale pre-trained model architectures"
            },
            "weaknesses": {
                "value": "- Could be clearer in some sub-sections given the large number of moving parts\n- Experiments need to be more thorough for each of the components (task diversity, scene validity, training supervision validity, etc)\nI elaborate on these points further in the Questions section below."
            },
            "questions": {
                "value": "The presented approach has a very large number of moving parts (involving many different large pre-trained models), which I tried to understand as follows. (This is also to ensure my correct understanding of the method, which the authors can correct if needed and further clarify in the paper).\n1) **GPT-4** is used as the main LLM for task proposal. The assets can then be either retrieved or generated.\n2) For asset retrieval, **Sentence-Bert** is used to embed the description of the asset, which is then matched to the top-k similar embeddings for assets in the Objaverse. Since retrieval based on the language embedding may not be perfect, the asset is verified by captioning an image of it with a VLM, then feeding this along with the desired asset description and task description into GPT-4 to verify its validity. For reliability, two VLMs (**Bard** and **BLIP-2**) are used, and both captions must be valid for the asset to be used.\n3) If asset retrieval fails, then the mesh is generated; the method uses **Midjourney** for text-to-image generation followed by **Zero-1-to-3** for image-to-mesh generation.\n4) **GPT-4** is then used several further times, to decompose the proposed task into sub-goals; choose a method (RL, motion planning with action primitives, trajectory optimization); generate reward functions via in-context learning; and select the action space for some tasks (delta-translation or target location of the end effector).\n\nI have the following comments:\n- The text in sections 3.1 and 3.2 could perhaps be shorter; I found it a bit difficult to follow with a lot of text including details interspersed throughout.\n- Consider an additional graphic for section 3.2, or at least a structured/bullet list. There are a lot of moving parts, and it took a while for me to understand how they all interact at the different stages.\n- There is some additional work on learning diverse skills in simulated environments (in some cases in addition to real-world) that should be cited.\n\t\n\tJiang et al, 2022, VIMA: General Robot Manipulation with Multimodal Prompts\n\n\tMajumdar et al, 2023, Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?\n\n\tBousmalis et al, 2023, RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation\n\n- Given the significant complexity proposed, I think the experiments should be more thorough and quantitative to do justice to the complexity of the method. I address each of the experimental sections below:\n\n**Task diversity**: Measuring task diversity using just the language descriptions may be prone to biases (eg. task suites may describe tasks differently, with different levels of verbosity). Providing the same measures in state space (eg. perhaps just the diversity of robot joint motions required to solve the tasks) or image space (eg. the final image showing goal configurations for each task) would be more convincing.\n\n**Scene validity**: While Figure 4 shows the BLIP-2 scores for asset retrieval (and ablates some of the verification stages), it\u2019s not clear (i) how much the method relies on retrieval versus generation (ie. when retrieval fails), (ii) how viable the generated assets are versus retrieval; and (iii) how important the different moving parts are beyond the specific verification stages (ie. how important is it to have both Bard and BLIP-2? Why Sentence-Bert?)\n\n**Training Supervision Validity**: This would be more convincing with any quantitative results, even something like the average number of decompositions per proposed task; the average duration to solve each full task versus sub-goals; performance if solving the full task directly via planning (if possible), etc.\n\n**Skill Learning Performance**: The quantitative results show improvement over an RL-only baseline, but it would be more helpful to show this over many more than 4 tasks; and also report the relative performance of all three methods (ie. separating trajectory optimization and planning over action primitives). Action primitives look pretty high-level: grasping, approaching and releasing a target object. How often is this route selected? And how much of the performance is due to working with an easier planning problem in a much higher-level action space rather than RL?\n\n**System**: I think final system performance needs to be a quantitative analysis. As it stands, I unfortunately don\u2019t have a good sense for how well the overall method works, in terms of how many different tasks it can solve and to what degree, and the nature of those tasks (eg. what objects, what behaviour/affordance, etc).\n\n\nAll in all, I was intrigued by the ideas proposed in this paper, and believe that such a method can be impactful. I would like to be in a position to accept this for publication, but feel that more quantitative analysis is required before that is possible."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6409/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698841695863,
        "cdate": 1698841695863,
        "tmdate": 1699636713196,
        "mdate": 1699636713196,
        "license": "CC BY 4.0",
        "version": 2
    }
]