[
    {
        "id": "jKsbDhV1pE",
        "forum": "V6JRkfj9dU",
        "replyto": "V6JRkfj9dU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3380/Reviewer_xTTm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3380/Reviewer_xTTm"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose that the generalization error of neural networks scales with a rate of 1/\\sqrt{n}. Their theoretical analysis establishes a mini-max risk by assuming bounded parameter norms, while empirical evidence supports the 1/\\sqrt{n} rate in MNIST and CHP datasets.\n\nWhile the paper addresses an important question related to the scaling law, I find the theoretical evidence somewhat lacking. The main point of the paper is not entirely clear, and the authors need to articulate their primary contribution better. Moreover, there is limited discussion of the technical challenges and the extension of previous work to ReLU networks. (See limitation part for more details.)\n\nTherefore, I cannot recommend acceptance for the time being."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The authors establish a mini-max bound for ReLU neural networks, which appears to be valid.\n2. While the main contribution needs further clarification, the topic itself is interesting."
            },
            "weaknesses": {
                "value": "My major concern is that it seems that the justification of this paper is not enough. \nI am still confused about which point is the main point that the authors want to show. \nThere are many alternatives, but they did not convince me.\nFor example, the technical contributions? The authors claim that previous works cannot be applied to ReLU networks, but I am not sure whether this extension has many technical difficulties.\nAnother one is that the theory can match the practice, but I believe that the empirical observations in this paper are not enough to support this (with only the MNIST and CHP datasets).\nSo I suggest that the authors could first tell me what the *major contribution* is and then convince me that the major contribution is significant. \nI would increase my score then.\n \nFor the theory part:\n1. The authors assume a bounded weight norm. However, this is not true. Some existing papers [e.g., uniform convergence may be unable to example generalization in deep learning] have pointed out that the weight norm may increase with n, which may change the rate.\n2. The authors only focus on the mini-max rate without considering the optimization performance. This is dangerous.\n3. The result requires d to be large enough. How large? Would it be exponential with the sample size n?\n\nFor the empirical part:\n1. More experiments (e.g., CIFAR-10, ImageNet) should be added.\n2. The theory shows that the network size is not important; can the authors provide more experiments on this? \n3. If there are experiences that do not perform 1/\\sqrt{n} rate, could the authors provide some reasons?\n\nBesides, I do not think adding too many derivations in the main body of the paper is good. This only harms readability. Why not provide some insights and defer all the details to the appendix?\n\nIn conclusion, while this paper touches on an important topic, it requires substantial improvements in terms of theoretical clarity, empirical evidence, and overall presentation."
            },
            "questions": {
                "value": "See limitations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3380/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698236082245,
        "cdate": 1698236082245,
        "tmdate": 1699636288772,
        "mdate": 1699636288772,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XgnPzeh0JV",
        "forum": "V6JRkfj9dU",
        "replyto": "V6JRkfj9dU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3380/Reviewer_NyqE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3380/Reviewer_NyqE"
        ],
        "content": {
            "summary": {
                "value": "This paper studied how much data is required to get a well-generalized neural network. It explores the generalization error of neural networks and suggests that it scales at $1/\\sqrt{n}$ in terms of the sample size $n$. In detail, the authors provide both the upper and lower bounds for a specific neural network architecture and data distribution.  The authors also conducted some experiments to support their claims."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper offers theoretical insights into the generalization error of neural networks, providing both upper and lower bounds.\n\n- This paper not only establishes a bound for deep-ReLU networks but also empirically validates its findings."
            },
            "weaknesses": {
                "value": "- This paper appears to overclaim its results. Titled \"HOW MANY SAMPLES ARE NEEDED TO TRAIN A DEEPRELU NEURAL NETWORK?\" it investigates a much narrower setting than suggested. Firstly, the paper overlooks the training process, assuming that the optimizer can derive the best learner from a given function class. Therefore, the term \"train\" in the title is misleading. Secondly, the scope is limited to feed-forward neural networks with square loss, neglecting widely used structures like CNNs and transformers. In light of this, I recommend that the authors either provide additional commentary within the paper or refine the title and contributions to reflect the actual scope of the study.\n\n- The paper adopts a teacher-student network setting with Gaussian input $x$ and noise $u$. The impact of the scale of $x$ and noise $u$ on the results in Theorem $1$ remains unclear. Additionally, the l1 constraint $v_1$ mentioned in Theorem 1 is not reflected in the final bound in (5), and the reason for this omission is not clear.\n\n- Various papers demonstrate that a faster rate $O(1/n)$ is achievable under specific settings. It would be beneficial if the authors could address these findings, particularly discussing the conditions under which this faster rate could be realized for DNNs.\n\n[1] https://dl.acm.org/doi/pdf/10.5555/3455716.3455772\n\n[2] https://dl.acm.org/doi/pdf/10.5555/3455716.3455772\n\n[3] https://proceedings.mlr.press/v54/mehta17a/mehta17a.pdf\n\n[4] https://proceedings.neurips.cc/paper_files/paper/2015/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf"
            },
            "questions": {
                "value": "- What will happen if we consider a noiseless teacher-student model, i.e., $\\sigma = 0$?\n\n- This paper considers the setting where the input $x$ is isotropic. Will the results still hold when the covariance of the input $x$ is not identity?\n\n- Each optimization algorithm has its own implicit bias. Will the results improve or worsen, given another specific data model and a specific algorithm?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3380/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698888067737,
        "cdate": 1698888067737,
        "tmdate": 1699636288703,
        "mdate": 1699636288703,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QqEW0xjBxG",
        "forum": "V6JRkfj9dU",
        "replyto": "V6JRkfj9dU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3380/Reviewer_NcCL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3380/Reviewer_NcCL"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces an algorithm-free lower bound on the generalization error for deep ReLU neural networks by building up on Fano's inequality from information theory. The lower bound scales as $\\frac{1}{\\sqrt{n}}$ instead of the usual $\\frac{1}{n}$ which proposes that when considering data-agnostic scaling laws, $-\\frac{1}{2}$ exponent is more suitable than $-1$."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I like the step-by-step explanations of the paper and I like that it is focused on the rate $\\sqrt{\\frac{\\log(d)}{n}}$ of the generalization error. \nI was not familiar with Fano's inequality but the refs in the paper to Wainwright are to the point and it was sufficient to understand the results of this paper.\n\n- Elegant use of Fano's inequality for ReLY nets\n- It is interesting that deep networks follow the same rate as shallow networks. This is consistent with the literature.\n- Simulations show better fit to $c_1 \\frac{1}{\\sqrt{n}} + \\alpha $."
            },
            "weaknesses": {
                "value": "Although the authors do a good job of explaining the important components of their main result, the organization of the paper can be improved much further. See below for some points  \n\n- I'd prefer deferring the proof of theorem 1 to the appendix as it is not very insightful (In fact the proof is a straightforward combination of the lemmas). \n- For the proof of Lemma4, the authors give an 8-step explanation which is simply reading through each step in the following inequalities. I'd prefer it if the most important step was emphasized and the other steps were left to the reader. For me the most important step there is the explicit integral formulation of $D_\\text{KL}$ but I have no idea where $\\frac{1}{\\sigma^2}$ comes from. \n\nOn the other hand, I find the empirical evaluation limited as it is. There is only one architecture used for Figure 1 (a certain width and depth, which would be good to include in the caption). Is this proposed scaling law also valid for really narrow networks like say width 20? The rate here does not depend on the architecture, is it also reflected in practice?\n\nMinor feedback:\n- Eq (2), please specify that all activation functions are ReLU already here. \n- In conclusion, please specify \"networks\" as deep ReLU feedforward networks."
            },
            "questions": {
                "value": "I am confused about the input data distribution. It is specified as $\\mathcal{N}(0, I)$ at the beginning of Section 2 however layer then the authors introduce $L_2(\\mathbb{P}_x)$ norm. Is $\\mathbb{P}_x$ here Gaussian. If so, $h(x)$ comes from the metric $\\rho$, not from the density of the distribution $\\mathbb{P}_x$?\n\nI would increase my score if the authors provided more experiments (see weaknesses) and/or provided an alternative in-depth discussion/intuition instead of the proof of 4.1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3380/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699313669523,
        "cdate": 1699313669523,
        "tmdate": 1699636288641,
        "mdate": 1699636288641,
        "license": "CC BY 4.0",
        "version": 2
    }
]