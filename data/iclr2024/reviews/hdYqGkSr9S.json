[
    {
        "id": "LHRz0QNJJD",
        "forum": "hdYqGkSr9S",
        "replyto": "hdYqGkSr9S",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4022/Reviewer_jHnf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4022/Reviewer_jHnf"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on exploring the zero-shot recognition of Vision-Language Models (VLMs) in terms of Granularity and Specificity. Driven by this objective, this paper provides a benchmark to test the behavior of VLMs on different levels of granularity. \n\nThe main two observations are  1) VLMs cannot correctly classify images from both fine-grained and coarse-grained perspectives. They are better at moderately fine-grained concepts.  2) In the meantime, this work also shows VLMs cannot detect whether the text correctly describes the given image. Instead, they could be sensitive to the specificity of text and struggle to distinguish between the challenging positives like single-label prompts, and hard negatives like poisoned captions with small changes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "+ [*The motivation is sound and convincing*] I like the idea of studying the VLM from granularity and specificity. For each perspective, this work provides a corresponding benchmark for measuring the performance. Such a study can provide insights into the zero-shot performance of VLM. \n\n+ [The evaluation way is interesting and reasonable] The evaluation protocol compares a) the prompt of the coarse-grained class and b) aggregates the predictions from prompts of its fine-grained children classes. This is intuitive and makes sense to me. From such protocol, this work draws the claim that VLMs are better at moderately fine-grained concepts. But I do have a concern, please see the weakness.\n\n+ [Specificity Robustness is well-delivered] From such analysis, we know the scores of contrastive vision language models can easily be distracted. I could guess such observation may be caused by the training data where the text is uncorrupted. I am wondering any solution to mitigate such specificity."
            },
            "weaknesses": {
                "value": "- [Hierarchical Labels vs. Coarse-grained Labels] There is a paper (Chils: Zero-shot image classification with hierarchical label sets in ICML 2023) that shows that using fine-grained/ subclass-level labels helps zero-shot classification. It seems the conclusions of this work and ICML work are different. Please discuss this.\n\n- [Specificity of Text] I think the observation that VLMs could be sensitive to the specificity of text is straightforward. When training VLMs, they are given specific texts. If we use the proposed modified version for training, I would expect them to perform better. Please comment on my thoughts."
            },
            "questions": {
                "value": "- Please discuss the different observations between your work and ICML work. Also, comment on my thoughts for the bias caused by the training manner. \n\n- Section 4.2 (Impact of Pre-training Data) is not clear to me. Please elaborate your points here."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4022/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698751406539,
        "cdate": 1698751406539,
        "tmdate": 1699636364996,
        "mdate": 1699636364996,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "M7JQnO5kvl",
        "forum": "hdYqGkSr9S",
        "replyto": "hdYqGkSr9S",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4022/Reviewer_stdj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4022/Reviewer_stdj"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a benchmark for evaluating the granularity and specificity of text prompts in vision-language pretraining such as CLIP. Experiments are conducted over several state-of-the-art vision language models (e.g., CLIP, OpenCLIP-LAION, BLIP, etc.). For granularity, the benchmark includes images from ImageNet-1K dataset in which the label hierarchy is extracted by WordNet. The evaluation includes matching the cosine similarity of an object name with its parent and child category names. Results show that the models are better at recognizing moderately fine-grained concepts than course-grained concepts. For specificity, the data for evaluation includes captioning example from MS-CoCo, in which caption text is adjusted. The results show that short (single label) or long captions can produce lower scores than captions with correct specific details that also include small errors. Overall, the evaluation points to limitations of these models in the zero-shot classification task."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The study and finding of the limitations of VLMs is of interest."
            },
            "weaknesses": {
                "value": "The evaluation on the technical side is rather straightforward, and I am not sure how surprising the results are. Although this study may provide few valuable insights, I do not think it meets the ICLR novelty threshold."
            },
            "questions": {
                "value": "none"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4022/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760118053,
        "cdate": 1698760118053,
        "tmdate": 1699636364860,
        "mdate": 1699636364860,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "l4X9K2rAlk",
        "forum": "hdYqGkSr9S",
        "replyto": "hdYqGkSr9S",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4022/Reviewer_pPRh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4022/Reviewer_pPRh"
        ],
        "content": {
            "summary": {
                "value": "Authors analyze the performance of off-the-shelf VLMs (e.g. CLIP, Open-CLIP, BLIP, FLAVA, etc.) on their ability to correctly match paired visual and textual descriptions at different levels of granularity and specificity. To this end, authors propose two benchmarking protocols based on ImageNet and COCO-Captions. They find that VLMs achieve the highest accuracy at recognizing \"moderately fine-grained concepts\" and VLMs struggle to correctly rank different captions with incorrect details. Authors evaluate several VLMs under their proposed setup and provide analysis on their results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Representative Baselines. Authors evaluate a representative set of VLMs with different backbones and pre-training datasets. \n- Sensible Analysis. Authors conduct sensible analysis on the OTS pre-trained models to back up their claims. All of the results of the analysis are intuitive and reasonable."
            },
            "weaknesses": {
                "value": "- Lack of Novel Insights. The primary claims in this paper about the performance of VLMs on specificity and granularity are not new, primarily because these have been explored in the language community [1, 2]. This is particularly important because both specificity and granularity are primarily language-based reasoning tasks. For example, an image of a leopard is unlikely to be close in feature space to an image of a bird even though both are examples of animals. In contrast, the language embedding contributes significantly more to this problem. Similarly, the evaluation of specificity is largely dependent on the text encoder's ability to capture fine-grained details (which is well studied in the language community.) Further, the conclusion that the distribution of data plays a significant role in VLM performance has been explored in [1]. Since VLMs are often trained on uncurated web-sources, we expect that they will capture the distribution and correlations found on the web. Therefore, the conclusion that VLMs are better at recognizing \"moderately fine-grained concepts\", is simply a restatement that VLMs are better at matching images with commonly occurring captions. To take an extreme example, a VLM is unlikely to score the concept \"object\" and an image of a \"leopard\" highly since these hierarchical correlations (while true) rarely occur in the training data. Similarly, a VLM is unlikely to score the concept of \"leopard\" highly with its scientific name \"Panthera pardus\" because it is also rarely seen during training. With respect to the specificity benchmark, the insight that vision language models can be easily distracted has also been explored in [2] (Note that although [2] focuses on language, the textual bias of the specificity task makes this work relevant). \n- Textual Bias in Specificity Benchmark. As the authors point out in the supplement, language-only methods are competitive on this benchmark, suggesting that the protocol should be amended to more accurately to evaluate vision-language capabilities, rather than just language understanding. In fact, many VLM benchmarks face the same problem of not requiring vision to solve the task. \n- On the Importance of Granularity from Vision. The granularity benchmark actually tests a model's alignment with word-net, which may not be universally accepted. For example, searching the web for \"road-cone\" and \"barrier\" show two visually distinct items. Although people may agree that road-cone and barrier are related, it is unclear if road-cone is a child class of barrier. Therefore, simply training a VLM in alignment with word-net rather than the broader internet may artificially boost performance on this benchmark, but perform worse in practice. \n\nReferences\n\n[1] Large Language Models Struggle to Learn Long-Tail Knowledge. Kandpal et. al. ICML 2023\n\n[2] Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts. Jang et. al. NeurIPS Workshop 2022.\n\n[3] When and Why Vision-Language Models Behave like Bags-Of-Words, and What to Do About It? Yuksekgonul et. al. ICLR 2023."
            },
            "questions": {
                "value": "- What is the Summary Metric? Although the in-depth analysis is appreciated, it is difficult to identify the best and worst performing methods (as is common in a benchmark). How can the methods be ranked for both benchmarks?\n- How Should Training be Amended? In light of the analysis of this paper, what are the right steps to amend VLM pre-training. Although authors suggest fine-tuning with hard-negative text prompts, this again seems to bias the model to this particular benchmark (no longer benchmarking zero-shot recognition), and does not provide a solution in general. \n\nPlease revise the text of this paper, there are many spelling and grammar errors (e.g. Figure 5 poisoneded)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4022/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4022/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4022/Reviewer_pPRh"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4022/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817975398,
        "cdate": 1698817975398,
        "tmdate": 1699636364791,
        "mdate": 1699636364791,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ph68hzMwc7",
        "forum": "hdYqGkSr9S",
        "replyto": "hdYqGkSr9S",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4022/Reviewer_PhzX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4022/Reviewer_PhzX"
        ],
        "content": {
            "summary": {
                "value": "This paper presents new benchmarks for Vision-Language Models (VLMs) in zero-shot recognition tasks, emphasizing granularity and specificity. The authors use adapted ImageNet and MS-COCO datasets to test VLMs' performance across varied granularity levels and their sensitivity to text specificity. Key findings reveal that VLMs excel at recognizing moderately fine-grained concepts but struggle with high-level coarse concepts and are sensitive to the specificity of language prompts. This in-depth evaluation uncovers significant limitations in state-of-the-art models like CLIP."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is clear, and well-written. \n2. The authors point out an interesting problem regarding granularity and specificity existing in current vision-language alignment models. It designed two simple benchmarks to evaluate those perspectives.  Their benchmarks address an important gap, making the paper a valuable resource for other researchers.\n3. One of the standout findings, the improved performance through direct propagation from leaf nodes, offers fresh perspectives in the domain of zero-shot recognition. This insight could influence future model designs and strategies."
            },
            "weaknesses": {
                "value": "1. Lack of in-depth Analysis: The influence of prompt design on classification is a critical aspect that seems to be overlooked in the paper. An in-depth analysis of how prompt design effectiveness may sway classification results, especially in terms of granularity, is necessary to provide a comprehensive evaluation.\n\n2. Going Beyond Common Knowledge: The paper highlights that performance in granularity and specificity improves when the testing scenario mirrors the training set, a well-known fact in the field. A more substantial contribution could be made by delving deeper into this issue, perhaps by providing baseline solutions or strategies to enhance granularity and specificity in pre-trained models.\n\n3. Connection to Specific Vision Tasks: While the paper provides a high-level motivation for the need of such benchmarks, it stops short of connecting the dots to specific vision tasks. A more robust argument could be made by demonstrating how improvements in the proposed benchmarks translate to advancements in other vision-language tasks, such as open vocabulary object detection[1], open vocabulary tracking[2], and text-to-image generation[3]. At least, a high-level discussion is needed in this regard.\n\nThese points aim to encourage a more thorough exploration of the topics and a clearer demonstration of the benchmarks\u2019 applicability to real-world tasks.\n\n[1] Gu, Xiuye, et al. \"Open-vocabulary object detection via vision and language knowledge distillation.\" arXiv preprint arXiv:2104.13921 (2021).\n\n[2] Li, Siyuan, et al. \"OVTrack: Open-Vocabulary Multiple Object Tracking.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[3] Rombach, Robin, et al. \"High-resolution image synthesis with latent diffusion models.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022."
            },
            "questions": {
                "value": "Evaluation Using COCO: Why are COCO's original captions more effective than detailed ones? Is there any assurance that the model hasn't been exposed to any COCO images or captions during training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4022/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836894280,
        "cdate": 1698836894280,
        "tmdate": 1699636364697,
        "mdate": 1699636364697,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aIji41llrN",
        "forum": "hdYqGkSr9S",
        "replyto": "hdYqGkSr9S",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4022/Reviewer_1Zb8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4022/Reviewer_1Zb8"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to benchmark the zero-shot recognition ability of VLM in the axes of varying granularity levels and the specificity of language inputs. The findings show that existing VLMs tend to struggle with semantic granularity and are sensitive to text specificity. The study can help guide future works to benchmark and improve VLMs in these axes, making VLMs more robust and useful at scale."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper presents a thorough study on the zero-shot recognition capability of a suite of VLMs in terms of granularity and specificity. Both are important areas that the community should be aware of.\n\nThe findings on granularity are informative as it shows the VLMs are best at recognizing concepts in similar granularity as the training data distribution (raw scores of ancestor are much worse than aggregating max scores of the leaves). Figure 4 (left) connects the empirical findings of Figure 3 to the training data, which is convincing.\n\nThe observation of specificity is quite informative as it shows the model is best at associating images and texts with the right amount of information. \n\nThe studies in Table 2 and 3 are performed with a broad range of VLM, which makes the conclusion robust to modeling choice."
            },
            "weaknesses": {
                "value": "1. Apart from the analyses, it'd be great to explore some simple ideas to mitigate the granularity and specificity issues in these VLMs by light-weight finetuning. Some ideas include: \n- Augment the alt-text with LLM to increase the amount of information\n- Generate hard-negatives by replacing the nouns\n- Adjust the granularity by replacing the fine-grained concepts with their parent in the hierarchy\nSince the analysis points to the mismatch between training and test data distribution, it'd be natural to bridge the distribution gap by data augmentation.\n\n2. It's not obvious how to translate the findings of this work into improvements in the downstream application of CLIP. Take the granularity study for example. Let's say we want to build an open-vocabulary detector on the some super categories of LVIS. To achieve the best performance, we'd need a way to generate the fine-grained categories associated with those super-categories, where the super-categories may not be in the WordNet hierarchy. One idea is to use the LLMs to help generate fine-grained categories so that we can apply the max-score aggregation in Fig 2 (c).\n\n3. Although the analyses are comprehensive and interesting, the findings are not very surprising. It seems to boil down to the limitations of the data that naturally exists at large-scale at the end of the day. I'd recommend to take the discussion section outside of conclusion and expand on it more to address the limitations."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4022/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698973315107,
        "cdate": 1698973315107,
        "tmdate": 1699636364628,
        "mdate": 1699636364628,
        "license": "CC BY 4.0",
        "version": 2
    }
]