[
    {
        "id": "MKyBzvCGAy",
        "forum": "BdPvGRvoBC",
        "replyto": "BdPvGRvoBC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5493/Reviewer_EwA6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5493/Reviewer_EwA6"
        ],
        "content": {
            "summary": {
                "value": "The paper provides a tight convergence analysis of federated averaging with clipping under two scenarios: per-sample clipping, where sample gradients are clipped during local optimization, and per-update clipping, where sample gradients are not clipped but the entire user update for each round is clipped. It demonstrates that per-sample clipping converges to a neighborhood of a stationary point, while per-update can converge to any accuracy if the inner step size is small enough. An extended analysis that includes added noise for differential privacy is provided."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The writing is very clear and the analysis is insightful. Clipping in FL is an important problem to study, as some means of bounding sensitivity is needed to achieve differential privacy."
            },
            "weaknesses": {
                "value": "The experiments would be stronger if continued for more communication rounds.\n\nMinor things:\nThe \"assumptions\" column of Table 1 needs formatting (sometimes assumptions are referred to as An, sometimes just as n)"
            },
            "questions": {
                "value": "What is the meaning of the dotted line in Figs 1/2c?\n\nIt is unusual as far as I know to obtain a DP guarantee with per-sample clipping. Does that come from bounding the sensitivity by taking the worst case ||y_i|| given the fixed number of local steps \\tau? That seems like it would be really weak. So I'm surprised that \"per-update is no better than per-sample in terms of the optimal privacy/utility trade-off\". Can you provide any intuition there?\n\nCould there be any utility in doing *both* per-sample and per-update clipping? How hard would it be to extend the analysis to that case, and if you can, what does it say?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5493/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698362162950,
        "cdate": 1698362162950,
        "tmdate": 1699636561438,
        "mdate": 1699636561438,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7PnADseym2",
        "forum": "BdPvGRvoBC",
        "replyto": "BdPvGRvoBC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5493/Reviewer_VZxT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5493/Reviewer_VZxT"
        ],
        "content": {
            "summary": {
                "value": "In summary, this work focuses on analyzing the effect of per-sample clipping and per-update clipping in private federated learning theoretically, and improves the theoretical results from previous works. Specifically, the major improvement lies in two aspects: 1. Fewer assumptions than previous works. Previous works like Zhang et al. (2022) and Yang et al. (2022) rely on extra assumptions like the uniformly bounded stochastic gradient or bounded $\\beta$-moment of the stochastic gradient; 2. Convergence rate under the arbitrary clipping threshold, while previous works only provide rate under specific choices of the clipping threshold."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "As far as I can see, when compared to existing works, the theoretical bounds in this work are certainly more appealing from three aspects.\n\n1. Relying on minimal assumptions for federated learning.\n\n2. Convergence guarantee under arbitrarily clipping threshold. In my view, this is the most important improvement compared to existing work, because, in practice, the clipping threshold is usually a hyperparameter. A continuous bound on the clipping threshold is certainly more helpful for understanding the effect of this hyperparameter.\n\n3. A more interpretable bounds that uncover the relationship among convergence, data heterogeneity, and clipping threshold."
            },
            "weaknesses": {
                "value": "So far I didn't see a major weakness in this work, and the theoretical results appear to be correct, although I didn't carefully check the math details. \n\nWhile I acknowledge that this is certainly a solid work, I would not consider the contribution significant. Because, firstly, the contribution is mainly on the theoretical exploration, and does not lead to practical guidance for hyperparameter tuning. On the other, there has been some theoretical exploration on this topic, and it seems that many proof techniques of this work come from Koloskova et al. (2023). Therefore, I give a \"fair\" score for the contribution and only recommend it for borderline acceptance."
            },
            "questions": {
                "value": "So far I have no other questions"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5493/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638784084,
        "cdate": 1698638784084,
        "tmdate": 1699636561335,
        "mdate": 1699636561335,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OlzNnyNP9V",
        "forum": "BdPvGRvoBC",
        "replyto": "BdPvGRvoBC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5493/Reviewer_CsYx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5493/Reviewer_CsYx"
        ],
        "content": {
            "summary": {
                "value": "The paper theoretically analyzes the convergence of the FedAvg algorithm with per-sample or per-update gradient clipping on heterogeneous data. They prove the upper bound for the expectation of gradient norm during training for a general class of learning objectives that satisfies bounded gradient variance, bounded gradient dissimilarity, and distributed $(L_0, L_1)$-smoothness. The main theoretical insights are as follows.\n- Under per-sample clipping, the expected gradient norm converges to a neighborhood with a size that depends on the gradient dissimilarity, the stochastic variance in gradient, and the clipping threshold (even when we set the step-size to be infinitesimal).\n- By contrast, under per-update clipping, the expected gradient norm can converge to an arbitrarily small level when the clipping threshold is reasonably large and the step-size is small, at the cost of more communication rounds.\n\nThe authors also perform logistic regression on MNIST to numerically support their insights on the effect of data heterogeneity on convergence under clipping."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The analysis in this paper holds for arbitrary choice of clipping threshold, while prior works generally either assume a large enough clipping threshold or assume homogeneous data.\n- The authors drew interesting comparisons between two clipping methods, per-sample clipping, and per-update clipping, highlighting that the quality of converged solution under per-sample clipping is highly limited by data heterogeneity. In contrast, per-update clipping enjoys convergence to arbitrary accuracy under data heterogeneity."
            },
            "weaknesses": {
                "value": "1. The authors proved drastically different convergence results under per-sample clipping and per-update clipping, even though the two clipping methods are equivalent when $\\tau = 1$ (despite a local step-size $\\eta_l$). This seems counterintuitive and needs more clarification. \n2. Although the analysis holds for arbitrary clipping threshold, satisfying convergence to an accurate solution still relies on setting a large enough clipping threshold. This insight makes sense theoretically (as a larger clipping threshold enables the training process to be closer to unclipped training), yet it is quite different from practice. (For DP learning, generally, a small clipping threshold such as 0.1 enables good performance [a, b].) \n3. Another less critical weakness is that the main theorems (Theorem 1 and Theorem 2) seem to be direct extensions of the prior work [Koloskova 2023]. Consequently, it needs to be clarified how non-trivial are the additional efforts made in this work.\n\nReferences:\n- [a] Tramer, Florian, and Dan Boneh. \"Differentially Private Learning Needs Better Features (or Much More Data).\" In International Conference on Learning Representations. 2020.\n- [b] De, Soham, Leonard Berrada, Jamie Hayes, Samuel L. Smith, and Borja Balle. \"Unlocking high-accuracy differentially private image classification through scale.\" arXiv preprint arXiv:2204.13650 (2022)."
            },
            "questions": {
                "value": "- Could the author explain why this local update step-size $\\eta_l$ would contribute to significantly different convergence behaviors between per-sample and per-update clipping? See weakness 1 for details.\n- Could the authors comment on this discrepancy between recommended large clipping threshold in this paper and the small clipping threshold used for practical DP learning?\n\n\nOther minor comments:\n- Is there a reason why, in the experiments of Figures 1 and 2, the clipping threshold for per-sample clipping is chosen to be much larger than the clipping threshold for per-update clipping?\n- In Table 1, $L$ is not defined. In Theorem 1, $M$ is not defined. In Corollary 1, $g_{i, t}$ cannot be found in Algorithm 1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5493/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5493/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5493/Reviewer_CsYx"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5493/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764310180,
        "cdate": 1698764310180,
        "tmdate": 1700577150439,
        "mdate": 1700577150439,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HGfKoroeq9",
        "forum": "BdPvGRvoBC",
        "replyto": "BdPvGRvoBC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5493/Reviewer_6Kwh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5493/Reviewer_6Kwh"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of clipping in federated learning. More specifically, the authors consider the per-sample and per-update clippings in FedAvg, and derive the corresponding convergence guarantees of FedAvg with these two clipping techniques. The authors also discuss how these two results can be utilized in the differentially private federated learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The strengths of the paper are as follows:\n1. The authors provide the convergence rate of FedAvg with two different clipping techniques.\n2. The authors show that how their results can be applied in the privacy protection setting."
            },
            "weaknesses": {
                "value": "The weaknesses of the current paper are as follows:\n1. It is unclear how the results are appropriate for privacy setting.\n2. The results cannot recover the unclipped results when the clipping parameters goes to infinity."
            },
            "questions": {
                "value": "The problem studied in this paper is very interesting and can be very useful in other related problems, such as differentially private federated learning. However, I have several questions about the current paper:\n1. It seems that when we choose $c$ as infinity, the results (e.g., Theorem I) cannot reduce to the unclipped results (e.g., LocalSGD  Koloskova et al. 2020), and I'm wondering what steps cause this discrepancy?\n2. I'm not sure how the results can be applied to the differentially private setting. The authors consider the stochastic setting, and thus the authors need to specify what is the dataset you want to protect when you apply Corollary I and Corollary II. From my understanding, it would be more meaningful if the authors can provide the finite sum results with bounded stochastic gradient assumption instead of the stochastic setting with bounded variance assumption for the application of differentially private setting.\n3. On page 5, comparison to the previous works, why you can claim that the established results can recover the rate of the centralized clipped mini-batch SGD?\n4. I'm wondering when you consider the finite sum with bounded stochastic gradient assumption, how the lower bound result will look like in terms of the clipping parameter $c$ and the bounded gradient norm?\n5. According to Corollary I and Corollary II, it seems to me that there is no need to use any local update. If this is the case, why don't you just use the private variant of the Minibatch SGD?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5493/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785476850,
        "cdate": 1698785476850,
        "tmdate": 1699636561110,
        "mdate": 1699636561110,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sTvLXtdkow",
        "forum": "BdPvGRvoBC",
        "replyto": "BdPvGRvoBC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5493/Reviewer_6LCG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5493/Reviewer_6LCG"
        ],
        "content": {
            "summary": {
                "value": "This paper studies two different clipping methods, per-sample clipping and per-update clipping, in distributed DP-SGD. Per-sample clipping captures a case where the user clips the update in each local iteration; per-update clipping is more similar to a local SGD scenario where users do local updates for multiple rounds before clipping the update and communicate with the central server. This paper demonstrates that when the clipping threshold is large enough or learning rate small enough, for second-moment bounded gradient, the bias due to clipping can go to zero."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper generalizes the convergence analysis in SCAFFOLD to study the clipped DP-SGD. The proof seems solid and the claims on the clipping bias make sense to me."
            },
            "weaknesses": {
                "value": "1. My main concern is that the authors do not make the implication of the theoretical results presented clear. I am puzzled by the motivation of studying Algorithm 1 with a fixed learning rate. It seems that Algorithm 1 can also apply both an inner and an outer learning rate and they should produce the similar bias control as what is claimed in Algorithm 2. So, this make the comparison between Algorithm 1 and 2 in Section 3.3 very confusing, given that after incorporated with the inner learning rate, both of them can achieve arbitrary accuracy. So, what do the analysis want to tell? Which clipping method should we apply in practice? Overall, this makes the practical impact of the theoretical results weak. \n\n2. The privacy-utility tradeoff is not well studied in this paper. In Appendix D.4, the authors briefly discuss the iteration number T required to achieve the balanced point between convergence progress and utility loss resulting from noise. I would suggest plugging the expression of v with $\\epsilon$ and $\\delta$ into the bound, and compare with existing works, such as \"Differentially private empirical risk minimization revisited: Faster and more general\". It is not clear to me whether the proposed analysis brings any improvement either in terms of the optimal utility-privacy tradeoff or the efficiency, i.e., the convergence time. The clipping bias needs to be carefully controlled such that the three terms: convergence advancement, utility loss by noise, and the bias should be all in the same degree. From the complicated expression, I find it hard to figure out in practice, do we want a large bias but slow convergence with large DP noise or the converse way.  \n\n3. For an ICLR paper, I feel the experiments need to be strengthened. The experiments on both MNIST and CIFAR10 do not report the test accuracy but only the loss. Still, we do not know **under the corresponding  optimal parameter selection** of the two clipping methods, which one performs better in practice. There is no comparison presented with existing empirical works on DP-SGD, such as \"Unlocking High-Accuracy Differentially Private Image Classification through Scale\". \n\n4. The code is not released."
            },
            "questions": {
                "value": "1. What is the optimal utility-privacy tradeoff/ utility bound under the optimal parameter selection of Algorithm 1 and 2 (in particular, Algorithm 1 with both inner and outer leaning rate)? In particular, what is the tradeoff considering the bias caused. \n\n2. Which clipping method we should use in practice?\n\n3. Can the different clipping methods produce better performance compared to existing works?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5493/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5493/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5493/Reviewer_6LCG"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5493/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698793061180,
        "cdate": 1698793061180,
        "tmdate": 1699636560992,
        "mdate": 1699636560992,
        "license": "CC BY 4.0",
        "version": 2
    }
]