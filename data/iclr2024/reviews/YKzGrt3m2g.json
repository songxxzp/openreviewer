[
    {
        "id": "fROtdD4Vjb",
        "forum": "YKzGrt3m2g",
        "replyto": "YKzGrt3m2g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2944/Reviewer_TsNZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2944/Reviewer_TsNZ"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the algorithmic side of in-context learning. The authors claim that the transformer implements ICL via the iterative Newton\u2019s method. The paper provides numerical comparisons among the behaviors of ICL, GD, and iterative Newton\u2019s method. The results show that the ICL is more similar to iterative Newton\u2019s method. The authors also provide the approximation results to support this claim."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper adopts different metrics to indicate that the algorithm implemented by ICL is closer to iterative Newton\u2019s method than GD. In addition, the ill-conditioned setting is also included to demonstrate this similarity. The theoretical results from the approximation view are provided to verify the claim in the paper."
            },
            "weaknesses": {
                "value": "Some of my concerns are not addressed after reading this paper:\n\n1. It is unclear why the similarity of errors is a reasonable metric. It would be helpful if the paper could provide more clarity on why the $l_2$ norm is not directly used to measure the distance between predictions instead of the error. In addition, the scale of the error is important to the problem in the linear regression problem. Since normalizing the error is mentioned in the paper, it would be beneficial to provide a more detailed explanation of why this normalization is necessary.\n\n2. I have a query about the reasonability of the definition of induced weights for the LLM. Concretely, given any examples in the prompt, if we scale the $x_{query}$ by a constant, will the output be scaled by the same constant? If not, it may be improper to define the induced weights for the LLM. More numerical verifications of this should be provided. This verification is essential since the softmax module is known to be a strongly non-convex function.\n\n3. There is some uncertainty about using the **best** hyperparameter to measure the similarity between algorithms as discussed in the paper. For example, we can simply define a new algorithm as a linear combination of the iterative Newton\u2019s method and GD. Trivially, this algorithm with the best hyperparameters will fit ICL of LLM better than iterative Newton\u2019s method, since iterative Newton\u2019s method is this new algorithm with the combination coefficient as $1$. Obviously, such a comparison is meaningless. The main reason is that setting the best hyperparameter gives too much **freedom** to the algorithm. Thus, it is very helpful to discuss why we can use the best hyperparameter to compare the algorithms in the setting of this paper.\n\n4. It will be beneficial to discuss whether it is possible to approximate the iterative Newton\u2019s method with causal attention. It will better align the theoretical results and the empirical findings."
            },
            "questions": {
                "value": "The questions are listed in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2944/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698252291848,
        "cdate": 1698252291848,
        "tmdate": 1699636238080,
        "mdate": 1699636238080,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "A5Dfe8r53t",
        "forum": "YKzGrt3m2g",
        "replyto": "YKzGrt3m2g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2944/Reviewer_zXY7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2944/Reviewer_zXY7"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates how transformers solve linear regression problems, providing a novel perspective compared to previous work which demonstrated that transformers could implement a gradient descent update at each layer. In contrast, this paper empirically demonstrates that transformers tend to implement an algorithm more akin to the iterative Newton's method. Furthermore, the authors present theoretical findings that prove the existence of transformer constructions capable of performing Newton's method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper introduces two distinct metrics for measuring the similarity between different algorithms applied to solving linear regression problems, contributing valuable tools for future research.\n2. A comprehensive set of experiments is conducted to delve into the relationships between Newton's methods, gradient descent, Transformers, online gradient descent, and LSTMs. The findings indicate that Transformers approximate the iterative Newton's method, whereas LSTMs align more closely with online gradient descent.\n3. The authors provide a proof, demonstrating the existence of a Transformer architecture with $O(k)$ layers that can perform $k$ iterations of Newton's method on given examples, which supports their empirical findings."
            },
            "weaknesses": {
                "value": "1. The manuscript appears to have been submitted without thorough proofreading, as it contains numerous typographical and grammatical errors. \n2. The paper lacks crucial experimental details. While some information is provided, such as data distribution, transformer architecture, and in-context length, many important implementation specifics are omitted. For instance, the paper does not disclose the learning rates used for the iterative Newton's method and gradient descent, the criteria for selecting $T$ in relation to $d$ for calculating similarity of induced weights, or the methodology employed to determine the best matched hyperparameters as per Definition 4.\n3. The depiction in Figure 3 raises concerns. Given a sufficient sample size and a noise-free data setting, all methods\u2014Newton's method, gradient descent, and transformers\u2014should empirically converge to a correct prediction. Intuitively, this should result in a similarity score of $0.99$ between the final output of the transformer (after the $L$th layer) and the other two methods. However, this is not reflected in Figure 3's right lower corner. This discrepancy prompts further investigation: could additional training iterations, an optimized learning rate, or other modifications enhance the gradient descent results, enabling it to make precise predictions akin to the other methods?"
            },
            "questions": {
                "value": "Please refer to the Weaknesses section for potential areas of inquiry and clarification."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2944/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2944/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2944/Reviewer_zXY7"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2944/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801092556,
        "cdate": 1698801092556,
        "tmdate": 1699636238001,
        "mdate": 1699636238001,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HArPgkOmLy",
        "forum": "YKzGrt3m2g",
        "replyto": "YKzGrt3m2g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2944/Reviewer_CBs3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2944/Reviewer_CBs3"
        ],
        "content": {
            "summary": {
                "value": "This paper performs numerical simulations to show that transformers learn to implement an algorithm very similar to Iterative Newton's Method rather than Gradient Descent. They also show that Transformers can learn in context on ill-conditioned data and theoretically show that transformers can implement k iterations of Newton's method with O(k) layers."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The papers perform numerical simulations to demonstrate that the algorithms implemented by transformers are more similar to Newton's method. The experimental results are interesting. \n2. The paper is written clearly."
            },
            "weaknesses": {
                "value": "1. This paper only compared two optimization methods: Newton's method and gradient descent (with a particular step size?). It is very straightforward to see that transformers can implement other first-order methods, including accelerated gradient descent and approximate message passing. It is not clear if there could be another first-order method that is more similar to Newton's method. \n2. Construction that transformers can implement Newton's method exists in [1] Section 8. \n\n[1] Looped Transformers as Programmable Computers. Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D. Lee, Dimitris Papailiopoulos."
            },
            "questions": {
                "value": "1. What is the step size for GD in experiments? Is the result robust to the tuning of step size? \n2. Why, in Figure 3, do Newton's method and Transformer layer 12 have a similarity of 0.994, but GD and Transformer only have a similarity of 0.88? Intuitively, GD and Newton's method should both converge to the minimum-norm OLS solution. In Figure 9, it seems to say that the CosSim of w_{GD} and w_{OLS} is small when the number of in-context examples is less than 20. However, people have proved that GD starting from zero initialization should converge to the min-norm least square solution. Why is there a mismatch between theory and your experiment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2944/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698948820302,
        "cdate": 1698948820302,
        "tmdate": 1699636237941,
        "mdate": 1699636237941,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VctOTioc8N",
        "forum": "YKzGrt3m2g",
        "replyto": "YKzGrt3m2g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2944/Reviewer_eSFG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2944/Reviewer_eSFG"
        ],
        "content": {
            "summary": {
                "value": "This paper showed that trained Transformers can implement some higher-order optimization method like Newton method. They measured  the algorithmic similarity based on the cosine difference of the errors and the induced weight, and showed that Transformers are more  similar to Newton's iterate rather than vanilla gradient descent. They showed that roughly one-layer of TF can  implement three steps of Newton's step on linear regression problem. They also provide theoretical guarantee that bi-directional attention model with ReLU activation function can implement k-steps of Newton's iterate using O(k) layers. \n\nGenerally speaking, this is an interesting work, but the result is not too solid. I will explain it in the weakness section. It is likely for me to raise my score given the rebuttal."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The insight that Transformers can implement higher order optimization methods which are superior to vanilla gradient descent is very interesting topic and worth investigating in the future. The motivation that viewing transformer as some meta-optimizer is very promising.\n\n2.  The visualization is clear and the theory looks good. They proved that O(k) layers of Transformers can implement k-steps of Newton's iterates, which is good.\n\n3. They considered the comparison to LSTM, as well as testing on the ill-conditioned linear regression case, which solidify this paper a lot.\n\n4. The literature review is good and the writing is clear and easy to follow."
            },
            "weaknesses": {
                "value": "1. The greatest issue is that there is gaps between the theoretical results and experiments results, so it is not clear **what algorithm does TF exactly implement?** The author proves that the TF can implements k-steps of Newton's iterates using O(k) layers (at least k layers, right?) But the experiments showed that TF can approximate 3 Newton's step using one single layer. This suggests that, probably, the trained TF ca do something smarter than Newton's step, and this is what we care about the most: can we discover some new or more efficient algorithms by Transformers?\n\nI know this is a difficult question, so I do not expect the authors to answer it thoroughly, but I think it will be better to have an explanation about 'why one layer of TF can implement 3 steps of Newton's step'? If you can provide some intuition about  how does it exactly achieve this, this will definitely be a perfect result and improve this paper by a lot."
            },
            "questions": {
                "value": "1. The second question is about figure 3 and figure 7. Since the main claim in your paper is 'TF are more similar to Newton's iterate than vanilla gradient descent' and your main evidence is these two figures, we need to examine these two papers carefully. \n\n1.1 In figure 7 for example, in column 8, there are 4 items with the same value (.988), why do you only highlight one of them? How do you choose it? Since you claimed that from 3th-9th layer, there is a linear trend for the number of Newton's iterate versus the number of TF layer, it matters which items you highlight. For example, if you highlight some other entries with the same value inside, this will not look like a linear trend at all. Similarly, in figure7 right, there are many 0.884 in the 7th column, so why do you highlight one of them?\n\n1.2 In both subfigure of figure 7, there are multiple items with very similar values. For example, in the column 9 on the left, there are .992, 0.991, 0.990, 0.993. Since you chose the highest value in each column, then one natural question is, is this choice statistically significant? Did you do the experiment for multiple times? Using different random seeds, will the items you chose change or not? I think since the values in grids are too closed, verifying the significance is critical. \n\n1.3 How do you tune the step size of the vanilla GD method and Newton's iterate? This is critical since different step sizes correspond to totally different learning algorithms.\n\n1.4 In principle, both Newton's method and vanilla GD will converge to OLS, but why the maximal similarity between TF and Newton is larger than the maximal similarity between TF and GD? For example, in figure 7, the maximal similarity between TF and Newton is 0.974, while the GD is 0.907, which is strange, right? With enough steps, both GD and Newton should approximately equal to OLS, so the maximal similarity value between  TF and GD (Newton) should be the same, when GD and Newton converge. Why are they different in your experiments?\n\n2. About LSTM:\n\n2.1 Why do you claim 'LSTM is more  similar to OGD than to Newton's method'? In the fig5 left, this shows that LSTM is more similar to '5 steps of Newton', while in the fig5 center, the LSTM curve is closer to that of OGD. Comparing these two, how do you compare which one is more similar to LSTM? Why don't you claim that LSTM is more similar to '5-step of Newton' based on the left figure?\n\n2.2 There is some issue with Table1. Your claim is 'LSTM is more similar to OGD than to Newton/GD'. In order to prove this, you need to compare the three numbers in the right column: 0.920, 0.808, .831. Based on this, actually LSTM is more similar to Newton than to OGD. In your paper, you highlight the larger number in each row, which actually means 'OGD is more similar to TF than to LSTM'. But this does not match your main claim. Do you have any explanation?\n\n3. About the theory: There is one issue for Theorem 1. You claim that 'there is a TF such that for any set of in-context sample ......'. This can not be true, since Newton's iteration require some condition on the \\alpha (in your definition of M_0) to converge. Basically, for each set of in-context example, even if your TF construction implement the Newton's iterate, you still need to choose the proper \\alpha to make this iterate converge. The proper choice of \\alpha should be depend on eigenvalues of X^\\top X, and the Transformer you construct should depend on this \\alpha, which again depend on your dataset {x_i, y_i}.\n\n4. Some small questions:\n\n4.1 In section 3, you first sample a covariance matrix \\Sigma from some distribution, and then sample feature vectors for each task, but in most of your results, you fix this covariance matrix. I suggest that you should build your set up based on this fixed covariance case, since basically training on fixed cov data and random cov data will be very different.\n\n4.2 In measuring the similarity between errors and induced weight, why do you use cosine metric instead of common L2 norm? is there a particular reason to use cosine? I think if your goal is to claim that TF is mimicking some existing algorithm, why not use L2 norm? A small cosine value only indicates the closeness in direction, which will weaken your arguments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "/"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2944/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2944/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2944/Reviewer_eSFG"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2944/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699684863966,
        "cdate": 1699684863966,
        "tmdate": 1699684863966,
        "mdate": 1699684863966,
        "license": "CC BY 4.0",
        "version": 2
    }
]