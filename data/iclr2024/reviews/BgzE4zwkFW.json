[
    {
        "id": "8XOwHBIyc5",
        "forum": "BgzE4zwkFW",
        "replyto": "BgzE4zwkFW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1960/Reviewer_cQrp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1960/Reviewer_cQrp"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a curriculum reinforcement learning approach, MECE, which optimizes an RL agent's morphology and environment through co-evolution. The authors train two policies to automatically modify the morphology and change the environment, creating a curriculum for training the control policy. Experimental results demonstrate that MECE significantly improves generalization capability compared to existing methods and achieves faster learning. The authors emphasize the importance of the interplay between morphology and environment in brain-body co-optimization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-structured, with a relatively clear introduction.\n\n2. The paper includes comprehensive experiments on rigid robot co-design tasks, demonstrating the superiority of the proposed algorithm. The ablation studies effectively isolate each component's contribution and provide valuable insights into the algorithm's effectiveness."
            },
            "weaknesses": {
                "value": "The significance of the paper's contributions is a bit unclear. It is not the first to propose using co-evolution method to co-design brain, body and environment. The proposed methods should be compared with more strong baselines. Curiously, can and how this system extend to the real world?"
            },
            "questions": {
                "value": "1. How general is the proposed approach, beyond the tasks and environments considered in the experiments?\n\n2. Is the proposed MECE method computationally efficient?\n\n3. Have you encountered any scalability issues when applying MECE to more complex tasks or environments?\n\n4. It is not clear to me how environments are produced and how the agents perform in your environment (Figure 4), do you have a video?\n\n5. It seems that MECE's performance is not much better than Transform2Act, can you provide more results on different tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1960/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1960/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1960/Reviewer_cQrp"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1960/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698595118245,
        "cdate": 1698595118245,
        "tmdate": 1699651836294,
        "mdate": 1699651836294,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uCO3Ds5Hhe",
        "forum": "BgzE4zwkFW",
        "replyto": "BgzE4zwkFW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1960/Reviewer_jNfL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1960/Reviewer_jNfL"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the problem of joint optimization of the policy and the morphology of a learning agent. The authors\u2019 motivation is described in the claim written in the introduction: \u201ca good morphology should improve the agents\u2019 adaptiveness and versatility, i.e., learning faster and making more progress in different environments.\u201d To realize it, the authors propose the novel framework where the morphology and the training environment are jointly evolved. In the proposed MECE scheme, three policies are introduced: one for the control of an agent\u2019s action, one for the evolution of the morphology, one for the evolution of the training environment. Inside this scheme, the authors define reward functions for the training of the morphology policy and for the training of the environment policy. The authors have performed comparison with several baseline approaches on three control tasks and ablation studies have been conducted to confirm the effectiveness of each algorithmic component."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. A novel framework for morphology optimization aiming at obtaining a morphology under which a control policy  can be quickly adapted to an unseen environment.\n\n2. Promising empirical results compared to baseline approaches, but the experimental procedure is questionable, see below."
            },
            "weaknesses": {
                "value": "1. As far as I understand from what is written in the introduction, the motivation of the morphology optimization in this paper is to obtain a morphology with which the agent can adapt its policy quickly to unseen tasks. It is also written in the second question of the experiments. However, it seems that the reported results in figures are average performances of the agent obtained at each training time step on randomly-selected environment. Therefore, the performance evaluated in this paper is the one for domain randomization. It is different from the motivation. The efficiency of the adaptation of the policy under the obtained morphology is not evaluated. My understanding might be wrong as the evaluation procedure was not clearly stated. Please clarify this point. \n\n2. It could be better if the design choices of the proposed approach is more elaborated. In particular, it is not clear how the reward functions (1) and (2) reflect the author\u2019s hypotheses \u201ca good morphology should improve the agent\u2019s adaptiveness and versatility, i.e., learning faster and making more progress in different environments\u201d and \u201ca good environment should accelerate the evolution of the agent and help it find better morphology sooner\u201d. It is also not clear why the authors want to train policies for morphology evolution and environment evolution instead of just optimizing the probability distributions over these spaces, despite the fact that these policies are not used afterwards and only the obtained morphology is used in the test phase. \n\n3. The clarity of the explanations could be improved. First, the notation inconsistencies makes it confusing. For example, r^m vs r_m, r^E vs r_e, and E and Env. If they are the same, please use the same notation. Algorithm 2 was also not very clear. How could pi_m be updated by using D where transition history doesn\u2019t necessarily have a reward information r_m? The same applies for pi_e."
            },
            "questions": {
                "value": "Please clarify the points given in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1960/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799894030,
        "cdate": 1698799894030,
        "tmdate": 1699636127380,
        "mdate": 1699636127380,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AI2yo9zU9I",
        "forum": "BgzE4zwkFW",
        "replyto": "BgzE4zwkFW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1960/Reviewer_QUnH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1960/Reviewer_QUnH"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an approach to co-optimize both the morphology and environments of robots. The morphology and controller of the robot is updated, while the environment is progressively changed. The result of the employed co-evolutionary approach are environments that progressively get more complex, providing a good learning signal for the agent. The approach is compared to ablated versions, which demonstrate that the co-evolution of morphology and environment is beneficial, in addition to comparisons with modifications of methods such as POET, which typically only optimize the robot\u2019s controller but not its morphology."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Interesting approach that could make robots more robust to varying environments\n- Good ablation baseline comparisons"
            },
            "weaknesses": {
                "value": "- Environment modifications seem limited (e.g. only environment roughness in the case of the 2D environment)\n- Comparisons to other methods are a bit ad hoc, e.g. as the authors note, POET was not developed to deal with changing morphologies. In addition to randomly sampling environments here, I would suggest a slightly more advanced baselines that samples environments of increasing complexity\n \nMinor comment:\n\n\"CMA-ES (Luck et al., 2019) optimizes robot design via a learned value function.\u201d -> their method is not called CMA-ES. CMA-ES is used an evolution strategy for  optimisation"
            },
            "questions": {
                "value": "- \"When the control complexity is low, evolutionary strategies have been successfully applied to find diverse morphologies in expressive soft robot design space\u201d -> how does the control complexity in this paper compare to the one by Cheney et al.? One could say the soft robots in Cheney et al. (2013) are more complex than the robots co-evolved in this paper.\n- How expensive is the approach of co-evolving the three different policies? And how does the computational complexity compare to the other baseline approaches?\n- It would be good to see some pictures of the evolved environments\n- What would happen if you start 3d-locomotion and gap-crossover with the same initial robot as in 2d-locomotion? There already seems to be a lot of bias given with the initial design."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1960/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833013742,
        "cdate": 1698833013742,
        "tmdate": 1699636127300,
        "mdate": 1699636127300,
        "license": "CC BY 4.0",
        "version": 2
    }
]