[
    {
        "id": "N0fxeLf8ln",
        "forum": "2UxSXuzrap",
        "replyto": "2UxSXuzrap",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2202/Reviewer_3r2o"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2202/Reviewer_3r2o"
        ],
        "content": {
            "summary": {
                "value": "Unlearnable example attacks are used to safeguard public data against unauthorized training of deep learning models. To break the data protection of unlearnable example methods, this paper introduces UEraser, an adversarial augmentation (PlasmaTransform, TrivialAugment and ChannelShuffle) based method to help model to learn semantic information from unlearnable example. Similarly to adversarial training, UEraser tries to find an augmentation to maximize the training loss rather than to find a perturbation to maximize the training loss (adversarial training does)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper focuses on the under-explored topic to learn semantic information from unlearnable examples, which is interesting and impressive.\n2. This paper is organized logically and written clearly. The visualization results of different unlearnable example methods are impressive.\n3. Impressive accuracy improvements on on four datasets (CIFAR-10, CIFAR-100, ImageNet, SVHN) compared with standard training."
            },
            "weaknesses": {
                "value": "1. The motivation and insight of this paper are not clear. As mentioned in Sec. 1 \" These perturbations can form \u201cshortcuts\u201d [12, 16] in the training data to prevent training and thus make the data unlearnable in order to preserve privacy.\", [12,16] does not reveal that unlearnable perturbations are shortcuts from models. Such two references are not suitable and convincing. Is there any observation or experiment to confirm that unlearnable perturbations are shortcuts from models? The same confusion occurs in Sec. 3.2 \" Geirhos et al. [12] reveal that models tend to learn \u201cshortcuts\u201d, i.e., unintended features in the training images.\". [12] only expounded that models prefer to learn shortcuts. Whether and why unlearnable perturbations are shortcuts from models is not clear at all.\n2. The explanation of why UEraser performs better than adversarial training is self-contradictory. As mentioned in Sec. 3.2, this paper believed that UEraser augmentation policies more effectively preserve the original semantics in the image than adversarial training. However, adversarial training set a $\\ell_{p}$ bound to constraint larger change on original input. The distance between adversarial training input and original input is much more small than the distance between UEraser augmented input and original input. In this case, whether UEraser augmentation policies more effectively preserve the original semantics in the image than adversarial training is quite doubtful. This paper should focus on analyzing why UEraser augmentation works and adversarial training does not. More insights should be proposed rather than engineering experiments.\n3. As shown in Table 2, CutOut, CutMix does not work on unlearnable examples, but ChannelShuffle does. Not sure if all spatial transformation-based augmentation does not work and color transformation-based augmentation does? If so, why not filter out spatial transformation-based augmentation in TrivialAugment like shear and rotate to improve performance?\n4. Clerical errors in Sec. 3.2. \"Compared to UEraser, although UEraser-Lite may not perform as well as UEraser on most datasets, it is more practical than both UEraser-Lite and adversarial training due to its faster training speed.\" should be \"Compared to UEraser, although UEraser-Lite may not perform as well as UEraser on most datasets, it is more practical than both UEraser and adversarial training due to its faster training speed.\""
            },
            "questions": {
                "value": "1. It is confused about the experimental setup. In Sec. 4 this paper resizes all images of ImageNet-subset to 32x32. However, as mentioned in Table 8, the input size of ImageNet-subset is 224x224x3. Which one is the real experimental setup? In addition, the operation of resize should not be implemented to ImageNet-subset, because the efficient of UEraser should be verified on high-resolution images perturbed by unlearnable methods, which are aligned with the experimental setup of EM and REM.\n2. This paper repeats that UEraser-Lite has a fast training speed. How fast it is? I'd like to know the comparison results of execution time between UEraser (UEraser-Lite, UEraser, UEraser-Max) and adversarial training.\n3. Why the augmentation of UEraser-Lite is Channleshuffle? Is there any other augmentation (equalize, posterize, plasmabrightness) that can achieve the same result as Channleshuffle?\n4. Table 2 and Table 3 should add the results of UEraser trained model on clean data. Considering as an attacker, you have no idea whether the training is clean or not. The results of adversarial training and other unlearnable methods (AR, OPS, TAP, NTGA, HYPO) in Table 3 should be shown out.\n5. There is no sensitivity analysis of hyperparameter W. How to pick a suitable value of W and K when deploying UEraser?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2202/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698599233965,
        "cdate": 1698599233965,
        "tmdate": 1699636154251,
        "mdate": 1699636154251,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dWc9hbxv3E",
        "forum": "2UxSXuzrap",
        "replyto": "2UxSXuzrap",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2202/Reviewer_E5wZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2202/Reviewer_E5wZ"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the following adversarial poisoning problem: An adversary uses training data with \"small perturbations (so in particular not hard to distinguish with normal training data\" to tamper the training process so that the trained model will generalize poorly in the distribution sense.\n\nThe paper proposes a new \"data augmentation\" mechanism, which is solve objective (3) under various augmentations.\n\nQuite some experiments are performed to demonstrate the effectiveness of the method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall this paper is a nice read. There is a clear principle that guides the paper (objective (3)), and the results seem pretty solid. UEraser seems to work pretty well in all experiments.\n\nIt is also a bit surprising that \"data augmentation\" actually works. What is missing in previous work?"
            },
            "weaknesses": {
                "value": "For one thing, the paper seems rather straightforward in principle, we try to solve the objective (3) under various augmentation methods. So as we expand the augmentations, we should get better results.\n\nOne thing is what is really the cost (which I don't think the paper has much discussion), do we need a tremendous amount of augmentation in order to make sure the learning does not learn the short cut? Also, what happens if the adversary is aware of the augmentations the training method is using? (so in that sense the paper needs to be more clear about what is the security model -- that is, what knowledge does the adversary has?)"
            },
            "questions": {
                "value": "I don't have specific questions, my main concerns are described above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2202/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698601364095,
        "cdate": 1698601364095,
        "tmdate": 1699636154165,
        "mdate": 1699636154165,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nxUbN609jd",
        "forum": "2UxSXuzrap",
        "replyto": "2UxSXuzrap",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2202/Reviewer_6kgo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2202/Reviewer_6kgo"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles unlearnable example attacks in deep learning, introducing \"UEraser,\" a defense method utilizing adversarial data augmentations to neutralize these attacks. UEraser extends the perturbation budget beyond typical attacker assumptions, aiming to maintain model accuracy on poisoned data without significant loss on clean data. A faster variant, \"UEraserLite,\" is also presented. The authors demonstrate UEraser's effectiveness across various state-of-the-art unlearnable example attacks, outperforming existing defenses and showing resilience against adaptive attacks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper introduces \"UEraser,\" a novel defense against unlearnable example attacks using adversarial data augmentations. This approach is novel and extends the perturbation budget beyond typical attacker assumptions, showcasing a new direction in defending against these types of attacks.\n\n- The authors have conducted extensive experiments to validate the effectiveness of UEraser against various state-of-the-art unlearnable example attacks. The results demonstrate that UEraser outperforms existing defense methods and is resilient against potential adaptive attacks, providing a strong empirical basis for the proposed approach.\n\n- The introduction of \"UEraserLite\" offers a faster and more efficient alternative to UEraser, making the proposed defense more accessible and practical for real-world applications."
            },
            "weaknesses": {
                "value": "- While the paper presents a novel approach to defending against unlearnable example attacks, the contribution can be considered incremental. The use of adversarial training and data augmentation for defense is not entirely new, and the extension to unlearnable example attacks, while valuable, builds upon existing knowledge and techniques.\n\n- The paper could benefit from exploring additional evaluation of UEraser and UEraserLite, particularly in terms of computational efficiency/time cost compared with existing defenses. \n\n- The paper primarily focuses on empirical results, and a more comprehensive theoretical analysis of why UEraser works and under what conditions it is most effective could strengthen the paper."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2202/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698754718503,
        "cdate": 1698754718503,
        "tmdate": 1699636154100,
        "mdate": 1699636154100,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TzubXWtUA5",
        "forum": "2UxSXuzrap",
        "replyto": "2UxSXuzrap",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2202/Reviewer_iWnX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2202/Reviewer_iWnX"
        ],
        "content": {
            "summary": {
                "value": "Unlearnable example attacks refer to data poisoning techniques used to safeguard public data against unauthorized training of deep learning models. These methods introduce subtle perturbations to the original images, making it challenging for deep learning models to effectively learn from such training data. Current research indicates that adversarial training can partially mitigate the impact of unlearnable example attacks, while common data augmentation methods are ineffective against these poisons. However, adversarial training is computationally demanding and can lead to significant accuracy loss.\n\nThis paper presents the UEraser method, which surpasses existing defenses against various state-of-the-art unlearnable example attacks. UEraser achieves this through a combination of effective data augmentation techniques and loss-maximizing adversarial augmentations. Unlike current state-of-the-art adversarial training methods, UEraser employs adversarial augmentations that extend beyond the assumed 'p perturbation budget' used by current unlearning attacks and defenses. This approach enhances the model's generalization ability, thereby safeguarding against accuracy loss. UEraser effectively eliminates the unlearning impact through loss-maximizing data augmentations, restoring trained model accuracies.  On challenging unlearnable datasets like CIFAR-10, CIFAR-100, SVHN, and ImageNet-subset, generated using various attacks, UEraser achieves good results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper highlights that unlearnable examples can be mitigated through various data augmentation techniques, potentially leading to the generation of more resilient unlearnable examples when facing adaptive poisoning."
            },
            "weaknesses": {
                "value": "1. After carefully conducting experiments of the UEraser using the official code provided by the authors, a notable disparity emerged in comparison to the reported results in the paper. On the CIFAR-10 datasets, specifically, for EM poisons, the best accuracy achieved during training was 74.46\\%, while the accuracy in the final epoch was 68.46% (25\\% lower than reported). In the case of LSP poisons, the best training accuracy reached 90.23\\%, with a final epoch accuracy of 73.43\\% (12\\% lower than reported). This observation indicates that although the UEraser appeared effective during mid-training, the models eventually converged to shortcuts present in the unlearnable examples. Given that there is often no clean validation dataset available, many papers on unlearnable examples (UE) typically report the accuracy achieved in the last training epoch. Furthermore, with a fixed learning rate of 0.01 throughout the training process, I've observed that the accuracy of models trained on clean images struggles to converge to a satisfactory level, reaching approximately 92% on CIFAR-10 (2% lower than reported) and around 70% on CIFAR-100 (4% lower than reported) in my experiments. Therefore, it is recommended that the authors thoroughly review their results and consider reporting the accuracy in the last epoch for a more equitable comparison.\n\n2. In the experiments conducted with the UEraser-Max approach, the performance on the EM and LSP attacks shows a gap compared to the reported results on the CIFAR-10 dataset (EM: 62.25\\%, 33\\% lower than reported; LSP: 88.33\\%, 6\\% lower than reported). Interestingly, the use of a fixed learning rate of 0.01 (too large to converge to the optimal model when conducting UEraser-Max) is not likely to reach an accuracy of 95.24% for EM poisons, which is even 0.5% higher than what is typically achieved in standard training on **clean** CIFAR-10.\n\n2. It's essential to consider the standard evaluation settings and practices in the field. The results on ImageNet-subset are not convincing, as most operations on ImageNet-subset in papers related with Unlearnable Examples do not resize the images to $32 \\times 32$. Instead, they follows the default data augmentations, and images are resized to $224 \\times 224$ during training, it would be advisable to align with these practices in your experiments for better comparability with previous works. For your reference, usually, the clean performance on the ImageNet-subset should be around 80\\%, and the performance of ISS facing several UE methods is around 55\\%.\n\n3. Training on CIFAR-10, CIFAR-100, and SVHN datasets takes a similar amount of time. To provide a more comprehensive evaluation of the proposed method's performance and robustness, it would be valuable to expand the experimental analysis to include CIFAR-100 and SVHN, similar to the approach taken for CIFAR-10. This extended evaluation would help assess how well the model generalizes across different datasets and under various UE attack methods.\n\n4. The proposed methods do not exhibit robustness to adaptive poisoning. Table 4 demonstrates that when faced with adaptive poisoning (UEraser-Max), the defensive performance experiences a significant drop, ranging from 15% to 30%. To facilitate a fair comparison, it would be beneficial to report the performance of adaptive poisoning when facing ISS.\n\n5. The concept presented in this work is not particularly innovative, and the achieved performance heavily relies on empirical augmentations for defense, which can be significantly undermined by adaptive poisoning."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2202/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2202/Reviewer_iWnX",
                    "ICLR.cc/2024/Conference/Submission2202/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2202/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761164130,
        "cdate": 1698761164130,
        "tmdate": 1700049732571,
        "mdate": 1700049732571,
        "license": "CC BY 4.0",
        "version": 2
    }
]