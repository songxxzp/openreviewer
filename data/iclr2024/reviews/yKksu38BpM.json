[
    {
        "id": "9fVryfqT9J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1902/Reviewer_LB8S"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1902/Reviewer_LB8S"
        ],
        "forum": "yKksu38BpM",
        "replyto": "yKksu38BpM",
        "content": {
            "summary": {
                "value": "This is an empirical paper on Neural Tangent Kernel (NTK) surrogate models. The content of the paper is two-folded:\n\n1. Several approximations of NTK are introduced and evaluated quantitatively by various metrics through different experiments, showing how theses approximations capture the decision mechanism of Neural Networks (NN) on classification problem.\n2. Then the paper argues how the NTK surrogate models give explanation for NN decision and states its limitation on SVM and adversarial attacks.\n\nThe paper includes a detailed and motivated introduction to Trace NTK and pseudo NTK, and experimental results on various data sets and  NN models. Its appendix contains a detailed section explaining the relationships between different kernels introduced in the paper, and a detailed result of the experiments together with visualisations."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality: The paper is innovative to use the Kendall-$\\tau$ rank correlation to evaluate the approximation, such as TrNTK, pNTK, CK...,  on the empirical NTK (eNTK). The angle to experiment on explaining NN by surrogate NTK is also novel. \n\nQuality: The paper is written nicely with rigorous definitions and detailed descriptions on the experiments. \n\nClarity: The paper clearly states the problem and presents their experiments. Also the motivation of the paper is clearly elaborated. \n\nSignificance: The paper is important in the area of explainable AI through the lens of surrogate NTK. This paper could lead to more research on related topics."
            },
            "weaknesses": {
                "value": "There is barely any flaws in the paper, and the limitation of the experiments is clearly stated in the limitations subsection in section 5."
            },
            "questions": {
                "value": "I have only one question:\nIn section 5, You mentioned: \"...an interesting follow-on work would investigate using kernel functions in K-Nearest Neighbors surrogate models.\" How much argument of this paper can transfer to KNN or generally any other surrogate models on explaining NN decision?\n\nAlso, there are some of the minor typos in the paper:\n\nSection 2 PRELIMINARIES Neural Networks for Classification third line: it should be \\mathcal{Y} instead of Y.\n\nAppendix F FORMAL DEFINITION OF EVALUATION METRICS last equation: it should be SS_res instead of SS_ret."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1902/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697457725298,
        "cdate": 1697457725298,
        "tmdate": 1699636120758,
        "mdate": 1699636120758,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LuyctMK6pE",
        "forum": "yKksu38BpM",
        "replyto": "yKksu38BpM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1902/Reviewer_dXvz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1902/Reviewer_dXvz"
        ],
        "content": {
            "summary": {
                "value": "This paper contributes to the growing literature of approximating NNs with simpler, more interpretable, models. A common approach is to approximate NNs with the empirical Neural Tangent Kernel (eNTK). However, computing the eNTK can be computationally unfeasible so simpler approximations have been proposed in the literature. The authors focus on this problem by studying the empirical properties of one such approximation, the trace NTK, and adapt random projection methods to make it more computationally attractive. Furthermore, the authors propose the Kendall rank correlation as a new measure to assess the faithfulness of the surrogate kernel method to the NN. \n\nThe main contribution of the paper is to show that the trace NTK and projected trace NTK can be used to generate faithful surrogate models of the underlying NNs. The authors show this through a variety of empirical exercises across different benchmark datasets (MNIST, CIFAR) and models (CNNs, ResNet18, BERT). They compare how good different NTK approximations are with respect to the underlying NN in terms of prediction error and rank correlation and find that trace NTK has good performance. Additionally, the authors compare the different models representations through a data attribution exercise and a poisoned data attribution exercise and identify which surrogate models perform better in each case.  Finally, the authors show the practicality of the projected NTK methods by analyzing computational complexity of each method. The most relevant finding is that the trace NTK and projected trace NTK  perform similarly in settings in which the projected trace NTK required an order of magnitude less computation time."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper tackles a common problem in an active area of research: that of finding computationally attractive NTK approximations to NNs. While the main proposed methods in the paper are drawn from the literature, the authors are original in adapting random projection methods to the trace NTK and pseudo NTK and considering the rank correlation as a sensible alternative to prediction error for assessing faithfulness.\n\nThe main strength of the paper lies in the extensive set of empirical evidence comparing various NTK approximations with different NNs architectures across tasks and datasets. Furthermore, the authors provide an extensive appendix with additional exercises and comparisons. Overall, researchers looking to use NTK approximations may find these exercises useful in choosing which method to use depending on their task and their computational constraints."
            },
            "weaknesses": {
                "value": "While the paper offers a wealth of empirical evidence for the methods it investigates, its central weakness is that it is unclear what the main findings and contributions are. The paper does a lot of things and it would benefit from more succinctly explaining what it is trying to achieve and how each exercise demonstrates it.  \n\n* The paper should be more clear about its relative contribution to the literature (and what its main contribution is). The paper gives confusing statements about what is new and what is taken from the literature. The authors state that the 3 main contributions are (1) new kernel functions for NTK approximation, (2) first to show that eNTK models are consistently correlated with NNs across experiments, (3) first to compare NNs decisions through NTKs through a data attribution and explain by example strategy. \n\n\n* For point (1) however the authors also state that the tr NTK was introduced in Chen et al. 2021 (end of page 2) and that the random projections approach is based on Park et al. 2023 (end of page 1).  Is the main contribution of the paper proposing a new NTK method or evaluating empirical exercises? \n\n* The authors consider different alternative NTKs to compare to the trNTK, but never compare in the main text the methods to the eNTK or the pNTK. Given that the motivation of the paper in the abstract, introduction etc is to approximate the eNTK it is odd that this not done in the main text of the paper. While computational constraints are important, maybe it could be done a simpler dataset (MNIST)? \n\n* For point (2) if using the rank correlation is new it should be clearly stated as a major contribution. The paper repeatedly expresses that other measures are flawed and while the authors give some reasons why, without a proper theoretical statement the authors should at least relate these notions more directly to the findings of the empirical examples. For example, what is a clear case in which using fit or pearson correlation would be misleading in the sense that two NTK models give you very different attributions despite having the same fit to the NN, but rank correlation is not misleading.\n\n* For point (3) the paper should explain more carefully why these are carried out. If the goal is to assess how good a NTK approximation is by considering whether it performs similarly to the NN in a data attribution task then this should be the focus of the results. It seems that the authors do these exercises in the appendix, but in the main write up they just give an instance of this and its unclear how much we can learn from it. If well addressed I would be inclined to raise my score.\n\n* The paper could benefit from better exposition and more clear presentation. For example, the choice of what is defined in the main text vs appendix and when it is defined is sometimes odd. The eNTK, while being referenced to extensively, is never properly defined in the main text. The, trNTK0 is introduced in Additional Kernel Functions after the trNTK without motivation, despite featuring prominently in the appendix when the different methods are compared.\n\n* The paper may also suffers from typos and plots are sometimes misleading (squished axis in Figure 1). Some typos include pseudo vs psuedo, missing points, figure labels that overlap, subscripts in mathematical notation etc. Some references are also repeated."
            },
            "questions": {
                "value": "Besides the questions raised in the weakness section regarding the key contributions. I also have some additional questions:\n\n* Which Chen et al. paper is the main reference for trace NTK, I was confused by the reference. \n* Is it true that when rank correlation <1 there exists not invertible mapping? \n* In the case in which the rank correlation is 1 is the invertible mapping unique? How does this result translate to the exercises and neural net behavior? Should we expect the same data attributions as the NN? Expanding on the implications of a good rank correlation vs test accuracy seems key to show the usefulness of the paper for researchers.\n* Given the data attribution with kernels theory in page 3, wouldn\u2019t you be able to test directly whether a kGLM is an \u201cideal surrogate\u201d (according to eq 2) by comparing across all data points the NN confidence in each class with the data attribution for each class? Is there a way beyond fit/correlation measures to more systematically compare how well the kernel performs in the data attribution exercise besides evaluating individual examples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1902/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1902/Reviewer_dXvz",
                    "ICLR.cc/2024/Conference/Submission1902/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1902/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698517307729,
        "cdate": 1698517307729,
        "tmdate": 1700664373667,
        "mdate": 1700664373667,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tey6gwytxC",
        "forum": "yKksu38BpM",
        "replyto": "yKksu38BpM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1902/Reviewer_e28a"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1902/Reviewer_e28a"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use the approximate empirical neural tangent kernel (eNTK) as a faithful surrogate model for neural networks. Focusing on NNs for classification, the authors define the trace neural tangent kernel (trNTK), which is the cosine similarity between the concatenated gradients of all logits with respect to the parameters for a trained NN. The trNTK is then plugged into a kernel general linear model (kGLM) to obtain a surrogate model for the NN, which can be used to attribute the prediction of the NN to the training data points. To evaluate the faithfulness of such surrogate models, the authors argue that a preferred way is to measure the rank correlation between the softmax probabilities of the surrogate model and the NN for the correct class. A random projection variant of the trNTK is also proposed to reduce the computational cost. The experiments show that the proposed surrogate model is generally more faithful than other kernel-based surrogate models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is clearly written and easy to follow.\n- The proposed surrogate model is simple and easy to implement. trNTK performs consistently better than other neural kernels.\n- The rank correlation seems to be a better metric than existing alternatives for evaluating the faithfulness of surrogate models for classification NNs. It takes into account the global structure of the predictions.\n- Based on the proposed surrogate model and data attribution method, the authors observe that the attribution is NOT dominated by a few data points. This is an interesting observation and has practical implications."
            },
            "weaknesses": {
                "value": "- Only the rank correlation of the softmax probabilities for the **correct** class is considered. However, to be faithful enough, the surrogate model should also behave similarly to the NN for the **incorrect** classes. An important application of data attribution is to explain why a NN makes a wrong prediction. This is not considered in the paper.\n- Eq. (4) is confusing. In the denominator, the $\\cdot ^ {\\frac{1}{2}}$ is applied to the inner product. However, according to Appendix C and the definition of cosine similarity, the $\\cdot ^ {\\frac{1}{2}}$ should be applied to the sum, not the inner product.\n- The quality of Figure 2 could be improved."
            },
            "questions": {
                "value": "- Is the non-sparsity of the attribution a general phenomenon or just a property of trNTK? Is it a consequence of the statement \"It has been suggested that this normalization helps smooth out kernel mass over the entire training dataset\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1902/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1902/Reviewer_e28a",
                    "ICLR.cc/2024/Conference/Submission1902/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1902/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698600416189,
        "cdate": 1698600416189,
        "tmdate": 1700663362726,
        "mdate": 1700663362726,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vL5v90bcV2",
        "forum": "yKksu38BpM",
        "replyto": "yKksu38BpM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1902/Reviewer_zBfF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1902/Reviewer_zBfF"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes new variants of eNTK and implements faster approximate versions as well, and then evaluates them on a few different tasks / visualizations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Paper evaluates variants of eNTK in further depth compared to prior work\n- Paper is relatively well written, though missing some important details"
            },
            "weaknesses": {
                "value": "- Content: Surfacing similar images is a not a meaningful evaluation of attribution. It is a good sanity check, but doesn't say anything about surrogacy. For example, finding similar images using CLIP similarity would also show similar images, though CLIP is in no way a \"surrogate\" to the model being studied\n\n- More broadly, I'd be more careful about making any claims of \"data attribution\" (which has a specific meaning as used in recent ML) as the paper does carry out any counterfactual evaluations.\n\n- Overall, the contributions seems somewhat marginal. Also, the fast approximate versions implemented primarily rely on prior work (Park et al.)'s implementation, so not sure there is much to claim as contribution there (since Park et al. also used it for faster approximations to eNTK).\n\n- Writing: is hard to follow at times and doesn't provide the relevant details (see Questions).\nOn one hand, the paper goes into more detail than necessary in defining rank correlation / R2, etc from scratch,\nand at the same time, doesn't actually provide details about what those measures are computed over exactly.\nIt's possible I missed it, but at least doesn't seem very clearly written based on my multiple attempts to parse this information."
            },
            "questions": {
                "value": "- Confused by what the rank correlation is measured over exactly. I understand it's measured between the truth model outputs and surrogate model outputs, but what is it varied over? Are you measuring across different inputs x?\n- A bit confused by what the message/takeaway of the box/distribution plots are. Can the authors elaborate?\n- It seems that the eNTK is only defined in Appendix D, so it's a bit hard to contextualize pNTK and trNTK when they are introduced"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1902/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698952909141,
        "cdate": 1698952909141,
        "tmdate": 1699636120506,
        "mdate": 1699636120506,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Fvy0f7tIYb",
        "forum": "yKksu38BpM",
        "replyto": "yKksu38BpM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1902/Reviewer_yHxi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1902/Reviewer_yHxi"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates kernel-based surrogate models based on various approximations of the Neural Tangent Kernel (NTK) to provide explanations for deep neural networks. A primary contribution is showing that computationally-feasible approximations to the empirical NTK provide high-fidelity surrogate models, and that much cheaper projection-based approximations provide accurate estimates of the empirical NTK. Appealing to existing literature on explanation-by-example, the paper develops a simple score for data attribution. A synthetic data experiment shows that the proposed attribution score accurately attributes erroneous model predictions to poisoned data, giving some confidence that the proposed score is capturing some notion of similarity between data points."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper provides a potential solution to a very important problem, i.e. data attribution based on a trained neural network checkpoint.\n* The authors draw a very important distinction\u2014which is somewhat obvious but not well-reflected in the literature\u2014between difference in test accuracy (TAD) and correlation of model outputs. The addition of Kentall-$\\tau$ is important, and will hopefully shift the way future works evaluate the fidelity of surrogate models.\n* The paper provides a clear definition of proposed kernel approximations, with strong computational justification for the speedup e.g. of the trace approximation. Equation 2 clearly defines the working definition of a \u201chigh-fidelity model\u201d and adds to the clarity of exposition.\n* The experiments include a sufficient diversity of alternative kernel estimates to demonstrate the value of the proposed projected-trace-NTK approach. The inclusion of uncertainty based on multiple runs is highly appreciated.\n* The inclusion of experiments on Bert-base significantly strengthens the paper, indicating that the method is not specific to the computer vision domain.\n* The discussion that explanations are not sparse is an important acknowledgement of the proposed data attribution method. In particular, the following statement is poignant: \u201cpresenting the top highest attribution training images without the context of the entire distribution of attribution is probably misleading.\u201d\n* The paper\u2019s title is very strong and well reflects the work\u2019s primary contributions."
            },
            "weaknesses": {
                "value": "* The paper relies on previous work to establish credibility of attribution-based scores for neural network explanation. It doesn\u2019t seem obvious that attribution is the same as similarity for learned kernel functions.\n* I find the second sentence in the abstract confusing. I expected this trend to have to do with using kernel-based models for data attribution rather than to \u201cinvestigate a diverse set of neural network behavior\u201d. Isn\u2019t the goal of your paper exactly to apply kernel models to investigate network behavior?\n* The 3rd experiment on qualitative evaluation of attribution is weak. A user study is probably beyond the scope of this paper, and I believe the work is strong enough to stand without such a study. However, the paper would significantly benefit from some discussions about how these attributions could be better qualitatively evaluated in the future.\n* The claim about Peason correlation is not very well explained: \u201cThese point clouds serve as anchors that force the covariance, and therefore Pearson correlation, to be large. We require a measure that does not conflate the covariance with faithfulness.\u201d Is the problem here that correlation is not computed between model logits for each test point?\n* The paper never explicitly defines the empirical NTK in its own notation. Could you add this prior to defining the trNTK or pNTK in order to allow an easier discussion of the approximations introduced?\n* The take-away from Figure 2 is not exactly clear. Is this just meant to show that attribution scores are not sparse?\n* Your Chen ICML\u201922 reference is duplicated. Did you intend to cite two different papers?\n* The notation is non standard. Most papers use $y$ not $z$ for ground-truth labels. I can see this causing some readers mild confusion.\n\nSmall issues:\n* In the last paragraph of the \u201cRelationship to the Pseudo Neural Tangent Kernel\u201d section, the reference to Eq 3 is to the wrong equation.\n* The inclusion of all four panes in Figure 1 seem a bit superfluous, I\u2019m not sure what this is supposed to show that cannot be shown in a single figure.\n* Given that one of the 3 experiments claimed in the paper is a qualitative evaluation, I think it is important to include one of the data attribution figures from the supplemental material in the main text.\n* The main text cites Figure 4.a, which is in Supplemental Material.\n\nSmall typos:\n* 2nd paragraph of the Introduction: \u201cIts well established\u201d should be \u201cit\u2019s\u201d (or better yet spell out \u201cit is\u201d to be less colloquial.\n* Undefined reference (?) at the bottom of page 2."
            },
            "questions": {
                "value": "1. Have you considered using the kernel function directly to evaluate sample similarity? Why do you choose to include the weights from the kernel machine in all attribution scores?\n2. You fit the parameters $W, b$ on the ground-truth labels $z$ from the  training dataset. If the goal is to create a surrogate that emulates a neural network, why don\u2019t you fit these parameters on cross-entropy loss with the class probabilities predicted by the NN? This would be consistent with your objective in Eq. 2.\n3. It is not totally clear how the Kendall-$\\tau$ statistic is computed. A couple of sentences would make this portion more reproducible. Do you take the matrix of all the logits produced on a test set ($N x C$) for the NN and for the surrogate model, flatten these two matrices into vectors, and compute the rank correlation? Is the rank correlation computed per-test-output and averaged over the test set?\n4. Why is trNTK initially introduced with cosine normalization and projNTK not? Don\u2019t your experiments include cosine normalization for all kernels?\n5. Is there any theoretical statement you can add about the variance of the projection-based kernel estimates, e.g. based on JLS? The choice of 10240 dimensions seems arbitrary and model-dependent.\n6. Does the introduction of cosine normalization explain the experimental result \u201cthat the highest attributed images from the trNTK (and furthermore all evaluate kernel functions) have relatively small mass compared to the bulk contribution, suggesting that the properties of the bulk\u201d?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1902/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1902/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1902/Reviewer_yHxi"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1902/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699111973356,
        "cdate": 1699111973356,
        "tmdate": 1699636120441,
        "mdate": 1699636120441,
        "license": "CC BY 4.0",
        "version": 2
    }
]