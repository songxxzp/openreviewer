[
    {
        "id": "oh9pV1SqZe",
        "forum": "13D1zn0mpd",
        "replyto": "13D1zn0mpd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7115/Reviewer_a2T8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7115/Reviewer_a2T8"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the storage challenges posed by models fine-tuned for multiple downstream tasks. It presents efficient methods for reusing fine-tuned models, specifically PERU-FFT which merges and prunes task-specific vectors of fully fine-tuned models, and PERU-LoRA which employs a lower rank matrix for approximating the LoRA matrix in LoRA fine-tuned models. Through experiments in computer vision and natural language processing, the proposed methods significantly surpass traditional model merging techniques in performance, and while being more parameter-efficient, they match the performance of dedicated task-specific fine-tuned models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well written and easy to follow. The proposed methods are not only effective but are also parameter-efficient, making them valuable for real-world applications."
            },
            "weaknesses": {
                "value": "The primary limitation of the paper is its constrained novelty. The methodology seems to draw heavily from pre-existing techniques, and it lacks distinct contributions that set it apart. Please see questions for details."
            },
            "questions": {
                "value": "1.\tThe novelty of the proposed method appears somewhat constrained. At a glance, the approach seems to be an amalgamation of model merging, and parameter-efficient tuning and pruning. The good performance of the proposed method mainly comes from the task-specific parameters. It would be valuable for the authors to elaborate on any unique contributions or differentiating aspects of their method that go beyond this apparent synthesis.\n\n2.\tIn Section 3.2, the authors have chosen to use singular value decomposition (SVD) to approximate the product $A_t B_t^{\\top}$. An intuitive approach might be to directly reduce the rank of matrices $A_t$ and $B_t$. Can the authors provide insights into why this direct rank reduction was not pursued? Are there challenges or drawbacks associated with it that the proposed SVD method circumvents?\n\n3.\tIn the proposed method, several points of clarification regarding the comparison with LoRA emerge. Firstly, it would be beneficial to understand what distinguishes the proposed method from LoRA. While the primary technique appears to focus on reducing the rank of $A_t B_t^{\\top}$. Secondly, when referencing Table 4, one can observe that PERU-LoRA (16) has 232M parameters, which is only a marginal 3% reduction compared to the Single-Task model, yet its performance seems to lag behind. It raises the question of whether this slight reduction in parameters warrants the observed decrease in performance. Expounding on these aspects would provide a deeper understanding of the method's value proposition and potential areas for improvement.\n\n4.\tThe proposed approach appears to have a broad compatibility with several existing model merging methods, including Fisher-Merging (Matena & Raffel, 2022), RegMean (Jin et al., 2023), and TIES-Merging (Yadav et al., 2023). It would be insightful to understand how the performance of the presented method fares when juxtaposed with these model merging methods. \n\n5.\tThe supplementary material provided appears to be incorrect. It seems the authors mistakenly included the main text within the supplementary section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7115/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7115/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7115/Reviewer_a2T8"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7115/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698483943876,
        "cdate": 1698483943876,
        "tmdate": 1700651967017,
        "mdate": 1700651967017,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FcSe8y2EU5",
        "forum": "13D1zn0mpd",
        "replyto": "13D1zn0mpd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7115/Reviewer_tpTw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7115/Reviewer_tpTw"
        ],
        "content": {
            "summary": {
                "value": "The paper targets the task of merging multiple fine-tuned task-specific models into a single multi-task model in a training-free manner. For full fine-tuned models, authors inject a sparse task vector into the merged model by magnitude pruning. For LoRA fine-tuned models, authors use a lower-rank matrix to approximate the LoRA matrix by singular value decomposition. By adding a small number of task-specific weights to the merged model, the merged model shows significant improvement over existing methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The writing is clear and easy to read.\n2. The performance improvement is significant. \n3. The method is evaluated on both CV and NLP with different backbone sizes."
            },
            "weaknesses": {
                "value": "1. The paper targets the task of merging multiple fine-tuned task-specific models into a single multi-task model. However, why is this problem important? What's the practical value and potential impact in real-world scenarios? Could you give a specific real-world example where a user needs such a merge model? Model users are typically interested in the performance of their tasks. If the user needs to deal with multiple tasks, it's not very likely that some fine-tuned models online can directly solve their problems unless the fine-tuned data are almost the same as the users. \n\n2.  TIES-Merging has shown that keeping only the top-20% of the parameter based on magnitude does not degrade the performance too much. The proposed method just adds the top-m% parameters to the merged model. I think it's quite incremental and the contribution is marginal. \n\n3. Typically, users need a multi-task model to handle data in a similar domain. The current benchmark is so diverse, with handwritten digits, remote-sensing, cars etc datasets. It would be great to see the performance of the merging algorithm on similar datasets. Could the authors provide merging performance within the Natural, Specialized, and Structured groups in the VTAB dataset? \n\n4. Given the prevalent use of CNN-based models like ResNet in various domains, it is crucial to understand how well the proposed PERU-FFT method generalizes to these architectures. Including experiments or discussions on this aspect would broaden the paper's applicability and appeal to a wider audience.\n\n\n5. Because PERU adds task-specific weight to the model, I think it would be great to compare it with multi-task training performance, where the multi-task model is trained on all data with a specific head for each task. \n\n6. The authors did not mention the limitations of their method and potential future work. The paper does not explore or discuss potential failure cases of the proposed methods. Understanding when and why the methods might fail is crucial for practical applications."
            },
            "questions": {
                "value": "Please refer to the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7115/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7115/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7115/Reviewer_tpTw"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7115/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698698977991,
        "cdate": 1698698977991,
        "tmdate": 1699636840995,
        "mdate": 1699636840995,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "V9Hr6gS7nW",
        "forum": "13D1zn0mpd",
        "replyto": "13D1zn0mpd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7115/Reviewer_wHfz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7115/Reviewer_wHfz"
        ],
        "content": {
            "summary": {
                "value": "Utilizing distinct fine-tuned models for different tasks imposes a significant burden on storage and servicing requirements. Current merging methods exhibit a significant performance disparity when compared to single-task models. This work proposes a Parameter-Efficient method for ReUsing fine-tuned models (PERU), which enhances by employing pruning on fully fine-tuned models and implementing Singular Value Decomposition (SVD) on LoRA fine-tuned models. The experimental result demonstrates that PERU outperforms existing merging methods and achieves comparable performance compared to single-task fine-tuned models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The use of pruning and SVD on merging fine-tuned models is simple but effective.\n2) The result is a significant improvement over the previous methods."
            },
            "weaknesses": {
                "value": "1) According to Algorithm 1 and Algorithm 2, the PERU method still uses a specific model for a specific task, which is different from previous merging methods that use the same parameters for all tasks. In the paper, you mention that merging task models into a shared one will cause a parameter inference problem, which affects the performance. While PERU circumvents this issue, it introduces a new challenge by employing task-specific parameters. Consequently, direct comparisons between PERU and other methods may not be entirely equitable.\n\n2) The absence of ablation experiments utilizing only a subset of tasks. As the number of tasks increases, will the performance of your method be affected?"
            },
            "questions": {
                "value": "1) Do previous methods typically require the specification of dataset sources when addressing different classification tasks? If not, is your method capable of performing similarly without declaring the dataset origins for each task?\n\n2) With a limited scope of only two or four tasks, does your proposed method continue to significantly outperform existing merging methods?\n\n3) What is the result if we don\u2019t prune for $v_t$ or $u_t$ and we direct prune $\\theta_t$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7115/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7115/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7115/Reviewer_wHfz"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7115/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761482431,
        "cdate": 1698761482431,
        "tmdate": 1699636840884,
        "mdate": 1699636840884,
        "license": "CC BY 4.0",
        "version": 2
    }
]