[
    {
        "id": "ls688g6Lxg",
        "forum": "sHEJJmzBIN",
        "replyto": "sHEJJmzBIN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2876/Reviewer_rZ5m"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2876/Reviewer_rZ5m"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed an adversarial training strategy to enhance the generation ability of LLMs, it generated multiple external branches along the input sequence and add RL loss over these branches. The manual evaluation on 10k+ annotations is conducted to demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.A GAN-style training schema is proposed based on benefit of parallel computation of Transformer, which generates multiple branches and results in dense learning signals, this improves the model generation ability without any new training data.\n2.Extensive human evaluation and analysis to demonstrate the effectiveness the method."
            },
            "weaknesses": {
                "value": "1.Both generator and discriminator need to be tuned. This results in expensive computation and memory consumption, especially for large LLMs."
            },
            "questions": {
                "value": "1.For different generation tasks (such as Wiki, Fiction generation), was the proposed method tuned separately? In other words, did the current version support multi-task tuning?\n2.For sequence S with length N, K unique starting points is randomly sampled from [1, N-1] as branches. As each generated branch consists of d (e.g., 16) tokens, if the sampled starting point is N-1, is the corresponding generated branch easy to be recognized than that corresponding to other points? Or should we sample the starting point from the range [1, N - d]?\n3.For each branch, if the termination token was generated, was the branch still generated until its length = d? \n4.How about the training complexity w.r.t. branch number and depth of generation?\n5.For robustness analysis, how to repeat the random selected characters? It is better to give an example. Is the noise sensitive to the position of selected characters?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2876/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698599691049,
        "cdate": 1698599691049,
        "tmdate": 1699636230947,
        "mdate": 1699636230947,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "62nhTIJvdI",
        "forum": "sHEJJmzBIN",
        "replyto": "sHEJJmzBIN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2876/Reviewer_MWH7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2876/Reviewer_MWH7"
        ],
        "content": {
            "summary": {
                "value": "The paper presents Branch-GAN, a novel method that improves text generation quality in large language models (LLMs) by using adversarial training. It overcomes the deficiencies of current models such as repetitive texts and looping issues by generating multiple branching sequences from each training sample, providing dense signals for both generator and discriminator. This leads to stable training dynamics and high-quality text generation, using only a fraction of the training data typically required. Human evaluations confirmed that Branch-GAN outperforms much larger baseline models and has robustness against noisy inputs. The paper concludes that adversarial training enhances LLMs' capabilities and suggests further exploration into adversarial learning for increased data efficiency and application to larger models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Advantages:\n\n - Higher Quality Text Generation: Branch-GAN can generate texts of significantly better quality on average compared to other models, as confirmed by human evaluations.\n - Data-Efficient Fine-Pretraining: The method achieves this enhanced performance while utilizing less than 0.01% of the original training data, indicating exceptional data efficiency. \n - Stable Training Dynamics: By generating multiple branching sequences from each training sample and processing them in parallel, Branch-GAN provides a dense and stable learning signal, leading to more stable training dynamics compared to existing language GANs that leverage sequence-level rewards (e.g. SeqGAN, RankGAN etc). I especially like the idea of using three prediction heads such that it implicitly incorporates the benefits of scheduled sampling, which is an algorithm that is known to practically help the model produce better outputs, despite some criticism from How (not) to train your generative model [https://arxiv.org/abs/1511.05101 ]."
            },
            "weaknesses": {
                "value": "Disadvantages:\n\n - Confusing Illustrations: While many of the important elements have been shown in Fig1, I would really appreciate if the authors can considering showing some technical details of the discriminator in Fig 1 too. With the current illustration it's really difficult to know enough details about how the dense adversarial rewards are constructed, which are actually the most novel part of the proposed method. (The efficient pretext encoding reusing trick is good, but it's not what I think that differentiates BranchGAN from other language GANs)\n\n - Lacking Mode Collapse Detection and/or Diversity Measure [Addressed during Rebuttal]: According to some previous criticism against language GANs from Language GANs Falling Short [https://arxiv.org/abs/1811.02549 ], most previous language GANs that have shown effectiveness in producing more coherent output sequences generally can be fragile against the comparison of simply annealing the temperature of language models. In other words, it could be the case that BranchGAN, too, sacrifices diversity/mode coverage in exchange for precision/quality. A proper study on this is needed to respond to such challenges. **This is dangerously needing attention** because the remarkably increased PPL of the BranchGAN-generated samples by Pythia 6.9b and remarkably increased generation quality reminds me of some previous language GANs that also over-claim themselves to be able to generate high-quality samples, eventually turned out to be rather trading-off between quality and diversity and had zero improvement when considering both aspects."
            },
            "questions": {
                "value": "See Weaknesses)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The dataset the models (Pythia/GPTNeoX and the BranchGANs finetuned from them) use, known as the Pile dataset, is now under controversy for its violation of copyright. It has been taken down (up to Oct. 2023) from the open Internet. While this does not affect the academic values of the paper, the authors and ICLR should be careful when open-sourcing the trained model parameters."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2876/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2876/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2876/Reviewer_MWH7"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2876/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820837259,
        "cdate": 1698820837259,
        "tmdate": 1700537171846,
        "mdate": 1700537171846,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vJhRQCaygT",
        "forum": "sHEJJmzBIN",
        "replyto": "sHEJJmzBIN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2876/Reviewer_Hqg8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2876/Reviewer_Hqg8"
        ],
        "content": {
            "summary": {
                "value": "This paper presents Branch-GAN, a method for improving text generation with Transformers using generative adversarial networks (GANs). Branch-GAN generates multiple branching sequences from each training sample, and trains the generator and the discriminator in parallel. The paper shows that Branch-GAN models produce higher quality texts than large language models (LLMs) and other baselines, according to human evaluation and automatic metrics. The paper also explores the effects of noise, depth, and sparsity on text generation, and discusses the potential applications and limitations of Branch-GAN."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. **Novel method**: A novel method for training Transformers in a GAN setup, which improves text generation quality with fewer model parameters and less data. I really like this idea to branch the sequence for efficient discrimination.\n2. **Human evaluation**: A large-scale human evaluation study that shows that Branch-GAN models generate significantly better texts than LLMs and other baselines.\n3. **Robustness**: An analysis of the robustness of Branch-GAN models to noisy contexts, showing that they can generate texts with stable TTR and perplexity."
            },
            "weaknesses": {
                "value": "1. **Lack of comparison**: The paper does not compare Branch-GAN with other language GANs or cooperative language GANs that use pre-trained Transformers.\n2. **Limited evaluation**: My main concern is that the paper only evaluates text generation on two domains (Wikipedia and Fictional Stories) and does not test the generalization ability of Branch-GAN on other domains or tasks, such as summarization, dialogue, or question answering."
            },
            "questions": {
                "value": "How well does Branch-GAN generalize to other domains or tasks, such as summarization, dialogue, or question answering? What are the challenges or limitations of applying Branch-GAN to these scenarios?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2876/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826400860,
        "cdate": 1698826400860,
        "tmdate": 1699636230775,
        "mdate": 1699636230775,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Wwn0id3STj",
        "forum": "sHEJJmzBIN",
        "replyto": "sHEJJmzBIN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2876/Reviewer_RVL4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2876/Reviewer_RVL4"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new strategy for training language models using a combination of adversarial and MLE style losses. Historically, Language GANs have been hard to get to work well---successful methods have mainly resorted to primarily relying on MLE losses with a minor amount of adversarial fine-tuning. This work tries to alleviate some of the optimization issues for discrete language GANs by using a efficient training strategy that simultaneously optimizes multiple generations that branch off of a supervised sequence at various points in time. Combined with a MLE + dense rewards criterion, the paper shows that an effective language models can be learned."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is clearly written, and the approach is simple while also appearing to be very successful. The results are also well-evaluated (at least for the specific text-continuation setting that is focused on). More broadly, I think that the paper introduces an interesting efficient batching technique for training language models with reinforcement learning --- that can possibly be applicable to other types of rewards beyond GANs (e.g., such as RLHF models). \n\nThe dataset of manual text quality rankings also seems like a valuable contribution if released, and the analysis on correlation (or lack thereof) of automatic metrics with human judgements could be of interest to practitioners."
            },
            "weaknesses": {
                "value": "Though the human evaluation on text completion is good, I'm really quite curious to see how the Branch-GAN performs on tasks that are more divergent from the pre-training setup, such as prompting tasks. \n\nI also find the loss function in Eq. (1) to be poorly motivated. Why choose this variant, over say REINFORCE/PPO with a baseline/advantage function? (I would also at least move more of the description of the loss in A.1 to the main text.)"
            },
            "questions": {
                "value": "- I'm not clear about Table 4 in the appendix --- which model is being evaluated? I would have thought Branch-GAN, but don't understand why it would be compared \"vs. Branch-GAN 1B*\". \n- I'm also curious about win-rate _between_ models (it's noted in Sec. 5 that GPT-4 is consistently preferred over Branch-GAN, but I could only find comparisons to original texts in the paper)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2876/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698908516220,
        "cdate": 1698908516220,
        "tmdate": 1699636230706,
        "mdate": 1699636230706,
        "license": "CC BY 4.0",
        "version": 2
    }
]