[
    {
        "id": "Essd2McrCC",
        "forum": "RT5yHR0zwp",
        "replyto": "RT5yHR0zwp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7348/Reviewer_ofst"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7348/Reviewer_ofst"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a lightweight point-based end-to-end network (PEPNet) for camera relocalization from event cameras.  The network extracts spatial and implicit temporal features through a hierarchical structure and then explicitly attains temporal information via an attentive bi-directional LSTM (A-Bi-LSTM).  Experimental results on the DAVIS 240C CPR dataset show its good performance as well as the effacacy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The way to treat the time dimension for event cameras in a point-based network is insightful.\n\n2) The resultant network is lightweight yet performs well on the benchmarking dataset."
            },
            "weaknesses": {
                "value": "-- The symbol for concatenation operation is ambiguous. The same symbol is also used in Fig. 3 (Extractor), but it seems it does not mean concatenation there.\n\n-- Fig. 4 can be replaced with qualitative comparison to the baselines rather than just some existing data samples from the dataset.\n\n-- Some citations for the baselines are missing in Section 4.2."
            },
            "questions": {
                "value": "1) Section 3.2.2: What are the dimensions of PG and PS? How is the substraction done? Could the authors elaborate this in detail?\n\n2) Section 3.2.3: Does f correspond with the ReLu operator in Fig. 3? It seems there is no ReLu before the summation operator in Fig. 3, but there is an f in Eq. (9). Why are they different?\n\n3) Are the random data train/test splits the same as the baselines? \"We randomly select\" sounds like the authors use a different random split although following the same strategy. I wonder whether the comparison on this setting is really fair or not.\n\n4) How is PEPNet_{tiny} obtained exactly?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7348/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7348/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7348/Reviewer_ofst"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7348/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698065872919,
        "cdate": 1698065872919,
        "tmdate": 1699636879095,
        "mdate": 1699636879095,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eHFxVlN8wz",
        "forum": "RT5yHR0zwp",
        "replyto": "RT5yHR0zwp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7348/Reviewer_ndwh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7348/Reviewer_ndwh"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the regression task of six degrees of freedom event camera poses. Considering that the existing methods neglect fine-grained temporal information in events, the proposed PEPNet directly deal with the raw point cloud to use the high-temporal resolution and inherent sparsity of events."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed PEPNet outperforms other methods in both performance and running speed."
            },
            "weaknesses": {
                "value": "Weakness:\n\n1.\tThe event camera data in the method is actually more like video data (not 3-D spatial data as point clouds), probably other video-based feature extraction methods could be utilized for the task.\n\n2.\tThe proposed hierarchy structure, Bi-LSTM, and attention mechanisms are very common modules in the area of CNNs models for processing video data. This reduces the contributions of the method a little. However, the method does achieve better performance than previous methods.\n\n3.\tIs there any ablation study on the loss term weight?\n\n4.\tAre there intermediate visualization results of the attention mechanisms to show what has been learned in the module?\n\n5.\tSome typos: Second line of P6."
            },
            "questions": {
                "value": "As above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7348/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7348/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7348/Reviewer_ndwh"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7348/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833443008,
        "cdate": 1698833443008,
        "tmdate": 1699636879003,
        "mdate": 1699636879003,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g97b5sJebj",
        "forum": "RT5yHR0zwp",
        "replyto": "RT5yHR0zwp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7348/Reviewer_9ZiA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7348/Reviewer_9ZiA"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a lightweight DNN for pose estimation using event-based camera data. The evaluation is done on a DAVIS-240C camera dataset that contains a variety of camera motions and egomotion ground truth."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is clearly written and illustrations support the text well. If the code for this work is released, this would be a notable contribution, as a lightweight method could be readily used in robotic applications."
            },
            "weaknesses": {
                "value": "1) The literature overview does not include a few works on event-based pose estimation - there are methods based on 3D pointcloud analysis (https://openaccess.thecvf.com/content_CVPR_2020/papers/Mitrokhin_Learning_Visual_Motion_Segmentation_Using_Event_Surfaces_CVPR_2020_paper.pdf), and self-supervised methods (https://arxiv.org/pdf/1903.07520.pdf) - and more, all of which were evaluated on more complex datasets than in this paper. How would these approaches (to DNN, loss functions, event encoding) compare with this work?\n\n2) CPR Dataset evaluated in the paper contains mostly planar scenes or one-dimensional motion, making it easier for the network to overfit. It is also hard to gauge the full 6dof performance of the method, when using simplistic data. I would recommend evaluating on MVSEC (https://daniilidis-group.github.io/mvsec/) and/or EV-IMO (https://better-flow.github.io/evimo/download_evimo_2.html) - both datasets have pose ground truth.\n\n3) Since the problem of motion estimation is geometric, it would benefit the method if the loss function incorporated some geometry constraints. In classic vision, sota egomotion pipelines leverage this successfully, and there were prior works on event cameras doing the same.\n\n4) On motion estimation problem, with a random split, it is highly likely the network overfitted."
            },
            "questions": {
                "value": "1) Minor, in abstract: \"These cameras inherently capture movement and depth information in events\" - I would argue that event cameras are similar to classic ones in captureing depth. They provide continuous 'tracking', but the depth is not directly measured. At the very least, the advantage is not obvious.\n\n2) In event-based processing it is specifically important to understand how exactly the events are fed into the DNN, and what are the implications of the approach / which other similar methods exist in literature. The Algorithm 1 answers this to a degree, but I am not sure I understand if the temporal window is always fixed, and if the number of the events within this window is subsampled to a constant value. What would happen if there are fewer events than Np (1024) due to the lack of motion?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7348/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699219161278,
        "cdate": 1699219161278,
        "tmdate": 1699636878895,
        "mdate": 1699636878895,
        "license": "CC BY 4.0",
        "version": 2
    }
]