[
    {
        "id": "2OemyJ4Nu3",
        "forum": "qJ5EFFGuFU",
        "replyto": "qJ5EFFGuFU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1037/Reviewer_vMNL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1037/Reviewer_vMNL"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an image inpainting approach. The authors addressed the shortcomings of existing implicit representation approaches that tends to ignore overall semantics of the image and only looks to preserve appearance. The authors proposed an implicit representation that adds semantic information of pixels through alignment with CLIP text features."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well written and easy to follow. \n- The motivation of the work is clearly outlined. \n- The integration of semantic information in preserving certain structures in the image during inpainting is intuitive and the experimental results demonstrate the effectiveness."
            },
            "weaknesses": {
                "value": "- I believe the technical novelty of the approach is limited since the improvement mainly comes from the rich representations of the clip embeddings. CLIP embeddings have been extensively used in many zero-shot tasks that exploit the strong semantics learned by the clip embeddings e.g. ZegClip (CVPR 2023), Hierarchical Text-Conditional Image Generation with CLIP Latents (arXiv 2022), NUWA-LIP (CVPR 2023). \n- None of the approaches that authors compare against use text embedding alignments. In particular, I believe a similar text-based alignment can be made with the implicit representations of LIIF. \n- Lack of comparison with recent approaches like NUWA-LIP.\n- How does the authors' approach compare against powerful generative models like diffusion model which are excellent at image impainting as well."
            },
            "questions": {
                "value": "Please take a look at the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1037/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1037/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1037/Reviewer_vMNL"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1037/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817415285,
        "cdate": 1698817415285,
        "tmdate": 1699636030020,
        "mdate": 1699636030020,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xH0UnTtQNE",
        "forum": "qJ5EFFGuFU",
        "replyto": "qJ5EFFGuFU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1037/Reviewer_N8gT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1037/Reviewer_N8gT"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an implicit representation method to tackle the task of image inpainting. Semantic information across pixels is introduced to help produce better reconstruction results. Specifically, two main modules are constructed, i.e. a semantic implicit representation to obtain text-aligned embeddings with CLIP and an appearance implicit representation that incorporates the semantic embedding. The proposed method achieves superior performance than previous works."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea of incorporating CLIP-based text-assisted semantic information is reasonable to obtain higher-quality reconstructed images. The shown performance promotion over compared methods is also significant.\n- Extensive experimental studies are provided to demonstrate the effectiveness of the proposed method.\n- The code is also provided for reproducing."
            },
            "weaknesses": {
                "value": "- One main concern comes from the application of this stream of methods. In other words, the current evaluation benchmark is made manually and may be too theoretical. I wonder if any real-life application cases can be shown, e.g. recovering objects that are occluded or blurred via dramatic camera motions. If there are more proper real application cases, there is no need to be limited to the ones I list.\n- Another concern lies in the computation cost. It is suggested to compare the inferring and training cost with previous methods, as it seems the two-module framework may be costly."
            },
            "questions": {
                "value": "- What is the main advantage of the coordinate-based implicit representation method over diffusion-model ones for image inpainting? Diffusion models have shown great power in recent generation tasks, also including inpainting. It is suggested to discuss this question and include necessary related works, which will determine the significance of the contribution.\n- Can the proposed method apply to any in-the-wild images, not limited to the used datasets? If yes, it is better to show some samples."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1037/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831069295,
        "cdate": 1698831069295,
        "tmdate": 1699636029917,
        "mdate": 1699636029917,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sSqT96wzx4",
        "forum": "qJ5EFFGuFU",
        "replyto": "qJ5EFFGuFU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1037/Reviewer_N8Xr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1037/Reviewer_N8Xr"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a new implicit representation named SAIR, shorted for Semantic-Aware Implicit Representation. SAIR uses MaskCLIP to extract pixel-level semantic features from CLIP model, and combine the representation with LIIF to learn an implicit representation conditioned on the semantic features. Authors evaluate the semantic aware implicit function by reconstructing the masked regions on CelebAHQ and ADE20K dataset. The proposed methods outperforms prior works like LIIF."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. I like the idea of introducing CLIP feature into LIIF. Although the model is conditioned on the masked image input, the CLIP feature is high-level enough to capture the semantic information in the image. \n2. The inpainting results outperform both LAMA (inpainting-based method) and LIIF (implicit representation method) by a reasonable margin."
            },
            "weaknesses": {
                "value": "1. Some reference format is not correct. For example, CelebAHQ, ADE20K in the introduction. \n2. Some equations are not consistent across the paper. In Eqn(2), SIR takes I, M, p as input, but in Eqn(4)(5), SIR only takes I, p as the input. I would suggest authors to make notations consistent and clear. \n3. AppEncoder is not clearly defined in Section 4.3. I think it is sometimes mixed with SIR. \n4. The result in Table 5 is confusing. Authors trying to study the effect of SIR block, but after removing SIR, the network is just AppEncoder ConvNet. Authors didn't explain clearly how to evaluate ADE20K mIoU with AppEncoder alone.  \n5. Figure 1 is kind of confusing. The green arrow and red arrow point to \"Hair\" and \"Eye\". But I don't think the proposed model will predict the text label of the masked pixel. \n6. One key ablation I would suggest authors add in both Table 1 and Table 2, is that compare SAIR without CLIP and with other networks other than CLIP, e.g. ImageNet pre-trained models."
            },
            "questions": {
                "value": "1. In the dataset section, authors states CelebA and ADE20K have 19 and 150 classes respectively. Are these semantic labels of the dataset used during training and testing?\n2. In Section 5.3, authors state \" we used CLIP_T to filter the image feature CLIP_I\". The term filter is not very clear or straightforward. Are authors trying to imply \"dot product\"?\n3. Is there a loss for semantic feature reconstruction? If not, how could SIR reconstruct the semantic features, as stated in Section 5.3. \n4. Is there any comparison with mask ratio 0? Just compare to the original LIIF on super-resolution tasks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1037/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1037/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1037/Reviewer_N8Xr"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1037/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698987380281,
        "cdate": 1698987380281,
        "tmdate": 1699636029859,
        "mdate": 1699636029859,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "357IRkOLL2",
        "forum": "qJ5EFFGuFU",
        "replyto": "qJ5EFFGuFU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1037/Reviewer_jBR4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1037/Reviewer_jBR4"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a novel implicit representation learning method to tackle the limitations of existing approaches which learn the mapping function heavily relying on the appearance information. The core of the proposed method is the Semantic-Aware Implicit Representation learning procedure, consisting of a Semantic IR module which learns pixel-wise semantic features with aggregated information from neighbors, as well as an Appearance IR which reconstruct the RGB values based on both semantic and appearance information. Experiments are conducted on CelebAHQ and ADE for image inpainting task, demonstrating the effectiveness of the proposed SAIR."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- the motivation and the corresponding solution is easy to follow\n- the experiments validate the contributions of different components of SAIR"
            },
            "weaknesses": {
                "value": "- How to understand the claim that the $f_\\theta$ of SIR learns the **text-aligned** embeddings (Eq (4))? \n    - though the operation similar with MaskCLIP dose not alter the text-aligned feature space, these features are then processed by learnable $\\theta$, there is no guarantee that the embedding space is text-aligned.\n    - why do the authors highlight the **text-aligned** embeddings? If I understand correctly, the embedding space is just an enhanced pixel-wise semantic feature space.\n- Why mapping the original CLIP feature space by $f_\\theta$ performs better than the original CLIP feature space? Furthermore, the details about how to implement '*models without SIR block*' is not clear. \n- there is the lack of experimental details about ablation study, like which dataset is incorporated for ablation?\n    - in section \"*Study on the models with/without SIR block.*\" of 5.3, why not directly use the GT segmentation maps instead of calculating by CLIP features?\n- the figures 1 and 2 are duplicated, they demonstrate the almost same information.\n- is the propose SAIR robust/generalizable for other degraded images, like raining or noised images."
            },
            "questions": {
                "value": "please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1037/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699027596613,
        "cdate": 1699027596613,
        "tmdate": 1699636029790,
        "mdate": 1699636029790,
        "license": "CC BY 4.0",
        "version": 2
    }
]