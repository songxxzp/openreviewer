[
    {
        "id": "gWpaJ9YHF9",
        "forum": "yIKjkRZBrX",
        "replyto": "yIKjkRZBrX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2343/Reviewer_Yx7H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2343/Reviewer_Yx7H"
        ],
        "content": {
            "summary": {
                "value": "This paper studies learning variable-length skills from data to accelerate RL. It assumes that novel state-actions in data are critical dicision points and the low-level skills should terminate at such points. Thus, the paper proposes NBDI, training an ICM to estimate the state-action novelty and labeling the termination for each data sample. Then, the paper adopts the method of SPiRL, training a latent skill representation, an action decoder (with a termination decoder), and a skill prior, providing temporal abstractions for training a high-level policy with RL. Experiments in Maze show that the method outperforms SPiRL."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Extracting skills with variable lengths is beneficial for hierarchical RL. The paper points out a significant issue that existing methods which learn skills from data only use fixed-length skills.\n\n2. Terminating skills at novel state-actions is an interesting attempt to address this problem. Experiments in Maze show that the proposed method outperform the method without variable-length skills (SPiRL); In robotic simulations, it slightly outperforms SPiRL."
            },
            "weaknesses": {
                "value": "1. The assumption that \"novel state-actions are critical decision points\" is **too strong**. I find that datasets used in this paper just meet this assumption: in Maze, the behavior policy only chooses diverse actions at crossroads; in Kitchen, the dataset consists of expert policies accomplishing a fixed set of skills, making the actions diverse at skill success points only. If we use other environments (e.g., robotic tasks without clearly defined skills) or non-expert data collection policies with stochasticity, this assumption will no longer hold, and it is doubtful whether NBDI can outperform SPiRL.\n\n2. In implementations, the ICM is trained and evaluated on a fixed dataset, which **may not reflect the notion of \"novelty\"** discussed in the paper. In the original usage of ICM, it can estimate novelty in online RL, since the model cannot predict unseen transitions. However, this paper uses ICM to estimate novelty for the training data. Assuming that the model can overfit the dataset, the ICM losses on all training samples are small, thus ICM can fail to evaluate the state-action novelty in the dataset. \n\n3. According to the context, the presentation of the Theorem in Section 4.2 seems redundant. The theorem tells that breaking skills into shorter pieces can improve the high-level controller. But the paper studies how to determine the skill termination points, instead of breaking pre-defined fixed-length skills into shorter pieces. Also, the conclusion in the theorem is straight-forward, while the detailed notations (Q and V) in it are no longer used in the paper. I think the the theorem should be moved to the Appendix.\n\n4. The method is incremental to SPiRL. Its improvement on the two robotic simulation tasks is small, according to Figure 6."
            },
            "questions": {
                "value": "1. As I discussed in the Weaknesses (1), when the behavior policies in dataset are non-expert and stochastic, it is questionable whether NBDI can still provide benefits. I think more experimental results are required; otherwise, the authors should make this assumption and limitation clear in the paper.\n\n2. In the experiments, how did you adjust the model capacity and training steps of ICM? If ICM overfits on training data, can it still represent state-action novelty, as I discussed in Weakness (2)?\n\n3. When generating termination labels for the dataset, how to convert the continuous prediction loss of ICM into binary termination labels? Does this require selecting a threshold manually?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2343/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698575748845,
        "cdate": 1698575748845,
        "tmdate": 1699636166700,
        "mdate": 1699636166700,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "z0FmdqAGbH",
        "forum": "yIKjkRZBrX",
        "replyto": "yIKjkRZBrX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2343/Reviewer_Pqwe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2343/Reviewer_Pqwe"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an approach to for learning variable-length skills by detecting decision points based on the state-action novelty. The algorithm is built based on SPiRL in the offline reinforcement learning setting. The proposed approach achieves positive results on a navigation task and two simulated manipulation tasks for down-streaming evaluation of the learned skills."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed approach to find critical decision point during skill discovery makes sense and is an important problem to study in skill/option discovery.\n\n2. The paper is generally well-written and easy to understand. Source code is provided."
            },
            "weaknesses": {
                "value": "My main concern for this paper is with the empirical evaluations.\n1. Lack of option discovery/learning baselines. As the authors also mentioned in the introduction section, option framework and its related algorithms typically learn a termination condition which functions as a tool to determine whether the agent needs to switch to a different skill. Several recent papers [1, 2, 3] propose to do skill/option discovery from offline data while learning a flexible termination condition just like this paper. But none of them are compared in the empirical results or discussed in the related work section. The proposed approach is only compared to SPiRL which uses a fixed skill length.\n\n2. As it's a offline RL test setting, I think standard offline RL algorithms should be compared to instead of SAC. Moreover, as a general skill discovery method, I would suggest the authors test the proposed approach on more domains.\n\n[1]. Learning options via compression. Neurips 2022.\n[2]. Learning robot skills with temporal variational inference. ICML 2020.\n[3]. Opal: Offline primitive discovery for accelerating offline reinforcement learning. ICLR 2021."
            },
            "questions": {
                "value": "1. Figure 4 is a little confusing. Different training objectives lead to different termination improvement occurrences, but is one of the three results better than the other two? It's not quite straight-forward to me.\n2. In section 4.2, the authors claim \"The termination improvement theorem basically implies that we should terminate an option when\nthere are much better alternatives available from the current state.\" So how do the authors decide which option is a better alternative in this paper's scope?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2343/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698707477013,
        "cdate": 1698707477013,
        "tmdate": 1699636166622,
        "mdate": 1699636166622,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "31AYxX47Qp",
        "forum": "yIKjkRZBrX",
        "replyto": "yIKjkRZBrX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2343/Reviewer_zT8h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2343/Reviewer_zT8h"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a unsupervised skill learning algorithm that can extract variable-length skills from a task-agnostic offline dataset. Variable-length skills are motivated by the observation that fixed-length skills often move past critical decision points, like crossroads, resulting in suboptimal performance on downstream tasks. The authors propose to detect such decision points using state-action novelty. The resulting algorithm, Novelty-based Decision Point Identification (NBDI), learns skills together with their termination probability and a prior over the skill space using a latent variable model similar to SpiRL [1]. The authors furthermore provide a complementary perspective on variable termination of skills by relating it to the potential performance gains achieved by terminating and switching to higher-value options, arguing that state-action novelty serves as a proxy for situations where this is likely to be beneficial. Experiments in two maze environments and two robotic manipulation tasks demonstrate that the variable-length skills learned by NBDI lead to better performance in downstream tasks.\n\n[1] Karl Pertsch, Youngwoon Lee, and Joseph Lim. Accelerating reinforcement learning with learned skill priors. In Conference on robot learning, pp. 188\u2013204. PMLR, 2021."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The need for variable-length skills is well motivated by practical examples and a more theoretical argument based on the potential benefits of early option termination.\n\nThe paper is furthermore well written and structured and easy to follow. The figures generally do a good job in conveying the intuition behind the algorithm as well as in illustrating the three phases of the algorithm.\n\nThe variations of NBDI with different kinds of novelty signals for the training of the termination probability are a good addition to the experiments as they demonstrate that considering state-action novelty is crucial.\n\nThe authors furthermore already made the code public."
            },
            "weaknesses": {
                "value": "The algorithm is motivated with the concept of novelty, explicitly using a count-based notion of novelty in equation (1). In the experiments section the paper then briefly mentions that Intrinsic Curiosity Module (ICM) is used to obtain state-action novelty values. However, ICM obtains its intrinsic motivation signal from the prediction error of the next state in a learned feature space which is conceptually somewhat different. I think it would be a good idea to discuss to which extent the exact nature of the curiosity signal influences the learned skills.\n\nAs evident in the motivating examples, the learned skill termination probabilities depend on the data, in particular on where different actions have been chosen frequently. This poses the question of how dependent NBDI is on a suitable structure being present in the offline dataset. For example, could NBDI work with exploratory data, and if yes, under which circumstances? How does the complexity of the environment influence the requirements on the data? A discussion of these questions would be a good addition to the paper.\n\nOverall the algorithm is fairly close to SpiRL but this is made transparent so it is not really a problem.\n\nThe additional ablations in Appendix A are quite interesting, in particular, the study of the novelty threshold. It would therefore be good to mention them more explicitly in the main text."
            },
            "questions": {
                "value": "* In the paragraph \u201cOption Framework\u201d, in the definition $\\beta: \\mathcal{S}^+ \\rightarrow [0, 1]$ the symbol $\\mathcal{S}^+$ is not defined anywhere. What does it stand for?\n* In equation (5) $\\tilde{r}$ is added to the objective in each time step even though $\\tilde{r}$ itself is already the cumulative discounted reward for the execution of one skill. Could you maybe explain how to reconcile these definitions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2343/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2343/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2343/Reviewer_zT8h"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2343/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769675653,
        "cdate": 1698769675653,
        "tmdate": 1699636166555,
        "mdate": 1699636166555,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NFAg9RkOhu",
        "forum": "yIKjkRZBrX",
        "replyto": "yIKjkRZBrX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2343/Reviewer_6rC9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2343/Reviewer_6rC9"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an approach to learn variable length skills in RL, closely building on SpiRL [1]. This is done using state-action novelty to predict a termination condition for the skills. \n\n\n\n\n\n\n\n\n[1] - Pertsch, Karl, Youngwoon Lee, and Joseph Lim. \"Accelerating reinforcement learning with learned skill priors.\" Conference on robot learning. PMLR, 2021"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "RL with skills can enable much more efficient learning, and is an important problem. Learning variable length skills should allow for more effective learning. However, I have some concerns regarding this paper - please see weaknesses."
            },
            "weaknesses": {
                "value": "1. Clarity of proposed approach\n\nThe method of variable length skills relies heavily on using state-action novelty to prediction termination conditions. However the exact details for how the state-action novelty module is trained is not explained in the paper (box (i) in the method figure). The paper includes some description of characterizing novelty as inverse visitation count, so do the authors explicitly maintain counts of each seen state? (This will be difficult to scale to environments with continuous states and actions). Or is there some other metric of pseudo counts, involving density estimation? What is the relative effect of using different means to estimate the novelty metric, to predict termination? The reason for this particular analysis is that the novelty estimation is the main contribution of this paper. \n\n2. Significance of contribution \n\nThe proposed method heavily builds on prior work [1], for the components that learn skills and perform RL in the skill space. From the experimental results, the quantitive gains over [1] seem very marginal in the more complex environments (sparse block stacking, kitchen). Can the authors include some analysis of how their discovered skills differ qualitatively in these more complex envs, as compared to [1]?. What are the specific cases in these environments that variable length skills are actually helpful/needed? \n\n\n[1] - Pertsch, Karl, Youngwoon Lee, and Joseph Lim. \"Accelerating reinforcement learning with learned skill priors.\" Conference on robot learning. PMLR, 2021"
            },
            "questions": {
                "value": "Please address questions in the weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2343/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2343/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2343/Reviewer_6rC9"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2343/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699970280204,
        "cdate": 1699970280204,
        "tmdate": 1699970280204,
        "mdate": 1699970280204,
        "license": "CC BY 4.0",
        "version": 2
    }
]