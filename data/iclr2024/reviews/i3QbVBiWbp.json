[
    {
        "id": "lwthci0i9h",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1292/Reviewer_bkkh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1292/Reviewer_bkkh"
        ],
        "forum": "i3QbVBiWbp",
        "replyto": "i3QbVBiWbp",
        "content": {
            "summary": {
                "value": "The paper proposes a transformer-based autoencoder architecture for quantum state tomography (QST) with imperfect measurement data. A transformer-based encoder is pre-trained to extract informative latent representation (ILR) with the task of measurement frequency reconstruction, which is succeeded by a transformer-based decoder to estimate quantum states from the measurement operators and frequencies. Extensive simulations and experiments demonstrate the remarkable ability of the proposed model to deal with imperfect measurement data in QST."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper introduces a novel and interesting idea of building Transformer-based neural network for quantum state tomography.\n\n- The paper clearly introduces the QST preliminaries, well presents the ill-posed challenge for QST and insightfully discusses the related works.\n\n- The experiment result shows that transformer auto-encoder can reconstruct quantum states far better than the baseline models from imperfect measurement data."
            },
            "weaknesses": {
                "value": "- The idea of using transformer self-attention layers for QST is not strongly motivated, and hence not theoretically sound to me. \n\n- The model does scales poorly with the number of qubits due to the exponential number of operators in a complete set of QST measurement, so the contribution is limited.\n\n- The latent representation contains a mixture of encoded features and raw input features. This seems not reasonable in principle for transformer-based models, especially when the raw features and encoded features are quite different across different samples.\n\n- The experiment is a bit slim and cannot well show the value of the proposed model. \n  - It appears that the baseline models are linear regression models without pre-training, so it is an unfair comparison because the proposed model is exposed to far more data than the baseline models due to the presence of the encoder.  Are there stronger NN baselines? Is it possible to train the non-pre-training baselines with both pre-training data and training-data for this work?\n  - The ablation study is missing, whereas it is necessary for this model to justify the design of different model components, such as i) having the missed operators and ii) training a frequency decoder instead of directing the state decoder in pre-training."
            },
            "questions": {
                "value": "Please kindly see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1292/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1292/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1292/Reviewer_bkkh"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1292/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697284540486,
        "cdate": 1697284540486,
        "tmdate": 1700587506391,
        "mdate": 1700587506391,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TRlfnBySmV",
        "forum": "i3QbVBiWbp",
        "replyto": "i3QbVBiWbp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1292/Reviewer_wK9G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1292/Reviewer_wK9G"
        ],
        "content": {
            "summary": {
                "value": "This manuscript introduces a transformer-based architecture designed to address the challenge of quantum state tomography with imperfect measurement data. The authors present the encoder-decoder framework of their model and illustrate a pre-training technique for the encoder, enabling it to reconstruct high-quality frequencies from imperfectly measured data. Furthermore, the authors show the model's effectiveness by employing it in the reconstruction of arbitrary 2-qubit and 4-qubit quantum states, as well as in the prediction of their properties."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This manuscript presents a versatile model capable of simultaneously performing quantum tomography and predicting quantum properties.\n- The paper introduces a pre-training strategy aimed at enhancing the robustness of the proposed model.\n- This paper applies the proposed model to quantum state tomography of arbitrary quantum states rather than focusing on specific states or predefined quantum state sets."
            },
            "weaknesses": {
                "value": "- I have doubts about the scalability of the proposed model for large-scale quantum systems, especially considering the exponential growth in the number of cube operators required. This implies that the dimension of the input layer for this model would increase exponentially . If this holds true, the resulting model would become exceedingly large when applied to large-scale quantum systems.\n\n  For another, the experiments about QST in this paper are limited to 2-qubit and 4-qubit quantum states. Even for 4-qubit pure states, when $N_t = 100$ and no operators are masked, the reconstruction fidelity is approximately $1-e^{-2} \\approx 0.865$, which is not high. If this limitation is attributed to the relatively small value of $N_t$, the authors may consider conducting additional experiments on 4-qubit states (or even larger quantum system) to address this concern. I was unable to locate such experiments in the appendix, which predominantly shows a series of additional experiments conducted on 2-qubit states. \n\n- I believe the proposed model lacks novelty in some sense. While it incorporates a transformer architecture, the fundamental encoder-decoder framework closely resembles those found in existing references [1] and [2] for quantum state tomography and quantum state learning. Furthermore, I feel that the pre-training strategy introduced here is similar to the setting in [2], which involves predicting measurement results for unmeasured bases.  I would appreciate it if the authors could clarify the distinction between \"masked\" operators in this paper and the unmeasured bases described in [2].\n\n  [1] Ahmed, Shahnawaz, et al. \"Quantum state tomography with conditional generative adversarial networks.\" *Physical Review Letters* 127.14 (2021): 140502.\n\n  [2] Zhu, Yan, et al. \"Flexible learning of quantum states with generative query neural networks.\" *Nature Communications* 13.1 (2022): 6222."
            },
            "questions": {
                "value": "Major concerns:\n\n- I have stated two major concerns in the \"Weakness\" section above, with one relating to scalability and the other relating to novelty.\n\nMinor questions:\n\n- In the part of predicting quantum properties, the authors utilize locally rotated GHZ states and W states. Could the authors provide information on the range of values associated with the properties to be predicted for these two types of states?\n\n- I have some doubts about the motivation of using the model for property prediction. Why not compute the properties directly from the predicted density matrix, especially considering that the state decoder is designed to generate the density matrix as the output?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1292/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1292/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1292/Reviewer_wK9G"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1292/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698566687502,
        "cdate": 1698566687502,
        "tmdate": 1699636056200,
        "mdate": 1699636056200,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yMkIeQpFbM",
        "forum": "i3QbVBiWbp",
        "replyto": "i3QbVBiWbp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1292/Reviewer_aafB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1292/Reviewer_aafB"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors a transformer-based autoencoder architecture tailored for quantum state tomography with imperfect measurement data. However, the introduction of quantum mechanics is not explicit. In addition, some important points should be emphasized."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "One significant advantage of this method is its capability to provide more comprehensive information when dealing with imperfect measurement data. By using a transformer-based encoder, it effectively extracts latent information from imperfect measurement data, improving the accuracy of quantum state estimation."
            },
            "weaknesses": {
                "value": "Please review the comments below."
            },
            "questions": {
                "value": "1. On Page 1, the authors have mentioned that, \"To uniquely identify a quantum state, the measurements must be informatively complete to provide\nall the information about \u03c1 (Je\u02c7 zek et al., 2003). The exponential scaling of parameters in $\\rho$ requires\nan exponentially increasing number of measurements, each of which requires a sufficient number\nof identical copies (Gebhart et al., 2023).\" However, this statement is not entirely precise. When the density matrix is low-rank [1] or takes the form of a matrix product operator [2], the POVM may not be informatively complete. Consequently, when a low-dimensional structure exists within the density matrix, many traditional methods can be applied with significantly fewer repeated measurements, which is an important direction to explore compared to neural network-based approaches. The reviewer suggests that this structural aspect should be included in the introduction.\n\n[1] J. Haah, A. Harrow, Z. Ji, X. Wu, and N. Yu, \u201cSample-optimal tomography of quantum states,\u201d IEEE Transactions on Information\nTheory, vol. 63, no. 9, pp. 5628\u20135641, 2017.\n\n[2] Zhen Qin, Casey Jameson, Zhexuan Gong, Michael B Wakin, and Zhihui Zhu.  \u201cStable tomography for structured quantum states,\u201d arXiv preprint arXiv:2306.09432, 2023.\n\n2. In Section 3.1, PRELIMINARIES ABOUT QST, it is advisable to use the notation $2^n$ instead of just $d$. This change is necessary to establish the proper context for the definition of a qubit as introduced in Section 3.2, THE ILL-POSED QST PROBLEM. Additionally, it would be beneficial to introduce the concepts of Hermitian, positive semidefinite (PSD) structure, and unit trace in the density matrix earlier in the section for improved clarity.\n\n3. In Figure 2, due to the missing definition of qubit, for readers without any quantum background, it is hard to compute the total number of density matrices. Consequently, the number of missed measurements will be meaningless.\n\n4. In part \"QST process using a transformer-based autoencoder\", should the architecture need to be designed anew for different qubits and POVMs, the authors should underscore this requirement.\n\n5. The reviewers suggests that the authors should add the convergence rate of infidelity for different algorithms.\n\n6. In the section 4.2 RECONSTRUCTING DENSITY MATRICES, the use of 2-qubit and 4-qubit examples may be considered limited. It would be beneficial to include discussions involving at least 8-qubit systems for a more comprehensive analysis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1292/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1292/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1292/Reviewer_aafB"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1292/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698690173307,
        "cdate": 1698690173307,
        "tmdate": 1699636056117,
        "mdate": 1699636056117,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4L0VKDjQc6",
        "forum": "i3QbVBiWbp",
        "replyto": "i3QbVBiWbp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1292/Reviewer_aFck"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1292/Reviewer_aFck"
        ],
        "content": {
            "summary": {
                "value": "The submission extends the concept of the masked autoencoder to enhance the sample complexity of quantum state tomography. The authors have conducted numerical simulations involving systems of up to 12 qubits to assess the performance of their proposal. Nonetheless, several statements throughout the paper and the configurations used in the numerical simulations introduce confusion, making it challenging to discern the precise contributions of the submission."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The utilization of deep learning techniques to improve quantum state tomography (QST) represents an emerging and promising field. Nevertheless, the current body of work focused on designing specialized learning models for quantum state tomography remains relatively limited. The submission effectively addresses this gap and presents intriguing results."
            },
            "weaknesses": {
                "value": "The primary weakness of the submission stems from inaccuracies in statements and the presence of confusing settings. The presence of incorrect or imprecise statements obscures the novelty and technical contributions of the proposed method. Additionally, while the authors have conducted a series of numerical simulations, the absence of a comparative analysis with state-of-the-art methods hinders our ability to gauge the practical advancements offered by the proposed method."
            },
            "questions": {
                "value": "1)  The motivation behind designing the auto-decoder structure is not entirely clear. It remains uncertain whether the authors aim to directly adapt the concept of Masked autoencoders to tackle QST tasks or if deeper insights are guiding this choice. Providing more context on this decision would enhance the submission's coherence.\n\n2) The use of a state decoder to predict state properties appears to introduce confusion. If a user's primary interest lies in estimating specific properties, more efficient methods may be available than the proposed approach. It is essential to consider that state reconstruction, even with the inclusion of masked operations, can be resource-intensive and time-consuming.  \n\n3) The numerical simulations are limited to older methods for QST. Consequently, it remains uncertain whether the purported contributions and advantages can be effectively realized in practical applications. To establish the practicality and competitiveness of the proposed approach, a systematic examination involving a wider spectrum of advanced deep learning methods is imperative. For instance, recent studies [Ahmed, Shahnawaz, et al. \"Quantum state tomography with conditional generative adversarial networks.\" Physical Review Letters 127.14 (2021): 140502.]  have explored the use of incomplete POVM information in conjunction with a generative adversarial learning scheme to address QST tasks, and a thorough comparative analysis with such contemporary approaches would greatly enhance the submission's value and relevance. \n\n4) In Table 3, the authors benchmark the proposed method for estimating coherence and entanglement of GHZ and W states with 8/12 qubits. Given that this task has also been investigated in a study by Zhu et al. in 2022, a comparative study becomes imperative. The relevant results would provide valuable insights into the relative strengths and weaknesses of these two methods for the specified task."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1292/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698736138703,
        "cdate": 1698736138703,
        "tmdate": 1699636056043,
        "mdate": 1699636056043,
        "license": "CC BY 4.0",
        "version": 2
    }
]