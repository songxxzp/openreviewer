[
    {
        "id": "3KQjJfboAo",
        "forum": "vrBVFXwAmi",
        "replyto": "vrBVFXwAmi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5408/Reviewer_TiJ4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5408/Reviewer_TiJ4"
        ],
        "content": {
            "summary": {
                "value": "This manuscript introduces a framework named QTAPE designed to address the challenge of quantum property estimation. The authors utilize an neural network-based model to leverage different types of quantum data to make the estimation.  Additionally, they showcase the model's effectiveness by deploying it to estimate properties in various many-body systems."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This manuscript employs a framework composed of unsupervised pre-training and supervised fine-tuning phase for estimating the properties.\n\n- The proposed method demonstrates versatility, as evidenced by its effectiveness across various types of tasks, as illustrated by the numerical experiments. \n\n- The proposed model exhibits satisfactory performance even in large-scale quantum systems.\n\n- The presentation of the paper is excellent."
            },
            "weaknesses": {
                "value": "- While the authors emphasize the unsupervised pre-training as a key novelty of their proposed method, I just feel that the approach they employ in the pre-training phase, involving the regeneration of measurement results, follows a similar logic to that found in [1]. In [1], the model generates measurement results from incomplete measurement results. Although the proposed model here incorporates additional input information, such as physical conditions, I still believe the authors should cite this previous work. \n- Furthermore, the fine-tuning phase of the model shares similarities to the framework described in [2]. Despite the results demonstrating the superiority of the proposed method in Table 1, it would be valuable for the authors to engage in a more in-depth discussion regarding the distinctions between their model and this prior work. Given that Q-TAPE without pretraining also outperforms NN-Classical Shadow, it would be appreciated if the authors could provide further analysis or intuitive explanations for the results to highlight the novelty of the proposed model.\n\n[1] Zhu, Y., Wu, Y. D., Bai, G., Wang, D. S., Wang, Y., & Chiribella, G. (2022). Flexible learning of quantum states with generative query neural networks. Nature Communications, 13(1), 6222.\n\n[2] Wang, H., Weber, M., Izaac, J., & Lin, C. Y. Y. (2022). Predicting Properties of Quantum Systems with Conditional Generative Models. arXiv preprint arXiv:2211.16943."
            },
            "questions": {
                "value": "- See \"Weakness\" section above.\n- In the section concerning the prediction of correlation functions for the anisotropic Heisenberg model, it would be beneficial if the authors could include an analysis of the prediction MSE for correlation functions associated with two quantum sites at **different** distances."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5408/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5408/Reviewer_TiJ4",
                    "ICLR.cc/2024/Conference/Submission5408/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5408/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698596741604,
        "cdate": 1698596741604,
        "tmdate": 1700192396850,
        "mdate": 1700192396850,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BJZyt188Jd",
        "forum": "vrBVFXwAmi",
        "replyto": "vrBVFXwAmi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5408/Reviewer_2sNf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5408/Reviewer_2sNf"
        ],
        "content": {
            "summary": {
                "value": "There is a lot of hope for using machine learning to advance quantum physics. Many supervised machine learning algorithms have been studied and proposed for important foundational problems in quantum physics, such as classifying quantum phases of matter and predicting properties of quantum systems. Inspired by the power of unsupervised models (such as LLMs and pre-trained models in CV), the authors present a pre-trained model Q-TAPE for quantum physics. The authors showed that the pre-trained model enables strong performance with limited training data, which can be very useful as large-scale experimental data are hard to obtain."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Machine learning for quantum many-body physics is an important research topic that has been subject to extensive studies in recent years. As of now, a good pre-trained model for quantum physics problems is unavailable. This work could initiate a fruitful line of research to build a powerful pre-trained model for quantum many-body physics.\n\nThe extensive experiments conducted in this work showcase the potential for using a good pre-trained model to significantly reduce the amount of experimental data needed. For example, in the largest system size, L = 31, and smallest experimental data of Fig. 3, the performance improved by >15% using the pre-trained model."
            },
            "weaknesses": {
                "value": "The authors only considered two downstream tasks (classifying phases and predicting properties).\nMore tasks would make the work even stronger.\n\nThe work is purely empirical (which I find to be a minor issue for this direction of research)."
            },
            "questions": {
                "value": "I can understand how language has many universal features, which makes pre-training very useful. However, I don't yet see what kind of features in quantum physics problems are \"universal.\" Could the authors provide some visualizations of the pre-trained model to showcase what kind of universal features are learned by Q-TAPE? Could the authors also provide more discussions (and possibly an example) to illustrate why pre-training is useful in quantum physics? I believe the authors' argument that pre-training can be very useful in quantum physics, but I am not entirely sure how.\n\nAs noted in the Limitations paragraph, the work would be stronger if the authors could provide experiments for a few more downstream tasks. I think predicting entanglement entropy should not be too hard, and I would like to see if pre-training helps there.\n\nIt would also be interesting to see for what downstream tasks the pre-trained model Q-TAPE is not particularly helpful now. From the perspective of building along this line of thinking, I think illustrating the shortcomings of the current model is actually very useful.\n\nThere are some typos in the Limitations paragraph in Section 5: \"... for expereiments.Though\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5408/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698626835610,
        "cdate": 1698626835610,
        "tmdate": 1699636548324,
        "mdate": 1699636548324,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iPFAKL9G8c",
        "forum": "vrBVFXwAmi",
        "replyto": "vrBVFXwAmi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5408/Reviewer_qDBp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5408/Reviewer_qDBp"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors leverage a pre-trained LLM strategy to construct the Q-TAPE architecture to deal with quantum data, which involves an unsupervised pre-trained model and fine-tuning for specific tasks. Numerous experiments have been conducted to show the promising efficacy of the Q-TAPE."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The work leverages the pre-training strategy that has attained triumph in large language models. \n\n2. This work aims at building quantum datasets that can be conceptually equivalent to the corpus used to train LLMs."
            },
            "weaknesses": {
                "value": "1. The idea of generating and collecting quantum data is important, but quantum data is more fitted to quantum machine learning models like quantum neural networks and quantum graph neural networks, instead of the classical ones. \n\n2. Even though the work somehow makes the generated quantum data suitable for the classical LLM architecture, the new hybrid quantum-classical architecture is not interesting. For classical-to-quantum data conversion, a simple quantum tensor encoding, amplitude encoding, or a more advanced method of the quantum kernel can efficiently deal with quantum data generation. If you could collect quantum datasets, the use of quantum machine learning models is more important and that is the most advantage of quantum neural networks. \n\n3, Similar techniques of pre-training strategy have been shown in many quantum machine learning works. For example, Yang's quantum NLP work has demonstrated the effectiveness of pre-training hybrid LLM for dealing with quantum data. Moreover, Qi's recent work has provided a solid theoretical understanding of pre-training LLM for quantum machine learning. Unfortunately, those important and very related works are not cited in this work. \n\nRef. [1] Yang, C.H.H., Qi, J., Chen, S.Y.C., Tsao, Y. and Chen, P.Y., 2022, May. When Bert meets quantum temporal convolution learning for text classification in heterogeneous computing. In IEEE International Conference on Acoustics, Speech and Signal Processing (pp. 8602-8606). IEEE.\n\nRef. [2] Jun Qi, Chao-Han Huck Yang, Pin-Yu Chen, Min-Hsiu Hsieh, \"Pre-Training Tensor-Train Networks Facilitate Machine Learning with Variational Quantum Circuits,\" arXiv:2306.03741v1. \n\n4. In the experiments, the authors do not discuss the in-distribution and out-of-distribution quantum data. For the out-of-distribution quantum data, the proposed pre-training LLM architecture cannot ensure a good performance. So, some new quantum-aware optimization algorithms need to be more deeply investigated."
            },
            "questions": {
                "value": "(1) Why not directly use quantum machine learning like quantum neural networks to deal with the quantum datasets? since the hybrid quantum-classical architecture can attain competitive empirical results. \n\n(2) Is there some out-of-distribution quantum data being existed in the datasets? If so, how to deal with the case."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5408/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5408/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5408/Reviewer_qDBp"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5408/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698632820639,
        "cdate": 1698632820639,
        "tmdate": 1700170617298,
        "mdate": 1700170617298,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EJlYYNVdNs",
        "forum": "vrBVFXwAmi",
        "replyto": "vrBVFXwAmi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5408/Reviewer_1o9o"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5408/Reviewer_1o9o"
        ],
        "content": {
            "summary": {
                "value": "The submission introduces Q-TAPE, a versatile pre-trained model for quantum systems, aiming to enhance property estimation. Drawing inspiration from Large Language Models' success in other domains, Q-TAPE offers several key advantages: it leverages a rich set of quantum data for comprehensive training, adopts an unsupervised and task-agnostic approach, excelling in classifying quantum phases and predicting correlation functions, especially with limited data, and seamlessly transfers knowledge from pre-training to specific properties estimation tasks. Extensive experiments validate Q-TAPE's efficacy across various quantum tasks. Furthermore, the authors commit to making the source code openly available, fostering further research and applications in quantum property estimation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Q-TAPE presents a novel and promising approach to understanding quantum systems. Diverging from previous methods that primarily rely on supervised learning, Q-TAPE advocates an unsupervised pretraining combined with downstream task finetuning, a methodology akin to the training of large language models. This paradigm shift offers a more experimentally-friendly approach, emphasizing adaptability and robustness. The impressive results achieved, especially with quantum systems involving up to 31 qubits, underscore the significant potential of Q-TAPE, solidifying its strengths in advancing quantum property estimation."
            },
            "weaknesses": {
                "value": "Several minor concerns about the submission are listed below. \n\nFirstly, it is essential to empirically investigate the scaling behavior of the required number of measurements for each training example concerning the qubit count. A polynomial scaling that ensures satisfactory performance is desirable, as an exponentially scaling behavior may impede the practical utility of Q-TAPE for larger quantum systems.\n\nSecondly, there is a need for a discussion on the storage method employed for the training dataset and the corresponding memory cost. For larger qubit counts, as the number of measurements increases, the memory cost may become prohibitively expensive even though only the measurement results and the measured operators are stored. It would be beneficial to know if the authors have developed advanced methods to address this challenge, as the bottleneck of employing deep learning for quantum problems could shift to data management.\n\nLastly, some missing references that are pertinent to the content of the submission should be discussed, ensuring a comprehensive and well-referenced presentation of the work. Concrete examples include Refs [1] and [2], which separately address the ability of deep neural networks to simultaneously accomplish multiple tasks and the fundamental role of datasets in quantum system learning.\n\n[1] Wu, Ya-Dong, et al. \"Learning and Discovering Quantum Properties with Multi-Task Neural Networks.\" arXiv preprint arXiv:2310.11807 (2023).\n[2] Du, Yuxuan, et al. \"ShadowNet for Data-Centric Quantum System Learning.\" arXiv preprint arXiv:2308.11290 (2023)."
            },
            "questions": {
                "value": "The questions are listed in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5408/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5408/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5408/Reviewer_1o9o"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5408/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698736823520,
        "cdate": 1698736823520,
        "tmdate": 1700636011649,
        "mdate": 1700636011649,
        "license": "CC BY 4.0",
        "version": 2
    }
]