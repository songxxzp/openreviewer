[
    {
        "id": "N8UfEj89Hk",
        "forum": "IjMUGuUmBI",
        "replyto": "IjMUGuUmBI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8971/Reviewer_2W2r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8971/Reviewer_2W2r"
        ],
        "content": {
            "summary": {
                "value": "The described work presents GraphChef, a method for achieving explainability in Graph Neural Networks (GNNs). It distills GNNs into decision trees with categorical hidden states using Gumbel softmax, essentially creating interpretable decision trees from the trained GNN. The decision trees are then pruned for interpretability based on accuracy. Evaluations on benchmark datasets show that while GraphChef maintains accuracy on some tasks, it may sacrifice performance on others in favor of obtaining an interpretable decision tree."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The article presents an innovative approach to achieving interpretability in Graph Neural Networks (GNNs) by replacing original network modules with interpretable ones, demonstrating that this approach maintains comparable learning performance. This is a novel solution to a significant problem in GNN research.\n\n2. The paper differentiates itself from previous work by focusing on the extraction of global decision rules for entire datasets rather than providing localized explanations for individual outputs. This unique perspective on GNN interpretability is practical and valuable.\n\n3. The article is well-structured, easy to understand, and supported by comprehensive empirical evaluations. It also offers a user-friendly web interface for visualizing decision trees, making it accessible for downstream usage. This approach is both novel and promising for improving the interpretability of GNNs."
            },
            "weaknesses": {
                "value": "1. The method's applicability is restricted to graphs with very few features, making it unsuitable for more complex datasets. The authors acknowledge this limitation, but addressing it could involve techniques like dimension reduction or prototype learning when discretizing the hidden states to handle more feature-rich graphs.\n\n2. The proposed method's use of one-hot hidden states may reduce the model's expressiveness, especially for complex problems. For these, the resulting decision trees could become overly complex and challenging to interpret.\n\n3. The method's performance in terms of explanation is described as only \"comparable\" to post-hoc methods, and it may not offer significant advantages over using standard GNN models followed by post-hoc interpretability techniques."
            },
            "questions": {
                "value": "How does the model perform on large datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8971/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8971/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8971/Reviewer_2W2r"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8971/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697839685737,
        "cdate": 1697839685737,
        "tmdate": 1700638260039,
        "mdate": 1700638260039,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QajRDej9sU",
        "forum": "IjMUGuUmBI",
        "replyto": "IjMUGuUmBI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8971/Reviewer_8CuG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8971/Reviewer_8CuG"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces GraphChef, a Graph Neural Network (GNN) model that integrates decision trees to provide human-comprehensible explanations for each class in a dataset. The authors note that while GNNs are popular for graph-based domains, they are often black-box models that lack interpretability. GraphChef aims to address this issue by generating decision trees that show how different features contribute to each class. The authors demonstrate the effectiveness of GraphChef on the PROTEINS dataset and other explanation benchmarks that require graph reasoning. They also highlight the importance of small trees to ensure that the generated recipes are understandable to humans. Overall, GraphChef provides a promising approach for generating interpretable explanations for GNNs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality:\n- The integration of decision trees with GNNs to provide human-comprehensible explanations is a novel approach that has not been explored extensively in the literature.\n- The authors also introduce a new benchmark dataset for graph-based explanation methods, which can be used to evaluate the effectiveness of different models.\n\nQuality:\n- The authors provide a thorough evaluation of GraphChef on multiple datasets and benchmarks, demonstrating its effectiveness in generating interpretable explanations for GNNs.\n- The paper includes detailed discussions of the limitations and future work of GraphChef, which can guide future research in this area.\n\nClarity:\n- The paper is well-organized and clearly written, making it easy for readers to follow the authors' arguments and understand the technical details of GraphChef.\n- The authors provide several examples and visualizations to illustrate the effectiveness of GraphChef in generating human-comprehensible explanations.\n\nSignificance:\n- The lack of interpretability of GNNs is a significant challenge in many graph-based domains, and GraphChef provides a promising approach for addressing this issue.\n- The authors highlight the potential applications of GraphChef in safety-critical domains such as medicine, where it is important to understand how decisions are made.\n- The paper also contributes to the broader goal of developing explainable AI methods, which can increase trust and transparency in machine learning models."
            },
            "weaknesses": {
                "value": "The authors note that GraphChef can struggle to create recipes for datasets with a large feature space, which limits its applicability in some domains. Future work could investigate alternative methods for combining multiple features into one split to address this limitation."
            },
            "questions": {
                "value": "I have no question."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8971/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698704169255,
        "cdate": 1698704169255,
        "tmdate": 1699637129896,
        "mdate": 1699637129896,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1U3PRh6gEz",
        "forum": "IjMUGuUmBI",
        "replyto": "IjMUGuUmBI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8971/Reviewer_VFKG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8971/Reviewer_VFKG"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an interesting idea of learning decision trees to get explainable models for graph learning tasks. Tree models have a long history and are well known for their explainability. Direct training of tree models on a graph is not straightforward. This work proposes a method of casting a decision tree from a trained neural model. In the empirical evaluation, the tree model shows slight accuracy drop but provides much better explanations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea of training decision trees from graph data is novel. It distills neural models to decision trees. It has several designs (e.g. via the dish network) that make the distilling possible. \n\nThe results of the paper show that the proposed method can provide better explanations while has slight performance drop."
            },
            "weaknesses": {
                "value": "In the empirical evaluation the work shows good performance. However, there should be some theoretical bound over the ability of the tree model. For example, it cannot exceed the expressiveness of the 1-WL algorithm. At the same time, it is unknown whether it can even match the ability of the 1-WL algorithm. I think a theoretical analysis is missing from the work."
            },
            "questions": {
                "value": "A few key steps are missing from the paper. Here are two questions. \n\nFirst, how does the model pool the information to get a graph representation? \n\nSecond, the dish model uses \"Gumbel-Softmax\" to GNN layers. As far as I know \"Gumbel-Softmax\" is designed for sampling continuous variables that approximate the categorical distribution. I don't understand why randomness is needed here. \n\nThird, \"We bootstrap the internal state h^0_v with an encoder layer on the initial node features x_v\" -- what does \"bootstrap\" mean?\n\nFourth, what does \"encoder\" and \"decoder\" mean? Do you use an autoencoder to recover the input or not?\n\nFifth, how many layers do you use in your model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8971/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799659459,
        "cdate": 1698799659459,
        "tmdate": 1699637129762,
        "mdate": 1699637129762,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "j5u3jKqnKO",
        "forum": "IjMUGuUmBI",
        "replyto": "IjMUGuUmBI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8971/Reviewer_2TRZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8971/Reviewer_2TRZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an explainable GNN model by combining decision trees into message passing framework. The idea is interesting and novel. The paper is well written and easy to understand. The main advantage of the proposed method is its ability to explain the whole dataset as compared to the existing methods that focus on explaining individual graphs in the data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The main strength is its ability to explain the whole dataset as compared to the existing methods that focus on explaining individual graphs in the data."
            },
            "weaknesses": {
                "value": "n/a"
            },
            "questions": {
                "value": "1. Please clarify the usage of Gumbel-Softmax to the latent embeddings in detail. I had a hard time understanding this.\n\n2. Why is performance of GIN and GraphChef is similar as listed in Table 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8971/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699123226136,
        "cdate": 1699123226136,
        "tmdate": 1699637129628,
        "mdate": 1699637129628,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZMh9O1PKV1",
        "forum": "IjMUGuUmBI",
        "replyto": "IjMUGuUmBI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8971/Reviewer_U2So"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8971/Reviewer_U2So"
        ],
        "content": {
            "summary": {
                "value": "This paper integrates decision trees into the message-passing framework of GNNs and proposes a self-explanatory GNN model called GraphChef. Inspired by the Stone-age model, GraphChef utilizes the Gumbel-Softmax function to induce categorical latent states for GNN layers, then uses the categorical inputs and outputs and distills them into decision trees. Since the decision-making process of decision trees is comprehensible to humans, GraphChef is able to provide a series of recipes to help us understand the behavior of GNNs. The authors have designed a series of pruning strategies for training decision trees to prevent overfitting, and experiments show that while providing interpretability, GraphChef also ensures expressive power on par with GIN on certain datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Unlike previous approaches that identify substructures or sub-features of the original input most relevant to the output, GraphChef offers a unique perspective for interpreting GNNs. The \"recipes\" it returns reflect higher-level behaviors of the model. Concurrently, GraphChef still provides a method to calculate heatmap-style importance scores for individual graphs.\n- The authors have designed a diverse and comprehensive set of network architectures for GraphChef, ensuring its suitability for both graph classification and node classification tasks.\n- The authors have developed pruning strategies for GraphChef that enhance readability and prevent overfitting, while only sacrificing a minimal amount of performance.\n- The recipes extracted by GraphChef on some small datasets are in alignment with human intuition."
            },
            "weaknesses": {
                "value": "- As the authors mentioned, a limited number of categorical states may restrict its expressive and interpretive power on large datasets, since the feature space of the inputs in large datasets is often vast. It is conceivable that the decision trees generated by GraphChef would also be immense, and an overly large decision-making process can be difficult for humans to directly comprehend.\n- For self-explanatory models, we usually need to understand the trade-off between interpretability and expressive power. However, I did not see such an analysis in this paper, which means we are unable to comprehend the model's balance between expressiveness and interpretability.\n- Pairwise Comparisons require O(n^2) space complexity, which is impractical for large-scale datasets (if a large state size is needed).\n- Although GraphChef can provide a series of decision rules to help us understand the behavior of GNNs at a higher level, the authors' experiments are all based on small-scale datasets (most of which are synthetic), and for complex real-world datasets, these decision rules may still be complex and difficult for humans to understand, especially when the decision trees are deep.\n- While the introduction of decision trees into the interpretation of GNNs is novel, concepts such as the Stone-age model, transforming categorical states to decision trees, and decision tree pruning are not new."
            },
            "questions": {
                "value": "- Regarding the training process of the decision trees, how are the three different types of branches (as shown in Figure 3) chosen during the training procedure? Also, which training algorithm is employed, C5.0 or CART?\n\n- Why does the structure of GraphChef often require more than five layers?\n\n- Why does the expressive power of GraphChef surpass GIN on certain datasets, and can you provide some qualitative analysis?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8971/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699236602915,
        "cdate": 1699236602915,
        "tmdate": 1699637129510,
        "mdate": 1699637129510,
        "license": "CC BY 4.0",
        "version": 2
    }
]