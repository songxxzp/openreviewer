[
    {
        "id": "f08ziItNxy",
        "forum": "SsbMoaQUyA",
        "replyto": "SsbMoaQUyA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3998/Reviewer_fBKp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3998/Reviewer_fBKp"
        ],
        "content": {
            "summary": {
                "value": "The paper studied the task of updating an LLM with new knowledge. The proposed method turns a corpus containing new information into instruction (question) and response pairs ready for fine-tuning. Existing LLM is used to do this task. Authors argue that existing LLM has a tendency of using previously trained knowledge than the new one, aka exposure bias. The proposed solution is to repeat the relevant new content before answering the question in model-generated fine-tuning data.\n\nExperiments are done with an instruction-tuned LLAMA-7B and two new corpora (CNN, NQ). Model-based evaluation showed that the proposed method outperforms the baseline model and simple fine-tuning (adding facts directly and adding q&a pairs without repeating the new facts). To see how the model would forget old knowledge after fine-tuning, experiments with different sizes of samples from existing instruction tuning sets are done, which shows that even a small set of old instructions can prevent significant forgetting."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "How to update the model with new knowledge is an interesting problem."
            },
            "weaknesses": {
                "value": "Reviewer orders the concerns by their importance.\n\n- Probably a wrong assumption about the limits of incorporating new information into an old model's response. Retrieval-augmented generation (RAG) is a widely-adopted practice now, as seen in ChatGPT's browser-plugin and Google Bard. It can equip the models with fresh information without the need of retraining. The assumption made in Section 4 paragraph #3, \"However, it is impossible to maintain an infinitely large memory to store the new information\" is not really a problem given modern search engines / information retrieval techniques.\n\n- The hypothesis of exposure bias is not properly validated. Higher evaluation numbers of context-aware fine-tuning data over facts and facts+naive are not a sufficient proof that exposure bias exists. Reviewer suggests either design a more direct experiments at exposure bias, or do detailed analysis of the wins/losses between the results of different methods to show that the differences are indeed caused by exposure bias.\n\n- Model-based evaluation is less convincing. All evaluations in the experiments are based on models. Reviewer thinks it's important to look at the results, either by human raters or with human analysis, to make sure the differences are not caused by the model's bias.\n\n- Evaluation lacks confidence intervals. Some of the differences are quite narrow, e.g. in NQ. The CNN set only has ~300 evaluation prompts (smaller). The evaluator itself is a model. All the factors call for a confidence interval on all numbers to make sure they are statistically significant.\n\n- Poor writing. Spelling errors and non-sense characters possibly from lack of proof-reading are common."
            },
            "questions": {
                "value": "Here are some detailed questions/suggestions.\n\nFirst of all, there are numerous simple writing errors and misspellings. E.g. \n\n- Last paragraph in Section 1, \"Additionally, we perform an additional study\", only one \"additional\" is needed. \n- Section 1 last paragraph last sentence (before \"To summarize\"). It's too long to be understandable.\n- Second claim at the end of Section 1: what is \"mbbvpropose\"?\n- Definition 2.4. \"Informtaion\"?\n- Right before Equation 5, what is \"vcvc?\"\n\nReviewer will not try to enumerate all of them. Some professional proofreading is needed (maybe using an LLM to help spell check, at least?)\n\nOther non-writing related questions\n\n- Please explain any new math notions when they first appear. E.g. section 2.2 first paragraph, X_T? Or in Definition 2.4 I_s(C)?\n\n- Right after Definition 2.3, the \"here P(r|A,i,T)\", it doesn't match anything in Equation (3).\n\n- Equation 7 is very confusing. What do you mean by indicator function and what do they do here?\n\n- Table 1: adding the new prefix in the response will effectively change the model's output. Do you mean we will need to do post processing from then on?\n\n- Page 5 footnote 1. Not clear what the authors are doing.\n\n- Section 4 2nd paragraph. GPT-4 has 170 trillion parameters? That's a trillion with a T.  What is the source of information?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "none"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3998/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698594890466,
        "cdate": 1698594890466,
        "tmdate": 1699636361953,
        "mdate": 1699636361953,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qiOINCDMIn",
        "forum": "SsbMoaQUyA",
        "replyto": "SsbMoaQUyA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3998/Reviewer_z86B"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3998/Reviewer_z86B"
        ],
        "content": {
            "summary": {
                "value": "Authors propose a solution to the novel problem of self-information update for LLMs, without human intervention. This is a very relevant problem in the context of LLMs, as it\u2019s important for LLMs to be up to date with the latest knowledge, and the current solutions involve training the model from scratch, or fine-tuning the existing model with the latest data, which can lead to catastrophic forgetting. Authors argue that fine-tuning can also lead to a situation where LLM ignore the new facts and knowledge, because of the exposure bias. A simple and practical technique is then proposed to handle the exposure bias problem."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Authors propose a simple solution to the important problem of updating LLMs with the latest information. Keeping the LLMs up to date with the latest facts and knowledge is essential for its\u2019 practical use-cases as a personal assistant and other applications. Existing commercial LLMs take months before they are updated with the latest information from the web. \n- The solution proposed is simple to implement. It involves forcing the LLM to generate the news article first, followed by generating the response next. \n- Experimental results from fine-tuning the LLaMA-7B model demonstrate the effectiveness of the method on different metrics and different datasets."
            },
            "weaknesses": {
                "value": "- The theoretical analysis is not very rigorous, but I don't see this as a major problem, and in fact authors have addressed this in the manuscript itself (Remark after Definition 2.4)."
            },
            "questions": {
                "value": "- In the Figure 2 (a), the performance after fine-tuning drops somewhere between [2K, 3K] replay examples, but then improves post 4K examples. How would someone applying this technique choose the replay buffer size to avoid the drop in the performance below the base model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3998/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699002622330,
        "cdate": 1699002622330,
        "tmdate": 1699636361879,
        "mdate": 1699636361879,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KGvZ0EWEhA",
        "forum": "SsbMoaQUyA",
        "replyto": "SsbMoaQUyA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3998/Reviewer_f8AU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3998/Reviewer_f8AU"
        ],
        "content": {
            "summary": {
                "value": "This work addresses the problem of continual fine-tuning of LLMs using new training corpora. In contrast to previous approaches that involve significant human effort in converting unstructured data into more structured data to make the best use of new training corpora in fine-tuning LLMs, this work attempts to eliminate the human involvement in the process. It proposes a self information updating task for LLMs that makes use of unstructured new training corpora in the fine-tuning of LLMs without requiring human editorial intervention. The self information updating task is formulated as a self-distillation task with the LLM as the teacher and the new training corpora as the context. Specifically, this consists of first generating instruction response pairs from the new training corpora by using the LLM and then using the resulting pairs for fine-tuning. However, this naive approach is plagued by the issue of exposure bias where existing information from LLM is prioritized over the novel information in the new training corpora. This issue is identified and analyzed theoretically and it is noted that the bias affects both response generation and probability of instructions. \n\nThe work proposes a heuristic for mitigating exposure bias. The key idea is to do a context-aware distillation which is essentially identifying the source of the instruction (new corpus or pretraining corpus) and making use of only probability terms that are relevant for the source. Given a training triple (i, r, d), where d is the article from the new corpus from which the LLM generated the instruction-response pair (i, r), the model is forced, during finetuning, to learn to generate d from i and then r from (d, i). Here d serves as the context for i to the LLM for generating i. \n\nThe work presents results from experiments on two datasets - CNN News and NQ Val. The first data set consists of a small news corpus (50 CNN news articles from the period March and April 2023) with a resulting evaluation set of 301 instruction-response pairs generated using GPT-4. The NQ Val data set consists of extracted paragraphs from Wikipedia pages (long answers for questions in the validation split of the Natural Questions benchmark) with an evaluation set consisting of questions and short answers. As the baseline an instruction-following model from the LLaMA-7B fine tuned with instruction-following data from Alpaca and InstructionWild is used. (Of these, a randomly sampled 300 instruction-response pairs is used as old data.) The work answer consistency and context consistency as the metrics for evaluating the proposed approach as well as the baseline. Experimental study reveals that the proposed approach produces significant improvement over the baseline on both metrics for instruction-response pairs from new corpus while giving marginally negative improvement for instruction-response pairs from the instruction fine-tuning corpus. In contrast, a combination of principled but subject to exposure bias methods (namely, Fact + Naive) gives slightly lower improvement for instruction-response pairs from new corpus while not degrading performance on instruction-response pairs from the instruction fine-tuning corpus."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Addresses an interesting and relevant problem in the context of finetuning LLMs. \n2. Provides interesting and useful theoretical analysis that highlights the source of exposure bias problem."
            },
            "weaknesses": {
                "value": "1. Dropping completely probability terms based on context (to get Equation 7 from Equation 5) no doubt prioritizes novel information in the new training corpus over existing information from LLM for instruction-response pairs generated from the new corpus. But this also affects the model's performance on old instruction-response pairs as evident from Table 2. A possibly better approach is to use all the terms in Equation 5 but with source dependent weight that is determined by hyperparameter tuning. \n\n2. While the idea of generating instruction-response pairs from LLM for new corpus is interesting and eliminates the need for human effort, it is also a potentially problematic issue as current LLMs are known to hallucinate and as a consequence the instruction-response pairs could be affected by it."
            },
            "questions": {
                "value": "1. In Table 2, the answer consistency score of Context-aware is significantly higher than that of the baseline for new corpus. This is very encouraging indeed. However, the performance of  Context-aware on new corpus is significantly lower than that of the baseline for Old (0.480 vs 0.699 for CNN News and 0.256 vs 0.699 for NQ Val). What explains this gap and what can be possibly done to reduce it?  \n\n2. In Table 2, the answer consistency score of the baseline for Old is nearly the same for both data sets (CNN News:0.696 and NQ Val:0.691). In contrast, Context-aware seems to be doing significantly better on CNN News than on NQ Val (0.480 vs 0.256). What explains this gap and what can be possibly done to reduce it? \n\n3. How many instruction-response pairs are in NQ Val and how many are used in evaluation?\n\n4. From Figure 2.a it appears that some forgetting is inevitable in the proposed approach and increasing the no of replay examples doesn't really help alleviate this problem beyond a point. One wonders if forgetting will become more severe when fine tuning is done continually and with more datasets than the two used in the experimental studies. Figure 2.b seems to indicating this (score for old reduces further when the model already fine-tuned with NQ Val is further fine tuned with CNN News). What can be possibly done to address this issue?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3998/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699533397146,
        "cdate": 1699533397146,
        "tmdate": 1699636361756,
        "mdate": 1699636361756,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qqSnA45gFg",
        "forum": "SsbMoaQUyA",
        "replyto": "SsbMoaQUyA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3998/Reviewer_b44p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3998/Reviewer_b44p"
        ],
        "content": {
            "summary": {
                "value": "The manuscript presents a comprehensive study of a novel approach to updating large language models (LLMs) with the latest information without requiring substantial human intervention. Recognizing the limitations of LLMs in accessing up-to-date information due to reliance on pretraining corpora, the authors propose a self-information updating task. The study also examines the issue of knowledge forgetting, proposing a solution with a compact replay buffer."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1\u3001The study introduces a novel and practical method for updating LLMs with the latest information, which is a significant advancement in the field.\n\n2\u3001The research delves into the challenge of knowledge forgetting, offering a promising solution with a replay buffer mechanism.\n\n3\u3001The experimental results showing improvements in factual consistency scores are impressive and indicative of the method's efficacy."
            },
            "weaknesses": {
                "value": "1\u3001The manuscript could provide a more in-depth analysis of how the integration of facts into training losses specifically counteracts exposure bias.\n\n2\u3001This paper only conducted experiments on factual consistency, which is not convincing enough. Furthermore, an explanation has not been provided as to why this task was chosen to demonstrate the effectiveness of the method for updating large language models (LLMs) with the latest information.\n\n3\u3001There are many unclear aspects in the formula explanations within the paper, leading to confusion. For example, it is not specified what the text sequence \\( x \\in X \\) in the context before and after Equation 1 specifically represents.\n\nOverall, the motivation of this paper and the explanations of the symbols used throughout the text are not very clear, and the expression still needs to be more clearly and concisely articulated."
            },
            "questions": {
                "value": "How does the setting of updating large models with new corpora proposed in this paper differ from continual learning and model editing? The paper lacks discussion and analysis in this regard."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3998/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699595163469,
        "cdate": 1699595163469,
        "tmdate": 1699636361577,
        "mdate": 1699636361577,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CFDv6oAqsO",
        "forum": "SsbMoaQUyA",
        "replyto": "SsbMoaQUyA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3998/Reviewer_o5o3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3998/Reviewer_o5o3"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a task called Self Information Updating (SIU) in LLMs, which requires the model to update itself using unstructured information sources, without the need for human intervention, resulting in a more practical task. Additionally, the paper introduces an approach, context-aware distillation, to address exposure bias, which tends to prioritize old information over new data when updating the LLMs. Finally, the effectiveness of this approach is demonstrated through evaluation using two new created datasets, revealing a notable 0.16 increase in the factual consistency score."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1- Reproducibility: The authors mentioned that their source code and data will be shared once their work is published. This means anyone can use these to reproduce their results.\n2- Two new datasets were developed to evaluate information updates.\n3- Factual consistency is increased by up to 0.16 using their new proposed approach.\n4- The forgetting problem is addressed to some extent by using a small portion of the original training data."
            },
            "weaknesses": {
                "value": "How big can be the information updating corpus? If we have big information updating\ncorpus, how it will affect the performance?"
            },
            "questions": {
                "value": "Please see the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3998/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3998/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3998/Reviewer_o5o3"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3998/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699739157292,
        "cdate": 1699739157292,
        "tmdate": 1699739157292,
        "mdate": 1699739157292,
        "license": "CC BY 4.0",
        "version": 2
    }
]