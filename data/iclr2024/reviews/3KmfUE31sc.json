[
    {
        "id": "8IOoEms2IK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3654/Reviewer_xekD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3654/Reviewer_xekD"
        ],
        "forum": "3KmfUE31sc",
        "replyto": "3KmfUE31sc",
        "content": {
            "summary": {
                "value": "The authors study unsupervised anomaly detection in images, presenting a reconstruction-based framework that combines transformers and GRUs to improve the fidelity of the reconstruction (decoding) stage. The proposed method is compared with a set of baselines from literature on two datasets: MVTec AD and CIFAR-10."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well organized.\n- The authors address an actively studied field."
            },
            "weaknesses": {
                "value": "- The method is not compared with recent state-of-the-art methods reporting better results (see [1]). The authors claim to achieve state-of-the-art results, but according to [1], this is clearly not the case. Since the method does not surpasses competing methods, its benefits are not well justified from a practical point of view.\n- CIFAR-10 is rather consider as a toy dataset, while MVTec AD is rather small in size. The proposed method should be evaluated on additional datasets to demonstrate its generalization capacity.\n- The proposed method is constructed on top of UniAD, which represents an incremental / limited contribution. The RAS framework should be demonstrate in conjunction with multiple approaches to be evidenced as a standalone contribution.\n- The ablation study is not comprehensive enough. For example, the contribution / necessity of each type of gate is not shown.\n- The implementation details are insufficient to reproduce the results. For example, the number of attention heads is not mentioned.\n\n[1] https://paperswithcode.com/sota/anomaly-detection-on-mvtec-ad"
            },
            "questions": {
                "value": "The are no particular questions besides addressing the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3654/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697363859830,
        "cdate": 1697363859830,
        "tmdate": 1699636321766,
        "mdate": 1699636321766,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s5xhpDXybv",
        "forum": "3KmfUE31sc",
        "replyto": "3KmfUE31sc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3654/Reviewer_m5gg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3654/Reviewer_m5gg"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an image-based anomaly detection (both at image-level and pixel-level evaluations) method based on the reconstruction of the data with a framework having so-called RASFormer, which enables capturing the spatial relationships among different image regions by enabling the \"temporal\" dependencies during the reconstruction process. The experimental analysis was applied to two standard datasets with an in-depth analysis of the method as well as comparing it with the SOTA."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Although the proposed method's RASFormer is very similar to GRU, it is somehow distinguished from GRU by incorporating a transformer for exploring spatial information within the feature maps. \n- The experimental analysis demonstrates that the proposed method brings in improvements, particularly for the cases where contextual correspondence is important (such as industrial manufacturing)."
            },
            "weaknesses": {
                "value": "1) Overall, the paper's writing is satisfactory; however, the introduction/abstract should be improved. For instance, the first sentence makes a very strong claim, even though in some cases, the anomaly detection task can classify an image as normal or abnormal without necessarily localizing the abnormal regions. Additionally, the last sentence of the introduction needs proper citations. Moreover, I fail to see the significance of specifically mentioning \"industrial manufacturing\" in the opening sentence.\n\n2) It is unclear whether the metrics used in this paper align with prior research. For example, in a recent ICCV work [A], the reported results of prior research do not always match those presented in this paper.\n[A] https://openaccess.thecvf.com/content/ICCV2023/papers/Yao_Focus_the_Discrepancy_Intra-_and_Inter-Correlation_Learning_for_Image_Anomaly_ICCV_2023_paper.pdf\n\n3) Several related methods exist, especially in the proceedings of ICCV 2023, which the authors may not have been able to include in their comparisons. However, it is important to note that some of these methods, in certain cases, outperform the proposed approach and should be considered for inclusion. Nevertheless, I do not consider this a reason for rejection, given the constraints of ICCV data and the submission deadline of ICLR and particularly due to fact that the paper brings in technical novelties to the task."
            },
            "questions": {
                "value": "- Weakness (2) \nPlease also comment on (3)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3654/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698059481463,
        "cdate": 1698059481463,
        "tmdate": 1699636321696,
        "mdate": 1699636321696,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IJO6ofTwgz",
        "forum": "3KmfUE31sc",
        "replyto": "3KmfUE31sc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3654/Reviewer_5D2p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3654/Reviewer_5D2p"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a method for unsupervised anomaly detection. Basically, it adopts the framework of a previous method (UniAD) and improves UniAD by proposing a RASFormer block to replace the vanilla Transformer block. The motivation of RASFormer is to enhance the contextual correspondence between decoded features and is implemented by using a reset gate to filter the prior decoded features. On the pixel-level task and image-level task, the proposed method (RAS) outperforms UniAD and other anomaly detection methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed RASFormer block is computationally efficient and easy to implement.\n2. RASFormer experimentally improves the original transformer block.\n3. The experiments are extensive and convincing."
            },
            "weaknesses": {
                "value": "1. Writing\nThe role of contextual embedding x_t should be carefully explained. Why x_t is a 'spatial version of image feature'?  How is x_t updated?\nThe authors claim the 'reconstruction as sequence (RAS)' framework is a key contribution of the method. However, it seems the main framework of RAS is the same as UniAD. \n\n2. Novelty\nThe framework of RAS is similar to UniAD. The gating method in RASFormer block looks like GRU or LSTM. Although RAS outperforms UniAD experimentally, the novelty and overall contribution are limited."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3654/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698671333817,
        "cdate": 1698671333817,
        "tmdate": 1699636321526,
        "mdate": 1699636321526,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "l50025P2hY",
        "forum": "3KmfUE31sc",
        "replyto": "3KmfUE31sc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3654/Reviewer_rUeR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3654/Reviewer_rUeR"
        ],
        "content": {
            "summary": {
                "value": "An anomaly detection process is proposed in the industrial manufacturing processes. The proposed method is based on reconstructing an input image (or its features) using an auto-encoder type transformer based deep network which is trained on the normal data only. For inference, if the reconstruction error is small, the input is considered as normal, otherwise  anomalous.  A Reconstruction as Sequence (RAS) framework is proposed  which reconstructs the image in multiple steps or stages and tries to better utilise the contextual information available within the image. Results are demonstrated on an often used MVTec-AD dataset and also on CIFAR10 dataset considering some classes as normal and training the model using only these classes. The remaining classes are considered as abnormal and later the trained model tries to identify a given image as normal or abnormal. One may think of such algorithms as normal-supervised (or one-class supervision) instead of using the term `unsupervised'. It is because, if during training some data from abnormal classes is also included in the training data, the discrimination will be lost. Therefore these methods are indeed not `unsupervised' rather  have normal class supervison."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. A transformer based architecture is proposed which better utilises the context and relationships among different parts of the input image.  \n2. Good performance is reported on two datasets. \n3. The proposed method has outperformed UniAD method."
            },
            "weaknesses": {
                "value": "1. The usage of terms temporal dependencies, temporal dynamics is confusing since the proposed algorithm handles only images, not videos.\n\n2. Authors have focussed too much on a NIPS2022 paper UniAD, while many good papers have appeared after that work such as: PNI : Industrial Anomaly Detection using Position and Neighbourhood Information (ICCV23) having  99.56 (image level) 98.98 (pixel level) performance. \n\n3. Performance improvement compared to the baseline UniAD  is just 1.9% and 0.7% on MVTec AD. Such improvement  cannot be considered as significant.  \n\n4. For throughput comparisons, much faster methods are also available.  See the papers with code `Anomaly Detection on MVTec AD' webpage. \n\n5. Most SOTA methods are reporting results on additional anomaly detection datasets like BTAD, VisA etc, where as CIFAR10 although employed by UNiAD, is a classification dataset. For a better comparison with other methods, same set of datasets should have been used. \n\n6. The compared methods US, CutPaste, MKD, FCDD are neither introduced nor referred. It is perhaps due to copying results from UniAD without even referring who are the compared methods.  \n\n7. In Table 4, 29% more throughput is obtained when no encoder transformer is used while a single decoder transformer is employed (Te=0, Td=1). That would mean the CNN features are directly input to decoder transformer  as one can infer from Fig. 2. Decoder will change those features to image? I guess there is no sequence or step by step decoding in this case. Authors need to explain how decoding is done without encoding step?"
            },
            "questions": {
                "value": "Please answer all questions in weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3654/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3654/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3654/Reviewer_rUeR"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3654/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762769677,
        "cdate": 1698762769677,
        "tmdate": 1699636321414,
        "mdate": 1699636321414,
        "license": "CC BY 4.0",
        "version": 2
    }
]