[
    {
        "id": "N5mlwvsj5i",
        "forum": "nY9nITZQjc",
        "replyto": "nY9nITZQjc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5086/Reviewer_ju3D"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5086/Reviewer_ju3D"
        ],
        "content": {
            "summary": {
                "value": "The authors identify two main gaps in current benchmarks for assessing multimodal intent recognition systems: multi-turn conversational interactions in the real world contain out-of-scope utterances (not relevant to the intent detection taxonomy), and the existence of multiple parties/agents in dialogs.\nTo address this gap, this paper proposes the MIntRec2.0 dataset with 1.2K conversations containing 15K annotated utterances harvested from three TV series totalling ~12 hours of dialogue.\nThe authors use strong LLM baselines (ChatGPT), human evaluation, and existing methods to populate the benchmark evaluation results and identify challenges in the dataset that are not addressed by existing models."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1: The authors give detailed human performance results and identify the gap between SOTA multimodal approaches and humans. Furthermore, this indicates the benchmark is fairly difficult (not simple common sense) considering human performance of 71% with ~7% of training data.\n\nS2: Comparison of resources in Table 1 is clear and convincing; in particular this dataset seems to be the first to include multi-party dialogs, and one of the only datasets with OOS labels. The expanded intent classes for the coarse-grained \"Express Emotions\" and \"Achieve Goals\" existing intents make sense and are sufficiently distinct from one another to add value to the taxonomy."
            },
            "weaknesses": {
                "value": "W1: Would have liked to see more discussion on the effect of incorporating multi-modal information aside from mentions of numbers and \"indicating the challenge of using multi-modal information on out-of-scope data\". A case study or deeper slice of results would be illuminating here, or even a deeper analysis in the main paper of what primarily constitutes OOS.\n\nW2: More context should be provided on why 1-4% increases on metrics are considered significant in this case (is it statistically significant or is there some other meaning here e.g. for real life use cases?)\n\nW3: Some clarity in the ChatGPT vs. Humans evaluation would be helpful - were the 10 dialogues of 227 utterances fixed across experiments, or were other few-shot training samples of 10 dialogs with different intent class balances attempted / were metrics aggregated? It would be helpful to picture whether the metric improvements are conditioned on specific intent classes."
            },
            "questions": {
                "value": "Q1: Given the TV series (comedy), how were things like laugh tracks or other \"audience signals\" of e.g. humor taken into account? What are the primary differences from the authors' perspective between these data sources and multi-turn conversations in the real world that intent detection systems would mainly work with?\n\nQ2: Were annotators queried about what modalities of information were most valuable to their understanding of the conversational intent? This would be helpful information to gauge the differences between how humans and MMI systems work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5086/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5086/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5086/Reviewer_ju3D"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5086/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697900740476,
        "cdate": 1697900740476,
        "tmdate": 1700674515017,
        "mdate": 1700674515017,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Pb6J58Vjd7",
        "forum": "nY9nITZQjc",
        "replyto": "nY9nITZQjc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5086/Reviewer_kGfg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5086/Reviewer_kGfg"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes MIntRec2.0, a large-scale multimodal multi-party benchmark dataset that comprises 1,245 high-quality dialogues, totaling 12.3 hours, which is interesting and valuable for multimodal training and evaluation. The proposed dataset serves as a valuable\nresource, providing a pioneering foundation for research in human-machine conversational interactions, and significantly facilitating related applications."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed dataset is interesting and valuable for research in human-machine conversational interactions. The motivation is clear, the authors proposed three limitation including single-turn utterances, scales and out-of-scope utterances. The overall structure is well organised. In addition to more than 9,300 in-scope samples, it also includes over 5,700 out-of-scope samples appearing in multi-turn contexts, which naturally occur in real-world open scenarios, enhancing its practical applicability. Furthermore, they provide comprehensive information on the speakers in each utterance, enriching its utility for multi-party conversational research. This paper is a good dataset and resource paper."
            },
            "weaknesses": {
                "value": "no obvious flaws. The figures should be expanded."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5086/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5086/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5086/Reviewer_kGfg"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5086/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698526305119,
        "cdate": 1698526305119,
        "tmdate": 1700494751135,
        "mdate": 1700494751135,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wNODXTFOEd",
        "forum": "nY9nITZQjc",
        "replyto": "nY9nITZQjc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5086/Reviewer_SFq3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5086/Reviewer_SFq3"
        ],
        "content": {
            "summary": {
                "value": "Multimodal intent recognition is very important to natural human-computer interactions and has gained more attention in recent years. The authors released a new version of MIntRec, named MIntRec 2.0, which contains more categories and considers out-of-scope scenarios. It will support more explorations in this field.\nThere are also some limitations of this work, the authors should add more experiments to support the effectiveness of this dataset and improve the writing to make the paper more clear."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This work builds a larger multimodal intent recognition dataset under interaction scences with 30 categories of fine-grained intent annotations and some out-of-scope samples, which is closer to the real-world scenarios."
            },
            "weaknesses": {
                "value": "1. The dataset is only from three different TV series, which limits the diversity of scenes and topics.\n2. The multimodal fusion performance is not obvious in some metrics, the authors should explain the results more clearly, which can support the effectiveness of the multimodal intention dataset.\n3. There are also some mirror errors, such as: \n1) A representative sample is depicted in Figure 5.  -> Figure 1\n2) Interpretations of both the expanded and existing intent categories can be found in Table 7 and Appendix G, respectively. -> Table 2"
            },
            "questions": {
                "value": "1. What do you think are the reasons why the improvement of using non-verbal multimodality is not obvious?\n2. The intention of a speaker in interactions is very difficult and complex, so what do you think are the influencing factors?\n3. What do you think is the difference between a human-computer interaction dataset and the proposed human-human interaction dataset for the multimodal intent classification task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5086/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698858365260,
        "cdate": 1698858365260,
        "tmdate": 1699636499289,
        "mdate": 1699636499289,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AaWbp2R9Nv",
        "forum": "nY9nITZQjc",
        "replyto": "nY9nITZQjc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5086/Reviewer_Un9C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5086/Reviewer_Un9C"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the \"MIntRec2.0\" dataset, which offers a significant advancement in the field of multimodal intent recognition, addressing critical gaps in existing benchmarks by including multi-turn conversations, out-of-scope (OOS) utterances, and multi-party interactions. The dataset, derived from three TV series, encompasses 12.3 hours of dialogue across 1,245 conversations, totaling 15,000 annotated utterances. The expanded intent classes and inclusion of OOS labels mark a notable progression from previous datasets, aiming to bridge the gap between current benchmarks and real-world conversational scenarios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Dataset: The dataset's scale and inclusion of multi-party dialogues with both in-scope and OOS samples enhance its realism and applicability in human-computer interaction research.\n- Approach: The framework efficiently handles multimodal data, and the detailed annotation, including speaker information, enriches its utility for diverse conversational research.\n- Experiment: The comparison of human performance with state-of-the-art multimodal approaches in the dataset highlights the existing gap and provides a challenging benchmark for future research."
            },
            "weaknesses": {
                "value": "- The dataset's sourcing from only three TV series restricts the diversity of scenes and topics. This limitation might not fully represent the vast array of real-world conversational contexts.\n- The unclear performance improvement in some metrics for multimodal fusion needs a more explicit explanation. This clarification will support the dataset's effectiveness in demonstrating the advantages of multimodal approaches.\n- Additional experiments could better demonstrate the dataset's effectiveness, particularly in the nuances of multimodal intent recognition.\n- The lack of detailed discussion on the incorporation and impact of multi-modal information, especially regarding out-of-scope data, is a notable omission. More detailed analyses or case studies would illuminate the challenges and benefits of using multi-modal data.\n- The comparison between ChatGPT and human evaluations could benefit from more detail, such as the consistency of dialogue samples across experiments and how results might vary across different intent classes."
            },
            "questions": {
                "value": "1. Why is the improvement from using non-verbal multimodality not more pronounced? What are the key challenges in leveraging these modalities to enhance intent recognition accuracy?\n2. Considering the complexity of intent in human interactions, what are the major influencing factors that the dataset and framework account for, and how are ambiguous or contradictory multimodal signals handled?\n3. What are the significant differences between datasets focused on human-computer interactions and the proposed human-human interaction dataset in terms of multimodal intent classification tasks?\n4. How do factors like laugh tracks or audience reactions in the TV series-based dataset influence the intent recognition process, and how do these sources differ from real-world multi-turn conversations?\n5. Were annotators asked about which modalities were most valuable in understanding conversational intent? Such insights could help compare human perception with multimodal intent recognition systems."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5086/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5086/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5086/Reviewer_Un9C"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5086/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699856269265,
        "cdate": 1699856269265,
        "tmdate": 1699856269265,
        "mdate": 1699856269265,
        "license": "CC BY 4.0",
        "version": 2
    }
]