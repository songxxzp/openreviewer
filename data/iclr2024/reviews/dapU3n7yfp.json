[
    {
        "id": "6LSShH7gDu",
        "forum": "dapU3n7yfp",
        "replyto": "dapU3n7yfp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5978/Reviewer_osGE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5978/Reviewer_osGE"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new optimization algorithm for eliciting toxic outputs from pre-trained language models. The authors demonstrate that their algorithm outperforms other adversarial attack baselines in its efficacy for eliciting toxic content. They also show that their approach can be used to identify and mitigate potential risks associated with the deployment of language models in real-world settings. Overall, the paper's contributions include a new optimization algorithm for eliciting toxic content, a demonstration of its efficacy, and insights into the potential risks and benefits of using such an approach in practice."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper is well-written and clear to read.\n\n+ The idea is novel and the performance improvement is obvious across different settings.\n\n+ Sufficient ablation studies are conducted to support the claim."
            },
            "weaknesses": {
                "value": "- Limited evaluation: The authors only evaluate their approach on a single dataset. This could limit the generalizability of their results and make it difficult to draw broader conclusions about the effectiveness of their approach.\n\n- Limited scope: The paper only focuses on the generation of toxic content from language models and does not address other potential risks associated with their deployment, such as bias or misinformation. This could limit the paper's relevance and impact in the broader context of language model research. \n\n- Additionally, the paper only tests on 1/2/3-word output, which is not practical. It is unknown whether the proposed method would work for longer toxic outputs."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I personally do not like the motivation of this paper, i.e., designing methods to elicit toxic content from LLMs. This could lead to bad results if no proper defense methods are developed. Given the fast spread of LLMs today, such algorithms could involve potential ethical concerns."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5978/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5978/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5978/Reviewer_osGE"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5978/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698389240406,
        "cdate": 1698389240406,
        "tmdate": 1699636639446,
        "mdate": 1699636639446,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DNE6nNUBjC",
        "forum": "dapU3n7yfp",
        "replyto": "dapU3n7yfp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5978/Reviewer_QEDL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5978/Reviewer_QEDL"
        ],
        "content": {
            "summary": {
                "value": "With the emergence of LLMs, it is crucial to discover and modify potential toxic outputs before deployment.\nIn this work, authors propose ASRA, a new optimization algorithm that concurrently updates multiple prompts and selects\nprompts based on determinantal point process. Experimental results on six\ndifferent pre-trained language models demonstrate that ASRA outperforms other\nadversarial attack baselines in its efficacy for eliciting toxic content."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The experiments show significant perfanmance improvement over previous works.\n* The authors provide detailed and insightful analysis about hyperparameter tuning and future application."
            },
            "weaknesses": {
                "value": "* The proposed method seems a little complicated, especially the DPP procedure.  I'd like to know it's speed (e.g. throughput ) compared with \nprevious works. \n* Experiments limits on eliciting toxic text of up to 3 words. In practice, however, what we really care about is not generating\ntoxic text at any length. Therefore I have a concern about this work's transferability to a more practical scenario."
            },
            "questions": {
                "value": "* I want to know that could DPP be possibly replaced by other simpler method, e.g. methods often used in document summarization task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5978/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5978/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5978/Reviewer_QEDL"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5978/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698647095335,
        "cdate": 1698647095335,
        "tmdate": 1700640001001,
        "mdate": 1700640001001,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vz26NMJXVl",
        "forum": "dapU3n7yfp",
        "replyto": "dapU3n7yfp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5978/Reviewer_xui8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5978/Reviewer_xui8"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a reversing PLM approach to generate prompts that lead to toxic outputs. The proposed method consists of an approximation step, a refinement step, and a selection step implemented via a DPP model. Empirical studies with 6 PLMs indicate the effectiveness of the proposed approach. \n\nOverall, this is an interesting paper with a decent algorithm and compelling results. Minor concerns:\n\n(1) The evaluation sets seem small (a few hundreds of examples), is it possible to report some evaluation results on large datasets?\n\n(2) Normally, it is more important to detect prompts without any malevolent words but leading to toxic responses for a PLM. However, from the cases shown in the Appendix, the prompts found by the proposed method usually contain malevolent words. This may limit the application of the proposed method in practice. Then, is it possible to further improve the method by avoiding malevolent words when reversing PLMs?\n\n(3) Since the algorithm needs to search every word in the vocabulary, when the vocabulary size is big, will the complexity becomes an issue in application of the method?"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "decent algorithm\ncompelling results"
            },
            "weaknesses": {
                "value": "small evaluation sets\ncomplexity could be an issue"
            },
            "questions": {
                "value": "see the summary"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5978/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698683000825,
        "cdate": 1698683000825,
        "tmdate": 1699636639194,
        "mdate": 1699636639194,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yeCrMsVWqb",
        "forum": "dapU3n7yfp",
        "replyto": "dapU3n7yfp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5978/Reviewer_crro"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5978/Reviewer_crro"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses the challenge of preventing pre-trained language models from generating toxic content. The authors focus on the process of \"reversing\" language models to intentionally produce such content, which serves as a test to identify and mitigate potential risks before deployment. They present ASRA (Auto-regressive Selective Replacement Ascent), an optimization technique designed to efficiently elicit toxic output from language models. ASRA works by updating multiple prompts simultaneously and using a determinantal point process to select the most effective ones. The algorithm was tested on six different language models, and the results showed that ASRA surpasses other adversarial attack methods in terms of eliciting toxic content. Additionally, the research found a significant link between the success of ASRA and the perplexity of the generated output, with less connection to the language models' size. The findings suggest that reversing language models with a comprehensive dataset of toxic text can be a strategic method for evaluating and improving the safety of language models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper proposes a novel method of textual adversarial attack for generation tasks, which looks generalizable for different text generation tasks.\n- The empirical results look convincing, outperforming the baselines by a large margin\n- This paper focuses on AI safety, which is an important topic recently regarding safeguarding the output of LLMs. The authors offer a potential tool for developers to identify and fix vulnerabilities before deployment."
            },
            "weaknesses": {
                "value": "- Lack of baselines: The attack algorithm is based on HotFlip (2018), which is a bit old and less effective than the recently proposed baselines. I am wondering if the authors have compared with adversarial attack baselines proposed more recently such as Seq2sick [1], which shows better optimization effectiveness for text-to-text generation tasks. \n- Lack of defense models (detoxified models): While I appreciate the authors\u2019 efforts in comparing different pretrained models, it would also be interesting to evaluate against different defense approaches/detoxification approaches, such as [2,3,4], and confirm whether the attack is still effective. \n- Validity of toxicity evaluation: The authors mention that their evaluation setup \u201ccan be applied to bridge the evaluation of toxicity in different PLMs\u201d, and \u201cspeculate that the success rate of ASRA attack might be positively correlated with the toxicity of language models.\u201d However, I do not see any evidence about whether the ASR here can be a good proxy to reflect model toxicity. Given that the model toxicity evaluation is conducted by evaluating model responses with a lot of different inputs and contexts, the setup of this paper is to evaluate model responses given \u201cunnatural\u201d prompts. I am thus suspicious about whether the test above can give an accurate evaluation of model toxicity. \n- There is an important concern regarding the potential misuse of this work by malicious users to bypass safety controls of LLMs and elicit model toxicity. I believe it would be valuable for the paper to include a discussion of this aspect. \n\n[1] Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples. (AAAI 2020)\n[2] Plug and Play Language Models: A Simple Approach to Controlled Text Generation (ICLR 2020)\n[3] DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts (ACL 2021)\n[4] Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models (NeurIPS 2022)"
            },
            "questions": {
                "value": "The proposed optimization seems generalizable for different text generation tasks. Have you ever used the approach to attack other text generation tasks (such as [1])?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There is an important concern regarding the potential misuse of this work by malicious users to bypass safety controls of LLMs and elicit model toxicity. I believe it would be valuable for the paper to include a discussion of this aspect."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5978/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699309719566,
        "cdate": 1699309719566,
        "tmdate": 1699636639105,
        "mdate": 1699636639105,
        "license": "CC BY 4.0",
        "version": 2
    }
]