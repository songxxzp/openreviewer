[
    {
        "id": "lczut8kGcf",
        "forum": "H56qXKaNpF",
        "replyto": "H56qXKaNpF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4395/Reviewer_dBej"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4395/Reviewer_dBej"
        ],
        "content": {
            "summary": {
                "value": "This paper introduced an efficient image inpainting method.\nThe proposed method has two steps, image inpainting in low-resolution followed by ViT-based image super-resolution (ViTSR).\nEspecially, positional embedding interpolation (PEI) is proposed to handle resolution discrepancy in training and inference for ViTSR.\nPEI bicubically interpolates the positional embedding used in training to a target resolution in the inference phase.\nThis simple idea is verified in a number of experiments including image inpainting, super-resolution, colorization, and deblurring."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Resolution in training and inference no longer have to be the same due to the proposed PEI. PEI is simple and practical."
            },
            "weaknesses": {
                "value": "I understand the key idea of this paper is inpainting in low-resolution followed by SR is efficient with acceptable visual quality. So I think any SR model can be adopted, but why ViT is selected in the SR model in this paper? Why it is essential? Have the authors tried just using ELAN or other SR models? \nSpecifying LPIPS values in Table3 will be more helpful to readers since the ViTSR is trained using perceptual losses, as in Table4."
            },
            "questions": {
                "value": "It is unclear how much PEI deteriorates SR performance. I think it can be verified by comparing it with fixed-scale ViTSR models (ie. ViTSR w/ PEI vs separate ViTSR models for 2x, 4x, and 8x upscale factors w/o PEI). Have the authors tried this kind of experiment as an analysis?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4395/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4395/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4395/Reviewer_dBej"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4395/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698487012696,
        "cdate": 1698487012696,
        "tmdate": 1699636412843,
        "mdate": 1699636412843,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KB1GgZM2XP",
        "forum": "H56qXKaNpF",
        "replyto": "H56qXKaNpF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4395/Reviewer_gKZS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4395/Reviewer_gKZS"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a position embedding interpolation method for high-resolution image inpainting. The authors developed a computation-efficient framework in which the guided diffusion is used for coarse inpainting on the low-resolution images and a ViT-based SR model to refine the images and obtain the high-resolution result. The overall organization and presentation are good and the paper is easy to follow, however, I think the main problem is the novelty of this paper is limited."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Here are the strength points of this paper:\nThe authors proposed a framework using guided diffusion and ViT for high resolution inpainting. The experimental results show that the proposed method outperform the compared methods in image inpainting and super-resolution tasks on CelebA, Places2 and other datasests."
            },
            "weaknesses": {
                "value": "Here are the weak points of this paper:\nThe novelty of the proposed framework is limited. The method in this paper consists of guided diffusion, ViT, and position embedding. There have already been multiple transformer-based SR methods proposed in the previous 3 years, e.g. IPT and SwinIR. Therefore, I cannot see a clear motivation or explanation for designing the framework. The problem analyzed in the abstract is also unclear."
            },
            "questions": {
                "value": "Here are my detailed comments and suggestions. \n1.\tThe diffusion model is trained with low-resolution data. In image inpainting, it generally relies on the quality of the generated prior, which may result in a performance loss or detail loss. Will subsequently applying a super-resolution model cause an amplification of distortion issues? The author needs to explain this clearly.\n\n2.\tLack of novelty. The paper combines existing modules such as guided diffusion, ViT, and position embedding.\n\n3.\tThere are grammar errors that need attention. For example, in the introduction section, the phrase 'super-resolution the image' should be 'super-resolve the image'.\n\n4.\tThe paper is titled 'image-to-image' tasks, but the experiment section only shows image inpainting and super-resolution tasks. It is suggested to conduct a wider range of experiments to demonstrate the effectiveness of the proposed method.\n\n5.\tThe paper lacks an ablation study.\n\n6.\tIn section 4.3, ViTSR+ needs more specific clarification. There is ambiguity here because in super-resolution methods, the '+' symbol generally signifies self-ensemble.\n\n7.\tMany of the compared super-resolution methods are based on GAN training. This training approach is not conducive to PSNR and SSIM metrics. However, in Table 3, the authors only compare PSNR and SSIM and do not include metrics such as PI, NIQE, LPIPS, and FID, which are more suitable for GANs. Therefore, we need the authors to provide an explanation for the fairness of this comparison."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4395/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698635707507,
        "cdate": 1698635707507,
        "tmdate": 1699636412770,
        "mdate": 1699636412770,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SxiRvTM2pV",
        "forum": "H56qXKaNpF",
        "replyto": "H56qXKaNpF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4395/Reviewer_B9FJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4395/Reviewer_B9FJ"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed to use position embedding to make the ViT trained on low-res image be applied to high-res input during inference. They showed the inpainting results of concatenating a diffusion-based inpainting model in low-res and a ViT for super resolution purpose. The authors also claimed the proposed ViT can be working well for many restoration tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Interesting exploration of applying position embedding interpolation to improve the generalization of ViT to high resolution input."
            },
            "weaknesses": {
                "value": "- The motivation of this paper is vague. The reviewer is not sure about the main focus of the paper: the inpainting / restoration task in higher resolution, or the position embedding interpolation trick for ViT. The authors emphasized on the position embedding in the title, while even the ablation study related to that is moved to the appendix. \n- The concept of efficiency is not clear enough. Do the authors mostly mean the training efficiency or the inference time? It seems the authors try to claim the proposed tricks enable the training to be done on lower-res image, while the model can be equally good when being applied to high-res images. While it can only be called efficient training tricks, but the entire ViT cannot be claimed as efficient. If the authors want to claim the pipeline of low-res inpainting + high-res ViT is efficient, then what is the purpose of showing image restoration results, and why the pipeline is better than LDM in efficiency?\n- Inpainting task may require the high-res masked image when doing super resolution since it needs to resolve the boundary seam issues. While this paper (Fig.4) did not show the composited results and the proposed ViT also did not include the original high-res image as the input. So it was not designed for inpainting. \n- The authors did not show the evidence that applying bicubic interpolation is worse than other lightweight preprocessing using an off-the-shelf upsampler. \n- Table 1 missed many results even though LaMa can work on any resolution input. Not sure whether the authors have controlled the testing dataset when showing these numbers."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4395/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698658869192,
        "cdate": 1698658869192,
        "tmdate": 1699636412679,
        "mdate": 1699636412679,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tsKNm8doll",
        "forum": "H56qXKaNpF",
        "replyto": "H56qXKaNpF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4395/Reviewer_qNze"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4395/Reviewer_qNze"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a computational efficiency network for inpainting. The diffusion model is used for inpainting the low-resolution input and ViTSR with position embedding interpolation achieves multi-resolution inferring. The proposed network obtains superior performance in high-resolution image inpainting and super-resolution tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors combine the diffusion model and ViT with position embedding interpolation to achieve efficient image-to-image translation. The network achieves high efficiency for high-resolution image inpainting. The model achieves superior performance on high-resolution image inpainting and super-resolution tasks."
            },
            "weaknesses": {
                "value": "The reviewer thinks the novelty of the paper is limited. Specifically, this paper achieves high efficiency by combining existing models, such as diffusion models and ViT. The only interesting point is the combination pattern, especially position embedding interpolation, to realize multi-resolution inferring. However, this technique is already widely used in other domains. In addition, the authors use many augmentation methods and loss functions to achieve high performance."
            },
            "questions": {
                "value": "1. How does the network perform if using the same image augmentation as the employed guided diffusion (Dhariwal&Nichol).\n2.  In Tab.1, the reviewer finds that the proposed network is inferior to DiffIR in many metrics.\n3. For complexity verification in Tab.2, the proposed framework is only compared with diffusion models. Is that because the authors only employ diffusion models for low-resolution inpainting?\n4. In Fig.3, the reviewer thinks that the results of the proposed network in the top two rows are not as good as that of DiffIR, such as the teeth in the first image and the beard in the second image.\n5. Why there is no performance of ViTSR+ in Tab.4 for CelebA-HQ? Why do the authors conduct the experiments on this dataset while other papers do not?\n6. The reviewer finds that in the contribution part and conclusion section, the author aims to propose an efficient network for high-resolution image inpainting. In the title, the related keywords are \"Image-to-image ViT\". The reviewer thinks that this is not suitable since the authors do not propose an efficient ViT, just proposing a combination method to achieve high efficiency."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4395/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4395/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4395/Reviewer_qNze"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4395/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834203647,
        "cdate": 1698834203647,
        "tmdate": 1699636412611,
        "mdate": 1699636412611,
        "license": "CC BY 4.0",
        "version": 2
    }
]