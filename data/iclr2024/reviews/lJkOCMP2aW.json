[
    {
        "id": "eogpsp59VJ",
        "forum": "lJkOCMP2aW",
        "replyto": "lJkOCMP2aW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2153/Reviewer_LNPN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2153/Reviewer_LNPN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes multi-scale transformers with adaptive pathways. The proposed method integrates both temporal resolution and temporal distance for multi-scale modeling. It further enriches the multi-scale transformer with adaptive pathways. Experimental results showed the efficacy of proposed method and state-of-the-art performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1)  It's novel to propose multi-scale transformers with adaptive pathways. \n\n2) It's novel to integrate both temporal resolution and temporal distance for multi-scale modeling.\n\n3) The experiments showed state-of-the-art performance."
            },
            "weaknesses": {
                "value": "1.The current time series forecasting datasets are pretty small, and performance may be satuated or over-fitting. Could the method be used for larger datasets?\n\n2. Scalformer [1] also uses the multi-scale nature of time series data, this paper didn't mention and compare the similarities and differences with Scalformer.\n\n[1] Shabani, Amin, et al. \"Scaleformer: iterative multi-scale refining transformers for time series forecasting.\" ICLR (2023)."
            },
            "questions": {
                "value": "1. The current time series forecasting datasets are pretty small, and performance may be satuated or over-fitting. Could the method be used for larger datasets?\n\n2. Scalformer [1] also uses hierarchical design and the scales of time series data, Could this paper compare the similarities and differences with Scalformer.\n\n[1] Shabani, Amin, et al. \"Scaleformer: iterative multi-scale refining transformers for time series forecasting.\" ICLR (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2153/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2153/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2153/Reviewer_LNPN"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2153/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699060260877,
        "cdate": 1699060260877,
        "tmdate": 1700775081291,
        "mdate": 1700775081291,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1SaChqAPQw",
        "forum": "lJkOCMP2aW",
        "replyto": "lJkOCMP2aW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2153/Reviewer_Sqch"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2153/Reviewer_Sqch"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new multi-scale Transformer architecture for long-range time series modeling.The Discriminant Fourier Transform (DFT) is utilized to determine the patch sizes so as to divide the input time series into patches of different sizes, thus enabling cross-scale information fusion. In the multiscale transformer block, intra-patch attention and inter-patch attention mechanisms are utilized to perform attentional operations, thus enhancing the processing of temporal information.Experiments show the proposed method achieves state-of-the-art performance among existing models and exhibits superior generalization capabilities across different transfer scenarios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.The paper is well written and well-motivated.\n2.The Multi-Scale Router combines the advantages of patch division and seasonality decomposition.\n3.Dual attention helps to harmonize operations between intra-patch and inter-patch components, allowing the transformer to efficiently process time series data."
            },
            "weaknesses": {
                "value": "Time-series Dense Encoder (TiDE) is also a popular long-term time-series forecasting benchmark.But the paper does not include experiments comparing the proposed method to TiDE."
            },
            "questions": {
                "value": "Why is there a gap between the benchmark data in the paper and the original paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2153/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699084496326,
        "cdate": 1699084496326,
        "tmdate": 1699636148315,
        "mdate": 1699636148315,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vN19rBewUx",
        "forum": "lJkOCMP2aW",
        "replyto": "lJkOCMP2aW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2153/Reviewer_5cD6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2153/Reviewer_5cD6"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a variation of PatchTST architecture in the context long-horizon time series forecating."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- An interesting study that makes an incremental step towards making transformers effective at long-horizon forecasting task\n- Paper is clearly written and topic is important for the ICLR audience"
            },
            "weaknesses": {
                "value": "- \"Recent advances for time series forecasting are mainly based on Transformer architectures\". I would say this statement is not aligned with the most recent empirical results and contradicts existing facts. Having read the following papers one could argue that the transformer based models in time series forecasting have been basically a disaster in the recent years, mainly because authors of papers based on transformer-driven models disregarded including some basic baselines in their studies. Please rewrite intro and related work accordingly.\n    - Challu et al. N-HiTS: Neural hierarchical interpolation for time series forecasting. AAAI'23\n    - Zeng et al. Are transformers effective for time series forecasting? AAAI'23\n    - Li et al. Do Simpler Statistical Methods Perform Better in Multivariate Long Sequence Time-Series Forecasting? CIKM'23\n- Not all datasets are present in the study. Please include additional results on ILI and Traffic from PatchTST\n- Please include results from Zeng et al. Are transformers effective for time series forecasting? AAAI'23 in your table and you will see that your results are not state of the art. This makes the results unconvincing, because basically a very complex model is not able to use the same inputs as very simple models in an effective way. Transformer based papers have consistently failed to include appropriate baselines in the studies creating a large gap in methodology and undermining the ultimate reliability of these studies. The work can be interesting if authors show that with the proposed modifications a transformer based model can be more effective than much simpler and faster models presented in Zeng et al. and Li et al.\n- The model seems to borrow conceptually very heavily from the PatchTST model without explicitly recognizing the source of inspiration. Without a detailed explanation of the actual difference between the two architectures the proposed architecture appears to be a minor perturbation of the original PatchTST."
            },
            "questions": {
                "value": "- When talking about multi-scale processing in related work please discuss relation to Challu et al. N-HiTS: Neural hierarchical interpolation for time series forecasting AAAI'23, which seems to be relevant work on multi-scale modelling"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2153/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2153/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2153/Reviewer_5cD6"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2153/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699391502019,
        "cdate": 1699391502019,
        "tmdate": 1699640280538,
        "mdate": 1699640280538,
        "license": "CC BY 4.0",
        "version": 2
    }
]