[
    {
        "id": "Vmx1Kd9MCX",
        "forum": "9RIbNmx984",
        "replyto": "9RIbNmx984",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4977/Reviewer_jHhA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4977/Reviewer_jHhA"
        ],
        "content": {
            "summary": {
                "value": "This paper studies a theoretical model (based on random features) of temporal difference learning of the value function in a markov reward process using neural networks with one hidden layer. The key high-level parameters studied are the number of parameters $N$ in the hidden layer, the number of distinct states visited $m$, and the $\\ell_2$ regularization parameter $\\lambda$. The paper focuses on the regime where $N,m \\to \\infty$ but the ratio $N/m$ remains constant. Under suitable assumptions, the main results of the paper give numerically solvable equations for computing the asymptotic mean-squared Bellman error (both the true error, and the empirical error observed during training). These results then imply that the double-descent phenomenon occurs in this regime i.e. that the true mean-squared Bellman error decreases when the ratio $N/m < 1$ (the under-parameterized regime), has a large spike at $N/m =1$ (the interpolating regime), and then further decreases when $N/m > 1$ (the over-parameterized regime). Furthermore, the paper shows that increasing the regularization parameter $\\lambda$ mitigates this double-descent phenomenon, with sufficiently large values of $\\lambda$ eliminating the spike at $N/m =1$ altogether. Empirical results on simple reinforcement learning tasks are in very good agreement with the theoretical predictions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper gives strong theoretical grounding for the occurrence of the double-descent phenomenon in temporal difference learning.\nFurthermore, the results demonstrate that double-descent can be mitigated by sufficient regularization, which could potentially have practical applications.\nFinally, the empirical results in the paper demonstrate that the theoretical predictions are sufficiently accurate (in terms of the rates at which the predicted quantities concentrate) that even in small examples there is good agreement between theory and the empirical measurements."
            },
            "weaknesses": {
                "value": "The paper gives strong results for the random features model, but it is somewhat unclear how practically relevant these are.\nWhile it is typical to study such models in order to prove high-level statistical properties that may be relevant to practical neural networks, it would be nice to have some discussion of when/whether these results might be relevant.\nFor example, when proving results in the NTK regime, it is actually true that even deep neural networks of sufficient width will behave as predicted given appropriate random initialization. Of course, the NTK regime requires a width that is unrealistic and thus does not necessarily capture the practical dynamics well. The mean-field regime is designed to (at least somewhat) mitigate this issue with the NTK regime.\nIt is unclear from the discussion in this paper precisely how the double-asymptotic regime fits-in to this picture with regards to modelling deep neural networks (as opposed to single-hidden layer networks).\nSuch discussion of the relevance of the double-asymptotic regime to deep-neural networks would also help to clarify the novelty of this work when compared with prior work on value learning in the NTK and mean-field regimes (e.g. the cited 2022 paper by Agazzi and Lu)."
            },
            "questions": {
                "value": "As discussed in the weaknesses above, is there something additional that can be said about how studying the double-asymptotic regime where $N/m$ is constant can give insights into the behavior of deep neural networks? In particular, the NTK and mean-field regimes may allow for theorems that apply to deep neural networks, but perhaps with somewhat unrealistic levels of over-parameterization. Is there any similar statement that can be made in this regime? Or is there some fundamental trade-off where the realistic parameter choices in the double-asymptotic regime preclude results applying beyond a single hidden layer."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4977/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4977/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4977/Reviewer_jHhA"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4977/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698750142940,
        "cdate": 1698750142940,
        "tmdate": 1699636485103,
        "mdate": 1699636485103,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BWvtx92Dmg",
        "forum": "9RIbNmx984",
        "replyto": "9RIbNmx984",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4977/Reviewer_a6mC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4977/Reviewer_a6mC"
        ],
        "content": {
            "summary": {
                "value": "This paper examines the behavior of the (regularized) LSTD estimator within the random feature model and the on-policy evaluation context. It investigates a doubly asymptotic regime in which both the number of parameters and the number of distinct visited states tend to infinity at the same rate. A double descent phenomenon is observed between the empirical mean squared Bellman error (MSBE) and the population MSBE. The authors derive analytical equations for both the asymptotic empirical MSBE and the asymptotic population MSBE. Based on these derivations, they demonstrate that the double-descent phenomenon diminishes as either the regularization increases or the number of distinct unvisited states decreases. Empirical experiments corroborate the theoretical findings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The setting provides a clean model to study TD methods in the over-parametrized regime, and the results are also fairly clean (at least conceptually, despite some of the linear algebra expressions being messy). The findings clearly demonstrate a theoretical over-parametrized double-descent regime for LSTD in this asymptotic setting, which to my knowledge, is a novel and interesting finding. I have not checked the proofs in great detail but from what I have seen they do check out. The experimental results, despite only looking at simple environments, appropriately confirm the theory."
            },
            "weaknesses": {
                "value": "One weakness is that the double descent behaviour is said to be related to the behaviour of the correction factor $\\delta$ but I do not think that this is demonstrated analytically. What is precisely happening to \\delta when N=m, or alternatively what is happening to the difference between the empirical MSBE and the true MSBE? This is one of the main selling points of the paper so it would be good to expand on this. \n\nOther than that, there are a few writing nitpicks, which I outline below. \n1. \"We use the regularized LSTD algorithm, [...] because it converges to the same solution as other TD algorithms.\" This is true without regularization, but is it true with regularization?\n2. \"Theoretical studies of TD algorithms often focus on asymptotic regimes, in either finite or infinite state space, where the number of samples n -> \\infty while keeping the number of model parameters N fixed\". The authors likely meant that it has not been studied what happens when parameters and samples both converge to infinity, but this sentence also implies that only asymptotic analyses of TD exist (as opposed to finite-sample), which is not correct. \n3. Section 3 \"Notations\" paragraph: what is the \"Euclidean norm\" of a matrix? Does the authors mean the spectral norm or the Frobenius norm?\n4. Items 4 and 5 in the \"Contributions\" paragraph both repeat that the correction terms vanish when the regularization increases and/or the number of unvisited states go to zero.\n5. The notation for feature matrix $\\Sigma_S$ is unfortunate since it clashes with typical notation for covariance matrices. Is there a reason not to use the more conventional $\\Phi$ notation for this matrix.\n6. Section 3 \"Linear Temporal-Difference Methods\" section: the Mean-Square Projected Bellman Error is undefined.\n7. Section 4.1. For the sampling paradigm I do not think the authors literally mean $(s,s') \\sim \\mu(s) P(s,s')$. This latter expression would be the marginal probability of observing s' only, and is just a number (not a distribution one can sample from).\n8. Section 4.1. \"We consider the on-policy setting, where $D_train$ is derived from a sample path of the MRP or its stationary distribution $\\pi$. These are very different (one is i.i.d.), are both truly considered?\n9. Some vector norms are indexed by $\\pi$, but many other norms are not indexed (e.g. Equation 8, 9,10). Is this a typo, or are all unindexed norms the Euclidean norm?\n10. Relatedly: the closed form for the LSTD estimator is defined without the usual diagonal D_\\pi, which implies that the algorithm performs regression on the Euclidean norm rather than $L_2(\\pi)$ (one can compare with the more classical weighted version e.g. in Amortila, Jiang, Szepesv\u00e1ri ICML 2023). In this case the LSTD estimator is different from the usual weighted one, can the authors discuss whether this is a meaningful difference and/or whether the results would hold for the weighted estimator?\n11. \"Under the assumption that all states are visited $\\hat{MSBE}(\\theta)$ converges to $MSBE(\\theta)$ with probability 1, as the number of collected transitions $n \\rightarrow \\infty$ (Bradtke & Barto, 1996).\" I do not think that this is shown in that paper, which instead deals with the projected bellman error.\n12. Typo in Assumption 3: should be O(1/\\sqrt{m})"
            },
            "questions": {
                "value": "Questions:\n1. Why are the two asymptotic quantities the parameter number and the number of unvisited states as opposed to the parameter number and just the sample size?\n2. What are the technical difficulties when studying regularized LSTD for policy evaluation as opposed to regularized least squares for the regression case ($\\gamma = 0$). The text mentions that there is a lack of positive definiteness in the LSTD matrix -- is this the only difficulty? How is this difficulty overcome?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4977/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4977/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4977/Reviewer_a6mC"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4977/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815022694,
        "cdate": 1698815022694,
        "tmdate": 1699636485022,
        "mdate": 1699636485022,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0Krr5dZOCt",
        "forum": "9RIbNmx984",
        "replyto": "9RIbNmx984",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4977/Reviewer_7TMf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4977/Reviewer_7TMf"
        ],
        "content": {
            "summary": {
                "value": "The authors investigate a double-descent phenomenon in RL, where the mean-squared Bellman error (MSBE) initially decreases as the number of learnable parameters $N$ increases, then increases when $N$ equals the number of distinct visited states $m$, and finally decreases again in the overparameterized regime as $N \\to \\infty$. An analogous effect has been previously identified in supervised learning but not RL. The authors focus their analysis on least-squares temporal difference (LSTD) with random features to simulate a wide 1-layer network without needing to model the complexities of feature extraction. Analytical expressions for the MSBE as a function of the ratio $N / m$ and the $l^2$-regularization coefficient $\\lambda$ are derived, which indicate a spike in MSBE near $N / m = 1$ and $\\lambda \\approx 0$. This behavior is verified empirically by LSTD experiments in small environments, such as Taxi. Notably, it is experimentally shown that the peak at $N / m = 1$ disappears when either $m = |S|$ (the state space is covered) or $\\lambda$ is increased."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- I really like the idea of studying LSTD with random features to model neural networks. It is a useful abstraction to remove the noise of TD updates and other complexities of online learning like state visitations, changing features, and potential divergence with nonlinear FA.\n- The mathematical analysis is excellent and highly significant. The results are clear and the remarks after the theorems provide helpful interpretations for them. The math is correct to the best of my knowledge (although I only checked a small part of the appendix). It is apparent that a lot of thought and effort was put into this paper.\n- The experiments clearly demonstrate the hypothesized double-descent phenomenon, and also answer focused questions like to what extent regularization or changing the discount factor can change the double-descent peak. The analysis is good and connects the experimental observations back to the derived terms in the theorems.\n- The paper organization is great. The contributions, related work, and preliminaries/system model are explained well and help ease the reader into the main results."
            },
            "weaknesses": {
                "value": "- I wonder if the observed double-descent behavior may be specifically due to the analysis of MSBE rather than some other learning objective, like the mean-squared value error (MSVE), the error relative to $v_\\pi$. For example, it is known that TD methods that optimize MSPBE can converge to a solution far away from the minimum MSBE, which may explain the large peak around $N = m$. However, if we measure against MSVE instead, would we still see this? I imagine that if we have a set of features with some MSVE, and then add more features, we could just set those corresponding new weights to zero to preserve the MSVE. This seems to imply to me that the MSVE would be monotonically non-increasing as $N$ increases, although I am not sure if I am missing something. Perhaps the authors could better justify their choice of MSBE in this regard.\n- The paper says that only one random seed (42) was used to generate the training data, so I have to guess that only one seed was used to generate the random features as well. Since the MSBE is highly dependent on these \u201cnetwork\u201d features, it is impossible to tell from just one trial that the double-descent observation is a common phenomenon over the distribution of possible feature initializations. It would be helpful to show the results averaged over a number of different networks, with confidence intervals shown.\n\n**Minor:**\n- On the start of page 4: \u201cDeep RL can be cast as a special case of LFA, where the neural network learns both the feature vectors and the parameter vector.\u201d While I see what you mean here, I personally found it confusing to describe deep RL as a special case of linear FA, since deep RL is nonlinear FA. I think this could be reworded for clarity.\n- Also page 4: \u201cLinear TD methods are LFA methods that minimize the MSBE\u201d but then later in the paragraph it is correctly noted that TD methods \u201cminimize MSPBE rather than MSBE.\u201d The first part should say something like TD methods *try to* minimize the MSBE, but actually minimize MSPBE."
            },
            "questions": {
                "value": "1. Can the norm term in $\\bar{\\text{MSBE}}(\\hat{\\theta})$ (Theorem 5.3) be simplified to $\\|| \\bar{r} + (\\gamma P - I) \\frac{1}{\\sqrt{n}} \\frac{N}{m} \\frac{1}{1+\\delta} \\Phi_S U_n \\bar{Q}_m (\\lambda) r \\||^2_D$, i.e., factoring out $\\gamma P - I$? Or was it left expanded intentionally?\n1. Why was recursive LSTD used for the experiments? Since only the final weights are needed, would it be more efficient to just sample data from the behavior policy and then explicitly calculate the TD fixed point like $w = A^{-1} b$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4977/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4977/Reviewer_7TMf",
                    "ICLR.cc/2024/Conference/Submission4977/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4977/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815457675,
        "cdate": 1698815457675,
        "tmdate": 1700722609033,
        "mdate": 1700722609033,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "88Fq8l27gf",
        "forum": "9RIbNmx984",
        "replyto": "9RIbNmx984",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4977/Reviewer_xcHJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4977/Reviewer_xcHJ"
        ],
        "content": {
            "summary": {
                "value": "In this work the authors provide a theoretical analysis of regularized LSTD in a setting where the number of parameters of the value function approximation N and the number of distinct visited states m go to infinity with a fixed ratio N/m. The authors do so by relying on random matrix tools, in particular using features of the form sigma(WA) where W is a random feature matrix (Nxd) applied to any set of d-dimensional states A. The connection to neural networks is via this simplification via random matrices and can be likened to an N-dimensional single layer network, where the simplification is less objectionable due to the asymptotic regime.\n\nIn this setting, with some minor assumptions, the authors go on to show that the asymptotic empirical MSBE can be related to the two-norm of the resolvent applied to the reward vector, plus a second order correction term, and that the empirical MSBE approaches the asymptotic almost surely. The authors then show, including empirically for a small number of examples, that it is the second order term that contributes to double-descent behavior. A similar story can be shown for the true MSBE, albeit with a more complicated term and second-order factor.\n\nThe authors also show that the empirical variant reduces to a similar approach of Louart et al. for supervised learning when gamma, the discount factor, goes to zero. Additionally as the number of features, equivalent to the width of the network, or the regularization term increases the correction factor diminishes (thus leading to less prominence of the double-descent behavior)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper provides a clear, concise (as much as possible) presentation of their results. They relate the work to the literature well, particularly how this complements and extends work from the supervised learning literature. Similarly the effect of increasing the regularization strength or width of the \"network\" is beneficial. The empirical experiments are similarly good in explaining the effect of the theoretical work."
            },
            "weaknesses": {
                "value": "I don't think the paper has particular weaknesses, however I would ask what the practical implications of this work are? This is less a comment on this work and more a question directed towards the theoretical analsysis of double-descent in general, in that the random-feature assumption while seemingly valid asymptotically departs from how such work might be used in practice. Note that the authors do leave an extension to \"deep\" neural networks for later work. Similarly the authors discuss policy evaluation as a next step.\n\nOverall I would like to more discussion of how to place this within the \"standard/practical\" setting, although I do admit that actually attacking that problem is beyond the scope of this already long paper."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4977/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699113561421,
        "cdate": 1699113561421,
        "tmdate": 1699636484830,
        "mdate": 1699636484830,
        "license": "CC BY 4.0",
        "version": 2
    }
]