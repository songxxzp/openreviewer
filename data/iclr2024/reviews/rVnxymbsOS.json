[
    {
        "id": "TYFyCX1v4c",
        "forum": "rVnxymbsOS",
        "replyto": "rVnxymbsOS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission738/Reviewer_HX8x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission738/Reviewer_HX8x"
        ],
        "content": {
            "summary": {
                "value": "This paper explores how LMs capture the factual knowledge: by constructing an internal KG or simply memorized QA pairs.\nTo do that, the authors use LLaMA to construct a synthetic data set and train GPT-2 with it. Then, the verification is done by prompting and probing."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The topic is good. This paper studies a very interesting research problem. Figuring it out can help people better understand how LMs encode factual knowledge, and how to better enhance/edit factual knowledge. \n2. The synthetic setting is novel. Researching the factual knowledge captured during pretraining is fairly intractable, since it's hard to annotate facts from the pretraining corpus and run analysis experiments on pretraining. This paper constructs a synthetic pretraining environment using LLaMA and reproduces the pretraining of small GPT-2."
            },
            "weaknesses": {
                "value": "My main concern is the experiment setting. The general design is okay, but some essential points are missing. I'll elaborate in detail below.\n  1. The template to construct the synthetic dataset is problematic. \n\n      a) First of all, it's not diverse. Even if using LLaMA, it works more like a simple verbalizer. We know that factual knowledge is sparse in the text (compared to linguistic knowledge): some facts exist across multiple sentences. But in the synthetic dataset, facts are verbalized in a quite limited way. For analysis, it's fine since LM is trained in a general way. However, this paper includes the pretraining part. I don't think pretraining with this dataset is reasonable. It's easy for LM to learn shortcuts.\n      \n      b) Second, the biography is too short and quite similar to each other. LMs may capture and extract facts relating to one person or topic. Such short and similar text might cause LMs to be confused when learning facts. Even for humans, it's hard to differentiate persons/topics with such short and similar introductions.\n\n2. Contributions 1 and 2 are not clear to me. \n\n      a) The synthetic data is generated randomly, and there could be conflicts between each other. Researching the generalization is a bit confusing (contribution 2). \n\n      b) While for contribution 1, it seems the authors just try to test if GPT-2 generalizes well to understand the question and extract facts for answering. I don't know if GPT-2 is pretrained from scratch or not (see question 2). If yes, I don't think such pretraining data can let GPT-2 understand language. If not, then the setting is not reasonable, and facts may have been learned in GPT-2 before training.\n\nIn summary, I think this paper explores an interesting topic. However, the above concerns are essential to prove that the findings are correct. If the authors can address my concerns successfully, I'll be glad to raise my score to strong accept."
            },
            "questions": {
                "value": "1. Why use LLaMA-30B, not 65B or 7B?\n2. For the GPT-2 small, do you \"pretrain\" it from scratch or pretrained GPT-2?\n3. Why use LoRA for the finetuning? If I understand correctly, the model is quite small, and it's totally unnecessary to use parameter-efficient tuning. It not only reduces the test accuracy but also makes the setting harder to follow. And thus much information in Figure 2 is redundant.\n4. The augmentations in section 4.2 are confusing. Why this kind of augmentation? Do you follow existing work or propose it at first? From my view, I don't think this can work as augmentation. It can just alleviate the problem of the problematic templates.\n5. Why call the dataset a semi-synthetic biography? Generated from LLM doesn't mean the fact is true. I think it's still synthetic data.\n6. What does the \"(Part A)\" mean in the title?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission738/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission738/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission738/Reviewer_HX8x"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission738/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698286801087,
        "cdate": 1698286801087,
        "tmdate": 1700669138303,
        "mdate": 1700669138303,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HXXyxIrtOo",
        "forum": "rVnxymbsOS",
        "replyto": "rVnxymbsOS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission738/Reviewer_kyUn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission738/Reviewer_kyUn"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes the gap between knowledge memorization and utilization of LLMs. Upon a carefully created benchmark, it examined the effect of different training processes and data augmentation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* This paper provides valuable insights into knowledge memorization and extraction of LLMs, especially the influence of training processes. It could have broad impact on related research areas, such as addressing knowledge conflicts."
            },
            "weaknesses": {
                "value": "* The limited scale of the experiments restricts the generalizability of the conclusions. This paper conducts experiments on a single dataset focused on individuals, employing a relatively small model size."
            },
            "questions": {
                "value": "* Result figures are a bit hard to read due to the small font size."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission738/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815368357,
        "cdate": 1698815368357,
        "tmdate": 1699636000973,
        "mdate": 1699636000973,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AGXtbw1ywt",
        "forum": "rVnxymbsOS",
        "replyto": "rVnxymbsOS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission738/Reviewer_DDxF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission738/Reviewer_DDxF"
        ],
        "content": {
            "summary": {
                "value": "This paper primarily addresses the research question: How do language models memorize factual knowledge during pre-training, and how do they extract and apply this learned knowledge for reasoning and answering questions? To answer this question, the authors create two biographical text datasets, one generated using templates and the other through prompting the LLAMA model. Additionally, a question-answering dataset is developed to evaluate the model's capability to extract knowledge. The key findings through controlled experiments include:\n1. During pre-training, models tend to focus on quickly learning to answer questions rather than comprehending the complete knowledge behind the answer. This behavior might hurt the model's generalization performance on unseen data.\n2. If the representation in the pre-training dataset is too uniform, it might impact the model's knowledge acquisition, potentially affecting its generalizability. Hence, data augmentation to diversify knowledge representation can help the model better link related concepts, enhancing knowledge extraction."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-motivated, presenting a clear research question. The authors offer a lucid explanation of the significance and rationale behind their study.\n2. Utilizing synthetic datasets as a means to control variables is an ingenious approach.\n3. The experimental design is also ingenious, unveiling intriguing findings about model knowledge acquisition and extraction. Notably, the discovery that models prefer learning a simplified form of knowledge extraction before grasping the related underlying knowledge during pre-training offers valuable insights into the learning process. Additionally, the probing experiment results are compelling, suggesting that if training data representation is too uniform, the model might struggle to select the right tokens for knowledge storage."
            },
            "weaknesses": {
                "value": "1. While the paper presents meaningful insights into how models learn and store knowledge, the exploration into knowledge extraction seems lacking. The findings, especially regarding data augmentation enhancing generalization, aren't particularly novel. The paper doesn't delve deeply into how models select, extract, and utilize knowledge during inference time.\n2. The study appears limited in terms of tasks and datasets. It's unclear if the findings are generalizable to other domains or tasks. Exploring multiple tasks and datasets could have further validated the generalizability of the conclusions.\n3. Some of the results are confusing, and locating corresponding values within the paper can be challenging. For instance, section 4.1 references pre-train + QA fine-tuning results in Appendix E, but Appendix E does not include these results."
            },
            "questions": {
                "value": "Why focus solely on biographical data? What makes you believe that results from this type of data can be generalized to other domains and tasks, thereby addressing the overarching research question about knowledge learning and extraction in language models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission738/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission738/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission738/Reviewer_DDxF"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission738/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822863538,
        "cdate": 1698822863538,
        "tmdate": 1699636000884,
        "mdate": 1699636000884,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JWh5lqBkwb",
        "forum": "rVnxymbsOS",
        "replyto": "rVnxymbsOS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission738/Reviewer_vHD7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission738/Reviewer_vHD7"
        ],
        "content": {
            "summary": {
                "value": "The proposed paper, entitled \"Knowledge Storage and Extraction in Language Models (Part A),\" aims to explore how transformer-based language models store and extract knowledge during training and inference. The authors focus on factual knowledge, such as that found in a knowledge graph, which the language model needs to memorize from the training corpus, encode in its weights, and later extract to answer questions or perform logical reasoning during inference.\n\nThe paper addresses the following key questions:\n\n1) How do language models memorize knowledge during training, and extract it later to answer questions or perform logical reasoning during inference?\n\n2) Can language models be fine-tuned to extract knowledge from specific domains, and if so, how do they achieve this?\n\n3) What is the relationship between the model's knowledge extraction ability and different diversity measures of the training data?\n\n4) How does the model encode knowledge attributes at the hidden embedding of entity names, and is there a correlation between this encoding and the model's knowledge extraction ability?\n\nTo study these questions, the authors propose conducting an in-depth analysis using a controlled set of semi-synthetic biography data. They construct a synthetic dataset of 100k biographies, including attributes such as birthday, birth city, and major of study, and pretrain the language model on this dataset. The authors then evaluate the model's knowledge extraction ability by fine-tuning it on question and answer (QA) pairs for a fraction of individuals and testing its ability to answer QAs about the remaining fraction."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1) The authors conducted experiments to evaluate the effectiveness of knowledge augmentation and the impact of partially augmenting data on knowledge extraction for non-augmented data. The results of these experiments highlighted the significant role of these methods in improving performance on downstream tasks. \n\n2) The paper presents a well-balanced tradeoff between knowledge control and naturalness of data by accurately generating and subsequently rewriting it using LM. This approach enhances the clarity of results."
            },
            "weaknesses": {
                "value": "1) The authors did not provide sufficient information on QA evaluation and there is no established method to extract and assess answers from generative models. The experiments were conducted solely with the GPT-2 model, which is a decoding-only transformer. It would have been beneficial to include experiment results with encoder-decoder models such as T5 or similar alternatives."
            },
            "questions": {
                "value": "1) How many tokens was used for the training model (Size of generated datasets in terms of tokens with and without augmentation)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission738/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698897556158,
        "cdate": 1698897556158,
        "tmdate": 1699636000819,
        "mdate": 1699636000819,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "U8HxNRAcGT",
        "forum": "rVnxymbsOS",
        "replyto": "rVnxymbsOS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission738/Reviewer_xjTh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission738/Reviewer_xjTh"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates a very important question regarding how LLMs extract knowledge from text. This is different from the LLMs ability to regurgitate text that it had seen during training by being able to complete generations given a prefix. Instead, this paper investigates whether LLMs are actually able to understand facts by evaluating whether it can answer questions related to what it has been trained on. However, it is hard to study this phenomenon on LLMs that have been trained on web-text because it is very hard to ensure what the LLM might have seen during training. Instead, the authors study this in a semi-synthetic setting where the LLM is trained on synthetic biographies generated by templatized texts or LLama-30B. The LLM that has been pre-trained on scratch is a GPT2-small model. \n\nThe paper considers two different training paradigms - (a) Mixed training: in which the LLM is pre-trained on all the biographies as well as 50% of the question answer pairs, and (b) Pre-train + Fine-tune setting where the LLM is first pre-trained on biographies and then fine-tuned on 50% of the QA pairs. Note during mixed-training, the LLM observes biographies and QA pairs interspersed, but in pre-training+fine-tune the LLMs first observes only biographies and then only QA pairs. The performance of the model is tested on held-out QA pairs $P_{\\mathrm{test}}$\n\nIn the mixed-training setting, the LLMs are able to generalize to held-out questions but the paper shows that the learning behavior of the models are a bit abnormal. For example, instead of first learning from biographies, the LLMs instead first learns from the QA pairs (as demonstrated by rapid increase in accuracy of token prediction) and then aligns the encoded knowledge with the BIO data to learn and extract knowledge and generalize it to the held-out questions. The authors point out this is akin to \u201cstudents studying to pass the test\u201d. \n\nThe previous hypothesis is confirmed in the pre-train+fune-tune setting where the model fails to generalize at all on $P_{\\mathrm{test}}$. This is surprising as in both cases the model is trained on the same amount of data. This demonstrates that LLMs are great at memorizing and regurgitating text but that is not equivalent to extracting knowledge from it. \n\nThe paper next shows that knowledge augmentation-i.e. Augmenting the BIO set with multiple paraphrases, sentence permutations, and replacing pronouns with full-names increases the generalization accuracy (9.7% -> 96.6%). A possible explanation of this hypothesis is that exposing the model to varied expression of identical knowledge lets it focus on semantics rather than surface forms. This hypothesis is confirmed by training two kinds of linear probes that indeed show that upon knowledge augmentation, the representation of entity names capture attributes about the entities that are amenable to be extracted via fine-tuning for QA. The paper also shows that this generalization also holds true when a part of the data is repeated, the model can learn and apply its knowledge to new entities."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "**Originality**\n\n* The paper is original and does an investigation of a crucial problem about how knowledge is extracted by LLMs during pre-training. To do so, the paper adopts a synthetic setting and does exhaustive experiments to prove its hypothesis\n\n**Quality**\n\n* The paper is of high quality. The experiments carried out to address the research inquiries are comprehensive, and the derived findings have been meticulously analyzed and interpreted.\n\n**Clarity**\n\n* The paper is very clearly written and explained.  It is easily accessible to readers, ensuring that they encounter no difficulties in understanding its content.\n\n**Significance**\n\n* The paper makes substantial contributions by addressing the vital issue of how Language Models extract information from text. It reveals that Language Models acquire knowledge in a manner that significantly deviates from human learning processes. Furthermore, the paper underscores the importance of redundancy and repetition in knowledge extraction for Language Models. This insight offers a potential explanation for the remarkable generalization abilities observed in Language Models trained on web-text, which frequently exhibits these characteristics.\n\n* These results have several practical applications too - e.g. it might be important to rewrite infrequently occurring data inorder for LLMs to capture them during pre-training."
            },
            "weaknesses": {
                "value": "* Even though I really liked the results in the section \u201cCelebrity might help minority\u201d, I think an open question that is not addressed in the paper is how many celebrities are needed to save minorities. In other words, how many entities with frequently repeated information are required for generalization to unseen/new entities?\n* How much effect does scale have? The current experiments have been conducted by a GPT-2 small/medium model. Is it worth considering that some of the current findings might not hold true for models which are larger by orders-of-magnitudes? Would they demonstrate different kinds of learning?"
            },
            "questions": {
                "value": "* I think it would be nice to have some results/discussion regarding if all the results would hold true for truly large LMs (>= 65B parameters). It is unclear to me if the emergent capabilities of the models allow them to learn differently. \n* An analysis of how much rewriting is required would also be an interesting result and could have several practical implications."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission738/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698963477512,
        "cdate": 1698963477512,
        "tmdate": 1699636000750,
        "mdate": 1699636000750,
        "license": "CC BY 4.0",
        "version": 2
    }
]