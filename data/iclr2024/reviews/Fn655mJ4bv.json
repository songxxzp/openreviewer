[
    {
        "id": "TG2gCfv0kw",
        "forum": "Fn655mJ4bv",
        "replyto": "Fn655mJ4bv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5041/Reviewer_Nt2X"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5041/Reviewer_Nt2X"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a post-hoc interpretation method for structured output models that can only be accessed as black-boxes (ChatGPT for example, although it is not mentioned in this paper). The approach consists of two primary components: the energy block and the interpreter block, both of which are neural networks and require training. The former is an energy-based model (EBM) that is trained to evaluate the consistency of a structural input-output pair. The latter identifies the key features influencing a particular target output using a neural network followed by a Gumbel-Softmax layer. The training objective for the interpreter is to minimize the energy difference between the target output and probing output given the same subset of features. The proposed method is evaluated on both synthetic and real-world datasets, and the results show that it outperforms existing methods designed for single-output models, including LIME, SHAP, and L2X."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Originality: The problem definition, which caters specifically to structured output models and considers structural dependencies, stands out in the field of interpretation techniques, as most existing methods are designed for single-output models. The incorporation of EBMs showcases a novel approach to model interpretation.\n\n- Clarity & Quality: The paper is well-organized and systematically breaks down the proposed method, making it easy to follow. Definitions, equations and the role of each component are clearly explained.\n\n- Significance: Addressing the limitations of existing interpretation techniques, especially their neglect of inter-variable dependencies in structured output models, holds significance in the broader context of model transparency and interpretability. The problem the authors tackle is particularly relevant nowadays because of the increasing popularity of LLMs."
            },
            "weaknesses": {
                "value": "- My biggest concern is that the objective function for training the interpreter seems to be valid but not well-motivated. The interpreter is trained to minimize Eq. (5), the energy difference between the target output and probing output given the same subset of features. However, it is unclear to me why minimizing this objective function makes the interpreter faithful to the structured output model. After all, the model to be explained operates on the entire input feature space, not just a subset of features. Probing the model with a small subset of features may not be a good approximation of the model's behavior because the masked inputs are almost always out-of-distribution and the model output may be arbitrary. The only connection between the interpreter and the normal operation of the structured output model is the pretraining of the energy block, which is weak. The authors should provide more justification for the proposed objective function.\n- Although it is natural to use EBMs as a surrogate for structured output models, training deep EBMs is known to be difficult and less scalable than other generative models. This makes the proposed method less practical and even more unreliable."
            },
            "questions": {
                "value": "- How the constrained optimization problem in Eq. (9) & (16) is solved? Is it solved exactly or approximately? I believe this is a hard optimization problem for general EBMs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5041/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5041/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5041/Reviewer_Nt2X"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5041/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698684919880,
        "cdate": 1698684919880,
        "tmdate": 1699636493846,
        "mdate": 1699636493846,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HkQNBFQkjS",
        "forum": "Fn655mJ4bv",
        "replyto": "Fn655mJ4bv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5041/Reviewer_nGJs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5041/Reviewer_nGJs"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method that focuses on the interpretation of structured outputs. The authors  use an energy-based interpretation to predict certain input that is most relevant the structured pairs. They use an greedy method to iteratively optimize the objective function and further evaluate the proposed method in several datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. interpret structured output model seems an interesting and valuable topic."
            },
            "weaknesses": {
                "value": "1. The problem under investigation appears to be of significant value. However, the author's evaluation is limited to small-scale or synthetic datasets. This raises concerns about the scalability of the proposed method and its efficacy on larger datasets.\n\n2. Regarding the greedy approach to parameter learning, the author does not delve into an analysis of this SGD-like method nor provide relevant references. It remains unclear whether this greedy optimization truly converges to the optimal solution and how it might impact the energy model.\n\n3. While the core idea is presented clearly, the paper's structure and flow are challenging to navigate. Additionally, the notation used lacks clarity and could benefit from further refinement."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5041/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734449803,
        "cdate": 1698734449803,
        "tmdate": 1699636493758,
        "mdate": 1699636493758,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E1uarUs7vd",
        "forum": "Fn655mJ4bv",
        "replyto": "Fn655mJ4bv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5041/Reviewer_ZVtY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5041/Reviewer_ZVtY"
        ],
        "content": {
            "summary": {
                "value": "This work examines the problem of predicting which input features effect a specific part of a black box structured output model. The method consists of an interpreter model which outputs binary selections of input features related to a certain part of the structured output, and an energy model that approximates the structured output prediction distribution. The interpreter model sets unselected features of the input to zero. By training the interpreter to match the energy between selected features of $x$ paired with ground truth states for the relevant output structure and freely varying states for other outputs, the interpreter learns to select features of the input that are highly predictive of the relevant output structure. An algorithm for jointly learning the interpreter model and energy model is presented. Experiments on synthetic data show that model can correctly identify structured outputs when the ground truth is known, and that the method outperforms the Lime and L2X explainability methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The method explores a novel angle of using an energy function to improve interpretability of structured output models.\n* Experimental results show improved performance compared to the Lime and L2X methods.\n* Unlike the Lime and L2X methods, the proposed method can take into account all parts of the structured model output instead of just the target element when analyzing interpretability."
            },
            "weaknesses": {
                "value": "* Even in toy examples, the model performance degrades heavily as the number of features increases, even for a relatively small amount of features such as 20.\n* The Lime and L2X models used for benchmark comparison are relatively old models. It would be good to compare with more recent models if possible (although I am not an expert in this area).\n* There are some practical issues setting the non-selected input states to 0, as mentioned in Section 3.3. Rather than setting states to 0, it would be better to somehow not included non-selected input states in the prediction at all. But it's not clear how to do this for certain models like ConvNets."
            },
            "questions": {
                "value": "* Are there more recent benchmarks for comparison?\n* Is there a more elegant solution for suppressing non-selected input states rather than setting them to 0?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5041/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5041/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5041/Reviewer_ZVtY"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5041/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833503677,
        "cdate": 1698833503677,
        "tmdate": 1700636851936,
        "mdate": 1700636851936,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ecZQjXleN1",
        "forum": "Fn655mJ4bv",
        "replyto": "Fn655mJ4bv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5041/Reviewer_gHPF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5041/Reviewer_gHPF"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel approach to provide explainability of energy-based models in structured output prediction.  The  idea is to learn an interpreter network that predicts the k most important input variables for predicting a single output. Therefore the first contribution is to define a loss function that allows to learn such a model with the notable difficulty that the network to explain is a black-box.  The second contribution is the way the interpreter module is learned in practice since there is a non differentiability involved here. As for the architecture, the authors use a deep neural network followed by a Gumbel-Softmax unit: the re-parametrization trick allows to avoid direct sampling for the output of the neural network and replace it by a continuous approximation.\nThe method is showcaesd on a toy dataset and and real-world datasets in image segmentation and multilabel-classification of texts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strengths: \nOverall, the paper is well written and reads easily. \nThis work presents one of the first approaches to post-hoc interpretation of structured output prediction. The proposed approach applies when the output to be predicted is a binary vector which includes a broad variety of tasks like multi-label classification, semantic segmentation... Any structured output method that predicts a bag of items for instance (bag of substructure) will be also eligible (even if not considered in this work, except for text).\nThe optimization problem solved to learn the interpreter is appealing with a nice way to rely on the difference of energies associated to the perturbation of inputs by the interpreter. This is really the strong novelty of the paper, for me far beyond its application to structured output prediction."
            },
            "weaknesses": {
                "value": "*The paper takes a specific angle to intepretability of structured output prediciton, by considering the input features as tabular data. When dealing with images at least, identifying \"independently\" the important pixels involved in the prediction is not what I expect from explainability.  I would be interesting for raw data like images by identifying a region in the image or a concept as a function of the input space as an explanation. I think a discussion here is expected. \n\n*The learning algorithm is not sufficiently well documented and I have questions about its robustness against the choice of hyperpameter : is it robust to tau ? how does the learning algorithm react if we change k ? do we obtain close results if change k by k-1 or k+1 for instance ? What the impact of these parameters on the final \"explainability\""
            },
            "questions": {
                "value": "Please see questions above as well.\n\n1) Behaviour of the learning algorithm (see previous remark).\nI would like to have more insights about the behaviour of the learning algorithm - I would like to see a study about the robustness of leanring when varying k. \n\n2)   Is it possible to incorporate in SOInter a way to encourage the identification of correlated input features for instance by taking into account the relationship between features ?\n3) Did you study the robustness of your approach against noise in test images ?\n4) on text multi-label classification you proposed as an evaluation metrics the post-hoc F1 and the relative F1\nPlease re-formulate more clearly the relative F1 (I think there is a typo).\nThe deceptive results may be due to the nature of  input text representation: even the words that are not considered as important by the interpreter can help to give some context and improve the performance. Can you comment on that ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5041/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699289772826,
        "cdate": 1699289772826,
        "tmdate": 1699636493572,
        "mdate": 1699636493572,
        "license": "CC BY 4.0",
        "version": 2
    }
]