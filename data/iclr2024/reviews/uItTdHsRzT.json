[
    {
        "id": "WSzPYgKXGl",
        "forum": "uItTdHsRzT",
        "replyto": "uItTdHsRzT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2364/Reviewer_AERq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2364/Reviewer_AERq"
        ],
        "content": {
            "summary": {
                "value": "To protect the intellectual property of deep learning models, previous techniques commonly embed secret information within the model. Copyright owners can later extract this embedded information using specialized trigger inputs for verification. However, these methods are often vulnerable to removal attacks, such as model fine-tuning and pruning. To address this shortcoming, the authors propose a method that deeply couples the watermark with the model's functionalities. Under this scheme, attempting to remove the watermark will inevitably degrade the model's performance on normal inputs, rendering the model unusable and thus protecting the model. To achieve this goal, the authors introduce two techniques for generating watermark triggers\u2014namely, the Direct Feature-Fusion Method and the Invisible Feature-Fusion Method\u2014to combat the 'forgetting phenomenon' commonly observed with model retraining. Additionally, to deepen the coupling relationship, the authors propose a training method that randomly masks model weights, aiming to spread the watermark information throughout the entire model. Empirical evidence shows that their method offers better protection performance compared to other existing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors focus on the issue of Intellectual Property (IP) protection for deep learning models and propose two novel feature-fusion methods to mitigate the impact of removal attacks. By employing a random masking strategy, they further promote the spread of watermark information within the network. The efficacy of their approach is validated through experiments.\n\n2. The proposed function-coupled watermarking concept for DNN IP protection is simple yet effective. The authors demonstrate the superiority of their method over existing ones in preserving benign accuracy under multiple datasets and various models. Additionally, the method shows robustness against multiple removal techniques such as fine-tuning, transfer learning, pruning, and overwriting, and the corresponding experimental data is thoroughly and reasonably explained."
            },
            "weaknesses": {
                "value": "1. In Section 3.1 on FEATURE-FUSION DESIGN part, the authors claim that their approach differs from previous trigger-pattern-based watermarking methods, which introduce out-of-distribution features. However, based on the visual results in Figure 1, the first method, DIRECT FEATURE-FUSION METHOD, appears easily detectable by the human eye as differing from the original dataset. The second method, INVISIBLE FEATURE-FUSION METHOD, although more covert, resembles techniques adapted in some black-box adversarial attacks (e.g., [1]SurFree, [2]f-mix), and adversarial examples are generally considered to be out-of-distribution (OOD) data. Therefore, authors need to provide more specific empirical evidence to substantiate the uniqueness of their approach.\n   \n2. In Section 3.1.1 Direct Feature-Fusion Method, the mathematical expression used by the authors for the target class set is [*], which is unconventional. Typically, sets are denoted by {*}. Also, their mathematical notation for \"excluding i and j\" is also not concise and formalized. A more appropriate representation might be t \u2208 {1, ..., N_c} \\ {i, j}.\n\n3. In Section 3.2 Masking During the Training Phase, the authors also discuss the connection and differences between their random masking training approach and Dropconnect and Dropout. While their method offers a more flexible manipulation on model weights, they do not provide comparative experiments to show whether random masking is indeed superior to Dropconnect and Dropout.\n\n4. In Section 4.3 Robustness, the authors need to further clarify the selection process for fine-tuning data. They need to specify whether this data is a subset of the original training dataset, or if it is drawn from a separate dataset that was deliberately kept apart from the initial training process. The origin of the fine-tuning data may affect the outcomes of the Robustness experiments.\n\n5. In Section 4.4 Adaptive Attacks and False Positive Exploration, the authors utilize a deep learning-based detector to verify the robustness of the two types of fusion watermarks. They demonstrated that deep learning-based detectors are ineffective against invisible-fusion watermarks. I am curious whether OOD detection methods ([3]LID) could identify these invisible-fusion watermarks. If they cannot, this may answer my worries about the first weakness.\n\n[1] Maho T, Furon T, Le Merrer E. Surfree: a fast surrogate-free black-box attack[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 10430-10439.\n\n[2] Li X C, Zhang X Y, Yin F, et al. Decision-based adversarial attack with frequency mixup[J]. IEEE Transactions on Information Forensics and Security, 2022, 17: 1038-1052.\n\n[3] Ma X, Li B, Wang Y, et al. Characterizing adversarial subspaces using local intrinsic dimensionality[J]. arXiv preprint arXiv:1801.02613, 2018."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2364/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2364/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2364/Reviewer_AERq"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2364/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698084558761,
        "cdate": 1698084558761,
        "tmdate": 1699636169043,
        "mdate": 1699636169043,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E4BEFhHyQg",
        "forum": "uItTdHsRzT",
        "replyto": "uItTdHsRzT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2364/Reviewer_9Gj7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2364/Reviewer_9Gj7"
        ],
        "content": {
            "summary": {
                "value": "The submission proposes a backdoor-based solution for model watermarking which constructs watermark triggers with in-distribution training data. Applying a random masking training strategy on model weights, the watermark injection method improves the resistance against watermark removal attacks such as fine-tuning, transfer learning and weight pruning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Different from previous trigger patterns, which are generated with out-of-distribution samples of the training dataset, the submission proposes to combine in-distribution images as watermark triggers, i.e., the feature fusion methods. The coupling of model watermark and model functionalities improves the robustness against fine-tuning based attacks.\n+ The random masking strategy generalizes the watermark function to different neurons of the model and further enhances the defensive ability, which may be adopted by other watermarking schemes."
            },
            "weaknesses": {
                "value": "+  **Weak threat model**: As it is generally considered that *model extraction is the de facto strongest attack for diminishing the watermark* [1] and much effort has been invested to enhance the robustness of black-box watermarking schemes against such an attack [1, Jia et al.(2021)], the submission should involve model extraction attacks in the adversarial scenario. Otherwise, simply demonstrating the resistance against fine-tuning-based and pruning-based attacks is not convincing enough for the comparison with baseline methods and for the realistic applications. \n+ **About watermark detection**: The adaptive detection of watermark images is conducted by training a classifier to distinguish the trigger images. Such an adaptive attack seems insufficient to demonstrate the robustness against the pre-processing attacks. As the assumption about the attacker's capability is strong that the original training dataset and watermarking scheme could be utilized to perform detection, the attacker is motivated to simply compare the input images with the original training samples in pixel-level difference to conduct filtering. Besides, even though the attacker cannot obtain the original training dataset, he/she may still adopt the outlier detectors (as illustrated in Jia et al.(2021)) to identify trigger samples, which is more practical and efficient under  realistic scenarios.\n\n[1] Kim B, Lee S, Lee S, et al. Margin-based Neural Network Watermarking. ICML 2023."
            },
            "questions": {
                "value": "Though constructed with in-distribution data, the proposed watermark triggers still function as outliers to inject watermark information, thus the robustness against adaptive attacks and detection methods like model extraction, outlier detection and pre-processing should be considered and demonstrated."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2364/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698112200095,
        "cdate": 1698112200095,
        "tmdate": 1699636168956,
        "mdate": 1699636168956,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hUeSAHoh15",
        "forum": "uItTdHsRzT",
        "replyto": "uItTdHsRzT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2364/Reviewer_DRqs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2364/Reviewer_DRqs"
        ],
        "content": {
            "summary": {
                "value": "This article introduces a novel DNN watermark that enhances robustness against watermark removal attack by coupling the watermark and model functionality. The feature-fusion trigger samples and a weight-masking approach are employed to embed the watermarks. Unlike previous approaches that depended on concealed features acquired from out-of-distribution data, this article leverages features acquired from in-distribution data. The experimental results shown in this paper seem considerable."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method conceptually makes sense.\n\n2. The experimental results shown in this paper seem considerable."
            },
            "weaknesses": {
                "value": "1. The technical part of the paper is weak. The overall algorithmic pipeline appears to be rather na\u00efve, with the generation of trigger samples relying solely on the concatenation or weighted overlay of two training set images. Furthermore, the paper lacks a theoretical explanation that justifies the proposed method.\n\n2. The experiments are insufficient. The authors mentioned that their proposed invisible feature-fusion strategy could evade visual detection but did not give relevant experimental results."
            },
            "questions": {
                "value": "1.\tIn Section 3.1.2, \u201cwe propose an invisible feature-fusion strategy that avoids visual detection by auditors.\u201d However, the paper does not provide experimental evidence to support the claim that the proposed method can evade visual detection. It is recommended that the authors include the Peak Signal-to-Noise Ratio (PSNR) of trigger samples as evidence of their invisibility. Furthermore, how should the value of $r$ be chosen to ensure the invisibility of triggers?\n\n2.\tPlease explain how the utilization of samples from the original training set for watermark trigger generation enhances resistance against watermark removal attacks.\n\n3.\tIn Section 3.2, the paper mentions, \u201cby using random masking, we can distribute the watermark function equally to each neuro\u201d. Is there any experimental or theoretical evidence to support this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2364/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2364/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2364/Reviewer_DRqs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2364/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698651523155,
        "cdate": 1698651523155,
        "tmdate": 1699636168877,
        "mdate": 1699636168877,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FgDQSKN2Kk",
        "forum": "uItTdHsRzT",
        "replyto": "uItTdHsRzT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2364/Reviewer_NZHm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2364/Reviewer_NZHm"
        ],
        "content": {
            "summary": {
                "value": "This paper presents the new function-coupled watermarks for DNNs, by leveraging the in-dist training data, not out-of-dist. To achieve function coupling, they introduce a new training strategy with a random mask and demonstrate the effectiveness of their approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "They did a great job for defining the watermarking problem and well summarized the key prior research. \n\nAlso, the proposed method appears to be simple, yet performing well across extensive experiments. \n\nThey introduce a new training strategy: feature-fusion and a joint training approach as fuse them as watermark triggers and randomly masking the model weights during training that spreads the embedded watermark throughout the network.\n\nTherefore, it aims to strengthen the resistance of watermarks to common removal assaults, such as pruning and fine-tuning."
            },
            "weaknesses": {
                "value": "There are a lot of typos, inconsistency and a few incomplete sentences in the paper. Also, citation format is wrong throughout the paper missing parenthesis: \u201c(\u201c Li et al. \u201c)\u201d, which hinders the readability of the paper. \n\n\nThis can be a small or big problem. In the provided code, there are comments written in Chinese, allowing this paper is written by Chinese. This can possibly violate the anonymity of requirement. \n\n   # \u5c06\u6570\u636e\u96c6\u539f\u672c\u7684\u6807\u7b7e\u4e0e\u8bad\u7ec3\u4e2d\u9700\u8981\u7684\u7c7b\u7f16\u53f7\u76f8\u4e92\u5173\u8054\u3002\n # \u5c06\u6240\u6709\u7684\u9884\u6d4b\u7ed3\u679c\u653e\u7f6e\u5230\u540c\u4e00\u4e2alist\u4e2d\n\nand many more\u2026\n\nThe proposed random masking and direct/invisible feature-fusion on in-dist data sounds a bit simple, and would be good to provide more sound and theoretical foundation to support the proposed simple method works well."
            },
            "questions": {
                "value": "Please address the weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Code comments written in Chinese is not that great for anonymity."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2364/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699434881388,
        "cdate": 1699434881388,
        "tmdate": 1699636168811,
        "mdate": 1699636168811,
        "license": "CC BY 4.0",
        "version": 2
    }
]