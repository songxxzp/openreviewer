[
    {
        "id": "obs1F2TGgJ",
        "forum": "qL6brrBDk2",
        "replyto": "qL6brrBDk2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3847/Reviewer_NKYa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3847/Reviewer_NKYa"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the training of high-performance neural networks on small amounts of training data by optimizing data augmentation. Existing methods in this field optimize the transformation itself in the feature space, which limits the available data augmentation transformations and has high computational complexity. This paper differs from these existing approaches by optimizing the importance weights of the input features and the soft labels to be assigned to the augmented data, thereby meta-learning the data augmentation available in the existing data augmentation pipeline. Focusing on features and labels in data augmentation has not received much attention, and the idea is novel. Furthermore, the paper proposes a method to approximate the bi-level optimization of meta-learning with validation data to a single-level optimization by borrowing the idea of gradient matching and developing an efficient algorithm. Experiments verify the effectiveness of the proposed method on various datasets, tasks, and combinations of data augmentation methods. On the other hand, the paper does not provide an evaluation of the first-order approximation or the computation cost, and there is room for improvement in this aspect."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper proposes a novel data augmentation optimization strategy that optimizes the importance and labels of input features. The idea is very interesting and original.\n+ The paper proposes a first-order approximation method to efficiently compute meta-learning that requires bi-level optimization.\n+ The paper applies and evaluates the proposed method not only on image datasets but also on table datasets. This evaluation is important in supporting the paper's claim that the method can be applied to any data augmentation pipeline.\n+ The paper provides experimental results on the recently widely used CLIP pre-trained models, effectively demonstrating the impact of the proposed method."
            },
            "weaknesses": {
                "value": "- Even though the paper proposes a first-order approximation method, it does not provide an evaluation of this method. In other words, the paper should provide a performance comparison with the usual bi-level optimization and a computation cost comparison with other data augmentation strategies such as Fast AutoAugment.\n- The writing of the paper is not necessarily of high quality. For example, Theorem 1 is very difficult to read because it contains multiple claims that make up the entire solution. Theorems and corollaries should be split for each claim, or if the propositions are ambiguous, they should be replaced with detailed explanations for each component, rather than in the form of a theorem. In fact, the proofs provided by the Appendix are almost obvious and make little theoretical contribution."
            },
            "questions": {
                "value": "- Do you think the proposed method can be applied to consistency-based semi-supervised learning with data augmentation, e.g., FixMatch [a]? The study of estimating the importance of samples and labels is well studied in the field of semi-supervised learning rather than in the field of data augmentation (e.g., FreeMatch [b]). If it can be shown that the scheme of the proposed method can be implemented in semi-supervised learning, the impact of this paper on the community will be even greater.\n\n[a] Sohn, Kihyuk, et al. \"Fixmatch: Simplifying semi-supervised learning with consistency and confidence.\" NeurIPS 2020.\n\n[b] Wang, Yidong, et al. \"Freematch: Self-adaptive thresholding for semi-supervised learning.\" ICLR 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3847/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698386087732,
        "cdate": 1698386087732,
        "tmdate": 1699636342846,
        "mdate": 1699636342846,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0EtBJmJ3ed",
        "forum": "qL6brrBDk2",
        "replyto": "qL6brrBDk2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3847/Reviewer_PN9Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3847/Reviewer_PN9Y"
        ],
        "content": {
            "summary": {
                "value": "This article contributes a workflow named as SAFLEX as data augmentation. Here is a summry:\n\n(1) Authors unveil a novel parametrization for learnable augmentation complemented by an adept bilevel\nalgorithm primed for online optimization.\n(2) Author's SAFLEX method is distinguished by its universal compatibility, allowing it to be effortlessly\nincorporated into a plethora of supervised learning processes and to collaborate seamlessly with an\nextensive array of upstream augmentation procedures.\n(3) The potency of authors' approach is corroborated by empirical tests on a diverse spectrum of datasets\nand tasks, all underscoring SAFLEX\u2019s efficiency and versatility, boosting performance by1.2% on\naverage over all experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "They have considered experiments of different data types and model training as downstream tasks, which demonstrate their workflow as a robust one."
            },
            "weaknesses": {
                "value": "From a model perspective, this is a good one as topic of adaptive learning, though a  little bit off the topic of this conference. \nFrom data augmentation perspective, it is better to demo some more experiments in downstream task involves with high dimensional data."
            },
            "questions": {
                "value": "Is there any empirical experiments that SAFLEX can contribute to some other applicable downstream task like model training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3847/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698642834749,
        "cdate": 1698642834749,
        "tmdate": 1699636342720,
        "mdate": 1699636342720,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rQpvcuWHjB",
        "forum": "qL6brrBDk2",
        "replyto": "qL6brrBDk2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3847/Reviewer_GM6j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3847/Reviewer_GM6j"
        ],
        "content": {
            "summary": {
                "value": "The paper argues that data augmentations can suffer with two main issues - 1. The augmented samples can become out of distribution to the training distribution and 2) the augmented samples can belong to a different class than the original sample. To tackle the first issue, the authors propose to add sample weights (w_i) to the augmented samples. Samples which are farther from the training distribution can be assigned a smaller weight. To tackle the second issue, the authors propose to make the one-hot label as soft-label to capture the uncertainties. \nTo learn the sample weights and the soft-label the authors pose a bi-level optimization problem where in the inner loop, the model parameters are optimized over the training and augmented samples and in the outer loop the optimal augmentation parameters are optimized for. \n\nThe authors conduct experiments across three settings - 1. medical datasets, 2. tabular datasets and 3. for contrastive learning approaches. Across all the experiments the authors show improved performance on top of standard augmentations such as RandAug, Mixup and CutMix."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation in the paper about identifying the two issues with standard augmentation and then solving it by learning sample weights and soft-labels is really clear."
            },
            "weaknesses": {
                "value": "1. The main issue is a lack of proper baselines. Papers such as [1] have already explored using soft labels for augmentations where the softness is derived on the basis of augmentation strength. This paper's novelty thus gets limited. There is no comparison with [1] in any of the experiments. The authors should do a proper comparison with [1] and justify how their approach is better than it. \n\n2. To solidify the experimental results the authors should also experiment with stronger architectures and datasets such as ResNet-101 over ImageNet as done in [1].\n\nI am willing to update my ratings if my concerns are addressed. \n\nReferences - \n\n1. Soft Augmentation for Image Classification. Liu et al. https://arxiv.org/pdf/2211.04625.pdf"
            },
            "questions": {
                "value": "I have already mentioned it in the weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3847/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799107058,
        "cdate": 1698799107058,
        "tmdate": 1699636342616,
        "mdate": 1699636342616,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NjAubkI7qx",
        "forum": "qL6brrBDk2",
        "replyto": "qL6brrBDk2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3847/Reviewer_EBBs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3847/Reviewer_EBBs"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a principled method for data augmentation. To this end, the paper presents a bilevel optimization framework for weighing and soft-labelling the augmented data in order to compensate for the adverse generalization effects of weak, strong and sometimes meaningless augmented examples. Although the impact of data augmentation for generalization, in particular deep learning frameworks, has been substantial, there is still a lack of principled ways of doing data augmentation. This paper has identified this gap and convincingly addressed the problem."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper is well-written and easy to understand. \n\nThe diagrams and the equations are easy to follow.\n\nThe experiments are performed on diverse datasets with various tasks, including medical imaging and tabular data.\n\nThe results are highly encouraging."
            },
            "weaknesses": {
                "value": "A few important previous works on sampling and purifying GAN synthetic data are relevant to this paper.  It is important to acknowledge and discuss their contributions in the paper. \n\nCaramalau, Razvan, Binod Bhattarai, and Tae-Kyun Kim. \"Sequential graph convolutional network for active learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\nBhattarai, Binod, et al. \"Sampling strategies for gan synthetic data.\" ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020."
            },
            "questions": {
                "value": "I like the paper. Please see a few comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3847/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699042537294,
        "cdate": 1699042537294,
        "tmdate": 1699636342531,
        "mdate": 1699636342531,
        "license": "CC BY 4.0",
        "version": 2
    }
]