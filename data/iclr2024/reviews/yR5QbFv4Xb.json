[
    {
        "id": "rEJuBlF7th",
        "forum": "yR5QbFv4Xb",
        "replyto": "yR5QbFv4Xb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4641/Reviewer_Ff8r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4641/Reviewer_Ff8r"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method for training an additive neural network, where the final prediction is the summation of learned contributions for each feature. The contribution for each feature is also learned on the fly, and is trained to approximate the Shapley value of each feature. On several tabular datasets, the authors show that their method is capable of attaining high performance while also learning Shapley values (i.e. contributions) for each feature which are meaningful."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "### Interesting idea to have the model learn Shapley values on its own\n\nThe core concept is unique and also very interesting. Shapley values can be somewhat difficult to estimate, and it is intriguing to have them be learned from the data during training, within the framework of an interpretable additive model.\n\n### Good mix of different datasets and benchmarks\n\nThe authors did a good job of using different tabular datasets and benchmarking across different methods that also provide interpretable features, as well as non-interpretable methods like a simple MLP."
            },
            "weaknesses": {
                "value": "### Partial marginal contribution scores might not be meaningful\n\nI have my doubts as to how meaningful the learned marginal contribution scores from $\\Delta$ can be in general. The goal is to have $\\Delta$ for a single feature $i$ match its true Shapley score, but in the absence of true Shapley labels, the network is regularized to satisfy the following: 1) the summation of the $\\Delta$ values over all features $i$ should be the label $y$ (Equation 3); 2) for any subset of features $\\mathcal{S}$ which include $i$, different permutations of $\\Delta$ on that subset should be the same value (Equation 4); and 3) partial sums of $\\Delta$ values should also equal the label $y$ (Equation 5). As the authors point out, while the first two conditions are sufficient to ensure the model learns permutation-invariant contribution scores which add to the final label $y$, they alone are not sufficient to ensure that the contribution scores for each feature are meaningful (e.g. the feature contributions can be 0 for every feature except the last one). Condition 3 is meant to fix this, but it is not clear how that is done.\n\nIn particular, condition 3 (Equation 5) seems like it trains the network to make sure that the partial sums of contribution scores (i.e. for any subset of feature $\\mathcal{S}$) to be equal to the label $y$ _as if_ it had the full set of features. For example, consider a 2-feature dataset where feature 1 does not matter and the true label is simply equal to feature 2. Then in Equation 5, $L_v$ on just feature 1 would try and make the importance of feature 1 match the label, but in reality the contribution of feature 1 should just be 0.\n\n### SASANet addresses traditional Shapley challenges in a potentially inefficient way\n\nThe traditional challenges with computing Shapley values are: 1) how to tractably estimate the marginal contribution of feature $i$ when the number of subsets of features is exponential; and 2) given a subset of features, how to obtain a value/prediction when most models are not capable of handling partial inputs.\n\nSASANet addresses these challenges by effectively training over many subsets, thereby forcing the model to be robust to fewer features, across different subsets. This might lead to significant inefficiencies in training, given the possible subsets and orderings, especially as the number of features becomes larger. The time analysis in section 4.4 is appreciated, but that seems to only include the time taken to compute the feature importances _post hoc_. How does the training time of SASANet compare to other methods?\n\n### SASANet is only applied to small tabular datasets\n\nIn this work, SASANet is only being applied to a few small tabular datasets. Especially given the potential brittleness of the marginal contributions and the potential inefficiencies (as described above), it is possible that this method does not translate well to larger tabular datasets (e.g. gene expressions) or non-tabular datasets (e.g. images, text).\n\n### Minor typos\n\n- Shapley is spelled Shapely in several places\n- Section 3.2, Specially \u2192 Specifically\n- Typo in loss equation in 3.5"
            },
            "questions": {
                "value": "In the analysis of the accuracy of Shapley values, how were ground truth values obtained for RMSE?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698685636400,
        "cdate": 1698685636400,
        "tmdate": 1699636444050,
        "mdate": 1699636444050,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YS5HjUlOs9",
        "forum": "yR5QbFv4Xb",
        "replyto": "yR5QbFv4Xb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4641/Reviewer_2AVx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4641/Reviewer_2AVx"
        ],
        "content": {
            "summary": {
                "value": "This study proposes a new self-interpretable network based on Shapley values. Based on the permutation-based definition, the authors explicitly use a model to learn the marginal contribution of each input feature in each specific permutation. Then, they further learn a model to predict Shapley values of each feature based on its marginal contributions in different permutations. The task labels are used as additional supervision to ensure the performance of the network."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The network in this paper uses the multi-head attention layers to take inputs of varying lengths, which avoids problems with masking features or formulating the distribution of features.\n\nThe proposed method achieves similar performance to black-box models on tabular datasets. Besides, the authors evaluate the quality of attributions generated by the proposed network in different aspects, which demonstrate the reliability of the attribution."
            },
            "weaknesses": {
                "value": "- The proposed network is not sufficiently interpretable, because the internal modules are still black boxes. This network makes predictions while automatically providing Shapley values of each input feature. However, the modules for computing marginal contributions and Shapley values are constructed as multi-head attention layers and learned by back-propagation. Therefore, the internal computation is still unexplainable. In comparison, the SHAPNet (Wang et al., 2021) makes all the layer-wise propagation of features in the network interpretable.\n- The loss function in equation (5) is confusing. It seems that the right-hand formula in equation (5) equals $L_m(x_S,y,O_S)$, because this term is independent of $(x\u2019,y\u2019)$. I check the proof in the appendix and I guess there is a typo: it should be $L_m(x_S,y\u2019,O_S)$. In this way, $\\sigma(f_c)$ is forced to model the distribution $p(y|x_S)$.\n- The presentation of the paper needs improvement. The authors introduce the function of each module but do not provide an overview of the whole framework. Thus, when I read it the first time, it takes much time to understand the whole network. Besides, I suggest the authors clarify the models to be learned/optimized in each loss function. Otherwise, it may be confusing that whether equation (3) optimizes $\\Delta(\\cdot,\\cdot;\\theta_\\Delta)$ or $f_c$. \n- The scalability of the proposed method to complex tasks is questionable. First, the training and testing of the model need various permutations of input features, which leads to a huge computational cost. Second, the authors do not conduct experiments on language or image data. I wonder whether the performance of the proposed method on more complex tasks can still match the performance of black-box models, especially ResNets or transformers, which are more widely used than MLPs in applications.\n- What is the computational complexity in the inference stage? Does it only use the Shapley module for inference? If yes, then what is the benefit of learning an additional marginal contribution module? If no, then the complexity in the inference stage is still $2^n$."
            },
            "questions": {
                "value": "- Why are you learning a model to predict the marginal contribution $\\Delta$ instead of directly predicting $f_c(x_S)$, which is used in KernelSHAP and Frye et al., (2019)?\n\n(Frye et al., 2019) Shapley explainability on the data manifold. In ICLR 2021.\n \n- Typos: Section 3.5: $O_k,k$,$O_K$, $k=1$, and $O_i$ are all subscripts."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698746369739,
        "cdate": 1698746369739,
        "tmdate": 1699636443962,
        "mdate": 1699636443962,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WGYoKhlOBn",
        "forum": "yR5QbFv4Xb",
        "replyto": "yR5QbFv4Xb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4641/Reviewer_JuUA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4641/Reviewer_JuUA"
        ],
        "content": {
            "summary": {
                "value": "The article presents a new self-interpreting approach called SASANet, having in mind to incorporate Shapley values into the additive self-attribution literature. The approach is compared to black-box approaches and empirically demonstrates their usefulness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The approach seems theoretically well-grounded; the literature review seems thorough and complete."
            },
            "weaknesses": {
                "value": "**Major**\n\nThe only points I would like to raise concern the numerical experiments.\n\n1 \u2013 The comparison to other self-attribution methods in Table 1 is convincing, but I think other conclusions drawn from the experiment reported in Table 1 are misleading.\n\n1.1 - I don\u2019t understand why comparing SASANet to interpretable approaches, more specifically to a linear regressor and a single decision tree. Those simple methods are sure to obtain somewhat deceiving performances when compared to a huge neural network, where having some black-box architecture is not a problem. Inherently interpretable predictors have advantages of their own (being fully transparent, for example) that can\u2019t be simply quantitatively compared to others, and thus shouldn\u2019t be compared solely in terms of performances. When it comes to interpretable machine learning, there is a huge literature on that matter, especially in an era where interpretability has been put forward for various reasons, so simple linear regressor and decision tree might just lead to underestimating the potential of interpretable approaches and overestimate the power of SASANet.\n\n1.2 \u2013 I don\u2019t feel like the comparison to black-box models is fair. Was the MLP of reasonable size? No detail is shared on that matter. Comparing an MLP to a richer and more complex architecture involving transformers is questionable. Also, LightGBM dates a bit (2017); the goal of those experiments is to compare SASANet to state-of-the-art approaches when it comes solely to performances (i.e. black-boxes) but I don\u2019t feel like those baselines are sufficient.\n\n2.1 \u2013 Table 2: I\u2019m not sure about how to interpret this table. Why is it that having the worst performances is sought? I understand that since the top-5 relevant features were removed, but why not therefore look at the relative variation in performances?\n\n2.2 - Fidelity experiments: It is stated in the abstract that \u00ab\u00a0SASANet is shown more precise and efficient than post-hoc methods in interpreting its own predictions\u00a0\u00bb. I don\u2019t feel like this has been demonstrated in any way. The protocol for this demonstration is explained as follows: \u00ab\u00a0We observed prediction performance after masking the top 1-5 features attributed by SASANet for each test sample and compared it to the outcome when using KernelSHAP, a popular post-hoc method, and FastSHAP, a recent parametric post-hoc method. The results are shown in Table 7.\u00a0\u00bb And the conclusion that is drawn is the following: \u00ab\u00a0SASANet\u2019s feature masking leads to the most significant drop in performance, indicating its self-attribution is more faithful than post-hoc methods.\u00a0\u00bb There is a logical gap here. Much information remains unknown: Were the features considered by the approaches correlated (to those in the top-5) in any way? Were the importance given by each approach to their top-5 approximately the same? Why not remove, for example, the features accounting for the first 20% importance? Also, Shapley values measure the difference in prediction, not the difference in performance. Therefore, a feature could have a great impact on the predictions while not affecting the performances at all (or smaller than expected). E.g. having an error of -x instead of x, thus a same squared error. Considering The protocol is simply insufficient to claim that \u00ab\u00a0SASANet is shown more precise and efficient than post-hoc methods in interpreting its own predictions\u00a0\u00bb.\n\n**Minor**\n\n1 \u2013 Typo in Theorem 3.4: \u00ab\u00a0with ample permutation\u00a0\u00bb.\n\n2 \u2013 At the beginning of section 4.2: \u00ab\u00a0Table 6 shows average scores from 10 tests; Appendix J lists standard deviations.\u00a0\u00bb; Table 1 should be named, not Table 6. The same thing occurs later on: Table 7 is mentioned while referring to Table 2. Otherwise, Tables 1 and 2 aren\u2019t referred to anywhere in the article."
            },
            "questions": {
                "value": "1 \u2013 When it comes to feature attribution approaches, the time is reported in Table 3, but how does the training time of SASANet compare to others?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The 9-page limit is exceeded."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4641/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4641/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4641/Reviewer_JuUA"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801919527,
        "cdate": 1698801919527,
        "tmdate": 1699636443884,
        "mdate": 1699636443884,
        "license": "CC BY 4.0",
        "version": 2
    }
]