[
    {
        "id": "Ce2b5iDPFe",
        "forum": "kvByNnMERu",
        "replyto": "kvByNnMERu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7957/Reviewer_emQE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7957/Reviewer_emQE"
        ],
        "content": {
            "summary": {
                "value": "This paper extends some works from shape analysis into the space of neural networks. That is, how does one measure the distance between two NNs where a NN is represented as a mapping h:features ->R^N. The main idea is to think of this mapping into the space of point clouds and hence one can use a Kendall Shape space type distance. The key difference contribution I see is the fact the point clouds have some stochastic properties to them so the shape distance is measuring the distance between expectations. The authors spend a great deal with considering the bounds of these distances."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "A key component of this paper is the bounds on the distances when using an estimator for the cross covariance estimator. The authors present Lemma 1, 2 and Theorem 1 to do this (and theorem 2). The authors explain this bound well as it makes sense that the higher the dimension, the more landmarks need to be observed. \n\nThe authors show how under a variety of scenarios their estimator is performing. The variety is suitable."
            },
            "weaknesses": {
                "value": "When referencing the shape distances such as (2) and (3) the authors cite Williams 2021 but these are simply common shape distances which have been around for much longer. Unless this specific formulation is novel to Williams 2021 I don't see how it's different than just measuring the distance on a circle.\n\nWhen considering the \"failure modes of plug-in...\" there is the missing component of mentioning scale. If h_i differ in scale their $\\rho$ distance can be large but their $\\theta$ distance can be very small as it rescales in the denominator.\n\nSome of the figure axis are impossible to read."
            },
            "questions": {
                "value": "When setting up the background, the authors define $h_i:\\mathcal{Z}\\rightarrow \\mathbb{R}^n$ as a function representing each neuron. I have no problem with this definition. My question is, why do the authors limit their writing to that of NNs while this is true for a much wider class of functions. I may have missed something in an assumption but it seems these functions can be quite generally considered.\n\nFor (5) isn't the last equation enough?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7957/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7957/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7957/Reviewer_emQE"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7957/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698431335108,
        "cdate": 1698431335108,
        "tmdate": 1699636978275,
        "mdate": 1699636978275,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ENPtYDfxR4",
        "forum": "kvByNnMERu",
        "replyto": "kvByNnMERu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7957/Reviewer_GC8w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7957/Reviewer_GC8w"
        ],
        "content": {
            "summary": {
                "value": "The paper theoretically quantifies the performance of some typical shape distances between neural representations and the dependence of the performance with respect to the number of samples. The authors find that typical methods have low variance but high variance, and instead propose a new method that enables a tunable bias-variance tradeoff."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper contains significant insight into the performance of 'plug-in' estimators of shape distance between two neural representations and the performance dependence on the number of samples and the dimensionality of the ambient space.\n- The paper identifies the key component behind the non-ideal performance of these estimators (the $||\\Sigma_{ij}||_\\ast$ term) and proposes a simple but intuitive and effective estimator to allow a tunable bias-variance tradeoff.\n- There are adequate experiments on synthetic data to validate this new estimator."
            },
            "weaknesses": {
                "value": "- The paper should contain some more intuition-building sentences so that the mathematical formulation is more easily digestible. However, the authors have done a really nice job of making the mathematical foundations and derivations themselves clear.\n- One small experiment with VAEs (even if it is confined to the appendix) might be useful for quantifying the performance of stochastic networks."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7957/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7957/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7957/Reviewer_GC8w"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7957/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698673765157,
        "cdate": 1698673765157,
        "tmdate": 1699636978123,
        "mdate": 1699636978123,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "D2Ea5I7tdK",
        "forum": "kvByNnMERu",
        "replyto": "kvByNnMERu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7957/Reviewer_sJ3L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7957/Reviewer_sJ3L"
        ],
        "content": {
            "summary": {
                "value": "In this paper the authors study the estimators of the so-called \"shape distance\", more precisely Procrustes size-and-shape distance $\\rho$, and Riemannian shape distance $\\theta$., that are distances defined on data manifold. They measure the uncertainty (bias and variance) of the empirical estimates of these distances, relying on centration inequalities, and design a new estimator whose bias/variaqnce tradeoff can be controlled. Their theoretical results are illustrated on:\n* a synthetic experiment\n* measure of calcium in the neural activity of a mouse"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "### Clarity\n\nThe paper is well written, and the and theoretical results are explained in a way that make it accessible to a broad range of readers. The proof techniques are well explained. \n\n### Quality\n\nExperiments are convincing and support"
            },
            "weaknesses": {
                "value": "### Out of scope: no link with neural networks\n\nThis work has nothing to do with artificial neural networks. In the statement of the problem, the authors assume that thew observations are $X=h_i(Z)$ with $Z$ the input data  and $h_i$ the neural network. No hypothesis is made on $h_i$ nor $Z$. The whole paper could be rewritten with $X$ to get rid of the assumption that the measures are the outputs of a neural network. Even Theorem 2 mentions \"neural\" for no good reason: authors only meant to talk about the existence of some r.v $X$.  \n\nThe only experiment linked with neural network is in Sec 4.2, and the \"neural data\" is actually *calcium measurements* from mouse primary visual cortex, which are typical tabular data.  \n\nOverall, a conference or journal focused on statistics seems to be a better match for the content of the article than ICLR.  \n\n### Novelty\n\nThe novelty is poor: all the proofs are classical bias-variance decomposition and concentration inequalities. \n\nMoreover, there exists previous work on the topic that are not covered in the literature review:   \n   \nKent, J.T. and Mardia, K.V., 1997. Consistency of Procrustes estimators. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 59(1), pp.281-290.\n  \nGoodall, C., 1991. Procrustes methods in the statistical analysis of shape. Journal of the Royal Statistical Society: Series B (Methodological), 53(2), pp.285-321."
            },
            "questions": {
                "value": "### Application to artificial neural networks\n\nWhich results your method yield in the latent space of an *artificial* neural network?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7957/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7957/Reviewer_sJ3L",
                    "ICLR.cc/2024/Conference/Submission7957/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7957/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699099403195,
        "cdate": 1699099403195,
        "tmdate": 1700736219512,
        "mdate": 1700736219512,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TaC8xlkSq7",
        "forum": "kvByNnMERu",
        "replyto": "kvByNnMERu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7957/Reviewer_ATPd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7957/Reviewer_ATPd"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel estimator for distances between neural representations from both biological and artificial neural networks. The key technical contribution is a new moment based estimator of the nuclear norm of the cross covariance matrix which has significantly lower bias than the plug in estimator. The authors provide both theoretical and empirical analysis to illustrate the benefits of the proposed estimator."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The problem is clearly identified and analysed theoretically, and the estimator proposed to solve it is principled and technically sound. The theoretical analysis in Section 3 is fairly detailed and appears to be novel. It clearly explains the flaws in the plugin estimator and the subsequent derivation of the new estimator is also well grounded in theory.\n\n2. Simulations and experiments match the intuition used to derive the estimator (Fig 2A) and clearly highlight the strengths of the estimator over the plug-in baseline."
            },
            "weaknesses": {
                "value": "1. The overall motivation of the problem is a bit weak. The experiments don't clearly illustrate the downstream benefits of this work. Further experiments showing tangible gains on at least one real world application would greatly strengthen the paper in my opinion.\n\n2. The main technical weakness of the approach appears to be its effect on the variance. While deriving the estimator the authors seem to primarily focus on reducing the bias compared to the plug-in estimator and acknowledge at the end of Section 3.3 that the bias is being bounded at the expense of the variance. This is also seen in the experiments (Fig 2) where the proposed estimator appears to have a significantly higher variance than the moment estimator. This raises questions about the reliability of the estimator, specifically it is not clear if it can reduce the mean squared error for all distributions. This is further illustrated in Fig 4 where on real neural data it appears that other than the case where similarity is set to 0 (A) the plug-in estimator performs comparable to (C, D) or better than (B) the proposed estimator."
            },
            "questions": {
                "value": "1. In the last equation on page 5, why is $x \\in [0,1]$?\n\n2. In Section 4.2 it is acknowledged that the proposed estimator is highly variable in low SNR regimes and so neurons with the highest SNR are selected. Can you provide the relative proportion of such neurons in the presented scenario and also (if possible) comment on their prevalence in general? This is important because if the relative proportion is small then it means that the proposed estimator can only be applied in very few cases."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7957/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699335138380,
        "cdate": 1699335138380,
        "tmdate": 1699636977897,
        "mdate": 1699636977897,
        "license": "CC BY 4.0",
        "version": 2
    }
]