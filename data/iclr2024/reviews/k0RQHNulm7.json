[
    {
        "id": "eVfN2Vag6Q",
        "forum": "k0RQHNulm7",
        "replyto": "k0RQHNulm7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1422/Reviewer_QE51"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1422/Reviewer_QE51"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework for cross-modality distillation, which aims to transfer knowledge from a source modality with rich information to a target modality with limited information. The framework leverages contrastive learning to exploit both positive and negative relationships in the paired data, and distills generalizable features for various downstream tasks. The paper also provides theoretical analysis and empirical results to support the effectiveness and versatility of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to follow.\n2. The paper provides theoretical analysis and empirical results to support the effectiveness and versatility of the proposed method across diverse modalities (e.g., images, sketches, depth maps, videos, and audio) and tasks (e.g., recognition and segmentation)."
            },
            "weaknesses": {
                "value": "1. While the paper reviews relevant literature on cross-modality distillation and contrastive learning, it omits references to recent works on multi-modal distillation, specifically [1-2], which employ an online distillation strategy different from the approach presented here. The authors are encouraged to provide a comprehensive comparison with these works. How does this paper compare with these works? Are there any advantages or disadvantages of using different distillation strategies and loss functions?  [1-2] appear closely related to this work, and it would be valuable to engage in a detailed discussion with these papers, especially regarding the learning of positive and negative relationships during distillation.\n2. I appreciate the theoretical results of this paper but the two distillation losses, CMD and CMC, appear somewhat simplistic.. The CMD loss is just the cross entropy loss and the CMC loss is exactly the CLIP loss. Moreover, the CMD loss can be seen as a within-modal regularizer of multi-modal learning, which has been used in [2-3]. Consequently, it seems that these losses have been adopted from the multi-modal learning community with minimal modification, potentially diminishing the novelty and significance of the proposed cross-modality contrastive distillation framework. What is the novelty and significance of the proposed framework? How does it differ from existing methods for multi-modal learning or cross-modality transfer?\n3. The paper neglects to discuss recent work such as LiT [4], which employs a locked image model in multi-modal tuning. The concept of the locked operator in LiT appears akin to cross-modality distillation in this paper. A detailed comparison between this paper and LiT, in terms of methodology and performance, is essential to elucidate the distinctions and commonalities between the two approaches."
            },
            "questions": {
                "value": "The main points to address in the rebuttal primarily stem from the \"weaknesses\" section we discussed earlier. Specifically, it would greatly benefit our understanding if the authors could provide a more extensive explanation of their method's contribution, ideally through a detailed comparison with the works below.\n\n[1] Align before Fuse: Vision and Language Representation Learning with Momentum Distillation, NeurIPS 2021\n\n[2] Graph Matching with Bi-level Noisy Correspondence, ICCV 2023\n\n[3] CrossCLR: Cross-modal Contrastive Learning For Multi-modal Video Representations\n\n[4] LiT: Zero-Shot Transfer with Locked-image text Tuning, CVPR 2022"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1422/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698241273647,
        "cdate": 1698241273647,
        "tmdate": 1699636070694,
        "mdate": 1699636070694,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cG3gJPFeBq",
        "forum": "k0RQHNulm7",
        "replyto": "k0RQHNulm7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1422/Reviewer_qhgx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1422/Reviewer_qhgx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a cross-modality distillation method with contrastive learning. Existing self-supervised methods leverage a few pairwise unlabeled data to distill the knowledge by aligning features or statistics between the source and target modalities.  The Cross-Modality Contrastive Distillation (CMCD) framework proposed in this paper considers the unpaired and unlabeled data in source and target modalities. The convergence analysis reveals that the distance between source and target modalities significantly impacts the test error on downstream tasks within the target modality which is also validated by the empirical results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper considers that mass multi-modality data is not paired in the real world. For memory and privacy-restricted scenarios where labeled training data is generally unavailable, this setting will be more practical. This motivation may stimulate follow-up research.\n\n2. There is sufficient convergence analysis and detailed settings of experiments in the paper. \n\n3. The experiments cover several modalities, such as image, sketch, depth map, and audio, and two downstream tasks of recognition and segmentation."
            },
            "weaknesses": {
                "value": "1. The proposed method follows the self-supervised knowledge distillation framework which is widely used in existing works. The common approach is using contrastive learning between source modality and target modality, and fine-tuning on labeled data in target modality, such as Ref[1].\n    \n2. The authors claim that both positive and negative correspondence are leveraged in the abstract. However, it seems that the negative correspondence is only used in the pretrain stage of source modality.\n\n3. There are too few ablation experiments in the paper, e.g., it can be seen that different ResNet backbone networks cause less impact in Table 2.  I'm wondering if the transformer will make a big improvement.\n\n [1] Jin W, Lee D H, Zhu C, et al. Leveraging visual knowledge in language tasks: An empirical study on intermediate pre-training for cross-modal knowledge transfer[J]. arXiv preprint arXiv:2203.07519, 2022."
            },
            "questions": {
                "value": "Q1: Why not utilize unpaired data in the target modality?\n\nQ2: If there is no labeled data in the target domain, is the model still effective?\n\nQ3: In Table 1, what\u2019s the downstream task of each dataset? It\u2019s not clear to me in its current form."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1422/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1422/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1422/Reviewer_qhgx"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1422/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698597257612,
        "cdate": 1698597257612,
        "tmdate": 1699636070609,
        "mdate": 1699636070609,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "33wNOIVnCb",
        "forum": "k0RQHNulm7",
        "replyto": "k0RQHNulm7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1422/Reviewer_3kmi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1422/Reviewer_3kmi"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses cross-modality distillation for data with limited information. Existing methods focus on aligning features between source and target modalities but overlook negative relationships in unpaired data. The authors introduce \"generalizable cross-modality contrastive distillation (CMCD)\" that leverages both positive and negative correspondences, outperforming existing methods across various modalities and tasks. They emphasize the impact of modality distance on downstream task performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed CMD and CMC losses seem to be novel.\n2. Experimental results show the superiority of the proposed two losses over previous methods.\n3. Theoretical analysis shows that the performance of proposed two losses are controlled by the alignment of latent feature distributions."
            },
            "weaknesses": {
                "value": "1. The motivation for the proposed distillation is not clear to me. If we do not use the labels of the source modality, only do self-supervised learning on the source data (though source and target data are paired, in my understanding they share labels), and finally only supervised trained on target data, why do we expect improvements (though results are improved)? The pipeline of SSL (source), alignment (source + target), and FT (target) is not reasonable as we actually can direct SSL (target) + FT (target).\n2. Theoretical analysis cannot support why the losses are useful as it only proves that the test error of the target task is bounded by the distance between two distributions. However, there is no theorem that the proposed losses can achieve a smaller distance than the previous method.\n3. The experiments actually show that SSL (source) - alignment (source + target) - FT (target) is better than SSL (target) - FT (target) and FT (target), which is counterintuitive. I would like to see a more detailed analysis of why this happens.\n4. Experimental details are missing, e.g., epochs/lr of SSL, alignment, and FT, which is essential for evaluating the results without reproducing the experiments.\n\nOverall, too many details of the experiments and settings are missing. I would like to request authors add details for reproducibility."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1422/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1422/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1422/Reviewer_3kmi"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1422/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698711254339,
        "cdate": 1698711254339,
        "tmdate": 1699636070539,
        "mdate": 1699636070539,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hKFEMspLFc",
        "forum": "k0RQHNulm7",
        "replyto": "k0RQHNulm7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1422/Reviewer_s3Ct"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1422/Reviewer_s3Ct"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method for a cross modality distillation problem. The proposed method is based on contrastive learning in order to take both positive instance pairs and negative pairs into account, while existing methods typically rely on the positive pairs. Furthermore, the paper provides theoretical analysis on the error bound of the proposed method. The effectiveness of the proposed method is verified on a wide variety of cross modal transfer setting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- S1. The proposed approach is simple and reasonable, and it turns out to be effective in different cross modal transfer learning scenario.\n- S2. The paper provides theoretical analysis on the error bounds and discusses the characteristic of the method based on the analysis, i.e., in which situations the proposed method is expected to work well.\n- S3. The paper generally reads well."
            },
            "weaknesses": {
                "value": "- W1. The originality of the proposed method is not that outstanding because equation (2) is a straightforward adaptation of self-supervised distillation [Fang et al., 2021] to the cross modal setting and equation (3) is also a straightforward adaptation of what was proposed in the paper of CLIP [Radford et al., 2021]. I do acknowledge the theoretical analysis part, but the novelty of the method itself is rather limited.\n- W2. The claim in the 2nd last line of section 3 \n> \u201cIt indicates that if source and target modalities have more common information or patterns, \nthe algorithm will have a higher probability of distilling more information from the source modality to the downstream task in the target modality.\"\n\n    lacks objective evidence. The paragraph \u201cRelationship with the generalization bound\u201d in section 4.1 discusses it, but the discussion is rather subjective. It would become much more convincing if the authors can provide more objective evidence. For example, it may be interesting to provide the analysis on the relationship between the performance and estimated total variation distance between two datasets. \n- W3. Some important experimental setting is not described in the main paper. What are the values of M and m?\n\nTypo and minor suggestions.\n1. In Figure 1, it is better to clearly indicates which figure corresponds to which method.\n1. Please check the grammar of the sentence after equation (13) \n> \u201cDetailed proof our the Theorem 3.3 in Appendix A.3.\u201d\n1. The first sentence of section 4 \n>\u201cTo demonstrate the efficiency of our algorithm, we conduct extensive experiments on various cross-modality tasks.\u201d\n\n    efficiency -> effectiveness?\n1. In P7, 3 lines from the bottom, \n>\u201cour method utilizing CMD/CMC loss achieves top-1 accuracies of 72.61%/73.24% on Sketchy, outperforming the best baseline by a margin of 3%.\u201d\n\n    The margin is less than 2% as SOCKET+LE achieves 71.33%."
            },
            "questions": {
                "value": "Is it possible to apply both CMD and CMC?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1422/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699171898885,
        "cdate": 1699171898885,
        "tmdate": 1699636070432,
        "mdate": 1699636070432,
        "license": "CC BY 4.0",
        "version": 2
    }
]