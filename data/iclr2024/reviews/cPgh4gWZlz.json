[
    {
        "id": "nIjM7qFODb",
        "forum": "cPgh4gWZlz",
        "replyto": "cPgh4gWZlz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8874/Reviewer_TzUb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8874/Reviewer_TzUb"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed chain-of-knowledge (CoK), a novel framework that augments LLMs by dynamically incorporating grounding information from heterogeneous sources. The CoK framework consists of three stages, reasoning preparation, dynamic knowledge adapting, and answer consolidation. This paper provided rich experiments and analysis."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1. This paper proposed chain-of-knowledge (CoK), a novel framework to enhance the factual correctness of LLMs with heterogeneous knowledge sources. CoK can dynamically select one or multiple knowledge sources based on the types of questions.\n\nS2. Cok participates in Cot which modifies incorrect answers in each reasoning step by introducing external knowledge, thereby generating the correct answer.\n\nS3. This paper performs extensive experiments on knowledge-intensive tasks spanning a range of domains, including factual, medical, physical, and biological."
            },
            "weaknesses": {
                "value": "This paper bears a high resemblance to VE with the main distinction being the inclusion of multiple knowledge sources, which might be considered a relatively ordinary contribution. The AQG retrieves relevant knowledge from multiple knowledge sources by converting questions into SPARQL, SQL, and natural language queries. The SPARQL query generator is fine-tuned based on question-SPARQL pairs. I think this training process may make it challenging for AQG to generate SPARQL queries for complex and compositional questions. Because it cannot guarantee that the sub-questions derived from the CoT decomposition are all simple."
            },
            "questions": {
                "value": "Q1. The authors specifically evaluated the proposed framework in experiments, with a focus on the biology, medical, and physics domains, showing better results compared to the VE method. The knowledge sources used by the author include Wikidata, medical Flashcard, UpToDate, ScienceQA Physics, and ScienceQA Biology. I'm wondering if the authors also incorporated these knowledge sources into the VE method. If not, I believe such a comparison would be unfair.\n\nQ2. LLMs generate multiple results based on self-consistency, if the consistency falls below a threshold, the proposed method is initiated to correct the answer. I'm curious about how often such a process needs to be introduced in practical experiments. Which of the three extraction methods (natural language, SPARQL, and SQL) do the authors consider to be more reliable for LLMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8874/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8874/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8874/Reviewer_TzUb"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8874/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698457615483,
        "cdate": 1698457615483,
        "tmdate": 1699637117157,
        "mdate": 1699637117157,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "y9A2B0Ilt7",
        "forum": "cPgh4gWZlz",
        "replyto": "cPgh4gWZlz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8874/Reviewer_SrPL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8874/Reviewer_SrPL"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the chain-of-knowledge (CoK) framework designed to augment the performance of LLMs by reducing instances of information hallucination and improving factual correctness. The CoK operates in three stages: reasoning preparation, dynamic knowledge adaptation, and answer consolidation. A distinctive feature of CoK is its ability to dynamically tap into diverse knowledge sources, including structured databases like Wikidata, using an adaptive query generator (AQG) capable of generating varied query languages, such as SPARQL, SQL, and natural language queries. The framework emphasizes progressive correction of the generated rationales, minimizing the risk of error propagation across reasoning steps. Experimental evaluations indicate that CoK enhances the performance of LLMs on knowledge-intensive tasks across different domains, surpassing the chain-of-thought (CoT) baseline by an average of 4.3%."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- CoK addresses the inherent limitations of existing methods by leveraging diverse knowledge sources, both structured and unstructured, improving the accuracy and factual correctness of LLM outputs.\n- The adaptive query generator (AQG) showcases versatility, enabling seamless transition between specialized models and generic LLMs, allowing effective querying across different knowledge source formats.\n- Progressive rationale correction reduces the risk of error propagation, ensuring more reliable generation of answers.\n- Comprehensive experiments covering various knowledge domains, demonstrating a consistent performance boost over the CoT baseline."
            },
            "weaknesses": {
                "value": "- While the paper emphasizes the use of diverse knowledge sources, it might benefit from a more explicit discussion on the scalability and efficiency of the framework as the volume and variety of sources grow.\n- The framework's dependence on the AQG's capability to generate effective queries for all types of knowledge sources might be a potential bottleneck, especially for highly specialized domains.\n- Although the paper mentions improvement over the CoT baseline, deeper insights into the limitations and areas where CoK might not perform optimally would be beneficial. Except for the limitations of \u201cKnowledge Sources\u201d and \u201cKnowledge Retrieval\u201d discussed in Appendix G."
            },
            "questions": {
                "value": "- How does the CoK framework handle situations where knowledge sources provide conflicting information? How are you going to solve it?\n- What are the computational overheads introduced by the AQG and the dynamic knowledge adaptation process, especially when querying multiple heterogeneous sources?\n- Have there been considerations or plans to extend the CoK framework to accommodate real-time or streaming knowledge sources?\n- Can you provide insights into the training and fine-tuning process of the AQG, especially its adaptability across different query languages and knowledge sources?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8874/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698586520324,
        "cdate": 1698586520324,
        "tmdate": 1699637117036,
        "mdate": 1699637117036,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dCrQqzah2R",
        "forum": "cPgh4gWZlz",
        "replyto": "cPgh4gWZlz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8874/Reviewer_bxPp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8874/Reviewer_bxPp"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a chain-of-knowledge framework to augmenting LLM in question answering by dynamically retrieving the grounding information from multiple sources. The proposed method first gives the rationale in steps. Then the search query is generated by determining the domain involved in each rationale and the final query. Eventually, the rationale is corrected by the information obtained from the query to give a more plausible answer. The authors conducted experiments on multiple datasets and defeated the compared few-shot methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This article proposes a novel idea to reduce the hallucination of LLM by correcting each rationale in the chain of thought.\n2. The proposed method considers structured and unstructured queries for heterogeneous data sources.\n3. The authors conducted detailed experiments and analysis on multiple datasets from different domains."
            },
            "weaknesses": {
                "value": "1. The general outline of the suggested approach is somewhat understandable, but the crucial stages lack the necessary level of specificity. In my opinion, a key point in the effectiveness of the proposed method is how to construct accurate queries to retrieve grounding information, but the description in section 3 is still not clear enough. For example, how to build a high-quality question-sparql dataset to ensure the accuracy of this step. While the prompt used with the training loss function is provided in the appendix by the authors, I believe it is important for the main body of the paper to be self-contained, and critical steps like this should not solely rely on the information provided in the appendix.\n2. How does the author determine the table or tables used when generating a SQL query for a rationale, and how can they ensure that the query generated by ChatGPT is valid and answerable if table schema information is not provided. Ditto for SPARQL.\n3. Baseline performance on the hotpotQA dataset seems too poor. Existing work [1] has achieved an accuracy of 0.37 when reviewing ChatGPT.\n\n[1] Zheng, Shen, Jie Huang, and Kevin Chen-Chuan Chang. \"Why does chatgpt fall short in providing truthful answers.\" ArXiv preprint, abs/2304.10513 (2023)."
            },
            "questions": {
                "value": "1. How does the author ensure that LLM can give faithful answers (query) when performing knowledge adaptation?\n2. The proposed method leaves the decomposition of the question (i.e., the generation of each rationale) to ChatGPT. I am interested in whether this method is only applicable to chain-style question? For more complex questions, where the corresponding SPARQL contains more than one inference path, can the proposed method handle it?\n3. When generating SQL (SPARQL) queries, how do the authors guarantee that the produced queries are valid?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8874/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8874/Reviewer_bxPp",
                    "ICLR.cc/2024/Conference/Submission8874/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8874/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698659969922,
        "cdate": 1698659969922,
        "tmdate": 1700635024720,
        "mdate": 1700635024720,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bfmlrjvVlf",
        "forum": "cPgh4gWZlz",
        "replyto": "cPgh4gWZlz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8874/Reviewer_84zL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8874/Reviewer_84zL"
        ],
        "content": {
            "summary": {
                "value": "This is a very interesting paper on Chain of Knowledge and integrating the question answering from LLM with traditional source of knowledge such as KG to improve the overall accuracy of question answering in domain-specific text.\nMain contributions:\n1. Chain of Knowledge for factual correctness\n2. Adaptive query generator for identifying knowledge source and converting queries in respective form\n3. Progressive correction of rationale using CoK\n4. Following domains have been chose for the work factual, medical, physical, and biological."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is very well written.\nThe authors have considered good number of scenarios for knowledge, and carefully selected wikidata as the knowledge source, provided the limitation on retrieval of knowledge in general.\nI enjoyed reading this paper."
            },
            "weaknesses": {
                "value": "Please see the questions section for more."
            },
            "questions": {
                "value": "* For the given SPARQL query SELECT ?answer WHERE { wd:/Souleymane Sane/ wdt:/child/ ?answer . \u00b4 }, do you use some library to convert the entities/relation in their specific identifier form (entity linking), how do you know which is the correct identifier, in specific cases where similar natural language entities have two identifiers.\n* Did you try domain specific questions with ChatGPT? Upon trying the example given in Figure 1 on ChatGPT, I could see the result as George Gamow. Maybe you should come up with an example where ChatGPT would hallucinate. Or if the idea here is to show open source LLM, then this example makes complete sense. Although it should be mentioned in this case which model is used (Llama2-7b-chat? or instruction tuned by yourself)\n* It is possible that the knowledge injected through Wikidata doesn't answer the question asked, what would be done to answer the question in that case?\n* Can you please explain how do you find specific relation for querying with an entity? There is a possibility of multiple entity attached with one relation. It is also possible that multiple relations connected with multiple entities, and it may be difficult to identify the relation depending on the question. What would be done in that case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8874/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828162076,
        "cdate": 1698828162076,
        "tmdate": 1699637116814,
        "mdate": 1699637116814,
        "license": "CC BY 4.0",
        "version": 2
    }
]