[
    {
        "id": "E3stUXd0Mz",
        "forum": "RN7RzMxwjC",
        "replyto": "RN7RzMxwjC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission615/Reviewer_pH7N"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission615/Reviewer_pH7N"
        ],
        "content": {
            "summary": {
                "value": "Harmony World Models: Boosting Sample Efficiency for Model-based Reinforcement Learning\n\nIn this paper, the authors observe that the coefficients of reward loss and dynamic loss, if not chosen carefully, could affect the performance of the final model-based RL algorithm.\nAnd the authors propose to use a set of learnable parameters to approximate loss scale to balance the loss.\nThere are performance improvements introduced as shown in the experiments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well written and easy to understand.\n2. The proposed method is quite straight-forward, making it reproducible and easy to apply to existing algorithms."
            },
            "weaknesses": {
                "value": "1. There\u2019s no good ablation experiments on different reward blending schemes.\n\n    The chosen method for blending/weighting the loss terms looks reasonable, but not necessarily the best method or even the most reliable one. \n\n    There needs to be some other weighting methods tested in the experiment sections.\n\n    Some simple weighting methods should be compared, which include for example the one in [1] and some other straight-forward heuristic methods.\n\n2. It would also be helpful to show how sensitive the proposed method is. \n\n    Does it require separate tuning for each task? \n    \n    If it needs to be tuned, how much effort is needed? Is it stable across random seeds?\n\n3. More baselines and environments are needed.\n\n    The proposed method is only compared against dreamerV2 in a small subset of tasks. \n\n    What will happen if it is compared to MuZero, TD-MPC and SimPle etc as mentioned in the paper?\n\n    And what will happen if the algorithm is tested in similar environments such as GO, those atari environments or locomotion control tasks from image?"
            },
            "questions": {
                "value": "The performance of DreamerV2 seems too bad in some environments like Cheetah Run, Walker Run, which is hard to figure out why considering DreamerV2 was applied to far harder problems and succeeded and DreamerV1 has better performance with the same number of training samples.\n\nWhy does it happen?\n\n[1] Kendall, Alex, Yarin Gal, and Roberto Cipolla. \"Multi-task learning using uncertainty to weigh losses for scene geometry and semantics.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7482-7491. 2018."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission615/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698355719511,
        "cdate": 1698355719511,
        "tmdate": 1699635989185,
        "mdate": 1699635989185,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jktg6GIMD7",
        "forum": "RN7RzMxwjC",
        "replyto": "RN7RzMxwjC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission615/Reviewer_WtCd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission615/Reviewer_WtCd"
        ],
        "content": {
            "summary": {
                "value": "This paper identifies the multi-task essence (the dynamics and the reward) of world models and analyzes the disharmonious interference between different tasks. The authors empirically find that adjusting the coefficient of reward loss (which is overlooked by most previous works) can emphasize task-relevant information and improve sample efficiency. To adaptively balance the reward and the dynamic losses, the authors propose a harmonious loss function that learns a global reciprocal of the loss scale using uncertainty. The experiments show that the proposed method outperforms Dreamerv2 with a considerable gap and has better generality than DreamerV3 and DreamerPro."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper systematically identifies the multi-tasking essence of world models, which is essential to model-based methods and is overlooked by most previous work. The whole paper is well-written and easy to follow.\n\n> The proposed method has generality to other model-based RL algorithms and significantly improves the sample efficiency."
            },
            "weaknesses": {
                "value": "> The novelty of the proposed method is limited. The authors build the optimization process of world models as a multi-task optimization and then use uncertainty weighting to harmonize loss scales among tasks. More state-of-the-art methods, e.g., multi-objective optimization or multi-agent learning methods, can be considered and may further improve the performance."
            },
            "questions": {
                "value": "> In Fig.15 & 16, HarmonyWM $ w_d=1 $ learns faster than Harmony WM at the beginning of some tasks. Could the authors give some explanations about this?\n\n> The authors claim \"the root cause as the disharmonious interference between two tasks in explicit world model learning: due to *overload of redundant observation signals* ...\". Why does, for example, Denoised MDP, which identifies the information that can be safely discarded as noises, serve as a baseline in the experiments?\n\n>  Is there a sufficient reason why the reward modeling and the observation modeling tasks are scaled to the same constant? Should we maintain the equilibrium between these tasks? Or can we emphasize some tasks at some specific states?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission615/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698685920389,
        "cdate": 1698685920389,
        "tmdate": 1699635989117,
        "mdate": 1699635989117,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PJi16YbDBU",
        "forum": "RN7RzMxwjC",
        "replyto": "RN7RzMxwjC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission615/Reviewer_72ZS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission615/Reviewer_72ZS"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes viewing model learning from a multi-task perspective and introduces a method to adjust the trade-off between observation loss and reward loss to train the world model. Combined with DreamerV2, the proposed Harmony World Model achieves a noticeable performance improvement over DreamerV2 in several domains."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method is novel and interesting.\n2. Compared to DreamerV2, there is a significant improvement in performance.\n3. The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. Although the method proposed in this paper is novel, DreamerV3 has already addressed the issue of differing scales in reconstructing inputs and predicting rewards using Symlog Predictions. This paper primarily conducts extensive experiments in comparison with DreamerV2 and lacks more comparisons and discussions with DreamerV3. This significantly diminishes the contributions of this paper.\n\n2. Moreover, as TDMPC is mentioned in the paper as one of the Implicit MBRL methods, I believe it should also be considered as one of the important baselines for comparisons across different benchmarks.\n\n3. The experimental environments chosen in the paper are all easy tasks from different domains. I think experiments should be conducted in more challenging environments, such as Hopper-hop and Quadruped-run in DMC, Assembly and Stick-pull in MetaWorld, etc. Moreover, given the current trend in the community to conduct large-scale experiments, having results from only eight environments seems somewhat limited."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission615/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698699784699,
        "cdate": 1698699784699,
        "tmdate": 1699635989052,
        "mdate": 1699635989052,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9jBhocmGZA",
        "forum": "RN7RzMxwjC",
        "replyto": "RN7RzMxwjC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission615/Reviewer_QvDA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission615/Reviewer_QvDA"
        ],
        "content": {
            "summary": {
                "value": "Typical MBRL methods optimize for two tasks - observation modeling (aka explicit MRL) and reward modeling (aka implicit MBRL). The paper hypothesizes that there is interference between the two tasks and proposed an alternate scheme for weighing the losses corresponding to the two tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well written and flows nicely.\n\n2. The authors experiment with 4 set of environments and the results look reasonable"
            },
            "weaknesses": {
                "value": "1. The paper calls out the following as it contribution: \"To the best of our knowledge, our work, for the first time, systematically identifies the multitask essence of world models and analyzes the disharmonious interference between different tasks, which is unexpectedly overlooked by most previous work\". While I agree that MBRL can be formulated could be multi-task RL problem (it is clearly multi-objective problem), the paper does not do a through job at analyzing \"the disharmonious interference between different tasks\". e.g. they do not study if and why there is an interference between different tasks. Note that in the multi-task literature, interference often refers to progress on one task, hindering the progress of another task. What the authors show is that adhoc setting of scalars for the different losses can hurt the performance on the RL task but they do not show that it hurts the performance on the tasks being directly optimized for. The distinction is important in the multi-task setup (which is what the paper uses).\n\n2. While the paper formulates the problem as a multi-task problem, they do not compare with any multi-task baseline that could balance between the different losses in equation 3. So while they show that adjusting the loss coefficients help, they do not show if their approach is better than other multi-task approaches."
            },
            "questions": {
                "value": "Listing some questions (to make sure I better understand the paper) and potential areas of improvement. Looking forward to engaging with the authors on these questions and the points in the weakness section.\n\n1. In equation 1, the input to the representation model $q_{\\theta}$ should be $o_t$ right ?\n2. The paper seems to suggest that \"observation modeling\" task is a new task. Is that correct ?\n3. The word \"harmony\" in HWM - does it come from some mathematical properties or it refer to \"harmony\" (tuning) between the losses?\n4. Regarding \"the scale of L r is two orders of magnitude smaller than that of L o, which usually aggregates H \u00d7 W \u00d7 C dimensions\", it usually doesnt matter what the dimensions of output are as the loss is averaged over the dimensions.\n5. Are the findings 1, 2, 3 are new findings ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission615/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698717119371,
        "cdate": 1698717119371,
        "tmdate": 1699635988981,
        "mdate": 1699635988981,
        "license": "CC BY 4.0",
        "version": 2
    }
]