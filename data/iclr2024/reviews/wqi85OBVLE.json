[
    {
        "id": "i8GOt3CSKB",
        "forum": "wqi85OBVLE",
        "replyto": "wqi85OBVLE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3919/Reviewer_Qwty"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3919/Reviewer_Qwty"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel training schema called \"reward adaptation\" to leverage existing learned Q functions (with pre-defined source reward settings) to expedite learning a target Q function(in a new and target reward setting). The key idea is to maintain two Q-function variants (Q* and Q-min) for each source reward setting and use them to compute bounds on the expected return under the target reward function. Using reward shaping to tighten the bounds, many actions can be safely pruned before learning the target Q function. This \"Q-Manipulation\" approach is proven to retain optimality. Empirical results in a variety of domains show faster convergence compared to baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The problem of reward adaptation provides a new perspective on transfer learning and modular RL. Being able to leverage existing behaviors to learn new ones more efficiently has many useful applications.\n\n2) Theoretical analysis of computing bounds on the expected return and the effects of reward shaping is thorough. The proof of retained optimality after pruning is important.\n\n3) Empirical results demonstrate significantly faster convergence across different domains, validating the effectiveness of Q-Manipulation for pruning unpromising actions. Comparisons to relevant baselines are adequate."
            },
            "weaknesses": {
                "value": "1) The linear programming formulation for computing the reward shaping function does not scale well. Approximation methods need to be considered for large state/action spaces.\n\n2) Requiring both Q* and Q-min doubles the learning cost for source behaviors. It would be useful to analyze if pruning is possible with just Q* and \u03c0*.\n\n3) The assumption of target reward being an exact polynomial of source rewards is limiting. More analysis on effectiveness when the target can only be approximated is needed.\n\n4) Comparisons to a broader variety of baselines (e.g., transfer learning methods) could be more informative about the relative merits of this approach. More results on complex environments, such as Atari Games, should be better inducted to show that this method is effective."
            },
            "questions": {
                "value": "1) According to Algorithm 1, Is Q* and Q-min pre-learned in an offline form? If so, prune action with steps 1-4 may be considered to be inefficient. \n\n2) The advantage baseline in A2C seems low-cost; how about considering it for pruning action?\n\n3) When does action pruning occur? In the exploration, in the target Q update, or both. It seems that the prune behavior is conservative. \n\n4) Could you visualize the pruned action space in the training process and compare it to the original action space?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3919/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698676637531,
        "cdate": 1698676637531,
        "tmdate": 1699636352348,
        "mdate": 1699636352348,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "j1UydBMue5",
        "forum": "wqi85OBVLE",
        "replyto": "wqi85OBVLE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3919/Reviewer_hu2d"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3919/Reviewer_hu2d"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce the reward adaptation (RA) problem where the agent learns to adapt to a _target_ reward function while having access to behaviors learned from multiple _source_ reward functions. The authors then focus on a restricted scenario where the target reward function is a polynomial function of the source reward functions. They introduce a method, Q-Manipulation, for action pruning in learning the target task by employing the optimal Q-function from the source tasks, with the hope that learning will be more efficient with the reduced action set. Q-Manipulation estimates the upper bound and the lower bound of the Q-value for each action from its Q-values from the source tasks. An action is then pruned when its Q-value upper bound is below the Q-value lower bound of another action. The authors also introduce a reward shaping based method to tighten the upper bounds and the lower bounds to further facilitate pruning. Tabular experiments demonstrate the effectiveness of Q-Manipulation at action pruning, and further show accelerated learning on the target task."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This work addresses an important question of how to learn more efficiently on a target task by leveraging knowledge from related source tasks. Progress on this question can further broaden the applicability of reinforcement learning to real-world applications.\n\n* Overall the paper is easy to follow. The problem of RA is well motivated and the main idea behind Q-Manipulation is straightforward.\n\n* The experiments are well-designed to test the key attributes of the Q-Manipulation algorithm, such as the portion of pruned action and the downstream impact on learning efficiency."
            },
            "weaknesses": {
                "value": "* I am afraid that the theoretical results for reward shaping is incorrect. Specifically, I have doubts regarding the correctness of Lemma 5. When reward shaping is applied, $Q_{R_{F}}^{\\*}$ and $Q_{R_{F}}^{\\mu}$ should be offset towards the same direction, not the opposite directions. In fact, as the authors note in Eq. (15), Lemma 5 implies that $Q^{\\*}$ can be smaller than $Q^{\\mu}$ after reward shaping, meaning that the optimal policy changes, which conflicts with the theory built in [3].\n\n* The novelty of the RA problem is questionable. In the second paragraph of the Introduction section, the authors write \"In this paper, we introduce Reward Adaptation (RA), ...\" However, this problem has been widely studied by existing works such as [1] and [2]. Discussion on the connection to these existing works is missing in the manuscript.\n\n* All experiments were conducted in tabular toy environments and the improvement in learning efficiency is marginal. The authors did not comment on the scalability of Q-Manipulation to the function approximation settings. It would be more convincing if the authors can provide more empirical evidence supporting the effectiveness of Q-Manipulation in larger environments.\n\n\n## References\n[1] Barreto _et al_, Successor features for transfer in reinforcement learning, NeurIPS 2017\n\n[2] Barreto _et al_, The option keyboard: Combining skills in reinforcement learning, NeurIPS 2019\n\n[3] Ng _et al_, Policy invariance under reward transformations: Theory and application to reward shaping, ICML 1999"
            },
            "questions": {
                "value": "* Could the authors address my question regarding the correctness of Lemma 5?\n* Could the authors comment on the scalability of Q-Manipulation?\n* Could the author clarify the connections to existing works such as GPI [1] and Option Keyboards [2]?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3919/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698793292799,
        "cdate": 1698793292799,
        "tmdate": 1699636352204,
        "mdate": 1699636352204,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Iefzr8SMOq",
        "forum": "wqi85OBVLE",
        "replyto": "wqi85OBVLE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3919/Reviewer_xHsM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3919/Reviewer_xHsM"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the reward adaptation problem, where an agent with access to optimal behavior in source MDPs must quickly learn optimal behavior in a target MDP with a new reward function. The authors assume the target reward function is a polynomial function of the source reward functions in a finite MDP setting and propose the \"Q-Manipulation\" method to enable action pruning before learning the target behavior."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is overall well-written.  \n\nThe proposed method is novel and interesting."
            },
            "weaknesses": {
                "value": "As a core contribution, the authors claim that \u201cWe introduce the problem of reward adaptation\u201d. However, I am not sure that it is valid. It seems to me the reward adaptation formulation in Section 2.1 of this paper is an extension of the \u201cTransfer via successor features\u201d problem of [1], where the authors assumed the target reward function to be the linear combination of source reward functions (see Section 4 of [1]). If this is not the case, please clarify. If this is the case, it is important to refer to the successor features literature, and compare your method with the methods proposed in [1], both conceptually and empirically. \n\nBased on the derivations in Sections 2.2 and 2.3, the action pruning strategy is heavily reliant on the assumption that the target reward function is a polynomial function of the source reward functions. In the settings where this assumption is violated, there is a risk that even the optimal actions in the target MDP are pruned. The transfer learning techniques that use potential-based shaping ideas can safely avoid this optimality issue. \n\nIn a single source MDP setting, for the reward adaptation problem, the technique proposed in [2] can be applied. Here, the Q-value function in the source domain can serve as a potential function to shape the reward function in the target domain. In this case, the target reward function does not need to be a polynomial function of the source reward function. In the case of multiple source MDP setting, we can use a weighted combination of the Q-values as a potential function to shape the reward function in the target domain. One can learn better weights for combining via the bi-level optimization framework proposed in [3]. It is important to compare your approach to this transfer via shaping technique, both conceptually and empirically. \n\nReferences:\n\n[1] Barreto et al. Successor Features for Transfer in Reinforcement Learning. 2017.  \n\n[2] Brys et al. Policy Transfer using Reward Shaping. 2015. \n\n[3] Hu et al. Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping. 2020."
            },
            "questions": {
                "value": "Minor comments:\n\nIn Section 2.2, the authors state that the influence of discounting can be safely ignored, e.g., when MDPs with absorbing states are considered. In the proofs of Lemma 2 and 3, the discounting factor is ignored; whereas, in the proof of Lemma 4, it is not ignored. Please formally/explicitly write the type of MDPs considered in the proofs. \n\nDue to high/overlapping variance in the convergence plots, it is not very clear that the proposed method outperforms the current set of baselines. The authors could consider additional presentation of the results (e.g. in a tabular form)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3919/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799465160,
        "cdate": 1698799465160,
        "tmdate": 1699636352110,
        "mdate": 1699636352110,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ehcTIbRfbF",
        "forum": "wqi85OBVLE",
        "replyto": "wqi85OBVLE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3919/Reviewer_Z39j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3919/Reviewer_Z39j"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a problem they call \"Reward adaptation\", where an agent which has been previously trained on a set of different reward functions can be more quickly trained on a new reward function. A method is proposed to compute upper and lower bounds on the Q function for a new reward function, given that the reward function is expressed as a polynomial of the existing reward functions and the system had previously kept the q values and the q values of the negative reward from every state in the environment. These upper and lower bounds are used to eliminate actions during exploration for the new reward."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper looks at an important problem of transfer learning in RL. The justification for the proposed method in 2.2 seems correct, and it should have few downsides if the domain satisfies the assumptions that make the approach possible."
            },
            "weaknesses": {
                "value": "Section 2.3 appears to have several errors.  In particular, Lemma5 appears to be wrong. A counter-example would be adding a constant potential to every state, which should not increase the min Q value. This is straightforward to check in the 1-state, 1-action where all policies are the same; the min and max Q-values would be the same even after a potential shaping term was added, contradicting Lemma 5.\n\nI believe the error is on the second line of the derivation of (14). -{R_F} after reward shaping is -R + -F, so once we apply Ng et al.1999 we get a + phi(s) term rather than a negative due to the double negative. There is then another negative remaining outside the square brackets, which makes (14) match (13).\n\nAnother hint that this has to be wrong is that the constraint is added to (15) to ensure \"the upper bound remains greater than or equal to the lower bound\", but that should be mathematically impossible if the theorem was valid.\n\nAnother issue with this section is that the Ng. 1999 paper requires SAS rewards, but this paper is written with SA rewards, so the theory does not apply directly as stated.\n\n\nIn addition, the paper could also be made significantly more clear. For instance, rather than defining the minimum achievable reward as Q_{-R}, there is a new symbol introduced (which confusingly includes mu), and then it is immediately pointed out that this is the same as Q_{-R}. It seems like this observation is so straightforward as to not need a Lemma, and the added notation not only uses a lot of space but makes the rest of the paper much more difficult to follow.\n\nMany of the methods are also redefined as acronyms halfway through, in a way that is not self-documenting. It becomes excessively difficult to keep track of the differences between Q-M, RS, Q-D, and Q.\n\nFinally, the method is quite difficult to motivate. The main example they point to is if you had a self-driving car that was trained to \"either be fast or safe\", you could warm-start it to learn the other, but this is far from how self-driving cars work. Even so, it is hard to imagine how \"fast\" or \"safe\" could be expressed as a polynomial of the other, as is required by their method."
            },
            "questions": {
                "value": "How do you think the method could be extended past polynomial combinations of existing reward functions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3919/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839462711,
        "cdate": 1698839462711,
        "tmdate": 1699636352039,
        "mdate": 1699636352039,
        "license": "CC BY 4.0",
        "version": 2
    }
]