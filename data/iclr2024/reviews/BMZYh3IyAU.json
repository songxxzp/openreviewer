[
    {
        "id": "gaOf3sbObu",
        "forum": "BMZYh3IyAU",
        "replyto": "BMZYh3IyAU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6965/Reviewer_Va6u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6965/Reviewer_Va6u"
        ],
        "content": {
            "summary": {
                "value": "The paper studied how to accelerate a federated optimization method using local training and communication compression.\n\nTo that end, the author proposed CompressedScaffnew and proved that this algorithm achieves the total communication complexity of $\\widetilde{O}(\\sqrt{d \\kappa} + d)$ for the strongly convex case, which is the current SOTA result."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is mostly easy to follow, while some parts are vague and hard to understand.\n\nThe theoretical result in the paper is new and exciting. It is good to see that acceleration, message compression, and local training could be combined organically.\n\nThe numerical experiments support the theories."
            },
            "weaknesses": {
                "value": "1. No lower bound for the communication complexity.\n\n2. Some parts are not very clear (see Question for more details).\n\n3. No conclusion in the paper. Possible extensions are not discussed.\n\n4. Discussion on generally convex cases seems not to be sufficient as the strongly convex case. The empirical experiments are only for the strongly convex case."
            },
            "questions": {
                "value": "1. At the beginning of the second paragraph of Section 2, the author claims that ``It is very challenging to combine LT and CC.`` \nHowever, the proposed algorithm is somewhat quite similar to Scaffnew. It makes me wonder what the difficulty is, while the solution is not a huge deviation from a previous algorithm. I found that the author tried to explain this point at the end of Page 4. However, I found this part really hard to grasp, given that the algorithm CompressedScaffnew is introduced in the next section, and many terminologies are not formally defined (such as ``control variates`` and ``two mechanisms``). I hope the author could elaborate more on the difficulties, which would make readers more clear on the novelty.\n\n2. Is there any insight for the choice of $\\eta$ in (7)? Why is it in this specific form?\n\n3. I know the main focus of this paper is on convex deterministic optimization. However, I would like to whether the proposed technique could be extended to stochastic /non-convex optimization. This would definitely benefit future work.\n\n4. Most of the focus was put on the strongly convex case. Note that the author also provides the convergence rate for the generally convex case. However, the author didn't discuss the communication complexity of that case.  What is the corresponding communication complexity? Is it still better than the baseline methods?\n\n5. Another question that puzzles me is that in Theorem 2, the author studied the convergence of gradient norm $\\|\\nabla f(\\tilde{x}_i^T)\\|$ \n rather than the objective loss $f(\\tilde{x}_i^T) - f(x^{\\star})$. Is there any difficulty to do so?\n\nI will increase my point if the author could address my questions well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6965/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698634709474,
        "cdate": 1698634709474,
        "tmdate": 1699636813447,
        "mdate": 1699636813447,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qem7wC0bXW",
        "forum": "BMZYh3IyAU",
        "replyto": "BMZYh3IyAU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6965/Reviewer_5itr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6965/Reviewer_5itr"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a new algorithm for FL with communication compression and local steps. Authors provide theoretical analysis that shows linear convergence and outperform other SOTA approaches. Double acceleration $\\tilde{O}(\\sqrt{\\kappa}\\sqrt{d})$ is achieved by local training and specific compressor. Simple experiments on logistics with theoretical hyperparameters show the superiority of proposed method compared to Scaffnew and GD."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Strong theoretical result, that improves complexities of SOTA methods with no assumptions on similarity, relying solely on the assumption of strong convexity and smoothness.\n2. Double acceleration is theoretically achieved by combination of local steps and compression. \n3. Paper is well written and efficiently presents mathematical details."
            },
            "weaknesses": {
                "value": "1. Experiments. The comparison is solely between GD, Scaffnew, and the proposed method (Scaffnew + CC). It is noteworthy that the absence of a comparison with methods incorporating communication compression ([1, 2]) and other SOTA approaches limits the comprehensiveness of the experimental evaluation. Furthermore, there is no surprise that the proposed method outperforms Scaffnew as the authors claim (page 8) that without compression the proposed algorithm achieves Scaffnew rate. Additional experiments with other baselines are warranted to provide a more comprehensive analysis\n2. Experiments. It would be beneficial for the authors to conduct an additional set of experiments with fine-tuned hyperparameters (HP), as theoretical HP are typically unknown during the optimization process\n3. Compression. The proposed framework works with specific compressor and does not allow for famous and practically successful compression techniques like Top-K and random dithering. \n\n[1] Horv\u00e1th, Samuel, et al. \"Stochastic distributed learning with gradient quantization and double-variance reduction.\" Optimization Methods and Software 38.1 (2023): 91-106.\n\n[2] Haddadpour, Farzin, et al. \"Federated learning with compression: Unified analysis and sharp guarantees.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2021."
            },
            "questions": {
                "value": "1. Title. Why the paper is called \"The First Theoretically Successful Combination of Local Training and Communication Compression\" if there already exist combination of local steps and communication compression with theoretical guarantees, e.g. [1, 2]? \n2. Experiments. I wonder how the convergence changes with different choice of $p$, as it is allows to balance between local training and communication.  \n\n\n\n[1] Horv\u00e1th, Samuel, et al. \"Stochastic distributed learning with gradient quantization and double-variance reduction.\" Optimization Methods and Software 38.1 (2023): 91-106.\n\n[2] Haddadpour, Farzin, et al. \"Federated learning with compression: Unified analysis and sharp guarantees.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6965/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6965/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6965/Reviewer_5itr"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6965/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830464812,
        "cdate": 1698830464812,
        "tmdate": 1700493692856,
        "mdate": 1700493692856,
        "license": "CC BY 4.0",
        "version": 2
    }
]