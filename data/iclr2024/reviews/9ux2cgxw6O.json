[
    {
        "id": "SCluH7MCAn",
        "forum": "9ux2cgxw6O",
        "replyto": "9ux2cgxw6O",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3522/Reviewer_WhZ8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3522/Reviewer_WhZ8"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on long video editing with training-free diffusion models.  The authors propose a useful cross-window attention mechanism to ensure the consistency and length of the video. They also leverage DDIM for accurate control and a video frame interpolation model to mitigate the frame-level flickering issue. The authors presented rich and excellent experimental results, and provided their code and products in supplementary materials."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Long video editing, consistency maintenance, and structural fidelity are fundamental issues in text guided long video editing. The authors propose a simple and systematic solution for training free long video editing. The authors present a good number of experiments validating the effectiveness of their approach. The paper is overall well written and easy to follow."
            },
            "weaknesses": {
                "value": "1. The authors have pieced together too many other people's methods to achieve the goals, so their model capabilities are limited, such as being unable to perform shape transformations, object additions or deletions. So their products are limited to color or style changes.\n\n2. The innovation of the methods proposed by the authors is limited, as they have not effectively established the temporal information of the video. The so-called cross-window is naive (a minor change of ControlVideo **[1]**) and may be helpful for local video smoothing, but it still cannot truly establish the temporal dependence of long videos. This is reflected in the experimental results that although the generated video actions are locally coherent, the overall appearance is somewhat strange.\n\n**[1]** Zhang, Yabo, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo and Qi Tian. \u201cControlVideo: Training-free Controllable Text-to-Video Generation.\u201d ArXiv abs/2305.13077 (2023): n. pag."
            },
            "questions": {
                "value": "Cross-window attention is proposed to improve inter-window consistency, then how do the authors ensure consistency within the window? \nAs shown in Figure 5 (a), fully crossframe attention-based ControlVideo not only maintains the loyalty of the background, but also maintains the temporal consistency of the target subject. On the contrary, the author's method blurs the background and changes the target subject in the temporal sequence (the car window turns red)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3522/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3522/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3522/Reviewer_WhZ8"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3522/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698525115946,
        "cdate": 1698525115946,
        "tmdate": 1699636306041,
        "mdate": 1699636306041,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fnsRHtY7yn",
        "forum": "9ux2cgxw6O",
        "replyto": "9ux2cgxw6O",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3522/Reviewer_59Nz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3522/Reviewer_59Nz"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to address the problems of limited video length and temporal inconsistency in text-driven video editing task.\nIt introduces a training-free pipeline based on ControlNet (i.e., LOVECon) for efficient long video editing, including three key designs.\nIn specific, LOVECon develops a novel cross-window attention mechanism to ensure the consistency of global style.\nSecondly, it fuses the latents of source video to obtain more accurate control.\nFinally, it incorporate a video frame interpolation model to deflicker the generated videos."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "See summary."
            },
            "weaknesses": {
                "value": "1. The main contribution of long-video editing is limited in paper. The cross-window attention mechanism is common in long video editing/generation [1].\n2. Question about visualization results of ControlVideo-II [2] in Fig.3. ControlVideo-II manages to achieve fine-grained control in original paper[2] (e.g., hair color), but fails to do so in Fig.3. Could the authors explain the reason?\n\n[1] Fu-Yun Wang, Wenshuo Chen, et al. Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising.\n[2] Min Zhao, Rongzhen Wang, et al. Controlvideo: Adding conditional control for one shot text-to-video editing."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3522/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698571818935,
        "cdate": 1698571818935,
        "tmdate": 1699636305975,
        "mdate": 1699636305975,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GOg2Nw0NBN",
        "forum": "9ux2cgxw6O",
        "replyto": "9ux2cgxw6O",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3522/Reviewer_EDK1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3522/Reviewer_EDK1"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a new long video editing method based on video controlnet and text-to-image latent diffusion models. The main contributions of the proposed method are 1) a cross-clip sparse attention to ensure the consistency of the long video while saving memory, 2) a latent fusion mechanism based on attention maps of the editing target, and 3) a frame smoothing mechanism based on near frame interpolation. The experimental results on 30 videos and the CLIP-based metrics demonstrate that the proposed method has better temporal consistency and video quality than the compared baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- [writing] This paper is easy to follow, and the overall writing is good to me.\n- [method] This work tackles the challenging task that editing long videos.\n- [experiment] The visual results show that the proposed method allows the editing on the target area while maintaining the other image regions intact. Quantitative results also verify that the proposed method has better video quality and temporal consistency."
            },
            "weaknesses": {
                "value": "- [method] The first drawback of the proposed method is that it is very similar to the VideoControlNet, where the main difference is the modification to tackle long video editing cases. 1) The cross-window attention is a special sparse attention mechanism, which has been widely used by other vision models. 2) The latent fusion module has also been used in other works. 3) The frame interpolation mechanism is a long-history video frame smoothing approach. Although combining existing approaches to solve a challenging problem is meaningful, I am hesitant to give a high score on this work since the \"added\" modules are all very common and bring limited insight into the community.\n- [experiment] In the visual comparison, I find the main visual benefit of the proposed method is that it only edits the target region while keeping the other part intact, which, according to my understanding, is contributed by the latent fusion module, making it hard to evaluate the effectiveness of the cross-window attention. I think more visual (like fig. 5a) and quantitative comparisons without the latent fusion would help.\n- [experiment] The experiments are only based on 30 videos, which is far from enough to show the robustness of the proposed method. Moreover, I feel CLIP-based metrics may not be reliable enough to evaluate the overall quality of the generated videos. IS and FVD (if having a larger evaluation set as the reference) should be used for the quality comparison."
            },
            "questions": {
                "value": "No"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3522/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698805984095,
        "cdate": 1698805984095,
        "tmdate": 1699636305876,
        "mdate": 1699636305876,
        "license": "CC BY 4.0",
        "version": 2
    }
]