[
    {
        "id": "j2hghh0IdS",
        "forum": "3uITarEQ7p",
        "replyto": "3uITarEQ7p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3819/Reviewer_mFxj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3819/Reviewer_mFxj"
        ],
        "content": {
            "summary": {
                "value": "The rise of expansive, publicly accessible language models has transformed the AI landscape. These models can perform impressively, but their deployment in privacy-sensitive contexts raises concerns due to data sensitivities. As a remedy, there's growing interest in training these models using differential privacy (DP). However, the trade-off is stark: as data volume increases, ensuring privacy becomes costlier due to the need to introduce more noise. An emerging approach to this challenge is DP training of large models by fine-tuning them on limited private data\u2014a method that's proving to be highly effective. But a significant obstacle remains: the resulting models, while powerful, are often too large and inefficient for real-world deployment, especially in systems designed to serve vast user bases. This limitation has spurred research into model compression, which seeks to encapsulate the capabilities of these models without compromising utility. This study focuses on fine-tuning language model, initially pre-trained on a selectively chosen subset of publicly available data, on private data using differential privacy methods. This paper leverages insights from private data to judiciously curate a subset of public data for pre-training purposes. The proposed framework unfolds in three phases: firstly, a privacy-preserving algorithm selects a subset of public data for pre-training (referred to as selective pre-training). This is followed by non-private pre-training on this chosen public data. The final phase involves private fine-tuning using the standard DP-SGD methodology."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- **Innovative Data Selection**: The paper introduces a novel approach of leveraging private domain knowledge to judiciously select subsets of public data for pre-training. This methodology ensures relevant data selection, enhancing the model's effectiveness in domain-specific applications.\n \n- **Enhanced Performance**: The strategies employed lead to notable performance gains, demonstrating the efficacy of the proposed methods in comparison to conventional techniques.\n\n- **Optimized Model Size**: One of the standout achievements of this work is the successful reduction in model size. This not only makes the model more resource-efficient but also more feasible for deployment in real-world scenarios with resource constraints.\n\n- **Empirically Supported Findings**: The experiments presented in this work are thorough, and claims made by the authors are substantiated with appropriate evidence."
            },
            "weaknesses": {
                "value": "Notable points that can enhance the paper and improve readers comprehensibility:\n\n- In the paper, PRV accountant was chosen for the DP analysis. Could the authors elaborate on the rationale behind this selection, especially considering the moment accountant is often a more prevalent choice in similar works? Understanding the specific advantages or reasons for this choice would offer greater insight into the methodology.\n\n- In Section 4.1.1, the authors provided details about the size of the token but the size of the sequence for OpenWebText isn't specified. For clarity and consistency, it would be beneficial to include the sequence size for OpenWebText, especially since the sequence size for the target dataset is mentioned. This addition will offer a more direct comparison and help readers understand the relationship between the two datasets more effectively.\n\n- I observed that Figure 1 and Figure 4 have variations in the y-axis and gridline spacing. To enhance the clarity and ease of comparison between the two figures, it might be beneficial to standardize the grid spacing across both. This consistency would aid readers in drawing more direct comparisons between the charts.\"\n\n- In Section 1.1(Differential Privacy as a Tool For Model Compression), the authors mention, \"Observe that the performance of a tiny model with 21 million parameters can match the zero-shot performance of GPT2-XL public model with 1.5 billion parameters.\" Upon reviewing the referenced Figure, it appears that the tiny model's performance is slightly lower in terms of accuracy (approximately 33.9% for Sel. PT+DP as opposed to 35.1% for GPT-XL) and exhibits a higher perplexity (around 49 for Sel. PT+DP compared to 44 for GPT-XL). Could the authors confirm if my interpretation aligns with your findings? While the tiny model doesn't precisely \"match\" (i.e 33.9% != 35.1%) the GPT-XL's performance, it still delivers a commendable performance considering the vast difference in parameter size (21 Million versus 1.5 Billion). It might be beneficial for clarity if the authors could consider revising this section to reflect these nuances."
            },
            "questions": {
                "value": "- In Section 1.2, could you specify the particular English corpus used to train the EnglishGPT-124M model? Additionally, for the EnglishGPT-82M's training data, is it a subset of the aforementioned corpus, or is it an entirely distinct dataset that bears distributional similarities to the Enron dataset?\n\n\n- Could the authors expatiate on how they have defined the unit of privacy protection in this work? Specifically, is each sequence treated as a single datapoint such that its addition or removal wouldn't impact the algorithm's output? If so, does the sequence length have an influence on the privacy budget? In other words, would a longer sequence length entail a higher privacy cost?\n\n- \"In Appendix C, when discussing the Data Selection for the Enron Email, there seems to be a mention of the data being \"6 times larger\" than the target data (likewise 6 times smaller). However, in Section 3.1, it's indicated that negative examples are \"five times larger than the number of positive examples\". Could there be a discrepancy between these two sections? It would be helpful for clarity if these numbers are consistent throughout the paper.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3819/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698316568939,
        "cdate": 1698316568939,
        "tmdate": 1699636339482,
        "mdate": 1699636339482,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "T21i8m2nhy",
        "forum": "3uITarEQ7p",
        "replyto": "3uITarEQ7p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3819/Reviewer_CqKu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3819/Reviewer_CqKu"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel framework for training differentially private language models with a focus on model compression and efficiency. The authors propose a selective pre-training approach, which involves using a privacy-preserving algorithm to select a subset of public data for pre-training. This is followed by private fine-tuning via differentially private stochastic gradient descent (DP-SGD). The main contributions of the paper are:\n\n1. A new framework for training domain-specific language models with differential privacy, which includes selective pre-training and private fine-tuning.\n2. Demonstration that selective pre-training is crucial for smaller models to achieve better performance when privately fine-tuned with DP-SGD.\n3. State-of-the-art results on standard NLP benchmarks, outperforming previous methods in the literature.\n4. An empirical evaluation showing that smaller models trained with the proposed framework can match the performance of much larger models without access to private data, highlighting the promise of private learning as a tool for model compression and efficiency.\n\nThe paper also discusses the real-world impact of the proposed framework, as it has been used to train an industry-grade differentially private text prediction language model that serves many NLP applications in a large AI company."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors have made several concrete contributions in their paper, which can be assessed across the following dimensions:\n\n1. Originality: The authors introduce a novel framework for training differentially private language models, focusing on model compression and efficiency. They propose a selective pre-training approach, which involves using a privacy-preserving algorithm to select a subset of public data for pre-training. This approach has not been explored in the context of differentially private learning before, and it represents a creative combination of existing ideas and techniques.\n\n2. Quality: The paper is well-written and presents a clear and coherent argument. The experiments are well-designed and rigorously conducted, with appropriate baselines and a thorough analysis of the results. The authors demonstrate that their framework achieves state-of-the-art performance on standard NLP benchmarks, outperforming previous methods in the literature.\n\n3. Clarity: The paper is well-structured and easy to follow, with clear explanations of the proposed framework, the experimental setup, and the results. The authors use appropriate figures and tables to illustrate their findings, making it easy for readers to understand the key points. The paper also provides a detailed description of the implementation details, which is helpful for reproducing the experiments.\n\n4. Significance: The proposed framework has the potential to significantly advance the field of differentially private learning, particularly in the context of language models. By demonstrating that selective pre-training can lead to smaller models that match or surpass the performance of larger models without access to private data, the authors highlight the promise of private learning as a tool for model compression and efficiency. This has important implications for real-world applications, where model size and inference time are critical factors.\n\nOverall, the paper is an original and significant contribution to the field of differentially private learning, with high quality and clarity. The proposed framework has the potential to advance the state-of-the-art in training efficient and high-performing private language models, which is a key challenge in the field."
            },
            "weaknesses": {
                "value": "While the paper presents a novel framework for training differentially private language models and achieves state-of-the-art results, there are a few areas where the work could be improved:\n1. **Privacy guarantees:** The paper provides differential privacy guarantees only with respect to the private dataset and not the public dataset. It would be beneficial to extend the analysis to consider the privacy risks of the public data as well. This could involve incorporating privacy amplification techniques or exploring the composition of privacy guarantees for the entire framework.\n2. **Analysis of scaling behavior:** The paper touches upon scaling laws in private deep learning, but a more in-depth analysis of the scaling behavior of the proposed framework would be beneficial. This could involve studying how the performance and efficiency trade-offs change with model size, data size, and privacy parameters.\n3. **Generalization concerns**: As the theoretical analysis is lacking, it is reasonable to be concerned about the generalization ability of the proposed method. It would be beneficial if the authors could theoretically or empirically (i.e., with more evidences)  show its generalization ability. \nBy addressing these areas, the paper could provide a more comprehensive and robust framework for training differentially private language models, ultimately leading to more efficient and high-performing models."
            },
            "questions": {
                "value": "Overall, this method is interesting, simple, and effective (overwhelming SOTAs). I have the following suggestions:\n1. **Veritable privacy protection**: It will be interesting to discuss with authors about *what is the real privacy of the private dataset*. In the proposed method, a selected dataset is set as a proxy of private dataset. Although the selection is protected via DP, the resulting proxy dataset is *similar* to the private dataset. In this case, is the DP sitll meaningful? \n1. **Comparison with other model compression techniques**: The paper shows that the proposed framework can improve upon existing model compression techniques when used alone. However, it would be interesting to explore how the framework can be combined with other compression techniques, such as knowledge distillation or pruning, to enhance model performance and compression ratios further.\n2. **Data selection algorithms**: Are there any plans to explore more sophisticated data selection algorithms, such as those based on importance resampling or data pruning? How do these alternative methods compare to the simple classification-based approach used in the paper in terms of model performance and efficiency?\n3. **Real-world applications**: Can the authors provide more details on the real-world impact of the proposed framework, such as the specific NLP applications it has been used for and the performance improvements observed in these applications?\n4. **Reproducibility:** Can the authors provide more details on the reproducibility of the experiments, such as the exact code and data used, as well as any potential pitfalls or challenges that other researchers may encounter when attempting to reproduce the results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3819/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698585199007,
        "cdate": 1698585199007,
        "tmdate": 1699636339411,
        "mdate": 1699636339411,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RnaBKQCnwk",
        "forum": "3uITarEQ7p",
        "replyto": "3uITarEQ7p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3819/Reviewer_kdmU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3819/Reviewer_kdmU"
        ],
        "content": {
            "summary": {
                "value": "the paper discussed the impact of data selection in the pre-training step on the performance of relatively small language models, and proposed an approach that provides differential privacy guarantees to the finetuned small language models on downstream tasks. The proposed approach involves three steps:\n1. privately select training data from a large corpus using a classifier optimized using DP-SGD.\n2. non-privately pre-train a language model using the selected training data\n3. privately finetune the model on the downstream task using DP-SGD.\n\nI would like to point out that 1 & 2 doNOT provide any formal differential privacy guarantees to the selected pre-training data, which means that the claim made in this paper is not valid."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "please see the weaknesses"
            },
            "weaknesses": {
                "value": "1. training a private model that predicts whether a sample in the collected corpus is similar to that in the downstream task only provides formal privacy guarantees to the predictions themselves, but doesNOT provide privacy guarantees to the data samples. \n\n2. training a non-private model on the selected data still leaks private information regarding the selected data, and the post-processing theorem doesNOT apply here. If we want to make it differentially private, or provide privacy guarantees to the pre-trained model, either the noise needs to be injected into the training data itself, or the model needs to be optimized using DP approaches."
            },
            "questions": {
                "value": "n/a"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3819/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698696791353,
        "cdate": 1698696791353,
        "tmdate": 1699636339321,
        "mdate": 1699636339321,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "D8P74Ma6Fn",
        "forum": "3uITarEQ7p",
        "replyto": "3uITarEQ7p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3819/Reviewer_3s2h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3819/Reviewer_3s2h"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an approach to compress the size of LLMs on private data. The idea is to start a pretrain a small model from scratch and select the pretraining data smartly, i.e. close to the distribution of private data. To achieve this, the authors propose a method for selecting private data by training a binary domain classifier privately. This classifier distinguishes whether a given data point belongs to the target data distribution or not, allowing the selection of data points with higher confidence from the target domain. After the model is pretrained on the selected dataset, then it is fine-tuned on the private dataset. The efficacy of the proposed approach is demonstrated through experiments conducted on various text datasets, encompassing diverse data and model sizes."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The topic is well-motivated, as it is important to study how to compress the LLMs for sensitive usecases where private learning is a must.\n- The method presented is straightforward and easily comprehensible. The authors have provided empirical evidence demonstrating the superior performance of their approach compared to the baseline through an extensive and large-scale experimental setup."
            },
            "weaknesses": {
                "value": "- The title is a bit misleading as compression typically means compressing a larger model into a smaller one while this work focuses on how to better pretrain a smaller model from scratch.\n- The technical innovation in this study is relatively constrained. The idea of training a classifier for selecting better data has been adopted in GPT3. Previous works, such as [1], have already introduced domain-specific pretraining, and techniques for data selection in training language models have been introduced in [2, 3]. The innovation of this work is the private learning of the domain classifier, which is limited. \n- Some other baseline are missing: 1) if one knows the domain of the private task, then one could select pretraining data in that domain without having to perform the DP data selection. 2) zero-shot learning of compressed or quantized larger models. \n\nReference\n\n[1] Gururangan, Suchin, et al. \"Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks.\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020.\n\n[2] Moore, Robert C., and William Lewis. \"Intelligent selection of language model training data.\" Proceedings of the ACL 2010 conference short papers. 2010.\n\n[3] Ruder, Sebastian, and Barbara Plank. \"Learning to select data for transfer learning with Bayesian Optimization.\" Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 2017."
            },
            "questions": {
                "value": "- DP training is known for introducing worse calibration to the classifier. How might that impact the data selection process?\n- One can also select the public dataset with DP to pretrain a large model, and then compress and finetune it on private data. How does this compare with pretraining a smaller model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3819/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772481542,
        "cdate": 1698772481542,
        "tmdate": 1699636339238,
        "mdate": 1699636339238,
        "license": "CC BY 4.0",
        "version": 2
    }
]