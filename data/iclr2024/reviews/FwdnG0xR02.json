[
    {
        "id": "O31j3Mfljq",
        "forum": "FwdnG0xR02",
        "replyto": "FwdnG0xR02",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7995/Reviewer_YKtn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7995/Reviewer_YKtn"
        ],
        "content": {
            "summary": {
                "value": "This work aims to address and understand biases in vision-language datasets, specifically the bias between the background context and the gender of persons referenced or appearing in the data. Demonstrating their approach on the COCO Captions dataset with CLIP-based models, the authors propose a method for modifying the dataset with synthetically generated \u201ccontrast sets\u201d where the gender of the subject is edited using InstructPix2Pix and the background fixed, such that the dataset can be balanced in gender."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This work tackles the important problem of understanding and addressing dataset bias.\n* The paper is relatively easy to understand.\n* To the best of my knowledge, this appears to be a novel approach for editing an attribute (gender) in an image and observing how it impacts dataset bias."
            },
            "weaknesses": {
                "value": "* Looking at qualitative examples of the GenSynth and GenSwap gender-edited images (Fig 1 in the appendix), the resulting images still have obvious artifacts, even after the various steps taken to verify quality and filter out low-quality images. From what I can tell, the authors have not really addressed how these artifacts may be a factor in their results.\n* I would be interested to see a comparison with other image editing methods other than InstructPix2Pix that may produce higher quality contrast sets. How does varying the quality of the edited images affect the observed gender biases for the different models?\n* The work only debiases for a single attribute (gender), and does not delve into what debiasing multiple attributes may look like. This limits the potential impact of this work (it\u2019s not quite usable in practice, and the observations about bias in CLIP are also limited to gender only).\n* The instructions for InstructPix2Pix only use gender editing, and does not control for other variables (e.g. skin tone, race)."
            },
            "questions": {
                "value": "* The Wang et al. 2021a paper is referenced frequently; it would be useful to the reader to discuss in more depth how this work differs from the former.\n* There is limited discussion of the CLIP models used; given how prominently they feature in the experiments, I would expect the authors to provide more background for the reader.\n* In general, I found that the Appendix contained a lot of relevant information (e.g. qualitative examples) that I would have liked to see in the main paper.\n* Section 3.2 \u201cExtracting Image Gender Labels...\u201d: The authors mention that the \u201cimages may be incorrectly labeled as undefined\u201d. What percentage of the images were mislabeled?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7995/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698210315898,
        "cdate": 1698210315898,
        "tmdate": 1699636984954,
        "mdate": 1699636984954,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ws32uuCxM1",
        "forum": "FwdnG0xR02",
        "replyto": "FwdnG0xR02",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7995/Reviewer_E6KN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7995/Reviewer_E6KN"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel gender-debiased dataset named GENSYNTH. The dataset aims for gender balance and context independence, achieved through the use of image generation models guided by prompts, which allow for control over the gender expression of the generated images, ranging from masculine to feminine. The experimental results, as depicted in Table 3, demonstrate that the utilization of the GENSYNTH dataset effectively reduces gender bias."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Addressing this issue is crucial for the machine learning research community.\n+ The methodology employed presents a plausible solution for mitigating gender bias.\n+ Utilizing generative models as a strategy for reducing bias holds significant promise."
            },
            "weaknesses": {
                "value": "I recognize that this paper addresses a critical issue; however, I believe it is not yet suitable for publication due to numerous absent discussions.\n\n- Technical novelty is weak:\nWhile the methodology is intriguing and the results are persuasive, the strategy appears somewhat simplistic, essentially constituting an application of generative models.\n\n- More detailed discussion should be conducted:\nThe instance depicted in Fig. 2 is gender-neutral. Nonetheless, in certain scenarios, other contexts might exhibit stronger gender biases, such as facial hair, attire, etc. Thus, merely manipulating images to appear more masculine or feminine does not consistently resolve the issue. The authors should explore and discuss the potential repercussions of generating new \"noisy\" or \"misleading\" examples.\n\n- More detailed discussion on bias freeness is needed:\nThe experimental results section predominantly presents numerical data, with a relatively modest improvement in performance compared to the baseline. The authors are encouraged to illustrate typical instances where gender bias mitigation is evident, as well as highlight persisting challenges, acknowledging that complete eradication of gender bias is unfeasible with the proposed method.\n\n- The potential introduction of new biases requires careful consideration:\nThe generative models are already acknowledged to be biased. The authors should thoroughly investigate potential risks associated with utilizing generative models for debiasing objectives. For example, in Fig. 3, adjustments are made to not only facial features but also skin color and clothing color, potentially leading to the inception of new biases. Furthermore, it is observed that the generative models seem to underrepresent Asian faces, potentially introducing additional biases."
            },
            "questions": {
                "value": "Please answer to my \u201cweakness part.\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7995/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7995/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7995/Reviewer_E6KN"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7995/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758995803,
        "cdate": 1698758995803,
        "tmdate": 1699675219265,
        "mdate": 1699675219265,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DU4SMTL7aV",
        "forum": "FwdnG0xR02",
        "replyto": "FwdnG0xR02",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7995/Reviewer_Nd3Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7995/Reviewer_Nd3Y"
        ],
        "content": {
            "summary": {
                "value": "In this work, author target the problem of data debiasing, and propose a pipeline to augment the COCO to generate an synthetic, gender-balanced contrast sets, by editing the person in the image without shifting the background, to prevent spurious relationship between gender and background. By using the generated dataset, authors shows that the conventional metric for measuring model bias, are highly biased by the bias from dataset. Author appeal for attention on dataset debiasing to the community."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. Author identify an vital problem in existing metric of measuring model debiasing, is less accurate due to being skewed by dataset bias. This provide new insight to the community and could be potential impactful.\n2. Author also identify background bias as a vital source for gender bias by showing result from spurious correlations classifier. \n3. Author provide viable framework to generate balanced dataset without spurious relationship, and propose a dataset under this framework.\n4. Most discussion of the paper is clear and easy to follow."
            },
            "weaknesses": {
                "value": "1. In this work, author only adopted COCO Captions dataset for tasking and gender as the debias attribute. It would be more convincing if author provide discussion and empirical result of how does insight draw from this work also applicable to other dataset and attribute. \n2. In section 3 and 5, author shows that standard metric are biased by spurious relationship in dataset itself. However, there's no followup discussion on how to measure model bias over constructed balance, spurious-relationship free dataset."
            },
            "questions": {
                "value": "The insight that spurious relationship between background pixel and foreground object seems to be generalizable beyond VLM and caption based dataset.  It would be helpful if authors can also provide some discussion on that. Also how does this insight be potential beneficial to other field like bias-free image generation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7995/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7995/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7995/Reviewer_Nd3Y"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7995/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699172602702,
        "cdate": 1699172602702,
        "tmdate": 1699636984720,
        "mdate": 1699636984720,
        "license": "CC BY 4.0",
        "version": 2
    }
]