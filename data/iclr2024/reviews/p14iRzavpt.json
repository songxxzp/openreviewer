[
    {
        "id": "KPAT1psMUf",
        "forum": "p14iRzavpt",
        "replyto": "p14iRzavpt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9063/Reviewer_QQFu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9063/Reviewer_QQFu"
        ],
        "content": {
            "summary": {
                "value": "The authors propose perturbing the conventional KL-divergence loss used for knowledge distillation in order to reduce the possible distributional shift between the teacher predictions and the ground-truth targets. They do so by using the Maclaurin series and perturbing the leading $M$ terms of this series and show a procedure to obtain good perturbation coefficients. They support their findings with both empirical and theoretical arguments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of perturbing the Maclaurin series of the conventional KL divergence is interesting and simple.\n- The empirical results (both simulated and real datasets) shows that their proposed loss indeed can improve performance across various datasets.\n- The method is theoretically grounded, and in general quite simple."
            },
            "weaknesses": {
                "value": "- Only the case of no known ground-truth targets (i.e. unlabeled distillation data) is considered, but this is only mentioned briefly on page 3, and should be made clear much earlier. Either way, it would be relevant to compare the proposed procedure to incorporating a weighted ground-truth loss as is common in many applications, as this also shifts the matching distribution towards the ground-truth distribution.\n- There is no common consensus on the aim of distillation procedures; whether it is to match the teacher as well as possible (see e.g. [1]) or to get the best-performing student on the ground-truth data (see e.g. [2]). The success of self-distillation is due to not perfectly matching the teacher, and such nuance should be evident from your introduction. Currently, it merely states that matching an imperfect teacher is suboptimal. Also, what is your definition of bias here?\n- You argue in the introduction that a grid search for the temperature is computationally expensive, but your proposed method still requires a search over the perturbation coefficients. Also, how does the search-time scale with $M$, number of sample perturbations, and validation set size?\n- Figure 4.b.: What is the point of this experiment? Injecting perturbations and choosing such perturbations randomly is very unlikely to cause an improvement in the performance. Your method is indeed outperforming random selection, but it also should?\n- Missing references to theoretical distillation works: Phuong and Lampert \"Towards Understanding Knowledge Distillation\", Mobahi et al. \"Self-Distillation Amplifies Regularization in Hilbert Space\", and Borup and Andersen \"Even your Teacher Needs Guidance: Ground-Truth Targets Dampen Regularization Imposed by Self-Distillation\". Also, comparison to the continuous categorical distribution would be highly relevant; Gordon-Rodriguez et al. \"The continuous categorical: a novel simplex-valued exponential family\".\n\nMinor:\n- Throughout the paper (especially Sections 2 and 3) you have inconsistent use of transposes of the various vectors. E.g. should $y_n$ in (2) be transposed?\n- \"This observation also resonates with our intuition that an accurate, well-calibrated, and certain teacher [...]\". How is well-calibrated and certain not contradictions here?\n\n\n[1] Stanton et al. \"Does Knowledge Distillation Really Work?\"\n\n[2] Beyer et al. \"Knowledge distillation: A good teacher is patient and consistent\""
            },
            "questions": {
                "value": "- How does your proposed PTLoss compare to using e.g. the likelihood of the continuous categorical [3,4]?\n- How does Theorem 1 differ from Proposition 3 and (8) in [5]? They appear equivalent.\n\n[3] Gordon-Rodriguez et al. \"Uses and Abuses of the Cross-Entropy Loss: Case Studies in Modern Deep Learning\"\n\n[4] Gordon-Rodriguez et al. \"The continuous categorical: a novel simplex-valued exponential family\"\n\n[5] Menon et al. \"A Statistical Perspective on Distillation\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9063/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9063/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9063/Reviewer_QQFu"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9063/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698397981738,
        "cdate": 1698397981738,
        "tmdate": 1699637141297,
        "mdate": 1699637141297,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1AuuFEg7vy",
        "forum": "p14iRzavpt",
        "replyto": "p14iRzavpt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9063/Reviewer_Bme6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9063/Reviewer_Bme6"
        ],
        "content": {
            "summary": {
                "value": "This paper argues that the teacher\u2019s output distributions can be biased from the ground truth due to various factors. Instead of forcing an out-and-out imitation of the original teacher model, the proposed PTLoss moderates the distillation objective by adding perturbations to the standard KL loss."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThis paper reviews the KL loss in knowledge distillation and proposes a good arguments that the teacher\u2019s output distributions can be biased from the ground truth due to various factors.\n2.\tThe proposed PTLoss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground truth distribution."
            },
            "weaknesses": {
                "value": "1.\tMore experiments details should be given instead of putting some statistical results."
            },
            "questions": {
                "value": "1.\tFigure 3 (a) is shown to verify our assumption that the teacher outputting a distribution closer to the ground truth distribution leads to a better student. Why the OHT (grey) method achieves better accuracy than LS (yellow) with lager L2-distance between pt and p*\n2.\tHow to quickly find a certain m in   ?\n3.\tHow to make sure that the added perturbations term is beneficial for the learning of knowledge distillation\uff1f\n4.\tIn Figure 1,   is  unknown, how to make sure that \u201cPTLoss implicitly shift   to    such that   \u201c?\n5.\tHow long to select the perturbation coefficients for each training dataset?\n6.\tThe derivation from Eq.4 to Eq.5 is poor. If Eq.11, if   is known, why do not use ground-truth directly?\nConcerns:\nAlthough the proposed coefficients selection method provides a principal way to determine the perturbation hyperparameters, it remains challenging to scale up the number of classes and the perturbation order."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9063/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734990044,
        "cdate": 1698734990044,
        "tmdate": 1699637141184,
        "mdate": 1699637141184,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uhcpesYevl",
        "forum": "p14iRzavpt",
        "replyto": "p14iRzavpt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9063/Reviewer_5qNN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9063/Reviewer_5qNN"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a new distillation objective (PTLoss) motivated by the inherent limitation of the KL-based distillation loss by which the student learns to match the distribution of the teacher ignoring the difference between the latter and the ground truth distribution. Their objective instead adds perturbations to the standard KL loss via a Maclaurin expansion whose (leading) coefficients are set to better match the distribution differences between the teacher and the ground truth. They provide theoretical justification for the latter, draw connections between the proposed an objective and other existing approaches, and present experiments on a variety of NLP datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed objective (PTLoss) is conceptually simple, well motivated and the theoretical results in Section 4.1 justify the motivation. Connections between the proposed approach and existing alternatives are provided (in the Supplementary Material). Experiments are (for the most part) convincing and that showing the effectiveness of the perturbation coefficient search is particularly welcome."
            },
            "weaknesses": {
                "value": "The quality score in (11) requires a set of labels, y, for the validation set which is described in the text as an unbiased estimator for p^*. However, it is not sufficiently clear whether these are ground-truth labels or estimated somehow. For the latter, the problem is that is not described how to obtain such unbiased estimates and for the former, that the model requires ground truth labels, which is problematic because is not consistent with the problem formulation clearly stating \"we are given an unlabeled distillation set\", not mentioning a validation set. Moreover, if such labels are available, one could for instance optimize the student with those and regularize with the standard distillation loss.\n\nThough equations (8) and (9) are well motivated, a more rigorous theoretical justification (via bounding) will strengthen the claims of the proposed objective, particularly in relation to the statements that the final learned student model and the second term of (9) becoming zero.\n\nThe results in Table 1 for all methods are an average over three trials, however, the variation is not presented.\n\nFrom Figure 4(b) is not clear why the authors used M=5 in their main experiments. Moreover, it is not discussed in general how to select M or the reasoning for using M>1 given the results in Figure 4(b)."
            },
            "questions": {
                "value": "Why using L2 in Figure 3(b) which is consistent with (11), but TVD in Figure 4(a)?\n\nWhat is used as validation set in the main experiments, Dev in Table 2? Is there a relationship between the size of the validation set relative to the distillation set and performance gain?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9063/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698908561163,
        "cdate": 1698908561163,
        "tmdate": 1699637141038,
        "mdate": 1699637141038,
        "license": "CC BY 4.0",
        "version": 2
    }
]