[
    {
        "id": "S2egwMTGj9",
        "forum": "rxD2ZCExRG",
        "replyto": "rxD2ZCExRG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1666/Reviewer_yjZC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1666/Reviewer_yjZC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed to solve the novel task of text-driven whole-body motion generation, which generates body, hand, and face motion simultaneously given a text description. The proposed method includes a holistic hierarchical VQ-VAE for hand and body motion encoding, a hierarchical GPT for hand and body motion generation, a cVAE for facial expression, and a pretrained text and motion alignment model. The paper also proposed to evaluate the alignment between text and motion with a novel evaluation metric TMR-R-Precision(256) and TMR-Matching Score. Experiments were conducted on the Motion-X, GRAB, and HumanML3D datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This is the first paper to generate holistic and vivid motions with body, hand, and facial expressions.\n- The authors pretrained a text-motion retrieval to align the text and motion embedding, bypassing the semantic gap between CLIP-based text embedding and motion.\n- The proposed methods show a clear advantage against SOTA motion generation methods in common metrics, except for multi-modality and diversity."
            },
            "weaknesses": {
                "value": "### Major issue\n\n- The paper emphasized generating \u201cvivid motions,\u201d and the key for a motion to be vivid is to have vivid facial expressions. However, the proposed solution does not show promising results in facial expression generation. The facial expression part is not evaluated against any baseline method, and the facial cVAE is disconnected from the other parts of the proposed method.\n\n### Minor issues\n\n- In section 2.1, $F$ should be $L$?\n- In section 2.2, the sentence is repeated twice\n    \n    > In the encoding phase, we input hand and body motions, yielding hand and body tokens through the hand encoder EncH(\u00b7) and the body encoder EncB(\u00b7),  respectively.\n    > \n- In Appendix C.1, Algorithm 1, line 4 in the for loop, $\\hat{z}^B=..., \\hat{z}^B));\\mathcal{C}^B);$ the second $\\hat{z}^B$ should be $z^B$."
            },
            "questions": {
                "value": "- What\u2019s the reason behind the name hierarchical-GPT? The model seems to interleave instead of hierarchical to me.\n- Why is it that in H2VQ, the hand is encoded before the body, and in Hierarchical-GPT, the body token is predicted before the hand? Is there any reason besides empirical performance advantages?\n- To what extent can the method generalize to OOD text descriptions?\n- In the supplementary video, in the T2M-GPT examples, the characters are all moving backward. Please confirm if the examples are rendered correctly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1666/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1666/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1666/Reviewer_yjZC"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1666/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698309821715,
        "cdate": 1698309821715,
        "tmdate": 1699636094607,
        "mdate": 1699636094607,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6N883PAd5z",
        "forum": "rxD2ZCExRG",
        "replyto": "rxD2ZCExRG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1666/Reviewer_JrQp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1666/Reviewer_JrQp"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the text-driven whole-body motions generation, including facial expressions, hand gestures, and body movements. The proposed framework consists of a holistic hierarchical VQ-VAE to compress the whole-body motion into two-level discrete codes. It also features a hierarchical-GPT model that predicts the discrete motion codes from input textual descriptions in an auto-regressive manner. Additionally, the author proposes a pre-trained text-motion-alignment model to enhance the alignment between given text and generated motions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is the first to target the text-driven whole-body motions generation task, aiming at generating high-quality, diverse, and coherent facial expressions, hand gestures, and body motions.\n\n2. The paper introduces a novel framework for text-driven whole-body motion generation, featuring a Holistic Hierarchical Vector Quantization for learning informative and compact representations at low bit rates, along with a Hierarchical-GPT for predicting hierarchical discrete codes for body and hand motions in an autoregressive manner.\n\n3. The paper proposes a pre-trained text-motion alignment model, which serves to provide textual embeddings instead of commonly used CLIP embeddings. Furthermore, it offers motion-text alignment supervision during the training process."
            },
            "weaknesses": {
                "value": "In my view, the proposed H2VQ and Hierarchical-GPT just extend the model introduced in T2M-GPT by incorporating hand gesture modeling. These modifications are rather straightforward. Firstly, in the context of vector quantization, they integrate the hand pose vector quantization with the body pose using a hierarchical strategy rather than directly quantizing the whole body pose. Secondly, The T2M-GPT has been modified to decode the body pose code and hand pose code alternately, rather than directly outputting the whole body pose code. The utilization of TMR for encoding textual descriptions is a more intelligent choice compared to CLIP embedding, and the incorporation of motion-text alignment supervision appears to be beneficial during the training. However, it's important to note that text-motion alignment has been employed in various previous works, including TEMOS, and the proposed TMR merely adopts a contrastive learning way through a retrieval target. The paper presents a substantial amount of contributions, but the technical designs lack novelty and a certain level of appeal from my perspective. As a result, I would recommend a rating of marginally above the acceptance."
            },
            "questions": {
                "value": "1. Currently, is it feasible or essential to generate diverse and realistic human poses and facial expressions using the available datasets? To my knowledge, most of the existing datasets lack diversity and realism in hand poses and facial expressions. From visualization results, I can discern certain minor distinctions in hand poses, although they may not be highly realistic, and I cannot find the differences in the generated facial expressions.\n\n2. How about the comparison with a simple baseline that directly combines SOTA models for facial expression, hand pose, and body pose generation?\n\nMinor Fix:\n\n1. On page 3, where it mentions, \"where $F$ and $d$ denote the number of frames and the dimension ...\", it may be advisable to replace $F$ with $L$ to maintain consistent notation.\n\n2. On page 4, there is a repetition of the sentence: \"In the encoding phase, we input hand and body motions, ...\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1666/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1666/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1666/Reviewer_JrQp"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1666/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698735971218,
        "cdate": 1698735971218,
        "tmdate": 1699636094532,
        "mdate": 1699636094532,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rYG9GN31qf",
        "forum": "rxD2ZCExRG",
        "replyto": "rxD2ZCExRG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1666/Reviewer_bynW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1666/Reviewer_bynW"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a framework for whole-body motion generation from text. It includes several core designs: 1) a holistic hierarchical VQ-VAE based on RVQ for body and hand motion reconstruction; 2) a hierarchical GPT for predicting fine-grained body and hand motions; 3) a pre-trained text-motion-alignment model used as a prior for text-motion generation stage explicitly; 4) a text-motion alignment supervision in the GPT preditor. Comprehensive experiments verify that the proposed model has significant advantages both quantitively and qualitatively."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors pioneered the task of whole-body motion (including the face and hand motion) generation from speech. To generate fine-grained hand and face motions, two core technique designs were introduced: 1) a holistic hierarchical VQ-VAE based on RVQ for body and hand motion reconstruction; and 2) a hierarchical GPT-based generation. To achieve a good alignment between text and motion, a text-motion retrieval model is pre-trained and used as a prior for the text-motion generation stage explicitly. Extensive quantitative and qualitative experiments were conducted to demonstrate the efficiency of the proposed method."
            },
            "weaknesses": {
                "value": "1. The technical contribution of this paper seems somewhat limited in the following:\n(1) the pipeline of the method is similar to the T2M-GPT where a VQ-VAE is used for motion reconstruction and a transformer-based GPT model is used for motion generation, while the tasks are different; \n(2) the pretraining of a motion encoder and a text encoder via aligning text and motion in a contrastive way is also not new such as TMR, and further using it to replace the clip is natural in the prediction stage. \n2. From the example of the visualizations, the textual description for facial motions focused on emotion (like happily, angrily), but the generated face shown in the paper is static, rather than dynamic motions, which lacks the demonstration of the emotion dynamics."
            },
            "questions": {
                "value": "Except for the above weaknesses, there are a few questions as follows: \n1. In Table 2, about the H2VQ, what is the size of the codebooks for reconstructing the body and hand motion? Besides, for vanilla VQ-VAE and RVQ, do you separately model the hand and body motions and then combine the motion as body-hand motions? How is the performance for VQ-VAE and RVQ when increasing from 1024 to 4096?\n2. Regarding facial generation,  what the text embedding is used? Clip or TMR?\n3. Since the hand & body, and face motions are separately modeled, how is the coherence of the generated motions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1666/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698749800360,
        "cdate": 1698749800360,
        "tmdate": 1699636094418,
        "mdate": 1699636094418,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "spZw4mFfWU",
        "forum": "rxD2ZCExRG",
        "replyto": "rxD2ZCExRG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1666/Reviewer_MYWU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1666/Reviewer_MYWU"
        ],
        "content": {
            "summary": {
                "value": "This paper is the first work that can generate whole-body motion from text description. This paper first proposes a Holistic Hierarchical Vector Quantization (H$^2$VQ) scheme to model the correlation between body and hand. Authors notice that facial expression is largely independent of body and hand, so they train a conditional VAE to generate facial motion independently. This scheme is reasonable, which is also verified in the task of SMPL-X reconstruction. Compared with the H$^2$VQ and facial cVAE, the text-to-motion alignment module is more interesting. If the author can release this module as claimed in their manuscript, this module will bring some meaningful progress to the community of text2motion."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors propose a promising task and give a thoughtful solution. From the results, we can easily judge the effectiveness of the proposed method. The writing is also very fluent."
            },
            "weaknesses": {
                "value": "1. The title '3.1.2 Evaluation' somehow is easily misleading. In this section, you introduced the evaluation metrics and compared methods. How about changing to 'Evaluation Details'?\n 2. I feel that the focus of this article is on how to generate physical movements in the hands. The discussion about facial cVAE is limited, and I have not found any experiments to analyze this module. \n3. Although it's hard to find compared methods in text-aligned whole-body motion generation, the authors can compare solely body generation results with previous works. But I didn't find this part. There are too few methods for comparison."
            },
            "questions": {
                "value": "My questions have been listed in the above Weaknesses. I have one more question on this paper: From visual results, I noticed some physically implausible artifacts, such as foot sliding. Can you give some discussion on this point?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1666/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1666/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1666/Reviewer_MYWU"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1666/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699207475794,
        "cdate": 1699207475794,
        "tmdate": 1699636094358,
        "mdate": 1699636094358,
        "license": "CC BY 4.0",
        "version": 2
    }
]