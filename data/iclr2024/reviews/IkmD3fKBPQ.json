[
    {
        "id": "r40GN1Wttt",
        "forum": "IkmD3fKBPQ",
        "replyto": "IkmD3fKBPQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2828/Reviewer_oT3c"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2828/Reviewer_oT3c"
        ],
        "content": {
            "summary": {
                "value": "Large Language Models (LLMs) have been increasingly capable. However, they still make many mistakes. Recent work has explored the idea of \u201cself-correction\u201d where LLMs refine their responses based on feedback to their previous outputs. This paper critically examines the role and efficacy of self-correction within LLMs. Central to the investigation is the definition of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, with no external feedback. The paper finds that LLMs struggle to self-correct their responses for reasoning tasks without external feedback, and at times, their performance might even degrade post self-correction."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. Contrary to prior results, the paper finds the self-correct methods in prior research such as Kim et al. (2023); Shinn et al. (2023) make use of oracle labels to guide the self-correction process. \n\n2. For self-correction through multi-agent debate (Du et al., 2023; Liang et al., 2023) to improve reasoning where multiple instances of an LLM critique each other\u2019s responses, the paper's results reveal that its efficacy is no better than self-consistency when considering an equivalent number of responses, highlighting the limitations of such an approach."
            },
            "weaknesses": {
                "value": "1. Intrinsic self-correction, defined as the model endeavors to rectify its initial responses based solely on its inherent capabilities without the crutch of external feedback, is not very clear to me. Does recall examples from parameter knowledge (see paper below) considered intrinsic self-correction? \nLarge Language Models as Analogical Reasoners\nhttps://arxiv.org/pdf/2310.01714\n\n2. It is not clear which prior papers have the various problems exposed. It would be very helpful to put them in a table. Furthermore, please provide details on where prior methods rely on oracle labels, specific problems on poorly constructed pre-prompts, specific benchmark results that are wrong. For example, I do not seem to locate which part Shinn et al. (2023) have the problems."
            },
            "questions": {
                "value": "The paper exposes an intriguing problem in prior work on self-correction of LLMs that shows, to the contrary that LLMs can not self-correct reasoning yet.\n\nHowever, it is still not clear how we should think about all the techniques to improve reasoning without external feedback. Does breakdown a problem into sub-problem and do step-wise verification paired with self-consistency considered self-correction? It would be very helpful to put all these techniques into perspective."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2828/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698554889229,
        "cdate": 1698554889229,
        "tmdate": 1699636226491,
        "mdate": 1699636226491,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3vvEHgZfcp",
        "forum": "IkmD3fKBPQ",
        "replyto": "IkmD3fKBPQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2828/Reviewer_N1pE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2828/Reviewer_N1pE"
        ],
        "content": {
            "summary": {
                "value": "The paper studies *intrinsic* self-correctness in LLMs in the context of reasoning, where no external feedback is provided to the language model i.e., just simply asking the model to detect a mistake in its output and fix it. Through experiments over three reasoning tasks (GSM8K, Commonsense QA, and HotpotQA), the paper does the following. First, it argues that what is currently referred to in the literature as \"self-correctness\" where external feedback is provided (e.g., whether the final answer is correct) is not practical and we should focus on settings where we do not know the answer. Second, self-consistency is a strong baseline and similar approaches such as multi-agent debate methods should be considered an instance of voting rather than self-correction methods. Third, to assess whether LLMs actually have the capacity for self-correction, the authors emphasize the importance of designing a good initial (pre-hoc) prompt that performs well as opposed to using a suboptimal initial prompt, where providing additional information in the feedback prompt can be useful.  Overall, the paper argues that current LLMs fall short when prompted to self-correct their reasoning and when no additional information is provided in the feedback prompt."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper studies an important direction that is now taking over the LLM scene and brings a fresh perspective on how good SoTA LLMs are at detecting their own errors. \n* Focusing on *intrinsic self-correction* is much needed in the current \"sea\" of self-correction papers.\n* The experimental design is sound. I liked the random guessing baseline with Commonsense QA.  \n* I have to say I enjoyed reading the paper: the flow is natural, the writing is good, and most of the arguments are intuitive and make sense. \n\nOverall the community would certainly benefit from this paper gaining wider visibility."
            },
            "weaknesses": {
                "value": "* I find the explanation in section 3.2.1\u2014why post-hoc prompting can lead the model to go from a correct to an incorrect answer\u2014unsatisfying. We know that the feedback prompt is changing the model output somehow. The question is *why?* I suggest providing more intuition here.\n* The paper discusses the issue but does not provide any hint at a potential solution. I understand this is not the point of the paper but hinting at potential directions to improve intrinsic self-correctness could make the paper even more valuable. \n* The authors focus only on ChatGPT and GPT-4. I think the community could benefit from seeing that the results discussed generalize to other open-source LLMs such as LLaMA"
            },
            "questions": {
                "value": "* Have you tried combining Multi-agent debating with self-consistency? \n* How much effort did you invest into finding the self-correct prompt you used \"Review your previous answer and find problems with your answer\"? Can't different variations of this same prompt lead to different results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2828/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2828/Reviewer_N1pE",
                    "ICLR.cc/2024/Conference/Submission2828/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2828/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698721212735,
        "cdate": 1698721212735,
        "tmdate": 1700550040272,
        "mdate": 1700550040272,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "b2CsLfx0dr",
        "forum": "IkmD3fKBPQ",
        "replyto": "IkmD3fKBPQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2828/Reviewer_No3p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2828/Reviewer_No3p"
        ],
        "content": {
            "summary": {
                "value": "This work investigates prior reports that LLMs can self-correct their own responses when prompted to do so. A critical distinction is made between self-correction with and without external feedback (termed intrinsic self-correction). The results show that given the same self-correction prompt used in prior work on particular benchmark datasets, LLMs often do not succeed in self-correcting their responses without external feedback."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Self-correction of today\u2019s LLMs is a highly significant topic. The paper clearly points out the crucial distinction between self-correction with and without feedback, and sheds light on the latter case (intrinsic self-correction). The usage of oracle labels to terminate self-critique is also examined.\n\nThe paper\u2019s organization and writing quality is uniformly high, making it a pleasure to read."
            },
            "weaknesses": {
                "value": "The most serious weakness of the paper is its misleading title, which baldly asserts a claim unsupported by the analysis and results. The words \u201ccannot\u201d and \u201cyet\u201d imply that even today\u2019s most capable LLMs (GPT-4) obtain zero benefit from self-correction in nearly all cases. The abstract quickly tones down the claim by saying \u201cour research indicates that LLMs struggle to self-correct their responses without external feedback\u201d, but even that statement goes beyond what is actually demonstrated by the experiments. A more properly measured title for this work would be \u201cReexamining the Ability of Large Language Models to Self-Correct\u201d. This is more like statements that appear later in the paper:  \u201cwe provide insights into the nuances of LLMs\u2019 self-correction capabilities\u201d, and \u201ctheir self-correction capabilities, particularly in reasoning, are still nascent.\u201d\n\nThe second major problem is the work\u2019s heavy reliance on a single, loaded prompt:\n\u201cReview your previous answer and find problems with your answer\u201d\n\nPractitioners in the rapidly moving field of prompt engineering recognize this as a highly leading prompt, essentially telling the LLM that problems do exist in the previous answer. This typically causes the LLM to find problems that aren\u2019t actually present. As the paper says at one point:  \u201ccareful consideration of prompt design is essential\u201d.\n\nHere\u2019s a longer list of important factors used routinely in prompt engineering that would be needed for a proper study of LLM self-correction:\n\n- Focus on GPT-4, since its capabilities are known to be significantly greater than those of GPT-3.5. This is reflected in Table 3. \n\n- Include diagrams like those in Figure 1 for GPT-4, not just for GPT-3.5.\n\n- Evaluate a set of reasonable, unbiased self-correction prompts. For instance: \u201cAssume that this answer could be either correct or incorrect. Review the answer carefully and report any serious problems you find.\u201d\n\n- Focus on the zero-temperature setting, since that\u2019s far less prone to spurious hallucination than 1. \n\nIn discussing the option of trying other self-correction prompts (\u201cSuch a search essentially leverages feedback from humans or training examples.\u201d), the paper conflates feedback received on a per-problem basis (as when using an oracle), with feedback received from the results of multiple self-correction prompts across entire datasets. The former does indeed go beyond the definition of intrinsic self-correction, but the latter is merely hard work. \n\nThe paper says that \u201cOur main objective is to encourage a more critical examination of self-correction experiments.\u201d But readers expect this paper to be a critical examination of that nature, not just a call for critical examination.\n\nThe paper also states that \u201cin the reasoning tasks studied in this paper, we did not observe any improvement through self-correction.\u201d That\u2019s true enough, but as pointed out above, the study was not carried far enough to shed much light on the general question of how reliably LLMs can self-correct.\n\n**Post-rebuttal Comments**\n\nI commend the authors for performing the additional experiments reported in Appendix B.2, using GPT-4 with an unbiased feedback prompt and zero temperature. These results on these two datasets are interesting, so I have raised my assessment of the paper\u2019s contribution from 2 to 3. \n\nI still view these limited results as insufficient to support the broad claim made by the paper\u2019s title:  \u201cLarge Language Models Cannot Self-Correct Reasoning Yet\u201d. The authors argue that the title\u2019s claim is restricted to \u201creasoning\u201d tasks, but this is not much of a restriction at all. \n\nRegarding other feedback prompts used in the experiments, the ones in Appendix B.1 only apply to GPT-3.5, which is widely known to not be good at self-critique. The authors imply that the feedback prompts from prior works are different, but Appendix A shows only the single, loaded prompt: \u201cReview your previous answer and find problems with your answer\u201d\n\nFor these reasons, I still view the work as fundamentally unsound, in the sense that the limited findings do not justify the headline-grabbing title of the paper.\n\n**Additional Post-rebuttal Comments**\n\n\n*we do not fully understand the argument that \"The authors argue that the title\u2019s claim is restricted to \u201creasoning\u201d tasks, but this is not much of a restriction at all.\"*\n\n\nLLMs are strong at memorization, but struggle with many kinds of reasoning. So while self-critique can be applied to both memorization and reasoning problems, application to reasoning is of greater interest and is being intensely studied. For this reason, restricting the consideration of self-critique to reasoning is not much of a restriction at all. \n\n\n\n*we will change the title to **\"Reexamining the Ability of Large Language Models to Self-Correct Reasoning\"** in the final version.*\n\n\nThis title would be in line with the experiments. So on the assumption that this will indeed be the title, I'm raising my rating from 3 to 6."
            },
            "questions": {
                "value": "Section 3.2 says \u201cIntuitive Explanation. If the model is well-aligned and paired with a thoughtfully designed initial prompt, the initial response should already be optimal\u201d  But why should this be intuitive or expected? Wouldn\u2019t similar reasoning conclude that the value of chain-of-thought prompting is itself unintuitive?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2828/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2828/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2828/Reviewer_No3p"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2828/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698956171633,
        "cdate": 1698956171633,
        "tmdate": 1700533350404,
        "mdate": 1700533350404,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TUaMWym9fv",
        "forum": "IkmD3fKBPQ",
        "replyto": "IkmD3fKBPQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2828/Reviewer_yEtd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2828/Reviewer_yEtd"
        ],
        "content": {
            "summary": {
                "value": "LLMs apparently have the ability to self-correct, as evidenced by previous publications. In the current article, which acts partly as a survey, the authors investigate how well LLMs can actually self-correct intrinsically and report nuanced, mixed results in terms of LLMs ability to perform such a feat."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper tackles a very important topic, has a good literature review section, and uses well-known and trusted datasets to investigate self-correction abilities. In particular, I appreciated the distinction between intrinsic self-correction and self-correction that leverages information from humans or training examples."
            },
            "weaknesses": {
                "value": "- Only a small set of questions (200) is used on GPT4, the remaining ones apply only to ChatGPT  \n- In terms of reasoning, there are much more challenging datasets out there\n- I found the presentation somewhat confusing since there wasn't a clear description of their methodology (e.g., were all self-correction prompts formulated as the examples in Figure 2, or did variations exist?).\n- Also, the wording is unfortunately often trendy rather than clear: From the conclusion: \"while LLMs represent a groundbreaking step forward in the realm of AI and language generation, their self-correction capabilities, particularly in reasoning, are still nascent\". \"Nascent\" can mean anything from the self-correction capabilities being \"not present\", \"weak\" \"promising\" etc. Please try to use more specific wording.\n- The paper is confusing to read, because there are somewhat contradictory statements: In the beginning the authors emphasize that they want to investigate *intrinsic* self-correction abilities. Then, on page 3, they state: \"With this in mind, we center our investigation on a pivotal query: Can large language models self-correct their reasoning?\" which seems to me to not imply intrinsic self-correction, but any type of self-correction. This unclarity seems to pervade the remainder to the paper.      \nE.g., in section 7 the authors state \" Some existing literature may inadvertently contribute to this confusion, either by\nrelegating crucial details about label usage to less prominent sections or by failing to clarify that their\ndesigned self-correction strategies actually incorporate external feedback. Our intention in this paper is to amplify these concerns and offer a comprehensive overview of the state of \u201cself-correction\u201d in LLMs\".      \nThis implies that the current paper was more like a survey, rather than establishing new results.    \nFurthermore, the title suggests that self-correction does not work, although the paper actually does not really argue this and in section 7 the authors remark \"The title, \u201cLarge Language Models Cannot Self-Correct Reasoning Yet\u201d, is not an outright dismissal of self-correction techniques\". Generally, it would be good to use a title that is completely representative of the paper. (Perhaps add \"intrinsic\" to the title?)"
            },
            "questions": {
                "value": "- I don't understand the second paragraph from section \"3.1.3 REFLECTION\". How does this guessing approach work? It seems that cannot be used for non-multiple-choice type answers, like in the case of the GSM8K dataset?          \nI cannot follow your footnote: \"For GSM8K, a similar random baseline might not exist, but the underlying rationale remains the same. Additionally, we can design a baseline, for example, by generating a random number each time. After a significant number of rounds, it may reach the correct answer, but such a kind of improvement is apparently not meaningful. A more direct justification is: If we already know the answer, why do we need to do this?\"      \n- \"Per the discussions in Section 3.1.3, since the idea that LLMs can self-correct their reasoning is not supported by the evidence so far, we turn our focus to the results in the intrinsic self-correction.\" I wasn't quite able to follow why section 3.1.3 shows this; Table 1 actually seems to show that self-reasoning is well-supported by evidence?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2828/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2828/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2828/Reviewer_yEtd"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2828/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699472036161,
        "cdate": 1699472036161,
        "tmdate": 1699636226257,
        "mdate": 1699636226257,
        "license": "CC BY 4.0",
        "version": 2
    }
]