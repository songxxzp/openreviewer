[
    {
        "id": "X1OXpY9WR4",
        "forum": "ZULjcYLWKe",
        "replyto": "ZULjcYLWKe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2608/Reviewer_As3y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2608/Reviewer_As3y"
        ],
        "content": {
            "summary": {
                "value": "The paper presents DMBP, a novel framework for robust offline RL. DMBP addresses the challenge of handling state observation perturbations by employing conditional diffusion models to recover the true states from perturbed observations. Additionally, it proposes a non-Markovian training objective to reduce the cumulative errors by minimizing the sum entropy of denoised states along the RL trajectory. Experimental results show that DMBP significantly enhances the robustness of current offline RL algorithms, effectively addressing various perturbations and incomplete state observations."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Overall, this paper is well-organized and easy to follow.\n\n* The introduction of the diffusion model as a means to recover true states from perturbed observations, addressing the challenge of observation perturbation, is both novel and sound. Besides, the inclusion of the regularization of sum entropy for denoised states across the RL trajectory is highly meaningful in the context of sequential decision-making, effectively mitigating error accumulation.\n\n* The proposed method can be applied to many model-free offline RL algorithms and significantly improves over prior works in terms of observation robustness and masked observations.\n\nI believe this paper represents a significant contribution to the field and will have a substantial impact on the research community."
            },
            "weaknesses": {
                "value": "* The literature review section lacks a discussion of another taxonomy: training-time and testing-time robustness. While this paper and many others focus on offline RL for testing-time robustness, there is another group of offline RL works that investigate training-time robustness [1] [2] [3]. These works involve corrupting the offline dataset and evaluating it in a clean environment. It would be beneficial to include the training-time works for a more comprehensive literature review.\n\n* It is not clear whether the states in this paper are normalized. Normalizing the observations would ensure fair observation corruption and may potentially impact the recovery ability of the diffusion model.\n\n* Currently, it seems that the diffusion model is trained using complete observations even in the experiments with incomplete observations. I am curious to know if the diffusion model can handle training with incomplete observations as well.\n\n* Additionally, the authors are suggested to report the training time and the inference time of the proposed method.\n\n* Typo: page 2 'A diagram of the proposed approach is shown in the fight subplot of Figure 1' --> 'A diagram of the proposed approach is shown in the right subplot of Figure 1'.\n\n[1] Wu F, Li L, Xu C, et al. Copa: Certifying robust policies for offline reinforcement learning against poisoning attacks[J]. arXiv preprint arXiv:2203.08398, 2022.\n\n[2] Zhang X, Chen Y, Zhu X, et al. Corruption-robust offline reinforcement learning[C]//International Conference on Artificial Intelligence and Statistics. PMLR, 2022: 5757-5773.\n\n[3] Ye C, Yang R, Gu Q, Zhang T. Corruption-Robust Offline Reinforcement Learning with General Function Approximation. arXiv:2310.14550."
            },
            "questions": {
                "value": "My questions are listed in the \"Weakness\" part:\n\n* The authors are suggested to provide a more comprehensive literature review.\n\n* It is necessary for the authors to clarify whether the observations are normalized. If not, it would be beneficial to include a comparison and provide further clarification.\n\n* The reviewer is interested in knowing if the diffusion model can also handle training with incomplete observations.\n\n* The authors are recommended to include information about the training time and inference time of the proposed method.\n\n* Fix the typo."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2608/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2608/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2608/Reviewer_As3y"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2608/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698257411611,
        "cdate": 1698257411611,
        "tmdate": 1699636199671,
        "mdate": 1699636199671,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bBbBIj1hW5",
        "forum": "ZULjcYLWKe",
        "replyto": "ZULjcYLWKe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2608/Reviewer_FLnj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2608/Reviewer_FLnj"
        ],
        "content": {
            "summary": {
                "value": "Offline reinforcement learning (RL) enables training without real-world interactions, but faces challenges with robustness against state observation perturbations caused by factors like sensor errors and adversarial attacks. The Diffusion Model-Based Predictor (DMBP) framework has been introduced to address these issues by predicting actual states using conditional diffusion models, focusing on state-based RL tasks. Unlike traditional methods, DMBP leverages diffusion models as noise reduction tools, enhancing the resilience of existing offline RL methods against various state observation perturbations. The framework utilizes a conditioned diffusion model to estimate current states by reversely denoising data and incorporates a non-Markovian loss function to prevent error accumulation. DMBP's advantages include improved robustness against different scales of noise and adversarial attacks, as well as the ability to manage incomplete state observations, making it suitable for real-world scenarios like robots operating with malfunctioning sensors."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. DMBP strengthens the resilience of existing offline RL algorithms, allowing them to handle different scales of random noises and adversarial attacks effectively.\n2. Unlike traditional approaches, DMBP leverages diffusion models primarily for noise reduction, rather than as generation models. This innovative use helps in better state prediction and recovery against observation perturbations.\n3. The framework introduces a non-Markovian loss function, specifically designed to prevent the accumulation of estimation errors over the RL trajectory.\n4. DMBP offers a more balanced approach than methods that train robust policies against worst-case disturbances. This ensures that policies do not become overly conservative, which can hinder performance in certain scenarios.\n5. DMBP's inherent ability, derived from the properties of diffusion models, allows it to effectively manage situations with incomplete state observations. This is particularly valuable in real-world applications, such as when robots operate with compromised sensors."
            },
            "weaknesses": {
                "value": "1. The paper has the assumption that perturbation on state space follows a Gaussian distribution. However, in practical settings, such perturbations might be biased and skewed. For example, a water drop on a camera might lead to distortion of captured images. Therefore, it might be better if the authors could elaborate on the assumption of the perturbations and how they are produced. \n2. Introducing a diffusion model for noise reduction could increase the complexity of the model, which may lead to longer training time and resource-intensive computations. The authors might want to include more details regarding training time and computational resources.\n3. Scaling to very large state spaces or handling very noisy environments might pose challenges, especially when using diffusion models. It would be better for the authors to consider testing on tasks with larger state spaces such as the humanoid. \n4. Eq.(2) is wrong as the recovered state should not be a probability."
            },
            "questions": {
                "value": "Same as the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2608/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2608/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2608/Reviewer_FLnj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2608/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731838190,
        "cdate": 1698731838190,
        "tmdate": 1700633495222,
        "mdate": 1700633495222,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EEFfIX0qcs",
        "forum": "ZULjcYLWKe",
        "replyto": "ZULjcYLWKe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2608/Reviewer_YkkL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2608/Reviewer_YkkL"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenges in offline reinforcement learning (RL), particularly the robustness against state perturbations. The authors introduce the Diffusion Model-Based Predictor (DMBP), a novel framework that employs conditional diffusion models to recover actual states in state-based RL tasks and proposes a non-markovian loss function to mitigate error accumulation. The framework is designed to handle incomplete state observations and various scales of random noises and adversarial attacks, improving the robustness of existing offline RL algorithms. DMBP is empirically evaluated, demonstrating significant performance improvements in terms of robustness against different perturbations on state observations without leading to over-conservative policies."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. It is significant for this paper to promote the robustness of offline RL methods.\n2. This paper shows its novelty by applying diffusion method to improve the robustness of offline RL against state perturbations, especially designing the non-Markovian loss function to reduce the accumulated error in state estimation.\n3. The proposed method demonstrates strong performance and has been extensively evaluated with different offline RL methods and noise types and in various benchmark tasks. \n4. This paper is generally well written and easy to follow and covers well related works."
            },
            "weaknesses": {
                "value": "1. One main concern is that observation perturbations can be potentially addressed by traditional methods, especially when we assume observation perturbations does not affect reward and transition functions. The reviewer is curious of how offline RL methods with Kalman Filter perform compared to DMBP?\n2. Minor issue: all results in tables should be shown with standard deviations."
            },
            "questions": {
                "value": "1. In the experiments, results show DMBP significantly improve the robustness performance of existing offline RL methods. The review noticed that most offline RL methods used are value-based. How will DMBP perform when used with weighted imitation learning methods, e.g., IQL?\n2. Is DMBP sensitive to hyperparameters (i.e., a, b, c) of the variance schedule? Any guidance to choose these hyperparameters?\n3. It is intutitve that diffusion denoising can perform well for random noise. Can the authors provide insights why it also works well for adversarial attacks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2608/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791710095,
        "cdate": 1698791710095,
        "tmdate": 1699636199426,
        "mdate": 1699636199426,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Rrxri7LrJp",
        "forum": "ZULjcYLWKe",
        "replyto": "ZULjcYLWKe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2608/Reviewer_ad2c"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2608/Reviewer_ad2c"
        ],
        "content": {
            "summary": {
                "value": "When facing observation perturbations, reinforcement learning agent may perform very poorly. Unlike many prior work which train a robust policy, this paper instead utilizes diffusion model as noise reduction tool to retrieve accurate state information. Then the proposed framework feeds the de-noised state to any offline RL algorithm. Additional techniques are introduced, e.g., specialized loss to facilitate multi-step diffusion accuracy. Extensive simulations are conducted on MuJoCo control tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed framework uses the latest diffusion model to solve offline RL with state perturbations. I believe there is a strong motivation behind this approach. Indeed, an important RL problem is studied with state-of-the-art tool.\n\n2. The paper is well-written. The motivations behind each section is quite clear. I enjoyed reading the paper a lot.\n\n3. I believe the contribution is solid in this paper: (a) proposed a non-Markovian loss that facilitates multi-step diffusion accuracy; (b) a tractable version of its VLB is proposed; (c) thorough simulation and ablation studies\n\n4. The proposed framework is very flexible and can work with any good offline RL algorithm."
            },
            "weaknesses": {
                "value": "There is no notable weakness in my opinion."
            },
            "questions": {
                "value": "1. When training different algorithms, do you use the same dataset size and batch size for all algorithms? \n\n2. In offline RL, especially the theoretical community, we are often concerned with the data coverage problem (full coverage vs partial coverage). I am interested in the performance of DMBP in this scenario. In particular, D4RL has millions of transitions in its dataset. Although one can argue that this is not that much for a continuous control task, we sometimes see more extreme case in tabular setting where the data is extremely limited, e.g., barely supporting the optimal policy's state-action visitation. I conjecture that if DMBP is paired with algorithm like VI-LCB[1] which is proven to be statistically efficient, it can also facilitate offline RL with scarce data setting. Do you have any insight on this?\n\n[1]Li, G., Shi, L., Chen, Y., Chi, Y., and Wei, Y. (2022a). Settling the sample complexity of model-based\noffline reinforcement learning. arXiv preprint arXiv:2204.05275\n\nOverall I think this is a great paper, and it should be a good contribution to ICLR."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2608/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819485531,
        "cdate": 1698819485531,
        "tmdate": 1699636199360,
        "mdate": 1699636199360,
        "license": "CC BY 4.0",
        "version": 2
    }
]