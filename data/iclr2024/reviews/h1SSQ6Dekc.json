[
    {
        "id": "GheNouW7iH",
        "forum": "h1SSQ6Dekc",
        "replyto": "h1SSQ6Dekc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3305/Reviewer_yShq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3305/Reviewer_yShq"
        ],
        "content": {
            "summary": {
                "value": "The paper explore the use of LLMs in the tasks of simultaneous translation (SiMT), which means \"Translate as we speak\". The task involves training a policy that, given a partial input, decides whether to continue to listen for more words from speaker, or to translate right away with the partial input. The goal is to achieve optimally low latency and high accuracy.\nThe paper does so with LLMs by finetuning the model on a causal alignment data for such task, which given a partial input, decides to generate the translation or a <wait> token to collect more context for the partial input.\nThe results show comparable with existing SiMT baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Though there are many papers about training LLMs to as decision-making agent, I consider doing so with Simultaneous translation, which is predominantly about speech, is novel and the task of SiMT can improved with the help from LLM.\n- The results show comparable with existing high-quality SiMT baselines, though I highly doubt the actual computational cost is anywhere comparable (see weakness). Future work should make up for this by achieve higher translation quality and latency, as well as in other lower-resource languages."
            },
            "weaknesses": {
                "value": "- Repeated inference of LLM is a huge computational cost, everytime a <wait> token is generated, the text prompt is updated and many tokens representations have to be recalculated without a theoretical room for caching. As such, real-life inference, with a fixed physical hardware, will be much slower compared to existing lightweight translation model. This is true for closed-source GPT models as well, as more tokens called to the API leads to more expensive bill.\n   - Therefore, I urge the authors to provide a real-life inference cost/wall-time comparison to have a better picture of the cost trade-off here and makes the paper complete. I would appreciate and change scores if such report is produced."
            },
            "questions": {
                "value": "- There are many papers about using LLMs as decision-making agent, please cite them."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3305/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3305/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3305/Reviewer_yShq"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3305/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698487135777,
        "cdate": 1698487135777,
        "tmdate": 1700014550712,
        "mdate": 1700014550712,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cWrZiwwm87",
        "forum": "h1SSQ6Dekc",
        "replyto": "h1SSQ6Dekc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3305/Reviewer_Hphj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3305/Reviewer_Hphj"
        ],
        "content": {
            "summary": {
                "value": "This work investigates the use of LLMs for simultaneous MT (SiMT). The training data is preprocessed by generating word alignments, and then inserting enough <WAIT> tokens into the target sequence such that no alignment link has a higher source than target position (called \"casually aligned dataset\"). A variation of the commonly used wait-k strategy is used for inference."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The setup is described clearly and is very straightforward, which makes this work easily reproducible. I also appreciate the results section, which includes the most natural ablations and is not overselling the results. In fact, the most obvious concern of using LLMs for SiMT - inference time - is acknowledged in the paper. The evaluation is based on (just) two language pairs and two LLM (sizes), which is definitely on the slim side, but it meets the minimum bar for me."
            },
            "weaknesses": {
                "value": "I don't think that this paper is particularly innovative. On a high level, it strikes me as one of the \"we tried LLMs for task X and it worked\" papers that are very common these days. That being said, I think that this is one of the better papers in that category due to the sober evaluation and clear writing. So although I wasn't inspired by this work, there is still value in publishing it for the sake of completeness of the body of literature on LLMs.\n\nFig. 2 looks broken.. I guess the key point here is that \"away\" is aligned to \"befreite\", but the alignment link is not shown in the original en-de alignment."
            },
            "questions": {
                "value": "- Have you compared LoRA fine-tuning with prompt-tuning or full fine-tuning (at least with the small LLMs)?\n- Have you tried small (<=1B) LLMs that would be more practical in a real-life SiMT scenatio?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3305/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698683927894,
        "cdate": 1698683927894,
        "tmdate": 1699636279705,
        "mdate": 1699636279705,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tw0pmS8qWr",
        "forum": "h1SSQ6Dekc",
        "replyto": "h1SSQ6Dekc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3305/Reviewer_jikd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3305/Reviewer_jikd"
        ],
        "content": {
            "summary": {
                "value": "This study focuses on simultaneous machine translation (SiMT). It explores the use of decoder-based language models for this purpose. The experiments conducted on English-German and English-Russian translations show results that are comparable to those of current state-of-the-art baselines."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The concept of employing large language models for simultaneous translation appears both novel and exciting.\n- The paper is clearly written and easy to understand.\n- The related work section provides a comprehensive summary of simultaneous translation research in the field of natural language processing."
            },
            "weaknesses": {
                "value": "- Figure 1 lacks clarity in terms of distinguishing when specific actions (READ/WRITE) occur. It would be more reader-friendly if the figure illustrated a step-by-step walkthrough (e.g., t=1, t=2, t=3).\n- In simultaneous translation, wall-clock time (actual speed) is a critical factor. It would be important to report or at least mention how long it takes to generate translations in this setting.\n- The experiment only presents BLEU scores; it lacks concrete examples of output, which would be beneficial for understanding the translation quality."
            },
            "questions": {
                "value": "- I wonder if there are any experimental results for a setting when the target language is English such as DE-EN (instead of EN-DE). Since LLMs are typically trained mainly in English, I wonder if it makes a difference in the performance between En-X and X-En.\n- Line 161. \"We did not use beam search during generation.\" Is there any reason not to use beam search?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3305/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3305/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3305/Reviewer_jikd"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3305/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698849776465,
        "cdate": 1698849776465,
        "tmdate": 1700705432449,
        "mdate": 1700705432449,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jfebyH4La1",
        "forum": "h1SSQ6Dekc",
        "replyto": "h1SSQ6Dekc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3305/Reviewer_xrvK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3305/Reviewer_xrvK"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method to enhance the performance of Language Model-based Simultaneous Machine Translation (SiMT) systems. The authors propose fine-tuning a pre-trained Language Model (LLM) using direct supervision on a dataset of causally aligned source-target sentence pairs. They demonstrate that the LLM can achieve simultaneous translation and input segmentation without the need for a separate policy, with performance that matches or surpasses existing state-of-the-art systems. The paper provides an overview of recent SiMT literature, details the system's architecture, data preparation, and training procedure, and showcases its performance on different language directions. The authors also discuss the limitations of their approach and suggest future research directions. Overall, the paper contributes a novel approach to improving SiMT systems by leveraging fine-tuning of pre-trained LLMs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper introduces a novel approach to improving SiMT systems by fine-tuning a pre-trained Language Model (LLM) with direct supervision on causally aligned source-target sentence pairs. This approach differs from previous methods that rely on separate policies or incremental decoding. By leveraging the capabilities of LLMs, the paper offers a fresh perspective on enhancing SiMT performance."
            },
            "weaknesses": {
                "value": "One of the main concerns regarding this paper is the reliance on a reference-based approach for the causal alignment introduced. While the paper claims to propose a novel method, similar ideas have been studied in previous simultaneous translation literature (e.g. [1]). However, the paper lacks a comparative analysis with these existing approaches in the experiment section, making it difficult to assess the novelty and superiority of the proposed method.\n\nFurthermore, a significant limitation of the reference-based approach is the potential mismatch between full sentence translation and simultaneous translation. The references used to generate the causal alignment are derived from complete sentence translations, which may not be suitable for the dynamic nature of simultaneous translation. Simultaneous translation requires the model to begin translation based on partial context, and the reference-based approach may not adequately capture the challenges and nuances specific to this task.\n\n[1] Simultaneous translation policies: from fixed to adaptive. ACL, 2020"
            },
            "questions": {
                "value": "1. Is the comparison in Figure 6 fair, considering that the authors only evaluate their method and two state-of-the-art models based on translation quality without considering latency?\n\n2. Did the authors observe a high variance in the latency of the resulting causal alignment, potentially due to the fact that the gold reference used is designed for full sentence translation rather than simultaneous translation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3305/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698907388241,
        "cdate": 1698907388241,
        "tmdate": 1699636279534,
        "mdate": 1699636279534,
        "license": "CC BY 4.0",
        "version": 2
    }
]