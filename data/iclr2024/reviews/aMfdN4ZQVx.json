[
    {
        "id": "5HoXZz69cm",
        "forum": "aMfdN4ZQVx",
        "replyto": "aMfdN4ZQVx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4148/Reviewer_Hqos"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4148/Reviewer_Hqos"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Deep Concept Injection (DCI) as an approach to achieving zero-shot cross-modal tasks without the need for additional training. This work conducts a comprehensive set of experiments and analyses to validate the effectiveness of this approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The method of constructing projection networks to replace fine-tuning is intriguing. The authors successfully inject observed concepts, facilitating cross-modal fusion in self-attention layers and feed-forward networks."
            },
            "weaknesses": {
                "value": "The paper claims to be the first to demonstrate the ability of Pre-trained Language Models (PLMs) to perform zero-shot cross-modal tasks without any training. However, there exist similar works, such as Tip-Adapter [1], which should be discussed and compared to provide context and clarify the novelty of this approach.\n\n[1] Zhang, Renrui, et al. \"Tip-adapter: Training-free adaption of clip for few-shot classification.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022."
            },
            "questions": {
                "value": "Does the Deep Concept Injection process incorporate additional information? While there is no explicit training involved in the design of this paper, has any training data been indirectly introduced into this process?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4148/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4148/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4148/Reviewer_Hqos"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4148/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698556505870,
        "cdate": 1698556505870,
        "tmdate": 1699636380283,
        "mdate": 1699636380283,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rVotfnHd5Q",
        "forum": "aMfdN4ZQVx",
        "replyto": "aMfdN4ZQVx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4148/Reviewer_Eqqp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4148/Reviewer_Eqqp"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a method to inject visual concepts into pretrained LLMs without any training for vision-language tasks. The authors leveraged a pre-extracted concept library to aggregate vision-related concepts into feed forward pretrained LLMs with probabilistic weighting. By properly constructing the concept library, the proposed DeepConceptInjection (DCI) model achieve state-of-the-art results on several vqa and video-qa tasks with significant training overhead reduction."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed model is training-free, by properly constructing the concept library and weighting the forward features of words in concept library, the resulting DCI can correctly adapt vision-related information into pretrained-LLMs for vision-language tasks. The resulting model achieve state-of-the-art results compared with existing VQA or Video-QA models with similar model scale."
            },
            "weaknesses": {
                "value": "Despite the good performance and simplicity of the method, this paper seems to be missing several critical key points:\n- First, the concept library seems to be extremely important in the whole DCI pipeline, however, how to construct this concept library given different input datasets or domains seems to be too simple for evaluating if this pipeline could be adapted to more general settings. The authors should consider adding more details to this concept library construction process for the reviewers to evaluate the contribution of this paper. \n\n- The overall architecture seems extremely simple and efficient. Given the good performance, the authors should have considered providing more insights on why this simple augmentation strategy, or put it another way, weighting input and features from the concept library, could help improve vision-language tasks. Is this augmentation enough?"
            },
            "questions": {
                "value": "See the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4148/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698755648442,
        "cdate": 1698755648442,
        "tmdate": 1699636380197,
        "mdate": 1699636380197,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ivf9WMSq9d",
        "forum": "aMfdN4ZQVx",
        "replyto": "aMfdN4ZQVx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4148/Reviewer_HYCX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4148/Reviewer_HYCX"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel approach for zero-shot video/visual question answering leveraging the capabilities of a Large Language Model. To achieve this, the authors translate relevant visual concepts into textual representations using a predefined vocabulary set. Subsequently, they introduce the Deep Concept Injection Module, which integrates these textual visual concepts into the input and feed-forward networks. The effectiveness of this method is validated through extensive experiments conducted on eight video question datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper proposes to inject the textual visual input into the feed-forward network, with experimental results affirming the efficacy of this approach.\n2. Extensive experiments on 8 benchmark video question answering datasets demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. As shown in Eqn. (8), the input features of feed-forward networks guide the aggregation of the output features of feed-forward network, which is hard to comprehend. More explanations are required.\n2. The authors adopt PLM to extract the features of textual visual concepts. However, when the length of the textual visual concepts exceeds one, it would be better to elucidate the specific feature extraction process.\n3. There exists \u2018?\u2019 and \u2018-\u2019 in Table 1. It would be better to explain the meaning of these quotes.\n4. While the ablation studies regarding hyper-parameters are included in the appendix, it would be beneficial to mention the best hyper-parameter settings in the implementation details to make the paper more concrete.\n5. This method demonstrates remarkable performance. Could the authors consider releasing the source code upon acceptance?"
            },
            "questions": {
                "value": "Please refer to the weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/a"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4148/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4148/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4148/Reviewer_HYCX"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4148/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698756714049,
        "cdate": 1698756714049,
        "tmdate": 1699636380119,
        "mdate": 1699636380119,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jkISxN8Wxq",
        "forum": "aMfdN4ZQVx",
        "replyto": "aMfdN4ZQVx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4148/Reviewer_YWMx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4148/Reviewer_YWMx"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a feed forward method to inject text prompts from visual inputs, for visual understanding tasks using large language model. Three variants of the method are presented, which are tested on different visual QA and dialogue datasets. Comparisons show that even without fine tuning the LLM with visual inputs, the system performs comparable or slightly better than competing algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The strengths of the paper are as follows:\n\n1. The method does not require any training, and works simply augmenting the forward path of LLM by injecting semantic visual concepts. \n2. Experiments have been conducted on various QA datasets assessing the performance of the method. The method looks like performing at par or better than some of the prior methods which do joint visual and text encoder training. It beats Flamingo, a visual-text encoder comprehensively on visual QA.\n3. The method can be used easily with different PLMs and paper shows application of the method in multi-modal dialogue system, where the method outputs look reasonable."
            },
            "weaknesses": {
                "value": "The weaknesses are as follows:\n\n1. The paper describes two variants of the DCI method in Section 3.2.1 and Section 3.2.2, however the variations presented in the experiments are based on the vocabulary selection method (DCI, DCI-A, DCI-LM). The authors have not assessed the performances of the previously described variants.\n2. In experiment section, the visual encoder has not been mentioned explicitly. Authors need to add that information in the tables as well as description.\n3. The vocabulary addition is a major step of the algorithm. Certain details and variations in vocabulary are missing: 1. What is the total number of visual concepts taken in the experiments, 2. What is the difference in output vocabulary of DCI-LM vs other variants. In equation 10, how are authors going from output of LLM to visual concepts. Question by itself can generate very open ended responses from LLM. Similarly for DCI-A, what is the number of top frequent words taken as dictionary. \n4. In Table 2, there is not much difference between the results of BLIP-2 and proposed DCI variants. Authors have not explained the reason behind no significant change in output accuracy wrt original BLIP-2.\n5. Examples of multimodal dialgoue systems show several images with named entities like Great Wall of China, orchid, etc. How is the current framework accounting for named entities in their method? Just visual input can potentially generate hallucinations in the LLM output. Authors have not explored the quality of the system in any details in the paper, hence the section does not add to the contribution of the paper."
            },
            "questions": {
                "value": "Questions to authors have been posted in weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Vocabulary generation is a major step in the algorithm. The source and potential biases of the vocabulary is not clear in the paper."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4148/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698847577503,
        "cdate": 1698847577503,
        "tmdate": 1699636380048,
        "mdate": 1699636380048,
        "license": "CC BY 4.0",
        "version": 2
    }
]