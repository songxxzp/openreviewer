[
    {
        "id": "tGXZTY8IzV",
        "forum": "XgdKE7tZn4",
        "replyto": "XgdKE7tZn4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3037/Reviewer_bUmq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3037/Reviewer_bUmq"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework for data augmentation using GANs, inspired by CycleGANs. The main part of the method is to provide an explicit control between intra-class augmentation and cross-domain augmentation. This controllability is achieved by accessing the decision boundary of a pre-trained hinge classifier. The proposed data augmentation mechanism is then evaluated on several binary classification problems on image data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The proposed method is sound and reasonable. \n\n* The experiments show that the method allows to improve the performance of a base classifier, and that it (often) outperforms other augmentation strategies."
            },
            "weaknesses": {
                "value": "* For now, the method is limited to binary classification. Could it be extended to multi-class classification? Self-supervised learning? Which are settings that benefit a lot from data augmentation. \n\n* Experimental validation: I feel that the comparisons are not strong enough and would require a more careful and detailed evaluation. Notably, it is surprising that the Traditional augment often lowers the performance of the standard network, while data augmentation are most of the times beneficial in deep learning. It seems that the authors could have tried different settings and could have tuned better traditional data augmentations. For example, you test different hyper-parameters on your method. You should also test different hyper-parameters on the concurrent methods, otherwise it is natural that the max of HyperDisGan is higher than the max of the other methods. \n\n* The proposed pipeline is a quite heavy pipeline with lots of hyper-parameters to tune. \n\n* Clearness: the paper would benefit, at the beginning of Section 3, from a clear definition/formalization of the setting of the proposed method, e.g. input/output of the generator. This comes partly in the section 3.3, but it is not completely clear. For example, it is stated that the generator is a function from X (or Y) to X (or Y), but then, in the loss functions, the generator takes as input two variables, such as $G_{x2x}(x_1,-d_h(x_1,x_2))$. It is only in Section 4 that it is stated that the generator takes as input an image along with a distance variable replicated on the spatial dimension. \n\n* Minor remarks on writing/typos: several use of \"an\" instead of \"a\", e.g. Figure 2 caption: \"An data augmentation\" or Section 3.2 \"an pre-trained\"; page 5 \"intro-domain\"; page 5: \"transformating\" -> \"transforming\"; Table 2: \"HyerDisGan\" -> \"HyperDisGan\"."
            },
            "questions": {
                "value": "* What about latent transformations in pre-trained GANs? It has been shown that hyperplanes separate classes or modes in the latent space of a pre-trained unconditional GAN. Could you extend your method to leverage pre-trained generators? For practitioners who want to apply your method, it would be way less costly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3037/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3037/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3037/Reviewer_bUmq"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3037/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698069372931,
        "cdate": 1698069372931,
        "tmdate": 1699636248872,
        "mdate": 1699636248872,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9m2M7oP9pl",
        "forum": "XgdKE7tZn4",
        "replyto": "XgdKE7tZn4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3037/Reviewer_aNvk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3037/Reviewer_aNvk"
        ],
        "content": {
            "summary": {
                "value": "This paper studies data augmentation with generative adversarial networks (GANs). The paper proposes a Cycle-GAN based method, HyperDisGAN, which takes into account the hyperplane distance between and within classes to generate samples that are useful for training downstream classifiers. HyperDisGAN uses a classifier pre-trained by hinge loss to learn transformations so that the distance on the hyperplane is as large as possible for inter-class sample transformations, and so that the distance is small for intra-class sample transformations. Experiments were conducted mainly to compare the proposed method with the Cycle GAN baselines and confirmed that the proposed method slightly outperforms the baseline in accuracy and AUC."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "+ The paper proposes an interesting data augmentation method based on CycleGAN that generates inter-class and intra-class samples by transforming real samples."
            },
            "weaknesses": {
                "value": "- The motivation to introduce the proposed method is weak. The issue presented by the paper is expected to be solved by a data augmentation method that interpolates between samples, such as mixup [a], but the paper introduces data augmentation by generative models without discussing this perspective at all.\n- The proposed method is not practical. The proposed method introduces a pre-trained classifier $C_\\text{aug}$, an inter-class generator $G_{x2y}$, and a intra-class generator $G_{x2x}$. These are not practical because they increase in proportion to the number of classes in the downstream classification task. In fact, the paper only evaluates the data set with a small number of classes.\n- Despite the significant increase in computational complexity, the performance gain given by the proposed method is negligible.\n- Experimental baseline is insufficient. Since the proposed method is a type of data augmentation, its performance should be evaluated by comparison with traditional/generative data augmentation methods. For example, traditional data augmentation methods such as mixup [a], CutMix [b], SnapMix [c], and generative data augmentation methods such as MetaGAN [d] and SiSTA [e] are appropriate as experimental baselines.\n- Writing quality is poor. The paper contains many undefined words (e.g., \"domain,\" \"location,\" and \"hyperplane\"), which confuse the reader. In addition, the overall algorithm and procedure are not explained, making it difficult to grasp the overview of the proposed method. In general, the paper does not meet the quality required for an academic paper.\n\n[a] Zhang, Hongyi, et al. \"mixup: Beyond empirical risk minimization.\" International Conference on Learning Representations (2018).\n\n[b] Yun, Sangdoo, et al. \"Cutmix: Regularization strategy to train strong classifiers with localizable features.\" Proceedings of the IEEE/CVF international conference on computer vision. 2019.\n\n[c] Huang, Shaoli, Xinchao Wang, and Dacheng Tao. \"Snapmix: Semantically proportional mixing for augmenting fine-grained data.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 2. 2021.\n\n[d] Zhang, Ruixiang, et al. \"Metagan: An adversarial approach to few-shot learning.\" Advances in neural information processing systems 31 (2018).\n\n[e] Thopalli, Kowshik, et al. \"Target-Aware Generative Augmentations for Single-Shot Adaptation.\" International Conference on Machine Learning (2023)."
            },
            "questions": {
                "value": "Nothing to ask. Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3037/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698212437085,
        "cdate": 1698212437085,
        "tmdate": 1699636248777,
        "mdate": 1699636248777,
        "license": "CC BY 4.0",
        "version": 2
    }
]