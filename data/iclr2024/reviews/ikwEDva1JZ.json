[
    {
        "id": "EvDZvKKegY",
        "forum": "ikwEDva1JZ",
        "replyto": "ikwEDva1JZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4750/Reviewer_z67a"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4750/Reviewer_z67a"
        ],
        "content": {
            "summary": {
                "value": "This works studies in-context learning in transformers using synthetic data. It extends previous work, by studying composition of a *fixed* non-linear function (L-layer MLP) with a linear function that is learned in-context. This work provides a construction of a transformer that can solve this task, but also demonstrates it empirically on synthetic data. Additionally, the authors provide a mechanistic understanding of the algorithm implemented by a trained transformer."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The results extend the the setup of Garg et al. to study in-context learning with more complex function classes. In particular, transformers can learn a composition of a *fixed* non-linear function with a linear function learnt from context. The authors in Figure 1 provide evidence that a transformer matches the optimal predictor.\n\nThe mechanistic analysis is thorough, and provides compelling evidence for the underlying 3-step mechanism. It is surprising that the mechanism is consistent across multiple training runs and adds to the results of the paper. \n\nThe results also hold when multiple different non-linear representations are used which further strengthens the main claims of the paper. I would have liked to see the results of 4.1.1 more prominently in the main paper, but I understand the authors are constrained by space. \n\nOverall, I think the results would be of interest to the community and the toy setup may be more representative of in-context learning in language models."
            },
            "weaknesses": {
                "value": "**Why are the constructive proofs important for understanding in-context learning in transformers?**\nI am aware that there are prior works that design transformers that are capable of in-context learning. However, I am not convinced of the importance and significance of these results. Couldn't we also find weights for other architectures (like large MLPs or LSTMs) and argue that they are capable of in-context learning. Is the existence of these model weights informative of what is learnt in practice?\n\n**Choice of non-linear functions.**  I think the authors could be more rigorous in evaluating the non-linear representations used in their setup. In particular, the non-linear representations are L-layer MLPs with the matrices being random orthogonal matrices. Do the results hold for other families of functions and does it fail to work for some other classes of functions? I think it would be helpful to clarify that the results are specific to this setup.\n\n**Is the synthetic setup an accurate toy-model to understand language models?** Like previous work, all the results are on synthetic data. It remains unclear if the toy setup is representative of in-context learning in language models. What kind real world tasks are captured by a composition of a fixed non-linear function and a linear function that is learnt from context?"
            },
            "questions": {
                "value": "1. Could the authors add more details on how the non-linear functions are created? Can the authors also clarify in the introduction/abstract that the functions are L-layer MLPs?\n\n2. Is it possible to show some of these results on other families of non-linear functions? For example, what happens if the functions are polynomials or exponential functions of the input? Are there scenarios where it fails empirically?\n\n3. What happens if we increase the number of layers used to create the representation from 5 to 15. Does the model start to fail if L=15 (even if transformer has only 11) layers or does it find a good approximation to the non-linear function using just 4-5 layers?\n\n4. Results in appendix E were very interesting! As future work, it would be great to investigate how many different non-linear functions can be learnt. I would also be interested in understanding if in-context learning becomes difficult and if the model sometimes struggles to identify the right non-linear representation.\n\n5. Ruiqi et al. (https://arxiv.org/abs/2306.09927) show that in-context learning fails if the linear functions are selected to be out-of-distribution. Is this also the case here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4750/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4750/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4750/Reviewer_z67a"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4750/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698691791579,
        "cdate": 1698691791579,
        "tmdate": 1699636457355,
        "mdate": 1699636457355,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "duHUCeQ4j7",
        "forum": "ikwEDva1JZ",
        "replyto": "ikwEDva1JZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4750/Reviewer_R1v9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4750/Reviewer_R1v9"
        ],
        "content": {
            "summary": {
                "value": "The goal of this paper is to theoretically and empirically understand the mechanism of in-context learning with underlying representations. Specifically, the setting considered is where there is a fixed representation function, chosen to be an MLP, and the ICL problem is to learn ridge regression on these representations. The transformer must learn this fixed representation function during pretraining and a regression hypothesis in-context. The authors theoretically show that it is possible to construct transformers that can perform ridge regression in supervised and linear dynamical system settings on fixed representations. Empirically, the paper verifies that transformers can learn to perform this type of ICL by probing for the emergence of mechanisms and values that should emerge according to theoretical construction."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper well-written.\n- The experiments do a good job at validating the claims by probing for the relevant information.\n- The results offer valuable insights into how in-context learning, which is very relevant and timely."
            },
            "weaknesses": {
                "value": "- Labels in figures and the figure captions can be more clear. For example, items like \"TF_upper+1_layer_TF_embed\" in Figure 4b are not very readable.\n- Section 3 could be significantly condensed by considering theorem 2 as a generalization of theorem 1 instead of presenting them separately.\n- Section 3.1 states that the representation function can be chosen arbitrarily, but Lemma B.3 requires a specific structure and non-linearity to work.\n- Although the paper does a good job of illustrating the claimed mechanism, it does not analyze settings where the mechanism breaks."
            },
            "questions": {
                "value": "- What happens when the representation function is of a different form? If either the transformer does not have enough layers or the width, is there an approximate representation function learned on which regression is performed, or does the entire mechanism fall apart?\n- How robust is learning of the representation function in settings where the pretraining data contains spurious correlations? Can we say anything about the transformer's ability to compositionally generalize with either the representation function, regression, or both?\n- What is OLS in Fig 1b?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4750/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823321331,
        "cdate": 1698823321331,
        "tmdate": 1699636457273,
        "mdate": 1699636457273,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vIxJaCSeE3",
        "forum": "ikwEDva1JZ",
        "replyto": "ikwEDva1JZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4750/Reviewer_67Bi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4750/Reviewer_67Bi"
        ],
        "content": {
            "summary": {
                "value": "Results illustrating the performance of transformers in ICL tasks that necessitate some degree of representation learning are presented. The theory can be partially validated through probing experiments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Clear and well-written. \n\nThis paper is one of the pioneering efforts to formalize how transformers execute ICL tasks that necessitate a degree of representation learning."
            },
            "weaknesses": {
                "value": "The theory only encompasses representational results by providing some settings of the parameters in a way that a transformer performs an ICL task. Given the transformer's highly expressive capability, these types of constructions are generally relatively straightforward.\n\nThere's no assurance that these theoretical constructs are truly internalized by the model during the training process. Although probing experiments gave us some confidence that, in specific instances, the theory can predict the model's behavior, these types of experiments generally don't offer robust guarantees. As a result, while the theory is logical and sometimes mirrors empirical events, it could be counterproductive to lean too heavily on these theoretical constructs. It might be necessary to carry out an analysis of training dynamics in order to theoretically determine under which conditions the model actually aligns with the theoretical constructs."
            },
            "questions": {
                "value": "The reviewer is open to learning about new evidences or analyses which address the points of the \u201cWeaknesses\u201d section above in this review."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4750/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824825232,
        "cdate": 1698824825232,
        "tmdate": 1699636457188,
        "mdate": 1699636457188,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xNOEPkibGm",
        "forum": "ikwEDva1JZ",
        "replyto": "ikwEDva1JZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4750/Reviewer_oQN2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4750/Reviewer_oQN2"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on understanding the internal mechanism by which a Transformer model solves an in-context learning task where the label $y$ for an instance $x$ linearly depends on a representation $\\phi^{\\star}(x)$.  A recent line of work has focused on explicitly constructing transformer models that can simulate various learning methods (e.g., gradient descent) on a training objective defined by the in-context labeled examples during a forward pass of the Transformer model. This paper extends this line of work by considering a more general data model where the final label depends on the input instance through a linear function of a representation. The paper provides explicit constructions for Transformer networks that can simulate ridge regression for 1) supervised learning with a representation, and 2) learning dynamical systems with a representation. The explicit constructions first aim to employ the underlying representation map $\\phi^{\\star}$ in the lower layers of the Transformer and then implement gradient descent in the upper layers of the Transformer.\n\nThrough experiments on synthetic datasets, the paper demonstrates that the performance of in-context learning via Transformers closely agrees with the performance of an optimal ridge predictor. Through probing analysis of the Transformer models, the authors show evidence that supports representation mapping following by label prediction aspects of their constructions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The paper successfully extends the recent line of work on showing the feasibility of in-context learning via empirical risk minimization during a forward pass by considering data models where labels depend on the input feature via a representation map.\n2) The paper is well-written and explains the key contributions and techniques clearly.\n3) The empirical results (on synthetic datasets) do indicate the feasibility/presence of the explicit in-context learning mechanism hypothesized in the paper."
            },
            "weaknesses": {
                "value": "1) Novelty of the technical contributions is limited given prior works of similar flavor that provide the feasibility of empirical risk minimization during forward pass. One of the aspects which authors claim to be novel is that they allow for representation-based learning. However, the underlying assumption is that the representation map is a multi-layer MLP model, which Transformers should be easily able to simulate through its MLP layers. In that sense, the results in not very surprising. \n\n2) The probing analysis is done only on a synthetic setup. It would be nice to get some supporting evidence for the proposed in-context learning mechanism on a real dataset.\n\nMinor issues:\n\n1) The authors may want to formally introduce/discuss the pre-training phase (e.g., Eq (10)) which learns the representation map early in the section on preliminaries. \n\n2) In the paragraph on **In-context learning** in Section 2, $\\mathcal{D}^{(j)}$ and $\\mathbf{w}_{\\star}^{(j)}$ are not defined before their usage."
            },
            "questions": {
                "value": "1) In general, the transformers are known to be universal approximators. In light of these, could authors comment on the significance of the key contributions of the paper, i.e., learning a representation map before applying gradient-descent in the representation space?\n\n2) Could the authors elaborate on using a **linear model** for their investigation on the upper module via pasting (Figure 4)?\n\n3) Currently the non-linearity in the true representation map is closely tied to the nonlinearity used in the Transformer network. Could the authors comment on generalizing this to broader nonlinearities in the true representation map? How would it increase the required number of layers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4750/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4750/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4750/Reviewer_oQN2"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4750/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699049538478,
        "cdate": 1699049538478,
        "tmdate": 1700691648908,
        "mdate": 1700691648908,
        "license": "CC BY 4.0",
        "version": 2
    }
]