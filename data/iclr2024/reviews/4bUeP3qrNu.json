[
    {
        "id": "Lpv1x3Fzgy",
        "forum": "4bUeP3qrNu",
        "replyto": "4bUeP3qrNu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8303/Reviewer_Q1gw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8303/Reviewer_Q1gw"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on a very important issue in human logical reasoning, namely syllogisms, in which reasonings are done based on two premises. This paper conducted a system comparison between such an ability between LLMs in the PaLM 2 family and humans."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The subject matter is very interesting and very well-grounded in theories in cognitive science.\n2. The whole systematic investigation is very well-designed."
            },
            "weaknesses": {
                "value": "My only concern is that it is quite unclear how the human data was post-processed as each syllogism is associated with 139 responses. If there is no further post-processing as the one on LM-produced data, then this is an insurmountable issue for me. I am saying this because the LM's outputs are actually majority voting results based on 30 runs. If humans did not do the same, then, for me, machines and humans were doing two very different tasks, making the conclusions extremely biased and, therefore, unreliable. Thinking of a situation where one asks an LM to run 30 times on a test case and 20 of them are correct, the accuracy is 100% after majority voting. Nevertheless,  if one asks humans to do the same but without majority voting, then the accuracy is only 66%, but their (i.e., humans and the LM) behaviours are very similar to each other.\n\nFor me, a better solution is dropping the majority voting and computing distributional similarities when comparing LMs with humans (I mean directly rather than the one in section 4.2)."
            },
            "questions": {
                "value": "I have no further questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8303/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698632142940,
        "cdate": 1698632142940,
        "tmdate": 1699637032181,
        "mdate": 1699637032181,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vuOjrTOyoE",
        "forum": "4bUeP3qrNu",
        "replyto": "4bUeP3qrNu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8303/Reviewer_Wr2g"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8303/Reviewer_Wr2g"
        ],
        "content": {
            "summary": {
                "value": "The paper analyzes the behavior of PaLM 2 on syllogisms and concludes that (1) it is generally more accurate than humans (and more so with a bigger size), (2) its judgment is still correlated with human judgment, and (3) it also has biases in incorrect logical reasoning (more so with a bigger size)."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper makes a clear contribution in understanding how humanlike LMs are by exploiting the already existent study of human performance on syllogisms. This study allows us to make a fine-grained examination of 64 syllogism types (2 premises * 4 orderings * 8 moods), each of which has human performance recorded from 139 participants. This approach makes the claims made in the submission believable."
            },
            "weaknesses": {
                "value": "- The findings are useful but not surprising.\n- Some natural settings are omitted, including few-shot prompting which is only mentioned. While the paper can't do everything, it seems a bit incomplete to not check the possibility that few-shot (which is more aligned with the human data) makes a big difference.\n- Only the PaLM 2 model family is considered, though that's understandable from a limited budget point of view. But we may not be able to discount the possibility that the LM behavior changes significantly with, e.g., GPT-4."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8303/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698753034136,
        "cdate": 1698753034136,
        "tmdate": 1699637032044,
        "mdate": 1699637032044,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HN2A3mqiRF",
        "forum": "4bUeP3qrNu",
        "replyto": "4bUeP3qrNu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8303/Reviewer_acDo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8303/Reviewer_acDo"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on comparing the logical reasoning abilities on syllogism between language models (PaLM 2 family) and human. The authors conduct tests on language models using 64 syllogistic forms about 30 content words, and compare their performance to the result from human reasoning in cognitive psychology research. Furthermore, they use a computational cognitive model based on Mental Models theory to explain and validate the logical reasoning of language models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work covers various human biases in syllogistic reasoning, presenting an in-depth comparison through extensive experiments. It examines the distribution of responses, variable ordering, and generation of syllogistic fallacies in both language models and humans.\n2. By employing the Mental Models Theory, this work provides a fresh perspective on understanding the logical reasoning abilities of language models."
            },
            "weaknesses": {
                "value": "1. The deductive reasoning capability of LLMs has been widely explored. The comparison of reasoning abilities between humans and language models in syllogism has already been explored in Dasgupta et al (2022). And Saparov & He (2022) also utilizes similar controlled techniques for analyzing the capabilities of language models. Besides, this work directly utilizes human results on syllogistic reasoning from previous work and only explore PaLM 2 models on a small set of data, which limits the contribution of this paper.\n2. The motivation to use mReasoner and the explanation of its details are not well presented.\n3. The description of the datasets and models for experiments are insufficient, such as the size of model parameters. Conducting experiments on different language models would enhance the conclusion of the study.\n4. There are some errors:\n> - In Section 2.3, Saparov & He (2022) and Saparov et al. (2023) both analyze natural language, contrary to what the authors claim about formal logic forms. \n> - There are spelling typos, such as \"efffect\" in the third paragraph of Section 4.2."
            },
            "questions": {
                "value": "1. This paper only focuses on syllogisms about 30 content words, which are all about person characters/professions. This limits the reliability of experimental observations and conclusions. Compared to other domains, syllogisms on person characters seems to have less content effects problems?\n2. The paper mentioned that \"we will use the less confusing term \u201cvariable ordering\" in Section 2.1, how did you select less confusing terms?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8303/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760078232,
        "cdate": 1698760078232,
        "tmdate": 1699637031933,
        "mdate": 1699637031933,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iZTJetucbC",
        "forum": "4bUeP3qrNu",
        "replyto": "4bUeP3qrNu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8303/Reviewer_hrcw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8303/Reviewer_hrcw"
        ],
        "content": {
            "summary": {
                "value": "This paper reports results on an family of LLMs' (the PaLM 2 models) zero-shot performance on syllogistic reasoning and compares it to experimental data from human participants.  Such inferences have lay at the heart of the field of psychology of reasoning for decades, and so this paper attempts to leverage that literature for understanding LMs.  They find that model performance increases with scale, and that (even in the case of \"super-human performance\") many of the errors the models make are similar to the ones that humans do.  The authors further compare the models' responses to a cognitive model of reasoning that was fit to human data, to identify which factors are responsible for the change in behavior with scale."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Uses cognitive science experiments and methods to understand large language models and make relatively direct comparison with human behavior.\n- Detailed analysis of model responses, e.g. looking at variable-ordering effects, beyond just summary statistics like accuracy.\n- Compares LM responses to a cognitive model fit to human data, to identify which factors drive model behavior at scale.\n- Appendix compares different prompting strategies as well."
            },
            "weaknesses": {
                "value": "- Results are only presented on one family of models, which are behind a closed API.  This has two problems: (1) While it is nice that this family has four scale variants, the results are still presented as about \"LMs\" in general, whereas we have learned about one in particular.   Another model family or two would help understand whether the results are in fact general. (2) Behind a closed API, the results are not necessarily reproducible.  I'd like to see at least one open model as well.\n- The dataset used for evaluation is relatively small.\n- Difficult choices are made for parsing LM generation to decide when it has generated a conclusion to the syllogism.  I would have liked to hear more about this choice point and whether they looked at other formats (e.g. multiple choice).\n- The tendency of the model to _never_ output \"nothing follows\" means that all of the interesting analyses in the paper were on a subset of the syllogisms (27/64) where nothing follows is never correct.  This also limits the generality of the results."
            },
            "questions": {
                "value": "- Up to 4 conclusions are valid: how then, was accuracy measured?  If the output is one of the valid conclusions?  And is this the same as in the human experiment?\n- Figure 5: the correlation between entropy and accuracy seems to \"go away\" for the Large model; do you have any thoughts on why this is?\n- It was a bit opaque to me how the mReasoner model was fit to the human data.  Can you say more about this process?  And, more importantly, is it a good fit to the data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8303/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699298874118,
        "cdate": 1699298874118,
        "tmdate": 1699637031775,
        "mdate": 1699637031775,
        "license": "CC BY 4.0",
        "version": 2
    }
]