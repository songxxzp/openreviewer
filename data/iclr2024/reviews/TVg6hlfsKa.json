[
    {
        "id": "m2QE36Fkd9",
        "forum": "TVg6hlfsKa",
        "replyto": "TVg6hlfsKa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1824/Reviewer_Gibf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1824/Reviewer_Gibf"
        ],
        "content": {
            "summary": {
                "value": "In this paper the authors propose a global-local adaptation method to seamlessly adapt the pre-trained DINOv2 model to produce both global and local features for the visual place recognition task. The proposed feature representation can focus on discriminative landmarks and eliminate dynamic interference. The output local features are used in local matching for re-ranking to further boost performance. This method outperforms other state-of-the-art methods on multiple datasets with high computational efficiency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThis paper is very well written and clearly presented.\n2.\tRecapitulation of related work is good.\n3.\tThe authors design a hybrid adaptation method to seamlessly adapt pre-trained foundation model to the VPR task. The method is novel and interesting, and the authors did not over-complicate it.\n4.\tThe experimental results are really good."
            },
            "weaknesses": {
                "value": "1.\tPitts250k is also a common VPR dataset. The proposed approach has shown excellent results on multiple benchmark datasets. Providing the results on the Pitts250k dataset might make the experiment more complete.\n2.\tRe-ranking top-100 candidates seems a common setting for two-stage VPR methods. However some works also show the performance with different numbers of re-ranking candidates [1], which can help other researchers choose the optimal number of candidates when using this method. I think it is also necessary to show the performance of different numbers of candidates.\n\n[1] Zhu, Sijie, et al. \"R2former: Unified retrieval and reranking transformer for place recognition.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1824/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698619066207,
        "cdate": 1698619066207,
        "tmdate": 1699636111857,
        "mdate": 1699636111857,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sVeQqVGpqA",
        "forum": "TVg6hlfsKa",
        "replyto": "TVg6hlfsKa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1824/Reviewer_mSaw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1824/Reviewer_mSaw"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a hybrid global and local adaptation method to adapt pre-trained foundation models to two-stage visual place recognition. The global adaptation is achieved by adding parallel and serial adapters in each ViT block. The local adaptation is implemented by adding up-sampling layers after ViT backbone to produce dense local features. A novel mutual nearest neighbor local feature loss is proposed to train the local adaptation module. This architecture achieves fast two-stage place retrieval and outperforms several SOTA methods. It is ranked first on the MSLS challenge leaderboard."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-organized and presents a good overview of the related work.\nThe approach is simple and easy to follow. \nThe experimental datasets are sufficient (conducted on 6 VPR benchmark datasets) and the results are excellent (outperform previous SOTA methods by a large margin).\nThis method can bridge the gap between the tasks of model pre-training and VPR using only a small amount of training data and training time. The two-stage retrieval runtime on Pitts30k is less than 0.1s (about 3% of the TransVPR method). This makes contributions to use pre-trained foundation models for real-world large-scale VPR applications."
            },
            "weaknesses": {
                "value": "This method achieves significantly better performance than other methods on several VPR datasets, and the authors qualitatively demonstrate some challenging examples. However, the motivation of the proposed method is not demonstrated well.  In particular, the gap of the tasks of model pre-training and VPR is not very clear to me. In addition, this paper does not show failure cases, which can inform future research in VPR."
            },
            "questions": {
                "value": "1. Will re-ranking more candidate places achieve better performance or hurt the results?\n2. This work finetunes the models on the MSLS dataset and further finetunes them on Pitts30k to test on Pitts30k and Tokyo24/7, which is the same as R2Former. However, the R2Former work provides the result on Pitts30k of the model that only trained on MSLS, which can prove the model's transferability to the domain gap. Can the proposed SelaVPR still outperform R2Former on Pitts30k using only MSLS for training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1824/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698626078820,
        "cdate": 1698626078820,
        "tmdate": 1699636111785,
        "mdate": 1699636111785,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "I1dLlCfPBc",
        "forum": "TVg6hlfsKa",
        "replyto": "TVg6hlfsKa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1824/Reviewer_S5xw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1824/Reviewer_S5xw"
        ],
        "content": {
            "summary": {
                "value": "1.\tVisual place recognition (VPR) is a fundamental task for applications in robot localization and augmented reality. This paper aims to bridge the gap between the tasks of pre-training and VPR, thus fully unleashing the capability of pre-trained models for VPR.\n2.\tThe authors introduce a hybrid adaptation method to get both global features for retrieving candidate places and dense local features for re-ranking. \n3.\tMeanwhile, a novel local feature loss is designed to guide the production of proper local features for local matching without geometric verification in re-ranking."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1.\tThis work is novel and solid. The proposed SelaVPR is a well-designed method and achieves very fast two-stage retrieval.\n2.\tThis paper makes two technically strong contributions: closing the gap between the pre-training and VPR tasks, and outputting proper dense local features for VPR task using DINO v2. The extensive ablation experiments and visualization results show that the proposed method well adapts the pre-trained model to the VPR task. The produced dense local features also perform well in local matching re-ranking.\n3.\tThis method achieves better performance than the SOTA methods with less training data and training time."
            },
            "weaknesses": {
                "value": "1.\tThis work adapts the pre-trained model to the VPR task. The global and local features produced by this hybrid adaptation seem to be useful for more visual tasks. Expanding the use of this method can make the contribution of this paper more obvious.\n2.\tThe clarity of the paper could be further improved."
            },
            "questions": {
                "value": "1.\tWhy is L2 distance used to measure global feature similarity, but dot product used to calculate local feature similarity?\n2.\tIs it feasible to re-rank top-k candidate images directly using coarse patch tokens from ViT? and how is the performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1824/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744613965,
        "cdate": 1698744613965,
        "tmdate": 1699636111706,
        "mdate": 1699636111706,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oEDBgEKMBw",
        "forum": "TVg6hlfsKa",
        "replyto": "TVg6hlfsKa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1824/Reviewer_LtSJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1824/Reviewer_LtSJ"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method to adapt foundation models to the task of visual place recognition, arguing that the object-centric focus of the training of foundation models does not align with the background/static-object attention needed in the VPR task. Thus, they propose, instead of fine-tuning the visual transformers, to extend the transform block with two mechanisms (added MLP) that operate as adapter for the global feature computation. Another local adaptation is done for reranking, similar to geometric verification of retrieved images."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ concept is straightforward: the idea behind the adaptation, and the motivation why it is needed to adapt from foundation models is clearly presented.\n+ experiments on relevant datasets: results are very good, improving on existing approaches\n+ the paper is easy-to-read"
            },
            "weaknesses": {
                "value": "_very limited or non-existing insights_\nthe paper is built solely around overcoming the results of existing methods, while insights and evidence-based contributions are not provided. After reading the paper I am left with a big question: \"why this method works and what do I learn that can perhaps use to design methods in different applications?\". Would the adaptation work also for CNNs pre-trained on ImageNet (as they share the same object-centric bias)? \nI would expect (at ICLR) a thorough analysis of reasons why the performance are much higher, what the implications of doing adaptation are, and what are the real scientific contributions behind this work (not just that the method gets better results than sota methods).\n\n_design choice weakly explained_\nno motivations or justification of why the adapters are designed in a certain way, and what the difference w.r.t. existing approaches for adaptation of foundation models are. What is the hypothesis behind this kind of design, and what explanations can be given (with experimental evidence) about their working principle?\n\n_data-efficiency not elaborated upon_\nas data-efficiency is a key argument about using foundation models, the authors indeed mention it but do not provide substantiable experimental evidence about how it benefits their approach. \n\n_parameter difference not well-analyzed_\nThe adaptation mechanisms proposed are still requiring the fine-tuning of +50M parameters, which is much more than other methods train. Summed up with the +300M parameters of the foundation backbones, these models account for much more capacity than whatever method used previously. The authors do not provide any discussion about this point, or experiments with adaptation of other (smaller) models.\n\nA missing reference:\nLeyva-Vallina et al., Data-Efficient Large Scale Place Recognition With Graded Similarity Supervision; CVPR 2023"
            },
            "questions": {
                "value": "- How would the adapters work with other (smaller) models?\n- What are the reasons, and interpretaions (with evidences) of why the proposed adapters work?\n- How the adapter parameter space influences the improve of performance, and how does it relate with 'smaller' backbones?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1824/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698940936568,
        "cdate": 1698940936568,
        "tmdate": 1699636111604,
        "mdate": 1699636111604,
        "license": "CC BY 4.0",
        "version": 2
    }
]