[
    {
        "id": "zmtyYAO6Ri",
        "forum": "wT8G45QGdV",
        "replyto": "wT8G45QGdV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1971/Reviewer_xWyo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1971/Reviewer_xWyo"
        ],
        "content": {
            "summary": {
                "value": "Consistent123 is an improved version of Zero123 by optimizing using extra cross-attention consistency with a progressive classifier-free guidance strategy. This cross-attention training also enables generating an arbitrary number of views during inference."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- A clear improvement in novel view synthesis. The video shared in the supplementary file clears shows the strength of Consistent123 over Zero123. \n- The cross-attention mechanism is simple yet effective."
            },
            "weaknesses": {
                "value": "1. Smoothness in Results. I noticed that the Consistent123 approach generally produces smoother results and seems to miss out on some finer details compared to Zero123. This observation is particularly evident in the first row of Fig. 4, and in the hat geometry depicted in Fig. 7: both in sections (a) bottom and (b) up. Have the authors identified potential strategies or modifications to address this shortcoming?\n\n2. Concerns regarding arbitrary-length sampling. The methodology adopted uses a fixed number of views (8 views) during training. I'm concerned that this fixed view might adversely affect performance when dealing with arbitrary-length sampling at inference. This concern arises from a potential mismatch between training and test distributions. Any reason why using fixed number views during training? Is it only for simple implementation and faster training? An ablation study showcasing the performance with a random number of views as well during training would provide valuable insights and address this concern. \n\n3. Ablation on Zero123 pretraining. Could you present results when Consistent123 is trained from scratch without Zero123 pretraining?"
            },
            "questions": {
                "value": "1. Presence of Ground Truth for clarity. I would recommend including the Ground Truth in Figs 1, 4, 5, and 6. Having a point of reference would greatly enhance the clarity and allow for a more informed evaluation of the results.\n\n2. Visualization of cross attention during training. The manuscript currently lacks visualizations for cross-attention dynamics during training. It would be beneficial for readers to see how these cross-attention maps evolve and converge throughout the training process. \n\n3. It makes the paper stronger if you can show better 3D reconstruction results. For example, you can use your Consistent123 inside RealFusion [1] and Magic123 [2] to show state-of-the-art image-to-3D results. \n\nOther minor suggestions:\n1. suggest to add more views in Fig 1 since there are empty space. You can also point out in the figure where Zero123 fails and you success to catch the audiences\u2019 attention quickly. \n2. Show back views in Fig 7 (a) bottom and (b) up. \n3. Better to add cross attention between views in Fig.2 (a) as well, like a few red lines across views at the denoised views.\n\n[1] Melas-Kyriazi, Luke, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. \"Realfusion: 360deg reconstruction of any object from a single image.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8446-8455. 2023.\n[2] Qian, Guocheng, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee et al. \"Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors.\" arXiv preprint arXiv:2306.17843 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1971/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698591703011,
        "cdate": 1698591703011,
        "tmdate": 1699636128752,
        "mdate": 1699636128752,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ikTC1Kjv1O",
        "forum": "wT8G45QGdV",
        "replyto": "wT8G45QGdV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1971/Reviewer_n2pA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1971/Reviewer_n2pA"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors aim to improve the view consistency for the novel view synthesis method based on image-to-image translation (i.e., Zero123). Specifically, They incorporate Zero123 with shared self-attention layers and additional cross-view attention layers. In addition, they propose a progressive classifier-free guidance strategy to balance the texture and geometry during the denoising process. Experimental results show that the proposed Consistent123 achieves better view consistency on multiple benchmarks compared to Zero123. The authors also demonstrate the potential of Consistent123 on various downstream tasks, such as 3D Reconstruction and image-to-3D generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method allows flexible view numbers compared to concurrent work MVDream. Experiments show that using arbitrary-length sampling with more view numbers could boost view consistency, indicating the effectiveness of the proposed method.\n2. The proposed method is intuitive yet effective. By adding additional attention mechanisms, the authors improve the view consistency of Zero123.\n3. The proposed progressive classifier-free guidance is interesting and alleviates the trade-off between geometry and texture."
            },
            "weaknesses": {
                "value": "1. The attention mechanisms are totally borrowed from previous work, such as shared self-attention from Cao et al. and cross-attention from video diffusion models.\n2. For the shared self-attention layers, when the views are totally orthogonal, how will this shared self-attention act? Can this self-attention find correct correspondence? For example, in Figure 5 (right), when there is no shared self-attention, the resulting first view seems much more interesting. It would be better to have self-attention visualization in this case.\n3. Considering Objaverse has 800K+ 3D models, the authors only picked up 100 objects for Table 1, which seems far from enough.\n4. The proposed Consistent123 loads pretrained weight from Zero123 and fixes these weights. It would be fair to also have a version training from scratch.\n5. Will the compromise solution introduce view inconsistency? It looks like there is no connection between the sampled views and the next round views.\n6. The results on 3D reconstruction seem poor, where the results from Neus are blurry and low-quality."
            },
            "questions": {
                "value": "1. For the shared self-attention layers, is there any positional embedding? If not, does introducing a camera pose-aware positional embedding help? Do you have any insight on this? \n2. Since the current conditions on R and T are still geometry-free, I am worried that the proposed method's upper bound is limited."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1971/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1971/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1971/Reviewer_n2pA"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1971/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698741205147,
        "cdate": 1698741205147,
        "tmdate": 1699636128680,
        "mdate": 1699636128680,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xwmXznLOV3",
        "forum": "wT8G45QGdV",
        "replyto": "wT8G45QGdV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1971/Reviewer_4aQC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1971/Reviewer_4aQC"
        ],
        "content": {
            "summary": {
                "value": "The paper introduced a way that improves novel view synthesis model, e.g. zero123 by considering a multiview input in the diffusion model. While consider a shared self-attention machanism that all views  query the same key and value from the input view, which provides detailed spatial layout information for novel view synthesis.  In the method, It supports Arbitrary-length Sampling and adopted Progressive Classifier-free Guidance, yielding a further improvement of the synthesis. \n\nThe resulting novel views looks more consistent than baseline. And from the supplimentary material."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The multiview input to the diffusion is good in achieving geometric consistency, comparing against zero123 base model. \n\n2. The design of progressive scheduler is interesting by jointly considering the benefit from large cfg vs small cfg, which leverage between texture and geometry. \n\n3. The paper demonstrates through qualitative and quantitative experiments that Consistent123 significantly outperforms baselines, zero123 in particular, in view consistency, showcasing substantial improvement in various downstream tasks."
            },
            "weaknesses": {
                "value": "Novelty is clear, while there are several publications available with open-sourced papers. Such as magic123,  zero123 xl, sync-dreamer.   for synthesizing new views and do the 3D reconstruction using SDS or direct pixel loss based on NeuS. Wonder the author may compare the results with the opensourced recon-models. \n\nFrom the experimental results after 3D reconstruction. It looks like a black biased back side are generated. Which in my perspective, they are no better than the pulished methods such as that has been implemented in threestudio [url: https://github.com/threestudio-project/threestudio] [which is available before the submission].  The autho may explain why the reconstructed results"
            },
            "questions": {
                "value": "1. The generated views are still not fully consistent before the 3D model is reocnstructed, while the renderred image after reconstruction looks much worse.  is there any thoughts in further improve the consistency so the quality gap between diffused output and render-view can be minimized ? \n\n2. How it generalizes towards more sophisticated images ?  Please also provide some faliure cases for a better understand of the limitations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1971/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770911823,
        "cdate": 1698770911823,
        "tmdate": 1699636128532,
        "mdate": 1699636128532,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "N1B40YotHN",
        "forum": "wT8G45QGdV",
        "replyto": "wT8G45QGdV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1971/Reviewer_bDyq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1971/Reviewer_bDyq"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel method to synthesize a set of images of any objects from novel view given a single image as input. One of many challenges in this task is to generate consistent images in terms of geometry and appearance. To this end, the authors propose to generate multiple images simultaneously and enable cross-attention between novel images at different viewpoints. To strike a balance between geometry and texture of generated images, they also propose a progressive Classifier Free Guidance (CFG) after observing that a larger CFG often leads to better geometry but poor texture and a smaller CFG causes an opposite result. Experiments demonstrate that the proposed method outperform a popular baseline,  Zero123, on image synthesize."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Consistency in novel view synthesis is at the core of many image-to-3D task. The proposed method is able to outperform Zero123 by a large margin qualitatively and quantitatively.\n- The observation of more generated views improving consistency Is useful to other image- or text-conditioned novel view synthesis works.\n- Paper is generally easy to follow"
            },
            "weaknesses": {
                "value": "- V_c should be after softmax in Eq. 5.\n- The name \"shared self-attention\" is confusing to me. It in fact is a cross-attention from the novel views to the input view. Why is it called self-attention?\n- Only qualitative results were presented for image-to-3D tasks."
            },
            "questions": {
                "value": "- How many views were used in the NeuS experiment?\n\n- No texture on the spray bottle in Figure 7?\n\n- Is the Super Mario a failure case since the object has flattened? Could it be related to the progressive CFG? Figure 3 seems to suggest that a large CFG could lead to flat objects."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1971/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698779701183,
        "cdate": 1698779701183,
        "tmdate": 1699636128446,
        "mdate": 1699636128446,
        "license": "CC BY 4.0",
        "version": 2
    }
]