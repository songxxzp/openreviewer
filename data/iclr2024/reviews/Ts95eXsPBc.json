[
    {
        "id": "PkPYVhbNEz",
        "forum": "Ts95eXsPBc",
        "replyto": "Ts95eXsPBc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6931/Reviewer_44cr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6931/Reviewer_44cr"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes Spatially-Aware Transformers (SAT) to keep spatial information in place-centric episodic memory.\nSAT maintains multiple spatial memories for respective places while ensuring the \"total\" size of memory (i.e., $L / K$).\nFor memory management, the author proposes the Adaptive Memory Allocator (AMA) that adaptively finds the best memory management policy by learning $\\pi(\\sigma | \\tau)$ using Q-learning given a task description, $\\tau$.\nThe proposed AMA outperforms the baselines (i.e., model w/o AMA) by noticeable margins regarding effectiveness and efficiency in various downstream tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is generally written well and easy to follow.\n- Extending Transformer-based memory architecture to spatial domains is well-motivated and sounds sensible.\n- Learning to choose the best memory management strategy from multiple candidates looks reasonable.\n- Experiments on various downstream tasks supports the generality of the proposed approach.\n- The proposed approach achieves strong performance gain with large margins."
            },
            "weaknesses": {
                "value": "- The novelty of AMA seems a bit weak, as it is basically policy learning that chooses the best action (here, strategy) that maximizes rewards. What are some core differences from conventional policy learning, especially related to memory management for spatial information?\n- The key idea for SAT seems to use separate networks for respective places. But can the architecture be useful in the case of a large number of places (i.e. what if $K -> \\infty$ that results in almost zero size of memory for each place)?"
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6931/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6931/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6931/Reviewer_44cr"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6931/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697997217288,
        "cdate": 1697997217288,
        "tmdate": 1699636808131,
        "mdate": 1699636808131,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KAfk4GBkKo",
        "forum": "Ts95eXsPBc",
        "replyto": "Ts95eXsPBc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6931/Reviewer_QvRD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6931/Reviewer_QvRD"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates the spatially-aware transformers (SAT) that incorporate agent's spatial and temporal experiences important to solve embodied problems. Specifically, it studies the influence of spatial and decisional information, and explores multiple memory management strategies (e.g., Place-centric vs Time-centric, First-In-First-Out vs Last-In-First-Out, and Adaptive Memory Allocator) on various downstream tasks, showing insights of different memory utilization and management approaches in addressing many machine learning problems (e.g., supervised prediction, image generation), and justifying the proposed SAT-AMA."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper studies the important problem of utilizing and managing spatial and temporal memory experienced by embodied agents that are essential to address various downstream tasks. Particularly, it considers the practical issue of memory constraint and perform experiments based on the popular transformer architectures. The research presented in this paper is very well motivated.\n\n- The paper is very technical solid. Almost all arguments are well-supported/justified by the highly-relevant and classic references and thorough experiments (in Appendix). It carefully compares place- and time-centric store and read, and progressively studies FIFO to the proposed Adaptive Memory Allocator based on SAT. I believe the extensive settings and results presented in this paper have the potential to inspire many future works.\n\n- Overall, this paper was a very enjoyable read to me. All details have been clearly presented (especially with the Appendix and all nice visualizations); The paper is nicely-structured, concise but contains massive valuable information. I believe many arguments and thoughts presented in this paper will be very constructive to relevant future research."
            },
            "weaknesses": {
                "value": "- The title of this paper is very misleading\n    - The paper does not propose any new formation of the transformer architecture to better model spatial information, but focuses on managing agents' episodic memory for addressing different downstream tasks. \n\n- This paper overclaims several contributions.\n    - The paper says \"we are the first to motivate, conceptualize, and introduce the notion of transformers capable of utilizing explicit spatial information\", but as the authors mentioned that defining and managing external memory have been extensively studied in previous machine learning literatures. It simply compares place- and time-centric store and hierarchical read methods that are intuitively suitable for different downstream tasks.\n    - One good example is the use of topological graph that stores observations of keypoints for agents in visual navigation, e.g., the widely applied DUET agent [1] for vision-and-language navigation [2], which is essentially close to the proposed SAT-PM-PH model. \n    - The experiments of action-conditioned image generation (Exp-5) and reinforcement learning agents (3.3) are not very convincing to me. Exp-5 is not a practical setting and 3.3 is relatively simple that cannot represent other reinforcement learning agents, see more below.\n\n- Missing experiments.\n    - I think this paper lacks investigation on the more recent embodied agents and their memory management approaches. \n    - The experiments presented in this paper are relatively small-scale and simple. I am concerning how the proposed methods might impact recent research that often apply more capable networks (and massive data) to learn generic spatiotemporal priors to facilitate representing and memorizing the observations. e.g., Figure 3 - an agent might be able to create a very compact representation for all ballet rooms from a single visit to each room? \n    - Following my previous point, I think this study emphasizes the scenario of memory constraint, but the experiments only considers small memory capacity, negelating the difference in representation (i.e., how to store an observation) and the difficulty in grounding from the queries to relevant memories. Overall, the generalization and practical influence of the proposed methods is not very clear to me.\n    - I think a valuable baseline is missing here: without explicitly defining any memory management strategies but gives the agent a certain memory budget and asks it to learn to update the memory by itself by training the agent on a mixture of data and tasks (e.g., a more general version of DNC mentioned in Appendix D.4).\n\n[1] Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation. Chen et al., CVPR2022.\n\n[2] Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments. Anderson et al., CVPR2018."
            },
            "questions": {
                "value": "I hope the authors can address some of my concerns in Weaknesses. I don't have any other questions here."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6931/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698458179906,
        "cdate": 1698458179906,
        "tmdate": 1699636808017,
        "mdate": 1699636808017,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HCcdCy9cfh",
        "forum": "Ts95eXsPBc",
        "replyto": "Ts95eXsPBc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6931/Reviewer_XL1m"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6931/Reviewer_XL1m"
        ],
        "content": {
            "summary": {
                "value": "Inspired by the significance of spatial context in the formation and retrieval of episodic memory, the paper proposes to add a spatial embedding into transformers and organize the episodic memory in a place-centric way to obtain a spatially aware Transformer model. The paper also proposes the Adaptive Memory Allocator, a memory management method based on reinforcement learning that aims to optimize the efficiency of memory utilization. The experiments on several environments demonstrate the advantages of the proposed model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- It's exciting and of significance to have a transformer capable of utilizing explicit spatial information and can act as better episodic memory.\n\n- Designing an adaptive memory allocator is useful\n\n- Extensive experiments on various environments and tasks.\n\n- The idea of incorporating spatial information into episodic memory is novel and interesting"
            },
            "weaknesses": {
                "value": "- It seems the \"spatial-aware transformers\" are achieved solely by adding a spatial embedding. And its specific design or implementation is not clearly mentioned. Only a sinusoidal positional embedding is mentioned in Exp-1, do other experiments also use this? How is this positional embedding enough to represent spatial relations, since space is not unidirectional as time? \n\n- The idea of an ADAPTIVE MEMORY ALLOCATOR is very interesting, but the implementation is quite trivial to me. What's the difficulty of learning this policy through Q-learning?\n\n- The organization of the paper could use some improvements, such as the missing Table 1 in the paper though mentioned in Exp-2, the confusing combination of the figures from different experiments, and many details deferred to the appendix, making it difficult to understand the details of the model or the experiments.\n\n- Much more details of the experiments are needed. What are the exact input and output of these environments? How is the input represented and fed into the transformers? How is the training implemented for different baselines?\n\n- The experiment environments are too toy setting for \"embodied agents\", it would strengthen the paper to add some real embodied environments, such as the Habitat[1] and TDW[2].\n\n[1] Habitat: A Platform for Embodied AI Research\n\n[2] ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation"
            },
            "questions": {
                "value": "See concerns in the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6931/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698623768965,
        "cdate": 1698623768965,
        "tmdate": 1699636807908,
        "mdate": 1699636807908,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "46bvrbUyVU",
        "forum": "Ts95eXsPBc",
        "replyto": "Ts95eXsPBc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6931/Reviewer_BEnq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6931/Reviewer_BEnq"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces and evaluates a novel framework comprising the Spatially-Aware Transformer (SAT) and Adaptive Memory Allocator (AMA) for a range of tasks demanding spatial awareness and effective memory management. Through a series of well-structured experiments, the authors demonstrate the capabilities of their proposed methods in various environments, such as the Room Ballet environment for prediction tasks, as well as in action-conditioned world modeling and spatially-aware image generation. The comprehensiveness of the methodology is evident, as it details how to incorporate spatial information into transformer models and effectively manage memory for different types of tasks. The experiments are extensive and cover different scenarios to validate the efficacy of the proposed framework."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "**Great Motivation:** The paper addresses a critical gap in existing models\u2019 inability to effectively integrate spatial information, which is very crucial for tasks in various domains. The introduction and literature review provides a compelling argument for why this integration is necessary, setting a solid foundation for the rest of the paper.\n\n**Comprehensive Method Explanation:** The authors provide a thorough and clear explanation of the Spatially-Aware Transformer and Adaptive Memory Allocator. The methodology section is well-structured, detailing each component of the system, the underlying theory, and the implementation specifics, which aids in the reproducibility of the results.\n\n**Extensive Experiments:** The paper goes beyond theoretical claims and validates the proposed framework through a series of diverse and challenging experiments. These experiments not only demonstrate the strengths of the SAT-AMA combination but also highlight its versatility across different tasks and scenarios. The image generation experiments are especially interesting and seem not to exist in its ancestor work of Towards mental time travel: a hierarchical memory for reinforcement learning agents."
            },
            "weaknesses": {
                "value": "**Usability in Complex Embodied AI Scenarios:** The paper, while comprehensive, could benefit from a deeper discussion on the applicability and scalability of the proposed methods in more complex embodied AI scenarios. Given the rising interest in virtual homes and ThreeDWorld with many rooms for task operation, readers would appreciate some discussion or insights into how the SAT-AMA framework can be adapted or scaled to meet the challenges presented by these intricate environments. A similar discussion could be like, \"Is the method scalable to larger environments?\" and \"How can the SAT-AMA framework be adapted for more intricate task operations?\" to provide a complete picture to the readers.\n\n**More discussion on AMA:** I appreciate the introduction and application of the Adaptive Memory Allocator (AMA) within the Spatial Awareness Transformer framework, as it presents a novel and promising approach to dynamically allocate memory based on task requirements. However, I find that there could be a more detailed analysis and discussion of AMA\u2019s strategy selection across different experimental settings and tasks. Understanding how AMA decides on specific spatial strategies could uncover valuable priors for selecting appropriate strategies based on the nature of the task, which would immensely benefit future work exploring new methods in this domain. It would be beneficial for the readers if the authors could provide insights into the distribution of strategies chosen by AMA, its dependency on the nature of tasks, and the correlation between strategy choices and task performance. Such analysis would not only deepen our understanding of AMA\u2019s workings but also offer practical guidance for researchers aiming to employ similar adaptive mechanisms in their work. A discussion on these aspects would significantly enhance the completeness and depth of the paper, providing valuable context and potentially guiding future innovations in the field."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6931/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6931/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6931/Reviewer_BEnq"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6931/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698868008246,
        "cdate": 1698868008246,
        "tmdate": 1699636807805,
        "mdate": 1699636807805,
        "license": "CC BY 4.0",
        "version": 2
    }
]