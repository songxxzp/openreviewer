[
    {
        "id": "deTlRptpNU",
        "forum": "7FeIRqCedv",
        "replyto": "7FeIRqCedv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3070/Reviewer_16rP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3070/Reviewer_16rP"
        ],
        "content": {
            "summary": {
                "value": "This paper examines the problem of class-specific semantic part segmentation, under a one/few-shot data setting.\n\nThe method proposed by the paper is to leverage recent advances and findings in diffusion models, specifically in stable diffusion the cross-attention modules capturing relevant spatial regions and being used for semantic correspondence.\n\nLearning is done by fine-tuning the text embedding from a stable diffusion model on an image and a corresponding part segmentation mask.  The embedding is optimized under a loss with three components, encouraging the cross-attention map, and introduced weighted accumulated self-attention map, to match the ground-truth segmentation, while not straying too far from the original stable diffusion loss.\n\nExperimental validation is done on car, horse, and face part segmentation, with comparable to favorable results when compared against several recent state-of-the-art baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- a nice framework for tackling the problem of one/few-shot semantic part segmentation, having clear benefits over existing proposed approaches in terms of amount of additional supervised annotations required for training, while maintaining similar performance\n\n- in general, paper details are clearly presented, including a thorough supplementary appendix"
            },
            "weaknesses": {
                "value": "- one of the stated contributions is the introduced weighted accumulated self-attention map.  This seems important enough that the incremental contribution of this component to the overall performance should perhaps be added to the main text rather than deferred to the appendix.  \n\n- further, within the appendix, Table 9 shows an improvement from adding WAS-attention, from 62.7 on average to 68.3.  I'm a little confused, then, on how this differs from Table 5, as I would have though the last row, setting $\\alpha=0$, would also correspond to dropping WAS-attention, and here the average performance is 68.0\n\n- lastly, on initial read, I was unsure of how text/text prompt was being used within the proposed method.  This was clarified by the first ablation study in A.2, but perhaps a sentence mention of this in the main text would also be helpful."
            },
            "questions": {
                "value": "see weaknesses above, in particular the issue raised in the second bullet"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3070/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778490862,
        "cdate": 1698778490862,
        "tmdate": 1699636252647,
        "mdate": 1699636252647,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bMp00kG2EX",
        "forum": "7FeIRqCedv",
        "replyto": "7FeIRqCedv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3070/Reviewer_VRf7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3070/Reviewer_VRf7"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a one-shot segmentation approach using the stable diffusion model. They pose the problem as one-shot optimization to perform object segmentation at different granularity levels conditioned on a single segmentation map. They take advantage of the self / cross-attention layer in the diffusion model to optimize the text embeddings for the given semantic segmentation task. They evaluate the proposed method on two public datasets and show its outperformance against the recent related work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed idea of optimizing the text embedding based on the attention maps for semantic segmentation is interesting and novel.\n\n2. The proposed method is shown to be advantageous in two datasets both quantitatively and qualitatively.\n\n3. The paper is well-written and easy to follow. The theoretical background is explained well.\n\n4. The limitations are discussed.\n\n5. The method is well-ablated for the different components."
            },
            "weaknesses": {
                "value": "1. The proposed method seems to be adapted for the segmentation task based on the work by Hedlin et al for unsupervised semantic correspondence.\n\n2. Related works which are not cited:\n[a] Burgert, Ryan, et al. \"Peekaboo: Text to image diffusion models are zero-shot segmentors.\" arXiv preprint arXiv:2211.13224 (2022).\n[b] Tian, Junjiao, et al. \"Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion.\" arXiv preprint arXiv:2308.12469 (2023).\n\n3. The number of works that are compared against is limited.\n\nMinor:\n1. Making the best result in the ablation study tables bold would improve the readability."
            },
            "questions": {
                "value": "1. Is there any specific reason behind not including the SegDDPM results in Tab. 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper is available on arXiv in ICLR template (mentioning \"Under review as a conference paper at ICLR 2024\"). I am not sure whether it is acceptable or not: https://arxiv.org/pdf/2309.03179.pdf"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3070/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699188853474,
        "cdate": 1699188853474,
        "tmdate": 1699636252577,
        "mdate": 1699636252577,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "doB5ePyED8",
        "forum": "7FeIRqCedv",
        "replyto": "7FeIRqCedv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3070/Reviewer_Fjuo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3070/Reviewer_Fjuo"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to retarget the Stable Diffusion model (SD) for few-shot semantic segmentation. Instead of taking the in-context learning or data generation approaches, this paper proposes a new pipeline which optimizes the text embedding in SD on the input image to \"find\" the text embedding to correspond to the segmented region. In addition to the cross-attention map of the optimized text token, it proposes a new self-attention map fusion module to regress the ground truth mask with a higher resolution. The proposed method achieves SOTA result on the benchmarks used in one previous work. The ablation studies shows the effectiveness of the model design."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- A novel and interesting idea. The idea of retargeting the feature representation in a pretrained generative model for few-shot semantic segmentation is not new. But it's novel to exploit the text branch in the text-based image generation model (i.e. Stable Diffusion). Instead of training the adaptor model to map between the generative features to the semantic masks which may overfit on input image features, the proposed method aims to find the text embedding which may be more generalizable.\n\n- The proposed method significantly outperforms SOTA result on the benchmarks used in the ReGAN paper. The ablation studies shows the effectiveness of the model design.\n\n- The paper provides enough details in the appendix for reproduction and the open-sourced code is straightforward to follow."
            },
            "weaknesses": {
                "value": "- Unconvincing importance of WAS attention: Figure 1 shows a good intuition that we need WAS attention to refine the object boundary. However, In Table 5, the contribution of the WAS attention doesn't seem to be significant. For the without WAS attention results (fig. 2a, 2c), will they be much improved by using GrabCut or other methods for segmentation refinement post-processing?\n\n- Lack of comparison on benchmark datasets like ADE-Bedroom-30 (used by segDDPM) and FSS-1000 (used in segGPT). The current quantitative results are only on horse/car/face datasets used in ReGAN. The result can be more solid with evaluation on diverse types of objects/parts.\n\n- Figure 3 is a confusing. Currently, it seems like the predicted noise is from the cross-attention map and the WAS-attention map. It'll be better to put the Unet from SD in the box. Then from this Unet, one output is the predicted noise from the original SD and the other output is fed to the attention-extraction module which selects layers and combines attention maps."
            },
            "questions": {
                "value": "- For the learned text embedding, are they interpretable? e.g. one can use the data-driven approach to find text tokens whose embedding are closest to the optimized ones.\n\n- How is the performance on other benchmark datasets like ADE-Bedroom-30 (used by segDDPM) and FSS-1000 (used in segGPT)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3070/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699297201402,
        "cdate": 1699297201402,
        "tmdate": 1699636252467,
        "mdate": 1699636252467,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EZY6t9ESeu",
        "forum": "7FeIRqCedv",
        "replyto": "7FeIRqCedv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3070/Reviewer_kAQy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3070/Reviewer_kAQy"
        ],
        "content": {
            "summary": {
                "value": "This paper presents SLiMe that allows for the segmentation of various objects or parts at different granularity levels with just one annotated example. The method leverages the knowledge embedded in pre-trained vision-language models and uses weighted accumulated self-attention maps and cross-attention maps to optimize text embeddings. The optimized embeddings then assist in segmenting unseen images, demonstrating the method's effectiveness even with minimal annotated data. The paper includes extensive experiments showing that SLiMe outperforms existing one- and few-shot segmentation methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1: SLiMe introduces a unique one-shot optimization strategy for image segmentation, which is useful when the available data is limited.\n\n2: The proposed method demonstrates superior performance over existing one- and few-shot segmentation methods in various tests, indicating its practical applicability and effectiveness.\n\n3: The paper showcases the method's versatility by successfully applying it to different objects and granularity levels, emphasizing its broad applicability."
            },
            "weaknesses": {
                "value": "My main concerns focus on the **text prompt**.\n\n1: The introduction of the text prompt is quite abrupt. In the Introduction, SLiMe is described as requiring only an image and a corresponding mask to achieve segmentation of any granularity. However, immediately after, the author talks about fine-tuning text embeddings. What is the definition of 'text' in this task? How are text embeddings obtained? And do different granularities correspond to the same text? The author is encouraged to provide further clarification.\n\n2: The role of \"text prompt\" in the method. The authors claim that \"our novel WAS-attention map to fine-tune the text embeddings, enabling each text embedding to grasp semantic information from individual segmented regions\". However, I haven't found evidence from the main text to illustrate the correspondence between \"text embedding\" and \"individual segmented regions\", especially in the arbitary granularity situation, which is one of the most important parts of this paper. Moreover, how to construct the text is also ignored."
            },
            "questions": {
                "value": "Please see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3070/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3070/Reviewer_kAQy",
                    "ICLR.cc/2024/Conference/Submission3070/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3070/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699585948661,
        "cdate": 1699585948661,
        "tmdate": 1700429094184,
        "mdate": 1700429094184,
        "license": "CC BY 4.0",
        "version": 2
    }
]