[
    {
        "id": "UvuYuVVl57",
        "forum": "SksPFxRRiJ",
        "replyto": "SksPFxRRiJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2763/Reviewer_YBBU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2763/Reviewer_YBBU"
        ],
        "content": {
            "summary": {
                "value": "The paper focus out-of-distribution generalization in neural networks, and specifically spurious correlations. The methodology considered in the paper is to remove spurious concepts from the neural network's representation of data. To mitigate the weakness of the pervious methods along this line, which is the removal of important features, the authors propose an algorithm that identifies orthogonal subspaces in the neural network representation, separating spurious concepts from the main task. Experiments are done on a toy dataset as well as three benchmark data commonly used in research of spurious correlations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(S1) The targeted problem of spurious correlation and OOD generalization is important.\n\n(S2) The authors dedicated detailed discussions on their proposed method both in main paper and appendix."
            },
            "weaknesses": {
                "value": "(W1) The organization and clarity of this paper needs improvement to enhance its readability. For example, section 2 seems to be a discussion on the problem setting, but it's not clear what exactly is the data distribution model that the paper focus on upon reading the section. The assumptions are not properly and formally stated. A more structured discussion, with clear definitions and assumptions, would make it easier to understand the focus of this paper.\n\n(W2) I would greatly appreciate more discussions on the experiment results. \n1. It seems that ERM outperforms many of the baselines in both the toy data and real data. Given that the baselines are intended to enhance the model's performance on the worst-group accuracy, it would be beneficial if the authors could shed light on why ERM seems to have a superior performance than the presented baselines. \n2. The selection of datasets\u2014Waterbirds, CelebA, and MultiNLI\u2014are frequently used to study spurious correlations. However, there exists a significant body of research [1-4] that specifically addresses these datasets and has demonstrated success in improving the worst-group accuracy. I understand that they are of different approach than this paper, but a proper discussion is needed on this line of methods and why they are not included in comparison. \n3. There are some methods in the previous line of work that are mentioned in Appendix E, but I'm confused which method exactly is implemented as the baseline. By group weighted (GW) ERM, the authors cited papers on last layer retraining (DFR) as well as paper discussing group/class-balanced baselines that are not related to last layer retraining. These methods share similar concepts but have different methodologies and performances. I would appreciate a clearer explanation/citation of the exact compared baseline method.\n4. In appendix E, it seems that the proposed method is not doing better than the baseline on the real datasets, but significantly outperforms it on the toy data. Is there a reasoning on why the toy data distribution specifically suits the proposed method, and would we be able to find similar distributions in real data?\n5. The considered datasets typically have a very large value of $\\rho$. For example, Waterbirds have $\\rho=0.95$. The experiments of this paper stops at $\\rho=0.9$, which does not seem like a conventional setting. It would be better to show improvement on the original dataset as well. \n\n[1] \"Just train twice: Improving group robustness without training group information.\" International Conference on Machine Learning. PMLR, 2021.\n\n[2] \"Environment inference for invariant learning.\" International Conference on Machine Learning. PMLR, 2021.\n\n[3] \"Correct-N-Contrast: a Contrastive Approach for Improving Robustness to Spurious Correlations.\" International Conference on Machine Learning. PMLR, 2022.\n\n[4] \"Robust Learning with Progressive Data Expansion Against Spurious Correlation.\" Advances in neural information processing systems. 2023."
            },
            "questions": {
                "value": "See in weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2763/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698783645064,
        "cdate": 1698783645064,
        "tmdate": 1699636219156,
        "mdate": 1699636219156,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JhqjeOIPyp",
        "forum": "SksPFxRRiJ",
        "replyto": "SksPFxRRiJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2763/Reviewer_YHWA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2763/Reviewer_YHWA"
        ],
        "content": {
            "summary": {
                "value": "This paper studies OOD generalization of neural network by removing spurious concepts with the proposed joint subspace estimation. The proposed method tries to separate spurious and main-task concepts in the embedding space, by jointly learning two low-dimensional orthogonal subspaces. It is an interesting work."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.The motivation is clear. Separating and removing spurious from main-task concepts can prevent the model from using the spurious concept for main-task classification.\n2.The organization and writing are well. \n3.Extensive experiments are conducted to demonstrate its effectiveness."
            },
            "weaknesses": {
                "value": "1. unclear description about the orthogonality assumption. How to guarantee this assumption at any scene? Why linear subspaces are orthogonal? Why linear?\n2.The novelty of the proposed jointly subspace estimation is limited and unclear.\n3.Why logistic regression can separate spurious from main-task concept?"
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2763/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698848907657,
        "cdate": 1698848907657,
        "tmdate": 1699636219086,
        "mdate": 1699636219086,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DXJ6F7GkYl",
        "forum": "SksPFxRRiJ",
        "replyto": "SksPFxRRiJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2763/Reviewer_SKcn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2763/Reviewer_SKcn"
        ],
        "content": {
            "summary": {
                "value": "This paper addressed an out-of-distribution generalization problem under learning from a training dataset with spurious concepts by introducing Joint Subspace Estimation (JSE). Specifically, the authors assume that the main-task and spurious subspaces are orthogonal in the embedding space, and they proposed an algorithm for estimating these two subspaces simultaneously. In experiments, the authors showed that JSE outperforms the other concept removal methods (INLP, RLACE, and ADV) on the benchmark datasets. (modified Waterbirds, CelebA, and MultiNLI)"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow. \n- The motivation for joint consideration of main and spurious subspace is sound and well reflected in the algorithm.\n- The proposed JSE outperforms the concept removal baseline methods on the benchmark datasets."
            },
            "weaknesses": {
                "value": "- One of my concerns is regarding the experiment settings and baselines. The benchmark datasets used in this paper (Waterbirds, CelebA, and MultiNLI) are also used to evaluate debiased learning algorithms (GroupDRO, DFR, JTT, etc.). Since the settings are the same, JSE should be compared with these debiased learning methods. Alternatively, another option can be made by designing and showing experiment settings that differentiate concept removal methods from debiased learning algorithms.  \n- The suggested algorithm contains double for loops, which looks costly. \n- The author leveraged PCA to reduce the computational cost, but it would bring out the information loss. \n- Assuming the known $y_{sp}$ is not practical. Recently published debiased learning algorithms do not require spurious labels."
            },
            "questions": {
                "value": "- Is it possible to train and classify another waterbird dataset in which waterbirds are on the sand background, and landbirds are on the forest background (without $y_{sp}$ for both training and validation dataset) if we leverage the subspace information from the original waterbirds dataset? \n- Does the orthogonality assumption always hold for every dataset and trained model? \n- Could you compare the computational cost of JSE with INLP?\n- Why $V_{sp}^\\perp$ is used to train a last layer instead of $V_{mt}$? Is there an ablation study?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2763/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2763/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2763/Reviewer_SKcn"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2763/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698865868813,
        "cdate": 1698865868813,
        "tmdate": 1699636219022,
        "mdate": 1699636219022,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ppzF8pLweY",
        "forum": "SksPFxRRiJ",
        "replyto": "SksPFxRRiJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2763/Reviewer_AJaV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2763/Reviewer_AJaV"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method for removing spurious correlations in the latent representation by estimating the two orthogonal subspaces --- one associated with the spurious concept and the other with the main-task concept. The proposed method, Joint Subspace Estimation, use statistical test to identify directions in the embedding space associated with the shortcut and main task. The method is evaluated on the Waterbird, CelebA and MultiNLI dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper tries to address an important problem in ML --- detecting and mitigation spurious correlations. The paper is well-written and easy to follow. \n\n* The idea of estimating orthogonal space is technically sound and novel.\n\n* Results on the CelebA and Waterbird dataset shows that the method is able to disentangle spurious concept from the main concept. Visualization in ** Fig 6. ** confirms and validates this.\n\n* The authors also evaluate an NLP dataset to demonstrate the method can work across different domains."
            },
            "weaknesses": {
                "value": "* The method depends on the availability of group labels (i.e., main task and spurious concept label), which is usually unavailable during training time. \n\n* The method assumes the pixel corresponding to the main concept and the spurious concept doesn't overlap. This may not always hold true --- for e.g., if the main concept is the shape and the spurious concept is colour, the pixels can overlap. \n\n* Definition of spurious concept in ** eqn 1** is not correct. \n> label $y_mt$ and the spurious features $x_sp$ are independent\n\nThey are independent but correlated in the training data (spurious **correlations**)\n\n> while the conditional and marginal distributions are same\n\nI think this is incorrect. If both the conditional and marginal distributions are the same, the joint distribution will be the same too.\n\n* Another major is the use of pre-trained Resnet50. Since it is trained on a large ImageNet data, it can extract features related to both main and spurious concepts. \n\n* The method is benchmarked against enough baselines. Baselines should also include methods that do not use linear subspace projections/estimations, such as [1,2]. \n\n\n\n\n[1] Correct-N-Contrast: A Contrastive Approach for Improving Robustness to Spurious Correlations\n[2] Just Train Twice: Improving Group Robustness without Training Group Information"
            },
            "questions": {
                "value": "* How do you define main concept and spurious concept? For e.g., in CelebA, gender classification is a much harder problem than blond vs. non-blond hair."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2763/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698888068560,
        "cdate": 1698888068560,
        "tmdate": 1699636218948,
        "mdate": 1699636218948,
        "license": "CC BY 4.0",
        "version": 2
    }
]