[
    {
        "id": "irHs0aW5Q3",
        "forum": "You77eOFDv",
        "replyto": "You77eOFDv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission714/Reviewer_qHhh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission714/Reviewer_qHhh"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a novel re-parameterization method named Re-parameterized Refocusing, which can establish connections across the channels of the learned conv kernel.\nExperiments show that the proposed method can improve the performance of many convnets in various tasks, such as image classification and segmentation, without introducing any computation cost in inference phase."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method is novel and effective.\n2. The experiments are solid."
            },
            "weaknesses": {
                "value": "None"
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission714/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698722825515,
        "cdate": 1698722825515,
        "tmdate": 1699635998680,
        "mdate": 1699635998680,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ohJ1SgSHDa",
        "forum": "You77eOFDv",
        "replyto": "You77eOFDv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission714/Reviewer_Z4v5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission714/Reviewer_Z4v5"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel approach to neural network training, specifically by bifurcating the weight training process into two distinct stages. However, I believe the validation of the method's effectiveness is not adequately comprehensive. Given the current state of the paper, my recommendation would be to not accept it in its present form."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The concept presented in the paper captures the interest.\n\n\nFigure 1 is exceptionally clear and effectively conveys the central concept of the method proposed in the paper.\n\n\nBecause the parameters are seamlessly integrated, the proposed method does not incur additional costs during the inference phase.\n\n\nThe paper employs techniques such as visualization to offer numerous valuable insights."
            },
            "weaknesses": {
                "value": "The proposed method incurs a higher training cost compared to the original approach. My concern does not lie with the cost itself; rather, I am questioning the accuracy and reliability of the validation process employed.\n\n\nThe authors believe that their method can indirectly connect information from different channels of the input, which is clearly a mistake. Let x = [x1, x2, ..., xc]; y = [y1, y2, ..., yc]. It is obvious that y1 does not contain the content from x2 to xc. If there is any, please prove it using the notation I provided.\n\n\n\nI am quite familiar with ImageNet, and I have concerns about the data presented in Table 1. I would like the authors to refer to the data from timm. The authors might argue that their values are lower than the standard libraries in timm because they only trained for 100 epochs, but I consider this a drawback. If the standard training procedure from timm was used, perhaps the authors' method would not show any gain. It is conceivable that the authors' method is essentially equivalent to providing more extensive training to an originally under-trained model, albeit with a longer training time. If a model is adequately and standardly trained, the authors' method should be unnecessary.\n\n\nIn the first experiment of Section 4.4, the authors should train all models to full convergence (for instance, more than 500 epochs) before making comparisons. Stepping back, when the authors retrain this model, do they use twice the training epochs?\n\n\n\nThe second experiment in Section 4.4 is incorrect. The authors should not simply use a small learning rate to fine-tune; instead, they should follow timm\u2019s practice of training from scratch for 500 epochs until convergence."
            },
            "questions": {
                "value": "See #Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission714/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782462699,
        "cdate": 1698782462699,
        "tmdate": 1699635998598,
        "mdate": 1699635998598,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vr32875tXh",
        "forum": "You77eOFDv",
        "replyto": "You77eOFDv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission714/Reviewer_3dDh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission714/Reviewer_3dDh"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a technique called Re-parameterized Refocusing Convolution, which is based on the idea of structural re-parameterization, i.e., incorporating more learnable parameters into the model during training and training them for better performance. These parameters are merged into the original model's parameters during inference to achieve the goal of not introducing additional inference costs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The approach in this paper can be viewed as \"convolution B of convolution A\", where A is the convolution parameter trained by the pre-training process and kept frozen once trained.B is the convolution parameter that continues to be trained.\n\nThe method in this paper has a slight advantage over several other structural reparameterization and weight reparameterization methods in terms of results.\n\nI think the conclusion of the final analysis, \"Re-parameterized refocusing reduces redundancy between channels\", demonstrates well the changes that the methods in this paper can make to a pre-trained convolutional model.\n\nThe experiments included a variety of convolutional models."
            },
            "weaknesses": {
                "value": "Observe that the ImageNet experimental results have about 1% performance improvement on many models, but also a lot more Params.\n\nThe network architectures that come into play are generally early CNN models such as ResNet, DenseNet, MobileNet family, etc. For modern convolutional architectures such as SlaK, RepLKNet, HorNet, etc., the effect is currently unknown."
            },
            "questions": {
                "value": "1\tThe refocusing technique seems to be one that can be iterated. Can the refocusing technique in this paper continue to be iterative? I.e., after doing one refocusing exercise, then the next one. Will the results continue to improve?\n\n2\tFor the base weight W_b, one of the points claimed by the refocusing technique is the possibility of establishing links between its individual channels. Why is this necessary? Each channel of the base weight kernel has its own role, so to link them?\n\n3\tDuring refocus training, the result after convolution of the base weight W_b with its previous features can be seen as a new \"feature\". The transform weight W_t can be seen as trainable to process this new \"feature\". This process is equivalent to fine-tuning the convolution after \"injecting\" new parameters. I would like to ask if any experiments with other models (e.g. new convolutional networks) have found that this degrades performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission714/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699527051549,
        "cdate": 1699527051549,
        "tmdate": 1699635998502,
        "mdate": 1699635998502,
        "license": "CC BY 4.0",
        "version": 2
    }
]