[
    {
        "id": "HKe7igwDrU",
        "forum": "NvbeD9Ttkx",
        "replyto": "NvbeD9Ttkx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5439/Reviewer_S4Ex"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5439/Reviewer_S4Ex"
        ],
        "content": {
            "summary": {
                "value": "his paper provides a novel algorithm to accelerate the first-order optimizers by incorporating second-order information. The proposed method is well-motivated and the experimental results show the efficiency of it."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The method is novel which incorporate the descent direction of first-order methods (base optimizer) with second-order information. The empirical study is comprehensive and show the efficiency of the proposed methods. This paper is an interesting attempt on accelerating the first-order methods by using second-order information."
            },
            "weaknesses": {
                "value": "The method requires to compute the extreme eigenvalues and vectors of Hessian by lanczos algorithm, it will raises much more computational cost per iteration than the first-order methods. Additionally, it is unknown how to choose $k$ and $l$ in the proposed algorithm. Neither the theoretical analysis and empirical study miss the part on evaluating how $k$ and $l$ affect the behavior of the methods. Besides, the parameters are too much, not only $k$, $l$ need to be chosen, Algorithm 2 also requires the parameter for learning rates $\\alpha$, the learning rate for the base optimizer."
            },
            "questions": {
                "value": "1. There are some optimizers which also use the hybrid directions of first and second-order methods, or partial information of the Hessian. The author may need to compare and discuss them [1 ,2].\n\n2. Can the authors provide some discussion and numerical evaluation on how to choose $k$ and $l$.\n\n\n**Reference**\n\n[1]. Zhang C, Ge D, Jiang B, et al. DRSOM: A Dimension Reduced Second-Order Method and Preliminary Analyses[J]. arXiv preprint arXiv:2208.00208, 2022.\n\n[2]. Liu H, Li Z, Hall D, et al. Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training[J]. arXiv preprint arXiv:2305.14342, 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5439/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5439/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5439/Reviewer_S4Ex"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5439/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698503927155,
        "cdate": 1698503927155,
        "tmdate": 1699636552894,
        "mdate": 1699636552894,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FI69WWwdnL",
        "forum": "NvbeD9Ttkx",
        "replyto": "NvbeD9Ttkx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5439/Reviewer_XiKZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5439/Reviewer_XiKZ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes FOSI -  a novel meta-algorithm that improves the performance of any base 1st order optimizer\nby efficiently incorporating 2nd-order information.\n\nIn each iteration, FOSI implicitly splits the function into two quadratic functions\ndefined on orthogonal subspaces, then uses a 2nd-order method to minimize\nthe first, and the base optimizer to minimize the other.\n\nEmpirical results shown on many tasks in real-world domain to show the efficacy of POSI."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Nice analytics with detailed derivations and explanation.\n\nLarge amount of empirical studies shown with experimental results.\nSteps of the algorithm are clearly specified.\n\nEnjoyed reading the paper."
            },
            "weaknesses": {
                "value": "A few failure cases may be discussed.\n\nAlthough decomposing the problem into two parts may not specifically be novel,\nFOSI\u2019s inverse preconditioner seems to be quite a good idea.\n\nSimilar work on those lines of decomposing may be mentioned."
            },
            "questions": {
                "value": "In the statement (step 2m Lemma 1):\n\"The preconditioner P is symmetric and PD:.\n\nIs it possible to use a different font for the matrix \"P\", which has no conflict of P (positive) in \"PD\" ?\nor vice-versa.\n\nHope the code of the paper, to implement FOSI for any application of Optimization,\nwill be released soon by authors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5439/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698557390150,
        "cdate": 1698557390150,
        "tmdate": 1699636552794,
        "mdate": 1699636552794,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LMDXbfaNr3",
        "forum": "NvbeD9Ttkx",
        "replyto": "NvbeD9Ttkx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5439/Reviewer_dEKh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5439/Reviewer_dEKh"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors present a hybrid method which addresses the challenge of incorporating second-order information in machine learning approaches due to the computational difficulty in high dimensions. This method first splits the space into two orthogonal subspace. Then, the author use first-order method to minimize one of the subspace and use second method to minimize the other one. They also analyze the convergence of FOSI and establish certain conditions under which the proposed method improves the base method. Numerical experiments illustrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea that splitting the raw space into two orthogonal spaces is interesting. The authors adopt the Lanczos to give a possible way to construct these spaces."
            },
            "weaknesses": {
                "value": "One of my major concern is that the memory consumption and computational complexity are very high especially for large-scale neural networks. This will limit the usage of the proposed method. Besides, it is not clear how to handle the communication cost  and the computation of $V$ in the distributed setting.\n\nThe scale of the network architecture used in the numerical experiments is limited. It will be more convincing if the authors can show the effectiveness of the proposed method in larger applications."
            },
            "questions": {
                "value": "In comparison with second-order methods, it is better to compare the proposed with KLBFGS but not LBFGS because the former method is designed specifically for deep learning tasks. It is suggested to add comparison with Shampoo[1] or NGPlus[2], which has good effectiveness in practice. \n\nit is better to tune the hyper-parameters of ADAM in numerical experiments to make the baseline strong enough. \n\nWhy the curve of FOSI-HB decreases in Figure 2? \n\nCan randomized numerical algebra be combined in ESE procedure? This will help reduce the computational cost.\n\n[1] Anil, Rohan, et al. \"Scalable second order optimization for deep learning.\" arXiv preprint arXiv:2002.09018 (2020).\n[2] Yang, Minghan, et al. \"An efficient fisher matrix approximation method for large-scale neural network optimization.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 45.5 (2022): 5391-5403."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5439/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5439/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5439/Reviewer_dEKh"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5439/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772409210,
        "cdate": 1698772409210,
        "tmdate": 1699636552692,
        "mdate": 1699636552692,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NTRQgKRPtq",
        "forum": "NvbeD9Ttkx",
        "replyto": "NvbeD9Ttkx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5439/Reviewer_UYzc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5439/Reviewer_UYzc"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors presented a novel optimization method called FOSI, which is a hybrid optimizer algorithm that combines the first-order optimizer with Newton's method. They presented some theoretical analysis as well as some empirical results from the numerical experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This submission is well-organized with clear language and structures. The authors gave detailed description and some theoretical analysis for the proposed algorithms. They also conduct a lot of numerical experiments on deep learning problems and these empirical results are pretty good compared with some state-of-art optimization methods. The idea is pretty interesting and enlightens some promising future direction for the optimization community."
            },
            "weaknesses": {
                "value": "There are some disadvantages regarding this submission.\n\nThe authors only gave the theoretical results for the stochastic optimization problem. What's the convergence rate for the general convex optimization problem? What is the convergence rate for the strongly convex setting? If the authors could add and present these theoretical analysis. This could significantly improve the quality of this submission.\n\nIt's better to put the detailed algorithm from the appendix to the main part of the paper.\n\nIt's better to put the section 5 related work part in the section 1 introduction part."
            },
            "questions": {
                "value": "Please check the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5439/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5439/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5439/Reviewer_UYzc"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5439/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698803890809,
        "cdate": 1698803890809,
        "tmdate": 1699636552575,
        "mdate": 1699636552575,
        "license": "CC BY 4.0",
        "version": 2
    }
]