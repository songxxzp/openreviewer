[
    {
        "id": "IMFX2onIVy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3092/Reviewer_pjci"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3092/Reviewer_pjci"
        ],
        "forum": "60e1hl06Ec",
        "replyto": "60e1hl06Ec",
        "content": {
            "summary": {
                "value": "The paper presents a new method to avoid the \"simplicity bias\" of neural networks, and learn more diverse features. This leads to models with better OOD generalization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Simple method.\n\n- Includes a theoretical analysis of the method under 2 lenses, a Gaussian mixture model, and causal structural model (appendix F2). These appear sound, given that the starting assumptions are met (see related comments in W1 below).\n\n- Large set of experiments on various benchmarks (from toy to realistic data).  Good performance on some realistic ones (e.g. Camelyon17)."
            },
            "weaknesses": {
                "value": "W1. The benefits of the proposed method are entirely based on the premise that simple features = bad features.\n\nI think this is clear to the authors and also made clear upfront, but the limitations should be stated more prominently. In the introduction:\n\"*we take the viewpoint that features that are usually regarded as bein spurious for the task are often simple and quite predictive*\"\nThis is not just a \"viewpoint\", the authors should clearly say that this is the critical premise on which the entire work depends.\n\nLater on:\n\"*Models trained with ERM tend to focus on simple features (such as background) that do not generalize*\"\nThis is not correct. Simple features can very well be features that do generalize. The fact that deep learning is successful more often than not, and that OOD generalization is only a subfield of research, is the proof that the features learned by default are usually \"good\" ones.\n\nIn appendix E:\n\"alleviating simplicity bias can be a useful inductive bias in such cases. This is consistent with our experimental evaluations on several datasets\"\nI'm not sure that the authors realize that there is a heavy selection bias in the datasets that they use (which are datasets specifically selected on even purposefully *builts* to contain the kind of simple-but-spurious features that they propose to avoid). The only dataset where the distinction between the relevant/spurious feature is more realistic and not as clear (CelebA) is also the dataset on which the proposed method is much less effective.\n\nIn the summary of contributions:\n\"*We empirically evaluate and validate the hypothesis that spurious features are simpler than invariant features, with respect to representative datasets from Table 1*\"\nI do not think this is a valid claim: most of these datasets are purposefully selected or built to showcase the benefits of methods that \"debias\" simple features (most prominently: colored MNIST, waterbirds). So by definition, this hypothesis will be verified. If the authors want to validate the hypothesis, they should sample a random selection of datasets (NOT only from the OOD/debiasing literature!).\n\nNote that these issues are not specific to this paper and plague the whole field of debiasing methods. The authors do already take steps to make explicit the meaning of \"spurious features\" so I'm hopeful that this paper can rise above the standard of the field with just a few more clarifications.\n\n---------\n\nW2. Novelty of the method. The general idea of training one standard (biased) model, then training another regularized by the first one, has been thoroughly explored in the literature. All the references below (some of which are cited in the literature review) are based on this idea of training one initial biased model, with various improvements on top.\n\nThe method proposed in this paper is an implementation this general idea with a conditional-MI regularizer. The technical novelty therefore seems very limited.\n\n[1] A too-good-to-be-true prior to reduce shortcut reliance\n\n[2] Learning from failure: Training debiased classifier from biased classifier\n\n[3] Towards Debiasing NLU Models from Unknown Biases\n\n[4] A Conservative Approach for Unbiased Learning on Unknown Biases\n\n[5] BiasEnsemble: Revisiting the Importance of Amplifying Bias for Debiasing\n\n[6] BoosterNet - Improving Domain Generalization of Deep Neural Nets using Culpability-Ranked Features\n\n[7] Chroma-VAE Mitigating Shortcut Learning with Generative Classifiers\n\n[8] Learning Debiased Classifier with Biased Committee\n\n[9] MaskTune Mitigating Spurious Correlations by Forcing to Explore\n\n[10] Measures of Information Reflect Memorization Patterns\n\n[11] Roadblocks for Temporarily Disabling Shortcuts and Learning New Knowledge\n\n[12] Self-supervised debiasing using low rank regularization\n\n[13] Unsupervised Learning of Unbiased Visual Representations\n\n[14] BiasAdv: Bias-Adversarial Augmentation for Model Debiasing\n\n---------\n\nW3. The results on subgroup robustness are not that great (Table 8). I suggest replacing the table (which is tedious to read) with a scatter plot showing average vs. worst-group accuracy. It then becomes clear that the proposed method (and others, with the exception of GroupDRO that can leverage the additional knowledge required to solve OOD generalization) \"simply\" shifts the balance of relevant/spurious features such that the gains in worst-group accuracy are almost matched by a decrease in average accuracy.\n\n---------\n\nW4. The \"future directions\" proposed in the conclusion are very generic. For example, proposing the following is pretty much useless as a statement:\n\"*further explore the capabilities and limitations of our approach and to also further understand its theoretical properties*\"\n\nWhat are you thinking about exactly? Why wasn't it done yet for this paper?\n\nI do not understand what this means: \"*auditing large models with respect to much simpler models*\".\n\nI also do not understand: \"*explore the power of similar approaches for other desiderata*\"."
            },
            "questions": {
                "value": "Please comment on W1 and W2.\n\nW1 is a very important issue, but I think it could be addressed with some edits.\n\nW2 is the critical issue (lack of technical novelty).\n\nW3 is (I believe) a consequence of W1.\n\nW4 is a very minor comment/suggestion."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3092/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3092/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3092/Reviewer_pjci"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3092/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697278227815,
        "cdate": 1697278227815,
        "tmdate": 1700761497414,
        "mdate": 1700761497414,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "svRKr5WeQP",
        "forum": "60e1hl06Ec",
        "replyto": "60e1hl06Ec",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3092/Reviewer_h3b4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3092/Reviewer_h3b4"
        ],
        "content": {
            "summary": {
                "value": "In this work authors propose a debiasing method based on minimizing conditional mutual information between a biased model and an unbiased one. The proposed method is tested on standard debiasing vision benchmarks and shows promising results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method does not require bias group labels\n\n- The proposed method shows good results overall \n\n- It does not require substantial training complexity compared to other methods"
            },
            "weaknesses": {
                "value": "- The approach is not completely novel; MI has been investigated in many works as a way to mitigate bias eg. [1,2,3] and most importantly  [4] which is very similar to the proposed method. \n\n- The experimental evaluation does not compare to many established debiasing techniques eg [5-10] just to name some of the most recurring techniques.\n\n\n[1] Zhu, Wei, et al. \"Learning bias-invariant representation by cross-sample mutual information minimization.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n\n[2] Han, R., Wang, W., Long, Y., & Peng, J. (2022). Deep Representation Debiasing via Mutual Information Minimization and Maximization (Student Abstract). Proceedings of the AAAI Conference on Artificial Intelligence, 36(11), 12965-12966. https://doi.org/10.1609/aaai.v36i11.21619\n\n[3] Ragonesi, Ruggero, et al. \"Learning unbiased representations via mutual information backpropagation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[4] Tartaglione, E. (2022, November). Information Removal at the bottleneck in Deep Neural Networks. In 33rd British Machine Vision Conference 2022,{BMVC} 2022, London, UK, November 21-24, 2022.\n\n[5] Lee, Jungsoo, et al. \"Learning debiased representation via disentangled feature augmentation.\" Advances in Neural Information Processing Systems 34 (2021): 25123-25133.\n\n[6] Byungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim, and Junmo Kim. Learning not to learn:\nTraining deep neural networks with biased data. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2019.\n\n[7] Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh. Learning de-biased\nrepresentations with biased representations. In International Conference on Machine Learning\n(ICML), 2020.\n\n[8] Mohsan Alvi, Andrew Zisserman, and Christoffer Nellaker. Turning a blind eye: Explicit removal \u02da\nof biases and variation from deep neural network embeddings. In Proceedings of the European\nConference on Computer Vision (ECCV), pp. 0\u20130, 2018.\n\n[9] Tartaglione, E., Barbano, C. A., & Grangetto, M. (2021). End: Entangling and disentangling deep representations for bias correction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 13508-13517).\n\n[10] Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, et al. Rubi: Reducing unimodal\nbiases for visual question answering. In Advances in neural information processing systems, pp.\n841\u2013852, 2019."
            },
            "questions": {
                "value": "- See weaknesses; \n\n- How does MI minimization relate to adversarial debiasing setups, i.e. when a classifier is trained on the bias labels (on the same feature space) and the main model is trained to minimize its accuracy while minimizing CE on the target task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3092/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3092/Reviewer_h3b4",
                    "ICLR.cc/2024/Conference/Submission3092/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3092/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698916632379,
        "cdate": 1698916632379,
        "tmdate": 1700586050529,
        "mdate": 1700586050529,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MlhaHYD0PM",
        "forum": "60e1hl06Ec",
        "replyto": "60e1hl06Ec",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3092/Reviewer_5jvr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3092/Reviewer_5jvr"
        ],
        "content": {
            "summary": {
                "value": "The paper focus on the issue of spurious correlation that affects the neural networks and make a connection between this phenomenon and simplicity bias. As models tend to favor learning simpler features over more complex ones, even if the latter are more informative, the paper links the similar phenomenon of learning spurious feature to learning simple features. To counteract this, the authors propose a framework that first trains a simple model and then regularizes it to encourage the use of a diverse set of features for predictions. The approach is demonstrated to be effective in various scenarios, enhancing OOD generalization and robustness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-structured and easy to follow. The authors made good efforts to motivate their study with empirical findings. The illustrative figures are helpful. \n2. The authors presented experiments across a wide range of datasets. The analysis is comprehensive and the evaluation of CMID is done on multiple aspects. CMID does achieve satisfactory results on a majority of these tasks."
            },
            "weaknesses": {
                "value": "1. Definition 1 lacks formality. The terms \"high (in-distribution) accuracy\" and \"certain complexity\" require clearer specification. Additionally, the description of a \"simple model\" as one with significantly lower complexity than benchmark models is vague. I understand that there might not be a specific threshold on \"high accuracy\", but I think this definition can be formalized better. Can we maybe define models with \"certain complexity\" as those capable of learning the training data so that the training loss converges to near-zero? Conversely, \"simple models\" could be characterized as those lacking the capacity to adequately fit the training data.\n2. Definition 2 also lacks clarity and formality. Another concern is that, spurious feature has been an established concept. It would be more appropriate to treat Definition 2 as a hypothesis, which needs to be substantiated with sufficient empirical observations.\n3. In Assumption 1, which outlines the data model, there is a need to ensure that the spurious feature here is consistent with that in Definition 2. Currently, it does not seem evident that the spurious feature in Assumption 1 matches that in Definition 2.\n4. (minor) Several recent works [1-2] also present theoretical results on the study of spurious correlation. Especially, [1] seems to align well with the finding in this paper but through different perspectives. It would be better have a brief discussion on the alignments and differences of these studies.\n\n[1] \"Robust Learning with Progressive Data Expansion Against Spurious Correlation.\" Advances in neural information processing systems. 2023.\n\n[2] \"Understanding and Improving Feature Learning for Out-of-Distribution Generalization.\" Advances in neural information processing systems. 2023."
            },
            "questions": {
                "value": "See in weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3092/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698924810917,
        "cdate": 1698924810917,
        "tmdate": 1699636255248,
        "mdate": 1699636255248,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x9PB7fgNsd",
        "forum": "60e1hl06Ec",
        "replyto": "60e1hl06Ec",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3092/Reviewer_Q6kW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3092/Reviewer_Q6kW"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to mitigate simplicity bias by incentivizing complex feature learning via a new regularization term. Specifically, the conditional mutual information between the model\u2019s features, and those of a pretrained \u201csimple\u201d model is added to the training objective.\n\n**Edit** Raised score to 5 based on author responses and additional experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow. The basic proposal of using CMI for regularization makes intuitive sense, and the evaluation covers a number of datasets and metrics for robustness."
            },
            "weaknesses": {
                "value": "The contributions can be considered among 3 axes: proposal, empirical evaluation, and theory. In this reviewer\u2019s assessment, each of these axes have weaknesses, and the sum total falls somewhat short of a solid contribution.\n\n* Proposal: Novelty w.r.t. other feature diversity methods e.g., ESB [1] which is closest in spirit, and uses a gradient-based diversity incentive instead of CMI. Also many other related papers cited by the authors in the appendix.\n* Evaluation: \n    * Weak empirical results, eg, a simple method like JTT[2] is beating CMID across almost all datasets for Group robustness. Refer to table 7.  \n    * In generalization, too (WILDS / Fairness), surprisingly CMID does not have the best OOD, something that would be expected of a method designed to avoid spurious features. \n    * Missing head-to-head comparisons against more recent methods (some of them cited but not compared), including LWBC [3], SIFER [4], ESB [1]. \n    * Not necessary, but authors may consider evaluating on other datasets such as BAR, NICO, Imagenet-A/C/R, other datasets from DomainBed, etc., for a more thorough comparison for the above and other methods.\n* Theoretical results appear weak / not directly applicable to the problem setting (please see below.)\n* Conceptual: A key, unaddressed, question (as far as I could tell) is what happens when the simple model also depends to some extent on complex features. It seems reasonable to worry that the regularization can cause more harm than help. \n    * relatedly, the idea that simple=bad/spurious, complex=good/invariant, for some mathematical definition of simple & complex, seems unlikely to hold water in real world applications.\n\n\n\nOther questions/comments.\n* As I understand it, Theorem 1 considers features {invariant, spurious} that are in the same complexity class (i.e., gaussian), and a linear predictor. The \u201csimple model\u201d is **guaranteed to only use** the spurious features. Under these circumstances the CMI biases the model towards the invariant features. This seems not very convincing:\n    * It would appear that given the strong assumptions above, the claim follows in a straightforward manner. In any case, guaranteeing the \u201csimple model\u201d only uses simple features seems difficult in practice.\n    * The central claim that simple and complex features are from different complexity classes (see e.g., Section 2 and figure 2) is not captured here. Even in that case, the interesting result / question would be when the \"simple\" model has nonzero weight on invariant features as well.\n\n\n\n [1] Evading the Simplicity Bias: Training a Diverse Set of Models Discovers Solutions with Superior OOD Generalization by Teney et. al.\n\n [2] Just Train Twice: Improving Group Robustness without Training Group Information by Liu et. al.\n\n [3] Learning Debiased Classifier with Biased Committee by Kim et. al.\n\n [4] Overcoming Simplicity Bias in Deep Networks using a Feature Sieve by Tiwari et. al."
            },
            "questions": {
                "value": "* Results: \n    * Texture vs shape: this is good, but a) does it materially improve accuracy on IN-9? Or generalization on IN challenge datasets? b) how does it compare against the other feature diversity approaches?\n    * Generalization: Why, in your understanding, do other methods have better generalization but potentially worse in-domain accuracy than CMI? Given the nature and intent of the regularization (learn features that are conditionally independent of \"simple features\" presumably also learned on indomain data), one would have expected the exact opposite finding."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3092/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3092/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3092/Reviewer_Q6kW"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3092/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699008088430,
        "cdate": 1699008088430,
        "tmdate": 1700713907985,
        "mdate": 1700713907985,
        "license": "CC BY 4.0",
        "version": 2
    }
]