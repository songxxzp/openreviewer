[
    {
        "id": "FN8gbACfcY",
        "forum": "fibxvahvs3",
        "replyto": "fibxvahvs3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5446/Reviewer_GSBU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5446/Reviewer_GSBU"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel dataset, GAIA, intended for evaluating general AI assistants. The tasks within the dataset are designed to be easy for humans, but challenging for state-of-the-art models. These tasks necessitate a variety of skills including web browsing, tool use, and reasoning. Furthermore, the answers to these problems are simple and easy to evaluate."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The design principles of GAIA are commendable and innovative, providing a comprehensive framework for creating similar datasets. This is the type of dataset that would ideally test all AI assistant models.\n2. The procedures for creating the dataset are reliable and well-structured, utilizing human input to generate questions and validate the quality of the dataset.\n3. The paper is well-written, providing clear and understandable content."
            },
            "weaknesses": {
                "value": "1. The dataset's size is relatively small, which might limit its applicability.\n2. The paper does not provide sufficient assurance regarding the coverage or diversity of the problems within the dataset."
            },
            "questions": {
                "value": "1. What are the main challenges that GAIA presents to current models/systems? Do these challenges stem from incorrect tool usage or mistakes made during step-by-step execution? Can you provide an error analysis of the current models?\n2. The paper mentions that \"level 3 are questions for a near-perfect general assistant.\" How do you define a perfect general assistant, and how do you ensure that good performance on your dataset (at level 3) implies a perfect general assistant? These questions could help determine if there are other abilities not covered by your dataset.\n3. What is the distribution of questions across the three levels?\n4. It would be helpful to have a detailed table showing the performance of each model, including the total score for all three levels."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5446/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762929877,
        "cdate": 1698762929877,
        "tmdate": 1699636554172,
        "mdate": 1699636554172,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5cxaoUp0Vz",
        "forum": "fibxvahvs3",
        "replyto": "fibxvahvs3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5446/Reviewer_FuKx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5446/Reviewer_FuKx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Gaia, a general purpose benchmark for image+text assistants. The questions generally require web searching and thus doing retrieval over a lot of documents with images. A few things:\n\n* There are 466 highly curated questions in the dataset \n* the questions are associated with 'levels' corresponding to the # of tools/steps required \n* at creation time, the answers can be found by looking at websites that don't have bots banned via robots.txt files\n* the creation process is crowdsourced and meant to focus on tool use\n\n\nThe paper evaluates GPT4, autogpt4, GPT4+plugins and includes human performance. Note that this is GPT4 without vision so it can't do any of the image questions, nor can it do retrieval on documents with images.\n\n----\n\nUpdate: I read the other reviews and the rebuttal, thanks (and thanks for promising to add GPT + vision results!). I still support this paper and recommend it be accepted; I can't find a compelling reason from the negative review (I think ICLR seems like a fine venue for this paper)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "To this reviewer, creating harder benchmarks that aren't (as) gameable is an important research direction. The ideas proposed seem interesting and probably important for the community to discuss further. E.g. what _is_ actually important for a multimodal assistant to have? which tools should it be able to use? etc etc. The idea of making it hard by requiring open-ended internet use seems like a novel contribution to this reviewer. The open-endedness should hopefully make it harder versus older multihop datasets like MS-Marco, etc. that already provide a list of optional passages to reason over."
            },
            "weaknesses": {
                "value": "I think the paper could use more details around how the benchmark works and was constructed, but that's stylistic preference. as a result I have some questions, will update my score if they can be resolved --"
            },
            "questions": {
                "value": "My main concern is about evaluation (wasn't sure whether to put these under 'questions' or 'weaknesses' as there's significant ambiguity here on my end:\n\n* How are humans scored? The paper writes \"Human score corresponds to the portion of correct answers by validation annotators for valid questions.\" Are these the same validators involved for validating the data or a new fresh set of annotators? This is important to this reviewer as there are many datasets that claim suspiciously high human performance because they didn't run validation with a new set of annotators.\n* How do you grade open-ended answers? e.g. \"St. Petersburg\" vs \"Saint Petersburg\". I couldn't tell from the paper how the correctness is actually determined.\n\nI'd also be curious as to the portion of answers that GPT4 can't solve right now because it doesn't have access to the right tools (e.g. image understanding)?, versus using those tools well. Similarly I'd be curious if GPT4 with vision can answer those questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5446/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5446/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5446/Reviewer_FuKx"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5446/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799312045,
        "cdate": 1698799312045,
        "tmdate": 1700715944680,
        "mdate": 1700715944680,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RFdAViPIX7",
        "forum": "fibxvahvs3",
        "replyto": "fibxvahvs3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5446/Reviewer_CDvr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5446/Reviewer_CDvr"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a new benchmark for General AI assistants, called GAIA. The tasks require complex reasoning and tool usage skills (e.g. web browsing, integrating information from different modalities, and so on). The data is human-generated and human-validated. The resulting benchmark is highly challenging for current LLMs while humans perform quite well on it."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper clearly identifies its goals and places itself in the context of previous research. I especially appreciated how the authors broke away from the common thread of evaluating AIs on tasks that are more and more challenging for humans, as opposed to focusing on cases which are easy for humans but challenging for AIs.\n\nIn this context, however, it might be wise to more clearly acknowledge works that also break away from the abovementioned trend. E.g. it would benefit the paper to say a few more words about the Abstraction and Reasoning Corpus (https://arxiv.org/abs/1911.01547) which was cited, but, in my opinion, without fully clarifying why it was relevant.\n\n- The paper is very clearly written and is a pleasure to read.\n\n- I believe that the paper has the potential to be highly impactful and of broad interest and use to large portions of AI and ML community."
            },
            "weaknesses": {
                "value": "Overall, the paper has a sound design and already acknowledges some of its limitations none of which I find to be a deal-breaker. That being said,\n\n- The biggest limitation not fully discussed is the dataset's modest size, which, unfortunately, restricts it solely to the role of a performance benchmark rather than a potential source of training data. This limits the paper's potential impact.\n\n- Another issue, discussed, but not fully acknowledged as a limitation, is the lack of \"partial success\" indicators. This is mitigated by the presence of questions with different difficulty levels, but might still be problematic. When solving a question requires a complex sequence of actions, it is highly desirable to have some measure of where the process breaks down. While the authors speculate that, potentially, in the future \"the paradigm of LLMs calling tools for every task other than text understanding might not stay\", the reality of today is such that having more nuanced/partial feedback would be helpful.\n\nNone of the issues listed above is a disqualifying weakness, however. I offer some suggestions on how, in my view, these issues could be mitigated in the next section."
            },
            "questions": {
                "value": "1) The authors insist that a component of human labor is crucial to create high-quality questions (\"we believe human annotators are currently essential to have diverse and grounded questions, as opposed to programmatically generated ones\"). However, in my opinion, this might be underselling one key idea behind their own work.\n\nIt seems that, fundamentally, the questions in GAIA are challenging because they require reverse-engineering a fairly random process of question generation, which includes randomly choosing which tool to use, what to google, and so on. So the problems are challenging for AIs because they are under-defined inverse problems. \n\nAt the same time, problem generation is a (straight)forward process. The instructions given in appendix C could, quite possibly, be adapted to tool-assisted LLM prompts. Such LLM-driven automation might offer a good compromise of data price and quality, and such approaches seem to be getting traction (e.g. https://arxiv.org/abs/2303.15056, https://arxiv.org/abs/2305.13877).\n\nI would like to highly encourage the authors not to dismiss this direction, as adding such automation might dramatically increase the impact of their work. In the very least, it might be worth acknowledging this possibility in the discussion.\n\n2) As a potential way to mitigate the absence of \"partial success indicators\", it might be useful to release a version of the dataset where one possible ground truth solution trace is explored and, potentially, in which associated files are pre-processed (e.g. images are replaced by their detailed verbal descriptions). This simplified dataset version would allow to better diagnose existing models, and to disentangle a) conceptual difficulties of aggregating information from different sources and b) procedural challenges (choosing what tool to use, what to google, and, in general, what information to collect)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5446/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827832579,
        "cdate": 1698827832579,
        "tmdate": 1699636553960,
        "mdate": 1699636553960,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Jqu7Syimy1",
        "forum": "fibxvahvs3",
        "replyto": "fibxvahvs3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5446/Reviewer_nJWa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5446/Reviewer_nJWa"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new benchmark for general AI assistants, GAIA. The benchmark consists of 466 questions across 3 levels of complexity, defined by the number of steps required to solve the task. The questions are designed with three principles in mind: (1) conceptually simple, but tedious for humans. (2) Interpretability, (3) Robustness against memorization. The process for generating the dataset started with authors designing initial questions, and sharing them with annotators alongside instructions to create more questions. Then, questions were validated by ensuring that additional annotators came up with the same response  to the questions. These validated questions formed the final set, which were then fed to GPT4 with and without plugins, and AutoGPT with the ChatGPT backend. Reported results show that GAIA is challenging for these agents, while being very easy for humans. Performance is also reported on a per-capability bases, with questions divided into 5 categories: Web Browsing, Coding, Multi-modality, Diverse File-type reading, and N/A."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. Important problem: As LLMs advance and become part of everyday life, General AI assistant benchmarks are exceedingly important. \n\n2. Markedly different from other benchmarks: existing benchmarks focus on Narrow AI: expertise in a specific domain. GAIA focusses on more general purpose tasks that require multiple steps and are tedious rather than being hard for humans. Such simple but tedious tasks are optimally suited for AI assistants, making it a great guiding principle for the benchmark.\n\n3. Paper easy to follow: The paper does a great job of presenting the approach, design decisions, and related work. The figures explain the work well too."
            },
            "weaknesses": {
                "value": "1. The venue is not a great fit. The majority of the contribution here is annotated dat, and the design decisions made in doing so. This would make the work a better fit for conferences focussing on these aspects, including CHI and UIST. There are no learned representations, or models, putting it out of the domain of the ICLR community.\n\n2. Size and composition of the dataset: While GAIA is a good start, 466 questions seems like a very small dataset for a general purpose AI agent. Furthermore, most of these questions come from web browsing, which makes the benchmark quite close to a narrow AI benchmark for web browsing.\n\n3. Experiments very thin: The utility of designing a benchmark could be justified if it taught reasonable insights about the behaviour of existing models. In its current form, the investigations presented in the paper do not offer any new insights about the behavior of these models."
            },
            "questions": {
                "value": "I don't have any particular questions, as I see a serious misfit with the conference."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5446/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699271537627,
        "cdate": 1699271537627,
        "tmdate": 1699636553852,
        "mdate": 1699636553852,
        "license": "CC BY 4.0",
        "version": 2
    }
]