[
    {
        "id": "2hUdi4YhDB",
        "forum": "TkP2RtR4hr",
        "replyto": "TkP2RtR4hr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5745/Reviewer_xUBm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5745/Reviewer_xUBm"
        ],
        "content": {
            "summary": {
                "value": "The authors propose two means of performing better data augmentation for (some?) NLP tasks. \n\nFirst, the authors propose a method to systematically adjust the degree of randomness during the augmentation process. This allows them to generate augmented data that is on the one hand diverse, but on the other hand not too different from the seed data, as data that is too different from the seed data may get annotated with incorrect pseudo-labels.  \n\nSecond, the authors propose a method that uses sentence embeddings, document embeddings, and Shapley values to augment data in a way that allows the authors to reliably generate pseudo-labels for generated data, even if the generated data is quite different from the seed data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper discusses data augmentation for natural language processing. This is an area that many practitioners care about. \n- Sections 2.2 and 2.2 situate the paper\u2019s work nicely."
            },
            "weaknesses": {
                "value": "- The code included in the supplementary materials of this submission does not include code used to evaluate the baseline methods. How did the authors evaluate the baseline methods? \n- There is mention of Hyperparameter tuning for the proposed method, but not for the baseline methods. Were the baseline methods hypertuned?\n- The authors compare their model with the Pegasus method (https://arxiv.org/pdf/1912.08777.pdf), EDA method, and AEDA method, but use evaluation datasets that are entirely non-overlapping with the datasets that the Pegasus/EDA/AEDA authors evaluated on. This makes it harder to be sure that the Pegasus, EDA and AEDA methods were given a fair shot in the comparison, especially since no code or hyper parameters were published by the paper reviewed here. \n- The paper is submitted under \u201cDatasets & benchmarks\u201d, but the paper doesn\u2019t describe a new dataset or benchmark. \n- The paper is hard to read. For example, Section 3.2.1 states that \u201c[the] method used to define the sentence pool required for augmentation is illustrated in Figure 3.\u201d But from the figure, the only thing that becomes apparent is that a set of documents is transformed into a sentence pool, without any details about how this happens.\n- The paper\u2019s scope is not clear. Which NLP tasks (e.g. text classification, text summarization) does the method described in the paper apply to? \n- The paper contains numerous small grammatical errors, e.g. \u201cdiverse researched\u201d should be \u201cdiverse research\u201d. \n- The scores in Table 5 vary widely between max and min values. The authors should use many more than 5 runs to reliably compare the performance of the models. \n- The authors appear to use a BiLSTM for classification. Would a transformer model perform better? \n- The authors describe two independent ideas (controlling level of randomness in augmentation, and a sentence-embedding methodology) in one paper. Have the authors considered separating these into two papers? This would make it easier to investigate the two ideas independently, from a scientific perspective."
            },
            "questions": {
                "value": "- It appears that the paper\u2019s method is for text classification. If my guess is correct, could the authors mention this in the abstract and introduction? This would help the readers grasp the paper more easily. \n- Section 3.1 begins with \u201cAs shown in Figure 1, the first step in the process involves \u2026\u201d. Here, it would be useful to spell out which process is being discussed, e.g. \u201caugmentation process\u201d. Of course the reader can go to the figure and check, but that disrupts the reading. \n- Why is the Range Ratio (equation 3) needed? Why can we not threshold on x directly, instead of first transforming x using a monotonic function, and thresholding afterwards? \n- The fonts in the figures are too small to be legible when the paper is printed. \n- The abbreviation \u201cXAI\u201d should be spelled out in the abstract. \n- In the abstract, \u201cincorporates\u201d should be \u201cincorporate\u201d\n- The paper mentions \u201cutilized a logistic regression-based equation\u201d. This is confusing, as the connection to logistic regression is very loose, and no model is being trained in this context. \n- The third paragraph of Section 2.1 is almost verbatim the second paragraph of Section 1. This should be revised to avoid needless repetition. \n- The authors typeset citations by writing \\cite immediately after the word preceding it, which results in text like \u201cCAM(Zhou et al., 2016)\u201d. Could the authors please instead use ~\\cite (with a tilde before the \\cite), so that the text gets typeset like \u201cCAM (Zhou et al., 2016)\u201d  (with a space after \u201cCAM\u201d)?\n- The authors write \u201ccalculates the magnitude of errors through back-propagation with respect to the input of the model\u201d, but it should be \u201cthe gradient of errors\u201d\n- The paper states \u201c\u2026 BBM tends to have less accurate interpretability than the ABM because of its fast computational ability\u201d. Did the authors mean this, and if so, could they please explain it? \n- In Table 1, the authors write \u201cEuclidean, unless otherwise specified\u201d. I could not find an alternative meaning of the norm specified anywhere in the paper. Can \u201cunless otherwise specified\u201d be removed? \n- In Table 1, can \u201cTypical meaning\u201d be replaced with \u201cMeaning\u201d?\n- Something is broken in the text snippet \u201cconditional mean, computes the conditional mean\u201d  in Section 2.2\n- Something is broken in the text snippet \u201cIn this study utilize the\u201d in Section 2.2\n- Many more grammatical errors or broken sentences. Please review the paper for grammar mistakes one more time. \n- Section 3.1 mentions \u201cthe fine-tuned BERT\u2019s classification head\u201d. This felt abrupt. Could the authors introduce the fine-tuned model appropriately?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5745/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698022877821,
        "cdate": 1698022877821,
        "tmdate": 1699636602656,
        "mdate": 1699636602656,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8ybkp0QKgP",
        "forum": "TkP2RtR4hr",
        "replyto": "TkP2RtR4hr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5745/Reviewer_HBAq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5745/Reviewer_HBAq"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new text augmentation technique which is able to control the diversity of augmentations. They utilize information from sentence embedding and document embedding in this approach. Experimental results on three text classification datasets show this method outperforms the baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work has provided codes in supplementary materials to give us a reference to check the reproducibility.\n2. Experimental results on three text classification benchmark datasets show the proposed method is effective."
            },
            "weaknesses": {
                "value": "1. The writing and presentation of this paper are poor, so this paper is hard to follow. \n\n    Firstly, some sentences are not clearly organized and the logic is not good. For example, some sentences are really hard for readers to understand: \"*The level of manipulation is the most important issue in text augmentation; low-level manipulation generates data similar to the original, resulting in inefficient augmentation because it cannot ensure diversity, whereas high-level manipulation causes reliability issues for labels and degrades the model\u2019s performance. Therefore, ...*\". \n\n    Secondly, there are many typos such as *It **introduced** diversity into the training process* -> *It **introduces** diversity into the training process*. \n\n    Thirdly, some abbreviations should be explained before being used. For example, this paper does not explain what is *XAI* in abstract.\n\n2. The motivation is not clear. This paper does not explain why the proposed method address *the level of manipulation* issue. Some previous work like AutoAugment[1] also presented techniques to automatically search for improved data augmentation policies.\n\n3. This paper omits many important references such as AutoAugment[1]. Also, it does not compare with these methods.\n\n4. The improvements of performance are marginal, where the average improvements over the state-of-the-arts are all smaller than 0.5 in Table 4. Thus, we are not sure whether this approach is really effective and robust.\n\n[1] Cubuk et al., AutoAugment: Learning Augmentation Policies from Data. CVPR 2019."
            },
            "questions": {
                "value": "See Weaknesses above. We encourage the authors to carefully modify and improve this paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5745/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698581087732,
        "cdate": 1698581087732,
        "tmdate": 1699636602553,
        "mdate": 1699636602553,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XUFKEt1BvI",
        "forum": "TkP2RtR4hr",
        "replyto": "TkP2RtR4hr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5745/Reviewer_WQ8f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5745/Reviewer_WQ8f"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel text augmentation technique aiming to strike a balance in the level of manipulation applied to text data, addressing a common challenge in NLP. It introduces a method to systematically adjust the data manipulation pool and proposes a sentence-embedding methodology for robust pseudo-labeling. The authors validate the approach through extensive experiments, demonstrating improved performance compared to existing methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The paper introduces a methodical and innovative approach to text augmentation, employing a logistic regression-based equation and cosine similarity to dynamically adjust the candidate sentence pool. This ensures a tailored balance between diversity and semantic similarity, directly addressing and systematically controlling the critical issue of manipulation level in text augmentation.\n2) The integration of sentence, document embedding, and XAI information for pseudo-labeling is innovative"
            },
            "weaknesses": {
                "value": "I hope my reviews will help make the work more robust.\n\n1) The authors highlight the challenges associated with preserving label reliability amid variations in text due to manipulation, with a particular focus on concerns about the reliability of newly assigned pseudo-labels. However, there are more advanced data augmentation methods available, such as those described in references [1], [2], and [3]. Reference [1] introduces a language model specifically trained to generate augmentations tailored to the class label, ensuring consistency between the input class label and the output augmentation. Reference [2] employs a multi-level optimization framework to tailor the augmentation process, benefiting the downstream classification model and maintaining class label consistency via a feedback mechanism. Reference [3] suggests a soft-augmentation approach based on embedding space, synthesizing gradients of all unseen words using a task-dependent similarity matrix. A comparison with these methods **or** a discussion of the proposed method's advantages over them would provide valuable insights. I understand it is challenging to compare with all these methods in limited time frame. However, a discussion would be insightful.\n2) The impact of the SHAP module, as well as the utilization of document and sentence embeddings remains unclear from an empirical standpoint. Including an ablation study in the paper, which breaks down and analyzes the role of each component, would help clarify their contributions and improve understanding.\n3) The experiments in the paper utilize BERT-base to demonstrate the effectiveness of the proposed text augmentation technique. To strengthen the evaluation, it would be insightful to extend the comparisons to include less powerful models, such as LSTM. An examination across a range of model complexities would offer a more robust validation of the proposed method.\n4) More details about SHAP in the paper would make it easier for readers to better understand it.\n5) It would be insightful to include the generated augmentations. I have a follow up question on it. (below)\n6) It would be insightful to have the standard deviation of the method displayed.\n\n[1] Anaby-Tavor, Ateret, et al. \"Do not have enough data? Deep learning to the rescue!.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 05. 2020.\n\n[2] Somayajula, Sai Ashish, Linfeng Song, and Pengtao Xie. \"A multi-level optimization framework for end-to-end text augmentation.\" Transactions of the Association for Computational Linguistics 10 (2022): 343-358.\n\n[3] Somayajula, S. A., Jin, L., Song, L., Mi, H., & Yu, D. (2023, July). Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training. In Findings of the Association for Computational Linguistics: ACL 2023 (pp. 8569-8588)."
            },
            "questions": {
                "value": "I hope my questions will add more insights to the paper.\n\n1) Could you please elucidate the specific advantages brought by the inclusion of document embeddings, especially considering that sentence embeddings are already incorporated into the model? \n\n2) I would appreciate some clarification on how the proposed system assigns pseudo labels, particularly in challenging scenarios where sentences contain contrasting sentiments or uncorrelated events with conflicting emotions. For example, in sentences like \u201cMovie is good. The actors and director did a bad job\u201d or \u201cMovie is good. The dinner in Hollywood was bad\u201d, the sentiments are mixed.  Could the authors provide examples of how the system handles such cases, and discuss the impact on pseudo label assignment? I understand that the use of cosine similarity and the systematic adjustment of the candidate pool aims to mitigate these issues. However, there might be instances where stochasticity in sentence selection could still lead to pseudo label inconsistencies. For instance, in a movie review task, the sentence \u201cMovie is good. The dinner in Hollywood was bad\u201d could yield a high cosine similarity due to the presence of terms like \u2018Hollywood\u2019, but the Bi-LSTM might assign a negative sentiment based on the word \u2018bad\u2019, leading to a potential discrepancy.  Understanding how the system navigates these complexities would provide valuable insights into its robustness and reliability in pseudo label assignment.\n\n3) The range ratio, indicative of the manipulation level through its pool length, is calculated using cosine similarity, denoted as 'x.' Specifically, 'x' is determined by comparing the embedding 'E\u02c6' of the seed sentence (Seed(E\u02c6)) with the embedding of a randomly selected sentence from the candidate pool (Random(E\u02c6)). This process systematically assigns a level of randomness for the range ratio calculation. However, it raises a question: Why is the range ratio dependent solely on a metric derived from a single randomly chosen candidate, rather than being based on an average or aggregate measure across the entire pool? Or alternatively there can be clustering based on the estimated x's for all the sentences in the pool and choose to have sentences from most related clusters.\n\n4) Can you provide more details on how the SHAP module is contributing to our understanding or interpretation of the model's predictions? Specifically, I am interested in understanding if and how it can shed light on the impact or effectiveness of data augmentations. Are there any specific insights or trends that can be derived from the SHAP values regarding how different augmentations might be influencing the model\u2019s predictions? It would be beneficial to discuss examples or specific cases to illustrate these points."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5745/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5745/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5745/Reviewer_WQ8f"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5745/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822087135,
        "cdate": 1698822087135,
        "tmdate": 1700595085026,
        "mdate": 1700595085026,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CMQ9HD0ak1",
        "forum": "TkP2RtR4hr",
        "replyto": "TkP2RtR4hr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5745/Reviewer_MCTc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5745/Reviewer_MCTc"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a technique for data augmentation in text. The main goal is to provide a way to control diversity and fidelity (meaning preservation) to construct augmentation dataset. This is achieved using reassignment of similarity scores to incorporate a wide informative pool of candidate sequences. Based on the formulation, when the new similarity score is high, a wide pool is selected while a smaller pool of candidate sequence is selected in case of lower scores. Additionally different embeddings with Bi-LSTM is used to form supervised datapoints.\n\nThe resultant augmented data is used for training BERT models, and shows competitive results against comparable baselines.\n\nThat being said, this paper needs a lot of rework and would not recommend acceptance in its current form."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Comparison against multiple competitive baselines and better than baseline results using this method.\n    \n2. Combination of multiple source of information for generating pseudo labels for the training data."
            },
            "weaknesses": {
                "value": "1. The writeup of this paper needs to be improved. A lot of details are missing. Kindly see Question/Comments section for the same.\n    \n2. Qualitative Analysis missing. Why are the labels more reliable?\n    \n3. Statistical Significance testing missing for quantitative evaluation."
            },
            "questions": {
                "value": "1. Section 2.2: SHapley Additive exPlanation\n    \n2. Figure 2: It is difficult to understand this figure. How are the embeddings calculated. Where do the final tokens appear from? Kindly enumerate the equations governing the embedding construction.\n    \n3. Section 3.1: What is BERT fine-tuned on?\n    \n4. Section 3.1.2: Why is 1-D used for representing document embedding? What information does it contain? Kindly perform an ablation study to show that those document embeddings play a significant role in the embedding construction.\n    \n5. Section 3.2.3: Excessive use of the word systematically.\n    \n6. Section 3.3: Why is a Bi-LSTM used here instead of a transformer model? Why does it lead to reliable labels?\n    \n7. Section 4.3: Are the results mentioned statistically significant? Kindly report numbers from a statistical significance testing.\n    \n8. Section 4.3: 500 training datasets -> 500 training datapoints\n    \n9. Section 4.3: Ablation study of components missing.\n    \n10. Conclusion: Kindly mention quantitative metrics in the summary instead of re-writing the abstract here.\n    \n11. In the writeup, there are multiple instances where words have been titlecased. Kindly rectify those."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5745/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834560407,
        "cdate": 1698834560407,
        "tmdate": 1699636602299,
        "mdate": 1699636602299,
        "license": "CC BY 4.0",
        "version": 2
    }
]