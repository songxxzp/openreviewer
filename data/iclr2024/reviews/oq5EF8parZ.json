[
    {
        "id": "J45FZRpDWo",
        "forum": "oq5EF8parZ",
        "replyto": "oq5EF8parZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5780/Reviewer_qmnH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5780/Reviewer_qmnH"
        ],
        "content": {
            "summary": {
                "value": "This work studies the open multimodal dialogue following user instruction in the conversations of multiple turns with multiple images. This work achieves this from three directions by proposing a model (SparklesChat), a dataset (SparklesDialogue), and a benchmark (SparklesEval). It also performs experiments on SparklesEval, the BISON binary image selection, and the NLVR2 visual reasoning task, on which SparklesChat outperforms MiniGPT-4 significantly."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This is one of the first works that studies multiple turns with multiple images for the open multimodal dialogue. Thus, it can additionally evaluate cross-image and cross-turn coherence and completeness of responses.\n\n2. It contributes a novel dataset named SparklesDialogue leveraging GPT-4.\n\n3. This work also proposes GPT-assisted evaluation named SparklesEval that can automate quantitative evaluation of a model\u2019s conversation across multiple images and dialogue turns.\n\n4. The Appendix and the supplementary material is helpful and very thorough."
            },
            "weaknesses": {
                "value": "1. It only consider two images per context, which could be too structure with little diversity.\n\n2. The SparklesChat model is not novel in that it is just an instruction tuned miniGPT-4. It could be removed from contributions. \n\n3. As described in Table 1, SparklesDialogue is not large-scale. \n\n4. Each conversation seems to have a very typical pattern with two images as described in section 2.  \n\u201cIn the first turn, the user initiates a reasonable and creative message regarding some images. In response, the assistant generates detailed answers that include comprehensive reasoning regarding the visual content. In the second turn, the user introduces a new image for further discussion, referencing both the new and previous images.\u201d\n\n5. Only the miniGPT-4 is compared as a baseline."
            },
            "questions": {
                "value": "1. Why only the two datasets - BISON and NLVR2 are chosen? Is there any other dataset to use?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5780/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698498402609,
        "cdate": 1698498402609,
        "tmdate": 1699636607999,
        "mdate": 1699636607999,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WbH03mw5Tg",
        "forum": "oq5EF8parZ",
        "replyto": "oq5EF8parZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5780/Reviewer_eU8x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5780/Reviewer_eU8x"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces SparklesChat, a multimodal instruction following model for open-ended dialogues across multiple images. This is MiniGPT4 fine-tuned on the machine-generated dialogue dataset released in the paper called SparklesDialogue. This contains word-level interleaved multi-image and text interactions with up to 3 images during the first turn and 1 image during the second turn. SparklesDialogue consists of two subsets: 1) SparklesDialogueCC which contain images from CC3M and captions generated by MiniGPT4 2) SparklesDialogueVG which contain images from Visual Genome and descriptions from GPT-4, based on human-annotated captions, objects, and regions. SparklesEval is a new GPT-assisted benchmark with 150 dialogs, introduced to assess conversational competence across multiple images and dialogue turns, through criteria such as Image Understanding & Reasoning, Cross-Image & Cross-Turn Coherence, and Relevance & Completeness of Responses. SparklesChat outperforms MiniGPT-4 and gets marginally close GPT-4 on binary image selection task and the NLVR2 visual reasoning task. The paper contains ablation study on the effect of dialog turns and SparklesDialogue subsets during training."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1.\tNew dataset SparklesDialogue for word-level interleaved multi-image and text interactions\n2.\tNew benchmark SparklesEval for word-level interleaved multi-image and text interactions\n3.\tDemonstration of improved performance over MiniGPT4"
            },
            "weaknesses": {
                "value": "1.\tSparklesDialogue contains subset SparklesDialogueVG, which was generated using GPT-4. The paper compares with performance of GPT-4 (method used to create the data set is also being evaluated on), while still performing worse although SparklesChat uses much richer image embedding. \n2.\tNo contribution in terms of novelty architecture. Main contribution is in the data set. \n3.\tOnly two turns per sample in the dataset. Longer sessions are probably more practical than more images per turn and limiting to just 2 turns. Dataset (that too, machine-generated) being the highlight of this paper, would have expected more.\n4.     Not clear how this extends to other approaches such as LLaVA. Results are shown only for Min-GPT4 extension."
            },
            "questions": {
                "value": "Q1) Section 5.2 mentions, SparklesDialogueVG and SparklesEval use the same sources of images and captions. This is suspected to be one of the reasons why model trained on SparklesDialogueVG performs better than model trained on SparklesDialogueCC. Isnlt this a serious issue, especially since SparklesDialogueVG is claimed to be the high quality subset?\n\nMinor typo\n1.\tTable 2: Column title should be A2 under \u201cTurn two\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper used GPT4 and MiniGPT-4 to create the datasets without any human review. It is unclear how safe the dataset is. Also, not clear about RAI."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5780/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698635408289,
        "cdate": 1698635408289,
        "tmdate": 1699636607893,
        "mdate": 1699636607893,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "55it6TPm98",
        "forum": "oq5EF8parZ",
        "replyto": "oq5EF8parZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5780/Reviewer_viMW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5780/Reviewer_viMW"
        ],
        "content": {
            "summary": {
                "value": "This paper presents SparklesChat, a multimodal instruction-following model for open-ended dialogues across multiple images. It introduces SparklesDialogue, a specialized machine-generated dialogue dataset, and achieves superior performance compared to MiniGPT-4 on vision-language benchmarks. SparklesChat's effectiveness is further demonstrated by its high score on SparklesEval, a benchmark for assessing conversational competence across multiple images and dialogue turns."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper addresses a key limitation in the field by introducing SparklesChat, a multimodal instruction-following model that integrates multiple images at the word level. This fine-grained integration of images and text is a novel approach that mimics natural human communication more closely.\n\n2. The paper presents SparklesDialogue, the first machine-generated dialogue dataset designed for word-level interleaved multi-image and text interactions. The dataset is constructed from different image and description sources, ensuring greater robustness and diversity. Additionally, the paper introduces SparklesEval, a comprehensive scoring system that quantitatively evaluates the model's conversational competence in multimodal, open-ended dialogues.\n\n3. The SparklesEval benchmark shows that SparklesChat's conversational competence significantly surpasses MiniGPT-4 and approaches the performance of GPT-4. These results highlight the potential of SparklesChat in real-world scenarios."
            },
            "weaknesses": {
                "value": "Considering the current status of single-image comprehension, which still requires further advancements, it appears that addressing scenarios involving multiple images may not be an immediate priority. Additionally, when considering the data construction approach described in the paper, it becomes evident that the model's capabilities are still constrained by the limitations of single-image understanding.\n\nIn my personal opinion, focusing on improving single-image comprehension would be more beneficial at this stage. Once single-image understanding is well-established, the demonstrated ability to handle multiple images, as showcased in the paper, should not pose significant challenges. It is crucial to ensure a solid foundation in single-image comprehension before delving into more complex scenarios involving multiple images."
            },
            "questions": {
                "value": "1. How do the Dialogue Demonstrations contribute to the data quality and diversity?\n2. Considering the impressive performance of GPT-4 with ground truth (gt) annotation, could the authors provide a baseline using a strong caption model with an instruction-tuned Language Model to address the challenges raised in the paper?\n3. Does the model in the paper have the capability to handle scenarios with more than two images, considering that the paper only showcases examples with two images?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5780/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5780/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5780/Reviewer_viMW"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5780/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698656640919,
        "cdate": 1698656640919,
        "tmdate": 1699636607778,
        "mdate": 1699636607778,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xcA7tDthRc",
        "forum": "oq5EF8parZ",
        "replyto": "oq5EF8parZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5780/Reviewer_9M6X"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5780/Reviewer_9M6X"
        ],
        "content": {
            "summary": {
                "value": "This work introduces SparklesChat, a multimodal instruction-tuned model designed to effectively engage in dialogues that encompass multiple images. Additionally, the constructed multi-image dialogue dataset and an evaluation benchmark are introduced."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This work focuses on a new scenario that is not well-explored by current large multimodal models, i.e. multi-image multimodal dialogue. \n\nThis work propose new training data, evaluation benchmark, and model for this scenario, which exhibit better performance than MiniGPT-4."
            },
            "weaknesses": {
                "value": "**1. The data construction process seems too trivial and not sound.** \n\nIn the data construction process to generate visual dialogue with multiple images, you provide multiple image-text pairs and ask GPT-4 to link them together, which I think is the simplest way to construct multi-image dialogues. \n\nBesides, this simple approach fails to yield effective samples. In Figure 3, the response from GPT-4 seems too naive, *i.e.*, in image #1, we see ..., in image #2, we witness..... This is just a concatenation of descriptions of two images.\n\n**2. Insufficient experiments.**\n\nI think current experiments cannot form a strong foundation to support the effectiveness of your model and training data.\n\n* Baselines. You compare your method only with MiniGPT-4, which in my understanding is an embarassingly weak and simple model & dataset. More comparisons are definitely needed.\n\n* Evaluation benchmarks. You use three benchmarks for evaluation, BISON, NLVR2, and your own evaluation data. Among them, BISON and NLVR2 are not commonly used benchmarks now. Besides, on your own evaluation data, you claim your performance apporach GPT-4. However, your self-constructed training data could share similar distribution to you eval data. To this end, I think this claim cannot well establish."
            },
            "questions": {
                "value": "More solid experiments could be helpful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5780/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5780/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5780/Reviewer_9M6X"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5780/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698754393791,
        "cdate": 1698754393791,
        "tmdate": 1699636607680,
        "mdate": 1699636607680,
        "license": "CC BY 4.0",
        "version": 2
    }
]