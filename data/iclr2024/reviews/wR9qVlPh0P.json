[
    {
        "id": "ivws073cnU",
        "forum": "wR9qVlPh0P",
        "replyto": "wR9qVlPh0P",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission509/Reviewer_aYHC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission509/Reviewer_aYHC"
        ],
        "content": {
            "summary": {
                "value": "This paper mainly focuses on visual prompting method. The authors propose a new framework which covers a wide range of design space for visual prompting including input scaling, pretrained model, output mapping. The authors introduce several difference techniques for these dimension which are shown to be effective in the experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method is simple yet effective.\n2. The writing is easy to understand."
            },
            "weaknesses": {
                "value": "1. The input scaling technique was discussed in EVP [1]. The authors may have some more discussion about the relationship and difference with this work.\n2. The different choices for pretrained model are almost the same as the original ones in VP. It would be better if the author can mention this in the paper.\n3. I doubt if the proposed FullyMap can be viewed as an output mapping method rather than a variant of linear probing. By using this method, the distribution of trainable parameters is totally different from the original VP-style methods like VP, EVP and BlackVIP [2] whose trainable parameters only concentrate on modifying the input space without the output space. It is for sure that such a method can bring significant improvement in the experiments. Even if you do not use the visual prompt but simply finetune the model with the additional last fc layer, I assume the model can have similar performance to the linear probed one.\n4. I recommend the authors adopt other datasets used in previous VP papers like SUN, CLEVR and RESISC, and compare the proposed method with other baselines like BlackVIP.\n\n\n[1] Wu J, Li X, Wei C, et al. Unleashing the power of visual prompting at the pixel level[J]. arXiv preprint arXiv:2212.10556, 2022.\n[2] Oh C, Hwang H, Lee H, et al. BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 24224-24235."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Reviewer_aYHC"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission509/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697943114363,
        "cdate": 1697943114363,
        "tmdate": 1700720882136,
        "mdate": 1700720882136,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k28UIxjE0i",
        "forum": "wR9qVlPh0P",
        "replyto": "wR9qVlPh0P",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission509/Reviewer_dWYk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission509/Reviewer_dWYk"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces AutoVP, an end-to-end framework that automatically selects optimal visual prompt (VP) configurations for vision downstream datasets. Tested on 12 datasets, AutoVP demonstrates improvements over the traditional linear probing approach, and on OOD test. The performance enhancements are in line with expectations. The significance of AutoVP lies in its automation of the VP configuration process, offering an extensive study for prompt engineering with gains."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. An extensive study for visual prompting on vision model such as ResNeXt, ViT, and CLIP model. \n\n2. AutoVP, by applying a series of established approach, from input scaling, to output label engineering, enables huge gain on the results."
            },
            "weaknesses": {
                "value": "1. The paper is evaluated on 12 visual recognition tasks, what about other tasks, given that this is a benchmark paper. Say Object Detection, Depth, Segmetnation. \n\n2. Reviewer appreciate this systematic study in applying all methods of VP and improve results. However, those results are expected.  Learn a bit of new knowledge after reading this, the reviewer would expect in general more surprising finding or impressive knowledge."
            },
            "questions": {
                "value": "No"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission509/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698864193924,
        "cdate": 1698864193924,
        "tmdate": 1699635977417,
        "mdate": 1699635977417,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5Pif4x4J4X",
        "forum": "wR9qVlPh0P",
        "replyto": "wR9qVlPh0P",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission509/Reviewer_PZJW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission509/Reviewer_PZJW"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces AutoVP, a comprehensive framework for automating VP design decisions. This framework covers several aspects, including optimizing the prompts, selecting suitable pre-trained models, and determining the best output mapping strategies. \nThe AutoVP along with a set of 12 image-classification tasks serves as a benchmark for evaluating VP performance. \n\nAutoVP is shown to outperform existing VP methods and achieves a considerable performance boost when compared to the linear-probing baseline. Above all, AutoVP offers a hyperparameter tuning tool for VP and a benchmark to further its advancements."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper presents its findings with clear figures and detailed statistical reports, making it easier for readers to grasp the results.\n2. This paper does not just present a tool but embarks on a detailed exploration of optimal configurations under various conditions, aiming at proving how different settings affect performance. It also examines the impact of domain similarity on VP performance."
            },
            "weaknesses": {
                "value": "1. While VP can potentially be used for a variety of vision tasks, the paper seems to focus primarily on image classification tasks, which may limit its applicability to broader vision problems. Are there any additional results on dense discriminant tasks?\n2. When utilizing CLIP as the pre-trained classifier within the framework, which visual backbone is employed, ViT or ResNet?\n3. About the proposed VP benchmark, why do the authors exclude some widely recognized 2D datasets, such as Caltech101, OxfordPets, StanfordCars, SUN397, EuroSAT, and FGVCAircraft, which are all common-used for 2D image classification task evaluation? What are the criteria for dataset selection?\n4. Table 2 reveals that AutoVP underperforms on **6** datasets out of **12** in the benchmark compared to Linear Probing (LP), e.g., AutoVP is **-6.5%, -10.2%, -12.1%** lower than LP on Flowers102, UCF101, and DTD respectively. The reported average accuracy improvement appears to be significantly influenced by the results from the SVHN dataset, which is **+27.5%**. Could the authors provide additional insights into this discrepancy? Furthermore, similar patterns observed in Tables 6 and 7 suggest that these results may not be consistently solid across varied datasets.\n5. The paper mentions ***std*** for AutoVP, indicating some randomness. It's important to think about how this inconsistency could affect the reliability of AutoVP, especially compared to the more stable method of linear probing. While Table 5 indicates a shorter execution time per run for AutoVP, one might infer that achieving the reported performance could necessitate multiple runs, thereby affecting the efficiency.  This inconsistency and the potential extra time needed make it less practical in certain situations."
            },
            "questions": {
                "value": "Please see the weaknesses mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission509/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699075738412,
        "cdate": 1699075738412,
        "tmdate": 1699635977336,
        "mdate": 1699635977336,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DgPCbhsFjG",
        "forum": "wR9qVlPh0P",
        "replyto": "wR9qVlPh0P",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission509/Reviewer_nUAn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission509/Reviewer_nUAn"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes AutoVP, an end-to-end expandable framework for automating VP design choices along with 12 downstream image classification tasks that can serve as a holistic VP-performance benchmark."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Clarity and Logic**: The paper is well-structured and presents complex ideas clearly, making it understandable for readers.\n\n**Useful Framework**: AutoVP is introduced as a versatile toolbox that simplifies the development of visual prompts, offering a modular design and comprehensive functionalities.\n\n**Improved Performance**: The models tuned with AutoVP demonstrate a significant performance improvement over previous baselines across various image classification tasks."
            },
            "weaknesses": {
                "value": "**Limited Novelty**: The framework largely combines existing methods, which might suggest a wrap-up of previous work rather than introducing new concepts, limiting the perceived novelty of the research.\n\n**Potential Overfitting**: AutoVP uses different settings for different datasets, raising the question of whether these are overfitted to specific tasks and what the implications are for a robust, universal setting.\n\n**Insufficient Analysis of Mapping Methods**: There is a lack of detailed comparison and analysis of the mapping methods used in visual prompting, which is necessary to understand their impact and provide more comprehensive insights."
            },
            "questions": {
                "value": "I would suggest expanding the testing of visual prompting from image classification to other tasks like detection and segmentation. This would help ensure that the AutoVP framework is versatile and not just fine-tuned for specific tasks. Aim to create a benchmark that evaluates how well visual prompting works generally, across various types of visual tasks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Reviewer_nUAn",
                    "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission509/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699587882353,
        "cdate": 1699587882353,
        "tmdate": 1700808471095,
        "mdate": 1700808471095,
        "license": "CC BY 4.0",
        "version": 2
    }
]