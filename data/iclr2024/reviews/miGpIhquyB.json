[
    {
        "id": "REwQirNHgK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9477/Reviewer_Afrd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9477/Reviewer_Afrd"
        ],
        "forum": "miGpIhquyB",
        "replyto": "miGpIhquyB",
        "content": {
            "summary": {
                "value": "This paper examines the generation of text datasets using Large Language Models (LLMs) with a focus on distributional metrics like data diversity and faithfulness. It reveals trade-offs between these metrics across different LLMs and training methods, highlighting the impact of popular instruction-tuning techniques on LLM text generation abilities."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The studied task on using LLMs for data generation is interesting and can be useful for the research community.\n\n2. The authors conduct experiments on various datasets and LLMs (including both open-sourced and close-sourced models).\n\n3. The paper is overall easy to read."
            },
            "weaknesses": {
                "value": "1. The authors only consider the most simple prompts for the target tasks. However, there are several works that aim to improve the quality of prompts to yield higher-quality datasets, some examples include:\n\n- Chung et al. \"Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions.\" ACL 2023.\n\n- Yu et al. \"Large language model as attributed training data generator: A tale of diversity and bias.\" NeurIPS D&B Track, 2023.\n\nIt is also important to note that some dimensions (e.g. diversity) have already been studied in this work. As a result, some of the conclusions in this paper are already known and there are not many new insights about using LLMs for data generation.\n\n2. Unsupported Claims. The paper raises a claim that \"reinforcement learning with human feedback (RLHF) in ChatGPT leads to a significant degradation in synthetic dataset generation capabilities.\" However, the paper lacks a clear explanation of how the authors attribute this performance drop specifically to RLHF. A more detailed description of the experimental setup and results related to this assertion would enhance the paper's clarity and credibility.\n\n3. In the main paper, the author only considers the average performance over different patterns, which can be less informative as different datasets show diverse patterns (according to Figure 5).\n\n4. For the metrics, it is somehow not clear why using `unique number of tokens` as the metrics of Diversity."
            },
            "questions": {
                "value": "1. Could you elaborate on why this paper primarily relies on simple prompts for target tasks, especially when recent research has emphasized advanced prompt engineering techniques for improving dataset quality? How might incorporating more sophisticated prompts affect the study's outcomes?\n\n2. Given that some dimensions, like diversity, have already been studied in this work, what new insights or contributions does this paper bring to the field of using LLMs for data generation? \n\n3. In the paper, you assert that \"reinforcement learning with human feedback (RLHF) in ChatGPT leads to a significant degradation in synthetic dataset generation capabilities.\" Could you provide a more detailed explanation of the experimental design and results that support this claim?\n\n4. What conclusions can be made after your study? What are the recommendations for practitioners to use LLMs for training data generation? Currently, it is not very clear after reading this paper, so I feel readers will not benefit much from this paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9477/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697153753799,
        "cdate": 1697153753799,
        "tmdate": 1699637191944,
        "mdate": 1699637191944,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yPyy646e3D",
        "forum": "miGpIhquyB",
        "replyto": "miGpIhquyB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9477/Reviewer_fcAm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9477/Reviewer_fcAm"
        ],
        "content": {
            "summary": {
                "value": "This work studies the quality of synthetic data generated by LLMs. The major contribution of this work is proposing a framework to evaluate LLM's ability to generate synthetic data for specific tasks, and compare behavior across different LLMs. The evaluation framework consists of five different axes: performance, complexity, conformity, diversity and faithfulness. These properties are either evaluated using accuracy-based metrics, or modified version of existing tools (e.g., distict-n, mauve, etc.). Using this framework, this work compares LLMs with different size, from different model families and with or without instruction tuning. The empirical study reveals interesting tradeoffs among the five axes, and also report general performance trends on overall performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Generating synthetic datasets is a very popular application of LLMs. This work provides a useful framework on evaluating this ability of LLMs.\n2. The empirical study shows interesting tradeoff from the models, and the reported performance trends can be useful for related applications."
            },
            "weaknesses": {
                "value": "1. I like the general idea of the proposed evaluation framework, but my biggest concern about this framework is the heavy use of DistilBERT accuracies in the evaluation framework. For the faithfulness metric, the framework is evaluating the performance of DistilBERT on the generated dataset. This confounds faithfulness with the difficulty (or complexity) of the dataset. This makes some of the finding questionable. For example, is there really a tradeoff between faithfulness and diversity/complexity, or is this correlation comes from the correlation between difficulty and diversity/complexity? I wonder if the authors can provide gold evaluation results for the DistilBERT models. \n2. This study only focuses on synthetic data generation for relatively simple classification tasks. It would be great if this work can include evaluation on some more complex tasks.\n3. While this paper proposes four other properties addition to the performance. There is not much discussion on the relationship between these properties and the final performance. So while this study show many interesting findings, it is unclear what users should do besides checking the performance rankings."
            },
            "questions": {
                "value": "1. For the value k in the diversity metric, are you keeping the example size the same, or the token size same?\n2. How do design or select prompts for the study conducted in your paper? Have you checked the sensitivity of the findings with respect to different prompts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9477/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9477/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9477/Reviewer_fcAm"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9477/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698006873170,
        "cdate": 1698006873170,
        "tmdate": 1699637191789,
        "mdate": 1699637191789,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9c9pOV3Yi5",
        "forum": "miGpIhquyB",
        "replyto": "miGpIhquyB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9477/Reviewer_wzFz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9477/Reviewer_wzFz"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the text generation capabilities of various large language models, proprietary and open, instruction-tuned and vanilla, by evaluating synthetic datasets generated from them. The datasets are evaluated in terms of\n1) diversity in vocabulary\n2) complexity, or difficulty in modeling them given by the performance of a model trained and evaluated in-distribution.\n\nBy comparing the generated datasets to existing (reference) datasets in similar tasks and domains, they are also evaluated in terms of\n3) faithfulness, given by the performance of models trained on the reference datasets and evaluated on the generated ones\n4) conformity, given by a measure of distributional similarity between the reference and generated datasets\n5) performance, given by the performance of models trained on the generated datasets and evaluated on the reference datasets\n\nBased on this evaluation framework, the paper discusses the tradeoffs between these aspects of generation quality, how they change across model families, and how instruction tuning affects these tradeoffs."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The evaluation framework is sensible and analyzing the capabilities of language models in terms of the tradeoffs between various aspects of generation quality is quite informative. The results of studying the effect of model size, the impact of instruction tuning, and that of the level of instruction tuning can potentially inform how to finetune future versions of language models."
            },
            "weaknesses": {
                "value": "This study has some missing details, several limitations, and potential confounders not accounted for in the experiments.\n\nMissing details\n\nMD1:The evaluation is done over four classification datasets, but the actual details of the tasks are missing in Section 4. Particularly for AGNews and ELI5, it is unclear what is being classified After reading the Appendix, the AGNews task seems to be some news genre classification, and the ELI5 task seems to be subreddit classification (maybe it should just be called \"subreddit classification\"?) This issue can easily be fixed by including explicit details in Section 4.\n\nMD2: The motivation behind the chosen evaluation metrics is somewhat unclear. Particularly, faithfulness, conformity, and performance seem to be measuring the difference between the generated and reference data distributions. Why do we need these three variants? Relatedly, one would expect these metrics to correlate highly with each other. Analyzing this further would be helpful.\n\nLimitations and potential confounders\n\nL1: It is unclear how noise in the datasets (due to inaccurate labels) affects the trends seen in tradeoffs. For example, is the increase in diversity beyond the the conformity threshold in Fig 2 simply be due to noise? Having humans classify (subsets of) the generated datasets, and introducing the accuracy of the synthetic datasets as an additional metric could make this clearer.\n\nL2: The biases in the reference datasets could also be affecting conformity, faithfulness and performance. It might help to include multiple reference datasets per domain-task combination to evaluate whether the trends hold across them.\n\nL3: It is possible that the models used for generating datasets have seen the reference datasets either during pretraining or instruction-tuning. This would inflate the quality measures according to conformity, faithfulness, and performance. This issue cannot be dealt with directly, but it would help to check the zero-shot performance of the large language models on the reference datasets, and take it int account while inferring the tradeoffs."
            },
            "questions": {
                "value": "- It would be helpful to put the reported diversity and complexity values in context. What are these values for the reference datsets?\n- Can you elaborate on the motivation behind the three metrics comparing generated and reference datasets (see MD2)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9477/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9477/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9477/Reviewer_wzFz"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9477/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698691016591,
        "cdate": 1698691016591,
        "tmdate": 1699637191657,
        "mdate": 1699637191657,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fPDC3rXMpU",
        "forum": "miGpIhquyB",
        "replyto": "miGpIhquyB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9477/Reviewer_PKWU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9477/Reviewer_PKWU"
        ],
        "content": {
            "summary": {
                "value": "This work studies the attributes of dataset generation, which has recently been explored as a way to train task networks without needing a natural, human-generated dataset. Particularly, this work studies 4 domains/tasks that dataset generation can be applied to (e.g. SST-2), and studies the trade-offs between different attributes: faithfulness, diversity, conformity, complexity, and performance, all of which the authors measure automatically. The authors find significant differences between different model types, especially finding that instruction-tuned models differ from classical LMs. Neither paradigm seems to completely dominate."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Overall, this type of contribution is sorely needed in dataset generation, which is still not a well-understood field\n- The attributes to study are diverse and relevant\n- Very interesting and informative conclusions drawn about the tradeoffs, e.g. the loss of diversity in generated datasets when using instruction-tuned models\n- paper is well presented and quite clear"
            },
            "weaknesses": {
                "value": "- I have concerns wrt the measurement of some of the aspects:\n   - faithfulness is measured as the accuracy on the synthetic set with a model trained on the reference (human) set. While being unfaithful is one reason this value may be low, it is not the only one. It is easy to imagine a *faithful* dataset on which this classifier will perform poorly, due to issues like style shift or poor generalization of the classifier. To be more concise: staking faithfulness on the accuracy of a classifier ignores the fact that this may be an issue of the classifier rather than the dataset that is being evaluated. \n   - similar issue with complexity, which is measured as inverse accuracy on a held out chunk of the synthetic set. While I agree that lower complexity will indeed raise this accuracy, high complexity is not the only reason this accuracy may decrease. \n- Overall, I would suggest renaming these metrics. They likely correlate with the values they are described as, but it is overly presumptuous to label them this way as there are many other factors. More direct names (e.g. complexity -> self-accuracy or something like this) might be more accurate, leaving discussion of factors affecting these values (like complexity) to the discussion\n- Tradeoffs (Figure 2) are only shown in terms of temperature, which may be a confounding factor. It would be good to show other curves, e.g. for values of top-p, because it is not clear if these tradeoffs may have to do specifically with the specific warping effect that temperature has on sampling distributions. Alternatively, being more precise in the paper text, that these are tradeoffs over temperature as the variable, rather than general tradeoffs."
            },
            "questions": {
                "value": "Have you tried variables besides temperature to test the tradeoffs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9477/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698693642880,
        "cdate": 1698693642880,
        "tmdate": 1699637191537,
        "mdate": 1699637191537,
        "license": "CC BY 4.0",
        "version": 2
    }
]