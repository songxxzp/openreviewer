[
    {
        "id": "3UF6hFDu4s",
        "forum": "sMFqEror1b",
        "replyto": "sMFqEror1b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4069/Reviewer_jYXa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4069/Reviewer_jYXa"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the problem of the theory of mind (ToM) by taking advantage of multi-modal information and the Bayesian inverse planning method. A new dataset is constructed with a focus on the scenarios where agents perform a sequence of household activities. The proposed method designs a collection of prompts and leverages a large language model for converting the multi-modal information (visual scene graph and textual description) into representations suitable for probabilistic inference. Experimental results demonstrate the effectiveness of the method in multiple settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ It is an interesting question whether or not large language models are able to infer the internal state of the other agents. The paper presents a new multi-modal dataset, which can facilitate future research along the direction. \n\n+ The proposed probabilistic model could potentially enable the understanding of how models aggregate information to address the problem of ToM, e.g., the relationships between observations and the dynamic of belief.\n\n+ The paper proposes a principled approach for integrating multi-modal information into an unified representation, which shows promise in inferring the belief and goal."
            },
            "weaknesses": {
                "value": "- Despite leveraging a probabilistic planning method and showing considerable improvement, the paper pays little attention to investigating the model\u2019s underlying mechanism. It would be more interesting to understand how the observed information influences the inference of belief, and which strategies are used by the models for tackling the challenges. \n\n- The authors highlight that the proposed method solves the problem in a zero-shot manner, however, the appendix mentions that it is fine-tuned with ground truth annotations, please explain. In addition, I feel like a considerable portion of important details are compressed into the appendix, which makes the main paper difficult to understand.\n\n- Constructing visual scene graphs is a challenging task, especially in naturalistic data of broader domains. How would the accuracy of predicted scene graphs affect the performance of the proposed method? With the increasing complexity of the visual scenes (e.g., from synthetic household environments to naturalistic scenarios in the wild), what are the potential ways of generalizing the method? Additionally, it seems that the paper does not mention which models are used for visual perception.\n\n- Related to the aforementioned comment, the textual parsing and information fusion rely heavily on pretrained large language models (e.g., GPT-4) and a collection of carefully-tuned prompts (for household scenarios), it is relatively unclear whether or not such a paradigm is able to accommodate the diversity of more complicated environments.\n\n- It appears that there is a significant variance in the performance of different types of problems, any idea why (e.g., GPT-4 fails drastically in setting 2.3)?"
            },
            "questions": {
                "value": "(1) How would the proposed method benefit understanding behind the model\u2019s inference of ToM?\n\n(2) What are the challenges and potential solutions for generalizing the method toward broader domains?\n\n(3) Is there any typical failure modes of the method, and why?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4069/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698635819943,
        "cdate": 1698635819943,
        "tmdate": 1699636371309,
        "mdate": 1699636371309,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iCSAPhjICx",
        "forum": "sMFqEror1b",
        "replyto": "sMFqEror1b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4069/Reviewer_Ewvn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4069/Reviewer_Ewvn"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a multimodal dataset called MMToM-QA for evaluating machines' understanding of a person's activities in a household environment. Based on the dataset, the authors propose a BIP-ALM method to encode information from multimodal inputs and utilize language models (LMs) for inference. They find that LMs and multimodal models are still lacking the knowledge to solve MMToM-QA, but BIP-ALM shows a promising direction."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is easy to follow and every details are clearly described."
            },
            "weaknesses": {
                "value": "MMToM-QA is flawed. VQA has been studied for years and the issues in crowdsourcing and annotations are clearly mentioned in many piror works. However, the paper lacks quantitative studies regarding these issues. For example, whether the questions designed can be attacked by counting the repeating semantic meaningful words, whether the text question already implied the answers. From Table 1, the high performance in text-only models may mean these flaws in dataset design.\n\nThe conclusion that \"large language models and large multimodal models still lack robust ToM capacity\" is not convincing, as the paper only tests BIP-ALM on the proposed MMToM-QA dataset. There are many other datasets that evaluate models' understanding of people's activities, such as VQA [1] and VCR [2]. The paper should compare BIP-ALM to the state-of-the-art methods on these benchmarks and also cite these benchmarks in related works.\n\nComparison to baselines is unfair. The paper tested BIP-ALM tuned on the dataset, yet use zero-shot approach to test the baselines such as GPT, LLaMA, InstructBLIP etc. So the pretraining of these baselines may fail to generalize to a totally different MMToM-QA data. The authors need to finetune these LLMs or prompt these models to provide a more fair comparison.\n\n[1] \"Vqa: Visual question answering.\"\n[2] \"From recognition to cognition: Visual commonsense reasoning.\""
            },
            "questions": {
                "value": "Please read the weaknesses I listed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4069/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698705388718,
        "cdate": 1698705388718,
        "tmdate": 1699636371232,
        "mdate": 1699636371232,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ihScyiJmOP",
        "forum": "sMFqEror1b",
        "replyto": "sMFqEror1b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4069/Reviewer_pPZm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4069/Reviewer_pPZm"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a benchmark and a model for evaluating theory of mind in machine learning models. The benchmark consists of synthetic videos, explanations, questions, and answers. The associated proposed model leverages GPT-4 for translating natural language text into symbolic representations as well as a visual perception model to extract visual information into symbolic representations. Several baselines are evaluated on the proposed benchmark on three modalities: multimodal, text-only, and video-only."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper includes a multimodal QA dataset with synthetic videos that can be potentially useful for future research and includes a human baseline."
            },
            "weaknesses": {
                "value": "1. The paper should do a better job of justifying the proposed task itself. The paper argues that Theory of Mind is important for developing machines with human-level social intelligence. However, it is unclear why having machines with \u201chuman-level social intelligence\u201d is necessary. First, can \u201csocial intelligence\u201d be measured or evaluated? It is still unclear how to define intelligence alone and how to measure it within the machine-learning context. Second, what types of applications will benefit from this skill? \n\n2. Without a clear goal and definition, it is hard to validate whether the proposed benchmark aligns with the task described in the introduction of the paper. Can we draw conclusions from the results in the proposed benchmark about a model\u2019s social intelligence? It is hard to say in the current setting.\n\n3. The text is unclear about using synthetic videos in the test set. As it is written in Section 3.1, it looks like the 134 test videos are real image videos, whereas the 1000 training videos are synthetically generated. However, in Section 3.3, it looks like the test videos are also synthetic. If test videos are not synthetic, how are they obtained?"
            },
            "questions": {
                "value": "- What are the statistics of the benchmark per question type?\n- Please include an overall column in Table 1 or the actual numbers in Figure 4."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4069/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698803916277,
        "cdate": 1698803916277,
        "tmdate": 1699636371158,
        "mdate": 1699636371158,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "08foNYFTJA",
        "forum": "sMFqEror1b",
        "replyto": "sMFqEror1b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4069/Reviewer_Djbm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4069/Reviewer_Djbm"
        ],
        "content": {
            "summary": {
                "value": "This benchmark evaluates ToM on multimodal data and various unimodal data, addressing the limitations of existing unimodal ToM benchmarks. The proposed method, BIP-ALM (Bayesian Inverse Planning Accelerated by Language Models), successfully integrates unified representations from multimodal data with scalable Bayesian inverse planning using language models. Comparative experiments reveal that while large language and multimodal models lack robust ToM capacity, BIP-ALM exhibits promising results by combining model-based mental inference and language models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation is clear and the problem tackled is indeed interesting and important.\n2. The dataset contribution is helpful for the community.\n3. The paper is overall well presented."
            },
            "weaknesses": {
                "value": "1. Currently, this benchmark still significantly lacks of comprehensive baseline comparisons and in-depth analysis to really show the audience **why** the identified problem is so important and challenging.\n\na. More baselines like few-shot GPT4/3.5, evaluating LLM/Multimodal LLM with chain-of-thought types of reasoning process, open-sourced models with different sizes should be further studied to provide a better understanding of the performance of existing methods on this task.\n\nb. Error analysis of existing models like GPT-4 or VideoLLAMA should be provided since it is important for the audience to understand how the proposed method solves the flaws in existing methods exactly. With the recent release of GPT4V, it will be great if some insights could be drawn from this case study as well.\n\nc. Currently the evaluation setting among different models is also not very clear. For example, models like VideoLLAMA actually takes video frames as input but other LLM models seem to only take parsed information. It is important to clearly annotate the exact format of each modality for all the model variants to make it clear to understand the possible difference from the input side.\n\n2. Currently, some limitation/ design choice is not well justified.\n\na. Why is the proposed method not applied to VideoLLAMA and Instruct BLip?\n\nb. Is the synthetic video data really capturing important goal and belief in real world? What is the domain gap? There should be at least case studies on some real procedural videos, investigating the possible domain shift to validate the usage of synthetic video and revealing the possible limitation of this benchmark more explicitly."
            },
            "questions": {
                "value": "Please check weakness for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Despite the usage of synthetic data, the authors should still further show that the synthesized data doesn't contain certain bias of gender or race, or any sensitive properties."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4069/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698845780762,
        "cdate": 1698845780762,
        "tmdate": 1699636371090,
        "mdate": 1699636371090,
        "license": "CC BY 4.0",
        "version": 2
    }
]