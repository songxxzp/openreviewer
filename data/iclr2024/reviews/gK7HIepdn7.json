[
    {
        "id": "TEiaMeczm9",
        "forum": "gK7HIepdn7",
        "replyto": "gK7HIepdn7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4993/Reviewer_A1cD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4993/Reviewer_A1cD"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel method for material-aware text-to-3D object generation. Previous work on this topic are limited in the sense that shading effects are baked into the material reflectance properties, thus not allowing for important applications like relighting or material interpolation, and reducing their quality. This paper introduces a BRDF autoencoder which allows text-to-3D generative models to return full BRDF parameters, by leveraging a VAE trained on a large dataset of BRDFs. This autoencoder, combined with a pre-trained text-to-image model and a radiance field representation, allows for the generation of 3D objects from text prompts with disentangled material properties, including surface albedo, roughness or normals. The paper evaluates their results with ablation studies and a user study, in which they compare their work with previous text-to-3D models across a variety of metrics."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper introduces an interesting and sound solution to a key limitation of text-to-3D generative models. Disentangling geometry (or shading) from reflectance remained a challenge for such generative models and this work introduces a valuable solution to this problem. \n- The method introduced in this paper is sound, and combines ideas from classical computer graphics and more modern neural rendering and generative model techniques in an interesting way, which may be valuable for many downstream applications and other problems. \n- The method is evaluated on a large variety of materials and objects, and the results show improved quality with respect to baselines. \n- The qualitative analysis is sound, and the user study, albeit limited, provides insights on user preferences for different works.\n- The paper is very well structured and mostly well written, making it easy to follow. \n- The material interpolation and edition methods are very interesting and the results are impressive. \n- The ideas for improving the VAE results and the semantic-aware material regularization are sound and may benefit future work on different topics. \n- The supplementary material provides valuable insights on the quality of the results and the impact of different individual components."
            },
            "weaknesses": {
                "value": "- I believe the paper could benefit from a better motivation. In particular, it is not clear why separating shading from reflectance is difficult and which approaches exists for this problem. Further, the lambertian assumption and its limitations require better explanation, to help the reader understand what are the challenges that this paper addresses and why the solution is valuable. I suggest looking into and referring the reader to \"A Survey on Intrinsic Images: Delving Deep Into Lambert and Beyond, Garces et al. IJCV 2022) for a contextualization of this problem.\n- The related work analysis is somewhat limited. First, I think this paper requires a more in-depth analysis of radiance field representations (At least Gaussian Splatting should be mentioned). Second, more recent work on generative models for material estimation should be included (ControlMat, UMat, SurfaceNet, etc.). Importantly, work on BRDF compression should also be analyzed (eg the work by Gilles Rainer et al. on neural BRDFs and BTF compression). Recent work on text-to-3D is missing (SMPLitex, HumanNorm), although these may be concurrent and thus not applicable to this submission. Finally, I think that this paper is also missing an analysis of illumination representations, as the authors only test basic environment maps, but other approximations (point lights, spherical gaussian, neural illumination approximations, spherical harmonics, etc) also exist and I believe should be mentioned.\n- I am doubtful about the soundness of some parts of the method, in particular Sections 3.2 and 3.3. (See the Questions section). \n- I have several concerns regarding the validation of this method, particularly in the ablation and the user study (See the Questions section). \n- There are important details missing, particularly in terms of computational cost. \n- The results are sometimes of a low quality (the geometries are sometimes very coarse and not sharp). While this is a limitation shared with previous work, I believe that it should at least be mentioned in the paper. \n- There is no analysis of limitations or suggestions for future work. I believe these should be included.\n- Implementation details are not enough for reproducibility."
            },
            "questions": {
                "value": "- How much of the capabilities of this method are linked with the radiance field representation that was chosen? That is, why was MIP-NeRF chosen and what would happen if other model was selected instead?\n- Why was Cook-Torrance chosen as the material model? What would happen if a more complex or a simpler model was used instead? This material model, in the form explained in the paper, does not model anisotropy, among other reflectance properties. This limits the generality of the materials and objects that can be generated with them. \n- Why was the TwoShotBRDF dataset used? There are plenty of other datasets of SVBRDFs available, of higher resolutions and with a different diversity of material classes. I am wondering how many of the limitations of the method (eg it struggles with metallic objects) are due to the dataset choice.\n- How were the hyperparameters of the different losses selected? What are their impact?\n- In section 4, could the authors provide an in-depth analysis of the computational cost of each part of the method? I think it would be interesting to see timings, memory usage and FLOPs. \n- The authors mention that \"we initialize DMTet with either a 3D ellipsoid, a 3D cylinder, ...\". How is this selected? Is it automatic?\n- What are the demographics of the user study? Are they instructed in any meaningful way? Can the authors provide a more detailed description of the test that each participant undertook? I am not convinced that it is fair to ask random people to measure the \"disentanglement\" of a text-to-3D generative model, as this is very hard to evaluate even for experts or automatic metrics. \n- How correlated are the generated objects with their materials? For example, if prompted for a \"goblet\", does it always generate metallic objects or are more variations allowed? How well does it generate implausible objects (eg \"a wool goblet\" or a \"ceramic pizza\", etc)?\n- How does this method handle fibrous objects (eg a fleece fabric or a knitted teddy bear)? \n- I suggest the authors include the illumination used in every render, particularly when showing results of relighting. It is unclear which type of illumination was used. Given the images that are shown, my guess is that very diffuse illumination was used to render the objects, which makes me wonder how well these 3D objects look on more directional illumination. \n- Could the material MLP be used to generate different set of BRDF parameters from different material models (eg Phong, complex Cook-Torrance, etc.) from the same latent space? If so, how would it impact the final results?\n- Transparency and surface scattering is very important in many real world materials and basic BRDFs cannot model such behaviours. How would the authors extend this work in order to generate full BSDFs? NeRF explicitly models alpha in their MLPs. Could this be combined with the Cook-Torrance BRDF in any way so as to allow for modelling these more complex effects? \n\n\n\nWriting improvement suggestions:\n- Page 1, Paragraph 2: \"The neural network ... has no sufficient motivation\". This sentence is anthropomorphizing the neural network and could be written in a different, more technical, way.\n- Page 1, Paragraph 3: \"There exist$\\textbf{s}$\", \n- Page 3, Section 2.3: \"Unluckily\" could be changed to \"Unfortunately\", which sounds better in my opinion. \n- Page 7: \"he/she\" --> \"they\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The authors have mentioned that \"I certify that there is no URL (e.g., github page) that could be used to find authors' identity.\" on their submission. However, I came across this URL which is not anonymized: https://github.com/SheldonTsui/Matlaber . I am not sure this complies with ICLR code of conduct and I would ask the ACs to look into this. My review has not been influenced by this in any way."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4993/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697557648871,
        "cdate": 1697557648871,
        "tmdate": 1699636486875,
        "mdate": 1699636486875,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tROj6mV4fC",
        "forum": "gK7HIepdn7",
        "replyto": "gK7HIepdn7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4993/Reviewer_Kg7h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4993/Reviewer_Kg7h"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel method to generate 3D assets with more disentangled reflectance maps by exploiting 2D diffusion priors and BRDF priors. To further improve disentanglement, a novel loss function is adopted to encourage piece-wise constant material. To me, the paper is well-written and easy to follow."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper works on an important problem, i.e. generating 3D assets with reflectance maps.\n* The paper improves previous work's results by incorporating more priors.\n* The paper performs a user study to demonstrate its advantage over the previous works.\n* The application demonstrated in the paper is interesting, including material editing and interpolating."
            },
            "weaknesses": {
                "value": "(1) To me, the techniqical contribution is limited. \n* Leveraging a trained BRDF prior to regularize the inverse rendering algorithm is a common way in the literature. As the author discussed in the related works, Neural-PIL and NeRFactor do very similar things. Other works also introduce a low rank prior to the spatially varying BRDF[1][2]. \n* I know that the paper is the first to introduce this prior in 3D AIGC. Are there more challenges to using the BRDF prior in the 3D AIGC pipeline?\n\n(2) The semantic-aware material regularization is not well evaluated in my eyes.\n* Many previous works have proposed techniques to regularize the material values, e.g., Munkberg et al., 2022. To me, if the author claimed L_mat is their main contribution, more comparisons to previous techniques are expected. However, the paper only compares their method to the w/o L_mat baseline.\n\n[1] Neural reflectance for shape recovery with shadow handling, CVPR2022\n\n[2] NeuFace: Realistic 3D Neural Face Rendering from Multi-view Images, CVPR2023"
            },
            "questions": {
                "value": "I find that the paper randomly samples the environment maps from a pool of collections and randomly rotates the map during training. I think such a multi-light setup can reduce the ambiguity in the inverse rendering process a lot. However, as shown in Figure 8, the reconstructed albedo is disentangled without the BRDF prior and the L_mat. Can the author provide some insights on these unexpected results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4993/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4993/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4993/Reviewer_Kg7h"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4993/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697615610666,
        "cdate": 1697615610666,
        "tmdate": 1699636486784,
        "mdate": 1699636486784,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2PBjYMUTZk",
        "forum": "gK7HIepdn7",
        "replyto": "gK7HIepdn7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4993/Reviewer_VHfX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4993/Reviewer_VHfX"
        ],
        "content": {
            "summary": {
                "value": "In this manuscript, the authors investigated a framework to generate material appearance in a text-to-3D latent diffusion model. They utilized a latent BRDF auto-encoder and compared their results with existing models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Estimating material properties is often overlooked in image generation. I agree that including them is essential for future image generation. However, I'm not convinced with the authors' model in terms of the following points."
            },
            "weaknesses": {
                "value": "The BRDF is one of the descriptions of physical material properties. Many natural objects include more than reflection, like absorption or sub-surface scattering. Adding the constraint of BRDF in their model makes the material appearance of output images narrower than other methods, like Fantasia3D. For example, the ice cream in Figure 3 by Fantasia3D looks translucent, but the authors' output lacks such a translucent material appearance, which is critical for foods. \n\nIn addition, the diffuse component of gold in Figure 1 is weird. The ground truth of yellow components for gold materials comes from the specular reflection of metals, not from diffuse components. The model does not look to capture material properties. \n\nFor the user study, the authors did not conduct any statistical tests. They cannot conclude anything without them."
            },
            "questions": {
                "value": "The authors compare their method only with text-to-images. However, in particular, material editing in Figure 7 has a long history in the Computer Graphics community, and many methods have been developed. The text-to-image is not only the way to edit material appearance. The authors should also compare them with their editing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4993/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4993/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4993/Reviewer_VHfX"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4993/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763449788,
        "cdate": 1698763449788,
        "tmdate": 1699636486692,
        "mdate": 1699636486692,
        "license": "CC BY 4.0",
        "version": 2
    }
]