[
    {
        "id": "X340xyYxq6",
        "forum": "ATEawsFUj4",
        "replyto": "ATEawsFUj4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5722/Reviewer_tNS3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5722/Reviewer_tNS3"
        ],
        "content": {
            "summary": {
                "value": "This work aims to generate talking avatars with two separate modules: 1. first disentangle motion and appearance; 2 then generate head motions in accordance with speech.  As the proposed method does not utilize 3DMMs, the proposed data-driven method is promising to generate talking avatars with better diversity and naturalness.  The reported experiments show better quantitative results and better visual quality."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This work establishes a large-scale talking avatar dataset for data-driven talking avatar generation.\n\n2. The proposed data-driven method could achieve taking avatar generation with superior performance on naturalness, diversity, lip-sync quality, and visual quality.  \n\n3. The proposed method is scalable, and the authors conduct experiments that show the larger model is employed, the better performance could be achieved.\n\n4. The authors show that the proposed method could support many related applications, such as controllable talking avatar generation and text-driven video generation."
            },
            "weaknesses": {
                "value": "The authors did not provide a large number of generation examples. I hope it is possible to see more visual results.\nFor example, \n1. long videos for the generated talking avatars;\n\n2. different reference video/frame but the same driving video;\n\n3. different driving video but the same reference video/frame."
            },
            "questions": {
                "value": "1. is it possible to drive cartoon characters (humanoid or non-humanoid)?\n\n2. will the proposed method and the established dataset be publically available?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5722/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698670908464,
        "cdate": 1698670908464,
        "tmdate": 1699636599026,
        "mdate": 1699636599026,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wUn0m5sjQC",
        "forum": "ATEawsFUj4",
        "replyto": "ATEawsFUj4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5722/Reviewer_Lkke"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5722/Reviewer_Lkke"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a data driven approach for generation of 2D avatars. The method disentangles motion and appearance and uses a diffusion model to allow motion generation conditioned on pose and speech data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The method is conceptually simple and sensible.\n2) It is shown to scale well in terms of model size and as a self-supervised method can utilize readily available training data at scale.\n2) Method requires very few pretrained components.\n3) Evaluation includes user study which is always good for addressing output quality.\n4) Method is highly flexible and allows a high degree of control from pose, facial attributes and text."
            },
            "weaknesses": {
                "value": "1) A comparison with https://arxiv.org/pdf/2012.08261.pdf for video driven is critically missing as a recently proposed SOTA method. In their paper they show improvements compared to face-vid2vid and FOMM which are used as baselines here and they provide a pretrained checkpoint."
            },
            "questions": {
                "value": "1) will the dataset be shared as part of this submission?\n2) It would be interesting to see an ablation on training data size to assess whether there are benefits from scaling data further.\n3) How does randomness from the diffusion model affect generations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5722/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5722/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5722/Reviewer_Lkke"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5722/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698702137079,
        "cdate": 1698702137079,
        "tmdate": 1699636598920,
        "mdate": 1699636598920,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3NP8TiZs8K",
        "forum": "ATEawsFUj4",
        "replyto": "ATEawsFUj4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5722/Reviewer_ft55"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5722/Reviewer_ft55"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an audio-driven talking head synthesis model named GAIA that is an end-to-end trainable data-driven solution. The model has two main stages 1. disentanglement of motion and appearance with VAE 2. speech-to-motion generation based on diffusion model. Also, the paper proposes a new talking head dataset with 8.2K hours of video and 16.9K unique speakers."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The manuscript proposes a new dataset.\n\nThe writing is supported by equations and well-drawn figures that make the explanation clear.\n\nAlthough the experiments with existing models are not enough (see weaknesses), the ablation study is rich and increases the overall quality."
            },
            "weaknesses": {
                "value": "The gap/ limitations of 3DMM-based models are found and addressed well by proposing an end-to-end trainable model. I am not sure it is novel enough as the other end-to-end trainable talking face synthesis models are not discussed enough.\n\nThe experiments are limited, especially comparison with end-to-end trainable models not provided. I suggest enriching the benchmarking with other existing models such as  PC-AVS and PD-FGC as they are also end-to-end trainable models. \n\nAlthough the writing quality is decent, it is hard to follow as it refers to other sections frequently and other issues (see Questions 1 and 2)."
            },
            "questions": {
                "value": "1. In section 3, what does 'we collect High Definition Talking Face Dataset (HDTF) (Zhang et al., 2021) and Casual Conversation datasets v1&v2 (CC v1&v2) (Hazirbas et al., 2021; Porgali et al., 2023)' and 'we also collect a large-scale internal talking avatar dataset named AVENA' mean? Does it mean you collect those datasets you use their sample in your dataset or you use their samples in your training but they are not in your dataset? From the supplementary material, my understanding is you collect AVENA and use samples from other datasets (HDTF, CC v1, and v2) in the training of your model. Could you please elaborate and make it clear?\n\n2. Why do you have a discussion section at the end of Section 4 Model? I think it makes more sense in/after experiments. So, you can consider reorganizing the manuscript to have a better flow and complete discussion.\n\n3. 3. I am not sure the model can be named as zero-shot as it requires one shot for unseen faces. So, could you elaborate on the following '... generates a talking video of an unseen speaker with one portrait image ...'?\n\n3. Ethical consideration is left in Appendix F. However, for this study ethical consideration is important. So, you might consider putting it into the main manuscript to give necessary importance if possible."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5722/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5722/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5722/Reviewer_ft55"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5722/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744584519,
        "cdate": 1698744584519,
        "tmdate": 1699636598809,
        "mdate": 1699636598809,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nhkezJyr7A",
        "forum": "ATEawsFUj4",
        "replyto": "ATEawsFUj4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5722/Reviewer_1S6P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5722/Reviewer_1S6P"
        ],
        "content": {
            "summary": {
                "value": "The authors propose to use a generative latent diffusion model to address the problem of talking head synthesis from audio and a single photo. The pipeline consists of a variational autoencoder that encodes the video frames into appearance and motion latent representations and a diffusion model that is trained to predict the pre-trained motion latent from audio and pose conditioning. The authors also propose to use a data filtering approach to remove the noisy samples from the dataset to achieve high-quality results. The experimental evaluation is quite extensive and includes audio, head pose, and text-driven examples."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Impressive quality of the results in terms of both lipsync and visual quality\n- The model's design is straightforward yet evidently effective\n- The paper is well-written and the evaluation is pretty extensive"
            },
            "weaknesses": {
                "value": "- Missing evaluation of disentanglement between appearance and pose latent codes, i.e., cross-reenactment with the motion codes extracted from the image of a different identity.\n- Missing discussion of the related works, such as [1, 2], that explored the concept of pose-identity disentanglement for talking head synthesis before this work.\n- As far as I can tell, the proposed method and the baselines were trained on different datasets. The resulting comparison evaluates the proposed framework _and_ the dataset at the same time. A comparison should include the experiments where base methods are trained on the same dataset, and the proposed method is trained on unprocessed datasets used in previous works.\n- Comparison of the inference time between the compared methods is not provided. I would also argue that some baselines, such as SadTalker, can be substantially improved, given the computational budget of the proposed method that runs the diffusion model for every time step. Ex., with the fine-tuning of the model given the source frame.\n\n[1] Burkov et al., \"Neural Head Reenactment with Latent Pose Descriptors\", CVPR 2020\n[2] Drobyshev et al., \"MegaPortraits: One-shot Megapixel Neural Head Avatars\", ACMMM 2022"
            },
            "questions": {
                "value": "- Please address the concerns mentioned in the weaknesses\n- Could the authors clarify if they plan to release the filtered dataset and the pre-trained models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5722/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826909162,
        "cdate": 1698826909162,
        "tmdate": 1699636598717,
        "mdate": 1699636598717,
        "license": "CC BY 4.0",
        "version": 2
    }
]