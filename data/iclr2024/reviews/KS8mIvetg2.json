[
    {
        "id": "2AU4wlHFOj",
        "forum": "KS8mIvetg2",
        "replyto": "KS8mIvetg2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9019/Reviewer_FS83"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9019/Reviewer_FS83"
        ],
        "content": {
            "summary": {
                "value": "This paper targets the problem of detecting test set contamination of black-box language models. The proposed method is based on two hypotheses: (1) the exchangeability of many datasets (distribution won't be affected after shuffling); and (2) if a language model is contaminated, it is more likely to find certain orderings of data samples than other orderings. Then a statistical test is proposed to compare the log probability of the dataset under the original ordering to the log probability under random permutations on sharded datasets. Experiments are conducted with one 1.4B-gpt2 model trained from scratch on 10 test sets, and the results prove the effectiveness of the proposed framework."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper targets an interesting and exciting problem in the community, test set contamination. \n- Based on the hypothesis, this paper proposed a contamination detection method, which is intuitive and easy to deploy in other settings.\n- The method is verified with a 1.4B language model trained from scratch, and the existing Llama2 model, both showing promising results even when the test set only appears a few times in the pre-training corpus."
            },
            "weaknesses": {
                "value": "- I'm most concerned about the definition of contamination used in this paper. Currently, the most popular definition of contamination follows the n-gram analysis. In real-world scenarios when training large language models, it's hardly seen to directly feed original data samples in their original ordering as shown in Figure 1. The application of this work could be greatly limited.\n- From Figure 3, it seems that the parameters for shards and permutations are sensitive and have to be carefully selected when being applied to other test sets.\n- The paper only targets direct sentence appearance in the pre-training stage. What about instruction-tuning data in the SFT stage?"
            },
            "questions": {
                "value": "- Could you further explain \"high false positives\" in existing n-gram-based analyses?\n- How did you deal with the labels for the data samples in test sets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9019/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9019/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9019/Reviewer_FS83"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9019/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698634294819,
        "cdate": 1698634294819,
        "tmdate": 1699637136358,
        "mdate": 1699637136358,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lPcg6UyLqg",
        "forum": "KS8mIvetg2",
        "replyto": "KS8mIvetg2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9019/Reviewer_gkwi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9019/Reviewer_gkwi"
        ],
        "content": {
            "summary": {
                "value": "This paper examines the issue of test set contamination in large language models (LLMs), referring to the phenomenon where LLMs memorize public benchmarks during their pretraining phase. Since the pretraining datasets are rarely available, this paper proposes a statistical test to identify the presence of a benchmark in the pre-training dataset of a language model without accessing the model\u2019s training data or weights. The intuition is the exchangeability of datasets\u2014 the order of examples in the dataset can be shuffled without affecting its joint distribution. If a language model shows a preference for any ordering of the dataset, it might have seen the data during pretraining. The test on the LLaMA-2 model identifies potential contamination in the MMLU benchmark, which is consistent with the results in the original LLaMA-2 report."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-\tThe idea of utilizing dataset exchangeability to identify test set contamination is novel and interesting. \n-\tThe proposed sharded likelihood comparison test addresses the tradeoff between statistical power and computational requirements of the permutation test, which is promising. The sharded rank comparison test also provides (asymptotic) guarantees on false positive rates.\n-\tExperimental results are promising. A GPT-2 model is trained from scratch on standard pretraining data and known test sets to verify the efficiency of the proposed method in identifying test set contamination. The method is also tested with an existing model, LLaMA2, on the MMLU dataset, showing general agreement with the contamination study results."
            },
            "weaknesses": {
                "value": "-\tAlthough a more efficient sharded rank comparison test is proposed, the computational complexity is still considerable. For example, testing 49 files using 1000 permutations per shard can take 12 hours for LLaMA2.\n-\tThere is no comparison with other baseline methods.\n-\tThe method relies on a strong assumption of data exchangeability, which may not hold in real-world datasets."
            },
            "questions": {
                "value": "If a dataset is not exchangeable, how effective is the method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9019/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9019/Reviewer_gkwi",
                    "ICLR.cc/2024/Conference/Submission9019/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9019/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698805958824,
        "cdate": 1698805958824,
        "tmdate": 1700664112747,
        "mdate": 1700664112747,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iFEG2apB2b",
        "forum": "KS8mIvetg2",
        "replyto": "KS8mIvetg2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9019/Reviewer_Rgfh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9019/Reviewer_Rgfh"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of identifying test set contamination in large language models, i.e., detecting that a test set is present in the pretraining data of a language model. The main idea behind the approach is that for test sets that have some canonical order of individual instances (e.g.: the order in which the dataset creators release the dataset), the likelihood of the test set in that order would be significantly higher than any random permutation of the dataset. Based on this idea, the paper proposes two versions of the test, one of which shards the test set and aggregates statistics over the shards to make the estimate more robust to potential biases in the model.\n\nThe tests are evaluated first by measuring their sensitivity when pretraining datasets are intentionally contaminated. It is shown that they are highly sensitive when the tests sets are large or have been duplicated enough in the pretraining data. The test is then used to measure contamination of the pretraining data used to train the Llama models and it is shown that the findings agree with prior reports."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This is clearly written paper and makes a strong contribution. The tests do not require access to model weights or pretraining data, making them practically useful."
            },
            "weaknesses": {
                "value": "The experiments do not compare the performance of the proposed tests to prior work. I understand that this work differs from say, the work from Carlini et al. in that this work focuses on set-level contamination, but how does aggregating instance-level statistics over a set compare?"
            },
            "questions": {
                "value": "- How does the performance of this method compare to that of prior work (see Weakness)\n- How sensitive is the proposed test to the model size?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9019/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9019/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9019/Reviewer_Rgfh"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9019/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698882170265,
        "cdate": 1698882170265,
        "tmdate": 1699637136074,
        "mdate": 1699637136074,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5otHPtssNS",
        "forum": "KS8mIvetg2",
        "replyto": "KS8mIvetg2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9019/Reviewer_jRLi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9019/Reviewer_jRLi"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a statistical test that given certain assumptions can indicate whether a black-box language model has been trained on certain datasets. This is a topic of increasing interest and importance given the prevalence of pretrained models that are trained on very large amounts of data.  The authors first propose a simple permutation test and identify some weaknesses with it. They then propose a more sophisticated sharded test. The authors show 2 kinds of experiments:\n\n(1) They test on a dataset where they have injected a small amount of certain test sets to see if their approach can detect them.\n\n(2) They apply their test to existing models such as Lllama-2 showing their approach can scale."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-Topic of large importance in the community given the direction of the field. \n\n-Novel approach with thorough empirical results. I have some questions about the definition of test set contamination below.\n\n-Well written and interesting."
            },
            "weaknesses": {
                "value": "I have some questions about the definition of test set contamination below."
            },
            "questions": {
                "value": "In Figure 1 the authors show test set contamination for BoolQ. But the examples there are unlabeled. Are the authors targeting unlabeled test set contamination i.e. the input is present in the pretraining data but not the label? \n\nWould be great to have some justification and explanation of this setting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9019/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699068022751,
        "cdate": 1699068022751,
        "tmdate": 1699637135936,
        "mdate": 1699637135936,
        "license": "CC BY 4.0",
        "version": 2
    }
]