[
    {
        "id": "Np1HZYdSOk",
        "forum": "A2mRcRyGdl",
        "replyto": "A2mRcRyGdl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2116/Reviewer_QtRh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2116/Reviewer_QtRh"
        ],
        "content": {
            "summary": {
                "value": "In this paper the problem of novel view synthesis of semantic labels is studied. Rather than rendering colour, the segmentation is rendered with a NeRF model. The authors propose a model and training procedure that can learn scene flow fields and semantic renderings of a given video sequence. Furthermore, the proposed method allows for quick adaptation on novel video sequences. Since no earlier work studies this problem, the authors label the nvidia dynamic scenes dataset with pixel-wise semantic labels. The experiments show that the semantic labels can be rendered accurately, and furthermore that we do not need labels for all frames in a sequence, and that we can use the semantic labels to mask out specific parts of the video and render it without specific objects."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- There is no existing dataset for the problem setup, so authors annotated the nvidia dynamic scenes dataset with pixel-wise semantic segmentation labels.\n- A strength of the method is that it allows for quick adaptation to new scenes, e.g. with just 500 iterations it can perform well given pre-training on other scenes. The reason is that the scene flow field is not learned from scratch, but rather from frame-wise video features, which does not need to be learned from scratch for each scene.\n- Augmenting NeRF models with semantic segmentation has been done for static scenes (e.g. Zhi et al 2021) but to the best of my knowledge not for general dynamic scenes, so the paper tackles a new problem setup.\n- The method is clearly described and ablations are provided for the main components."
            },
            "weaknesses": {
                "value": "- There are some baselines that would be reasonable to try that are missing from the paper. For instance, if we just render the rgb images with any NeRF method (e.g. MonoNeRF) and apply some video object segmentation algorithm (e.g. any top-performing method on the DAVIS dataset) or semantic segmentation method (trained on some dataset with overlapping labels), how well would that perform?\n- Since one of the applications mentioned in the paper is scene editing, i.e. removing some specific object, it is necessary to not just render semantics correctly but also rgb. There are no values provided for the standard novel view synthesis metrics (PSNR, SSIM, LPIPS) for rgb on the tested video sequences.\n- It would have been interesting to somehow visualise or discuss the flow fields. Since the objective is semantic rendering rather than colour rendering it is not clear if we need the scene flow to map to the same specific part of an object, or if it is sufficient or even beneficial to just map to anywhere within the same object. For instance, the consistency loss L_consis only enforces that points along flow trajectories should have the same label.\n\nMinor issues:\n- Missing related work: \u201cPanoptic neural fields: A semantic object-aware neural scene representation\u201d (CVPR 2022) also considers novel view synthesis for semantic segmentation from a video, although their method is limited to non-deformable dynamic objects.\n- Page 5: Does ground truth flow mean optical flow estimated from RAFT? If that is the case it should not be called ground truth.\n- Page 5: The closing parenthesis in eq. (7) is probably incorrect.\n- Page 8: \u201cBoundray\u201d typo\n- Page 8: Table 2 caption: \u201cqualitative\u201d should be \u201cquantitative\u201d"
            },
            "questions": {
                "value": "See everything under weaknesses.\n- When training for semantic completion or tracking, only a subset of the frames are used for semantic supervision. Is the same true for RGB supervision or are all frames used for that? \n- In Fig. 3, what are the indices of the frames that are shown? How far are they from the frames with semantic labels?\n- For the DynNeRF and MonoNeRF baselines, what exactly is the input to the semantic heads that are learned?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2116/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2116/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2116/Reviewer_QtRh"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2116/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698743629260,
        "cdate": 1698743629260,
        "tmdate": 1699636144337,
        "mdate": 1699636144337,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "w7qNUjTLoQ",
        "forum": "A2mRcRyGdl",
        "replyto": "A2mRcRyGdl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2116/Reviewer_5VQj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2116/Reviewer_5VQj"
        ],
        "content": {
            "summary": {
                "value": "The paper looks to solve the problem related to generating a novel view 2D semantic map, for dynamic scenes using continuous flow. Paper leverages optical flow for the foreground part of the images and uses volume density as a prior to determining flow feature contribution towards semantics. Authors evaluate this on Dynamic scene dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strengths:\n-The paper is well written with the objective clearly identified. It is structured well and has logically moving sub-section-wise explanations. \n-Tackles a well-known problem in terms of generating novel view synthesis for dynamic scenes, but for semantics.\n- Proposes a novel idea, that leverages optical flow to predict semantic labels for dynamic foreground pixels/regions.\n- Evaluate and compare the model on the Dynamic Scene Dataset."
            },
            "weaknesses": {
                "value": "Weakness:\n- Paper leverages optical flow output as one of the intermediate steps, but fails to discuss its shortcomings and how exactly do they handle occlusion and disocclusion related to both dynamic and static regions of the frame.\n- For the most part of the paper, the authors only compare with two dynamic scene-based works, Considering other related works in dynamic scene reconstruction, Would be great to see comparative baseline results, with a few more of these models with semantic head."
            },
            "questions": {
                "value": "- Could Authors share some of the shortcomings (a few qualitative results) which may be due to imperfect flow prediction, which results in bad performance during inference?\n- In Section 3.4: while calculating Semantic Consistency Constraint; Do we generate some sort of valid mask here to enforce the semantic consistency or is it done for all pixels, irrespective of occlusion or uncertainty?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2116/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2116/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2116/Reviewer_5VQj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2116/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698793723798,
        "cdate": 1698793723798,
        "tmdate": 1699636144256,
        "mdate": 1699636144256,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FHiEumEw3D",
        "forum": "A2mRcRyGdl",
        "replyto": "A2mRcRyGdl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2116/Reviewer_xtsS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2116/Reviewer_xtsS"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces learning a semantic field of the dynamic scene using NeRF given a monocular video. Given a sequence of input frames from a monocular video, precomputed optical flow, their Dynamic NeRF learns a semantic field so that it can render a semantic segmentation map at novel views. The method can be used for a couple of applications that output semantic field for unseen frames given partial frames."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Better accuracy over baseline methods\n\n  Compared to the two baselines, the method shows better accuracy on multiple tasks (scene completion, scene tracking, and semantic representation) in Table 1 and Table 2. Also it demonstrates better qualitative results in Fig. 3\n\n- New applications\n\n  The paper proposes interesting new applications, both dynamic scene tracking and completion that estimates semantic maps on unseen frames. (Fig. 1)"
            },
            "weaknesses": {
                "value": "- Outdated baselines\n\n  The paper compares their method with a couple of baselines (DynNeRF and MonoNeRF) but those are a bit limited. There are many other baselines for the dynamic NeRF task such as D-NeRF, RoDynRF, NSFF (Neural Scene Flow Field), etc. It would have been great if the paper provided accuracy on more baseline methods to make the comparison much fairer. \n\n- A bit difficult to follow the equations (from Eq. (4) to Eq. (8))\n\n  I am wondering if it's possible to put the mathematic notation from Eq. (4) to Eq. (8) into Fig. 2 for better understanding.\n\n- Clarity \n\n  Some parts of the paper have lack of clarity and make it hard to understand clearly. What is the meaning of '25%/50% semantic labels' in Fig. 3? I wonder if the paper can provide more details in the figure captions. How are the 25%/50% determined? \n\n- Marginal accuracy improvement in Fig. 4\n\n  The choice of low displacement seems not so critical for the accuracy gain. Maybe it would be good to have a justification or discussion on the result."
            },
            "questions": {
                "value": "- How much does the accuracy of the method depend on the off-the-shelf optical flow methods? Can it be critical?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2116/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2116/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2116/Reviewer_xtsS"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2116/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814111059,
        "cdate": 1698814111059,
        "tmdate": 1700721718844,
        "mdate": 1700721718844,
        "license": "CC BY 4.0",
        "version": 2
    }
]