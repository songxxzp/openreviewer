[
    {
        "id": "XJ7HjT7j5J",
        "forum": "Ys3uPmZGOR",
        "replyto": "Ys3uPmZGOR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4542/Reviewer_2vx7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4542/Reviewer_2vx7"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the concept of workers that encompass one or more information\nprocessing units, which can be either leader or follower (LFNN) that leveraging local error signals to learn. LFNN does not require backprobagation (BP) and global loss to achieve the best performance. Extensive experiments are performed to verify the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The design of leader-follower workers that performs collaborative learning in a neural network is very interesting.\n\n2. Experimental results seems to be promising."
            },
            "weaknesses": {
                "value": "1. Although the proposed method is claimed to be inspired from biological neural network, the rationale behind it is still unclear to me. I\u2019m not an expert in biological neural network, but I assume any improvement of the new design should have theoretical-grounded explanation. Could the author of the paper provide such theoretical proof on the proposed method?"
            },
            "questions": {
                "value": "Please refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809500137,
        "cdate": 1698809500137,
        "tmdate": 1699636431450,
        "mdate": 1699636431450,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bFpSmkIfAu",
        "forum": "Ys3uPmZGOR",
        "replyto": "Ys3uPmZGOR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4542/Reviewer_Mhh7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4542/Reviewer_Mhh7"
        ],
        "content": {
            "summary": {
                "value": "The paper introduced the worker concept to neural networks and divided the components of NNs into leaders and followers. By leveraging the local error signals, a novel BP-free training method is proposed. And extensive study shows that the proposed training method can achieve promising results on the benchmark datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea that combines the worker concept with the training of neural network is novel and  interesting.\n\n2. The proposed method is comprehensively evaluated and the performance on the benchmark is promising.\n\n3. The paper is well written and organized."
            },
            "weaknesses": {
                "value": "1.The proposed method can outperform other BP-free baselines on different benchmark datasets. However, the performance of proposed LFNN method seems not promising on complete ImageNet-1K, compared to BP method. It may suggest that the proposed method may not be effective in large scale datasets.\n\n2.The comparsion of the convergence of BP, other BP-free method and the proposed method is missing in the paper.\n\n3.The ablation study on hyper-parameter $\\lambda$ is missing in the paper."
            },
            "questions": {
                "value": "1.The results in Fig.2a suggest that the models with approximately 50%-60% leaders would achieve lowest performance. Can authors explain why that happen?\n\n2.The proposed method requires 90% leadership on Tiny ImageNet and ImageNet subset and 100% leadership on complete ImageNet for both LFNN and LFNN-$\\ell$. Does it mean that the demand for the leadership is increasing as the size of dataset grows? Would that increase the computation cost for the training?\n\n3.Compared to BP and other BP-free methods, is LFNN-$\\ell$ more time-efficient during the training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4542/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4542/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4542/Reviewer_Mhh7"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818373532,
        "cdate": 1698818373532,
        "tmdate": 1699636431369,
        "mdate": 1699636431369,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6WN8E79git",
        "forum": "Ys3uPmZGOR",
        "replyto": "Ys3uPmZGOR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4542/Reviewer_sRgK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4542/Reviewer_sRgK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a leader-follower neural network without back-probagation for learning, LFNN, by leveraging local error signals.\n\nLFNN shows better performance than other BP-free variants and achieves <2x speedup compared to BP."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The leader-follower neural networks, LFNN, without back-propagation is interesting.\n\nExperiments on CIFAR-10, Tiny-ImageNet and ImageNet are conducted to show the advantages of LFNN."
            },
            "weaknesses": {
                "value": "The citation reference seems not using ICLR template, e.g., the reference only uses numbers.\n\nThe architecture of LFNN is not clear. From the table 1, it looks LFNN took the architecture from [1], LocalMixer.\nI would like to know why this is not clearly described in the paper? And why more parameters are added based on [1]. \n\nThe math definition formula of L_{l}^{\\delta} is missing in eq. (1). Also the third term, L_{l}^{\\bar{\\delta}} is a little bit confusing. \nIt is a mean squared error. But the number of outputs from leaders seems different from followers in one layer.\n\nIn definition 2.1, leaders are decided by prediction errors. But for each hidden layer, how can we get the ground-truth labels?\n\nFor the loss, two are using cross-entropy loss and another one is using MSE loss. Are these three losses in a comparable range?\n\nFrom Table 1, LFNN are outperforming other baselines in [1] on accuracy. How about training time comparing to the baselines in [1]?\nAlso, what are the insights for the improvements?\n\nIn Table 3, why LFNN-l is doing better than BP on Tiny-ImageNet? Is this scalable to ImageNet?\n\n\n[1] SCALING FORWARD GRADIENT WITH LOCAL LOSSES"
            },
            "questions": {
                "value": "See the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822186362,
        "cdate": 1698822186362,
        "tmdate": 1699636431254,
        "mdate": 1699636431254,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xbVl6KgBd9",
        "forum": "Ys3uPmZGOR",
        "replyto": "Ys3uPmZGOR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4542/Reviewer_KQC9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4542/Reviewer_KQC9"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a leader-follower framework that enables neural network learning without the need for backpropagation. Specifically, the authors view deep neural networks as a composition of workers operating at the level of neurons, layers, or blocks. These workers are classified into leaders and followers, with the calculation of individual losses for each. Leader workers use both global and local losses, whereas follower workers are trained to replicate outputs identical to those generated by their corresponding leader workers. The proposed BP-free algorithm abstains from the utilization of global loss for training leader workers. The experiments conducted on MNIST, CIFAR-10, and ImageNet show the potential of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is overall clearly clarified and well organized.\n2. The proposed method offers an interesting approach to enable neural networks to learn without the use of backpropagation."
            },
            "weaknesses": {
                "value": "1. The proposed method is interesting; however, it lacks a comprehensive explanation of its effectiveness within the neural network learning framework. Specifically, there is a need for detailed elucidation of the mechanism behind the reduction of prediction errors for the leader workers from the intermediate layer, as well as the minimization of the distance between the leader and follower workers.\n - Why does the test accuracy decline between the 40% and 60% leaderships in Figure 2a?\n - In Figure 2b, what role does $L_l^\\delta$ serve in the comparison between $L_g$ and $L_g + L_l^\\delta$?\n - In Figure 2b, despite the similarity between $L_g + L_l^\\delta$ and $L_g + L_l^\\delta+L_l^\\hat{\\delta}$ in the later stages of training, what is the specific function of $L_l^\\hat{\\delta}$?\n2. Insufficient discussion is provided regarding the experimental results.\n - Occasionally, LFNN-$\\ell$ leads to superior generalization performance compared to LFNN, as indicated in the results of Table 1. What might be the underlying reason for this phenomenon?\n - While LFNN outperforms BP and LG-BP on simpler datasets in Table 1, it lags behind them in terms of ImageNet results. What could be the reason behind such outcomes?"
            },
            "questions": {
                "value": "(Copied from Weaknesses)\n - Why does the test accuracy decline between the 40% and 60% leaderships in Figure 2a?\n - In Figure 2b, what role does $L_l^\\delta$ serve in the comparison between $L_g$ and $L_g + L_l^\\delta$?\n - In Figure 2b, despite the similarity between $L_g + L_l^\\delta$ and $L_g + L_l^\\delta+L_l^\\hat{\\delta}$ in the later stages of training, what is the specific function of $L_l^\\hat{\\delta}$?\n - Occasionally, LFNN-$\\ell$ leads to superior generalization performance compared to LFNN, as indicated in the results of Table 1. What might be the underlying reason for this phenomenon?\n - While LFNN outperforms BP and LG-BP on simpler datasets in Table 1, it lags behind them in terms of ImageNet results. What could be the reason behind such outcomes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4542/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4542/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4542/Reviewer_KQC9"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699251841231,
        "cdate": 1699251841231,
        "tmdate": 1699636431188,
        "mdate": 1699636431188,
        "license": "CC BY 4.0",
        "version": 2
    }
]