[
    {
        "id": "cbWPcV04A9",
        "forum": "gfFVATffPd",
        "replyto": "gfFVATffPd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1031/Reviewer_MD6P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1031/Reviewer_MD6P"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an interesting finding that the magnitude of attention contribution to \u201cconstraint entities\u201d (something like key entities associated with the prediction) is well-correlated with popularity of the constraint entity, the constrainedness of the query, and more importantly, the correctness of the LLM prediction. The authors define metrics to quantify such attention contribution and expect to use the metric to predict factual errors of LLM predictions. On experiments across multiple datasets, the proposed method demonstrates comparable factual error prediction performance to the confidence (i.e. logits) baseline."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper presents interesting and novel findings of a phenomenon in the hidden representations of LLMs, that the magnitude of attention contribution is correlated with factualness of the prediction. It may inspire future researchers to discover and probe more interpretable inner patterns of LLMs, which could potentially help researchers understand and explain models\u2019 behaviours.   \n2. The authors perform comprehensive analysis to verify the correlation of attention contribution with various properties such as popularity, constrainedness, and factualness. These analyses are convincing and the different types of visualization are informative and efficient to convey the messages.  \n3. The paper is very well-written."
            },
            "weaknesses": {
                "value": "1. Although the findings are interesting to know, I doubt the practicality of the proposed method and the contribution it makes from a practical perspective:  \n    a. Attention contribution is computed assuming access to the constraint entities, which are not available in most of cases  \n    b. The proposed method is not better on factual error prediction than the simple confidence baseline. It seems to me it does not bring any new information beyond the confidence score\n\n2. I think this paper focuses on and only studies a narrow notion of factuality, where constraint entities exist as in a world knowledge domain. It is unclear whether attention contribution is a valid metric to explain other factual errors, such as mathematical errors and inference errors where the traditional notion of \u201centities\u201d may not always exist. This is important since researchers talk about factuality and hallucination actually in a quite broad scope in the LLM context nowadays, not limited to entity-based knowledge.\n\n3. From Figure 14-23, all the datasets look simple and toy. Although it appears that the experiments are conducted on a diverse range of datasets, these datasets turn out to be very similar style-wise \u2013 they are very similar QA tasks with one-entity answers. While I understand that it is difficult to perform experiments on more realistic data since the constraint entities will become unavailable, I am not sure how general the proposed findings are for LLMs given these toy datasets."
            },
            "questions": {
                "value": "1. In section 5.1 the authors mention fitting Logistic Regression, does that mean you need a training dataset with labels to learn the parameters w and b? If so, how large is the training data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1031/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698851348304,
        "cdate": 1698851348304,
        "tmdate": 1699636029345,
        "mdate": 1699636029345,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LOnu36wBbh",
        "forum": "gfFVATffPd",
        "replyto": "gfFVATffPd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1031/Reviewer_m32R"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1031/Reviewer_m32R"
        ],
        "content": {
            "summary": {
                "value": "This work focuses on investigating characteristics of attention patterns when LLM makes factual errors or hallucinates. They show that entity tokens (defined as constraining tokens) in the query input exhibit strong correlation with the answer entity tokens. The authors had three key observations 1.) the more popular an entity-relation pair is (i.e., appears more frequently in corpus), LLM is less likely to be factually incorrect about it 2.) attention scores can serve as a way to measure LLM confidence and correctness and 3.) the larger the LLM the more the attention scores and attention scores correlate in a similar way across different size models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.) This work looks at mechanistic interpretability especially with respect to factor errors made by LLMs, which has not been studied before as per my knowledge.\n\n2.) They do extensive analysis and show the effectiveness of their probing technique across different datasets and models.\n\n3.) SAT-probe can also help predict failures by analyzing attention scores mid-training, which can be useful of debugging purposes."
            },
            "weaknesses": {
                "value": "None I can think of"
            },
            "questions": {
                "value": "1.) The observation that \"when the LLM is accurate, there is more attention to constraint token\" suggests that when LLM is confident it is often right. How does this generalize in cases shown in prior work when LLMs confidently hallucinate? It would seem that attention scores would be high even when hallucinating. In your experience, is there any way to differentiate between attention scores (or some other variable) for correct vs incorrect confident answers.\n\n2.) In Figure 5, it looks like attention is summed across layers, its unclear if the final score was normalized by number of layers? Is it possible that attention scores are more comparable between 7B and 13B as compared to 7B and 70B because of the number of layers scaling?\n\n3.) Were there any insights as to how attention patterns vary across different layers, especially for different sized models? for e.g., higher attention scores for hallucinating tokens towards the later layers perhaps?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1031/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699039465264,
        "cdate": 1699039465264,
        "tmdate": 1699636029262,
        "mdate": 1699636029262,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6jOa1jt3Te",
        "forum": "gfFVATffPd",
        "replyto": "gfFVATffPd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1031/Reviewer_KSCt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1031/Reviewer_KSCt"
        ],
        "content": {
            "summary": {
                "value": "This work proposes to use Transformers\u2019 internal attention mechanisms to understand the internal mechanism behind factual errors, in contrast to previous work focusing more on discovering internal mechanism behind generate *correct* factual answers. It poses the problem of answering factual queries as a constraint satisfaction problem whereby each factual queries presents a conjunction of constraints (e.g. \u201cis a basketball player\u201d and \u201cwas born in 1988\u201d) and the query is answered correctly if each constraint is satisfied. The paper begins with a series of analyses linking attention on the constraint tokens with factuality, then proposes the SAT probe, a linear classifier over the attention on the constraint tokens, trained to predict when all constraints will be satisfied (and thus when the LLM will make factually correct predictions)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Casting factual queries as a constraint satisfaction problem is an interesting approach, and the specifically the finding that attention on the constraint tokens is correlated/predictive of factual correctness is novel.\n\n2. Compared to confidence-based classification, the SAT probe is more efficient \u2014 allowing the model to not have to go through the entire inference procedure \u2014 and fine-grained \u2014 allowing us to isolate of which exact constraint was disregarded.\n\n3. The experiments covered many different datasets and a wide range of model sizes \u2014 and the findings generally carried over between the various model sizes and datasets. The wealth of empirical findings is insightful and convincing."
            },
            "weaknesses": {
                "value": "1. My main point of contention is that the SAT probe actually underperforms confidence classification by a statistically significant margin in 17/27 of the settings studied in in Figure 6 (it would be good to note what type of error bars are being plotted and how many trials this was over/what the source of randomness was). This seems to undermine \u201cwe find that SAT PROBE mostly performs comparably to and sometimes better than the model\u2019s CONFIDENCE in the correctness prediction task\u201d (section 5.1, results).\n    1. Section 5.2 and appendix dive into results that demonstrate that analyzing attention at earlier layers can be useful \u2014 as it is generally more efficient and allows us to interpretably isolate the constraint that failed. This should be moved up and highlighted as one of the main advantages of this method. (The preliminary analyses on popularity and constrainedness predicting performance / attention predicting popularity and constrainedness can be compressed.)\n    2. Table 7 examines the ability of SAT probe to isolate *which* constraint was violated when there are multiple constraints. Some more detail is necessary here: in the dataset, when 1 constraint is violated, how often is it the case that both constraints are violated? Can we isolate the experiment to cases where one and only one of the constraints are violated, and have the SAT probe predict which one?\n\n2. I do wonder about the generality of thinking of factuality as a CSP \u2014 in particular simply treating it as a conjunction of constraints. There are many other types of composition, such as disjunction, nesting (multi-hop queries like \u201cparents of the President of the USA\u201d), etc., that cannot be dealt with in this framework. Furthermore, not are all factual queries can be dealt with just simply using constraints. For example, there may be additional reasoning operations on top of constraints (e.g. numerical counting). While I don\u2019t expect this one paper to comprehensively deal with all types of factual queries, perhaps some discussion of the coverage of this framework is warranted.\n    1. More generally, prior work has already examined factual queries from the lens of database (SQL) queries, which implicitly already carries constraints (i.e. the WHERE clauses) while also being more flexible to different types of constraint compositions, and many more operations. What is the comparative advantage to thinking of factual queries as conjunctions of constraints in this way?\n\n3. Furthermore, the name \u201cconstraint satisfaction problems\u201d generally suggests a different set of problems to me \u2014 where the key focus is not on evaluating the factuality of each atomic variables, but on how we may search through assignments to variables in order to create a permissible solution. (This is a relatively minor naming point, though does open up a potential question on the implication of thinking of queries as a constraint satisfaction problem, in the more generic sense, which could introduce much more novelty to this paper.)"
            },
            "questions": {
                "value": "1. Are there any takeaways from this analysis and method for how we can actually fix factual errors? Or perhaps prevent models from generating factual errors?\n\n2. This method mainly relies on LM uncertainty to pick up when the LM is wrong, which is fine (I\u2019m not sure if there\u2019s any other way to internally discover when the LM is lying without external tools). However, broadly speaking, are there ever cases of the LM being confidently wrong? Or is it the case that, because of the distribution of the pre-training data, whenever there\u2019s factual errors it will always be much less certain than when there\u2019s correct answers?\n\n3. Section 5.2, Early Stopping: \u201cFor Llama-2 7B and 13B, we can stop the inference early without degradation in the average performance and save 50% of wall-clock time on failures for most datasets.\u201d Is there a breakdown of wall-clock time for each of the datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1031/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1031/Reviewer_KSCt",
                    "ICLR.cc/2024/Conference/Submission1031/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1031/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699391524302,
        "cdate": 1699391524302,
        "tmdate": 1700495045115,
        "mdate": 1700495045115,
        "license": "CC BY 4.0",
        "version": 2
    }
]