[
    {
        "id": "UHx0sCzp6g",
        "forum": "oKglS1cFdb",
        "replyto": "oKglS1cFdb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1827/Reviewer_jEjy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1827/Reviewer_jEjy"
        ],
        "content": {
            "summary": {
                "value": "The paper considers the question of whether it is feasible to learn good representations for OOD generalization with only ID data, without considering inductive biases of the architecture and learning algorithm. First, the paper looks at an experiment where models are trained to learn the features of pretrained models that exhibit good OOD performance. It is found that these distilled models have OOD performance better than standard models only trained on the ImageNet training set, but not as good as the original pretrained models, suggesting that it is not possible to learn good OOD representations from ID data, even given access to \u201coracle\u201d representations known to perform well OOD. Via theoretical analysis of 2-layer ReLU networks, the paper then unveils a novel failure mode of OOD generalization called feature accompaniment. This failure mode is shown theoretically to stem from inductive biases of nonlinear networks, and is absent in deep linear models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper identifies a novel and intuitive failure mode of out-of-distribution generalization, distinct from the prevailing attention on spurious correlations\n* The paper provides principled theoretical foundations that prove the existence of this failure mode for 2-layer\n* The takeaways of the study are applicable to future theoretical study of OOD generalization. In particular, the paper attempts to make the highly of-interest case that existing theoretical models of OOD generalization may not cover why OOD generalization failure happens in practice."
            },
            "weaknesses": {
                "value": "* The claim in Section 2 of the existence of an OOD generalization failure mode beyond the reach of generalization theory, and related to feature learning may not be fully justified by the empirical results in the section. Please see Question 1 below.\n* The study does not suggest how one might make the findings actionable in an empirical setting to improve or predict OOD generalization ability. It is thus unclear how significant the results or the identified failure mode are.\n* Relatedly, the paper does not make a case for how much OOD generalization failure is attributable to this failure mode in empirical settings, if any at all."
            },
            "questions": {
                "value": "Question 1:\nI am unconvinced that the empirical results of section 2 imply the existence of a failure mode related to the feature learning process. The empirical result may not necessarily be due to nonlinear feature-learning dynamics in this experiment, but rather just that pre-trained CLIP models contain features covering a much larger data distribution than is captured by models distilled on ImageNet. In particular, if you distill a model from CLIP on the ImageNet training set, are some CLIP features that are not represented in ImageNet not likely to be left out? These features could still be helpful in OOD classification. For example, the OOD bird and car could have core features that look different from the core features seen in-distribution. Pretrained models may contain these features, while distilled models may not have learned them if they do not appear in the training set.\n\n\nQuestion 2:\nCould there be discussion on how feature accompaniment relates to the previous studies on simplicity bias and gradient starvation [1,2], which find that networks rely on simple features and ignore more complex features? In particular, work on Gradient Starvation [2] suggests that an increase in strength of a simpler feature inhibits the learning of other more complex features. Are these results contradictory to those suggested by feature accompaniment?\n\n\n\n[1] The Pitfalls of Simplicity Bias in Neural Networks, https://arxiv.org/abs/2006.07710\n\n[2] Gradient Starvation: A Learning Proclivity in Neural Networks, https://arxiv.org/abs/2011.09468"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1827/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1827/Reviewer_jEjy",
                    "ICLR.cc/2024/Conference/Submission1827/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1827/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817249758,
        "cdate": 1698817249758,
        "tmdate": 1699787104572,
        "mdate": 1699787104572,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TBhqAeKcfJ",
        "forum": "oKglS1cFdb",
        "replyto": "oKglS1cFdb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1827/Reviewer_9qmH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1827/Reviewer_9qmH"
        ],
        "content": {
            "summary": {
                "value": "This paper tries to study whether it is possible to learn OOD-generalizable representations with only in-distribution data. The authors discover a new failure model that they refer to as feature accompaniment, which is caused by the inductive biases of training process of nonlinear neural networks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* This paper studies OOD from the perspective of inductive bias, which has rarely considered in existing literature. They consider the training process of neural network, which is more practical than directly considering the global minimum.\n* From their theoretical analyses, they find an interesting failure mode ''feature accompaniment''. In my understanding, this means that due to the asymmetry of activation, each neuron tends to correlate more with one class than another. Then, this can further make the projection of gradients onto background features non-zero, which makes the final model also use background features to classify. I think this ''feature accompaniment'' may be a very fundamental phenomenon caused by the asymmetry of activation, which can also be used to understand other properties of neural network. I think my own research can draw some inspiration from it.\n\nI hope to obtain more insights from upcoming discussions with the authors and I'm happy to further raise my score."
            },
            "weaknesses": {
                "value": "* I think the experiment part in Section 2 is a bit disconnected from the theoretical part in Section 4. They consider different settings and different learning objective. I don't think theory in Section 4 can explain the experimental results in Section 2. I know that Section 2 is probably just a starting point of studying whether OOD-generalizable representations are learnable, so this's okay. But I think it could be better to connect them more in the writing. \n* The training process the authors mainly studied is based on ERM with regularization. Since there is only one training domain, this is okay. But I want to know if there are several domains whose background distributions are different, will the training process of Equation (1) (instead of ERM) still cause \"feature accompaniment\"? Are there any cases even if we have multiple domains we can still not learn a OOD-generalizable neural network?"
            },
            "questions": {
                "value": "Please see Cons."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1827/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698901745815,
        "cdate": 1698901745815,
        "tmdate": 1699636112030,
        "mdate": 1699636112030,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bssFR96CEv",
        "forum": "oKglS1cFdb",
        "replyto": "oKglS1cFdb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1827/Reviewer_Lh7e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1827/Reviewer_Lh7e"
        ],
        "content": {
            "summary": {
                "value": "This work tries to answer \"Can we learn OOD generalizable representations from in-distribution data?\" empirically and theoretically. \n\nIn the empirical part, the term **OOD generalizable representation\" mainly indicates a representation that contains rich features. The author investigates the OOD linear probing performance of three kinds of pretrained models: 1) a CLIP pretrained model on super large&diverse dataset; 2) a supervised pretrained model on Imagenet dataset; 3) a supervised pretrained model on Imagenet dataset with more objective information (i.e. prediction the representation of a CLIP model). \n\nThe author treats the 3rd model as the oracle objective function --- *\"representation learning objective itself cannot be further improved in general\"*. Hence concludes that *\"OOD generalizable representations may not be learnable using only ID data without explicitly taking into account the inductive biases of the model or the task.\"*\n\nIn the theoretical part, however, the term **OOD generalizable representation** changes to indicate \"a representation that doesn't contain spurious signals (or background feature signals) and only contains invariant signals (or core feature signals). The author uses a 2-layers Relu network to show that --- a non-convex network (especially with asymmetric activations) could \"learn and store\" some background feature signals in the representation even though these background features have no correlation with the target label."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- It is interesting to investigate out-of-distribution generalization problem through the rich-representation (a representation contains a rich set of features that could be redundant in-distribution but crucial out-of-distributation) point of view.  \n\n- It is also interesting to show that a non-convex network could \"learn and store\" some irrelative signals (per-example level spurious features) in the representation even though these signals are not (or weakly) correlated with the target label in the whole-dataset level."
            },
            "weaknesses": {
                "value": "- As I commented in the **Summary**, the empirical part and theoretical part use different principles. So that they can not support each other. Please check **Summary** for details. In my opinion, that is the biggest weakness.\n\n- In the empirical part, this work treats \"good OOD linear probing performance\" as \"good generalization representation\" (Figure 1). The principle here is \"rich-representation\"[1][2]. I suggest the author clarify the principle. \n\n- The author treats -- a supervised pretrained model on Imagenet dataset with more objective information (i.e. prediction the representation of a CLIP model) -- as the **oracle** objective function. By comparing this model (pretrained on Imagenet) with CLIP (pretrained on a large dataset), the author concludes that *\"OOD generalizable representations may not be learnable using only ID data without explicitly taking into account the inductive biases of the model or the task.\"* \n\nOn one hand, this comparison doesn't support the conclusion. From the rich-representation's principle (which is used in the linear probing experiment), OOD linear probing benefits from a representation that contains diverse and simple features. Indeed, CLIP (pretrained on a large dataset) contains rich features. But please remember that CLIP uses more data. It is possible that the model  above (pretrained on Imagenet) is already the best (by say \"best\", I mean a model that achieves the best OOD linear probing performance) imagenet pretraining model. In short, CLIP model (pretrained on a large dataset) should not be assumed as an achievable upper bound of other Imagenet pretrained model. \n\nOn the other hand, this Imagenet pretrained model is not **oracle** in terms of rich-representation. Compared with Imagenet's 1k target categories, indeed this object contains more supervision information (with the help of CLIP and CLIP's pretraining dataset). But how about 22k target categories, for example? \n\n- The theoretical section didn't discuss the relationship between works about SGD and features, e.g. [3][4][5].\n\n[1] Zhang, J., Lopez-Paz, D., & Bottou, L. (2022, June). Rich feature construction for the optimization-generalization dilemma. In International Conference on Machine Learning (pp. 26397-26411). PMLR.\n[2] Zhang, J., & Bottou, L. (2023, July). Learning useful representations for shifting tasks and distributions. In International Conference on \n[3]Andriushchenko, M., Varre, A. V., Pillaud-Vivien, L., & Flammarion, N. (2023, July). Sgd with large step sizes learns sparse features. In International Conference on Machine Learning (pp. 903-925). PMLR.\n[4]Blanc, G., Gupta, N., Valiant, G., & Valiant, P. (2020, July). Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process. In Conference on learning theory (pp. 483-513). PMLR.\n[5]Pezeshki, M., Kaba, O., Bengio, Y., Courville, A. C., Precup, D., & Lajoie, G. (2021). Gradient starvation: A learning proclivity in neural networks. Advances in Neural Information Processing Systems, 34, 1256-1272."
            },
            "questions": {
                "value": "- please see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1827/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1827/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1827/Reviewer_Lh7e"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1827/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699651416290,
        "cdate": 1699651416290,
        "tmdate": 1699651416290,
        "mdate": 1699651416290,
        "license": "CC BY 4.0",
        "version": 2
    }
]