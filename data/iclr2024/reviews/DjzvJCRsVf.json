[
    {
        "id": "ksW67CfnM0",
        "forum": "DjzvJCRsVf",
        "replyto": "DjzvJCRsVf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5422/Reviewer_xSsC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5422/Reviewer_xSsC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes CLIPSelf, which is a method to adapt ViT based CLIP model trained on full images to dense prediction tasks. CLIPSelf distill a student ViT model that has good region representation from its dense feature map from a teacher ViT model that only has good full image features. Specially, CLIPSelf aligns the dense feature maps with the teacher model at crops of an image. \n\nThe proposed methods show good performance in standard benchmarks for open-vocabulary object detection, semantic segmentation and panoptic segment. It is reported to be stronger in the main metrics of the respective benchmarks to be stronger than many recent works, under the same or similar settings.\n\nOne interesting advantage of this work is that it does not need region-text pairs for training per se, unlike some recent methods such as RegionCLIP. Nevertheless, combining CLIPSelf (as pretraining step) with RegionCILP (region-text pairs for finetuning) results in further improvements for open-vocabulary detection on OV-COCO."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The main strengths of the paper are\n\n* The paper is clearly written. It has sufficient details in the main text to reproduce the method.\n* The motivation of the paper is clearly presented. The paper includes an interesting analysis on the zero-shot image classification capability using image feature and region features, using CLIP ViT and CLIP CNN, respectively. The analysis shows that CLIP ViT has much better image features for zero-shot classification, but much worse dense features for the same task. This suggests that CLIP ViT has the potential to be much stronger than its CNN counterpart, but requires additional work to adapt the full image features to dense features.\n* The proposed CLIPSelf is fairly simple yet seems to be quite effective in improving the dense feature of CLIP models. This is evident in the various results on standard benchmarks from Table 3-7. The proposed CLIPSelf method seems to be among the strongest methods in standard settings in the listed benchmarks, and can improve existing methods when the CLIP pre-trained with the CLIPSelf target replaces vanilla CLIP models.\n* The details in the design of CLIPSelf have been tested carefully by ablation studies. For example, the design to use randomly sampled patches in distillation is validated by the results of classifying stuff masks."
            },
            "weaknesses": {
                "value": "The main weaknesses of the paper are\n\n* This paper does not include any studies on windowed attention based methods, such as Swin V2. This seems to have weakened the case for CLIPSelf, due to following reasons.\n\n  * In a sense, the analysis that compares CILP ViT and CLIP CNN can also be interpreted as \"CNN is more suited for dense prediction tasks\". Recent empirical studies do overwhelmingly show that transformer based model outperforms CNN based models in many computer vision tasks, including dense prediction tasks. From the analysis, it appears that CILPSelf is most helpful with backbones that does not have translation equivariance built-in. But many (if not most) best models for dense predictions tasks still use backbones that have this useful properties for dense predictions (e.g. Swin V2). In fact, that is the choice for some of the best recent open-vocabulary detectors, such as GLIP (v2) and grounding DINO. The choice to only study ViT backbones leave two obvious questions open: (1) Can CLIPSelf similarly benefit Swin-based backbones? (2) How does the current method compare to SOTA methods based on Swin v2, such as GLIP and grounding DINO? \n  * The square focus on ViT backbones also makes it harder, in certain cases presented in the experiments section, to interpret the comparison with prior methods. For example, in Table 3, CLIPSelf uses either ViT-B/16 or ViT-L/14 backbones. However, most of the cited methods are either from CNN backbone or Swin V2. In a few instances where the prior methods are based on ViT, the particular ViT variant is different from the two with CILPSelf."
            },
            "questions": {
                "value": "- I would like to see comparisons with GLIP, GLIP v2 and Grounding DINO.\n- I would like to see if the proposed method can only improve on ViT based backbones."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5422/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5422/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5422/Reviewer_xSsC"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5422/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698646761534,
        "cdate": 1698646761534,
        "tmdate": 1700717214992,
        "mdate": 1700717214992,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MAw5ItcqbI",
        "forum": "DjzvJCRsVf",
        "replyto": "DjzvJCRsVf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5422/Reviewer_CnGz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5422/Reviewer_CnGz"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a simple approach to improve the transferability of CLIP ViT models to open-vocabulary object detection and image segmentation.\nWhile CLIP ViT shows strong performance in image classification (as compared to CLIP ConvNets),\nits region-level representation is known to underperform in dense prediction tasks.\nThe authors first demonstrate this shortcoming through a simple experiment,\nthen propose a self-distillation approach named CLIPSelf to remedy this.\nIn their experiments, the authors apply the CLIPSelf procedure to fine-tune the CLIP ViT checkpoints made public by OpenAI,\nand then use the fine-tuned weights for open-vocabulary object detection and image segmentation.\nEmpirical results show that visual representations from CLIPSelf are better than CLIP for dense prediction tasks,\nand CLIP ViTs fine-tuned by CLIPSelf procedure are highly competitive on open-vocabulary detection benchmarks like COCO and LVIS."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Note:**\nI reviewed this paper for the NeurIPS 2023 conference.\nI will contextualize my current review based on the changes the authors have made since their previous submission.\nThe main idea of CLIPSelf has been unchanged since NeurIPS 2023 submission --\nthe authors have significantly revamped the experimental study with more suitable benchmarks and baselines,\nand added more implementation details in the draft.\n\nI identified some key strengths of the proposed approach in the NeurIPS 2023 submission, they still apply to the current submission:\n\n- The proposed approach (CLIPSelf) is conceptually simple and shows large improvements over CLIP on multiple evaluations shown in the paper.\n- CLIPSelf does not require any form of annotations beyond just images, this relaxation simplifies the data and modeling pipeline.\n- Authors have performed comprehensive evaluations with multiple downstream tasks involving dense predictions to show the versatility of the proposed approach.\n\nI find notable other strengths in the current submission:\n\n- Empirical results on open-vocabulary detection are quite strong. CLIPSelf outperforms prior works that use equivalent frozen backbones (e.g. F-VLM, CORA).\n- More notably, CLIPSelf outperforms the recent work of RO-ViT which proposes training a new backbone catered to downstream task _from scratch_.\n  I find this result particularly appealing as it shows that CLIPSelf can lightly adapt a wide range of publicly available checkpoints that were trained with relatively less consideration for the particular task of open-vocabulary object detection.\n- The writing clarity has also improved in this submission."
            },
            "weaknesses": {
                "value": "My review mainly contrasts the previous submission (NeurIPS 2023) with the current ICLR 2024 submission --\nsince the main story of the paper is mostly unchanged, I do not find any significant outstanding concerns.\n\nI have a few outstanding concerns and suggestions, which may further improve the paper.\nI am curious to hear the authors' thoughts on these and am willing to engage in the discussion:\n\n1. **Is self-distillation on task dataset (COCO) necessary?**\nMain CLIPSelf experiments use images from COCO/LVIS dataset to perform self-distillation.\nThe authors should consider disentangling the impact of self-distillation in isolation,\nas compared to self-distillation using images from the target downstream task.\nIf CLIPSelf does not _require_ COCO images, then it strengthens the contribution --\nthe authors may call for future works to \"fine-tune\" their CLIP ViT for an additional epoch to improve their out-of-the-box usability for dense prediction tasks.\nIt would be even better if CLIP ViTs retain their original zero-shot classification and retrieval performance after such self-distillation.\nIf the authors wish to do this experiment in the rebuttal, they may consider a large dataset of diverse unlabeled images\n(e.g. images from datasets with approx. 10-20 million samples like Conceptual Captions, RedCaps, YFCC-15M, Segment Anything dataset, etc.)\n\n2. **Accounting for the 'padding' patches during distillation.**\nCurrent state-of-the-art object detectors using plain ViTs (e.g. ViTDet and SAM) typically pad input images to a square.\nAny non-square input images are made square by adding zero pixels to the right and bottom after color normalization.\nPre-trained CLIP ViTs have not seen such patches so they encounter a train-test mismatch if they are kept frozen in the object detector.\nHave the authors considered randomly adding padding regions to image patches while performing self-distillation?\nI believe that accounting for this detail will ease the adaptation of CLIP ViTs for open-vocabulary object detection.\n\n**Summary:**\nThe authors have diligently incorporated the reviewer feedback from the previous conference submission along with other significant changes.\nIn my opinion, these changes have strengthened the technical contributions and improved the presentation of this paper.\nI have a few other outstanding concerns but the technical strengths already outweigh them.\nI believe this paper is ready to be a part of the published literature -- I am leaning towards an acceptance."
            },
            "questions": {
                "value": "Minor questions and suggestions:\n\n- Figure 3 (b) non-standard word use: \"initiate\" -> \"initialize\"\n- Figure A2 caption is incorrect: \"image segmentation\" -> \"object detection\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The broader impact statement is short and concise, but I believe it is sufficient."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5422/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698876711298,
        "cdate": 1698876711298,
        "tmdate": 1699636550688,
        "mdate": 1699636550688,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OMp55GZRz4",
        "forum": "DjzvJCRsVf",
        "replyto": "DjzvJCRsVf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5422/Reviewer_BRE4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5422/Reviewer_BRE4"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors proposed a simple yet effective method for learning fine-grained visual-language representations for open-vocabulary detection and segmentation tasks. Given that the original CLIP-based VIT models have a great degradation of performance for zero-shot fine-grained image understanding, the authors explored a cheap way of refining the visual representation from the original CLIP vision encoder by distilling the image-patch representation to regional dense feature representations. Through such a simple strategy, the region-level visual-semantic representation ability is immediately unleashed. According to the extensive evaluations of different tasks and settings, the refined representations significantly outperform the original ones and other state-of-the-art methods. Overall, I think the proposed method called CLIPSelf is a very simple yet effective way to refine the existing models, which could be a good way of learning better representations for open-vocabulary tasks under low budgets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method CLIPSelf, distilling CLIP models for fine-grained representation is simple yet effective. It is motivated by an interesting observation in the study of CLIP representation for image-level tasks to region-level tasks. The author also observed that cropping image patches significantly outperform feature pooling for ViT vision encoders. As a result, the authors proposed to distill the fine-grained understanding of visual content in image patches to the regional representations in the hidden feature space.\n\n2. The authors conducted extensive experiments to validate the effectiveness of the proposed methods, spanning from open-vocabulary object detection to semantic segmentation and then panoptic segmentation. It shows that the refined visual-semantic representations by CLIPSelf bring significant gain over the baseline model and also outperform strong previous works."
            },
            "weaknesses": {
                "value": "1. There are some related works that already explored the way of distilling patch-level representations to regional feature-level representations, such as ZeroSeg proposed in \"Exploring Open-Vocabulary Semantic Segmentation without Human Labels. Chen et al. ICCV 2023\". The overall methodology proposed in this work resembles the one in ZeroSeg, in that both attempted to distill the CLIP knowledge from patch to feature map and for open-vocabulary segmentation tasks.\n\n2. It turns out that the authors only used the COCO training set for distillation. A doubt on this setting is that it is probably very favorable to downstream tasks like COCO detection and segmentation, and ADE20K as well. Though this in-domain distillation brings a significant boost, the authors should have more studies and evaluations on more downstream datasets from various domains and also try using non-COCO training sets for distillation, which seems to be more realistic."
            },
            "questions": {
                "value": "Overall,I have two main questions:\n\n1. How to distinguish this work from ZeroSeg regarding the core learning algorithm?\n\n2. Are the improvements over CLIP baselines sourced from in-domain training? What if we use another dataset like CC3M for the distillation?\n\nOne minor question:\n\nHow to adapt the VIT models to different input image resolutions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5422/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698901916187,
        "cdate": 1698901916187,
        "tmdate": 1699636550591,
        "mdate": 1699636550591,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rOrBXuzPY7",
        "forum": "DjzvJCRsVf",
        "replyto": "DjzvJCRsVf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5422/Reviewer_yRsy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5422/Reviewer_yRsy"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed to address the problem of adapting vision Transformers pre-trained with image-language alignment (e.g., CLIP) for dense prediction tasks (e.g., object detection, semantic segmentation). The key idea is to distill features from the pre-trained Transformer model itself, in which region representation from a dense feature map matches those from cropped regions (using the pre-trained model). The proposed method was evaluated on multiple open-vocabulary object detection and semantic segmentation benchmarks. The results are solid."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper addressed an important and trendy topic on adapting large-scale pre-trained vision-language models (i.e., foundation models). \n\n* The paper is well written. Key concepts are clearly described. Technical details are easy to follow. \n\n* The core idea is well motivated and quite intuitive. \n\n* The experiments are extensive (three open-vocabulary dense prediction tasks across multiple datasets + ablations). The results are impressive."
            },
            "weaknesses": {
                "value": "It is somewhat unclear why the proposed method, which is conceptually simple, can work well. My bet is that the proposed self-distillation encourages localized visual features, i.e., features on the dense map capture local region representations (instead of global image representations). This is partially backed up by Fig 1 (b). The gap between dense features and cropped image features is negligible for CNN (where the features are localized), yet rather significant for ViT. Part of the reason lies in the design of ViT using global attention, where the dense features are diffused in a way that each spot encodes a global image representation. There might be ways to validate this explanation. For example, one possibility is a retrieval experiment using features on the dense maps. Features on dense maps from pre-trained CLIP ViT might yield similar images instead of similar regions, while features after distillation may better retrieve regions. It will be great if the authors can comment on this reasoning here and discuss the key intuition in the paper."
            },
            "questions": {
                "value": "If the explanation described in the weaknesses section is true or partially true, it brings two additional questions. \n\n* First, do those new localized representations (after distillation) still preserve the zero-shot recognition capacity of CLIP? One interesting experiment is to evaluate those features directly for zero-shot object detection / segmentation (e.g., see Table 4 in RegionCLIP). \n\n* Second, is the proposed method a remedy to a particular design (ViT with global self-attention)? This question is perhaps more fundamental. What if a CLIP model is trained using ViT with local self-attention (e.g., using Swin Transformers)? Will they still benefit from the method? This is much harder to validate directly, as there is no pre-trained models available. Yet one possible alternative is to modify the pre-trained ViT by replacing global attention with local ones, and then train for those open-vocabulary tasks. (Indeed, MaskCLIP can be considered as a special case of this modification, where the global attention at the end is replaced by a local attention of size 1x1.) It will be interesting to see how the proposed method compares to this alternative."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5422/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699515941623,
        "cdate": 1699515941623,
        "tmdate": 1699636550483,
        "mdate": 1699636550483,
        "license": "CC BY 4.0",
        "version": 2
    }
]