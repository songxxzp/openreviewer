[
    {
        "id": "Z2BMAvkHTU",
        "forum": "AssIuHnmHX",
        "replyto": "AssIuHnmHX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6895/Reviewer_LyN1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6895/Reviewer_LyN1"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the standard decoder-only transformers model for algorithmic tasks. Prior work has shown that standard transformers exhibit little length generalization on certain arithmetic tasks. Accordingly, the author first propose the RASP Conjecture to describe the kind of problems on which the transformers can perform length generalizations. The author also present empirical evidence to support the RASP Conjecture and length generalization. Finally, the author analyzes the hard tasks for transformers, such as parity and decimal addition, and proposes particular scratchpad formats to enhance the length generalization on these tasks.\n\n**Post rebuttal update**\n\nHaving reviewed the author's response, I am impressed by the meticulous effort the authors have done in answering questions and refining the manuscript. The work has effectively addressed my initial concerns. As a result, I have raised my point."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper is well-written: discussions and presentations are clear. \n\n* The RASP Conjecture is well-motivated.\n\n* The case study clearly explains why certain tasks are solvable (i.e., length generalizable) and why others aren't. For each solvable task, there is a corresponding RASP format.\n\n* The author designs particular scratchpad formats for parity and decimal addition and experimentally verifies the effectiveness of these designs."
            },
            "weaknesses": {
                "value": "* The scratchpad designs for parity and decimal addition, though effective, are a bit different from the RASP format. For example, it seems that the proposed scratchpad for addition is motivated by correcting the failure modes discussed in section 4.3. It would be nice if the author could clarify the connection between the scratchpad designs and the RASP. \n\n* This paper classifies the algorithmic tasks into easy and hard ones. The easy ones are solvable (i.e., length generalizable) by the standard transformers. As a general question, can the author recommend the possible directions to solve the hard ones? The proposed scratchpad solution requires a detailed understanding of the failure modes for each hard task, which doesn't seem quite scalable.\n\n* Missing citations. Transformers for algorithmic tasks is an interesting topic in the recent literature. It would be nice if the authors could discuss a few more papers. From the modeling perspective, [1] introduces certain twists on the transformer architecture to enable the length generalization on algorithmic/regular language tasks such as Even Pairs, Modular Arithmetic, Parity Check, and Cycle Navigation (i.e., the tasks in [2]). From the task perspective, [3] evaluates the model performance over randomly generated finite-state transduction tasks. The idea of evaluating over randomly generated tasks is complementary to the usual conduct of evaluating on particular tasks.\n\n\n[1] Chi, T.C., Fan, T.H., Rudnicky, A.I. and Ramadge, P.J., 2023. Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation. arXiv preprint arXiv:2305.03796.\n\n[2] Del\u00e9tang, G., Ruoss, A., Grau-Moya, J., Genewein, T., Wenliang, L.K., Catt, E., Cundy, C., Hutter, M., Legg, S., Veness, J. and Ortega, P.A., 2022. Neural networks and the chomsky hierarchy. arXiv preprint arXiv:2207.02098.\n\n[3] Valvoda, J., Saphra, N., Rawski, J., Williams, A. and Cotterell, R., 2022, October. Benchmarking compositionality with formal languages. In Proceedings of the 29th International Conference on Computational Linguistics (pp. 6007-6018)."
            },
            "questions": {
                "value": "See the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6895/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6895/Reviewer_LyN1",
                    "ICLR.cc/2024/Conference/Submission6895/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6895/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698011343007,
        "cdate": 1698011343007,
        "tmdate": 1700633194083,
        "mdate": 1700633194083,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "634UeSHVkO",
        "forum": "AssIuHnmHX",
        "replyto": "AssIuHnmHX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6895/Reviewer_q9T5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6895/Reviewer_q9T5"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses an important problem: in which algorithmic task transformers are more likely to length generalize in which format. \n\nThey propose a conjecture: transformers are likely to length generalize for a task if there exists a \"simple\" \"RASP-L\" program to solve that task. (The conjecture includes other conditions to ensure almost perfect in-distribution performances.) (The \"RASP\" programs are easier to represent and learn by transformers in the sense that their program parameterizations encourage parallel operators and discourage sequential, for-loop, and branch operations. The \"RASP-L\" programs further rule out numerically unstable operations such as they forbid float values and complex token indices processing.)\n\nThe authors also provide some experimental results that empirically align with the conjecture."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper discusses an important topic: the length generalization of transformers. It is easy to understand with many illustrative visualizations. They test the conjecture in six simple tasks whose positive/negative results all align with the conjecture. They also show improved/degraded length generalization performances as guided by the conjecture after reformulating some tasks. \n\nRegarding the concrete conjecture, at least some parts of it align with the reviewer's intuition and experiences. For example,\n* If human beings can implement a simple RASP-L program to solve a task, we do expect better length generalization of learned transformers.\n* Transformers do suffer from numerical issues in practice, either for counting tokens or modeling complex indexing mechanisms. The constraints on \"learnable\" in addition to \"RASP\" are interesting. \n\nThe reviewer also appreciates the simple trick on position embeddings for better length generalization: they concatenate all samples in a long sequence with random shifting while training transformers. This is different from the common choice, which only puts one sample in one sequence, for synthetic tasks. They find that \"packing and shifting the context allows all positional embeddings to be trained, and encourages the transformer to treat all positions symmetrically,\" therefore, towards better length generalization of transformers."
            },
            "weaknesses": {
                "value": "The main weakness is lacking the support and validation of the conjecture. The definition of RASP-L programs is also vague and unclear.\n\nRegarding the support and validation of the conjecture: \n* There is no proof or reasoning for the conjecture. There is little evidence saying that transformers actually learned or have any relationships with the \"RASP-L\" program in general. According to the experimental results where \"transformers mostly generalize to length at most 50 when trained with length<=40\", it is also hard to believe that the true \"RASP-L\" programs have been learned. \n* The experimental results and comparison may not be solid with confusing metrics:\n   - The metrics should characterize the differences between OOD and IID, instead of the absolute performances on OOD samples. As shown in Figure 1(a), generalization is about the difference in performances between in-distribution and out-of-the-distribution samples. This is important for, e.g., the addition task, because addition with 40 digits may already be out of the representation power of a 6-layer transformer in the naive forward-order. The IID performances could be as bad as the OOD ones. \n  - There is also a lack of constraints on achieving almost perfect in-distribution performances before evaluating their length generalization. \"Almost perfect IID performance\" is assumed in the conjecture. It is also strange to evaluate the OOD performances of random runs with imperfect IID performances. If this constraint is not enforced, the current analysis may confuse the optimization difficulties with the OOD generalization performances. \n   - Similarly, the metric as \"median among 20 runs\" is not enough to fully characterize the transformers' performance, given their non-convex optimization nature. Other metrics, such as \"maximum\" or \"ratio of almost perfect,\" could be more informative and practical. \n  - The tasks are simple: from count to addition/parity. \n* Ablation studies on \"without the index hints\" are needed. \n\nRegarding the format of the conjecture as \"RASP-L\" programs,\n* The learnable constraints are interesting, but there are no reasoning/proof/experiments to support their choices, such as which operators are \"learnable\" in which sense, how to find and choose them, and if the current set of operators can represent all \"learnable\" operations.\n* Similarly, there is no characterization of the function space represented by the \"RASP-L\" programs. \n* The optimization efficiency conjecture in the appendix is also poorly supported. I would suggest the authors remove it if there are no supporting results from more tasks."
            },
            "questions": {
                "value": "* How did you choose the \"learnable\" operators? Will the representation power of \"RASP\" programs be affected by this additional constraint? Can all \"generalizable\" tasks' solutions be represented by the \"RASP-L\" programs? \n* Are there results on the difference between the IID and OOD performances for most of the tasks?\n* Are there constraints as almost perfect IID performances before evaluating their length generalization performance? Are there results with metrics such as maximum among all runs?\n* Did you achieve almost perfect IID performances for the addition task with 40 digits with the naive forward order? \n* Can you explain why the length generalization performance of transformers is still limited (e.g., from length<=40 to length=50) even when the conjecture conditions are fulfilled?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6895/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6895/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6895/Reviewer_q9T5"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6895/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698615273049,
        "cdate": 1698615273049,
        "tmdate": 1700707354872,
        "mdate": 1700707354872,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cJxaLFHVsq",
        "forum": "AssIuHnmHX",
        "replyto": "AssIuHnmHX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6895/Reviewer_tXQ4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6895/Reviewer_tXQ4"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a conjecture about when a decoder-only autoregressive Transformer is likely to have strong length generalization ability on symbolic algorithmic tasks. There are three conditions: 1) the true next token function for the task is realizable with a *causal Transformer*; 2) the next token function can be represented by RASP-L (a subset of the RASP language) programs; 3) the training data is sufficiently diverse, which makes sure that the shortest RASP-L program can length-generalize. The conjecture states that Transformer is likely to generalize when the three conditions are satisfied. It is shown that the conjecture is supported by some case studies. On 3 tasks which have simple RASP-L programs, Transformer empirically generalize well with a moderate max train length (e.g., >30 on the *copy with unique tokens* task). On 3 tasks that \"do not admit\" simple RASP-L solutions, Transformers struggle to generalize. The conjecture is then applied to interpret why scratchpads can improve generalization. Specifically, on a case where the scratchpad increases the complexity of the RASP-L program, Transformers show decreased generalization performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- A novel approach for understanding the reasoning and out-of-distribution generalization ability of Transformers on the symbolic tasks.\n- The analysis in the case studies is well supported by empirical results.\n- Experiment details are well introduced."
            },
            "weaknesses": {
                "value": "- Some concepts are not clearly defined, making their meanings obscure and the statements less rigorous. For example, there is no reference or definition for the \"causal Transformer\" and \"causal masking\"; how to identify whether a RASP-L program is \"simple\" or \"difficult\".\n- The correlation between RASP-L representable and the generalization of Transformer need further discussions. Are the selected cases representative enough? Is there any task which can be represented by \nRASP-L but Transformers hard to generalize, or vise-versa? These answers are needed to make a stronger statement on the correlation between the two.\n- No discussions on the limitations of this work."
            },
            "questions": {
                "value": "- Please refer to the weakness part.\n- Minor question: Empirical results show that the max train length has a great impact on the generalization ability of Transformers. Is the proposed method helpful in analyzing this phenomenon?\n- Minor suggestion: Some important definitions, e.g., that of RASP-L, would better to be included in the main paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6895/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6895/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6895/Reviewer_tXQ4"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6895/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773297168,
        "cdate": 1698773297168,
        "tmdate": 1700712960037,
        "mdate": 1700712960037,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LajyCQnoQG",
        "forum": "AssIuHnmHX",
        "replyto": "AssIuHnmHX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6895/Reviewer_Ho88"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6895/Reviewer_Ho88"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to predict the feasibility of length generalization by checking whether the task can be implemented by RASP-L (L for Learnable).\n\n- RASP-L is a constrained version of the RASP language, where all variables are constrained to int8. This excludes certain operations such as index arithmetic (but still allow order comparison, predecessor and successor).\n- The paper shows that the difficulty in length generalization is correlated with the difficulty in writing the RASP-L solution.\n    - Tasks that have a simple RASP-L solution: count, mode, copy.\n    - Tasks that do not have a simple RASP-L solution: addition, parity, copy with repeated tokens.\n- The paper then shows that whether scratchpad is helpful can be explained by RASP-L as well: a properly chosen scratchpad can help with performance (e.g. by making the task solvable with induction heads), but improperly chosen scratchpad, which increases the difficulty of writing in RASP-L, can hurt the performance (e.g. on the mode task).\n\n_Post rebuttal update_: I've increased my score based on the discussions and changes in the revised paper."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper analyses different factors affecting length generalization.\n- The paper provides a variety of empirical evidence."
            },
            "weaknesses": {
                "value": "- The paper studies length generalization, though it's unclear what is considered as successful length generalization in the paper. [_Update_: the author has added a clarification.]\n    - Fig 1(b) is considered as demonstrating successful length generalization, even though the performance from all curves are dropping, some even to 0.\n    - In Fig 2(a) (i.e. the mode task), the paper seems to consider a test accuracy of 50% as reasonable generalization performance, per the comment in Sec 4.3.\n- The paper proposes to use the difficulty of RASP-L programs as an indicator for length generalization performance. However, I'm not sure what this view can teach us, either theoretically or empirically. [_Update_: the authors modified Sec 5.1 significantly on implications of the RASP-L formulation.]\n    - It's unclear how to empirically verify or act upon this conjecture. It's also not clear how to convert a task into RASP-L except for applying the definition directly, i.e. checking for whether certain criteria are satisfied.\n        - For example, two criteria provided in the paper are 1) whether the operation is non-causal and 2) whether precise index arithmetic is required. Hence one could also say that these criteria are indicative of length generalization performance, without resorting to the RASP-L language.\n    - I'm not sure whether these criteria provide new insights beyond findings in the existing literature.\n        - It is well known that auto-regressive generation cannot uncover non-causal information. [_Update_: previously I misunderstood what \"non-causal\" mean; the authors have clarified this in the rebuttal.]\n        - The concern about index arithmetic is essentially a problem with approximation (e.g. precision and weight norms), which has been raised in prior work. In particular, Hahn 2020, Chiang & Cholak 2022 and Liu et al. 2023(b) discussed limitations in the attention module, and Liu et al. 2023(a) discussed limitations in MLP.\n- Some experiments have been covered in prior work.\n    - The shifted position experiments are also included in Liu et al. 2023(a) (Fig 6 & 7).\n    - The scratchpad experiments are similar to those in Liu et al. 2023(a). This paper uses a different form of scratchpad, but the generalization performance is not very good (e.g. generalizing from length 25 to length 50 gets around 0.3 accuracy), which is related to my earlier comment about what is considered as successful length generalization.\n\n*References*\n\nHahn 2020: Theoretical Limitations of Self-Attention in Neural Sequence Models\n\nChiang & Cholak 2022: Overcoming a Theoretical Limitation of Self-Attention\n\nLiu et al. 2023(a): Transformers Learn Shortcuts to Automata\n\nLiu et al. 2023(b): Exposing Attention Glitches with Flip-Flop Language Modeling\n\n_Note: the paper has cited Hahn 2020, Chiang & Cholak 2022 and Liu et al. 2023(a)._"
            },
            "questions": {
                "value": "- Page 2, \"Possible Mechanisms\": it's not true that omitting positional encodings may lead to \"arbitrary length\" generalization, due to failures in both MLP and attention (please see the 4 citations mentioned above).\n- It would be better to include discussions on prior work that relate algorithmic reasoning tasks to formal languages.\n  - For example, for the discussion towards the end Section 2, please consider relating to e.g. Merrill and Sabharwal 2022 and the notion of uniformity in computational complexity (which footnote 2 has mentioned).\n- Fig 3: How many test sequences are there for each trial?\n- Fig 6(b): this is not really about length generalization, but more a failure of optimization?\n\n\nMerrill and Sabharwal 2022: The Parallelism Tradeoff: Limitations of Log-Precision Transformers"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6895/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6895/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6895/Reviewer_Ho88"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6895/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698861711831,
        "cdate": 1698861711831,
        "tmdate": 1700669703474,
        "mdate": 1700669703474,
        "license": "CC BY 4.0",
        "version": 2
    }
]