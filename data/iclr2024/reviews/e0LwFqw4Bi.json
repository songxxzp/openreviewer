[
    {
        "id": "gAyBvZehwP",
        "forum": "e0LwFqw4Bi",
        "replyto": "e0LwFqw4Bi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1883/Reviewer_FLNN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1883/Reviewer_FLNN"
        ],
        "content": {
            "summary": {
                "value": "In this paper, authors focus on enhancing the out-of-distribution generalization performance of foundation models regardless of their architectures with proposed a novel and Unified framework for Domain Generalization (UniDG). The core idea of UniDG is to finetune models during inference stage which saves the costs of iterative training. Specifically, authors encourage models to learn the distribution of testing data in an unsupervised manner and impose a penalty regarding the updating step of model parameters. The penalty term can effectively reduce the catastrophic forgetting issue via maximally preserving the valuable knowledge in the original model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "In other words, marginal generalization is proposed to update the encoder of Test-Time Adaptation (TTA) and differentiable memory bank is proposed to refine features for DG. Experiments on five datasets such as VLCS, PACS, OfficeHome and so on demonstrate the superiority compared with SOTA methods across 12 different network architectures."
            },
            "weaknesses": {
                "value": "The structure of paper is unfitable for most ML reader\u2019s habits, especially, the part of related work should follow the introduction. There will be a better logical relationship for most ML conference paper."
            },
            "questions": {
                "value": "However, there are some questions need to be clarified from authors.\n1.\tAlthough ablation study is provided in Table4, in appendix D, what is the main reason behind introducing matrix products of learning representations, prototypes and pseudo labels ? How to understand making the memory iteration is learnable and differentiable ? \n2.\tMoreover, L_i in formular 25 makes me confused. Please give the detailed explanation of discrepancy between formular 8 and line 9 of Algorithm 1. How about the hyper paremeter lambda in formular 8 ? There should add more ablation experiments.\n3.\tIn Figure 5, please give the detailed explanation why the accuracy of \u201cDomainNeXt\u201d is lower than base when the samples are small. \n4.\tThere are many typos in manuscript, such as what does \u201cDomainNeXt\u201d mean in Figure 7 of Appendix E ? VLCS and PACS is quoted wrongly in Appendix F. Many formular typos are typed in Appendix B.2\n5.     Last but not least, in Table 5 (b) efficiency of UniDG, I'm confused how to get the wall clock time. There is nothing to be analyzed from time complexity. How did you come to learn the conclusion that the proposed UniDG is a online learning scheme ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1883/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698745354931,
        "cdate": 1698745354931,
        "tmdate": 1699636118640,
        "mdate": 1699636118640,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kVCrfpgj2E",
        "forum": "e0LwFqw4Bi",
        "replyto": "e0LwFqw4Bi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1883/Reviewer_q4ii"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1883/Reviewer_q4ii"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on improving the generalization performance of foundation models finetuned during inference. The authors propose a penalty term that helps to reduce catastrophic forgetting during test-time adaptation. In particular, the authors propose Marginal Generalization - a tradeoff b/w freezing the encoder which would lead to underfitting and updating the decoder which would lead to catastrophic forgetting. The authors demonstrate empirically consistent improvement across different backbones on DomainBed benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Propose a tradeoff b/w freezing the encoder which would lead to underfitting and updating the decoder which would lead to catastrophic forgetting.\n- Consistently improved benchmarks."
            },
            "weaknesses": {
                "value": "- Theoretical insight why marginal generalization is important for generalization in unseen domains is explained well in Appendix, but is very unclear from the text of the main paper. I think this important aspect should be better discussed in the main text.\n- Also motivation for Differentiable Memory Bank should be more clearly written."
            },
            "questions": {
                "value": "- With the current formulation of Marginal Generalization, how can you avoid catastrophic forgetting on **source** domains, when even if you impose the distance constraint on the target domain, there are no guarantees it will be still obeyed on the source domain?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1883/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840682640,
        "cdate": 1698840682640,
        "tmdate": 1699636118572,
        "mdate": 1699636118572,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZahQAUe82G",
        "forum": "e0LwFqw4Bi",
        "replyto": "e0LwFqw4Bi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1883/Reviewer_mjNn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1883/Reviewer_mjNn"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the catastrophic forgetting issue during test-time training (TTA) for domain generalization. Specifically, this paper proposes a Marginal Generalization method to update the encoder for TTA, that is, Marginal Generalization aims to let the encoder learn representations of the target data within a certain distance from the representations obtained by the initial model. To cooperate Marginal Generalization, this paper also proposes Differentiable Memory Bank to facilitate TTA. Experiments on five domain generalization benchmarks demonstrate the effectiveness of the proposed methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The catastrophic forgetting issue during TTA for domain generalization is well motivated."
            },
            "weaknesses": {
                "value": "- The discussion about related work is not sufficient. In the section of related work, this paper simply listed many related works, but didnot discusses the relation between the proposed method and the mentioned related works.\n\n- This paper is more likely to be a Test-Time Domain Adaptation work. So I think Test-Time Domain-Adaptation is more suitable in this paper rather than Domain Generalization. \n\n- I dont believe it is the first time to discuss the catastrophic forgetting issue during TTA for domain generalization. But I do not see any discussions about how to solve the catastrophic forgetting problem in the existing works, such as [1][2], to name a few.\n\n- In the experimental part, such as Table 2, this paper didnot explain why the performances with TTA are inferior to that without TTA. In Table 2, PL, PLClf, SHOT, Tent, TentBN, TentClf are all inferior to None. It is kind of weird, which needs explaination.\n\n- As I can see in this paper, catastrophic forgetting is the main problem to be solve. However, most of experiments are conducted on domain generalization benchmarks to show how well the proposed method performs on the target domains. Only a simple ablation study in Table 5 is conducted to validate that the catastrophic forgetting issue has been mitigated via the proposed method. I think the organization of the experiments is mismatched with the major motivation discussed in this paper.\n\n[1] Continual Source-Free Unsupervised Domain Adaptation.\n\n[2] CoSDA: Continual Source-Free Domain Adaptation."
            },
            "questions": {
                "value": "See the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1883/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699346294118,
        "cdate": 1699346294118,
        "tmdate": 1699636118487,
        "mdate": 1699636118487,
        "license": "CC BY 4.0",
        "version": 2
    }
]