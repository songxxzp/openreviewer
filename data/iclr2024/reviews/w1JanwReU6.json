[
    {
        "id": "VE4OmTjmat",
        "forum": "w1JanwReU6",
        "replyto": "w1JanwReU6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8612/Reviewer_NYBN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8612/Reviewer_NYBN"
        ],
        "content": {
            "summary": {
                "value": "The paper studies gender bias in language models under non-stereotypical settings. The authors construct cloze-style evaluation datasets, both out of Winobias and Winogender and additional ones using OpenAI models, filtered (and constructed) for sentences that have low word-gender associations. These word-gender associations are calculated from the PILE dataset. They find that, in 20+ models, that models have low fairness scores (e.g. log probabilities for male words is higher than female words), and that fairness scores generally do not change significantly even as the gender-word association constraints is relaxed in evaluation datasets. This is true regardless of factors such as model size and deduplication of training data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors carefully construct evaluation datasets to be gender-bias free and introduce checks for quality and cleanliness for OpenAI-generated datasets. Additionally, they conduct a detailed analysis of how fairness dataset construction affects fairness metrics."
            },
            "weaknesses": {
                "value": "While the need to test for stereotypical or harmful bias is more directly obvious, this paper would benefit from emphasizing the importance of building non-stereotypical evaluation data. Additionally, while there is discussion on model size and deduplication, it may be worth seeing how pretraining dataset statistics with respect to # female nouns with # male nouns affect fairness metrics (the hypothesis being models trained on more male nouns will have less differences in fairness as gender correlation in evaluation datasets differs)."
            },
            "questions": {
                "value": "1. Could the authors please clarify the purpose of v(w) in equation 2? PMI already takes into account p_data(w). \n2. Could the authors please elucidate on the generated dataset contents (in terms of topics of sentences generated)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8612/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8612/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8612/Reviewer_NYBN"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8612/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769864667,
        "cdate": 1698769864667,
        "tmdate": 1699637077558,
        "mdate": 1699637077558,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "psGhQfKpNv",
        "forum": "w1JanwReU6",
        "replyto": "w1JanwReU6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8612/Reviewer_Uaff"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8612/Reviewer_Uaff"
        ],
        "content": {
            "summary": {
                "value": "There is consensus in the literature that language models are\ngender-biased in stereotypical contexts. This paper investigates\nwhether gender bias persists in non-stereotypical contexts as well. By\ninvestigating the preference for one gender versus the other in\nneutral contexts, it appears that gender preference/bias manifests in\nneutral contexts as well. The same stays true when removing words that\nare highly correlated with gender from existing benchmarks (Winobias\nand Winogender) and testing on this neutral subset. The paper analyzes\na large number of models and gender-preference seems to be true across\nthe board."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "An interesting analysis of gender preference in neutral contexts\nA comprehensive analysis for many models"
            },
            "weaknesses": {
                "value": "My main concern with the analysis relates to the little correlation\nthat exists between intrinsic measures of bias and bias in downstream\ntasks. As such, I am not sure whether or how this property would\ninfluence the fairness or the potential harms in a downstream\ntask. See more details in the Questions section.\n\nWhile I find the analysis interesting, I\nam not sure whether analyzing intrinsic measures of bias is useful or\nimpactful. There have been several papers that show issues with\nintrinsic bias measures: they are not robust and there is little to no\ncorrelation with bias measured for a downstream task. I recommend some\nof the following papers:\n\nhttps://aclanthology.org/2022.trustnlp-1.7.pdf shows how simple\nrephrasing of sentences with different lexical choices but the same\nsemantic meaning lead to widely different intrinsic bias scores\n\nhttps://aclanthology.org/2021.acl-long.150.pdf shows that intrinsic\nbias measures do not correlate with bias measured at the NLP task\nlevel\n\nhttps://aclanthology.org/2022.naacl-main.122/ describes more issues\nrelated to bias metrics\n\nhttps://aclanthology.org/2021.acl-long.81/ lists several issues with\ncurrent datasets/benchmarks for bias auditing\n\nWeaknesses\n\nIt is not clear how the findings presented in the paper could be used\nin studying the fairness and potential harms of a downstream task"
            },
            "questions": {
                "value": "In the light that there is no or little correlation between intrinsic\nbias measures and bias observed in a downstream task, how do you think\nthe analysis of gender bias/preference in neutral contexts is useful\nfor understanding either fairness or potential harms in downstream\ntasks? Based on the findings in the paper, what can we conclude for\ndownstream tasks? Is there some insight that could be useful in a\ndownstream task?\n\nI think a similar analysis could be performed in a downstream task by\neither studying the behavior of counterfactuals or by transforming the\nsentences in a task to be gender-neutral."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8612/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796598428,
        "cdate": 1698796598428,
        "tmdate": 1699637077435,
        "mdate": 1699637077435,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vdCvfl8Xew",
        "forum": "w1JanwReU6",
        "replyto": "w1JanwReU6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8612/Reviewer_Kwnc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8612/Reviewer_Kwnc"
        ],
        "content": {
            "summary": {
                "value": "The paper tries to understand biases in LLMs, especially in non-stereotypical settings. To be more specific, they create a new benchmark where each sentence is free of pronounced token-gender correlation. Evaluating several LLMs, they demonstrate that many LLMs show clear gender preferences, demonstrating biases in the neutral setting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea is straightforward, and the authors make the flow easy to follow. The authors did the analysis with different model families and demonstrated models show clear gender preference."
            },
            "weaknesses": {
                "value": "Since LLMs have gender bias is not new, I'm curious about what new aspects this paper is adding. It will be great if the authors provide more details about why we need to care about this neutral setting. And a deeper understanding of under which case models prefer certain gender will also make the paper stronger."
            },
            "questions": {
                "value": "- In the results, do you see under which cases/scenarios, a certain gender is preferred by a model?\n- For all the models evaluated in this paper, are they all trained on PILE dataset? If not, does the PMI result hold for all of them?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8612/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8612/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8612/Reviewer_Kwnc"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8612/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814282656,
        "cdate": 1698814282656,
        "tmdate": 1699637077332,
        "mdate": 1699637077332,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6oRdAHMQ49",
        "forum": "w1JanwReU6",
        "replyto": "w1JanwReU6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8612/Reviewer_WuG5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8612/Reviewer_WuG5"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on examining gender bias in large language models, particularly in non-stereotypical settings. It restricts the evaluation to a neutral subset of sentences with no strong word-gender associations. Authors find that 23 language models under test exhibit 60-95% gender bias indicating the presence of gender associated words might not be the only source of bias."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper addresses an important area in the field of language technology\n* The analysis in Section 4 is thorough considering various factors that can be impacting the results.\n* The presentation is clear with visualizations and tables used appropriately making it easier for the reader to understand the work"
            },
            "weaknesses": {
                "value": "* I am not very convinced with the correctness of generated sentences. As authors themselves mention in the limitations, the generation doesn't involve a human in the loop. Additionally, the generation is limited to a single model (ChatGPT) and having diversity in the models for a model-based benchmark construction would be a better and more fair way to go about it.\n\n* The definition of bias is also not very clear. Since it is a non-stereotypical setting, we see the models favour male gender over female(with models assigning at least 1.65\u00d7 more probability mass to male versions). What are the real world impacts of it? Does this lead to incorrectness in any manner? \n\n* What are some possible reasons of models behaving in this manner (spurious correlations)? \n\n* What are some instances where LMs prefer female. Can we find patterns to such instances. Are these because of implicit biases not fully discovered through word-gender associations? I think woking more towards this direction can lead to interesting findings."
            },
            "questions": {
                "value": "* Can the authors please provide examples of sentence variation of changing the fairness threshold?\n* Can the authors include results as in Table 1 from Ours-10 and other benchmarks? (Preferrably in the appendix)\n\nPlease refer the weaknesses section for other questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8612/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699315784478,
        "cdate": 1699315784478,
        "tmdate": 1699637077226,
        "mdate": 1699637077226,
        "license": "CC BY 4.0",
        "version": 2
    }
]