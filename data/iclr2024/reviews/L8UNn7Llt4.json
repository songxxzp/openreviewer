[
    {
        "id": "abNL1ok80M",
        "forum": "L8UNn7Llt4",
        "replyto": "L8UNn7Llt4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5312/Reviewer_g7U6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5312/Reviewer_g7U6"
        ],
        "content": {
            "summary": {
                "value": "This paper uncovers an explanation for a mysterious phenomenon among off-policy RL DICE methods: state-action level behavior constraints are more principled but action-level behavior constraints perform better empirically. The authors show how an action-level behavior constraint can be achieved from two different view points: Exponential Q-Learning (semi-gradient) designed to specifically impose an action-level constraint and the forward-gradient view put forth by the authors. This then implies that the key difference between the semi-gradient and true-gradient updates (true-DICE) is the backward-gradient. Given both the superior empirical performance of the forward gradient approaches and the desire to better match the more principled state-action approach, the authors propose to retain the backward-gradient component albeit with a twist: include the backward component but reject any part of it which aligns with the forward component (thereby removing any gradient cancellation). The authors then support this technique both theoretically (alleviates interference of forward/backward updates and feature co-adaption) and with extensive empirical validation (Fig 2 shows the method avoids overestimating V(s) outside the dataset distribution, Table 1 and Table 2 show it performs well on difficult tasks, and Fig 3 shows its worst-case performance is higher than other approaches)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I thought this paper did an excellent job of identifying and isolating a key problem, making an astute connection to research elsewhere in the literature (EQL), and then exploiting this connection to develop and evaluate a simple fix. I think the paper is mostly well-written with good theoretical and empirical support. I have a few questions regarding one of the theoretical statements and one of the figures, but otherwise have no other major concerns."
            },
            "weaknesses": {
                "value": "I would like to see a bit more explanation / background in the second-to-last paragraph of section 1 (Introduction) before you start discussing the bellman residual term and *true-gradient* methods."
            },
            "questions": {
                "value": "- In section 3, first paragraph, did you mean \"orthogonal-gradient\" instead of \"vertical-gradient\"?\n- Theorem 3: Given that you measure feature co-adaption with an un-normalized dot-product, couldn't the result in Theorem 3 be achieved by a shrinking of the feature vectors rather than a change in their orientation or relative representations? I assume your intention is to show that $\\nabla_{\\theta} V(s)$ and $\\nabla_{\\theta} V(s')$ are more different (or separated) under the orthogonal-gradient update (i.e., $\\theta=\\theta''$) than they are normally (i.e., $\\theta=\\theta'$).\n- Figure 2: I can see how (d) aligns well with the dataset support, but so does (b). Is (b) undesireable because the actual values V(s) across the offline states are poorly approximated (i.e.,\u00a0the gradient across V(s) is uninformative)? If so, can you please add that to the caption?\n- You say \"Hence, Eq.(36) < 0\" above Theorem 4? Is this a typo? Did you mean to refer to equation 6?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5312/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678825070,
        "cdate": 1698678825070,
        "tmdate": 1699636532444,
        "mdate": 1699636532444,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "t9DcA4mMmO",
        "forum": "L8UNn7Llt4",
        "replyto": "L8UNn7Llt4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5312/Reviewer_SNRk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5312/Reviewer_SNRk"
        ],
        "content": {
            "summary": {
                "value": "- The paper identifies two gradient terms when learning the value function using true gradients: the forward gradient (taken on the current state) and the backward gradient (taken on the next state) of OptiDICE. The authors argue that directly adding the backward gradient may lead to its degeneration or cancellation if these two gradients conflict. \n- To address this issue, the paper proposes a simple yet effective modification that projects the backward gradient onto the normal plane of the forward gradient, resulting in an orthogonal-gradient update, a novel learning rule for DICE-based methods.\n- Through toy examples and extensive experiments on complex offline RL and IL tasks, the paper demonstrates that DICE-based methods using orthogonal-gradient updates achieve state-of-the-art performance and high robustness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Very interesting perspective on poorly performing OptiDICE and proposes a novel orthogonal update algorithm\n- While the paper Is limited to DICE algorithm, the proposed orthogonal update seems to be able to be applied to other bootstrapping based deep RL methods.\n- The paper is clearly presented and easy to follow.\n- Experimental results are strong."
            },
            "weaknesses": {
                "value": "- The theoretical motivations for orthogonal gradient update do not seem to be sufficient. What we can know with the theoretical results presented are:\n  - If we put right \\eta, the orthogonal gradient can be no worse than semi-gradient\n  - There is a possibility that orthogonal gradient can help feature co-adaptation\n  - Based on these, O-DICE should perform on par with S-DICE on simple enough domains, and on complex domains, using DR3-like regularizations will make S-DICE to perform on par with O-DICE. Will there be additional source of better performance? Can we get better fixed point when orthogonal gradient with large \\eta is adopted (large enough to make it different from S-DICE)?\n- The Sikchi et al. (2023) trick seems to have a central role of the performance. details below."
            },
            "questions": {
                "value": "- Basically the orthogonal gradient technique used in this paper is not limited to DICE algorithms but we can adopt them on any deep RL algorithms. Can we get improvements on ordinary deep RL algorithms?\n\n- As far as I know, if we use the semi-gradient update for OptiDICE, it diverges since the second term dominates the first term, and the second term only increases the V function according to the monotonic increasing shape of f^*. It seems the trick of Sikchi et al. (2023) is the trick that makes the algorithm to work. According to the objective in the paper, it seems the algorithm is sampling the arbitrary experience instead of initial state distribution, and it seems to be weighting it much more heavily. Is there any theoretical guarantee that the proposed objective function gives similar solution to what we can get with OptiDICE?\n\n- OptiDICE actually tends to overestimate \\nu, and similar to above, if S-DICE and O-DICE do not overestimate even without double Q trick, I believe that it should be related to the Sikchi et al. (2023) trick. is Sikchi et al. (2023) applied to OptiDICE in experiments?\n\n- According to those reasons, I would like to see the results without the Sikchi et al. (2023) trick."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5312/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819688290,
        "cdate": 1698819688290,
        "tmdate": 1699636532338,
        "mdate": 1699636532338,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FqeI5NTikJ",
        "forum": "L8UNn7Llt4",
        "replyto": "L8UNn7Llt4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5312/Reviewer_8gZ1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5312/Reviewer_8gZ1"
        ],
        "content": {
            "summary": {
                "value": "This paper explores DICE methods, an important area of research in offline RL. DICE-based methods impose behavior constraints at the state-action level, which is ideal for offline learning. They show that when learning the value function using true-gradient update, there are the forward gradient on the current state and the backward gradient on the next state. And they analyze that directly adding the backward gradient may cancel out its effect if the two gradients conflict. To address this, they propose a simple modification that projects the backward gradient onto the normal plane of the forward gradient, resulting in an orthogonal-gradient update. This new learning rule brings state-level behavior regularization. Through theoretical analyses, toy examples, and extensive experiments on complex offline RL and IL tasks, they demonstrate that DICE-based methods using orthogonal-gradient updates achieve good performance and robustness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The advantages of this paper are as follows: \n1. This paper discusses the relationship between true gradient and semi-gradient for (2) and establishes an analysis of the correlation between offline and online training using semi-gradient. This provides a new perspective on why (2) is difficult to train. As a result, the paper introduces the design of orthogonal-gradient, which is logically reasonable. Additionally, the paper presents the relationship between orthogonal-gradient and feature co-adaptation, making this design even more compelling.\n2. This paper is written in a fluent manner with clear logic. The theoretical analysis is rigorous, and the experiments are abundant for comparison."
            },
            "weaknesses": {
                "value": "1. This paper claims that the only difference between itself and OptiDICE [1]  lies in whether to use orthogonal-gradient. However, from my understanding, there is a significant difference in the optimization objectives between OptiDICE and this paper. In [1], the corresponding optimization objective (11) includes not only the value function v but also the optimal density ratio w_v. Therefore, in terms of form, it is different from the optimization objective in this paper. As a result, the author's claim that they have found the mystery of why DICE-based methods are not practical is somewhat exaggerated.\n2 . However, I believe that since (11) still involves v(s'), it should be possible to apply orthogonal-gradient when computing its gradient. Therefore, the author should compare the original OptiDICE algorithm with and without orthogonal-gradient to show that orthogonal-gradient has individual and significant gain. \n3. Most RL algorithms involve Bellman operator operations, which require computing both forward and backward gradients. Therefore, orthogonal-gradient may not just be applicable to DICE-based methods, but also to most RL algorithms. This is much more important than improving DICE-based methods, and the author could consider this perspective.\n4. From an experimental perspective, this method requires two key hyperparameters and needs different hyperparameters for different datasets, which is a disadvantage compared to other methods.\n\n[1] Optidice: Offline policy optimization via stationary distribution correction estimation."
            },
            "questions": {
                "value": "1. The author claims that orthogonal-gradient can help consider state-level constraints, but there is no explicit explanation in the analysis as to why this constraint exists. Please provide a detailed explanation.\n2. Is the $\\epsilon$ in Theorem 4 different for different $s$ ? If so, I think Theorem 4 is trivial since we can easily find an state-wise $epsilon(s) $ to make  $V(s\u2019+\\epsilon)-V(s) $ flip sign. \n3. The algorithmic complexity of O-DICE has not been analyzed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5312/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5312/Reviewer_8gZ1",
                    "ICLR.cc/2024/Conference/Submission5312/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5312/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699022288489,
        "cdate": 1699022288489,
        "tmdate": 1700444197387,
        "mdate": 1700444197387,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i9CoD2QCwX",
        "forum": "L8UNn7Llt4",
        "replyto": "L8UNn7Llt4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5312/Reviewer_LpDw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5312/Reviewer_LpDw"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes Orthogonal-DICE algorithm, that incorporates the V-DICE algorithm with orthogonal-gradient update. The gap between EQL and OptiDICE is analyzed and theoretical proofs are given. Experimental results show that the proposed method achieve better performance than many state-of-the-art methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "see questions"
            },
            "weaknesses": {
                "value": "see questions"
            },
            "questions": {
                "value": "The paper is well-written and easy to follow. The high-level idea is clear with interesting motivation. Theoretical analysis shows the reason harm of backward gradient. The experimental result is also impressive and shows the difference in gradients clearly. I have one question about a special case when gradients are computed\n1. How do you calculate the orthogonal gradient when the angle between backward and forward gradients is more than 90 degrees? Especially when it is 180 degrees?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5312/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699369204670,
        "cdate": 1699369204670,
        "tmdate": 1699636532007,
        "mdate": 1699636532007,
        "license": "CC BY 4.0",
        "version": 2
    }
]