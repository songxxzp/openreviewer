[
    {
        "id": "FALzAcKK1S",
        "forum": "BKinRUoBN9",
        "replyto": "BKinRUoBN9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2487/Reviewer_Pm6d"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2487/Reviewer_Pm6d"
        ],
        "content": {
            "summary": {
                "value": "The paper conducted a comprehensive exploration of cross-modal knowledge distillation and its broader application in multimodal learning. SPDH is introduced to highlight the role if data distribution disparities across modalities in KD effectiveness and PSSM is proposed to mitigate the impact of data distribution shifts on cross-modal KD. Experimental results on four multimodal datasets validate the assumptions and provide directions for future enhancements in cross-modal knowledge transfer."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe paper is well-motivated with a focus on the effectiveness of cross-modal KD and its dependence on the distribution shifts in multimodal data.\n2.\tThe theoretical derivations are relatively sufficient and comprehensive.\n3.\tThe experiments validate the method for enhancing the effectiveness of cross-modal KD."
            },
            "weaknesses": {
                "value": "1.\tThe paper organization should be optimized, for instance, some detailed derivation can be put in the appendix while it is suggested to use more space for experiments and analysis (Sections 4.3 and 4.4).\n\n2.\tThe experiments show that the proposed enhanced KD method demonstrates performance improvement compared to the Baseline. However, can it achieve state-of-the-art performance compared to existing cross-modal KD methods (e.g., [a,b]) tailored to specific multimodal tasks?\n\n3.\tMore qualitative cases are suggested to be provided to better illustrate the effects the proposed enhanced cross-modal KD method has.\n\n[a] Hong Y, Dai H, Ding Y, \u201cCross-modality knowledge distillation network for monocular 3d object detection\u201d, in European Conference on Computer Vision (ECCV), pp. 87-104, 2022\n[b] Wu Z, Li Y, Huang Y, et al, \u201c3D Segmenter: 3D Transformer based Semantic Segmentation via 2D Panoramic Distillation\u201d, in The Eleventh International Conference on Learning Representations (ICLR), 2022."
            },
            "questions": {
                "value": "The weaknesses mentioned above should be carefully responded in the rebuttal phase.\nBesides, there are two more questions:\n1.    The figures are not clear enough, especially when zoomed in.\n2.    Some typo errors should be carefully checked and modified, like \u201cmodaalities\u201d in page 2."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2487/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698117396876,
        "cdate": 1698117396876,
        "tmdate": 1699636185357,
        "mdate": 1699636185357,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ANUsg3tDKK",
        "forum": "BKinRUoBN9",
        "replyto": "BKinRUoBN9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2487/Reviewer_voCm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2487/Reviewer_voCm"
        ],
        "content": {
            "summary": {
                "value": "This paper examines the influence of data distribution shifts on cross-modal knowledge distillation (KD) and establishes the circumstances in which cross-modal KD surpasses unimodal scenarios. It introduces the Solution Space Divergence Hypothesis (SSDH) to elucidate the difficulties encountered in cross-modal KD and proposes a technique known as the Perceptual Solution Space Mask (PSSM) to tackle substantial disparities in data distribution."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper includes theoretical analysis and method improvement, which is very good. The SSDH provides an insightful theoretical analysis of how data distribution shifts can lead to divergence between teacher and student solution spaces, hampering cross-modal KD. PSSM is an innovative practical method to enhance cross-modal KD by focusing on output features with smaller solution space differences.  Comprehensive literature review of cross-modal KD and related techniques like data distribution shifts."
            },
            "weaknesses": {
                "value": "1. Although this article provides a SOLUTION SPACE DIVERGENCE HYPOTHERSIS, the proposed PERCEPTUAL SOLUTION SPACE MASK (PSSM) is  simple and trivial  and plays a similar role to other common knowledge distillation methods.\n2. There are few experiments in this paper. Please compare it with more classic single-modal and cross-modal knowledge distillation methods."
            },
            "questions": {
                "value": "Please refer to the weaknesses.\nPlease provide evidence of the differences in effectiveness between the PSSM in this article and other classical knowledge distillation methods. Please compare it with more classical unimodal and crossmodal knowledge distillation methods as part of your experiments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2487/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698767028461,
        "cdate": 1698767028461,
        "tmdate": 1699636185280,
        "mdate": 1699636185280,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "76gK4D2ndX",
        "forum": "BKinRUoBN9",
        "replyto": "BKinRUoBN9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2487/Reviewer_GxY4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2487/Reviewer_GxY4"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates a weighted distillation loss by cosine similarity between teacher and student networks' logits under cross-modal setting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is well-written with clear assumptions and hypothesis, where later the hypothesis is experimentally validated by using synthetic Gaussian data."
            },
            "weaknesses": {
                "value": "My biggest confusion is that it is relatively difficult for me to connect data distribution shifts with KL divergence between two different probability distributions. From my understanding, the connection between data distribution shifts (here in this paper, modality differences) and ''solution space'' is a little bit absurd. Looking forward to further clarifications.\n\nAnd also please see questions."
            },
            "questions": {
                "value": "1. Please check Eqn. (11), it might be wrong after the = symbol;\n2. Why choose the cosine similarity function? Can other functions be used? What are the results?\n3. Just curious, for A.3, what are the results if input noisy MNIST to student and use MNIST-M for teacher?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2487/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698779965404,
        "cdate": 1698779965404,
        "tmdate": 1699636185171,
        "mdate": 1699636185171,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LZSeylBeFk",
        "forum": "BKinRUoBN9",
        "replyto": "BKinRUoBN9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2487/Reviewer_NCD3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2487/Reviewer_NCD3"
        ],
        "content": {
            "summary": {
                "value": "This work explored cross-modal KD (knowledge distillation) in multimodal learning. First, the hypothesis of solution space divergence (SPDH) is introduced to show that the success in cross-modal KD is decided by the data distribution shift. Then an effective method called PSSM (perceptual solution space mask) is proposed to enhance cross-modal KD. Experimental results on four popular datasets verify the effectiveness of the proposed hypothesis and method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Originality**: The paper proposes a hypothesis, SSDH, to show the key factor in cross-modal KD and a method, PSSM, to tackle the degradation in KD due to multimodal data. Both SSDH and PSSM are instructive.\n\n**Quality**: The paper provides extensive experimental evaluations of the proposed hypothesis and method. \n\n**Clarity**: The paper also provides sufficient background information and related work to situate the contribution of the proposed hypothesis and method in the context of existing literature on cross-modal KD, KD analysis, and distribution shifts.\n\n**Significance**: The paper has established the conditions under which cross-modal KD outperforms unimodal scenarios. This is very important for future research."
            },
            "weaknesses": {
                "value": "**Symbol List in the Appendix**: \nIt would be beneficial to include a comprehensive symbol list in the Appendix. This addition will enhance the clarity of the notation used throughout the paper.\n\n**Table Placement**:\nConsider moving some experimental results from the Appendix, specifically Tables 2, 5, and 6, into the main paper. On the other hand, certain equations from Sec. 3.2 might be more appropriately placed in the Appendix to streamline the main content.\n\n**Formatting Improvements**:\nThe format of the paper requires further attention. Specifically, the use of `\\cite` and `\\citet` appears confusing. A consistent and clear citation style should be maintained throughout the manuscript."
            },
            "questions": {
                "value": "Please address the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2487/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699610497210,
        "cdate": 1699610497210,
        "tmdate": 1699636185103,
        "mdate": 1699636185103,
        "license": "CC BY 4.0",
        "version": 2
    }
]