[
    {
        "id": "Qa9TsGua2r",
        "forum": "sTYuRVrdK3",
        "replyto": "sTYuRVrdK3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6130/Reviewer_keJ2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6130/Reviewer_keJ2"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a large suite of benchmarks for evaluating learned embeddings of proteins. Included are node-level evaluations (at the level of individual residues, e.g. inverse folding, metal binding site prediction) and graph-level evaluations (at the level of the entire protein, e.g. fold classification). The authors also provide a number of software tools, including dataloaders for various pretraining tasks. Finally, they evaluate selected architectures on various pretraining task/benchmark combinations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "New benchmarks are always welcome, and there is also great value in consolidating existing benchmarks. This paper does that well, as the selection here is quite broad, with good coverage. I appreciate the inclusion of both node-level and graph-level evaluations. Within each category, each choice of benchmark is accompanied by a specific rationale, which is also a strength. The documentation of the codebase also seems clear and easy to follow."
            },
            "weaknesses": {
                "value": "It would have been nice to see baselines for all of the downstream tasks in the benchmark using the tools in this software suite (or, lacking that, even scores copied from their respective papers if needs be). Certain tasks like \"reaction class prediction\" are currently missing. Part of the value-add here is the ease of running experiments on all of these tasks, and that isn't currently demonstrated in the current version of the manuscript. Also, consolidated baseline scores are useful sanity checks for reproduction experiments down the line.\n\nMiscellaneous stuff:\n\n- Please provide details about the confidence intervals in Table 2.\n> whereas pLDDT prediction and structure denoising benefit invariant models the most\n- I don't really understand what the basis of this claim (^) is. All four models do approximately as well in the pLDDT column, e.g."
            },
            "questions": {
                "value": "In Table 3, inverse folding (a downstream node-level task) is listed alongside four other tasks explicitly identified as pretraining tasks above. Is this intentional? \n\nAre the experiments in Table 2 at all realistic? I don't think these tasks ever be attempted using models with no pretraining at all in practice. Also, the ESM model without any pretraining can hardly be called an ESM model at all.\n\nDid you consider adding any restrictions on the interfaces between model embeddings and the downstream tasks? If the goal is to evaluate the embeddings themselves, it seems to me like there should be an attempt to standardize e.g. the size of the task-specific MLPs used."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewer_keJ2"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6130/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697846385907,
        "cdate": 1697846385907,
        "tmdate": 1699636664392,
        "mdate": 1699636664392,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AiZm1pdoRA",
        "forum": "sTYuRVrdK3",
        "replyto": "sTYuRVrdK3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6130/Reviewer_foh9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6130/Reviewer_foh9"
        ],
        "content": {
            "summary": {
                "value": "The authors have introduced a novel framework aimed at curating datasets sourced from public repositories like PDB. Their objective is to construct benchmark datasets that facilitate the evaluation of protein structure representation. This framework takes into account various settings, including different backbone models such as various GNN-based models and diverse feature representations, to enhance our understanding of protein structure presentation. Additionally, the authors explore a wide range of pretraining tasks, including sequence/structure denoising and inverse folding, before subjecting these models to a battery of diverse downstream tasks, operating at the Alpha carbon, residue, or overall protein levels.\n\nTo support this work, the authors have generously shared an anonymous GitHub link containing scripts for data preprocessing, which enables the creation of datasets for both pretraining and benchmarking. This contribution is particularly noteworthy as it addresses a critical gap in the field of protein structure representation learning. Historically, the lack of systematically curated benchmarking datasets covering aspects such as featurization, pretraining, and downstream task evaluation has hindered progress. The absence of such a framework has made it exceedingly challenging to compare or reproduce research findings in this specialized domain.\n\nHowever, there are some concerns regarding the paper's completeness. Notably, the authors have outlined a set of downstream tasks in Figure 1, yet a significant portion of these tasks remains unreported in the results section."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The unveiling of this framework, designed for assembling public datasets in order to generate pretraining and downstream benchmark datasets for the study of protein structure representation, is a noteworthy development.\n\nThe examination of how pretraining and featurization affect various downstream architectures and tasks proves to be engaging and insightful.\n\nThe timeliness and significance of this research topic cannot be understated, as it addresses the pressing need for a standardized framework that enables the comparison of various state-of-the-art methods using the same benchmark datasets."
            },
            "weaknesses": {
                "value": "Addressing the Issue of Potential Leakage:\n\nEfforts to mitigate potential data leakage are crucial to ensuring the integrity of benchmarking results, as such leakage could introduce misleading elements into the research findings. Have you considered the removal of overlapping sequences between the pretraining datasets and the downstream testing datasets to further safeguard against such issues?\n\nExpanding Featurization Methods:\n\nIn terms of featurization, the paper seems to primarily focus on simple feature extraction methods. It might be valuable if the authors explored the integration of the following additional featurization techniques:\n\n    Incorporating 3D presentations, such as Uni-Mol (available at https://github.com/dptech-corp/Uni-Mol).\n    Utilizing pretrained models trained on 2D data, like ESM-1b and ESM-2. Considering that Table 1 showcases results using ESM as a backbone model, which produced superior outcomes on the fold dataset, it could be beneficial to include the results for other ESM models in the table.\n\nDiverse Downstream Tasks:\n\nTable 1 predominantly presents results for two specific tasks. However, it appears that several other downstream tasks have not been included in this experiment. It is worth noting that many of these unreported downstream tasks are of substantial importance and are conspicuously absent from the paper. Is there a particular reason for not considering these tasks, and could they potentially be included to provide a more comprehensive view of the research findings?"
            },
            "questions": {
                "value": "1. Could you please address the potential leakage as discussed in the weakness comments?\n\n2. Could you please consider additional featurization as pointed out in the weakness comments?\n\n3. Could you please complete the experiments and provide the results with the downstream tasks that are missing in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6130/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698733590537,
        "cdate": 1698733590537,
        "tmdate": 1699636664212,
        "mdate": 1699636664212,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zWVL77A7Qy",
        "forum": "sTYuRVrdK3",
        "replyto": "sTYuRVrdK3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6130/Reviewer_5uRv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6130/Reviewer_5uRv"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the task of protein structure representation learning and aims to provide a robust and standardized benchmark for this task. \n\nIn this paper, the authors provide different pretraining datasets, downstream datasets, pretraining tasks, auxiliary tasks, featurisation schemes, and model architectures. They cover most of the widely used training strategies, datasets, and model architectures. \n\nIn addition, the authors run experiments using the provided code base and provide some observations and insights."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is well-written and easy to follow.\n\nThe provided datasets, GNN models, and training strategies are comprehensive."
            },
            "weaknesses": {
                "value": "1. In addition to datasets etc, I think a good benchmark should also provide experimental results with well-searched hyperparameters. In such case, future researchers can directly take results for a fair comparison. \n - However, in the current version, the authors didn\u2019t provide results on all downstream tasks. \n - In addition, I am not sure whether the hyperparameters are well-searched, since the best results reported here are still worse than some existing methods. For example, the best results on Fold (considering both with and without auxiliary tasks) are still worse than ProNet [1] and CDConv [2] which don\u2019t use any auxiliary tasks.\n\n2. Do ESM results in the table use pre-trained ESM weights?\n\n3. Some other pretraining strategies are used in Geom3D [3] and ESM-GearNet [4].  \n\n[1] Learning Hierarchical Protein Representations via Complete 3D Graph Networks.  \n[2] Continuous-Discrete Convolution for Geometry-Sequence Modeling in Proteins.  \n[3] Symmetry-Informed Geometric Representation for Molecules, Proteins, and Crystalline Materials.  \n[4] A Systematic Study of Joint Representation Learning on Protein Sequences and Structures"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6130/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698963631589,
        "cdate": 1698963631589,
        "tmdate": 1699636664071,
        "mdate": 1699636664071,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u4pm8v6N7C",
        "forum": "sTYuRVrdK3",
        "replyto": "sTYuRVrdK3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6130/Reviewer_W2RU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6130/Reviewer_W2RU"
        ],
        "content": {
            "summary": {
                "value": "This work presents an open benchmark for evaluating protein structure representation-learning methods. The benchmark includes a diverse set of pre-training methods, downstream tasks, and corpora and includes experimental and predicted protein structures. The structure-based pre-training and fine-tuning datasets and tasks emphasize tasks that enable structural annotation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Modular benchmark enabling rapid evaluation of protein representation learning methods across various tasks, models, representations, and pre-training setups.\n\n2. Analysis of model performance across these different representations and architectures.\n\n3. Using auxiliary tasks to improve the performance of both invariant and equivariant models.\n\n4. Providing tools and procedures for training and evaluating models."
            },
            "weaknesses": {
                "value": "1. The work is missing an explanation of the limitations of the featurization schemes and pre-training tasks.\n\n2. Would be beneficial to include a discussion about the generalizability of the benchmark results \nto the overall protein structure space, and how this translates to proteins not included in the current dataset.\n\n3. Missing a discussion about how geometric models may be improved to surpass sequence-based models.\n\n4. Missing information about the ease of use of the tools, and details about the computational resources required for using the benchmark.\n\n5. Missing (i) aggregation of methods for improving model performance; and (ii) computation of uncertainties in evaluations."
            },
            "questions": {
                "value": "Can the work be adapted for other biological macromolecules beyond protein structures?\nSee, for example, the recent reference:\nPerformance and structural coverage of the latest, in-development AlphaFold model, 2023.\nPredicting structure of proteins, nucleic acids, small molecules, ions, and modified residues. Providing a quantitative benchmark, and improving accuracy of protein-ligand structure prediction, protein-DNA and protein-RNA interface structure prediction, and protein-protein interfaces."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6130/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699386363499,
        "cdate": 1699386363499,
        "tmdate": 1699636663970,
        "mdate": 1699636663970,
        "license": "CC BY 4.0",
        "version": 2
    }
]