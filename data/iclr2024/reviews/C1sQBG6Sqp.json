[
    {
        "id": "ida6yItNAm",
        "forum": "C1sQBG6Sqp",
        "replyto": "C1sQBG6Sqp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1829/Reviewer_Vvvf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1829/Reviewer_Vvvf"
        ],
        "content": {
            "summary": {
                "value": "The authors introduced the dual random smoothing technique in order to mitigate the curse of dimensionality in the certified robustness bound in the realm of RS techniques."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. By partitioning the input into two orthogonal spaces, and apply RS to each of them, the authors mitigated the curse of dimensionality.\n2. The technique proposed can be used in conjunction with a number of other methods, outside of plain RS.\n3. Theorem 2 showcases that this kind of technique is agnostic of the exact partition scheme, which make the application of this technique more ubiquitous."
            },
            "weaknesses": {
                "value": "1. This paper does not convince me that DRS will be better at RS in in terms of classification accuracy. Although experiments can partially validate this idea, but I think some form of theoretical analysis is needed in order to explain that DRS is at least not a lot worse than RS, because experiments can be easily tuned to show better results.\n\n2. The authors did not explain the partitioning mechanism in too much detail, so I'm curious whether a different partitioning mechanism will yield drastically different classification results (in terms of dimensionality scaling it should be fine due to Theorem 2), because this is very possible. Also the authors should explain how to choose a good partitioning scheme as this is conceivably very important to DRS."
            },
            "questions": {
                "value": "If partitioning into 2 spaces is good for mitigating curse of dimensionality, what about 3 spaces and even more? theoretically speaking they should mitigate the problem better, but why choose 2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1829/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1829/Reviewer_Vvvf",
                    "ICLR.cc/2024/Conference/Submission1829/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1829/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698182173010,
        "cdate": 1698182173010,
        "tmdate": 1700635080190,
        "mdate": 1700635080190,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "O89JN1ZCTz",
        "forum": "C1sQBG6Sqp",
        "replyto": "C1sQBG6Sqp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1829/Reviewer_iAVM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1829/Reviewer_iAVM"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes \"dual randomized smoothing\" as a method to alleviate the curse of dimensionality faced by standard randomized smoothing. The proposed method splits inputs into two lower-dimensional components, then performs smoothing in the low-dimensional spaces before bringing the components together to form a final prediction. A certified radius of robustness is theoretically proven, and experiments on benchmark image datasets are given to showcase the enhanced robustness of the method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The introduction is thorough yet concise, and very nicely recapitulates the curse of dimensionality for randomized smoothing and clearly states the paper's contributions towards alleviating these issues.\n2. The related works section is thorough and well situates the proposed method within the vast amounts of works focused on enhancing randomized smoothing.\n3. The main theoretical result (robust radius for the DRS method) appears sound and provides novel insight into the dimensionality aspects at hand.\n4. The experiments are extensive and show nice improvements over standard RS.\n5. Overall, the paper is well-written and is very easy to read."
            },
            "weaknesses": {
                "value": "1. The authors claim that prior works fail to mitigate the curse of dimensionality for randomized smoothing approaches. However, they have missed the relevant works [1],[2], which should be discussed.\n2. The authors propose to use interpolation in Section 4.2 to utilize pre-trained models defined on the original input space, to act as the lower-dimensional classifiers. It is not clear whether they are using the same pre-trained model for both low-dimensional classifiers, or two different ones. Further, the potential downsides to using such interpolation schemes is not discussed. How much performance loss is there in taking this approach when compared to using two models $f^l$ and $f^r$ specifically trained for this dual RS approach? Of course, this would require additional pre-training, which may be costly for certain applications, but it would be good to know how much better the corresponding robustness certificates would be.\n3. First word in Section 5.3 is missing capitalized first letter.\n4. I think a bit more explanation would be useful on how the down-sampling is performed to break each input into its two parts. Specifically, it is sort of glossed over in text and left to the reader to interpret the procedure based on Figure 2. A simple one to two sentence explanation in the main body of the paper would suffice, preferably with an explicit pointer to the graphic of Figure 2, where the authors may want to state that the index set corresponds to a two-part partition of the images pixel indices, and $x^l$ is the vector composed of $x$'s pixel values $x_i$ with $i\\in\\text{idx}$, and $x^r$ is the vector composed of $x$'s pixel values $x_i$ with $i\\notin\\text{idx}$.\n\n[1] Projected Randomized Smoothing for Certified Adversarial Robustness, Pfrommer et al., TMLR, 2023\n[2] Intriguing Properties of Input-Dependent Randomized Smoothing, Sukenik et al., ICML, 2022"
            },
            "questions": {
                "value": "1. Where does the $5*10^{-7}$ come from in (4)? Is this from a pre-specified sample size in RS? It would be nice to briefly mention this in Section 3.2.\n2. Why restrict the two low-dimensional smoothing procedures to share the same variance? How restrictive/suboptimal is this in practice? Will your guarantees generalize to the setting with different smoothing variances?\n3. The authors claim that their bound is tight. Perhaps I missed this, but where is this claim proven/justified?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1829/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1829/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1829/Reviewer_iAVM"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1829/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698687353492,
        "cdate": 1698687353492,
        "tmdate": 1700243233212,
        "mdate": 1700243233212,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Y79eEiOS7U",
        "forum": "C1sQBG6Sqp",
        "replyto": "C1sQBG6Sqp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1829/Reviewer_h68g"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1829/Reviewer_h68g"
        ],
        "content": {
            "summary": {
                "value": "-"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-"
            },
            "weaknesses": {
                "value": "-"
            },
            "questions": {
                "value": "-"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1829/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698756480019,
        "cdate": 1698756480019,
        "tmdate": 1699636112342,
        "mdate": 1699636112342,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BhO39tICBJ",
        "forum": "C1sQBG6Sqp",
        "replyto": "C1sQBG6Sqp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1829/Reviewer_acuW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1829/Reviewer_acuW"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the certified robustness offered by randomized smoothing. Specifically, it addresses the curse of dimensionality associated with standard randomized smoothing algorithms. The authors introduce the concept of dual randomized smoothing, which partitions the input into two subsamples with dimensions $m$ and $n$ such that $m+n=d$. Each subsample is processed by a randomized smoothing classifier. The final decision of the algorithm is based on the most probable class from both classifiers. As a consequence, the dual randomized smoothing enhances the dimension dependence of $\\ell_2$ robustness radius from $O(1/\\sqrt{d})$ to $O(1/\\sqrt{m} + 1/\\sqrt{n})$."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper addresses the curse of dimensionality of DS and improves dimension dependence from $O(1/\\\\sqrt{d})$ to $O(1/\\\\sqrt{m}+1/\\\\sqrt{n})$. The idea of partitioning input into subsamples into subsamples is very interesting.\n\n- The proposed algorithm is easy and efficient to implement. Also, experiments show DRS have improved empirical performance over standard DS."
            },
            "weaknesses": {
                "value": "- The improvement from $O(1/\\\\sqrt{d})$ to $O(1/\\\\sqrt{m}+1/\\\\sqrt{n})$ is mostly a constant change.\n\n- Part of DRS in Section 4.2 a bit unclear. Specifically, how is the downsampling function implemented, and how does it make sure that every subsample gets enough spatial info from the original input?"
            },
            "questions": {
                "value": "(Based on first point in weakness) Have you considered further partitioning the input, for instance, into $k$ subsamples, and then leveraging $k$ randomized smoothing classifiers for the final decision? Would such an approach potentially enhance the dimension dependence even more, perhaps improving the robustness radius from $O(1/\\sqrt{d})$ to something like $O(\\sum_{i=1}^k 1/\\sqrt{d_k})$, where $\\sum_{i=1}^k d_k = d$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1829/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1829/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1829/Reviewer_acuW"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1829/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808172651,
        "cdate": 1698808172651,
        "tmdate": 1699636112254,
        "mdate": 1699636112254,
        "license": "CC BY 4.0",
        "version": 2
    }
]