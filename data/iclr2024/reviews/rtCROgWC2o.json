[
    {
        "id": "isymTL8Ite",
        "forum": "rtCROgWC2o",
        "replyto": "rtCROgWC2o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4405/Reviewer_VtiR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4405/Reviewer_VtiR"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a hierarchical approach to explaining poisoned artificial intelligence (AI) models. In particular, the authors design utilization measurements of trained AI models (in the form of computational graphs) and explain how training data are encoded in AI models\nbased on those measurements at three hierarchical levels (graph node, subgraph, and graph). The author evaluates their method by showing their definition (at three different levels) can result in different patterns and thus be identified for clean and poisoned classes."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "efficiently differentiating clean models from poisoned models seems a potential interesting problem."
            },
            "weaknesses": {
                "value": "1.experiments are not sufficient\n\n(1) limited qualitative examples are given, no quantitative evaluation\n\n(2) no baseline is compared with\n\n2.writing needs significant improvement\n\n(1) flow needs improvement\n\nMany parts of the paper are not connected well. It\u2019s better to add connection and summary for each section/subsection to inform the reader what to expect and the connections among paragraphs.\n\n(2) content can be better organized\n\nFor example, it is very unusual to put computational setup (which usually appears in the experiment section) in the summary section."
            },
            "questions": {
                "value": "-Quantitative evaluation and baselines?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4405/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697663209157,
        "cdate": 1697663209157,
        "tmdate": 1699636414174,
        "mdate": 1699636414174,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MxOVTsST9S",
        "forum": "rtCROgWC2o",
        "replyto": "rtCROgWC2o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4405/Reviewer_f9Xz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4405/Reviewer_f9Xz"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the explanation of AI models, especially the models under backdoor attack. Considering the different activation states of neurons between clean and poisoned inputs in clean and poisoned models, the authors define the utilization of neurons to represent their activation patterns. Then, they represent the activation patterns of models w.r.t. each specific class based on the utilization of neurons. By comparing the activation patterns in clean and poisoned classes and models, the authors identify the spatial location where the attack takes effect."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper is well-motivated, i.e., the authors attempt to explain the poisoned classes and models in the backdoor attack, which is crucial to the safety of AI applications.\n- The authors clearly introduce the motivation, objectives, problems to address, and challenges of this study in the introduction."
            },
            "weaknesses": {
                "value": "- There have been some similar metrics to the proposed method in this paper, and I think the method in this paper has no significant superiority or advantages over previous methods. For example, Ma et al., (2018) and Bai et al., (2021) have proposed to compare the activation frequency of convolutional filters in clean and adversarial examples. The authors are suggested to discuss and compare these methods.\n\n(Ma et al., 2018) Characterizing adversarial subspaces using local intrinsic dimensionality, in ICLR 2018.\n\n(Bai et al., 2021] Improving adversarial robustness via channel-wise activation suppressing, in ICLR 2021.\n\n- The definition of the tensor-state distribution $Q_j(c)$ is confusing. It is stated that each element $q_{ij}$ w.r.t. the state $i$ is the value $count_{ij}$ normalized by $n_j$. However, I\u2019m confused about how the $count_{ij}$ is computed. Furthermore, does this definition satisfy that the sum of all probabilities $q_{ij}$ equals 1? If yes, then why $\\sum_{i=1}^{n_j}q_{ij}\\le 1$? If not, then how can $Q$ be considered and referred to as a PDF?\n\n- The presentation of this paper is poor. First, the authors use graph terms to describe a model, but they do not clarify the definition of nodes and edges. It seems that the authors directly adapt the computation graph constructed by PyTorch. Second, besides the aforementioned problem with $Q$, some notations and subscripts are confusing. The input is sometimes denoted by $x$, sometimes by $\\vec x_i$, and sometimes by $\\vec x$. Also, the subscript $i$ sometimes represents an input and sometimes represents a tensor state.\n  \n- The ``hierarchical\u2019\u2019 in the title seems inappropriate. In my opinion, although the authors consider the neural network as a hierarchical graph, it does not mean that the proposed explanation method is also hierarchical. The authors represent the activation utility of each node (or layer) w.r.t. different samples with a scalar, and such scalars of different nodes are further concatenated to represent the utility of the entire model. Such a representation is not hierarchical.\n\n- The proposed method cannot well address the challenges of high dimension of feature, because this method requires recording the activation values of all computation units over different samples. Furthermore, the computational cost is high. Although the authors mention that the computational cost can be reduced by reducing the number of input samples or limiting the number of probes, these tricks may hurt the accuracy of the obtained explanation.\n\n- About experimental results: (1) In Figure 4, which is a frequency histogram, areas of different models (colors) are supposed to be the same. How do you draw the histogram here? (2) How to understand the raw tensor and unique tensor in Figure 6&7? (3) The datasets and models used in the paper are too limited. The authors are suggested to conduct experiments on various datasets and architectures. (4) The authors do not compare their method with previous methods as baselines.\n\n- Are there any new insights from the results? Although the proposed method indeed shows some differences in clean and poisoned models, how should we understand and then make use of such differences?"
            },
            "questions": {
                "value": "The quality of the figures in this paper is poor.\n\nSome typos: (1) page 4: $i\\in\\\\{1,\\ldots,M\\\\}$ $\\to$ $i\\in\\\\{1,\\ldots,m\\\\}$. (2) the third line on page 6: Equation 3 and 4 $\\to$ Equation 4 and 5."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4405/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4405/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4405/Reviewer_f9Xz"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4405/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698485334035,
        "cdate": 1698485334035,
        "tmdate": 1699636414099,
        "mdate": 1699636414099,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rYfSuwFTym",
        "forum": "rtCROgWC2o",
        "replyto": "rtCROgWC2o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4405/Reviewer_dEwM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4405/Reviewer_dEwM"
        ],
        "content": {
            "summary": {
                "value": "This paper attempts to develop an approach for understanding whether an AI model succumbs to poisoning from malicious signals, and if so, what is the extent of the poisoning. The paper adopts an explainable AI perspective by trying to probe trained models through visualizations of feature activations. The experiments are conducted on benchmarks from a challenge from 202 (TrojAI) and reveal differences in what the tensor-state values look like for clean vs. poisoned models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper targets an important problem of identifying poisoned AI Models that can be manipulated by malicious adversaries into producing outputs that have specific failure models. \n\n- The paper is well motivated, and is well positioned in the context of relevant prior works. The related works discussion is thorough and helps in establishing different \"high-level\" foundational concepts which the paper is based on. \n\n- The approach of analyzing utilization measurements of trained models from their computational graphs is novel in my understanding. \n\n- The paper goes beyond small networks (like LeCun Net) and small datasets like MNIST (common in prior works in this domain), and shows results on real-world traffic datasets. This is helpful in showing that the proposed analysis is potentially scalable to real-world deployment scenarios."
            },
            "weaknesses": {
                "value": "- The paper is a bit confusing to follow because some technical definitions and low level concepts are not clearly explained / defined. For example, terms like \"utilization measure,\" \"class encoding,\" \"poisoned model\" need clear definitions since there are crucial for understanding the approach and results.\n\n- The process of poisoning training images seems to induce significant variations in the poisoned vs. clean images. In the real world, such variations are usually minimal. It will be helpful to perform experiments where the variations are not as extreme, and involve more subtle pixel manipulations. \n\n- The paper talks of \"AI models\" throughout but the experiments are all conducted with ResNet101 models, and are thus limiting in terms of understanding how general is the approach for identification of poisoned models.\n\n- The visualizations in Figs 6 and 7 are interesting, but are not sufficient in terms of understanding the quantitative results clearly. It will be helpful to have more such qualitative visualizations. \n\n- Why are all the experiments only based on traffic sign category identification? \n\n- The approach doesn't have enough details in the paper for replication (for example, training data specs, model specs, visualizations of tensor-state values for random examples) and there is no supplementary material where these details are provided."
            },
            "questions": {
                "value": "- The paper is a bit confusing to follow because some technical definitions and low level concepts are not clearly explained / defined. For example, terms like \"utilization measure,\" \"class encoding,\" \"poisoned model\" need clear definitions since there are crucial for understanding the approach and results.\n\n- The process of poisoning training images seems to induce significant variations in the poisoned vs. clean images. In the real world, such variations are usually minimal. It will be helpful to perform experiments where the variations are not as extreme, and involve more subtle pixel manipulations. \n\n- The paper talks of \"AI models\" throughout but the experiments are all conducted with ResNet101 models, and are thus limiting in terms of understanding how general is the approach for identification of poisoned models.\n\n- The visualizations in Figs 6 and 7 are interesting, but are not sufficient in terms of understanding the quantitative results clearly. It will be helpful to have more such qualitative visualizations. \n\n- Why are all the experiments only based on traffic sign category identification? \n\n- The approach doesn't have enough details in the paper for replication (for example, training data specs, model specs, visualizations of tensor-state values for random examples) and there is no supplementary material where these details are provided."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4405/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807635151,
        "cdate": 1698807635151,
        "tmdate": 1699636414020,
        "mdate": 1699636414020,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dMFYS7sWNl",
        "forum": "rtCROgWC2o",
        "replyto": "rtCROgWC2o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4405/Reviewer_A5T6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4405/Reviewer_A5T6"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to uncover patterns in neural network models that under adversarial attacks. They analyzed the patterns from three levels: graph node (computation unit), subgraph, and graph representations. By experiments, they found that clean images and poisoned images may differ significantly in low-level utilizations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. This paper targets an interesting and important problem, which helps people to understand what actually happen to a neural network under adversarial attack.\n2. This paper proposes a novel hierarchical approach to study the inner patterns of neural networks under adversarial attacks.\n3. The experiments look convincing and reveal some interesting results."
            },
            "weaknesses": {
                "value": "1. I find it's a bit hard to follow the details of the paper. The presentation should be improved. \n2. I think it is not so surprising that clean images and poisoned images have different utilization patterns, especially when significant perturbations are added to clean images to create poisoned ones. The conclusion of this paper should be more convincing if you use advanced adversarial attack methods, where humans are unable to distiguish the poisoned images. \n3. More discussions should be included to explain the significance and the meaning of this work, such as the possible relations to defense strateges."
            },
            "questions": {
                "value": "1. Does this work provide any useful suggestions in defending against adversatial attacks?\n\n2. There are many works focus on adversarial examples (see Ian Goodfellow's works), which are hardly distinguished by humans. I believe they are more advanced attacks than that used in this paper. Do the conclusions in this paper still hold under those advanced attacks? \n\n3. The experiments show that the patterns of clean and poisoned images could be significantly different when the AI model classifies them as the same class. Such differences should originate from the noise or triggers. What if you just move or rotate a traffic signal in the clean image? Should the patterns change drastically?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4405/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699033922043,
        "cdate": 1699033922043,
        "tmdate": 1699636413958,
        "mdate": 1699636413958,
        "license": "CC BY 4.0",
        "version": 2
    }
]