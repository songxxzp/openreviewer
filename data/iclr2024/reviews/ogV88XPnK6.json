[
    {
        "id": "AIrci2WHF5",
        "forum": "ogV88XPnK6",
        "replyto": "ogV88XPnK6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8464/Reviewer_cRU3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8464/Reviewer_cRU3"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an approach based on graph neural processes for meta learning for drug discovery. The authors suggest replacing the MLP encoder of vanilla latent/deterministic neural processes with a graph neural network in order to capture higher-order interactions between the input covariables which in this case are atomic and atom-atom bond features. In addition, they propose a fine-tuning approach to adapt parameters after meta-training and an adoption of model-agnostic meta learning for NPs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Phrasing the problem of drug discovery as a meta-learning problem and using graph neural networks as encoders for neural processes is in the reviewer's opinion both original and reasonable.\n- The paper is well written and easy to follow, and the problems of meta-learning in drug discovery is well delineated.\n- The approach of fine-tuning is intuitive and seems reasonable."
            },
            "weaknesses": {
                "value": "- The main contribution of the paper is the usage of vanilla NPs with an encoder that is an adapted graph neural network of a previously introduced method [1] . In total, the contribution seems too incremental and too little.\n- The description of the methodology itself (molecular graph attentive encoder) is not detailed enough and very superficial (in total 5 lines of the entire manuscript).\n- The fine-tuning and MAML approaches for parameter adaption described in the paper are of little novelty. In addition, the theoretical benefit and motivation of the MAML tuning is not clear to the reviewer since NPs can generally already be considered as meta learnerns.? Empirically, the MAML tuning sometimes improves and sometimes worsens predictive performance (see, e.g., Table~1 FP-CNP with FP-CNP (MAML) or MG-LNP with MG-LNL (MAML)).\n- The experimental section seems very thin and more evaluations with missing competing methods should be made. See, e.g., [2] as a reference.\n- The reference section is incomplete and sometimes incorrect. For instance, the \"Attention is all you need\" paper is from 2017 and not from 2023 and misses the conference information.\n- The authors fail to cite relevant literature on graph neural processes, e.g. [3].\n\n[1] https://pubs.acs.org/doi/10.1021/acs.jmedchem.9b00959\n[2] https://arxiv.org/abs/2205.02708\n[3] https://arxiv.org/abs/2305.18719"
            },
            "questions": {
                "value": "- Some clarifications of the math of the encoder structure or an illustrative figure would in the reviewer's opinion improve the quality of the manuscript. While background on NPs is explained in sufficient detail (both in the main manuscript as well as the appendix), the actual method is not described at all.\n- The authors could evaluate the case where a NP has both a latent and deterministic encoder. See, e.g., [2]\n- As far as I can tell, the authors do not compare themselves against recent methods such as in [1]. Is this true and if so is there any reason for that?\n- LNPs are generally harder to train then CNPs. Is the poor performance of LNPs due to this fact or how can it be explained? Is it because the authors seemed to have trained only for a fixed number of iterations and not until converge (see Appendix C3.7)?\n\n[1] https://arxiv.org/abs/2205.02708\n[2] https://arxiv.org/abs/1901.05761"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8464/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8464/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8464/Reviewer_cRU3"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8464/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698663775177,
        "cdate": 1698663775177,
        "tmdate": 1699637056448,
        "mdate": 1699637056448,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uliwYG8yo7",
        "forum": "ogV88XPnK6",
        "replyto": "ogV88XPnK6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8464/Reviewer_DH7x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8464/Reviewer_DH7x"
        ],
        "content": {
            "summary": {
                "value": "Meta-learning is crucial in fields such as biology, where a variety of test functions exist and sparse data is typical. Additionally, uncertainty measures are typically of interest, to aid in deciding which of the predictions should undergo costly experimental validation. In this work, the authors benchmark deep neural process, a type of deep models that also model uncertainty, for few-shot learning. The authors show that even small modifications to the test functions can massively affect meta-generalization, and use two approaches to address this: fine-tuning and a single step of gradient descent on a MAML-trained neural process. They benchmark neural processes in DOCKSTRING, a dataset of docking scores of 260k ligands against 58 diverse proteins, using molecular fingerprints and graphs as input representations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-\tThe work is nice and easy to follow\n-\tElegant and simple experiment to show the disruption of meta-generalization with divergent test functions (Figure 1)\n-\tProvides useful take-aways in few-shot learning experiments"
            },
            "weaknesses": {
                "value": "- Restricted evaluation to DOCKSTRING"
            },
            "questions": {
                "value": "Surprisingly, I don\u2019t have any questions regarding the work itself. It was very clear, easy to follow, and thorough in the evaluation of deep NPs for few-shot learning in DOCKSTRING. I believe this is an important work in benchmarking deep NPs that would be of great use to the community."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8464/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698748717073,
        "cdate": 1698748717073,
        "tmdate": 1699637056334,
        "mdate": 1699637056334,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mfkGTePr9K",
        "forum": "ogV88XPnK6",
        "replyto": "ogV88XPnK6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8464/Reviewer_PKuZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8464/Reviewer_PKuZ"
        ],
        "content": {
            "summary": {
                "value": "This paper studies meta-learning approaches for molecular tasks, and focuses on introducing neural process (NP) for this application. Apart from building different NP models (CNP,LNP) for molecules, taking fingerprints (FPs) or molecular graphs (MGs) as input features, this paper emphasizes the challenge of meta-generalization in molecular tasks. To close to real world molecular applications, it sets up experiments with an unusual meta-learning setting: the correlation between training and testing tasks are controlled at a low degree, and the size of context varies in a large range. To deal with, this paper proposes to combine gradient-based adaptation (MAML, fine-tuning) with NP model. The authors tailor DOCKSTRING dataset, and detail empirical results show that MG-CNPc(fine-tuned) has a performance advantage in most cases."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThis paper comprehensively study NP-based models on molecular tasks, including different NP variants, different molecular features, different additional adaptation strategies.\n\n2.\tIt pointed out the challenge that tasks are highly diverse in real world molecular applications. And propose additional adaptation steps should be adopted based on NP models to increase the meta-generalizability.\n\n3.\tData processing and empirical results are shown in detail. It looks convincing that the proposed method could show advantage with such setting."
            },
            "weaknesses": {
                "value": "1. Lack of novelty. As a representative amortized meta-learner, NP has been widely studied. This paper adopts the most conventional NP models on molecular tasks. \u201cNP+gradient steps\u201d is also a popular way to improve meta-learning performance by combining two adaptation strategies. (in similar fields, there is [1]). It seems little technical contribution in this paper.\n\n2. The authors propose that existing datasets are highly homogeneous across tasks, while in reality the task diversity should be considered. However, there lacks evidence in this paper. No empirical results of existing popular datasets (e.g., fs-mol[2], moleculenet[3]), nor comparing them with real-world cases are provided.\n\n3. Lack of benchmark datasets and baselines. Since the proposed is following a standard meta-learning setting, existing few-shot molecular property prediction methods [4,5,6], should be considered. Among them, [5] is also applicable for regression task, which should be compared on DOCKSTRING. And the proposed method should also be applicable for classification tasks, so it should be tested on [2][3], and compared with [4,5,6].\n\n4. Poor organization of related works. The related works mix everything (i.e. datasets, methods) together, which are hard to read.\n\n[1] Zhang, Q., Zhang, S., Feng, Y., & Shi, J. (2023). Few-Shot Drug Synergy Prediction With a Prior-Guided Hypernetwork Architecture. IEEE Transactions on Pattern Analysis and Machine Intelligence.\n\n[2] Stanley, M., Bronskill, J. F., Maziarz, K., Misztela, H., Lanini, J., Segler, M., ... & Brockschmidt, M. (2021, August). Fs-mol: A few-shot learning dataset of molecules. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)\n\n[3] Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S., ... & Pande, V. (2018). MoleculeNet: a benchmark for molecular machine learning. Chemical science, 9(2), 513-530.\n\n[4] Wang, Y., Abuduweili, A., Yao, Q., & Dou, D. (2021). Property-aware relation networks for few-shot molecular property prediction. Advances in Neural Information Processing Systems, 34, 17441-17454.\n\n[5] Chen, W., Tripp, A., & Hern\u00e1ndez-Lobato, J. M. (2022, September). Meta-learning adaptive deep kernel gaussian processes for molecular property prediction. In The Eleventh International Conference on Learning Representations.\n\n[6] Schimunek, J., Seidl, P., Friedrich, L., Kuhn, D., Rippmann, F., Hochreiter, S., & Klambauer, G. (2023). Context-enriched molecule representations improve few-shot drug discovery. arXiv preprint arXiv:2305.09481."
            },
            "questions": {
                "value": "Please refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8464/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829257047,
        "cdate": 1698829257047,
        "tmdate": 1699637056195,
        "mdate": 1699637056195,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7sIyDVzAqo",
        "forum": "ogV88XPnK6",
        "replyto": "ogV88XPnK6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8464/Reviewer_P8e8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8464/Reviewer_P8e8"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors study Graph NP's performance in few-shot learning tasks, and propose fine-tuning strategies to further improve GNP's regression performance while maintaining good calibration. They also present a Bayesian optimization case study to showcase GNP's potential advantages."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Writing: Well-organized, easy-to-follow paper.\n2. Significance: Show that graph NPs are competitive in molecular few-shot learning tasks."
            },
            "weaknesses": {
                "value": "1. Applicability: Focuses on regression tasks only despite abundant data and baselines in classification.\n2. Novelty: Fine-tuning NPs during meta-testing are not novel contributions."
            },
            "questions": {
                "value": "1. Why didn't you extend Graph NPs into the classification setting, where the amount of data and baselines is abundant?\n2. Could you explain the results presented in Figure F.1, where the $R^2$ did not decrease as the percentage of training points sampled increase?\n3. Have you considered studying the impact of context/target set randomization on calibration of uncertainty estimates?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8464/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8464/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8464/Reviewer_P8e8"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8464/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698986228111,
        "cdate": 1698986228111,
        "tmdate": 1699637056082,
        "mdate": 1699637056082,
        "license": "CC BY 4.0",
        "version": 2
    }
]