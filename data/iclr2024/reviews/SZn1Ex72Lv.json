[
    {
        "id": "T41eCwNfwS",
        "forum": "SZn1Ex72Lv",
        "replyto": "SZn1Ex72Lv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1239/Reviewer_NTVM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1239/Reviewer_NTVM"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenge of poor generalization in Feed Forward Neural Networks (FNNs) by introducing the concept of block-operations. The authors propose a new architectural component, the Multiplexer, which enhances FNNs by incorporating block-operations. They conduct experiments to verify the effectiveness of this approach, comparing it to traditional FNNs. The Multiplexer exhibits several desirable properties, including consistent concept representation, reduced negative interference, improved specialization and transfer learning, and enhanced subnetwork reusability. It excels in learning algorithmic tasks with conditional logics and demonstrates 100% Out-of-Distribution (OOD) generalization in several cases. The paper suggests that block-operations hold promise for future research, potentially improving neural network compositionality and generalization."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper introduces a novel concept of block-operations, providing a fresh approach to enhance neural network performance.\n\nThe proposed Multiplexer demonstrates a range of desirable properties, including improved generalization and transfer learning, making it a practical advancement for neural networks.\n\nThe paper's findings open avenues for future research, offering the potential for enhanced network compositionality and generalization in complex architectures."
            },
            "weaknesses": {
                "value": "It appears that I am not well-versed in this particular research field, but after reviewing the paper, I observed that it may have been hastily prepared. The issues I noticed include inappropriate tables and figures, minor typographical errors, and a lack of overall organization.\n\nThe authors mention that \"there is no inductive bias that would cause FNNs to keep activation patterns for the same concept consistent throughout the network.\" This motivation should serve as the crux of the paper, yet it lacks a clear and thorough discussion.\n\nFurthermore, the authors state, \"In this way, we attempt to construct a neuro-symbolic system using a purely connectionist approach, as suggested by Greff et al. (2020) as a possible solution to the binding problem.\" This raises questions about whether the authors are merely adopting an existing solution. What, then, is the unique proposition of this paper? What specific contributions does it make to the field?"
            },
            "questions": {
                "value": "- Could you provide more context and a thorough discussion regarding the statement that \"there is no inductive bias that would cause FNNs to keep activation patterns for the same concept consistent throughout the network\"? How does this serve as the motivation for your research, and why is it important?\n\n- The paper mentions the attempt to construct a neuro-symbolic system using a purely connectionist approach, as suggested by Greff et al. (2020). Can you clarify how your approach differs from or builds upon this suggestion? What novel elements or contributions does your paper bring to the concept of a neuro-symbolic system?\n\n- Could you provide a clearer and more structured organization of the paper to improve its readability and comprehension for readers who may not be experts in the field?\n\n- What specific issues or challenges in the field of Feed Forward Neural Networks (FFNNs) are you addressing with your proposed concept of block-operations and the Multiplexer? How does this concept enhance FFNNs, and what practical applications or benefits can be derived from it?\n\n- Are there any empirical results or experiments that support the claims made in the paper regarding the performance and effectiveness of the Multiplexer compared to traditional FFNNs? Providing evidence would strengthen the paper's arguments.\n\n- How do the concepts of \"block-operations\" and \"Multiplexer\" relate to the broader field of neural network architectures, and what potential implications do they have for various applications beyond the scope of the paper?\n\n- Could you address the concerns raised about inappropriate tables and figures, as well as minor typographical errors in the paper? How can these issues be resolved to enhance the overall presentation of your research?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1239/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822693869,
        "cdate": 1698822693869,
        "tmdate": 1699636050545,
        "mdate": 1699636050545,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dZ0rbbjuxU",
        "forum": "SZn1Ex72Lv",
        "replyto": "SZn1Ex72Lv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1239/Reviewer_pnJf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1239/Reviewer_pnJf"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a multiplexer block to replace standard FNNs, to provide a inductive bias in neural networks that encourages re-use and modularity. The paper claims to introduce the concept of \"block operations\" to split hidden states into blocks, which can then be combined together / routed into different blocks. The multiplexer itself is just a neural network that takes M blocks and outputs N blocks.\nThe overall model is an \"SMFR\" block which consists of stacked \"Multiplexer + residual FNN\" blocks. \n\nThe experiments are as follows:\n- Addition / multiplication of 2 single digit numbers (mod 10)\n- Double addition / multiplication (i.e. the model needs to output two sum / products). \n- An algorithmic task which applies conditional rules on 5 different variables, and the objective is to track variable values at the end of these rule applications. \n\nFrom results, we find that the SMFR model does better than stacked FNNs."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Interesting research question on studying / encouraging modularity."
            },
            "weaknesses": {
                "value": "- The paper lacks soundness (and presentation quality though that may not be a cause for rejection). I'm not sure that the block-operations / multiplexers are doing something qualitatively different from an FNN with residual connection. The behaviour of the multiplexer can very well be simulated by a standard FNN, so i'm not exactly sure what the inductive bias is / where the improvements are coming from. At the very least, it would be good to know how well baselines were tuned / how do results change with slightly bigger models.\n\n- The paper also claims to introduce \"block operations\" to split activations into multiple blocks. This is definitely not new. For instance, even in ordinary self-attention, hidden states are divided into multiple blocks which are separately consumed by distinct attention heads. \n\n- The specific datasets chosen also seem to be extremely toy-ish for compositional generalization / modularity. Would the proposed approach work on e.g. MNIST (still toy-ish but larger scale than the datasets in the paper)."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1239/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699145847985,
        "cdate": 1699145847985,
        "tmdate": 1699636050478,
        "mdate": 1699636050478,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kjS8cY1OHs",
        "forum": "SZn1Ex72Lv",
        "replyto": "SZn1Ex72Lv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1239/Reviewer_frtd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1239/Reviewer_frtd"
        ],
        "content": {
            "summary": {
                "value": "The authors propose two new layers, the Multiplexer and the FNNR. The multiplexer can linearly interpolate between its input blocks, thus acting as a learnable permutation. The FNNR is a learned transformation with a gated recurrence. The goal of the model is to provide a way of internal routing within the state of the models, as a remedy for their lack of reusing the parameters."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Important problem\n- The authors propose a novel architecture to facilitate reusing of subnetworks in NNs\n- Interesting algorithm design"
            },
            "weaknesses": {
                "value": "- The main weakness is the quality and design of the experiments, which failed to convince me about the validity of the architecture, and the clarity of the described method.\n- The arithmetic tasks are all using 1-digit numbers. This limits the total number of possible training examples in the order of 100. This can result in very easy memorization, and it is also unclear if it provides enough \"motivation\" for the network to learn to route. The simplicity could be an explanation for why the difference is relatively small between FFN and SMFR in Tab 1. The original paper by Csord\u00e1s et al had 2-digit inputs, which resulted in ~10k possibilities, which is significantly higher.\n- The authors sometimes report performance on all hyperparameters averaged together. This unnecessarily presents their results weaker than they are, and it also makes it harder to understand the actual performance gain. I don't see the utility in averaging in bad hyperparameters. For example at the end of P6, they report an average of \"0.205 OOD accuracy\", and then \"...For these architectures, 75% of trials achieved 100% OOD accuracy\". So what is the mean accuracy of the architecture with the best hyperparameters then? Could you please report the mean (and if possible the standard deviation) on the best hyperparameter configuration, as usually done in the literature? Similarly, on page 7, it says \"there was a range of values where all experiments converged with 100% accuracy on all iterations\", yet in Fig 4 I don't see any method that is 100% on all iterations.\n- The claim that \"SMFRs can learn to automatically arrange unstructured data into the block format they need\" needs more support. Having 1 successful run out of 30 is not convincing enough. For example, input blocks could be decoded back with a linear problem after a single step to show if the architecture managed to disentangle the inputs.\n- Some details are unclear: For example, if the weights are shared in the Multiplexer, FNNR, and inside the \"Derived Blocks\" in FNNR. This should be described by equations in the main paper.\n- In FNNR block in Fig.3, and in the description below it, it mentions that \"the FNNR module uses the input blocks of the Multiplexer as its extra input\". What are the standard, non-extra inputs? What is the *output* of the Multiplexer used for?"
            },
            "questions": {
                "value": "- How are the inputs fed to the network? How are the outputs read out?\n- In the explanation of why SMFR performs better on Addition/Multiplication, at the bottom of P5, it says: \"... because the softmax used by the Multiplexer is also commutative\". What does this mean? The output of the softmax depends on the logits, and those on the specific input blocks unless the gating weights are shared between them. This does not seem to be the case, as far as I can tell from Fig 2 and 3, and the related text.\n- The architecture reminds me of a transformer with weights that are not shared between the columns, and with the embedded input chunked in multiple initial hidden states. Can you comment on this similarity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1239/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699347134098,
        "cdate": 1699347134098,
        "tmdate": 1699636050416,
        "mdate": 1699636050416,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Gvm8nI1dR5",
        "forum": "SZn1Ex72Lv",
        "replyto": "SZn1Ex72Lv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1239/Reviewer_VWus"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1239/Reviewer_VWus"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new neural structure called \u201cblock operation\u201d, that groups activations into atomic sections, and performs actions over these sections, and studies its empirical impact on the network\u2019s performance on algorithmic tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- **Idea**: The paper proposes new neural structures and does a good job explicitly listing each of the specific design goals they seek to realize through their approach.\n- **Topic**: Its research objectives of OOD generalization and ability to perform algorithmic tasks are important, and I personally think, under-explored themes in current deep learning research.  \n- **Evaluation**: The paper studies that idea through a nice variety of experiments of different kinds: addition and multiplication, double-addition, and algorithmic tasks.\n- **Discussion**: There are good discussion and limitations sections that cover different aspects of the presented approach (computational efficiency, training stability, interpretability and scalability)."
            },
            "weaknesses": {
                "value": "- **Semantic Grouping**: The paper claims that  block operations group the neurons into semantic units, but the grouping is actually done arbitrarily. The paper doesn\u2019t provide evidence for semantic grouping or disentanglement to properties etc, quantitatively or qualitatively. \n- **Clarity**: The writing, clarity and overall presentation could be improved significantly. This is especially the case up to page 5. Instead of having coherent sections with conjunctions and transitions, the paper is organized as lists of bullets or disconnected paragraphs, and in many cases not enough details are provided on each of them. Personally I think this form of succinct representation is less suitable for a paper, and would encourage the authors to work on the writing further both in order to increase its flow and coherency, and also to extend it to include more details. In the questions/suggestions section I refer to specific content that is missing through the different sections.\n- **Terminology**: Multiple terms are used without being defined clearly and precisely enough: e.g. routing, uniform representations, representation-preserving mappings, copy gate. A notable additional example is that of negative interference, a term that is mentioned repeatedly through the paper but isn\u2019t explained or illustrated. A further discussion, formal definitions or concrete examples could help clarify the mentioned terms. \n- **Datasets**: The experiments in the paper are performed over synthetic discrete data only. I would suggest either exploring multiple modalities or real-world data to strengthen the empirical evaluation.\n- **Baselines**: The experiments compare the new approach only to FNNs as baselines. Comparing to other approaches, such as recurrent networks and transformers, is in my opinion critical.\n- **Novelty**: The authors contrast between self-attention and the new multiplexers idea, mentioning the strength of multipliers allowing for different \u201cinputs and outputs\u201d. These could be modeled by cross attention (which is broadly used too but isn\u2019t mentioned in the paper). I think both a discussion of the conceptual differences as well as empirical head-to-head comparison between the proposed idea and cross attention will be very important.\n- **Background & Related Works**: Not enough context is provided on the current issues of FNNs, the prior attempts to address them, what are their specific limitations that the paper seeks to address. For instance, where the paper mentions that a prior work \u201ccalls for additional research on suitable inductive biases\u201d \u2013 what prior inductive biases have been explored in the literature, how is your approach different from them, etc. Likewise, the related works section mentions Capsule Networks, Routing Networks and Recurrent Independent Mechanisms, but provides a particularly brief description of them, and doesn\u2019t in which ways they are similar or different than the new approach. I would also recommend discussing slot attention and its extensions.\n- **Implicit Grouping**: It could be that FNNs perform some degree of grouping implicitly, even without externally imposing the grouping through inductive biases. In hierarchical deep networks it\u2019s likely to be the case. It would be therefore good to either show empirically that FNNs do not do that effectively enough on the tasks that you study, or rephrase/tone down the paper\u2019s claims about FNNS."
            },
            "questions": {
                "value": "- The paper says: \u201cfully-connected layers treat all neurons as independent of each other\u201d. I disagree with that on several levels. Since they have non-linearities, FNNs can model arbitrarily complex functions that do account for various subtle relations between the activations. They do the opposite of treating neurons as independent factors, and in fact research efforts on e.g. disentanglement aim to encourage them to treat latent factors more independently then what they do by default. In another level, models that are based on slots, tokens, key-value, or in the case of transformers heads, already incorporate inductive biases for grouping of activations into larger units. It would be good to discuss these in the paper.\n- Section 3 says: \u201cthere is no inductive bias that would cause FNNs to keep activation patterns for the same concept consistent throughout the network.\u201d. That\u2019s a very strong statement that I\u2019m not sure if there\u2019s a way to prove. I would recommend rephrasing it.\n- The last sentence in the abstract isn\u2019t clear: \u201cAdapting more complex architectures than the FNN to make use of them could lead to\u2026\u201d <-- are them the FNNs that are altered, or the current proposed idea, or future structural extensions?\n- Second introduction\u2019s paragraph:  investigated in how -> investigated how\n- In section 3: \u201cmore dense\u201d -> \u201cdenser\u201d."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1239/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699465644183,
        "cdate": 1699465644183,
        "tmdate": 1699636050355,
        "mdate": 1699636050355,
        "license": "CC BY 4.0",
        "version": 2
    }
]