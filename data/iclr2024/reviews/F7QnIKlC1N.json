[
    {
        "id": "lqEhvzchy1",
        "forum": "F7QnIKlC1N",
        "replyto": "F7QnIKlC1N",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5387/Reviewer_1WnG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5387/Reviewer_1WnG"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes GTMGC, a transformer-based architecture for end-to-end prediction of 3D groundstate molecules' conformations from their 2D graph.\n\nThe method makes use of MoleBERT for initial embedding as well as LPE for positional encoding.\nThe inputs are then processed by a Transformer-based model, where the self-attention modules are augmented with the adjacency matrix and learned/predicted atomic distance matrix in a weighted sum fashion. \n\nThe loss is augmented by a regularization of the middle distance matrix prediction.,"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is very clear and pleasant to read.\n2. The integration of various existing frameworks for the task of molecules' ground state conformation is original to some extent.\n3. The performances compared to the baselines are significant."
            },
            "weaknesses": {
                "value": "I believe there are two main weaknesses:\n\n1. Novelty: There are plenty of previous works (not cited) implementing Transformer architectures that implement the elements of the proposed self-attention, e.g., [1] uses the adjacency matrix, [2] makes use of the distance matrix (even cited in the manuscript), while the weighted summation is ubiquitous in many fields using Transformers [3]. The initial encoding is obviously not a technical contribution.\n\nThus, the contribution may be summarized as the integration of existing methods/approaches, augmented with the regularization loss on the distance matrix (the \\beta seems to bring very minor improvement).\n\n2. Strange results: I may be wrong, but according to Table 3 (ablation study) a *simple Transformer* architecture without any addition to the self-attention reaches 0.4395 (MAE) which is already better than all the other baselines. This is problematic. Also, the final improvement is only ~1%. Finally, the reported results seem to take the best-ablated model results for each metric, which is wrong.\n\nMoreover,  we have:\n\n3. The advantage of MoleBert over the standard atomic encoding is extremely shallow or even worse.\n4. Lack of comparison with other Transformer based methods.\n\n[1] *Molecule attention transformer.*\n\n[2] *Geometric transformer for end-to-end molecule properties prediction.*\n\n[3] *Axial-DeepLab*"
            },
            "questions": {
                "value": "Currently, it seems the proposed contributions don't bring any advantage over a simple (large) transformer model.\n\n1. One needs to know the capacity of the model in order to assess the origin of the good performance. \nAccording to weakness 2, it seems the good performances are obtained *almost solely* from a large/powerful standard Transformer model. From Table 5, the model seems much bigger than other methods. Also, the discrepancies between the tables are disturbing (one single metric should be used for the model validation).\n\n2. Ablating the initial LPE. \n\n3. It would be beneficial to have a comparison performance with (at least one) other molecule transformer-based methods such as [4,5,6] or others at a similar capacity.\n\n4. It would be interesting to look at the learned weighting parameters (\\gamma) of the self-attention to better understand the contribution of each (maybe even adding a weighting to the global attention).\n\n\n[4] Relative molecule self-attention transformer.\n\n[5] 3dtransformer: Molecular representation with transformer in 3d space.\n\n[5] Geometric transformer for end-to-end molecule properties prediction."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5387/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698246140324,
        "cdate": 1698246140324,
        "tmdate": 1699636545151,
        "mdate": 1699636545151,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xxe0kbsP5M",
        "forum": "F7QnIKlC1N",
        "replyto": "F7QnIKlC1N",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5387/Reviewer_Po8N"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5387/Reviewer_Po8N"
        ],
        "content": {
            "summary": {
                "value": "This work proposes GTMGC, a graph transformer model for ground-state molecular conformation prediction. GTMGC uses a novel self-attention module to achieve effective molecular structure modeling. Experiments show that the proposed GTMGC model achieves state-of-the-art performance in ground-state molecular conformation prediction benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality: The proposed graph transformer model is novel, with many novel technical contributions in effectively capturing spatial structures by self-attention mechanism.  \nQuality: The effectiveness has been effectively demonstrated by experiments.  \nClarify: The writing and presentation of this paper is good and well-organized.  \nSignificance: The contribution of this work is very useful and meaningful to chemical and molecular biological science fields as the proposed method can significantly accelerate the computation of finding ground-state molecular conformations."
            },
            "weaknesses": {
                "value": "There are actually many prior studies about formulating the mapping from 2D molecular graphs to 3D molecular conformations as a generative problem. Though they are different from the problem studied in this work, these models can be trained on the used Molecule3D datasets and evaluated by generating only one molecular conformation. However, authors do not compare with any of these methods. Authors are recommended to compare with at least one molecular conformation generation method, such as [1].\n\n[1] Torsional Diffusion for Molecular Conformer Generation. NeurIPS 2022."
            },
            "questions": {
                "value": "No additional questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5387/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698633401901,
        "cdate": 1698633401901,
        "tmdate": 1699636545032,
        "mdate": 1699636545032,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0HwdezqDf1",
        "forum": "F7QnIKlC1N",
        "replyto": "F7QnIKlC1N",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5387/Reviewer_BbTi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5387/Reviewer_BbTi"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel graph transformer specifically designed for 3D ground state prediction. Notably, the proposed architecture is versatile and applicable to a wide range of 3D supervised tasks. The key contributions of this work include:\n\nA novel architectural proposal that elegantly extends the classical attention mechanism to 3D molecular graphs. This extension incorporates edge and interatomic distances as biases for the attention mechanism, enhancing its capabilities.\n\nA successful demonstration of the effectiveness of this architecture in the realm of 3D ground state prediction, as well as its application to predict various other 3D molecular properties."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality:\nDespite numerous unsuccessful attempts to construct graph transformers using principles akin to the original transformer, this work stands out as it elegantly achieves the intended goal with minimal architectural complexity. This work avoids unnecessary biases that often detract from the model's effectiveness, making it easier for most researchers to apply their existing intuitions from sequence transformers to this novel architecture.\n\nQuality:\nThe architectural design and its application in ground state conformation prediction are well executed, as reflected in the results, solidifying its position as a favorable solution compared to alternative methods. The iterative refinement of $G_{cache}$ within the decoder represents a notable innovation that enhances model performance in conformer prediction. Ablation studies further clarify the significance of each component within the network, facilitating an understanding of their contributions to this specific modeling task.\n\nClarity:\nThe paper is well-written and maintains a high level of clarity, making it easily comprehensible for readers.\n\nSignificance:\nWhile predicting the ground state of a molecule remains relatively underexplored due to its limited relevance in specific applications, this work serves as a foundational step that can be extended to tackle the broader challenge of full conformer generation. Such an extension holds are very significant, especially in the context of drug discovery."
            },
            "weaknesses": {
                "value": "The assertion regarding the innovative utilization of the MoleBERT Tokenizer might be overstated, especially in light of the results presented in Table 2. Previous molecular graph papers, such as the MolGPS paper, have explored various atomic featurizations that could potentially outperform the approach presented in this work.\n\nTo allocate more space for related works and experiments, it would be beneficial to consider shortening or omitting certain sections, such as those in the introduction (implementation) and preliminary sections (multi-head and transformer). \n\nThe related work should be integrated into the main text rather than relegated to the appendix. It is crucial to comprehensively cover the various attempts to construct graph transformers and elucidate why they are ill-suited for the tasks at hand."
            },
            "questions": {
                "value": "Was molecular property prediction approached as a single-task or multi-task endeavor?\n\nCould you clarify the rationale behind placing the molecular property prediction results in the appendix, especially considering that they do not outperform SOTA across the board?\n\nIt could be valuable to assess the scalability of your architecture across various graph sizes, thereby determining where it potentially outperforms existing methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5387/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698780196807,
        "cdate": 1698780196807,
        "tmdate": 1699636544931,
        "mdate": 1699636544931,
        "license": "CC BY 4.0",
        "version": 2
    }
]