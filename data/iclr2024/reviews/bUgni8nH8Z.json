[
    {
        "id": "whOc7usDP2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4349/Reviewer_n38z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4349/Reviewer_n38z"
        ],
        "forum": "bUgni8nH8Z",
        "replyto": "bUgni8nH8Z",
        "content": {
            "summary": {
                "value": "This paper proposed a novel approach to understanding and improving ReLU-based neural networks. The authors delve into the characteristic activation values of individual ReLU units within neural networks and establish a connection between these values and the learned features. They propose a geometric parameterization for ReLU networks based on hyperspherical coordinates, which separates radial and angular parameters. This new parameterization is demonstrated to enhance optimization stability, convergence speed, and generalization performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper is written in a clear and easily comprehensible manner, making it easy for readers to follow.\n- The paper presents a unique and innovative approach to understanding ReLU networks by exploring the characteristic activation values. This fresh perspective sheds light on the inner workings of these networks, offering insights that were previously unexplored."
            },
            "weaknesses": {
                "value": "see questions."
            },
            "questions": {
                "value": "- Can the analysis apply to the existing advanced batch normalization improvements like IEBN [1], SwitchNorm [2], layer norm [3]. These missing works should be considered and added to the related works or analysis.\n\n- I am not very familiar with the topics covered in this article, I will consider these clarifications along with feedback from other reviewers in deciding whether to raise my score.\n\n[1] Instance Enhancement Batch Normalization: An Adaptive Regulator of Batch Noise, AAAI\n\n[2] Differentiable Learning-to-Normalize via Switchable Normalization, ICLR\n\n[3] Layer normalization, IJCAI"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4349/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4349/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4349/Reviewer_n38z"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4349/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697341354803,
        "cdate": 1697341354803,
        "tmdate": 1699636405943,
        "mdate": 1699636405943,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JkRlz1CQcR",
        "forum": "bUgni8nH8Z",
        "replyto": "bUgni8nH8Z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4349/Reviewer_cECr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4349/Reviewer_cECr"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a geometric parameterization method for ReLU networks to improve their performance. Some experimental results show the performance of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper proposes a geometric parameterization method for ReLU networks to improve their performance. \n2. Some experimental results show the performance of the proposed method."
            },
            "weaknesses": {
                "value": "Although the paper is theoretically and experimental sound, there are still some questions need to be discussed in this paper:\n1.\tThe main contributions of this paper are to propose one geometric parameterization for ReLU networks and input mean normalization. But the input mean normalization proposed in this paper is very similar to the mean-only batch normalization (Salimans & Kingma, 2016). What\u2019s the advantage of the former against the latter?\n2.\tThe experimental results are not convincing. The authors should compare the performance of the proposed algorithm on more models and datasets.\n3.\tBoth the English language and equations in this paper need to be improved."
            },
            "questions": {
                "value": "Although the paper is theoretically and experimental sound, there are still some questions need to be discussed in this paper:\n1.\tThe main contributions of this paper are to propose one geometric parameterization for ReLU networks and input mean normalization. But the input mean normalization proposed in this paper is very similar to the mean-only batch normalization (Salimans & Kingma, 2016). What\u2019s the advantage of the former against the latter?\n2.\tThe experimental results are not convincing. The authors should compare the performance of the proposed algorithm on more models and datasets.\n3.\tBoth the English language and equations in this paper need to be improved."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4349/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4349/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4349/Reviewer_cECr"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4349/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699108531742,
        "cdate": 1699108531742,
        "tmdate": 1699636405871,
        "mdate": 1699636405871,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1LGl5NZygc",
        "forum": "bUgni8nH8Z",
        "replyto": "bUgni8nH8Z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4349/Reviewer_BSGv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4349/Reviewer_BSGv"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes geometric parameterization (GmP) for ReLU networks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper introduced the characteristic activation sets of individual neurons and a geometric connection between such sets and learned features in ReLU networks. \n2. This paper then introduced geometric parameterization (GmP) based on radial-angular decomposition in the hyperspherical coordinate system. It also proves that the change in the angular direction under perturbation $\\varepsilon$ is bounded by the magnitude of perturbation $\\varepsilon$. This property is not held for standard parameterization and weight normalization.\n3. The authors provide some experimental results to show the advantage of the proposed GmP for ReLU networks."
            },
            "weaknesses": {
                "value": "1. The Gmp for the ReLU network with IMN in Eq.16 also seems applicable to the weight normalization (WN), i.e. change $u(\\theta)$ to $\\frac{w}{\\||w\\||_2}$. I am wondering what the result will look like. The authors should talk more about why they prefer optimization in the angular space instead of the weight normalization space since they are equivalent as shown in Eq. 9 ( $u(\\theta):=\\frac{w}{\\||w\\||_2}$).\n2. The theoretical proof only shows that the change in the angular direction under perturbation $\\varepsilon$ is bounded by the magnitude of perturbation $\\varepsilon$. Using $\\Delta \\phi$ as evidence for generalization is not theoretically justified. I think loss function values should be added as supporting evidence too.\n3. I found some of the experimental settings unsatisfactory. For example, the training settings of ResNet-50 are very different compared to standard settings. I expect to see a comparison of different methods with standard learning rate decay, i.e. first 30 epochs: 0.1, 30-60 epochs: 0.01, 60-90 epochs: 0.001. Under this setting, BN should achieve around 76.1% top-1 accuracy. Or, the authors should provide a convincing explanation why not use the standard training setting."
            },
            "questions": {
                "value": "1. In Eq.16, only input mean normalization (IMN) is used, why not further normalize the input features with its variance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4349/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699148457162,
        "cdate": 1699148457162,
        "tmdate": 1699636405791,
        "mdate": 1699636405791,
        "license": "CC BY 4.0",
        "version": 2
    }
]