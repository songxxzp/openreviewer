[
    {
        "id": "8uauW8Phyo",
        "forum": "OivQg0YU3I",
        "replyto": "OivQg0YU3I",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7018/Reviewer_tz8Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7018/Reviewer_tz8Y"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new meta-RL scheme, which decouples exploration and exploitation separately, and optimizes exploration to maximize the reward of the subsequent exploit while optimizing exploitation to maximize episode reward. By doing so, the exploration policy can perform purely sacrificially exploration but still be grounded on the exploitation reward. The proposed method outperforms baselines in bandit and 9x9 treasure room domain. Authors also provided impartial limitations and discussions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea is intuitively well motivated, and clearly demonstrated. Paper is well-written and easy to follow.\n\n2. Code of the proposed method, configurations of baselines and environments are all available."
            },
            "weaknesses": {
                "value": "1. Although the idea is intuitively well motivated, it might still be good to have some experimental/theoretical motivations behind the intuition. \n\n2. Minor comments:\n\na. In Fig.3, there is too much space in the fifth row in the caption. \n\nb. Structure-wise, I feel like section 3 can be splitted into 2 different subsections, so that readers can have a clearer overview of it. For example, two subsections could be \u201cmeta-rl\u201d subsection and \u201cother works addressing exploration\u201d. Of course you don\u2019t have to change."
            },
            "questions": {
                "value": "You discussed DREAM in the related work, which also decouples exploration and exploitation (only explores for one episode then starts exploitation). Why didn\u2019t you compare it with DREAM as well? Do you think in tasks you performed, only exploring for one episode is enough?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7018/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698690471312,
        "cdate": 1698690471312,
        "tmdate": 1699636822775,
        "mdate": 1699636822775,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2P9shVfQ8N",
        "forum": "OivQg0YU3I",
        "replyto": "OivQg0YU3I",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7018/Reviewer_fuX7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7018/Reviewer_fuX7"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the exploration-exploitation trade-off in meta-RL. Specifically, it proposes the First-Explore framework, which separates the exploration and exploitation stages to achieve faster adaptation. Iteratively, the exploration policy is first rolled out, whose trajectories are used as the context of the exploitation policy. Then, the exploitation policy is rolled out, and the reward of the resulting trajectory will be used as the signal to train the exploration policy. With this mechanism, the exploration policy is incentivized to collect information for achieving high rewards while not being punished by potential negative rewards. Empirical results show that First-Explore can learn effective exploration policies in simple multi-arm bandit and treasure-finding domains, outperforming other meta-RL algorithms."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The strength of the work is its originality and significance. To the best of my knowledge, the idea of the paper is novel, and it seems to be the only meta RL algorithm that can explore efficiently in tasks with punitive rewards for exploration."
            },
            "weaknesses": {
                "value": "However, the writing of the paper is not ready for publication, which hinders readers from understanding it easily and is its biggest weakness:\n* First of all, the paper should be reorganized and have more details of the proposed framework in the main text. The current version only has a vague description of the framework in the main text, which doesn\u2019t provide adequate information for the reader to understand the method. I suggest the authors should at least describe a complete instance of the proposed framework in the main text.\n* Second, the paper is full of typos and grammatical/format errors, which create significant barriers for the readers. See below for a few examples before pape 5:\n1. In the second paragraph of the Introduction, there should be a comma before \u201ce.g.\u201d\n2. In the definition of coincidental exploration, \u201cexploitation\u201d should be \u201cexploration\u201d\n3. On page 2, \u201cslow, and\u201d should be \u201cslow and\u201d\n4. On page 2, \u201cIn the dungeon\u201d should be followed by a comma. Same for \u201cwhile exploring\u201d and \u201cwith the fixed dungeon\u201d on the same page\n5. On page 2, \u201cbe will\u201d should be \u201cwill be\u201d\n6. On page 3, \u201cEach training step\u201d should be \u201cIn each training step\u201d\n7. On page 3, \u201chas memory of\u201d should be \u201chas the memory of\u201d\n8. On page 3, \u201cthe exploratory episodes number\u201d should be \u201cthe exploratory episode number\u201d\n9. On page 4, \u201cafter every explore providing\u201d should be \u201cafter every exploration,providing\u201d\n10. On page 4, there are $\\pi_{\\text{explore},\\theta,c}$, $\\pi_{\\text{explore},\\theta|c}$. Be consistent with the notation.\n11. On page 4, how is the expected reward of an episode defined? Also, it seems that the reward part is missing in $\\mathbb{E}(\\tau)$.\n12. On page 5, \u201cWhile an innovation\u201d is not a complete sentence.\n13. On page 5, \u201cas replacement\u201d should be \u201cas a replacement\u201d\n14. On page 5, $k$ is not defined in \u201c\\mu_{\\{1,\\dots,k\\}}\u201d\n15. On page 5, \u201c(Fig. 2 A b)\u201d should be \u201c(Fig. 2 A), b)\u201d\n16. On page 5, \u201c(c)\u201d should be \u201cc)\u201d\n\nOther minor suggestions\n1. Consider adding a pseudocode of an instance of the proposed algorithm in the main text.\n2. Consider breaking down some long sentences to make them easier to understand.\n\nIn addition, I have some questions about the experiments. See the Questions below."
            },
            "questions": {
                "value": "1. Does the x-axis in the plots of Figure 2 include the arm pulls from both the exploration and exploitation policies? It seems that it doesn\u2019t. If this is the case, how is the comparison to UCB1 (and maybe other algorithms) fair?\n2. During training, how does First-Explore handle the context?\n3. How sensitive is the proposed method to the episode length? It may be beneficial to investigate this to understand the applicability of the method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7018/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698754179883,
        "cdate": 1698754179883,
        "tmdate": 1699636822654,
        "mdate": 1699636822654,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BqA6zYc7SJ",
        "forum": "OivQg0YU3I",
        "replyto": "OivQg0YU3I",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7018/Reviewer_obj6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7018/Reviewer_obj6"
        ],
        "content": {
            "summary": {
                "value": "In First-Explore, then Exploit: Meta-Learning Intelligent Exploration, the authors introduce a novel method of meta-learning focused on the idea of alternating explorative and exploitative policies to enable what they call \u201csacrificial exploration\u201d. They argue that a major problem with the current literature in Reinforcement Learning is that they attempt to explore and exploit simultaneously, and that their method, which employs a policy whose focus is exploration, eliminates this issue."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper proposes a cogent analysis of exploration in the modern Reinforcement Learning literature, and certain of its weaknesses, through a well-written review of the recent literature. It  introduces a reasonable framework for a meta-learning algorithm and demonstrates that using that framework, significant improvement in average rewards and state coverage can be achieved."
            },
            "weaknesses": {
                "value": "The paper\u2019s description of the novel method is extremely lacking in detail, to the point that it is not possible to even sketch pseudocode for their idea from the relevant section. Their analysis of exploration in the present literature, while cogent, lacks serious consideration of the benefits of what they call \u201ccoincidental exploration\u201d, especially as pertains to traditional frameworks for exploration centered on regret minimization. The meta-learning aspect of the paper is also somewhat doubtful\u2014given the modest description present in the paper, it appears that the \u201cmeta\u201d-RL policy which is learned is in fact simply a pair of transformer-based models, each of which act based upon the context of the trajectories seen in a given \u201ctraining run\u201d. That is not, to my knowledge, the usual sense of the term \u201cmeta-RL\u201d. The paper also fails to conduct fair experiments, running the proposed algorithm for 5x as long as the comparators, without adequate justification. Further, this fact is obscured into the appendix. In my view, this behavior represents an attempt to fraudulently represent the proposed algorithm."
            },
            "questions": {
                "value": "1. When the paper claims that First-explore is capable of achieving higher cumulative reward, is this across the training of the meta-RL policy, or only on the last run of the RL method?\n2. What is the view of the authors on exploration methods which work based upon exploration bonuses? Are such methods sacrificial, or coincidental? These methods certainly seem capable of sacrificing at least some degree of reward for the purpose of exploration. Are these not \u201cstandard RL\u201d?\n3. What is the basis of the claim that \u201cOnly meta-RL is both a) computationally tractable and b) can potentially achieve human-level sample efficiency in complex environments.\u201d?\n4. Did the authors try any tasks where, concordant with the claim that \u201cOn complex tasks, the policies need to be learnt together\u201d, the policies did not need to be learned together?\n5. Why was the fact that First-Explore used a transformer reserved for the results section?\n6. How does the \u201cnovel loss based on predicting the sequence of future actions conditional on the episode having high reward, which preliminary experiments showed improved training stability.\u201d relate to the loss function of Upside Down Reinforcement Learning (Shmidhuber, 2020) (https://arxiv.org/abs/1912.02875)?\n7. Does \u201chand-coded strategy of picking the arm with the highest mean reward seen\u201d mean the same thing as \u201cgreedy policy with respect to observed rewards\u201d?\n8. Can the authors explain why myopically optimal exploration would generally be worse than the so-called \u201coptimal sequence of explorations\u201d? It would seem that there could be many reasons to prefer the myopic strategy.\n9. I am concerned that this work is more akin to previous works on multi-task RL, wherein a policy is trained to be able to perform many tasks, than to meta-RL, where (to my knowledge) it is typical to train the new policy in a very significant way, not just by changing the context of a transformer model. Would the authors address this concern?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7018/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7018/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7018/Reviewer_obj6"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7018/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807312166,
        "cdate": 1698807312166,
        "tmdate": 1699636822529,
        "mdate": 1699636822529,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3SsXxtmzSw",
        "forum": "OivQg0YU3I",
        "replyto": "OivQg0YU3I",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7018/Reviewer_RsHA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7018/Reviewer_RsHA"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an approach for meta-exploration by having separate explore and exploit policies. The context collected by the explore policy is provided to the exploit policy, and the explore policy is optimized to maximize the exploit policy reward."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "While meta-exploration for RL is an important problem, there are issues/concerns I have with the current version of the paper, please see weaknesses."
            },
            "weaknesses": {
                "value": "1. Comparisons and environments\n\nThe paper claims that the presented method can effectively meta-explore, but I have numerous questions regarding the experiments. How does the full proposed approach perform on standard meta-RL benchmarks [1], and how does it compare to meta-rl algorithms like pearl [1] and RL2 [2] on these environments? Is the sample efficiency better because of enhanced exploration, or is there a tradeoff? The paper proposes separate exploration and exploitation policies for meta-learning, how does this compare to meta-cure which also uses a similar approach [3], on the cheetah/walker/hopper meta-envs from that paper ? \n\n2. Idea formulation \n\nThe idea of learning how to collect data/explore in order to get sufficient context to subsequently exploit is very common in previous meta-RL algorithms ([1,2,3]). The approach of having explicitly separate policies for exploration and exploitation has also been studied, showing distinct information-seeking behaviors in the exploration phase [3]. Given this, the main difference of the proposed approach is the use of the transformer architecture, but this choice isn't ablated, so it is unclear if this is really important/effective for RL control tasks. \n\n\n[1] : Rakelly, Kate, et al. \"Efficient off-policy meta-reinforcement learning via probabilistic context variables.\"\n[2] : Duan, Yan, et al. \"Rl $^ 2$: Fast reinforcement learning via slow reinforcement learning.\"\n[3] : Zhang, Jin, et al. \"Metacure: Meta reinforcement learning with empowerment-driven exploration.\""
            },
            "questions": {
                "value": "Please address the questions in the weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7018/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699250001022,
        "cdate": 1699250001022,
        "tmdate": 1699636822427,
        "mdate": 1699636822427,
        "license": "CC BY 4.0",
        "version": 2
    }
]