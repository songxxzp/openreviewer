[
    {
        "id": "RHXdH3Dqlz",
        "forum": "QhoehDVFeJ",
        "replyto": "QhoehDVFeJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7093/Reviewer_QZey"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7093/Reviewer_QZey"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel method for reconstructing animatable human avatars from videos. The proposed method, named EMA, models canonical shapes, materials, lights and motions separately using different neural fields. With an analysis by synthesis framework, those terms can be optimized using image level losses via a differentiable marching tetrahedra algorithm. A mesh-based representation can be distilled from this representation, which greatly improves its rendering efficiency. Extensive comparisons are conducted with previous methods. Improved results are observed on the H36M dataset and comparable results are observed on ZJUMOCAP."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Extensive experimental results.\n\nThis paper conducted many experiments with comparisons with previous baselines and detailed analysis."
            },
            "weaknesses": {
                "value": "* Better illustrations are needed.\n\nMany illustrative figures in this manuscript lack proper notation and are hard to align with the text. It would make the reader understand better about the technical details better if more details were included."
            },
            "questions": {
                "value": "* Novelty?\n\nThe proposed method combines existing methods [1] and [2] (and many other papers in the field). Although one of the major differences is the efficiency in rendering, the same improvement can also be theoretically achieved by training a SDF and extracting a water-tight mesh for rendering afterward. It would be great to have a more thorough discussion on the merit of the current pipeline as well as other potential advantages it brings.\n\n\n\n\n\t\t\t\t\n\t\t\t\n\t\t\n\n\n* Comparison with other efficient frameworks?\n\nOn the efficiency side, the joint optimization of mesh and SDF is interesting. However, there are also many other sparse structures used for speeding up the rendering efficiency like using layered mesh representation [] and sparse volumes [3, 4]. It would enhance the quality of the manuscript if some comparisons or discussions were added.\n\n\n[1] Jacob Munkberg, Wenzheng Chen, Jon Hasselgren, Alex Evans, Tianchang Shen, Thomas Mu \u0308ller, Jun Gao, and Sanja Fidler. Extracting triangular 3d models, materials, and lighting from images. In CVPR, pp. 8270\u20138280, 2022. \n[2] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, and Ira Kemelmacher- Shlizerman. Humannerf: Free-viewpoint rendering of moving people from monocular video. In CVPR, pp. 16189\u201316199, 2022. \n[3] Tianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. \"Instantavatar: Learning avatars from monocular video in 60 seconds.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16922-16932. 2023.\n[4] Edoardo Remelli, Timur Bagautdinov, Shunsuke Saito, Chenglei Wu, Tomas Simon, Shih-En Wei, Kaiwen Guo et al. \"Drivable volumetric avatars using texel-aligned features.\" In ACM SIGGRAPH 2022 Conference Proceedings, pp. 1-9. 2022.\t\t\n[5] Donglai Xiang, Fabian Prada, Timur Bagautdinov, Weipeng Xu, Yuan Dong, He Wen, Jessica Hodgins, and Chenglei Wu. \"Modeling clothing as a separate layer for an animatable human avatar.\" ACM Transactions on Graphics (TOG) 40, no. 6 (2021): 1-15."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7093/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697828218802,
        "cdate": 1697828218802,
        "tmdate": 1699636836927,
        "mdate": 1699636836927,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WjJFQvxYE6",
        "forum": "QhoehDVFeJ",
        "replyto": "QhoehDVFeJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7093/Reviewer_imbQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7093/Reviewer_imbQ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method for learning articulable human avatar models from image data. This is accomplished by modeling the representation as a signed distance function and jointly optimizing the canonical shape, material and lights (for appearance), and skinning weights (for motion). Because the representation is modeled as a signed distance function, it can be extracted into a textured mesh and animated with learned skinning weights in order to efficiently render the human in new poses. The result quality is compared to a number of baseline works, including those optimizing textured meshes directly, and those training volumetric representations. It is demonstrated that the proposed method leads to better quality, along with the ability to change lighting and train significantly more efficiently."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "In my opinion, the strengths of the method are as follows:\n1. The paper is described clearly, and the method makes intuitive sense. Optimizing the representation's geometry (SDF) and appearance (materials and lighting), and motion (skinning weights) jointly seems like the correct approach to learn an avatar from only image data. The optimization objective proposed makes sense and seems well-posed for solving this under-constrained task.\n2. The proposed retains additional control over various factors which is unlike the existing methods. For example, volumetric methods do not model materials and lighting and thus cannot support relighting. This allows the proposed method to be applied in different applications which are not possible for other methods."
            },
            "weaknesses": {
                "value": "In my opinion, the weaknesses of the method are as follows:\n1. I view the comparisons as not being extensive. For example, this concurrent work seems to solve the same problem [1], and they compare to other baselines which appear better than the ones used in this method. Additionally, there exist a number of methods which use the SMPL mesh for skinning weights driving motion [2][3] for which the quality appears quite good. Why are these approaches not compared to in this work, as it appears that these methods are able to generate better results qualitatively? If this method is focusing on human body avatars, why learn general skinning weights with a skeleton instead of using those from a known human body model, such as SMPL.\n2. For the efficient training, I'm not sure why the representation actually trains faster? It seems to use the entire image as opposed to individual rays as in volume rendering, and marching to find the surface also requires a number of samples of the SDF representation. This results in each iteration having more rays, and just as many samples per ray, so I don't understand why the method actually trains faster.\n\n[1] https://lukas.uzolas.com/Articulated-Point-NeRF/\n\n[2] https://machinelearning.apple.com/research/neural-human-radiance-field\n\n[3] https://tijiang13.github.io/InstantAvatar/"
            },
            "questions": {
                "value": "I have no additional questions on the manuscript. Overall, the paper proposes a method which makes sense and learns human avatars which have capabilities that other volumetric methods are not capable of, such as relighting, efficient rendering and training, and compatibility with graphics pipeline. However, I do not understand why the existing state-of-the-art methods in generating avatars have not been compared to. If it is because they use the SMPL template for driving the motion of the human avatars, I don't view this to be a limitation, since this paper also focuses on humans. Understanding why these methods were not included in the comparisons, or adding them as comparisons, would significantly strengthen the paper and lead me to increase my score.\n\n**Update after author response**\n\nAfter reading the author response, I still remain borderline on the paper. I understand not comparing to methods which are designed for reconstructing from a monocular video, and appreciate the addition of some comparisons here. Additionally, I appreciate the additional timing results. I have thus increased my score a bit for the paper. \n\nHowever, the justification for not using the SMPL model does not seem convincing to me. If the reason is non-rigid deformations such as clothing, then it needs to be explicitly demonstrated in the paper that this is improved by the proposed method. Additionally, other contributions (such as the correction MLP from Neuman) give the ability to model these types of clothing deformations with the SMPL weights."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7093/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7093/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7093/Reviewer_imbQ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7093/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698646897152,
        "cdate": 1698646897152,
        "tmdate": 1701024058028,
        "mdate": 1701024058028,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PmAfgyquHM",
        "forum": "QhoehDVFeJ",
        "replyto": "QhoehDVFeJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7093/Reviewer_PTA7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7093/Reviewer_PTA7"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new framework for modeling dynamic human avatars with neural fileds. In this work, three different group of neural fields are adopted to record shape, material and motion information respectively and optimized jointly. Specifically, this work employs an 8-layer MLP to model the SDF of the canonical shapes. A 2-layer MLP with hash-encoding is adopted for efficient material query. SNARF is introduced for skinning weights, and another 4-layer MLP is used for non-rigid modeling. Finally, these neural fields are integrated under the Linear Skining framework and further rendered by a differentiable renderer to fit target images. Experimental results demonstrate that this new framework can achieve superior rendering quality with less training and inference times."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The proposed method is technically sound. It is clever to integrating LBS model and PBR materials within Neural Fields for high-quality rendering.\n\n+ Employing environment lights and non-rigid models is also a good way to enhance the rendering results.\n\n+ The experimental results are convincing. This work achieves better results with less training and inference time."
            },
            "weaknesses": {
                "value": "- It is difficult to read the main maniscript without supplementary materials. For example, crucial information such as Nerf structure should be included in the main maniscript.\n\n- This work would be further strengthened if more in-the-wild results were provided."
            },
            "questions": {
                "value": "Here are some concerns:\n\n1. As the framework takes into account environmental lights and the non-rigid model, how does this method perform on in-the-wild data?\n\n2. Can this method be used for avatar modeling with soft cloth (e.g. wearing a dress)?\n\n3. What is the geometric precision of this method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7093/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698771556438,
        "cdate": 1698771556438,
        "tmdate": 1699636836681,
        "mdate": 1699636836681,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HQgIETQgjZ",
        "forum": "QhoehDVFeJ",
        "replyto": "QhoehDVFeJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7093/Reviewer_XfaE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7093/Reviewer_XfaE"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method called EMA (Efficient Meshy neural fields for Animatable human Avatars) for efficiently generating animatable human avatars from videos. The main goal is to overcome the shortcomings of existing volume rendering-based methods in terms of training and inference speed and to achieve compatibility with rasterization renderers for direct application to downstream tasks.The EMA method jointly optimizes explicit triangular canonical mesh, spatially varying materials, and motion dynamics through end-to-end inverse rendering. These components are encoded by separate neural fields, eliminating the need for preset human templates, rigging, or UV coordinates. The authors also use differentiable rasterization techniques to learn mesh properties and forward skinning, improving the efficiency of the method.Compared to existing methods, the EMA method has significant advantages in training and inference speed. It is highly compatible with rasterization renderers, has a short training time, and fast rendering speed. Experimental results show that the EMA method has competitive performance in novel view synthesis, generalization to novel poses, and training time and inference speed."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The EMA method achieves real-time rendering through efficient mesh rendering. Moreover, it computes the loss on the entire image, and gradients flow only on the mesh surface, resulting in improved training speed. Compared to volume rendering methods, the EMA method has a shorter training time."
            },
            "weaknesses": {
                "value": "1.\tThe EMA method employs neural networks to encode canonical geometry, materials, and motion models. In practical applications, the complexity of these neural networks may affect the inference speed. Have the authors conducted experiments in this regard, or attempted to use simpler or more efficient neural network architectures to balance the inference speed and reconstruction quality?\n2.\tThe EMA method learns a fixed environment light and uses a Physically-Based Rendering (PBR) material model. In practical applications, would this lighting and material modeling approach potentially limit the inference speed to some extent? Is it possible to use simpler lighting and material models to further improve efficiency?\n3.\tThe EMA method employs pose-dependent non-rigid offsets to compensate for non-rigid cloth dynamics. Does this modeling approach increase computational complexity and impact inference speed? Are there any other more efficient methods to handle non-rigid cloth dynamics? Judging from the video results, it seems that EMA might also be limited in terms of non-rigid cloth dynamic modeling?\n4.\tThe EMA method relies on skeletal pose tracking of the input video. So is it conceivable that the accuracy of pose tracking might affect inference speed and result quality? Has the author considered the performance and speed of the method in the case of inaccurate pose tracking, I think this will be of great help in practical applications?\n5.\tIn the experimental part, it can be seen that in the case of fast training (10 minutes), EMA has a satisfactory result (better than ARAH). However, under the same hour, the advantage seems not obvious. And as I said before, when the pose tracking quality is poor, the EMA is also greatly affected.\n6.\tIn addition, are there other methods that also use mixed representation training methods? The improvements brought by this method have not been well explained and proven in experiments."
            },
            "questions": {
                "value": "As stated above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7093/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698848196835,
        "cdate": 1698848196835,
        "tmdate": 1699636836579,
        "mdate": 1699636836579,
        "license": "CC BY 4.0",
        "version": 2
    }
]