[
    {
        "id": "dAz7rrHpgb",
        "forum": "iAW2EQXfwb",
        "replyto": "iAW2EQXfwb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6772/Reviewer_cq3K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6772/Reviewer_cq3K"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes NCERL, an ensemble Reinforcement Learning (RL) method for game level generation with more diversity. NCERL uses a set of different Gaussian actors which outputs actions in the latent space, and the output is decoded into different level segments by a GAN decoder. The final output segment is chosen with the probability generated by a learned selector. To encourage diversity between different actors, a regularizer of Wasserstein distance between policy distributions (and weighted by selector) is added to the reward. The paper also gives a modified gradient and convergence proof on the new reward, as the regularizer depends on the whole policy instead of a single action. On a well known level-generation benchmark, NCERL achieves comparable reward (measuring game-design goals) with better level diversity, and can also perform a well trade-off between diversity and reward by controlling its regularization coefficient."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. As this paper uses a latent space for policy and existing decoder for segment generation, the problem addressed by this paper is not only interesting, but actually quite general: how to achieve a trade-off between policy diversity and performance, with game level generation being one of its real-world applications.\n\n2. The paper has a sound theoretical basis, with a convergence proof for the new reward (which, similar to soft actor-critic, depends on policy distribution instead of action) and a rigorous, modified version of gradient update.\n\n3. The proposed work is scalable, with a well-designed and parallelized reward evaluation framework."
            },
            "weaknesses": {
                "value": "**Some details of the papers are not presented clearly enough.**\n\n1. Figure 1 has many math symbols and no caption on high-level ideas of each component; in addition, the meaning of $i\\leftarrow 2$ in the figure is unclear.\n\n2. There is no clear definition on what a state is in the paper; the readers can only speculate that it is a latent vector from the end of Section 2.1 (\"... a randomly sampled latent vector is used as the initial state\").\n\n3. There is no description on how Wasserstein distance is calculated. It is true that 2-Wasserstein distance between Gaussian distributions can be easily calculated, but it would be better if the Gaussian property can be emphasized at the beginning of Section 3.2, a formula can be given to make the paper self-contained, and \"2-Wasserstein\" instead of \"Wasserstein\" is specified.\n\n4. typos: in conclusion, bettwe -> better."
            },
            "questions": {
                "value": "I have two questions:\n\n1. In Table 1, the trade-off of NCERL between reward and diversity in some of the environments does not follow the trend; for example, in Mario Puzzle, the diversity with $\\lambda$ seems to have two peaks (0.2 and 0.5), and the reward at $\\lambda=0.1$ is the worst despite of low diversity. Could the author explain this?\n\n2. Currently, there is only comparison between NCERL, ensemble RL methods and non-ensemble RL methods, and the encoding is over GAN. What is the performance of non-RL solutions, such as scripted (possibly with learnable parameters) solution, or supervised/self-supervised learning over more recent generative models such as diffusion models or VAEs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6772/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6772/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6772/Reviewer_cq3K"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6772/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697508689277,
        "cdate": 1697508689277,
        "tmdate": 1699636781170,
        "mdate": 1699636781170,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9h3SCvHZeX",
        "forum": "iAW2EQXfwb",
        "replyto": "iAW2EQXfwb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6772/Reviewer_t3p4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6772/Reviewer_t3p4"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an ensemble reinforcement learning approach for generating diverse game levels. The approach uses multiple sub-policies to generate different alternative level segments, and stochastically selects one of them following a selector model. The paper also integrates a novel policy regularisation technique, which is a negative correlation regularisation that increases the distances between the decision distributions determined by each pair of actors. The regularisation is optimised using regularised versions of the policy iteration and policy gradient, which provide general methodologies for optimising policy regularisation in a Markov decision process. The paper's contributions are:\n1. The proposed ensemble reinforcement learning approach for generating diverse game levels.\n2. The novel policy regularisation technique that encourages the sub-policies to explore different regions of the state-action space.\n3. The regularised versions of the policy iteration and policy gradient algorithms that provide general methodologies for optimising policy regularisation in a Markov decision process."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "originality: the paper proposes a novel approach for generating diverse game levels using ensemble reinforcement learning and policy regularisation. The paper develops two theorems to provide general methodologies for optimizing policy regularisation in a Markov decision process. The first theorem is a regularised version of the policy iteration algorithm, which is a classic algorithm for solving MDPs. The second theorem is a regularised version of the policy gradient algorithm, which is another classic algorithm for solving MDPs.\n\nquality: the paper provides a detailed description of the proposed approach and the regularisation technique. The paper also provides theoretical proofs of the regularised versions of the policy iteration and policy gradient algorithms. \n\nclarity: the paper is well-written and easy to follow. The authors provide clear explanations of the proposed approach and the regularisation technique."
            },
            "weaknesses": {
                "value": "the proposed approach assumes that the reward function is known and fixed. However, in practice, the reward function may be unknown or may change over time. Therefore, the proposed approach may not be applicable in such scenarios.\n\nthe paper only considers a single game genre (platformer) and a single game engine (Super Mario Bros.). The proposed approach may not be directly applicable to other game genres or engines."
            },
            "questions": {
                "value": "Q1: can the proposed approach be directly applicable to 3D game levels or other types of game content? or what are the difficulties in this extension, such as complex reward design or high computational burden?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6772/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6772/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6772/Reviewer_t3p4"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6772/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697718374674,
        "cdate": 1697718374674,
        "tmdate": 1699636781032,
        "mdate": 1699636781032,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZhWv4N9YLa",
        "forum": "iAW2EQXfwb",
        "replyto": "iAW2EQXfwb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6772/Reviewer_qX6Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6772/Reviewer_qX6Q"
        ],
        "content": {
            "summary": {
                "value": "**Problem Setting**\n\nWe want to do level generation but we want to induce some diversity in how the levels are generated. Here the policies define level generators which build the level through an MDP.\n\n**Algorithm / NN Structure**\n\nThe policies are defined as mixtures of Gaussians, implemented by creating a set of sub-policies along with a weighting. The weighting itself id modelled by a selector policy, creating a form of hierarchy.\n\nThe sub-policies are regularized to be diverse from each other using a Wasserstein distance. This distance is clipped, encouraging policies to be diverse only if their decisions are too close. Sub-policies have Gaussian action heads. Regularization is implemented as an auxilliary reward.\n\nA regularized version of policy iteration of the policy gradient are presented. Thus the agent is trained not only to optimize for diversity in the current timestep, but in future timesteps via a reguarlized value function.\n\nThe practical implementation is built off SAC.\n\nExperiments are presented on the Mario level generation benchmark. Results show that NCERL is able to achieve comparable reward to other methods, and outperform in terms of diversity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper proposes a clean and thorough study of a method to induce diverse level generation. The idea is to define policies as a mixture of sub-policies, then regularize those sub-policies so that diversity is increased. While this is a straightforward idea, it is especially applicable in a domain such as level generation where diversity is desired in itself rather than as simply a means towards exploration. The quality of the writing and presentation is solid and clear. Theoretical results are presented re-deriving the policy iteration and policy gradient update explicitly in terms of regularizing the diversity between sub-policies, and proofs are presented regarding convergence. The significance of this work stems from its thorough theoretical contributions."
            },
            "weaknesses": {
                "value": "Because the experiments are largely domain-specific and improve on diversity rather than pure performance, the significance of this work is limited. \n\nWhile there are novel derivations and a clean interpretation of regularizing the policy gradient, the idea of representing an agent as sub-policies has been explored in fields such as skill discovery and hierarchical reinforcement learning, which were not referenced in this work. \n\nThe description of the domain is unclear to me as a reader, e.g. what is the action space of an agent generating Mario levels? What are the criteria used to evaluate reward and diversity? I would have liked to see examples of the generated levels."
            },
            "questions": {
                "value": "See above for questions related to the experimental section.\n\nThe section on asynchronous evaluation seems orthogonal to the main contribution of the work. Asynchronous RL has been explored in the actor-critic setting (e.g. A3C), which this work uses as it builds off SAC. Is there a specific connection between the asynchronous implementation and the novel contribution here?\n\nWhat does the behavior of the weighting-selector policy look like? It would provide more clarity into the method to showcase how often this selector policy utilizes specific sub-policies, or if certain sub-policies go unused.\n\nIt may help to label the other comparison methods in Figure 4."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6772/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778387012,
        "cdate": 1698778387012,
        "tmdate": 1699636780900,
        "mdate": 1699636780900,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rjDWhsidFM",
        "forum": "iAW2EQXfwb",
        "replyto": "iAW2EQXfwb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6772/Reviewer_NGar"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6772/Reviewer_NGar"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method for online generating diverse game levels through an ensemble of negatively correlated RL generators. The authors derived a policy update operator under the diversity bonus. Apart from that, the authors propose an async framework for speeding up the training. Experiments show that the method is able to generate a wide range of policies through tunning the diversity coefficient $\\lambda$."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is written in clarity. Hypotheses are well supported by the experiments.\n2. Originality looks good to me (or maybe I am not following the OLG line of research, but I study MARL diversity, in which no noticeable significantly similar methods to my knowledge)"
            },
            "weaknesses": {
                "value": "To my understanding, this method adds a reward bonus/diversity constraint to the diversity among the policies, where the proof is kind of established in the literature. The effect of adding diversity regularization is similar to the quality diversity methods, where you are pursuing optimal in the new reward space. The role of $\\lambda$ is close to the Lagrange multiplier in the dual formulation of the original problem with a diversity constraint. I think it is ok to include them as contributions to the paper but building theoretical analysis on the interactions among ensemble policies or regularization effect would be more interesting. My ratings are subject to change."
            },
            "questions": {
                "value": "It is similar to (adversarial) diversity in populations of policies that learn incompatible policies or impose distance(entropy) regularization. Can the authors provide their view of how it compares to population-based methods with diversity regularization?\n\n[1] Xing, D., Liu, Q., Zheng, Q., Pan, G., & Zhou, Z. H. (2021). Learning with Generated Teammates to Achieve Type-Free Ad-Hoc Teamwork. In IJCAI (pp. 472-478).\n\n[2] Lupu, A., Cui, B., Hu, H. &amp; Foerster, J.. (2021). Trajectory Diversity for Zero-Shot Coordination. <i>Proceedings of the 38th International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 139:7204-7213 Available from https://proceedings.mlr.press/v139/lupu21a.html.\n\n[3] Cui, B., Lupu, A., Sokota, S., Hu, H., Wu, D. J., & Foerster, J. N. (2022, September). Adversarial Diversity in Hanabi. In The Eleventh International Conference on Learning Representations.\n\n[4] Rahman, A., Fosong, E., Carlucho, I., & Albrecht, S. V. (2023). Generating Teammates for Training Robust Ad Hoc Teamwork Agents via Best-Response Diversity. Transactions on Machine Learning Research.\n\n[5] Charakorn, R., Manoonpong, P., & Dilokthanakul, N. (2022, September). Generating Diverse Cooperative Agents by Learning Incompatible Policies. In The Eleventh International Conference on Learning Representations.\n\n[6] Rahman, A., Cui, J., & Stone, P. (2023). Minimum Coverage Sets for Training Robust Ad Hoc Teamwork Agents. arXiv preprint arXiv:2308.09595."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6772/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813928192,
        "cdate": 1698813928192,
        "tmdate": 1699636780781,
        "mdate": 1699636780781,
        "license": "CC BY 4.0",
        "version": 2
    }
]