[
    {
        "id": "shRPd8uu46",
        "forum": "Zc2aIcucwc",
        "replyto": "Zc2aIcucwc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8594/Reviewer_43gK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8594/Reviewer_43gK"
        ],
        "content": {
            "summary": {
                "value": "This manusript proposed a curated dataset containing three subsets with different sizes. The whole datasets contains approximately 100 million molecules with more than 13 billion labels combining graph-level and node-level ones. Each molecules is accompanied with quantum mechancial properties calculated using either DFT or semi-emperical methods. A Python library called Graphium is built to facilitate the access and utilization of the proposed dataset for machine learning applications. Some tools such as handling missing data and label normalization in Graphium are designed to reduce the friction for users. Lastly, the authors provides benchmark results on the proposed dataset using common GNN models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed dataset is large and inclusive. In each one of the subset (ToyMix, LargeMix, and UltraLarge), at least one of the dataset contains 3D conformer of molecules and QM properties. As the 3D GNN models (e.g. Equivariant GNNs) are becoming increasingly important in the molecular machine learning community, 3D molecular conformer and corresponding labels are valuable.\n\n2. The Graphium library can be handy for researchers who wants to join the molecular machine learning community but are halted by lack of experience with data processing and model training. With all datasets curated in this work included in the Graphium library, users can easily focus on innovating model architectures. \n\n3. Many node-level labels are included in this dataset. Most conventional tasks in molecular machine learning are graph-level tasks such as molecular property prediction because of the scarcity of node-level labels."
            },
            "weaknesses": {
                "value": "1. One minor weakness is that the potential noise in the bioassay data. For the PCBA1328 dataset of LargeMix, the authors curated the data from PubChem by collecting molecules and their properties with regard to different bioassays. Due to the inherent noise associated with the data generation process with bioassay, model training (especially multitask training) can be difficult."
            },
            "questions": {
                "value": "1. For the PM6_83M, the authors mention that the QM properties of molecules are calculated using semi-empirical methods as reported in the original paper of PM6 paper. However, in secion D.4, the authors state that \"Using the 3D conformation optimized by the DFT, we further computed the following 3D descriptors as labels\". Can the authors confirm if they run DFT on top of the PM6 dataset or not? If they do run DFT, can they briefly discuss the DFT method they use here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8594/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698624804369,
        "cdate": 1698624804369,
        "tmdate": 1699637075302,
        "mdate": 1699637075302,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Hzy2uRPXHN",
        "forum": "Zc2aIcucwc",
        "replyto": "Zc2aIcucwc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8594/Reviewer_BQyU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8594/Reviewer_BQyU"
        ],
        "content": {
            "summary": {
                "value": "The authors have developed an extensive data repository and designed a graph learning library, \u201cgraphium\u201d that supports the training of foundational models for molecular modeling. Authors have combined molecular datasets from various sources with labels for tasks targeting graph and node-level properties. The evaluation shows that training graph-based models on large amounts of quantum mechanical data improves the downstream performance of the model in resource-starved contexts, for example, modeling biological molecules."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper provides an extensive description of all the datasets that have been used and how they relate to each other in terms of what tasks they model and the labels provided. Particularly, the paper provides an in-depth explanation of (in the supplementary resources) how chemical and structural properties might relate to biological function, which can explain how properties learned in a quantum mechanical setting might help in a biological context. \n\n+ Authors have made their work publicly available, including the datasets and the graph learning library, and providing multi-level, multi-label, and multi-task learning mechanisms. This form of modeling, as shown, is crucial in developing foundational models that generalize well to tasks with limited amounts of data available.  \n\n+ The authors discuss the positional encodings and distill the literature on relative and global positioning into their \u201cgraphium\u201d data modeling pipeline."
            },
            "weaknesses": {
                "value": "+ The work is an important contribution to the ML and molecular learning communities. However, it restricts itself to data curation and performance results for different GNN-based models. The paper is missing crucial insights that might help inform the researchers interested in developing foundational models. For example, does the new dataset help improve other existing methods? Why the multi-task setting does not work well for all the datasets, and what are the data properties resulting in different performances for different datasets? How does this work translate into applications for the real-world setting? \n\n+ The paper does not show a comparison against any other existing pre-trained molecular models. Moreover, it would also be helpful to highlight the performance against supervised SOTAs.\n\n+ Although the paper mentions transformer-based models, such as Graphormer or GPS [1,2], in the supplementary section, it does not provide any results for them in the evaluation section. Showing results for them may be an essential consideration because they would show potentially better scaling to data than standard message passing, which tends to suffer from the problem of over-smoothing when the number of layers goes beyond three. Although the paper mentions this limitation and plans to do the comparison in the future, it's crucial to define how transformer-style models scale with the data that has been curated. \n\n+ Continuing on the previous point, it would be nice to see a scaling curve that shows how increasing model complexity improves performance as we provide more data to it.\n\n\nReferences:\n[1] Benchmarking Graphormer on Large-Scale Molecular Modeling Datasets arXiv:2203.04810\n[2] Recipe for a General, Powerful, Scalable Graph Transformer arXiv:2205.12454"
            },
            "questions": {
                "value": "+ Authors mention that due to resource constraints, they only use a small fraction of the data to train the ULTRALARGE models; can they discuss how would the results change if the entire dataset is used? As mentioned by the authors, there is a skew towards the QM tasks. \nWould the model overfit those tasks when trained with larger datasets and show degradation in performance for biological tasks? \n\n+ It's unclear if the paper shows that the model can generalize to unseen tasks in the downstream setting. \n\n+ Minor point: I am curious to see how the newer equivariant GNNs [3, 4] (specifically in the molecular modeling space) interact with having more data available to them.\n\n\nReferences\n\n[3] E(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials https://arxiv.org/pdf/2101.03164.pdf\n[4] Equivariant Graph Attention Networks for Molecular Property Prediction https://arxiv.org/pdf/2202.09891.pdf"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8594/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8594/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8594/Reviewer_BQyU"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8594/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785584651,
        "cdate": 1698785584651,
        "tmdate": 1700681234950,
        "mdate": 1700681234950,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "urQMwpeHS3",
        "forum": "Zc2aIcucwc",
        "replyto": "Zc2aIcucwc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8594/Reviewer_qFVJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8594/Reviewer_qFVJ"
        ],
        "content": {
            "summary": {
                "value": "Having access to a free and open database of chemical compounds with their quantum features and biological activities. The main issue with the existing ones is the shortage of enormous data over various tasks, with many of them lacking experimental measures, or quantum and chemical properties. One know fact is that a little change in these properties can cause a major bioactivity behavior change. To end this, authors has collected a multiple number of datasets over molecules that will be a great help to build foundation models on this literature, e.g., models that are trained over multi-task and multi-level molecular datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The contribution of this paper has been successfully delivered to the general audience. It will open doors to having more robust and representative models with the help of pre-training. \n2. Advantages over non supervised methods for pre-training the model, such as contrastive learning, is important because of the \"activity cliffs\" phenomenon. \n3. Many intricacies are considered into this, such as having both quantum and bioactivity features which are important for the future research. \n4. Graphium library provides a reliable and tidy tool that gives a better pace to doing research."
            },
            "weaknesses": {
                "value": "1. It's not clear what are exact labels collected for each sample and how they can be useful."
            },
            "questions": {
                "value": "1. Please provides more samples of each dataset category for the sake of better illustration."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8594/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698875644467,
        "cdate": 1698875644467,
        "tmdate": 1699637075088,
        "mdate": 1699637075088,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DdjcChUWWu",
        "forum": "Zc2aIcucwc",
        "replyto": "Zc2aIcucwc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8594/Reviewer_jQ36"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8594/Reviewer_jQ36"
        ],
        "content": {
            "summary": {
                "value": "The paper presents seven novel datasets that push the boundaries in both the scale and diversity of supervised labels for molecular learning. The authors also introduce the Graphium graph machine learning library to simplify the process of building and training molecular machine learning models. The paper argues that building effective foundational models for molecular modeling requires supervised training with both quantum mechanical (QM) descriptions and biological environment-dependent data. Overall, this paper aims to explore the possibilities of foundational models in molecular machine learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper presents seven novel datasets that are orders of magnitude larger than the current state of the art, covering nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels, currently the largest of their kind.\n\n- The datasets are designed for the supervised training of foundation models by combining labels representing quantum and biological properties acquired through both simulation and wet lab experimentation. The diversity of labels facilitates efficient transfer learning and enables the construction of foundational models by improving their generalization ability for a wide range of downstream molecular modeling tasks.\n\n- The paper introduces the Graphium graph machine learning library to facilitate efficient training on these extensive datasets. This library simplifies the process of building and training molecular machine learning models.\n\n- The paper provides an anonymized repo for reproducibility commitment."
            },
            "weaknesses": {
                "value": "- The evaluated baselines are too simple/out-of-date in some sense. Since this paper is providing a data-driven platform for accelerating the research of molecular foundation models, it would be better to have included more SOTA baselines for comparison, which can help analyze the usefulness and effectiveness of this database better.\n\n- It would be better to have a well-organized website with readable documents that can help understand the details and setups of different datasets/components better.\n\n- More data splitting settings (e.g. mofit-based, system-based, scaffold-based, etc.) should be further studied. Now the OOD generalizability of GNNs across different molecule datasets still seem to be unclear."
            },
            "questions": {
                "value": "- I would be curious about how the models trained on larger datasets perform on smaller datasets (using the same task or different tasks). Also the performance of applying a well-trained model on a small dataset to larger datasets would be interesting to study to understand the few-shot learning ability of those models over these datasets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8594/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699133130094,
        "cdate": 1699133130094,
        "tmdate": 1699637074981,
        "mdate": 1699637074981,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LjHoqbxCuk",
        "forum": "Zc2aIcucwc",
        "replyto": "Zc2aIcucwc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8594/Reviewer_Zdj5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8594/Reviewer_Zdj5"
        ],
        "content": {
            "summary": {
                "value": "This paper present seven novel datasets with different sizes, to facilitate the application and development of molecular foundation model. Multiple baselines are conducted and some experimental results and analysis are provided on several downstream tasks. Furthermore, Graphium graph machine learning library is proposed to offer more convenient implementations of multi-task and multi-level machine learning methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper compiles a comprehensive benchmark for the evaluation of molecular models. The dataset covers a large variety of molecules and labels at different levels, with particularly comprehensive and complete data for quantum-related tasks.\n2. The Graphium library appears promising as a convenient tool for researchers or developers to perform fine-tuning on various tasks using different backbones."
            },
            "weaknesses": {
                "value": "1. The diversity of this benchmark remains a problem. For example, most of the contribution comes from the quantum related property data, which cover a limited part in molecular property prediction.\n2. The paper just collected existing data instead of creating new data, which limits the significance.\n3. Baselines and discussions are not thorough, leading to unconvincing results.\n4. Some empirical settings are not clear or strict, making the comparison or benchmark not solid."
            },
            "questions": {
                "value": "1. Although the molecular numbers reach the million scale, it appears that the majority of the contribution comes from quantum data. The tasks lack diversity, and there is an imbalance among the three scales of data (toymix, largemix, ultralarge). For instance, the largest ultralarge dataset solely comprises quantum tasks. It would be more comprehensive if it included more biological tasks, such as ADMET properties.\n2. Seems that this paper did not create any new data, instead, they only collected some existing data to organize to different levels or merge different datasets, right? In this case, the novelty and contribution is limited. \n3. From Table 1, we observe that training on multitasks aids in improving the performance of ZINC12k and Tox21, but it doesn't yield the same benefit for Quantum tasks (QM9). What could be the reason behind this phenomenon?\n4. If adding the Quantum tasks enhances the performance of biological tasks, why does the multitask performance for the PCBA_1328 dataset in the largemix dataset at Table 2 not surpass that of the single task setting? The data size may not be the final answer and further analysis may be necessary.\n5. The baselines are too simplistic, as they only include three different types of GCNs. More models involving various types of self-supervised methods, which are essential for a foundation model, should be included in the experiments as baselines for discussion.\n6. If this dataset is aimed at the foundation model, it could potentially achieve the best performance on various tasks(both quantum and bio-related tasks) across different scales. However, this paper only tests various networks on data of different scales dataset, and it lacks related experiments or discussions.\n7. More sophisticated split functions beyond random, such as scaffold, can be explored, as the out-of-distribution (OOD) problem is common in large models and requires careful study."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8594/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699195596883,
        "cdate": 1699195596883,
        "tmdate": 1699637074879,
        "mdate": 1699637074879,
        "license": "CC BY 4.0",
        "version": 2
    }
]