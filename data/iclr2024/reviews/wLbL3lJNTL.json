[
    {
        "id": "rwZTshcthQ",
        "forum": "wLbL3lJNTL",
        "replyto": "wLbL3lJNTL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5814/Reviewer_VgNz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5814/Reviewer_VgNz"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes representation learning in reinforcement learning using multi-modal data sources. It aims to enhance representation learning by constructing Recurrent State Space Models tailored with specific objectives for each modality. This work's contribution lies in optimizing the integration of low-dimensional modalities (like proprioception) with high-dimensional, noisy modalities (such as images) to enhance representation learning for RL. The paper suggests employing a reconstruction loss for proprioception data and a contrastive loss for image observations. This work performs experiments on a modified version of the DeepMind Control Suite (DMC) and Mujoco tasks such as Cheetah Run and OpenCabinetDrawer task from ManiSkill2. The results underscore that utilizing a combined representation with appropriate loss functions can improve the performance of RL-based methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This framework introduces a clear joint training framework for multi-modal reinforcement learning. It employs reconstruction loss proprioception data and contrastive losses for noisy high-dimensional inputs, such as images. The method is straightforward in both comprehension and implementation.\n- Extensive testing on various benchmarks as tasks and baseliens including model-free and model-based RL baselines."
            },
            "weaknesses": {
                "value": "- Firstly, if the joint representations (adding proprio and images) improves performance over learning an image-only or proprio-only representation, I do not find this surprising. It makes sense that adding more informations improves the performance.\n- Secondly, adding different losses for each modality, contrastive for images and reconstruction for proprioception, as the central contribution of this work is weak. What can however make this paper a stronger contribution is pursuing other sensor modalities (depth images, surface normals, segmentations, etc) and then exploring various appropriate losses there. As it stands, the paper is only applying a typical reconstruction-based loss to the proprio and contrastive to the image, which are the current norms in the field - no exciting surprise!\n- Thirdly, even though this work provides extensive experiment results, in many figures, the methods showing the effect of each loss, for instance `Joint(R+R)` and `Joint(CV+R)`, the performance of these seem to be on-par with each other e.g. `Figure 8, Figure 2, Figure 3` etc. \n- Even though its nice the huge amount of analysis performed in this work, the concluding story is very hard to digest, specially since there are many different acronyms to their proposed method such as `Joint(R+R), Joint(CPC+R), Joint(CV+R)`. In addition, some baselines are missing for some tasks (e.g. `Figure 5`) and it makes drawing a final conclusion hard.\n- I also disagree with the following statement made in the paper ` While many self-supervised representation ...  neglect other available information, such as robot proprioception`.  Adding proprioceptive state observations along with images is not novel in the field (especially in robotics)."
            },
            "questions": {
                "value": "- Except for combining multi-modal inputs with appropriate losses per each, are there any other interesting take-away observations from this work?\n- Is there a reason many of the model-based baselines are missing for the `OpenCabinetDrawer` task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5814/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698293006883,
        "cdate": 1698293006883,
        "tmdate": 1699636613468,
        "mdate": 1699636613468,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s2cS9WQbNH",
        "forum": "wLbL3lJNTL",
        "replyto": "wLbL3lJNTL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5814/Reviewer_ZjLm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5814/Reviewer_ZjLm"
        ],
        "content": {
            "summary": {
                "value": "The paper studies RL problems with multiple modalities of different nature, i.e., images and proprioception. The authors argue that reconstruction and contrastive objectives for representation learning, studied separately in prior work, are better tailored to each modality and combined in a joint fashion. This proposal is realized in the recurrent state-space model (RSSM) framework, where the authors extend the formulation to multiple modalities. To further highlight the strengths of each combination, the authors propose new datasets and tasks, and perform an extensive comparison against a variety of baseline models."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Extends the RSSM model to multiple observation models to account for multi-modalities\n- Introduces two datasets with specific additional challenges: VideoBackgrounds and Occlusions\n- Introduces a new Locomotion benchmark with a focus on egocentric vision for obstacle avoidance (6 tasks)\n- Performs additional experiments in the OpenCabinetDrawer task with variations in lighting and surroundings\n- Extensive comparison over a large collection of model families"
            },
            "weaknesses": {
                "value": "Nothing stands out, beyond the remaining questions pointed out in the limitations section, also raised in the questions below."
            },
            "questions": {
                "value": "**Missing technical discussion:**\n- Is it possible to combine both contrastive learning paradigms? Or, to alternate between the two objectives at each epochs based on a given metric? Looking again at the discussion leading up to Eq.2 and Eq.3, there's no clear reason to favor one over the other. Moreover, as the two equations are pretty similar, perhaps it hints at a more general form. (Could the CV term be an alternative to the reward-based regularizer?). It's a useful ablation study to study CV or CPC in isolation, but since the experiments show distinct advantages to each formulation, it's likely the agent can learn to combine the two flavors. (Now I also see no reason why there's no Joint(CV + CPC) or Joint(CPC + CV) in the experiments. Makes me wonder why the authors completely overlook this option.)\n\n**Presentation:**\n- Abstract, last sentence: please clarify at this point what is meant by \"common practice\", as explained in the 3rd paragraph of the introduction.\n- Section 3:\n    - Suggest to break up the paragraph before Eq.2, possibly using a bold header corresponding to the block for CPC.\n    - The inputs to the score functions seems to be flipped at the end of S3.1\n- Section 4:\n    - It would help to e.g. move the \"Representation Learning Methods\" paragraph to the beginning of the section before any of the figures to help read the legend.\n    - ProrioSAC -> ProprioSAC"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5814/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811929115,
        "cdate": 1698811929115,
        "tmdate": 1699636613356,
        "mdate": 1699636613356,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OrgegMS4Fn",
        "forum": "wLbL3lJNTL",
        "replyto": "wLbL3lJNTL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5814/Reviewer_G7AG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5814/Reviewer_G7AG"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of reinforcement learning from observations collected across different sensors, and in this work, specifically, image observations and robot proprioception. The representation learning method aggregates historical observations through the Recurrent State Space Model (RSSM). The main difference from prior work is that the latent representation is now trained to be predictive of observations from multiple sensors. The experiments are conducted in various simulated vision-based environments, including an ego-centric cheetah-run task with obstacles."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The experiments study a range of and introduce some new environments, from locomotion to manipulation with moving backgrounds and with occlusions. These settings are particularly challenging because they rely (1) on robust representation learning and (2) on both vision and proprioception.\n- The proposed approach tackles both of these challenges by learning a representation that is task-relevant and represents both modes of observation.\n- The observation that the joint representation leads to more efficient RL over concatenation is useful for practitioners."
            },
            "weaknesses": {
                "value": "- The experiments only look at image observations and proprioception as the two modalities. It would be interesting to see this approach applied to other sensor modalities.\n- The extension of RSSMs to model both image observations and proprioception is a straightforward one, which is the primary contribution of this work.\n- It seems like the correct loss for each modality varies quite a bit across domains.\n- I'm still unclear on details for some of the comparisons and results (see Questions)."
            },
            "questions": {
                "value": "- Why do you think there is such a gap between Joint and Concat, where Concat performs the same as ProprioSAC on the cabinet tasks? It seems like Concat should be able to produce any representation that Joint can. Does Concat eventually converge to the same performance as Joint in Fig. 5 if we let it train for longer?\n- Do the DenoisedMDP and DreamerPro comparisons utilize observations from both modalities?\n- In Fig. 6 (right), are the images reconstructed from a separately trained decoder as a way to probe the representations? \n- How are the losses for different observation modalities weighted against each other?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5814/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699032912460,
        "cdate": 1699032912460,
        "tmdate": 1699636613253,
        "mdate": 1699636613253,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wtIWV2cRsX",
        "forum": "wLbL3lJNTL",
        "replyto": "wLbL3lJNTL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5814/Reviewer_nyC5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5814/Reviewer_nyC5"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework to jointly learn representations from vision and proprioception sensors based on the recurrent state space model (RSSM). It systematically studies the ways to combine contrastive and reconstruction losses on different sensor inputs through comprehensive experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. In general, the writing is clear and easy to follow\n2. The experiments are solid and comprehensive. The experiment section and appendix present the results of various joint representation learning designs (CV, CPC, reconstruction) as well as different ablations (concat, image-only, state-only) and baselines (model-free, model-based) under different environment settings. \n3. Code is provided with good reproducibility."
            },
            "weaknesses": {
                "value": "1. While the overall logic is clear and smooth, some specific notations and figures are confusing. \n\n    (a) From the figure plotted in the main paper or appendix, it's very hard to draw any conclusions about which representation learning method is the best. Instead of presenting the curves, drawing some bar charts about the final performance average over different settings and environments can be more straightforward.\n\n    (b) The figures in the main paper (Figure 2, 3, 4) are interleaving with model-free/model-based and occlusion/locomotion, which makes it hard to understand what's been delivered\n\n    (c) There is no explanation of e.g, Joint(CV+R), Joint(CPC+R) which make the confusion that the \"+R\" is for reward reconstruction.\n\n2. The results are not clear or convincing enough to draw a strong conclusion as in the discussion section\n\n    (a) The environmental and experimental design is not delivered clearly. Please address Questions 1. a for clarification.\n    \n    (b) \"In the more difficult settings, i.e., Occlusions, Locomotion (Fig. 4), and OpenCabinetDrawer (Fig. 5), using a joint representation gives the largest benefits\", \"In the Locomotion experiments, the CPC approaches (Fig. 4) have a significant edge over reconstruction\", which is only true for model-based occlusion (Fig. 4), and for Fig. 5 the gain is unclear (see question 1. b).\n\n    (c)  \"In the Locomotion experiments, the CPC approaches (Fig. 4) have a significant edge over reconstruction\". First, the gain is not significant. Second, Locomotion's model-based results are missing.\n\n    Understandably, there may not exist a unified framework that works best for model-based and model-free RL. Given that many details are missing, the conclusion seems too strong and not rigorous enough. Some possible improvements e.g. separately discuss (i) model-free and model-based, (2) locomotion and manipulation, and (3) standard image and background changes, to make a less strong but more rigorous conclusion. Also DMC's results are very saturated, it might be more convincing to include more diverse domains (e.g. more Maniskill/RLBench/FrankaKitchen results).\n\n3. The formulation in equation (4) is not mathematically rigorous. If CPC is applied to estimate the MI between the current representation and the next observation, the KL part should be factorized differently but not naively apply equation (1). \n\n4. Typo in caption: Figure 9 should be model-based results."
            },
            "questions": {
                "value": "1. Task and experimental details\n\n    (a) Is the \"standard image\" /\"video background\" / \"occlusion\" suite all modified from environments in Table 1? Are the curves averaged in each suite? How many seeds do you run for each task? The curves in each figure have low variance, which doesn't look like an average as different tasks require very different sample complexities in each suite. If you normalized that, how the normalization was done?\n\n    (b) Are the Maniskill results model-free or model-based? Why only a SAC baseline is included?\n\n    (c) Why the locomotion's model-free results are missing?\n\n2. Model details\n\n    (a) For the model-free results, are you also reconstructing reward based on latent representation z?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5814/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699129217795,
        "cdate": 1699129217795,
        "tmdate": 1699636613119,
        "mdate": 1699636613119,
        "license": "CC BY 4.0",
        "version": 2
    }
]