[
    {
        "id": "1Q7z0UGiIO",
        "forum": "N1gmpVd4iE",
        "replyto": "N1gmpVd4iE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7720/Reviewer_11VQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7720/Reviewer_11VQ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a framework that combines large language models (LLMs) and reinforcement learning (RL) to create strategic agents for the Werewolf game. These agents can reason about deceptions and make strategic decisions. The framework outperforms other LLM-based agents and is robust against human players."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-structured and clearly articulates the problem, methodology, and results.\n2. The quality of the work is strong, supported by empirical evidence. The framework not only outperforms other LLM-based agents but also shows robustness against human players, thereby validating its effectiveness."
            },
            "weaknesses": {
                "value": "1. The approach mainly combines prompt engineering with reinforcement learning (RL), specifically tailored for the Werewolf game. It's unclear how this would inspire or be applicable to other tasks.\n\n2. The paper does not clearly justify the need for using RL for action selection. There are alternative methods, such as in-context learning. What is the added benefit of the extra training cost incurred by using RL?\n\n3. The paper lacks explanations on how credit assignment is handled in a multi-agent setting. Additionally, it does not specify the reward structure. The impact of the hyperparameter 'N', which represents the number of generated actions, on the results is also not discussed."
            },
            "questions": {
                "value": "Please check the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7720/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7720/Reviewer_11VQ",
                    "ICLR.cc/2024/Conference/Submission7720/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7720/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698300282985,
        "cdate": 1698300282985,
        "tmdate": 1700714376907,
        "mdate": 1700714376907,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zkSGHFR5sT",
        "forum": "N1gmpVd4iE",
        "replyto": "N1gmpVd4iE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7720/Reviewer_WD8F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7720/Reviewer_WD8F"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the study of Werewolf, a many-player hidden team language game, as a benchmark game for AI systems, particularly LLMs. They then propose a baseline agent that is a composition of three methods centered on the combination of LLMs and a reinforcement learning policy. Their baseline agent begins by using an LLM to reason about its current state. This state is then used to generate a set of candidate actions. And finally, a policy selects an action from the set. They present preliminary quantitative and qualitative results of their agent."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Studies a many-player hidden-team language game, a class of games that is understudied.\n- Includes an ablation study of their proposed agent's implementation. \n- Concurrent work studying the same game suggests it is a test domain with a lot of interest from the community."
            },
            "weaknesses": {
                "value": "- The motivational claims about the advantages of this benchmark are tenuous. \n  - \"Prior work on LLM-based ... grounded in credible information ... to make wrong decisions.\" This claim isn't supported, and/or is making a weaker statement about the fragility of all single-agent RL not just LLMs.\n  - \"Moreover, the competitive nature ... employ strategically diverse actions ... exploited by their opponents.\" This is mostly a tautology that doesn't say anything specific about Werewolf. \n  - This is a _language game_, it would be more useful to discuss the game-theoretic properties of the game as the new dimensions and compare it to previous benchmarks for motivation. \n  - A claim is made that it is impossible to achieve strong play in Werewolf without communication. This claim could be playing with non-language/simple policies, which would additionally build-out a set of baselines. I would expect that there is a non-communication equilibrium that works OK (\u00e0 la Hanabi conventions). \n- Hidden role games are analogous to ad hoc teamwork and opponent-policy belief/likelihood modeling and these are not discussed nor used as potential baselines.\n- All problems/concepts of diversity are punted to just asking the LLM to be diverse. No guarantees of diversity or notions of what kind of diversity. \n- The SelfPlay algorithm isn't well described and takes many changes from existing algorithms without analyzing their impact. \n  - Particularly, the population is seeded with a pool of policies biased with prompts based on \"predefined personalities\". It would be good to understand what role this population, and subset(s), plays in the success of the algorithm."
            },
            "questions": {
                "value": "- Why is reliability on a 1-10 scale? It would be useful to include the steps that led to implementation decisions in the appendix.\n- Are all of the four attributes (reasoning, role, confidence, and reference) generated by the deduction LLM necessary? Is there any data on ablations of this information?\n- Why are reliability and confidence separate and somehow being treated as additive/substitutive? This feels a bit awkward and unintuitive. \n- This is more of a comment about LLM work generally, but at this level of agent complexity we're basically at a cognitive architecture with short-term and long-term memory. I think this is worth considering in implementations, baselines, and related work.\n- Why is self-attention used on the action embeddings? A much more natural approach is just to learn a Q value function.\n\t- This would be more flexible also because it could consider infinite candidate actions.\n\n\nExp 5.1 \n- I find Fig 3 very challenging to read. Usually red is \"bad\" and in this case it's meant to be good. And for some reason the lose-rates against the different opponents for your method is underlined? Maybe that's just me, but I spent a while trying to pull this apart. The underlining also does not come up in the caption or text.\n- Could you please include error bars? \n- Cross play is a pretty coarse performance measurement.\n\t- Especially in a team-game where you've got each player playing the same policy across the team.\n\t- Maybe this just highlights the regularities persistent across each type of agent and how predictable they are? \n\t- A stronger notion would be regret, try and find the _worst_ performing case for your method when considering all other possible agent types. \n\t- Was an experiment done with heterogeneous teams? \n\n\nExp 5.2\n- If possible, i think showing the first game performance and then the cumulative game performance as separate metrics would be insightful. A performance as a function of context (number of previously games played) would be even better.\n- Why does w.o training and diversity perform worse than vanilla? Doesn't this suggest that something may be amiss with how you're doing \"deduction\".\n- How do you separate 80 people into the 16 evaluation categories? You mention dividing the people into 4 groups of 20 people? How are they distributed into the different categories? \n- Please include error bars and per-category sample sizes \n- Similar to the previous cross-play figure, I found this table took a while to really unpack.\n- I think with error bars included the claim about monotonic improvement with added components won't hold.\n- Did you try just without diversity? \n\n\nExp 5.3\n- Error bars, it's hard to know if there is any meaningful improvement without them."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7720/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7720/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7720/Reviewer_WD8F"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7720/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698420244701,
        "cdate": 1698420244701,
        "tmdate": 1700516720180,
        "mdate": 1700516720180,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ycRBIRx9si",
        "forum": "N1gmpVd4iE",
        "replyto": "N1gmpVd4iE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7720/Reviewer_GVoB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7720/Reviewer_GVoB"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on developing strategic language agents for the deception-based multiplayer game, Werewolf, using large language models (LLMs) and reinforcement learning. Werewolf involves hidden roles, imperfect information, deceptive communication, and requires both cooperation and competition. \n\nThe proposed agent has three main components:\n\n- Deductive reasoning to organize and analyze information to deduce hidden roles.\n- Diverse action generation using LLMs to prompt for strategically diverse candidates.\n- Reinforcement learning policy trained via self-play and against a population of agents.\n\nComprehensive experiments show the agent achieves strong performance by combining LLMs and RL, outperforming other LLM-based agents and being robust against exploitation by humans. The agent exhibits sophisticated emergent behaviors like bluffing and sacrificing, showing the ability to generate diverse strategic play."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The environment design is thoughtful and provides an interesting testbed for studying social\ndeduction skills.\n- Examining emergent behaviors like concealment, cooperation, bluffing and sacrifice reveals insightful dynamics.\n- The zero-shot transfer of the RL policy to new language models demonstrates promising generalization capabilities."
            },
            "weaknesses": {
                "value": "The environment is quite interesting, but my main qualms are with the methods.\n\n- More implementation details are needed for the multi-agent RL algorithm to fully assess and reproduce the approach.\n- Additional rigorous evaluations of the MARL method would strengthen the results, such as multiple training runs and assessing cross-population transfer.\n- Lack of baselines: The justification for the particular method is lacking.  What about alternate forms of prompts and decomposing reasoning? Are there ablations of the method that can help understand where the performance gains are coming from?\n- What are the generalization settings that the models are tested for? Can the model generalize to new forms of the game? What are the limits? Is the train and test distribution the same?\n- Can the LLM itself be used as a reward model as in [1, 2] to choose actions?\n- Analogously a discussion on an alternative method where an LLM is used as a reward model to train a separate agent is needed, what are the advantages of an LLM agent?\n- The similarities to prior work like Cicero diminish the novelty claims. The key difference seems to be using language for reward estimation.\n- Motivation for studying and more importantly improving the performance of agents in an environment that encourages deception and lying by agents is lacking.\n- No ethics statement: Improving the deception qualities of an artificial agent warrants discussion of ethical and societal implications.\n- Providing quantitative results for emergent behaviors would substantiate that the examples shown are representative and not cherry-picked.\n- I would love to see more of an analysis of the failure cases.\n\n[1] Gandhi, Kanishk, Dorsa Sadigh, and Noah D. Goodman. \"Strategic Reasoning with Language Models.\" *arXiv preprint arXiv:2305.19165* (2023).\n\n[2] Kwon, Minae, et al. \"Reward design with language models.\" *arXiv preprint arXiv:2303.00001* (2023)."
            },
            "questions": {
                "value": "I have specified the questions and suggestions with the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7720/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7720/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7720/Reviewer_GVoB"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7720/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817671732,
        "cdate": 1698817671732,
        "tmdate": 1700710404474,
        "mdate": 1700710404474,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2QO6WwqYC5",
        "forum": "N1gmpVd4iE",
        "replyto": "N1gmpVd4iE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7720/Reviewer_s3DR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7720/Reviewer_s3DR"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework for using reinforcement learning to develop strategic language agents for playing the game of Werewolf, a complex multi-agent environment that requires both cooperative and competitive interactions. Specifically, this paper uses a population-based mechanism to train RL, that is, using the data collected from the past self and opponents for training, and then uses this policy to select the optimal reply(actions) for itself among the diverse reply(actions) given by the LLM. It is similar to replacing the tree search part in ToT with RL policy to choose. In the experiment, their agent achieve high win raate aginst other LLM models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The selected game is very interesting. It is a complex general sum game. It is very intuitive and meaningful to use LLM + RL to play roles against human players.\n\nThe paper distinguishes itself by integrating LLMs with RL, effectively harnessing their combined capabilities to navigate the intricate dynamics of the Werewolf game.\n\nThe empirical results looks good, and demonstrating robustness against human players.\n\nThe ability of potential RL policy to zero-shot transfer between different LLMs is discussed, which is helpful for the flexibility and generalization potential of their models."
            },
            "weaknesses": {
                "value": "The experimental scope of this paper is limited by a limited number of tests and a narrow selection of baseline comparisons, which may not verify that the results are better than other current prompt engineering-based methods.\n\nThe description of the experimental part is not detailed enough and some details are missing.\n\nIn this particular task, RL requires both strong language understanding capabilities, to comprehend the intentions behind all possible actions, and the ability to solve reasoning tasks. This dual demand can potentially result in low learning efficiency or pose challenges in the learning processes."
            },
            "questions": {
                "value": "Among the results for human players, the LLM-based agent has a slight advantage in winning rate compared to human players. But important details are missing, like how do humans take input? Voice or text? Have you considered more realistic scenarios, such as expressions and tones in conversations?\n\nIn the zero-shot transfer section table 2, while the unified RL policy keeps a similar win rate across different LLM models, this is primarily due to the foundational capabilities of the combined LLMs ensuring a balanced action space. However, this doesn\u2019t prove the RL policy\u2019s potential applicability to various reasoning tasks. Can the author provide more evidence to verify this, such as transferability between different games?\n\nDoes the author consider changes in the same game but different settings? For example, increasing the number of players from seven people to eight? Or adjust the player\u2019s role in the game? Can the proposed framework handle this situation? Does the RL policy need to be collected and trained again?\n\nDoes the selection of the RL policy raise concerns about consistency? For instance, if a player is assigned the role of a werewolf and chooses to lie and impersonate a different identity during the daytime conversation on the second day, should the lies told in subsequent days\u2019 statements be consistent?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7720/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699102643260,
        "cdate": 1699102643260,
        "tmdate": 1699636940913,
        "mdate": 1699636940913,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SgRp4sTrhR",
        "forum": "N1gmpVd4iE",
        "replyto": "N1gmpVd4iE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7720/Reviewer_Np2i"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7720/Reviewer_Np2i"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a new state-of-the-art LLM-based agent for the Werewolf game. The method is novel in that it combines large-language models with reinforcement learning over an action space that is proposed by the language model. The RL component is trained using a population approach. The authors demonstrate that the method outperforms existing baselines, and is not exploitable if one of the LLM-based players is replaced by a single human player. Emergent capabilities are analysed qualitatively."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- To the best of my knowledge this method for combining RL with LLMs is novel. It also seems to be to be fairly general and I could imagine it being profitably applied to other domains. \n- The method is well-described and sufficiently many algorithmic details are provided for it to be reproducible in future work. \n- The method compares to strong, reasonable baselines and achieves state-of-the-art results in a well-designed round-robin tournament. \n- The qualitative analysis is clear and interesting, providing insight into the capabilities of the agent. \n- This paper (along with concurrent work by Xu et al.) establishes Werewolf as an interesting new evaluation domain that tests hitherto unexplored properties of LLM-based agents."
            },
            "weaknesses": {
                "value": "- Mischaracterisation of the Cicero algorithm. It is not the case that this algorithm defines arbitrary language actions and then chooses from these. Rather, the algorithm uses a large language model for open-ended policy-conditioned dialogue, and then uses an RL model to choose from the (game-predefined) actions. Therefore none of the authors' baselines are similar to Cicero. This weakness is mitigated by the fact that the authors' method is meaningfully different to Cicero in any case, but they should take care to characterise these differences more precisely. \n- The human benchmarking results are rather weak, and some of the conclusions about robustness in this context feel like overclaiming to me. The real test of robustness would be to introduce one AI player in a game involving 6 other human players (as was done in the Cicero paper, for instance). Instead, the authors do the opposite, introducing one human into a game with 6 AIs. It it unclear whether this is a good test of robustness, because if the AIs play sufficiently out of distribution with respect to the human, it may be very hard for the human to have a sizeable degree of influence on the game. The authors should make it clearer earlier on in the paper that the robustness results are limited to the single-human setting, and discuss the limitations of this choice in the results section. \n- There is no ethics statement accompanying this paper, yet the authors are developing agents which have the incentive to bluff humans and to collaborate against humans. While I strongly believe that this research should be conducted, and I believe it can be ethically justified from many angles, it is beholden on the authors to make these arguments. Please include an ethics statement in any future version of the paper. \n- There are some missing literature citations that it would be good to include e.g. https://arxiv.org/abs/2305.19165, https://arxiv.org/abs/2303.00001."
            },
            "questions": {
                "value": "See \"Weaknesses\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7720/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699128089483,
        "cdate": 1699128089483,
        "tmdate": 1699636940817,
        "mdate": 1699636940817,
        "license": "CC BY 4.0",
        "version": 2
    }
]