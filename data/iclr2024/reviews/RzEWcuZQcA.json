[
    {
        "id": "cOZ9zE4sbn",
        "forum": "RzEWcuZQcA",
        "replyto": "RzEWcuZQcA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2210/Reviewer_iT8Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2210/Reviewer_iT8Z"
        ],
        "content": {
            "summary": {
                "value": "I have identified potential research integrity concerns in the current submission. Specifically, there appear to be significant overlaps with the supplementary document of a previous publication: 'Huang, C., Li, M., Cao, F., Fujita, H., Li, Z., & Wu, X. (2023). Are graph convolutional networks with random weights feasible?. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3), 2751-2768.' Notably, the proofs of Theorem 2 (completely copied and pasted from Huang et al's work) and the mathematical expressions from Eq. (16) to Eq. (22) resemble the contents of this existing work. Surprisingly, the authors did not reference this paper in their bibliography.\n\nDue to these potential ethical concerns, I am refraining from commenting on the technical aspects of the submission at this time."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Given these significant research integrity concerns, I find it imperative to address these issues before proceeding with a detailed technical evaluation."
            },
            "weaknesses": {
                "value": "Due to these potential ethical concerns, I am refraining from commenting on the technical aspects of the submission at this time."
            },
            "questions": {
                "value": "Due to these potential ethical concerns, I am refraining from commenting on the technical aspects of the submission at this time."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "* The proofs of Theorem 2 are DIRECTLY copied and pasted from copied and pasted from the supplementary material of the previously published work: \"Huang, C., Li, M., Cao, F., Fujita, H., Li, Z., & Wu, X. (2023). Are graph convolutional networks with random weights feasible?. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3), 2751-2768.\".\n\n* The mathematical descriptions from Eq. (16) to Eq. (22) are copied and pasted from  \"Huang, C., Li, M., Cao, F., Fujita, H., Li, Z., & Wu, X. (2023). Are graph convolutional networks with random weights feasible?. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3), 2751-2768.\" (Section 3.3, left hand side of page 2755)\n\nIt is concerning to note such direct overlaps without appropriate citation or acknowledgment. Given these significant research integrity concerns, I find it imperative to address these issues before proceeding with a detailed technical evaluation."
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723227644,
        "cdate": 1698723227644,
        "tmdate": 1699636154968,
        "mdate": 1699636154968,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OjlhtEZuTj",
        "forum": "RzEWcuZQcA",
        "replyto": "RzEWcuZQcA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2210/Reviewer_btEt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2210/Reviewer_btEt"
        ],
        "content": {
            "summary": {
                "value": "HashGIN is a novel graph neural network (GNN) model that achieves state-of-the-art results on graph classification tasks with significantly less training time and memory cost. The key innovation of HashGIN is to use random hash functions to approximate the injective phase in the Weisfeiler-Lehman test. This allows HashGIN to be trained with a single epoch of gradient descent, which is much faster than traditional GNNs that require multiple epochs to converge."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Here are some of the specific advantages of HashGIN:\n\nFaster training: HashGIN can be trained with a single epoch of gradient descent, which is much faster than traditional GNNs that require multiple epochs to converge.\n\nMore efficient memory usage: HashGIN only needs to store a single hash table for each graph, which is much more efficient than traditional GNNs that need to store a large number of intermediate matrices for each graph.\n\nState-of-the-art accuracy: HashGIN has been shown to outperform state-of-the-art GNN models on a variety of benchmark datasets."
            },
            "weaknesses": {
                "value": "NA"
            },
            "questions": {
                "value": "1. How does HashGIN perform when compared to graph transformer models such as GAT?\n\n2. How does the hash layer incorporate graph structure in the learning process?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698864058914,
        "cdate": 1698864058914,
        "tmdate": 1699636154884,
        "mdate": 1699636154884,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iNk4usg2If",
        "forum": "RzEWcuZQcA",
        "replyto": "RzEWcuZQcA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2210/Reviewer_bYNF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2210/Reviewer_bYNF"
        ],
        "content": {
            "summary": {
                "value": "HashGIN is proposed which a GNN architecture with random weights for unsupervised feature extraction on top of which a ridge regression is trained for graph classification after sum-pooling the extacted node features. The graph convolution layer is called k-HASH which has 2 steps: 1) k different (random) hashes are computed and concatenated for each of the node features, 2) afterwards sum-aggregation is performed over neighbours (with self-loop). Each random hash function is implemented as an MLP layer with weights and bias drawn from a uniform distribution. Theoretical results are stated regarding injectiveness and approximation guarantee. Results on graph classification benchmarks show improved performance over common baselines from previous work."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "_Idea_: The idea of using random hashes in GNN is interesting\n\n_Performance_: The results show better performance than end-to-end trained GNN baselines and favourable training complexity"
            },
            "weaknesses": {
                "value": "_Clarity_: I found the theoretical aspect of the paper completely incomprehensible. New equations pop out of nowhere without references or justification in several locations, statements are made without justification and unfollowable reasoning (see below for detail)\n\n_Novelty_: The k-HASH function is implemented as the concatenation of k MLP heads with random weights and bias. After that sum-aggregation is performed; then both steps are repeated.\nObservation 1: Concatenating k MLP heads is exactly equivalent to having one MLP head with k-times the dimension.\nObservation 2: repeating steps 1 and 2 multiple (say L) times reduces to applying a preprocessing MLP layer, and then L-1 steps of GCN layers, due to the exchangeability of the aggregation and linear projection steps. Hence, the used model is nothing other than GCN with random parameters.\n\n_Theory_: As I mentioned the theory is completely incomprehensible.\n\nFor example, I have no idea what Theorem 3 is trying to say, possibly something along the lines that there exists a k-HASH function (which is an MLP as discussed above) which is injective over multisets. This already contradicts Lemma 7 in [1], which states that ReLU MLP with sum-aggregation can't be injective. The proof is similarly hard to interpret. It starts off with saying \"The injectiveness of multiset X \u2282 X is a super problem of \u201cset membership problem\u201d\". Then, the majority of the \"proof\" is essentially a summary of Appendix B, which on the other hand, is almost word-by-word plagiarized from Section 5 in [2], it discusses the set membership problem. After this discussion, the inclusion of k-HASH functions somehow proves the original statement.\n\nI had similar issues all throughout this section. Another example is equation (16), I found the whole preceding discussion completely unintelligible, and I am not sure where the authors got the idea from that such a representation holds for functions on graphs? Also I have no idea what this $f: V \\to R$ is that we trying to approximate here. Before it is stated that \" the ideal function of graph classification that our model wants to approximate is actually the summation of these functions\". What functions? What is the ideal function of graph classification? Also I am not sure why a citation of Sobolev spaces needed after the condition that the derivative of the activation function is integrable.\n\n_Reproducibility_: Only the actual results are interesting, but no reproducibility statement or code is provided to reproduce the results.\n\n\n[1] Xu, Keyulu, et al. \"How Powerful are Graph Neural Networks?.\" International Conference on Learning Representations. 2018.\n\n[2] Kopparty, Swastik. \"Lecture 5: k-wise independent hashing and applications.\" Lecture notes for Topics in Complexity Theory and Pseudorandomness. Rutgers University (2013)."
            },
            "questions": {
                "value": "Please see above paragraph."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Appendix B is almost word-by-word copy and pasted from Section 5 in the lecture notes [2] referenced above."
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699390077325,
        "cdate": 1699390077325,
        "tmdate": 1699636154819,
        "mdate": 1699636154819,
        "license": "CC BY 4.0",
        "version": 2
    }
]