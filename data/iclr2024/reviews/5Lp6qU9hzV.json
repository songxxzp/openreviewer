[
    {
        "id": "306F58inqd",
        "forum": "5Lp6qU9hzV",
        "replyto": "5Lp6qU9hzV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4632/Reviewer_vhmU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4632/Reviewer_vhmU"
        ],
        "content": {
            "summary": {
                "value": "This paper models the task of AI-generated text detection as a partial Positive-Unlabeled (PU) problem and formulates the Multiscale Positive-Unlabeled (MPU) training framework to address the challenging task of short text detection without sacrificing long texts. In the PU context, this paper proposes length-sensitive Multiscale PU Loss, where a recurrent model in abstraction is used to estimate positive priors of scale-variant corpora. Besides, this paper introduces a Text Multiscaling module to enrich training corpora. Experimental results can demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- It is interesting and meaningful to investigate the task of AI-generated text detection.\n- It is a novel perspective to address the task of AI-generated text detection by taking it as a partial Positive-Unlabeled (PU) problem.\n- A novel Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the challenging task of short text detection without sacrificing long texts.\n- A Text Multiscaling module is also introduced to enrich training corpora.\n- Experimental results are extensive, which can demonstrate the effectiveness of the proposed method.\n- Ablation studies also support the proposed method."
            },
            "weaknesses": {
                "value": "- The writing of this paper should be further polished. For example, the double quotation mark was mistakenly used in the paper. For the typeset, there are too many tables that record the experimental results in the Experiment section. Maybe some tables can merged into a single one. It would be better if some tables could be transformed into figures. There are some words that are on the 10th page, but the requirement is that the main content cannot exceed 9 pages. \n- The review of related work can be further improved. Section 2.2 talks too much about PU learning. I think it is not necessary to provide the unbiased PU loss and the non-negative PU loss in such a detailed manner. In Section 1 and Section 2, some parts could be arranged into a single Related Work section."
            },
            "questions": {
                "value": "Please check the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4632/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4632/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4632/Reviewer_vhmU"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4632/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698652033406,
        "cdate": 1698652033406,
        "tmdate": 1699636442837,
        "mdate": 1699636442837,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vHEQi1t1mZ",
        "forum": "5Lp6qU9hzV",
        "replyto": "5Lp6qU9hzV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4632/Reviewer_PQsn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4632/Reviewer_PQsn"
        ],
        "content": {
            "summary": {
                "value": "This paper is motivated by the challenge of classifying short texts as either human or machine generated, i.e. fake text detection of short documents. The paper suggests treating short documents from machines as *unlabeled*, under the premise that such texts are in fact indistinguishable from human-written ones in some cases. From this assumption, the \"hammer\" of positive-unlabeled (PU) learning is applied to the problem. Specifically, a length-sensitive PU approach is proposed to account for the prior probability of positive-labeled data, which requires a \"multi-scaling\" data augmentation (sentence-level masking) to introduce a variety of sentence lengths at training time. On the TweepFake dataset, the approached approach outperforms a BERT-based classifier by 0.3 in terms of accuracy. On the more recent HC3 dataset, the proposed approach appears to outperform BERT-based classifiers by about 1% in accuracy for English and Chinese on full documents with larger improvements on sentence-level tasks (e.g., 85% vs 81% for a vanilla classifier for HC3-En-Sent)"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper addresses an increasingly important application, namely the automatic detection of machine-generated texts.\n* The approach appears to outperforms BERT-based classifiers across two benchmarks, both on long texts and at sentence-level detection. The improvements on sentence-level tasks are notable and appear fairly consistent across datasets and languages.\n* Ablations of the proposed approach are reported, showing the benefits of each component of the pipeline."
            },
            "weaknesses": {
                "value": "* The motivation for using a PU framework for fake text detection is not entirely clear. Conceptually, although certain short texts may be hard to distinguish as AI generated, pretending that they are unlabeled seems like an odd choice. Is there evidence that such shorts texts actually harm learning of a classifier using typical proper losses? \n\n* If the benefit is actually prioritizing learning on examples with less irreducible uncertainty, might other frameworks besides PU be appropriate as baselines? (e.g., noise-aware losses)\n\n* How does the use of the PU loss impact calibration? Uncertainty estimation is important for downstream use cases of AI text detectors.\n\n* S2.3 is difficult to understand. There is a lot of complexity without sufficient motivation or intuition for what it is aiming to accomplish, or discussion of alternate (simpler) approaches.\n\n* Eqns (12) -- (14) introduce a lot of unnecessary notation that is never re-used elsewhere in the paper."
            },
            "questions": {
                "value": "* It's unclear why a recurrent model is necessary in S2.3. Would an attention-based model also work here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4632/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698780761262,
        "cdate": 1698780761262,
        "tmdate": 1699636442743,
        "mdate": 1699636442743,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cKyMuVwpXX",
        "forum": "5Lp6qU9hzV",
        "replyto": "5Lp6qU9hzV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4632/Reviewer_scbq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4632/Reviewer_scbq"
        ],
        "content": {
            "summary": {
                "value": "It is difficult to classify shorter text as AI or human-generated text due to simple structure or phrasing. Motivated by this, the paper presents an innovative approach to classifying text as either AI or human-generated by framing it as a partially unlabeled (PU) task. The authors introduce a length-sensitive prior that complements the original PU loss function. They leverage a recurrent discriminator to determine the extent of unlabeled properties in each instance and introduce a multi-scale training strategy involving random sentence removal based on a Bernoulli distribution. The experimental results demonstrate that this approach yields better performance for short texts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n2.  With the increasing popularity of LLMs, classifying AI or human-generated text is a crucial task. Short texts present a unique challenge due to their simplistic structure, making this research particularly relevant.\n3. The motivation behind formulating AI-generated short-text classification as a partially unlabeled task is reasonable and aligns with the properties of classifying short texts.\n4. The introduction of a PU loss function that takes text length into consideration is a novel and promising contribution to the field."
            },
            "weaknesses": {
                "value": "1. It's worth noting that some hyper-parameters require careful tuning for optimal performance. The authors should consider discussing the practical implications of tuning parameters like \u03b3, especially when dealing with different large language models.\n2. To enhance the paper's overall impact, it would be more convincing if the authors conducted an analysis of the performance of general recurrent language models. This analysis could include examining how well such models align with ground-truth labels and providing case studies to illustrate their effectiveness."
            },
            "questions": {
                "value": "1. In Table 4, it's intriguing to observe that BERT-MPU performs better than Roberta-MPU on full text but worse on short text. The authors could provide further insights or hypotheses as to why this performance gap exists. This could help readers better understand the nuances of the approach and its applicability in different scenarios.\n2. It would be interesting to know whether the model's performance on classifying short texts would improve if the training dataset only consists of short texts.\n3. Does the proposed method harm the performance of extremely long texts? For extremely long text framing classification as a PU task may not be a good option."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4632/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830911910,
        "cdate": 1698830911910,
        "tmdate": 1699636442600,
        "mdate": 1699636442600,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aoaNCgIdXm",
        "forum": "5Lp6qU9hzV",
        "replyto": "5Lp6qU9hzV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4632/Reviewer_o5iQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4632/Reviewer_o5iQ"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenge of detecting AI-generated text, which is particularly difficult for short texts. The authors acknowledge that some short texts generated by AI closely resemble human-generated ones and should be labeled as \"unlabeled.\" To tackle this issue, the paper proposes a partial PU method designed for the detection of both long and short texts. Additionally, the authors introduce a corresponding loss function and employ a recurrent model to estimate the prior of P data. The experimental results demonstrate significant success."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The problem tackled in this paper is novel. The novelty of the problem addressed in this paper lies in the unique challenge of detecting AI-generated text, especially in the context of short texts. This is an important and emerging area of research with potential applications in various domains.\n\n2. The proposed method is innovative, especially in its consideration of prior estimation dynamically. The experimental results are also promising.  The experimental results demonstrate the effectiveness of the proposed method, which further strengthens the validity of the approach.\n\n3. The paper is well-written and presents the concepts clearly."
            },
            "weaknesses": {
                "value": "1. The organization of the paper could be improved. Section 2 contains a substantial amount of background information (Sections 2.1 and 2.2), and some content overlaps with Section 1. It is advisable to reorganize and eliminate redundancy for better flow and clarity.\n\n2. There are some instances of misleading information in Section 2.2 regarding the estimation and expectation of PU learning. Specifically, the description above Eq1 poses a problem as \\hat R represents an estimation rather than an expectation. Similarly, Eq 2 may hold for expectation but not necessarily for empirical estimation. Additionally, \\tilde \\pi should ideally be identical to \\pi for the expected version of Eq 3 to be valid. These points should be clarified and corrected for accuracy."
            },
            "questions": {
                "value": "What specific factors contribute to the difficulty in detecting short text? The paper seems to suggest that the challenge lies in the resemblance between AI-generated short texts and human-generated ones. If this is the case, is it truly necessary to distinguish between them? Additionally, is it feasible to accurately differentiate between them?\n\nCould you provide further insights into why \\tilde \\pi exhibits variation with respect to text length? It appears that it should reflect a characteristic of the dataset's distribution rather than the length of individual data points. Clarification on this point would enhance the understanding of the methodology."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4632/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699189631481,
        "cdate": 1699189631481,
        "tmdate": 1699636442510,
        "mdate": 1699636442510,
        "license": "CC BY 4.0",
        "version": 2
    }
]