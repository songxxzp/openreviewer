[
    {
        "id": "tDZtG8KeFG",
        "forum": "9JxQyat11M",
        "replyto": "9JxQyat11M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5022/Reviewer_KVmG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5022/Reviewer_KVmG"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes GC-CLIP to improve the zero-shot transfer performance for image classification tasks at inference time. It uses the existing open vocabulary detector (OWL-ViT) to select the bounding box of the main object, and proposes Multi-Margin Box Augmentation (MAug) to avoid losing potentially useful context information.\n\nThe result is positive but not significant on ImageNetS919 and CUB classification benchmarks, in which the gap is larger on datasets particularly for small objects."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The method is simple and shows positive results.\n\nAblations are comprehensive and solid.\n\nBoth qualitative and quantitative results are supplied.\n\nWell written and easy to read."
            },
            "weaknesses": {
                "value": "ImageNetS919 and CUB are carefully selected as the benchmarks, but the number of the benchmarks are usually too limited for zero-shot classification evaluations. For more solid results, it would be nice to also report results on common image classification tasks, such as ImageNet, VTAB, and OOD benchmarks (e.g. ObjectNet) etc.\n\nThe cost of this method was not clearly mentioned, which is meaningful given an additional detector is needed during the inference time.\n\nIn Table 2, the comparison between CALIP and GC-CLIP is not solid: we may be able to also say \u201cCALIP can improve performance over vanilla GC-CLIP\u201d (if this is the truth). The result in Table 2 is also hard to compare with Table 1. So I would suggest reusing the same setup as Table 1 to compare CALIP and GC-CLIP for a clearer result.\n\nEven though the empirical results are positive, the improvement looks a bit marginal, which might be less insightful to the community."
            },
            "questions": {
                "value": "as mentioned in \u201cWeaknesses\u201d:\n- could we report more results from more common image classification benchmarks?  \n- could we report the additional inference cost?\n\nmild comments:\n- In 5.4, could we also show the scores of the examples? So that we could understand a bit more possibly the confidence of the model (e.g. whether a smaller score could indicate that the model needs more context for classification?).\n- In 5.2: \u201cToo tight bounding boxes can make the models have unclear information\u2026\u201d instead of \u201c...having\u2026\u201d\n- In 5.4: the format of quotes seems wrong: \u201dland\u201d and \u201dsea\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5022/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5022/Reviewer_KVmG",
                    "ICLR.cc/2024/Conference/Submission5022/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5022/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698683147276,
        "cdate": 1698683147276,
        "tmdate": 1700671316502,
        "mdate": 1700671316502,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iRn9jKffcq",
        "forum": "9JxQyat11M",
        "replyto": "9JxQyat11M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5022/Reviewer_tiUR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5022/Reviewer_tiUR"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an inference pipeline that combines SOTA zero-shot image classification models (e.g. CLIP) with SOTA open vocabulary object detection models (e.g. OWL-ViT) to improve zero-shot classification of images with smaller objects. Notably, the proposed approach does not require any model training. The model relies on a combination of CLIP for whole-image and image crop classification and OWL-ViT for object localisation and cropping. The paper explores several hyper-parameters of the proposed pipeline (e.g. margin of the crops and random crop augmentation) and demonstrates that their method improves on several CLIP-based baselines and on OWL-ViT."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Solid paper, clearly written and well-motived.\n* The method is pragmatic, and seemingly driven by practical considerations of actually using CLIP and OWL-ViT models in real life applications.\n* The proposed inference pipeline does not require any training and thus can be readily used for many applications."
            },
            "weaknesses": {
                "value": "* The observations of current limitations of CLIP and OWL-ViT models are somewhat surface-level and I believe well-known (although possibly not written down in a publication)\n* The proposed solution to the observed limitations (i.e. the proposed inference pipeline) is as far as I know novel, but maybe better presented at a more computer vision focused conference.\n* Although the focus of the proposed approach is to correct failure cases of CLIP and OWL-ViT (i.e. small object classification), and the benchmarks were chosen to assess this specifically, it would be very useful to see the proposed method benchmarked on widely used zero-shot datasets like ImageNet.\n* It would be interesting to discuss the limitations of the proposed method. For example, it focuses on image classification, but CLIP and OWL-ViT provide more than that. For example, CLIP embeddings can be used for image-text retrieval, and OWL-ViT embeddings can be used for image-conditioned detection. Can the authors' method be extended to these use cases?"
            },
            "questions": {
                "value": "See Weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5022/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698749172021,
        "cdate": 1698749172021,
        "tmdate": 1699636490698,
        "mdate": 1699636490698,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yzy98PU5A4",
        "forum": "9JxQyat11M",
        "replyto": "9JxQyat11M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5022/Reviewer_EzG9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5022/Reviewer_EzG9"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method to crop input image to obtain more robust features for zero-shot object classification. The author use a pre-trained zero-shot object detection model to obtain an initial bounding box that is most responsive to an input prompt. The box is then enlarged before inputing into CLIP for classification. This approach brings consistent ZSC improvement using ViT-B model as baseline, tested on ImageNetS919 and CUB datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The approach is simple and easy to re-implement."
            },
            "weaknesses": {
                "value": "The novelty is limited. Many papers [1,2,...] have discussed the impact of cropping in image classification. The paper aims to find an optimal crop but there is no technical contribution since the heavy-lifting is done purely based on the pre-trained object detector. Perhaps the core contribution is to show that an object detector can be used for this purpose? I think it is incremental. \n\nThe potential applicability is limited. The method is very specific to CLIP and the core method doesn't work right off the bat but requires post-processing steps on top of the initial boxes. Further, this method is only suitable for classifying images whose labels associating to a small object it contains. The authors did provide some qualitative evaluation on these cases but I think it could benefit the paper to frame it more aggressively into this direction since it seems to me this is the only scenario where it might prove a significant advantage.\n\n[1] ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of Zoom and Spatial Biases in Image Classification - Taesiri et al. NeurIPS 2023. \n[2] Generating Features with Increased Crop-related Diversity for Few-Shot Object Detection - Xu et al. CVPR 23"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5022/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698837654059,
        "cdate": 1698837654059,
        "tmdate": 1699636490588,
        "mdate": 1699636490588,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qbPSNxxgqH",
        "forum": "9JxQyat11M",
        "replyto": "9JxQyat11M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5022/Reviewer_LrWg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5022/Reviewer_LrWg"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors identify that the Image Encoder of CLIP is more inclined to extract generic image representation, thus leading to performance degradation in zero-shot closed-set object classification tasks, especially for small objects. To address this problem, they proposed GC-CLIP, which crops and zooms in on the target itself by introducing a guided cropping method based on a zero-shot target detection model, thus improving the performance of CLIP."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1- The language is clearly presented. The authors use precise and concise language so that the reader can easily understand the background, methodology, and results of the study.\n2- Ablation studies are comprehensive. The authors demonstrated the superiority of GC-CLIP over CLIP through many ablation studies and analysed various factors."
            },
            "weaknesses": {
                "value": "1- I suggest the authors report the computational cost of GC-CLIP in the paper, including the parameters, FLOPs or the inference time, for a more comprehensive comparison with CLIP.\n2- I am confused about the necessity of combining OWL-ViT and CLIP, because the authors\u2019 results in the experimental section show that the difference between introducing OWL-ViT for guided cropping and using random cropping is slight, more results and analysis on different datasets should be provided to illustrate the advancement of guided cropping. In particular, the authors did not report results with random cropping alone when guided cropping was used on CALIP.\n3- An essential prerequisite for the successful application of OWL-ViT in the GC-CLIP is that OWL-ViT can provide a detection box for every target in the image. The authors have yet to carry out validation on more datasets to verify the impact and constraints of the detectors on their method, so it cannot judge the processing performance of GC-CLIP for other more complex datasets, and more analysis is needed.\n4- Authors should report the performance comparison of GC-CLIP with current popular methods.\n5- Authors also need to check for grammatical problems. For example, in the last sentence of paragraph 5 of the introduction section, there is a subject-verb inconsistency between \u201cthe cropped image\u201d and \u201cdecrease\u201d and \u201cresult in\u201d."
            },
            "questions": {
                "value": "Please refer to Weaknesses.\nMy main concern is the necessity of this guided cropping approach as it seems to have less difference in performance compared to what random cropping brings. There is a need for more results on more datasets and comparisons with other popular methods to demonstrate the performance of GC-CLIP."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5022/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698997389441,
        "cdate": 1698997389441,
        "tmdate": 1699636490508,
        "mdate": 1699636490508,
        "license": "CC BY 4.0",
        "version": 2
    }
]