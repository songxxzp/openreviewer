[
    {
        "id": "pd70CuFVSA",
        "forum": "ArpwmicoYW",
        "replyto": "ArpwmicoYW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3718/Reviewer_iDy7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3718/Reviewer_iDy7"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to promote the group fairness of the deep learning model in medical image analysis, with a specific focus on validation fairness. The authors propose a parameter-efficient fine-tuning method to update parameters regarding fairness. The proposed method is validated on five medical imaging datasets and outperforms compared methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper studies promoting group fairness, which is an important topic.\n- The motivation is well demonstrated.\n- The overall framework design is easy to follow.\n- The proposed method outperforms the compared methods."
            },
            "weaknesses": {
                "value": "- In the method part, the authors limit the method fairness in binary classification. It has not mentioned how to extend the methodology for multi-classification.\n- The first challenge of PEFT is related to the dataset itself, which is not a challenge for fairness.\n- The method details are not clear. E.g., how to solve the BLO problem by using TPE with SH.\n- It is not clear how to split the train/val/test data.\n- Since this method utilizes validation data to tune the model, it is not proper to report the validation AUC; instead, test AUC should be reported.\n- The experiment only validates the AUC within subgroups, more comprehensive metrics are expected (e.g., equal opportunity.)"
            },
            "questions": {
                "value": "- Why the metric for fair learning is to minimize the largest loss of a subgroup instead of pursuing a uniform loss distribution among subgroups?\n- How to explain the differences between masks by using different optimizing objectives?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3718/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3718/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3718/Reviewer_iDy7"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3718/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698589987590,
        "cdate": 1698589987590,
        "tmdate": 1699636328178,
        "mdate": 1699636328178,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "R3jMGQPtuU",
        "forum": "ArpwmicoYW",
        "replyto": "ArpwmicoYW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3718/Reviewer_XV7U"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3718/Reviewer_XV7U"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on an important field in AI, which is achieving group fairness in models, especially in medical diagnosis. They argue that this is essential but challenging due to the fairness generalisation gap where bias emerges during testing. The authors introduce a bi-level optimisation approach called FairTune, which optimises parameter-efficient fine-tuning (PEFT) techniques to balance model fit and fairness generalisation. The empirical results in the paper show that the proposed method enhances fairness across multiple medical imaging datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper proposed a new method to finetune the pretrained model, which is potentially benefits and convenient to the current hype of foundation models or large models that require large-scale pretraining. \n\nThe proposed PEFT achieves the best performance when compared with other fairness finetuning approaches. \n\nThe paper is well written and motivates clearly as well."
            },
            "weaknesses": {
                "value": "Given the model is proposed for finetuning a pre-trained model, could the authors provide some results that using the proposed approach on finetuning Masked Autoencoder or MOCO to see if this can improve fairness for self-supervised pretraining? \n\nFor evaluation metrics in fairness, DPD and DEOdds are very common to validate an algorithm's fairness, could the authors evaluate their methods on some of the datasets using those two metrics? \n\nThe datasets compared only contains a limited number of attributes, could the authors compare their approaches to fairness medical dataset containing more sensitive attributes such as the \"Luo, Yan, et al. \"Harvard Glaucoma Fairness: A Retinal Nerve Disease Dataset for Fairness Learning and Fair Identity Normalization.\" arXiv preprint arXiv:2306.09264 (2023).\""
            },
            "questions": {
                "value": "Please see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3718/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3718/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3718/Reviewer_XV7U"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3718/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698677256673,
        "cdate": 1698677256673,
        "tmdate": 1699636328095,
        "mdate": 1699636328095,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "piuike8Gwd",
        "forum": "ArpwmicoYW",
        "replyto": "ArpwmicoYW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3718/Reviewer_gkxu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3718/Reviewer_gkxu"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenge of minimizing demographic bias in AI models used for medical diagnosis. The authors highlight the fairness generalization gap, where deep learning models can fit training data perfectly and exhibit fairness during training but show bias during testing when performance differs across subgroups. To tackle this issue, they propose a bi-level optimization approach called FairTune. FairTune optimizes the learning strategy based on validation fairness by adapting pre-trained models to medical imaging tasks using parameter-efficient fine-tuning techniques. The authors demonstrate empirically that FairTune improves fairness on various medical imaging datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper recognizes the fairness generalization gap, where deep learning models exhibit perfect fairness during training but bias emerges during testing when generalization performance differs across subgroups.\n\n2. This work introduce a parameter-efficient fine-tuning technique as an effective workflow for adapting pre-trained models to downstream medical imaging tasks.\n\n3. The paper is easy to follow."
            },
            "weaknesses": {
                "value": "1. The absence of widely-used fairness metrics, such as Demographic Parity Difference [1,2,3] and Difference of Equalized Odds [1], in this work raises concerns about the completeness of the evaluation. Including these fairness metrics is essential for making the results more convincing.\n\n2. The benchmarking presented in the study appears to be incomplete. To provide a comprehensive comparison, it is advisable to include at least two additional fairness-aware methods in the experiments: Fair Supervised Contrastive Loss [4] and Group Distributionally Robust Optimization [5].\n\n3. Considering the relevance of MedFair [6], which evaluates fairness across various datasets, especially the significant CheXpert dataset for assessing fairness in medical applications, it would be beneficial to adhere to the experimental protocol and employ CheXpert for evaluating the proposed FairTune.\n\n4. It is worth noting that bi-level optimization can be computationally intensive and time-consuming due to the iterative optimization required in both inner and outer loops.\n\n5. When optimizing for fairness during fine-tuning, there is a potential concern regarding the impact on generalization performance, especially for unseen data or different subgroups. It would be valuable to clarify whether there are mechanisms in place to mitigate any adverse effects on generalization.\n\nReferences:\n\n[1] Alekh Agarwal, Alina Beygelzimer, Miroslav Dud\u00edk, John Langford, and Hanna M. Wallach. A reductions approach to fair classification. In ICML, volume 80 of Proceedings of Machine Learning Research, 60\u201369. PMLR, 2018.\n\n[2] Alekh Agarwal, Miroslav Dud\u00edk, and Zhiwei Steven Wu. Fair regression: quantitative definitions and reduction-based algorithms. In ICML, volume 97 of Proceedings of Machine Learning Research, 120\u2013129. PMLR, 2019.\n\n[3] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning. fairmlbook.org, 2019.\n\n[4] Sungho Park, Jewook Lee, Pilhyeon Lee, Sunhee Hwang, Dohyung Kim, Hyeran Byun; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 10389-10398\n\n[5] Sagawa, S., Koh, P. W., Hashimoto, T. B., & Liang, P. (2019, September). Distributionally Robust Neural Networks. In International Conference on Learning Representations.\n\n[6] Zong, Y., Yang, Y., & Hospedales, T. (2022, September). MEDFAIR: Benchmarking Fairness for Medical Imaging. In The Eleventh International Conference on Learning Representations."
            },
            "questions": {
                "value": "Please refer to point 1, 2, and 3 in the weaknesses to provide more convincing empirical evidence.\n\nMoreover, I noted that the code repository mentioned in the abstract has not been established. Providing access to the implementation code would greatly enhance the comprehensibility of this research during the review process."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3718/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698729631986,
        "cdate": 1698729631986,
        "tmdate": 1699636328018,
        "mdate": 1699636328018,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DpHMiNZWVa",
        "forum": "ArpwmicoYW",
        "replyto": "ArpwmicoYW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3718/Reviewer_Dgwu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3718/Reviewer_Dgwu"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces FairTune, a fine-tuning method for pre-trained models that aims to improve fairness with respect to sensitive attributes. The contribution lies in developing a technique that minimizes disparities in model performance between different demographic groups while maintaining high overall predictive accuracy. The method is demonstrated across various datasets and benchmarks, particularly in medical imaging, using the AUROC metric for evaluation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. FairTune provides a new pathway and improvement in reducing bias in AI models.\n2. The paper conducted extensive testing over multiple datasets.\n3. It leverages an ablation study to show the effectiveness of each component of the tuning process."
            },
            "weaknesses": {
                "value": "The paper may not fully address the computational costs or scalability issues associated with FairTune. Please see the questions for more details."
            },
            "questions": {
                "value": "1. The code link is not available.\n2. Can the authors examine the proposed FairTune on dataset with larger \"Gap\"? In Table 1, the Gaps for the datasets are relatively small.  Some improvements were limited, compared with full fine-tune. \n3. Can the authors provide insights into the computational overhead introduced by FairTune compared to traditional fine-tuning methods?\n4. What are the scalability considerations for applying FairTune to very large datasets or models?\n5. How sensitive is FairTune to the choice of sensitive attributes, and can it adapt to scenarios with multiple overlapping sensitive categories?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3718/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699179955708,
        "cdate": 1699179955708,
        "tmdate": 1699636327922,
        "mdate": 1699636327922,
        "license": "CC BY 4.0",
        "version": 2
    }
]