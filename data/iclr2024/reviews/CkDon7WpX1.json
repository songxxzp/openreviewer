[
    {
        "id": "TeErluHEjJ",
        "forum": "CkDon7WpX1",
        "replyto": "CkDon7WpX1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2175/Reviewer_CLFN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2175/Reviewer_CLFN"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to solve multi-label learning problem by jointly optimizing multiple objective function simultaneously. Considering that different loss functions may be potentially conflicting, the paper adopts a multi-objective optimization framework and solve the corresponding optimization problem by the traditional covariance matrix adaption (CMA-ES). Theoretical analysis on multi-label consistency is conducted on the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper proposed a framework to optimize multiple loss functions simultaneously. Although this idea is borrowed from multi-objective learning, it is a key point to achieve good performance on multiple metrics (loss functions) in multi-label learning. \n\n2. The paper proves the multi-label consistency for the proposed objective function although the main results are proved in the previous works."
            },
            "weaknesses": {
                "value": "1. I'm confused about one point: why did the author specifically focus on the learning model f? Does f have any impact on the proposed method? It seems to be a standard neural network. If there's anything I missed, please let me know.\n\n2. The introduction of the proposed method Section 2.3 is unclear. The paper did not provide a detailed explanation of how to optimize the proposed objective function. If an existing method was used, the technical contribution of the proposed method is limited.\n\n3. The experiments are weak due the following reasons:\n\n1) The dataset is relatively small, with the largest dataset containing only 5000 examples.\n\n2) The compared methods are not sufficiently advanced; the most recent method was proposed in 2022. Most of the other methods were introduced four to five years ago, or even earlier.\n\n3) I think that the proposed method should be compared with some commonly used loss functions, such as binary cross entropy loss, ranking loss, etc, based on the same base model.  \n\n4) The figures were not plotted carefully; the font size of legends and axis is too small."
            },
            "questions": {
                "value": "How to optimize the objective function proposed in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2175/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2175/Reviewer_CLFN",
                    "ICLR.cc/2024/Conference/Submission2175/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2175/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698578351552,
        "cdate": 1698578351552,
        "tmdate": 1700704716940,
        "mdate": 1700704716940,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sZTe6Aw6cN",
        "forum": "CkDon7WpX1",
        "replyto": "CkDon7WpX1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2175/Reviewer_Fh3C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2175/Reviewer_Fh3C"
        ],
        "content": {
            "summary": {
                "value": "The article introduces the challenges associated with multi-label loss functions, particularly their non-convex or discontinuous nature, which makes direct optimization problematic. Recognizing the inconsistencies between surrogate loss functions and their desired counterparts, the authors present a novel approach termed the \"Consistent Lebesgue Measure-based Multi-label Learner\" (CLML). This technique posits that optimizing the Lebesgue measure directly correlates to the optimization of various multi-label losses. Under a Bayes risk framework, the authors demonstrate the theoretical consistency of CLML."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe paper introduces a novel learning objective adept at handling various non-convex and discontinuous multi-label loss functions, which notably eliminates the need for surrogate loss functions. This innovative approach offers a more direct and potentially efficient solution to challenges associated with traditional multi-label loss optimization.\n2.\tThe authors have provided clear proof showing that their method is consistent. This adds credibility to their approach and assures users of its reliability in various applications.\n3.\tThe authors have conducted extensive experiments to demonstrate the advantages of their method. This thorough empirical validation underscores the effectiveness and robustness of their approach in real-world scenarios."
            },
            "weaknesses": {
                "value": "1. There appears to be a typographical error in Equation 2. Specifically, in the loss function L\\left(f\\left(x\\right),y\\right), the y should likely be denoted as y^\\prime. This inconsistency needs to be rectified to ensure clarity and coherence in the formulation.\n2. The optimization method in the last paragraph of section 2.3 is not elaborated in detail. To enhance clarity and provide readers with a holistic understanding, the authors are recommended to furnish a more detailed exposition on the employed optimization techniques.\n3. The methodology proposed in this paper appears to be tailored specifically for tabular data. This inherent design might limit its applicability to large-scale image datasets, which inherently possess a different data structure and complexity. Thus, the generalizability of the approach to diverse data types, especially image datasets, remains questionable."
            },
            "questions": {
                "value": "1. In Equation 2, can the authors clarify the notation used in the loss function L\\left(f\\left(x\\right),y\\right)? Is there a specific reason for using y instead of the seemingly more appropriate y^\\prime? \n2. Regarding the optimization method presented in the latter part of section 2.3, could the authors delve deeper into the specifics of this method? \n3. Given that the paper's methodology seems predominantly designed for tabular data, how do the authors envision adapting or evolving this method for more complex datasets, such as large-scale image datasets?\n4. Have the authors considered expanding the variety of multi-label loss functions to further enhance the model's performance? Additionally, would assigning different weights to distinct loss functions potentially lead to further improvements in the model's efficacy? It would be interesting to understand the impact of such variations on the overall performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2175/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698667664891,
        "cdate": 1698667664891,
        "tmdate": 1699636150742,
        "mdate": 1699636150742,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6tssHmYwnQ",
        "forum": "CkDon7WpX1",
        "replyto": "CkDon7WpX1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2175/Reviewer_XYJ3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2175/Reviewer_XYJ3"
        ],
        "content": {
            "summary": {
                "value": "The article introduces the challenges associated with multi-label loss functions, particularly their non-convex or discontinuous nature, which makes direct optimization problematic. Recognizing the inconsistencies between surrogate loss functions and their desired counterparts, the authors present a novel approach termed the \"Consistent Lebesgue Measure-based Multi-label Learner\" (CLML). This technique posits that optimizing the Lebesgue measure directly correlates to the optimization of various multi-label losses. Under a Bayes risk framework, the authors demonstrate the theoretical consistency of CLML."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The introduction of a groundbreaking learning objective that adeptly addresses non-convex and discontinuous multi-label loss functions, significantly eliminating the dependence on surrogate loss functions, showcases a more streamlined and potentially more efficient approach to traditional multi-label loss optimization challenges.\n2. The clear proof of method consistency provided by the authors lends strong credibility to their novel approach, ensuring its reliability and applicability across diverse scenarios."
            },
            "weaknesses": {
                "value": "1. A potential weakness of the algorithm is its limitation to tabular data, which may render it ineffective for handling large-scale image datasets prevalent in contemporary research and applications.\n1. In Section 2.3, the optimization algorithms are merely summarized in brief statements rather than being detailed through comprehensive formulas. It raises the question of whether the complexity of these optimization methods contributes to the model's inability to handle large-scale datasets.\n1. In Equation (2), the loss function and surrogate loss function incorrectly use $ y $ instead of $ y' $. This appears to be a typographical error."
            },
            "questions": {
                "value": "Could the authors provide more detailed formulations or explanations for the optimization methods mentioned in Section 2.3? I'm curious to understand their specifics. Additionally, could the authors clarify whether these methods are scalable to handle large-scale datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2175/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2175/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2175/Reviewer_XYJ3"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2175/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761253717,
        "cdate": 1698761253717,
        "tmdate": 1699636150645,
        "mdate": 1699636150645,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pG9ME95qLC",
        "forum": "CkDon7WpX1",
        "replyto": "CkDon7WpX1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2175/Reviewer_PXNK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2175/Reviewer_PXNK"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to leverage the existing Lebesgue measure to propose a method capable of comprehensively considering a range of multi-label loss functions to ensure their effectiveness. The paper also conducts experiments to validate the performance of this method with different loss functions on several datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper touches on a crucial issue in multi-label learning: different loss functions measure classifier performance differently.\n\n2. The proposed method in this paper is based on neural networks, allowing it to benefit from the advancements in neural network technology."
            },
            "weaknesses": {
                "value": "1. The motivation behind this paper is somewhat weird. Given the knowledge that different loss functions lead to different outcomes, why introduce a \"universal method\" instead of determining which loss function to use based on practical needs?\n\n2. This paper is an application of the Lebesgue method, and there are numerous other methods for multi-objective optimization. The paper does not compare its method with other multi-objective optimization techniques.\n\n3. The technical exposition in this paper is unclear and contains several issues.\n\n4. The method proposed in this paper does not exhibit a significant empirical difference from the latest method, CLIF. The experimental results are rather marginal improvement."
            },
            "questions": {
                "value": "1. What is the specific rationale behind choosing Lebesgue's multi-objective optimization method?\n\n2. In cases where classifiers resulting from different loss functions may conflict, why is the universal method proposed in this paper still necessary?\n\n3. Is there an issue with defining p(x) as equivalent to p(x|y) (above Eq 1), where the former depends solely on x, while the latter depends on both x and y simultaneously?\n\n4. Is there a problem with Eq 2, where the latter y should be y'?\n\n5. In the absence of a specified R, how does optimizing lambda(P(f)) achieve multi-objective optimization? If R consists of only one element, it is possible that H(F, R) could be empty."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2175/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699185651372,
        "cdate": 1699185651372,
        "tmdate": 1699636150588,
        "mdate": 1699636150588,
        "license": "CC BY 4.0",
        "version": 2
    }
]