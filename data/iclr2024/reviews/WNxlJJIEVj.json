[
    {
        "id": "BGy7Ynu6Ed",
        "forum": "WNxlJJIEVj",
        "replyto": "WNxlJJIEVj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission513/Reviewer_ogtz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission513/Reviewer_ogtz"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of learning state-action trajectory generation with diffusion models. To better leverage the high-return states in the offline dataset, the authors proposed a contrastive learning mechanism to drive the generated trajectory toward the high-return states. Experiments are performed on D4RL benchmarks to validate the idea."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The motivation for leveraging contrastive learning to guide the generation process is interesting and reasonable; \n\nThe paper is well-written and easily read."
            },
            "weaknesses": {
                "value": "The method seems on par or slightly worse than the baseline approaches; \n\nThe experiments were only conducted on a few simple periodic tasks, which could not sufficiently demonstrate the effectiveness of the method; \n\nThere is no analysis of failure modes and limitations."
            },
            "questions": {
                "value": "What is the task shown in Fig. 5? For visualizing the task, it would be better to align some of the high-return and low-return states to the visual observations of the environment, which may help readers better understand the task. \n\nFor Fig. 6, could the authors provide the annotations for the x-axis and y-axis? \n\nIt would greatly enhance the paper if the authors could offer a more in-depth analysis of failure cases. Additionally, aside from relatively straightforward periodic tasks, it would be beneficial if the authors explored more complex tasks. Demonstrating the applicability of their approach in scenarios like robot navigation or manipulation would significantly bolster the paper's overall impact and practical relevance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission513/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission513/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission513/Reviewer_ogtz"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission513/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698161215347,
        "cdate": 1698161215347,
        "tmdate": 1699637386401,
        "mdate": 1699637386401,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NWTSdeeTpT",
        "forum": "WNxlJJIEVj",
        "replyto": "WNxlJJIEVj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission513/Reviewer_FaMZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission513/Reviewer_FaMZ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel contrastive diffusion probabilistic planning approach to tackle offline reinforcement learning (RL) tasks. It expands upon the foundational Diffuser model, leveraging contrastive learning to enhance the quality of samples by generating high-return trajectories. This focus on sequence modeling within offline RL is both interesting and important. The paper is well-written, though it lacks some specifics, and the visual representations, particularly Figure 1, are insightful."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The focus on sequence modeling in offline-RL is both innovative and significant, addressing a crucial aspect of this field.\n- This paper proposed a novel method of integrating contrastive learning that showed improvement over the baseline Diffusers.\n- Figure 1 is quite illustrative and intuitive."
            },
            "weaknesses": {
                "value": "### Missing Details in Methodology:\n\n- The training process of the model remains unclear. While Equation 14 suggests end-to-end training, it is unclear where the contrastive loss is integrated. If added to the diffusion probabilistic model, which aims to reconstruct the un-corrupted trajectories, will this added loss diverge the learning, making the training unstable?\n- Is the contrastive loss involved during guidance sampling? \n\n### Insufficient Experiments:\n\n- The claim of 'significant improvements in medium and medium-replay datasets' seems overstated. The improvements are noticeable in only one task from each dataset compared to DD.\n- Extending experiments to more complex control tasks or scenarios with high-dimensional state/action spaces would substantiate the method's effectiveness.\n- A comparative test incorporating DD + Contrastive Learning would add effectiveness to the proposed method.\n- Figure 6 requires more explanation, particularly regarding the methodology for generating and comparing states in each showcased scenario.\n\n### The method introduces several additional hyperparameters, as depicted in Figure 7, indicating a significant sensitivity to these parameters, which could complicate the tuning process."
            },
            "questions": {
                "value": "- The distinctions among the three models presented in Figure 5 are not very clear to me.\n- For the experiments depicted in Figure 6, how are the generated states and actual states obtained in each case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission513/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698722217399,
        "cdate": 1698722217399,
        "tmdate": 1699637386303,
        "mdate": 1699637386303,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "olpRA5MBDa",
        "forum": "WNxlJJIEVj",
        "replyto": "WNxlJJIEVj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission513/Reviewer_F1CC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission513/Reviewer_F1CC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a diffusion-based trajectory generation based on contrasting high-return and low-return training samples, called CDiffuser. The core approach in the method is performing a contrastive learning between generated trajectory and samples in the dataset. The contrastive learning serves as a guidence to the diffusion process, and pushes the generated trajectories towards high return states and away from low return states. Experimental results on Gym show the improvements of the proposed algorithm."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of combining contrastive learning with trajectory generation is somehow novel.\n\n- The analysis on the similary of generated states are informative.\n\n- The ablation study on different loss terms are appreciated. The study on using high-reward samples only is great."
            },
            "weaknesses": {
                "value": "1. The performance improvement may not be significant. According to Table, 1 the performance is highly comparable to DD. \n\n2. The benchmark only uses Gym.\n\n3. The method is using one step generation from EDP. However, Table 1 and ablation study do not include the comparison against this method. \n\n4. The method highly relies on EDP to make the contrastive loss differentiable through the generated states, from my understanding. However, this could be hard to generalize to other diffusion-based methods. \n\n5. The guidence on return is confusing. The return is predicted from the very **first state** of the **noisy trajectory**, according to the third line after Equation (6). How can the prediction and the learned model be accurate, when solely from a noisy state?  And clarifications on its backpropogation is needed, since it only takes the noisy trajectory input, and the denoising process only takes one step.\n\n6. The original diffuser seperately trains the auxiliary return prediciton model on all data. This modification is not discussed and experimentally validated.\n\n7. Can the authors explain the reasons of intriguing properties presented in 4.4?\n\n8. The contrastive loss Equation (9) seems to not be a common form. Usually the denominator considers all the samples, for example in [2,3]. This is a concern on the correctness of this implementation and a justification is needed.\n\nBased on the points above, I am not convinced the proposed method is sound and could actually work in terms of training.\n\n[1] Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng Yan. Efficient diffusion policies for offline reinforcement learning. arXiv preprint arXiv:2305.20081, 2023.\n\n[2] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. \"Representation learning with contrastive predictive coding.\" arXiv preprint arXiv:1807.03748 (2018).\n\n[3] Khosla, Prannay, et al. \"Supervised contrastive learning.\" Advances in neural information processing systems 33 (2020): 18661-18673."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission513/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698727917242,
        "cdate": 1698727917242,
        "tmdate": 1699635977776,
        "mdate": 1699635977776,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SKwNKGbmIx",
        "forum": "WNxlJJIEVj",
        "replyto": "WNxlJJIEVj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission513/Reviewer_VS91"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission513/Reviewer_VS91"
        ],
        "content": {
            "summary": {
                "value": "This paper combines a trajectory planning method based on a diffusion model and contrastive learning to select states with higher returns. States are grouped into fuzzy sets of low and high reward and then used to constrain the trajectory planning by pulling states towards regions of higher return. An extensive ablation study, comparison with state of the art and hyperparameter search shows good results and the importance of all components."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The method is suprisingly simple yet effective. It can be probably easily adopted for a wide range planning problems or even tasks without explicit trajectories beyond the typical RL tasks in table 1."
            },
            "weaknesses": {
                "value": "It is hard to find weaknesses in this paper. Sometimes the sentences are a bit long and convey a lot of concepts at the same time which is not necessarily bad but harder to understand. One example is the sentence around equation 13. The impact of predictions in the future of planning could be elaborated a bit more, but this is just an example to illustrate my point.\n\nWhile being best and second best in the med-replay datasets, it could be argued if being so close to the other results can be called significant improvements and highlighting the second best is potentially done to have the results in the best possible light. However, the authors put their results in ample perspective and give reasonable hypothesis about the impact of expert examples."
            },
            "questions": {
                "value": "- More a suggestion, Figure captions like Figure 2 could provide more information. The general function of both modules as take away message for the reader could improve the figure understanding even though it appears in the text pointing to this figure\n\n- In figure 5 I find it hard to see what is supposed to be in and out of distribution. Maybe some circles could help making the points from section 4.4. All three figures also look very alike. I get the idea of comparison here but not sure about the overall value of this. The nuanced color changes are also hard to see and some people are color blind."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission513/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission513/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission513/Reviewer_VS91"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission513/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790774629,
        "cdate": 1698790774629,
        "tmdate": 1699635977699,
        "mdate": 1699635977699,
        "license": "CC BY 4.0",
        "version": 2
    }
]