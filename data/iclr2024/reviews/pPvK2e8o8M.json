[
    {
        "id": "IWCmgr8HNw",
        "forum": "pPvK2e8o8M",
        "replyto": "pPvK2e8o8M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2625/Reviewer_KFBC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2625/Reviewer_KFBC"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the concept of \"metacognition\", which refers to the model's ability to accurately self-evaluate one's own capabilities before generating text. They then propose to equip language models with this ability by first using ChatGPT to generate factual question-answer pairs in different domains, and then finetuning ChatGLM with LoRA in a meta-learning style. They claim that their model learns metacognitive abilities after training."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper constructs a factual question-answering dataset that can be helpful for future research."
            },
            "weaknesses": {
                "value": "1. One major weakness of this paper is that the concept of \"metacognition\" is not formally defined. In the introduction part, it mentions that it is the model's ability accurately self-evaluate one's own capabilities before generating text. However, no formal definition is presented and it is unclear how we can quantify such abilities. Therefore, it is unclear to how to evaluate a model's metacognition abilities and the experiments cannot support their claims such as \"their models can learn metacognitive abilities after training.\"\n2. The paper proposes several hypotheses that may be ungrounded. For example, in the introduction part they claim that \"RLHF is the main source of hallucination of recent LLMs\", but MLE-trained models can also hallucinate and RLHF can help with reducing hallucinations [1]. There are also other kinds of similar claims in the introduction section such as \"LLMs are not enough to fit the world\u2019s model.\"\n3. The writing of the paper lacks coherence. For example, they have put a lot of efforts on writing the retrieval-augmented models but didn't compare or use them. Also, it is unclear how one can reframe the task of \"instilling in LLMs the discretion to search only when absolutely necessary\" to \"teach LLMs to accurately self-evaluate their own capabilities before generating text.\" Teaching LLMs to self-evaluate their own capabilities is one way to implement this task, but they are not interchangeable.\n4. There are related works on model calibration and retrieval-augmented models that they didn't discuss in depth.\n\n\n\n[1]  Sun et al., Aligning Large Multimodal Models with Factually Augmented RLHF."
            },
            "questions": {
                "value": "1. How do you quantify \"meta-cognition\"?\n2. What is the difference between \"meta-cognition\" and \"model calibration\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2625/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698714524439,
        "cdate": 1698714524439,
        "tmdate": 1699636201843,
        "mdate": 1699636201843,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4kgVTSkH06",
        "forum": "pPvK2e8o8M",
        "replyto": "pPvK2e8o8M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2625/Reviewer_z9Ch"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2625/Reviewer_z9Ch"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce an algorithm designed to equip language models with sustained meta-cognitive abilities, drawing inspiration from meta-learning principles. The paper introduces an algorithm designed to endow large language models with metacognitive abilities, allowing them to evaluate their own capacity to generate responses to human instructions, and as a result, curtail the production of responses when the model is not adequately equipped to answer, significantly diminishing the incidence of hallucinatory text generation. Additionally, the authors present a novel ground truth dataset, encompassing 16,000 questions spanning 160 meticulously categorized domains, which can be utilized for fine-tuning and evaluating large language models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This submission set eyes on an interesting topic of introducing meta-learning to LMs so as to address the issue of hallucination. Their efforts in putting together a manuscript for this submission are acknowledged and appreciated."
            },
            "weaknesses": {
                "value": "(1) The manuscript\u2019s clarity and structural integrity could be greatly improved. The format of the manuscript leans more towards a school report than a scholarly research paper, and it currently presents challenges in understanding the main contributions and claims:\n* The motivation and main goal of the study need clearer definition; it\u2019s uncertain whether the focus is on addressing hallucination in text generation or applying meta-learning to LLMs.\n* The transition from discussing hallucination in Section 2 to model selection and metacognitive capabilities in Section 3 is abrupt and confusing. A more systematic introduction to the problem, followed by the detailed methodology, would enhance comprehension.\n\n\n\n\n(2) The design and execution of the experiments require substantial revision to robustly support the authors\u2019 claims:\n* Clarification is needed on the specific text generation task under study. Text generation is a very broad domain with many subtasks. The authors mention summarization and dialogue generation in the related work, but later train the model with QA setup. What exactly is the task that this work is trying to study?\n* Why did the authors choose ChatGLM-6B? Is it because this model is bilingual? Then why didn\u2019t the authors compare to other multilingual models such as BLOOM? Is it because this model supports multi-turn dialogue? Then there are also other dialogue models/chatbots available. Why didn\u2019t the authors compare to any of these related baselines?\n* The evaluation metrics need a detailed description, and the results presented in the tables require thorough analysis and interpretation.\n* The authors mentioned that there are a number of benchmarks for evaluating hallucination in text generation, then why didn\u2019t the authors report results on any of these benchmarks?\n\n\n\n(3) This submission did not mention a number of related works throughout the manuscript:\n* In the introduction, the authors briefly mentioned that metacognition is a concept in psychology, but did not point to any related studies.\n* The authors mention that their \u201calgorithm draws inspiration from meta-learning\u201d, but did not discuss any previous work on meta-learning.\n\n\n(4) Typos, formatting issues, etc:\n* Unpaired quotation marks: (e.g., Section 1: \u201dhallucination\u201d);\n* Missing reference when mentioning GPT-3, PaLM, Llama, etc in Section 3.1;\n* Misused citation format: e.g., Section 2: \u201cAnother approach is shown in (Feldman et al., 2023)\u201d -> the parentheses around the citation are unnecessary and should be removed to adhere to standard citation practices;\n* Table 5 and tables in the appendix extend beyond the page width;\n* What makes Figure 1 a figure? It appears to be two tables to me."
            },
            "questions": {
                "value": "Please kindly refer to the questions raised in the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2625/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698795779753,
        "cdate": 1698795779753,
        "tmdate": 1699636201768,
        "mdate": 1699636201768,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nte0Zhlqp5",
        "forum": "pPvK2e8o8M",
        "replyto": "pPvK2e8o8M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2625/Reviewer_AYMX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2625/Reviewer_AYMX"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an algorithm that endows language models with persistent metacognitive capabilities.  This empowers these models to evaluate their competence when interpreting human instructions, thereby avoiding the generation of responses beyond their abilities and mitigating the production of hallucinatory text."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper presents a highly innovative concept, utilizing the idea of meta-learning to address hallucinations. It is also the first method to mitigate hallucinations before the model's output. In terms of writing, the introduction of hallucination-related terms at the beginning of the article uses numerous vivid metaphors, making it very easy to understand and read."
            },
            "weaknesses": {
                "value": "1\u3001This article seriously lacks quantitative experimental results to verify the effectiveness of the method. The claim of the entire paper does not have sufficient experimental support.\n\n2\u3001However\uff0c there is also a lack of theoretical discussion further validating the method's effectiveness. \n\n2\u3001Additionally, due to the need for a substantial number of fine-tuned models, practical application can be challenging."
            },
            "questions": {
                "value": "The phrase \"Reduce Hallucination Text Generation\" in the title seems awkward. A more appropriate expression  could be \"Reducing Hallucinations in Text Generation\" or \"Reducing Hallucinatory Text Generation\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2625/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818645181,
        "cdate": 1698818645181,
        "tmdate": 1699636201550,
        "mdate": 1699636201550,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MUgOFz9Jwl",
        "forum": "pPvK2e8o8M",
        "replyto": "pPvK2e8o8M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2625/Reviewer_KoVi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2625/Reviewer_KoVi"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an algorithm designed to equip language models with enduring meta-cognitive capabilities. Drawing inspiration from meta-learning, the approach involves a process of fine-tuning models on diverse datasets, including the original untuned base model.\n\nwhat contributions does it make:\n1.This approach allows models to self-assess their competence when interpreting human instructions. \n2.This method improves the model's response quality by preventing responses beyond their learned capacities and reducing the generation of misleading or hallucinatory text.\n3.The paper provides a new ground truth dataset for evaluating large language models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.The paper is written in an easy-to-understand manner.\n2.The meta-cognitive ability adapts to the various fine-tuned versions of the primary model. This adaptability offers evaluations aligned with the knowledge capacity of the fine-tuned models. \n3.In each training cycle, the algorithm randomly selects multiple fine-tuned model versions and evaluates their meta-cognitive abilities."
            },
            "weaknesses": {
                "value": "1.The experiment conducted was inadequate, relying solely on ChatGLM-6B, which possesses a limited knowledge base.\n2.Table 1 lacks any analysis or interpretation."
            },
            "questions": {
                "value": "1.In the dataset constructed in this paper, the questions must be in line with objective facts, and they are all in English, can this be extended to a more general situation? How well does the model generalize outside of these specific scenarios?\n2.Compared with MAML, what do the support data and query data correspond to in the algorithm proposed in this paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2625/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698917237668,
        "cdate": 1698917237668,
        "tmdate": 1699636201488,
        "mdate": 1699636201488,
        "license": "CC BY 4.0",
        "version": 2
    }
]