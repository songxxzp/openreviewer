[
    {
        "id": "YYWdEi20Lq",
        "forum": "nATTIkte9f",
        "replyto": "nATTIkte9f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6951/Reviewer_4F9v"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6951/Reviewer_4F9v"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a new approach for fine-tuning large language models under differential privacy. It proposes an optimization framework that finds a good model parameter distribution subject to Renyi-DP constraints. Experiments show that the accuracy significantly surpasses existing works even with a very small privacy budget ($\\varepsilon < 3$)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem of fine-tuning large models on private data is extremely relevant in practice.\n2. The accuracy significantly surpasses previous work, especially in the high privacy regime."
            },
            "weaknesses": {
                "value": "1. There are many recent works on tight privacy accounting for differential privacy such as [1] and [2]. The authors should discuss whether they can be applied to the proposed algorithm.\n2. In section 3.3, the global optimization seems to be formulated for a fixed pair of neighboring datasets $D$ and $D'$. However, differential privacy requires all pairs of $D$ and $D'$, so to me, there should be a max over all possible $D, D'$ somewhere in the formulation. I hope the authors can explain where the worst case over all neighboring datasets is considered in this formulation.\n\n\nComment on writing:\n1. Many in-line citations do not have parenthesis and break the flow of the paper. For example: Section 2, DP-SGD Abadi et al. (2016). The reference should have a parenthesis around it.\n2. Section 3.1: \"We first define the differential privacy\". \"the\" should be removed.\n\n### References\n[1] Zhu, Y., Dong, J. &amp; Wang, Y.. (2022).  Optimal Accounting of Differential Privacy via Characteristic Function . Proceedings of The 25th International Conference on Artificial Intelligence and Statistics<, in Proceedings of Machine Learning Research 151:4782-4817 Available from https://proceedings.mlr.press/v151/zhu22c.html.\n[2] Individual Privacy Accounting with Gaussian Differential Privacy, Antti Koskela, Marlon Tobaben, Antti Honkela, ICLR 2023"
            },
            "questions": {
                "value": "1. In equation (2), is there a particular reason for the range of $\\alpha$ to be 2 to 129? Is it possible that the best parameter is achieved outside of this range for some datasets and problems?\n2. It is surprising to me that the noise level for $\\varepsilon =2,3$ is slightly larger than $\\varepsilon=0.2$. Could the authors explain this phenomenon?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6951/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784358188,
        "cdate": 1698784358188,
        "tmdate": 1699636811705,
        "mdate": 1699636811705,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RtSsoGCORO",
        "forum": "nATTIkte9f",
        "replyto": "nATTIkte9f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6951/Reviewer_9q2i"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6951/Reviewer_9q2i"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel DP framework for training large language models. It employs the composition of sub optimal DP mechanisms for fine-tuning LLMs. The method empirically outperforms the existing baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper provides a method to tightly compose the DP mechanisms compared to traditional gaussian mechanisms, which usually add a lot of noise. \n2. LMO subspace is an interesting way of finding LM geometries, as it provides universal RDP guarantees along with strong empirical results. \n3. Empirical results are strong compared to the baselines. \n4. Algorithm is relatively simple to implement compared to the standard DP-SGD implementations."
            },
            "weaknesses": {
                "value": "1. Algorithm 3 seems to be important, adding it to the paper would be nice.\n2. There is an inherent tradeoff between privacy and utility, however the results seem to be the opposite, as in even for extremely small values of epsilon, the accuracy of model is pretty high, can the authors please explain this observation?\n3. How would the results change if the models were trained from scratch instead of fine-tuning?"
            },
            "questions": {
                "value": "How can the proposed method be extended to vision tasks? or vision generative models?\nFor instance NAR generative models solve a bert like optimization problem during training, is there a trivial way to extend this method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6951/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6951/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6951/Reviewer_9q2i"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6951/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821730998,
        "cdate": 1698821730998,
        "tmdate": 1699636811580,
        "mdate": 1699636811580,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ksiBo1S4Py",
        "forum": "nATTIkte9f",
        "replyto": "nATTIkte9f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6951/Reviewer_tr6o"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6951/Reviewer_tr6o"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new way to train language models with differential privacy guarantees. Rather than adding Gaussian noise as done by DP-SGD and to the best of my knowledge all improvements that build on it, the authors build on a result by Mohammady (2020) and add a mixture of noise of several distributions. They extend the Renyi DP accountant to handle non-Gaussian noise. Finally, they also present empirically results showing that they can beat the DP-SGD state of the art. The improvements are particularly striking in the low $\\varepsilon$ regime."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- Strong empirical results. The paper presents results for good model utility with very strong privacy guarantees of $\\varepsilon < 0.1$. This is a significant improvement over current SOTA.\n- The authors provided a repo to reproduce the result.\n- The method is simple but very effective. It should be fairly straightforward to implement this method in most DP training libraries such as opacus which will be of great value to the community.\n- Comprehensive evaluation"
            },
            "weaknesses": {
                "value": "The main weakness of this paper is the presentation. The manuscript reads like a rushed submission that requires more careful proof reading. For example:\n- What does LMO stand for? It is introduced as \"Language model-based optimal\" but Appendix C says it's \"Laplace Mixture of Outcomes\" which has not been introduced at all.\n- Many references to the abstract for essential content which makes it difficult to read the paper fluently.\n- Inconsistent values throughout the paper e.g. for $\\alpha$ equation (2) states integers 2, ..., 129 whereas algorithm 1 states 3, ..., 130"
            },
            "questions": {
                "value": "- Can you explain what you mean by LM geometry in more detail? I found it hard follow the LM Geometry section? What are the specific characteristics? Is $\\mathbb{P}_{i/o}$ simply the conditional probability $\\mathbb{P}[o|i]$? This seems to be a conventional language model. I'm failing to understand what aspect of this definition is geometric.\n- Why did the authors chose RDP accounting over tighter accountants e.g. PRV accountant? PLRV based accountant should be able to handle multiple different noise distributions? Would that provide an even better privacy utility trade off?\n- Can you explain figure 1 in more detail and how it shows that the mixture of distributions covers the whole space?\n- It would be interesting to learn more about the mixture of noise distribution? What type of noise is the main contribution?\n- The authors often refer to the privacy curve $\\delta(\\varepsilon)$ as the PLRV, while they are equivalent they're not the same.\n\nAs stated above, I believe the main weakness of the paper is the overall presentation. It believe if the authors could improve the writeup the value to the community would significantly higher."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6951/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698835390607,
        "cdate": 1698835390607,
        "tmdate": 1699636811473,
        "mdate": 1699636811473,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MEvrgYKLlW",
        "forum": "nATTIkte9f",
        "replyto": "nATTIkte9f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6951/Reviewer_LUi6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6951/Reviewer_LUi6"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes LMO-DP, a framework for fine-tuning language models with DP. This work optimizes (minimizes) the noise added for DP to the cross-entropy in LM model training in a way that ensures a bounded RDP. The authors propose a sub-optimal reduction to the optimal optimization problem. The search space of LMO geometries is shown to be comprehensive. Altogether, this enables training of high utility models that outperform prior work by a significant margin, especially at low epsilons."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The main strength of this work is the strong empirical results. This work achieves extremely impressive empirical results that outperform prior work at even an order of magnitude larger epsilon. For example, Table 3 shows that LMO-DP at epsilon of 0.16 outperforms all prior work at even epsilon of 1.4.\n\nTo achieve these results, this work proposes a novel approach to optimizing (minimizing) the amount of noise added while satisfying a finite RDP guarantee. This is achieved by building on the work of Mohammady et al (2020). \n\nThese results are highly timely and significant, as LLMs are seeing increasing prevalence in academia and adoption in industry.\n\n(*) Finally, as both a question and a strength, this work seems independent of LMs and can potentially have larger impact. The main crux seems to be optimizing over the cross-entropy loss which is used more broadly than just LMs."
            },
            "weaknesses": {
                "value": "Though this work achieves impressive results, it is unclear how sound these results are, for several reasons. \n\n**First, the results presented are often times quite unclear and imprecise. This manifests in a few ways.**\n\n(A) The results rely heavily on the supplemental material, with no proofs or high-level descriptions of proofs provided in the main-text. This makes it difficult to follow the line of thought and verify if the high-level approach is sound. \n\n(B) One of the key algorithms is Algorithm 3 which is required to understand section 4.3 but does not appear in the main-text. \n\n(C) Some of the terminology is often imprecise: e.g., (1) \"corresponding parameters, including \\theta and others\" and (2) \"secondary optimization\" on page 6. What are \"others\" and where is this \"secondary optimization\". Neither seem to appear in Algorithm 3 (nor the initial optimization mentioned just prior). \n\n(D) This work compares several orthogonal methods to DP on different (often unrelated axes) in a rather confusing way. At the heart of this work is minimizing the amount of noise needed by DP. Yet, this work lists and compares with largely orthogonal work that explores memory reduction (ghost clipping) or parameter efficiency techniques (which improve memory/noise scale through the # of parameters) but  cna be used in conjunction with these methods. Why does Table 1 give low memory to this work? This seems like a false claim considering this work does not optimize memory but instead the noise standard deviation. Also, this work claims faster convergence but does not provide any theoretical guarantee to convergence, only some empirical exploration.\n\n**Second, the results in some places appear unsound, or, require additional explanation.**\n\n(E) The comparison of noise of LMO-DP to standard Gaussian noise for DP is unclear. The work claims \"less noise\" and relies on Figure 2 to compare this for several choices of epsilon. However, due to the scale, it is very vague what the actual difference is. The figure should use a log scale and also report useful statistics like the average reduction. Interpreting this figure, it looks as though the chosen noise is often 0 (or, vanishingly small). This would be an extremely significant improvement over DP that requires much more analysis to both understand how/where this is coming from and to ensure correctness. One key analysis would be to show what the noise looks like under the extremes (i.e., any single component), and, to for example, show how this noise changes as a function of the components.\n\n(F) The empirical details are lacking for both reproducibility and understanding of results. How large are the dataset sizes? How many classes are there? These details significantly influence DP learning.\n\n(G) Figure 5 is also extremely difficult to interpret. Looking at it, it looks like this model can be trained in <10 steps. This is quite surprising. How many steps does non-private training take?\n\n(H) Figure 4 and several tables indicate that LMO-DP with a full order magnitude less privacy cost can outperform DP-SGD. This is an outstanding feat that is likely due to weakness E) above. Understanding how this manifests is crucial and is currently underexplored.\n\n(I) Though I am not very familiar with the work of Mohammady et al. (2020), their work requires specifying both the utility metric and the query (to be protected by DP). Theorem 4.1 currently only shows the loss/utility metric (cross-entropy) but does not define the query. This makes it seem as though the query is acutally releasing the loss and not the gradients which would lead to privacy leakage. Please clarify."
            },
            "questions": {
                "value": "(*) in strengths.\n\n(F) In weaknesses.\n\n(I) In weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6951/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699239728637,
        "cdate": 1699239728637,
        "tmdate": 1699636811342,
        "mdate": 1699636811342,
        "license": "CC BY 4.0",
        "version": 2
    }
]