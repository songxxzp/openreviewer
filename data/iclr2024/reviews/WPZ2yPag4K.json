[
    {
        "id": "HxnXA8MZrV",
        "forum": "WPZ2yPag4K",
        "replyto": "WPZ2yPag4K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8435/Reviewer_mN58"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8435/Reviewer_mN58"
        ],
        "content": {
            "summary": {
                "value": "This paper studies approaches to improve the factuality of language models (LMs) by fine-tuning the LMs using reinforcement learning (the specific algorithm is DPO), and the two main types of the reward function are: 1) reference-based FactScore (referred to as \u201cDPO-FS\u201d) and 2) reference-free uncertainty measure based on self samples (referred to as \u201cDPO-MC\u201d). \n\nThe authors conduct the experiments on two tasks: 1) free-form generation of biographies and 2) medical question answering. The datasets were crafted specifically for their experiments, hence resulting in the small size (e.g., biography train/test = 296/59 and medical QA train/test = 150/50 instances). The base LM is Llama1 and Llama2. The main experimental results show that both DPO-FS and DPO-MC generate responses with a higher \u201ccorrect\u201d percentage than baselines (SFT and inference-time methods such as ITI and DOLA). Also, DPO-FS and DPO-MC achieve a higher correct percentage than Llama-2-chat. Lastly, the authors perform a human evaluation to validate the findings previously evaluated using GPT3.5."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper shows that DPO can be applied to improve the factuality of LMs as shown by DPO-FS and DPO-MC achieving better factuality, and to the best of my knowledge, the factuality-based reward has not been investigated yet. The paper also investigates both reference-free and reference-based, and shows the effectiveness of both methods."
            },
            "weaknesses": {
                "value": "1. Although existing work may have not used a factuality-based reward, the results in this paper are mostly the expected observations (e.g., applying RL-based training improves target rewards). For example, (Lu et al., 2022) applied RL (PPO) with a reward based on an external metric to improve toxicity, repetition, etc.\n\n2. The main findings (Tables 2, 3, 4, 5) are all based on GPT3.5 evaluation, and coupled with the fact that the test sets are small (e.g., 59 instances for biographies & 50 instances for medical QA), I\u2019m not certain how reliable the results are. Also, there is not much information in Section 5.5 about human evaluation, e.g., inter-annotator agreement, or how many annotators were employed.\n\n3. How does the DPO fine-tuned model perform on out-of-domain tasks? For example, when fine-tuning to improve factuality on biographies, does it also improve factuality on medical QA? And does its general performance change?\nThere is also a recent survey paper (Pan et al., 2023) about aligning LLMs for different aspects (including hallucination/factuality), and I think it would be useful for authors to incorporate additional relevant papers (i.e., those that apply RL to improve LMs) \n\n4. Weak base LM: This work uses Llama-7B as the base model, and at this size, the model may not yet be highly capable of long-form generation / medical QA. Previous works such as (Manakul et al., 2023) and (Mundler et al., 2023) investigated LLM hallucination with much larger LLMs (e.g., GPT3.5/4). It would be interesting to see, for example, when using larger models (either open-source such as larger Llama / Falcon-180B or private ones such as GPT-4), if the model still makes as many factual errors (because if they don\u2019t \u2013 due to the emergent ability when scaling up \u2013 fine-tuning may not be necessary or have little impact).\n\nThere is a recent survey paper (Pan et al., 2023) about aligning LLMs for different aspects (including hallucination/factuality), and I think it would be useful for authors to incorporate additional relevant papers (i.e., those that apply RL to improve LMs) \n\n*References*\n- (Lu et al., 2022) QUARK: Controllable Text Generation with Reinforced Unlearning\n- (Manakul et al., 2023) SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models\n- (Mundler et al., 2023) Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation\n- (Pan et al., 2023) Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies"
            },
            "questions": {
                "value": "My questions are related to the points in the weaknesses section. I'm looking forward to seeing your responses to the weaknesses above, especially point number 3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8435/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8435/Reviewer_mN58",
                    "ICLR.cc/2024/Conference/Submission8435/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8435/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697758264819,
        "cdate": 1697758264819,
        "tmdate": 1700739511286,
        "mdate": 1700739511286,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EMBilxuGJs",
        "forum": "WPZ2yPag4K",
        "replyto": "WPZ2yPag4K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8435/Reviewer_7K7x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8435/Reviewer_7K7x"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes fine-tuning language models to improve their factuality. Specifically, one reference-based and one reference-free method are explored to estimate the truthfulness of different model responses, the scoring/preference of which are then used to fine-tune the LMs with direct preference optimization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper writing is of high quality and easy to follow\n\n- The proposed method, regardless of reference-based or reference-free, shows improved factuality than the SFT baseline and the highest correct% among the compared methods.\n\n- The paper provides analysis and ablations of different variants such as fine-tuning the pretrained/chat models and combining with inference-time decoding method."
            },
            "weaknesses": {
                "value": "- [major] I have some concerns about the evaluation\n  - The test sets (50 and 59 examples in each domain, respectively) look very limited, making it bit hard to understand the actual improvement of model factuality.  How reliable are the results? Is 75% -> 81% a lot? I can't really answer these questions after reading the paper.\n  - I noticed that the total number of claims are often different for different methods. Could generation style (e.g., length) contribute to the seemingly better/worse results? I wonder if the authors have considered such factors.\n  - There is also no evidence indicting the improved factuality doesn't come at the expense of performance in other aspects. I understand the authors may not have enough labor/compute for a more comprehensive eval like GPT or Llama but LLMs, in my experience, can behave in mysterious ways when you over index on one specific objective.\n\n- [minor] The method is somewhat straightforward, which is not necessarily a bad thing if the evaluation can show meaningful improvements (that are worth fine-tuning specifically for factuality) than methods that modify decoding only."
            },
            "questions": {
                "value": "- I'm a little confused why choosing the largest bin to measure truthfulness in the reference-free setting. Does that mean the atomic claim doesn't really matter (\"Yo-Yo Ma was born in 1951\" and \"Yo-Yo Ma was born in 1955\" would both be converted to \"What year was Yo-Yo Ma born\")? So as long as two responses make a claim on the same fact, regardless if it's correct or wrong, they will receive the same truthfulness score? If the hypothesis is \"a language model\u2019s confidence in a generated answer is highly correlated with the probability that the answer is correct\", why not use the distribution to cross-check like in the reference-based setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8435/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8435/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8435/Reviewer_7K7x"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8435/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698458750306,
        "cdate": 1698458750306,
        "tmdate": 1700756117087,
        "mdate": 1700756117087,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "D4AH2d95MH",
        "forum": "WPZ2yPag4K",
        "replyto": "WPZ2yPag4K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8435/Reviewer_Pzp4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8435/Reviewer_Pzp4"
        ],
        "content": {
            "summary": {
                "value": "Authors propose to finetune Llama models for factuality in long-form generation tasks using DPO on automatically constructed preference pairs. Authors explore 2 methods for generating preference ratings: 1) Reference-based: Extracts atomic facts using GPT-3.5 and then use Lama-1-7B-based NLI model to determine correctness of each atomic fact with respect to the reference. Percentage of correct atomic facts is used to compare the factual correctness of samples. 2) Reference-free: Extracts facts using GPT-3.5, then use GPT-3.5 to convert a fact to a question (uses few-shot prompting). Then, through sampling answers multiple times from the model, they estimate the model's uncertainty for the actual answer. The model's uncertainty is used to compare the factual correctness of samples.\n\nThey evaluate their approach on two tasks: biography generation and open-ended medical QA. To accommodate for the reference-based metrics, they generate data based on individuals (for biographies) and medical conditions that have Wikipedia pages.\n\nResults show superior factual accuracy for DPO-based models on both tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- New results to show the benefit of using automated feedback for improving LLMs, targeting factuality for long-form open-ended generation.\n\n- Paper is well-written, experimental settings are well-defined, human evaluation is performed.\n\n- Paper also shows that DPO-finetuning is complementary to decoding-time factuality improvement method (DOLA), (DPO + DOLA outperforms DPO)"
            },
            "weaknesses": {
                "value": "- Idea itself is not novel, RLAIF has been consistently shown to be useful (here, authors used DPO instead of PPO). Though the application is new.\n\n- Including more fine-tuning based baselines can help understand the role of automated metrics. E.g., directly using prompts to compare factuality of two outputs w.r.t. the wikipedia article instead of extracting atomic facts. \n\n- Both DPO variants reduce number of correct facts on biography generation. This does not seem very surprising, given the optimized metric is the percentage of correct atomic facts. For example, a sample with 10 correct and 5 incorrect is preferred over 11 correct and 6 incorrect. Can this bias be removed from the fine-tuned model, maybe by changing the metric or comparing samples of similar lengths? Or is it the bias of evaluation metric?"
            },
            "questions": {
                "value": "Check questions in the Weakness section.\n\n- Between reference-free and reference-based metrics, there is a significant difference in the total number (correct + incorrect) of generated facts (almost 30% fewer) on biographies. What's the source of this bias, any possible explanations?\n\n- Could you provide statistics on the number of tokens in wining vs losing samples in all cases (dataset/model/metric)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8435/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8435/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8435/Reviewer_Pzp4"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8435/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785524107,
        "cdate": 1698785524107,
        "tmdate": 1699637051588,
        "mdate": 1699637051588,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oiDasYwPxq",
        "forum": "WPZ2yPag4K",
        "replyto": "WPZ2yPag4K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8435/Reviewer_2run"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8435/Reviewer_2run"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors construct a direct preference optimization (DPO) dataset for improving factuality using reference-based and reference-free truthfulness annotation techniques. Through the proposed method, they improve accuracy in two tasks (Biographies and Medical QA) without human factuality labels. The authors demonstrate that the proposed method (DPO-FS and DPO-MC) can be applied to Llama-2 and Llama2-Chat, and combined with a factuality-decoding approach (e.g., DOLA)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Motivation is intuitive and easy to understand\n- The proposed method improves the truthfulness of LLM without human factuality labels\n- The proposed method can be augmented with existing orthogonal approaches for factuality"
            },
            "weaknesses": {
                "value": "- Because the framework is simple and the method of scoring truthfulness and fine-tuning technique uses existing approaches, the proposed method appears to have limited contributions"
            },
            "questions": {
                "value": "In Biographies and Medical QA tasks, are DPO-FS, DPO-MC, and SFT fine-tuned on the training set of each dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8435/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8435/Reviewer_2run",
                    "ICLR.cc/2024/Conference/Submission8435/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8435/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808718382,
        "cdate": 1698808718382,
        "tmdate": 1700727151440,
        "mdate": 1700727151440,
        "license": "CC BY 4.0",
        "version": 2
    }
]