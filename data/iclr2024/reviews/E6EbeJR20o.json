[
    {
        "id": "FoSF8FZzcw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission12/Reviewer_B8S8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission12/Reviewer_B8S8"
        ],
        "forum": "E6EbeJR20o",
        "replyto": "E6EbeJR20o",
        "content": {
            "summary": {
                "value": "The paper proposes a novel method, i.e. the neural re-parameterized optimization, for optimizing FLAME parameters from a video sequence. Then, the paper applies the proposed technique to several large-scale in-the-wild video datasets. The fitting results form a novel dataset called NeuFace-dataset, which is another contribution of the paper. The paper also demonstrates the usage of the proposed dataset on face reconstruction and motion-prior learning."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* I do like the proposed neural re-parameterized technique for optimizing FLAME parameters, it provides insights for me. Sparse gradients are not desired for geometry optimization. By optimizing the neural network weights to indirectly optimize the FLAME geometry, dense gradients can be obtained from sparse landmark loss as shown in Fig.3.\nHowever, in the context of optimizing per-vertex displacement (this task is more challenging than optimizing 3DMM parameters), similar techniques [1][2] are proposed to obtain dense gradients. I suggest the authors discuss these more relevant works in the main paper.\n* The proposed dataset would benefit future research in this field.\n\n[1] Neural head avatars from monocular rgb videos, CVPR 2022.\n\n[2] Large steps in inverse rendering of geometry, SIGGRAPH Asia 2021."
            },
            "weaknesses": {
                "value": "My main concerns are listed as follows.\n\n1. I agree that the paper proposes the first method to introduce the neural re-parameterized technique to optimize FLAME parameters. As I know, previous work [1] has already introduced this technique to solve very relevant (actually more challenging) tasks; they adopt a neural network to re-parameterize the per-vertex displacement of a FLAME mesh. However, the submission does not discuss these very relevant works.\n\n2. I think the video FLAME fitting algorithm is not well-designed. One of the contributions of the paper is to use the proposed fitting algorithm to provide pseudo GT for the video dataset, so I think is necessary to push the quality of the fitting method as high as possible. I list some questions about the design choice here:\n* Why not use a photometric loss term? Many previous works have demonstrated that photometric loss can improve the geometry reconstruction quality, see Figure 6 in [2]. I'm curious when photometric loss is used, will the proposed neural re-parameterized technique still improve the results a lot?\n* Why not use a shared identity code?\n* From the supp. video, I find that the fitting results are still jittering, although better than the naive baseline of applying DECA to each frame separately. More advanced strategies beyond the temporal moving average method should be exploited, like the common-used optical flow loss, or stabilize the detected landmarks [3].\n* Thus, it gives me the impression that the paper does not do its best to improve the video-fitting results. I would like to hear from the authors to change my impression.\n\n3. Apply DECA to each frame is too weak to serve as a baseline for video fitting. There are many previous works [4-6] proposed to fit 3DMM into a video. Why not compare with these more strong baselines specialized in video fitting? In the supp. material, the method is compared to MICA-T, but I think more competitors are expected. At least they should be discussed in the paper. So I think the paper does not well evaluate the proposed technique for video fitting.\n\n[1] Neural head avatars from monocular rgb videos, CVPR 2022.\n\n[2] State of the Art on Monocular 3D Face Reconstruction, Tracking, and Applications\n\n[3] High-Resolution Neural Face Swapping for Visual Effects\n\n[4] 3D Shape Regression for Real-time Facial Animation\n\n[5] Real-time high-fidelity facial performance capture\n\n[6] Face2Face: Real-time face capture and reenactment of rgb videos"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission12/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission12/Reviewer_B8S8",
                    "ICLR.cc/2024/Conference/Submission12/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission12/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697182202148,
        "cdate": 1697182202148,
        "tmdate": 1700707130444,
        "mdate": 1700707130444,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "USsm9RVAsM",
        "forum": "E6EbeJR20o",
        "replyto": "E6EbeJR20o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission12/Reviewer_C76P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission12/Reviewer_C76P"
        ],
        "content": {
            "summary": {
                "value": "This article introduces a new video dataset with 3D face mesh pseudo-labels and provides a method for annotating spatio-temporally consistent 3D face meshes for existing multi-view facial video data. Based on the results provided by the authors, this dataset is valuable for related research."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The dataset introduced by the authors exhibits clear advantages in terms of data quantity, annotation accuracy, and spatio-temporal consistency, as evidenced by the provided data examples. These strengths are valuable for advancing research in the relevant field. Additionally, the optimization method proposed for achieving spatio-temporal consistency seems effective."
            },
            "weaknesses": {
                "value": "1. Unfair Comparison: The comparison with methods like DECA and EMOCA, which operate on single-view data (DECA-dataset and EMOCA-dataset), cannot utilize multi-view information. It can be argued that the proposed method leverages more information by utilizing multi-view data. Therefore, comparing the proposed multi-view approach to these single-view reconstruction methods may not provide a fair evaluation.\n\n2. The novelty is limited. The proposed temporal-consistency-loss and multi-view-consistency-loss seem more like separate regularizations (or averages) applied to pose, camera parameters, or face shape and expression coefficients to achieve reduced jitter in the reconstructed videos.\n\nI have doubts about the effectiveness of the multi-view-consistency-loss.  In the training set, only the MEAD dataset consists of multi-view video data, while VoxCeleb2 and CelebV-HQ have only single-view video data. Consequently, it appears that only the MEAD dataset can effectively leverage the multi-view consistency loss.  Table 1 illustrates that MEAD comprises a mere 1% of the total duration, suggesting that the majority of the proposed NeuFace-dataset primarily derives from VoxCeleb2 and CelebV-HQ.  In essence, it seems to be a data processing outcome achieved by applying inter-frame smoothing to existing methods. Although I appreciate the authors' effort and the contribution of NeuFace-dataset to the community, the paper's level of innovation may fall slightly below the standard typically expected at ICLR."
            },
            "questions": {
                "value": "1. As mentioned in the paper, the proposed dataset contains a large amount of data, and the preliminary 3D mesh results generated based on DECA (EMOCA) may have errors. Have the authors considered how to filter out failed reconstruction results?\n2. The quality of the reconstructed results for extreme facial expressions appears suboptimal. For instance, in Figure 5's top-left corner, where the open-mouth expression is depicted, the reconstruction of the mouth region does not seem consistent with the original input. Additionally, there appear to be imperfections in the reconstruction of closed-eye expressions.\n3. Given the analysis above, while the dataset's scale is certainly commendable, there seems to be room for improvement in terms of reconstruction accuracy. It might be worthwhile for the authors to consider utilizing such data as annotations for 3D landmarks rather than 3D mesh data. Additionally, have the authors explored the possibility of applying their proposed method to a different face model, such as the Basel Face Model (BFM), or investigating alternative pre-trained models instead of DECA or EMOCA?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission12/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638237250,
        "cdate": 1698638237250,
        "tmdate": 1699635924802,
        "mdate": 1699635924802,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DC6e0UeMhb",
        "forum": "E6EbeJR20o",
        "replyto": "E6EbeJR20o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission12/Reviewer_GPao"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission12/Reviewer_GPao"
        ],
        "content": {
            "summary": {
                "value": "In this proposed method, a 3D face database is built based on the neural radiance fields method. In general, the neural face representation tries to find the best 3D mesh representation through the neural network parameters to best fit the multiple views and temporal consistent faces. Multiview and temporal consistency losses are added on top of the 2D landmark loss in the EM-like optimization process. Based on the proposed method, a significantly larger 3D face database is built using existing public 3D videos. The authors also demonstrated the possible applications of the proposed database to improve 3D face reconstruction and to learn the 3D face prior."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed 3D face database is beneficial to the research community. The proposed database is significantly larger than the typical 3D face datasets. Experimental results also demonstrate good 3D estimation results."
            },
            "weaknesses": {
                "value": "The theoretical novelty of face reconstruction using Neural network parameterization is incremental."
            },
            "questions": {
                "value": "What are the typical failure cases of the proposed method? In the EM-like optimization, does the reconstruction always go to the right optional result?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission12/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698641748665,
        "cdate": 1698641748665,
        "tmdate": 1699635924709,
        "mdate": 1699635924709,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "md9JBrR4va",
        "forum": "E6EbeJR20o",
        "replyto": "E6EbeJR20o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission12/Reviewer_axFo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission12/Reviewer_axFo"
        ],
        "content": {
            "summary": {
                "value": "The paper presents NeuFace, an optimization algorithm for fitting a morphable model to a sequence of multi-view face images. To this end it refines a pre-trained NN to fit the target images. The optimized loss includes temporal and multi-view regularization terms minimizing the distance of the reconstructed mesh to the temporal moving average, in the case of the temporal term, and to the aligned average, in the multi-view loss case. The model is iteratively refined by alternating the estimation of the reconstructions used in the regularization terms with the optimization of the network parameter that minimize the loss.\n\nThe experimentation quantitatively compares the reconstructions with two competing algorithms, DECA and EMOCA, on MEAD, VoxCeleb2 and CelebV-HQ video datasets.\n\nFinally, the algorithm is used to build the \"NeuFace dataset\" as the result of the reconstruction of the 3D face meshes in these three datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper reads well and is properly set in the research context. It addresses a relevant problem, namely, 3D face landmark estimation, with many practical applications and open challenges. The paper contributes with a new dataset and shows that by using it we may improve the accuracy of different face processing algorithms. This will be of interest to the face processing community."
            },
            "weaknesses": {
                "value": "The the paper claims to investigate the reconstruction of image-aligned facial details on 3D meshes. However, the approach is based on optimizing the parameters of a 3DMM model, with the limitations of a linear model to represent fine facial details.\n\nIn the vertex accuracy evaluation experiments described in Sec. 3.4 and shown in Fig. 4, NeuFace optimization is compared with DECA and plain FLAME fitting. The paper does not describe the details of this experiment, specifically what are the train, validation and test data used for evaluating each algorithm. DECA results were produced by the plain pre-trained DECA model. We may assume that NeuFace optimization, as described in Sec. 3.2, was trained with some part of VOCASET and evaluated on a different part of it. It does not seem like a fair comparison, since DECA did not have the chance to see any part of VOCASET.\n\nFor the same reason, the quantitative comparisons in Table 2 seem also unfair, since the optimizations in NEUFACE-*-datasets could see part of MEAD, VoxCeleb2 and CelebV-HQ data, whereas those involving DECA and EMOCA datasets did not. In Sec. A.3 and Table S1 we can see that if we give DECA the chance to be refined on these datasets, the NME in MEAD reduces to 2.44, much lower than 4.65 shown in Table 2."
            },
            "questions": {
                "value": "There are important details missing:\n- Section 3.1 It would be good if you extended the explanation by adding the dimension of each FLAME parameter. Also, the backbone network, e.g. DECA, not only estimates the 3DMM parameters, but also texture, lighting and a displacement map to model details outside the 3DMM linear model. I understand that the approach discards the texture and lighting part, but what about the displacement map? \n- Section 3.2 does not explain how the ground-truth landmarks for equation 2 were obtained. Also, in the Multi-view consistency loss it does not explain where the confidence values for each vertex come from.\n- Experiments. The paper must clearly explain what is the train/validation/test data used in each experiment and confirm that the results shown in the accuracy evaluation in Fig.4 and Table 2 are correct and fair.\n\nOften the base pre-trained model fails dramatically. In this situation, averaging the estimated mesh with others in the regularization terms would ruin the optimization, since the average operation is not robust. Would an alternative robust operation, e.g. median, improve the results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety",
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper reads \"Since our dataset is acquired based on the existing public video datasets (Wang et al., 2020; Chung et al., 2018; Zhu et al., 2022), all the rights, licenses, and permissions follow the original datasets.\" However, some of these datasets were automatically gathered from the internet. So, it is unclear whether the new dataset is legally compliant."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission12/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785449631,
        "cdate": 1698785449631,
        "tmdate": 1699635924631,
        "mdate": 1699635924631,
        "license": "CC BY 4.0",
        "version": 2
    }
]