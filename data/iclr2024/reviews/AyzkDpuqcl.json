[
    {
        "id": "o71P8ykLYg",
        "forum": "AyzkDpuqcl",
        "replyto": "AyzkDpuqcl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1070/Reviewer_KQ36"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1070/Reviewer_KQ36"
        ],
        "content": {
            "summary": {
                "value": "The authors study methods for training and sampling from energy-based models (EBMs). \n\nThey propose Cooperative Diffusion Recovery Likelihood (CDRL), which is an extension of Diffusion Recovery Likelihood (DRL). The proposed CDLR aims to improve the sample quality of DLR, while also reducing the number of MCMC steps required during training and sampling. CDLR entails jointly estimating a sequence of EBMs and MCMC initializers, utilizing the cooperative training approach. Essentially, the main proposed method is a combination of DLR (Gao et al., 2021) and cooperative training (Xie et al., 2020).\n\nThe proposed method is shown to improve the sample quality of DLR in unconditional generation on CIFAR10 and ImageNet 32x32. It is also applied to some conditional generation (via classifier-free guidance), compositional generation and OOD detection experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well written overall. I found it interesting to read. The authors definitely seem knowledgeable and familiar with previous work.\n\nThe main idea of the proposed method, combining DRL with cooperative training, makes intuitive sense and is described well overall.\n\nThe proposed method seems to improve the DLR baseline in experiments."
            },
            "weaknesses": {
                "value": "The related work section is placed in the Appendix, which seems quite odd. As a result, the previous work on cooperative training is sort of \"hidden\" in the main paper. This should be described before Section 3.3.\n\nThe similarities and differences between the proposed approach using EBMs and diffusion models could be discussed in more detail. The \"Diffusion Model\" paragraph in the related work section is interesting, but I think this could be expanded on a lot more."
            },
            "questions": {
                "value": "1. I think that the proposed CDLR method makes sense, and that it seems like a promising method for improving EBM training/sampling. However, it is not entirely clear to me how the resulting EBM differs from diffusion models? What are the main pros and cons compared to diffusion models? Why should one use this type of EBM instead of a diffusion model? When / in which applications should one use this type of EBM?\n\n2. Would it be possible to compare CDLR with the DLR baseline also in the OOD detection experiment (Table 4)?\n\n3. Would it perhaps be possible to illustrate / give a schematic overview of the proposed approach in some kind of figure? (a sequence of noise levels, one EBM and initializer model per level etc. I just think that this perhaps could help illustrate the general idea)\n\n4. The experiment in Figure 7 is interesting. Could you report FID scores for (a) and (b)? I.e., for K=0 and K=15 steps of Langevin refinement. Could this experiment perhaps also be repeated for K=1, 2, 4, 8 steps of Langevin refinement? \n\n\n\nMinor things:\n- Section 3.1, \"...constrains the conditional energy landscape to be localized around y_t\": Should y_t be x_{t+1}?\n- Abstract, \"noisy versons\": versons --> versions.\n- Section 1, \"The initializer model proposes initial samples by making prediction of the samples at the current noise level given\": \"making a prediction\"? \"making predictions\"? \"by predicting the samples...\"?\n- Section 3.2, \"which may again requires MCMC sampling\": requires --> require?\n- Algorithm 1: noise level --> noise levels, sampling step --> sampling steps.\n- Algorithm 2: sampling steps L --> sampling steps K? (also some inconsistency with this in the Appendix)\n- 4.4: CDLR strong --> CDLR achieves strong?\n- G.5: doesn't gives --> doesn't give (or, \"does not give\", I suppose)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1070/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1070/Reviewer_KQ36",
                    "ICLR.cc/2024/Conference/Submission1070/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1070/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698402198166,
        "cdate": 1698402198166,
        "tmdate": 1700667189210,
        "mdate": 1700667189210,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ttChdjyUyq",
        "forum": "AyzkDpuqcl",
        "replyto": "AyzkDpuqcl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1070/Reviewer_S2Sh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1070/Reviewer_S2Sh"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a new method for learning energy-based models (EBMs) mimicking diffusion models. Specifically, within the diffusion-model framework, EBMs are modified to parameterize the denoising process; to accelerate the MCMC sampling process when training the EBM, the authors also use a simultaneously trained diffusion-like model as an ``initialzer model.'' Experimental results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The presented techniques are likely new."
            },
            "weaknesses": {
                "value": "The clarity should be improved. For example, consider the relationships between the proposed method and related works.\n\nThe contribution is kind of incremental. Based on the Diffusion Recovery Likelihood (DRL) (Gao et al., 2021) in Sec. 3.1, the authors proposed in Sec. 3.2 a new \"initializer model\" to amortize the expensive MCMC sampling."
            },
            "questions": {
                "value": "1. What are the main contributions of the proposed CDRL when compared with DRL (Gao et al., 2021)?\n\n2. In the paragraph following Eq. (5), why \"maximizing recovery likelihood still guarantees an unbiased estimator of the true parameters of the marginal distribution of the data?\"\n\n3. Eqs. (6) and (8) are quite like the fomula of the conventional diffusion model. Please elaborate on the relationships between them.\n\n4. It seems that Eq. (9) indicates a deterministic noising process, right? If so, why \"there\u2019s still variance for xt given x0 and xt+1?\"\n\n5. How to interpret Eq. (11), especially the $\\tilde p_{\\theta}$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1070/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1070/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1070/Reviewer_S2Sh"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1070/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698551301221,
        "cdate": 1698551301221,
        "tmdate": 1700725137938,
        "mdate": 1700725137938,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PFJWe3iZ09",
        "forum": "AyzkDpuqcl",
        "replyto": "AyzkDpuqcl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1070/Reviewer_nfgg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1070/Reviewer_nfgg"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the Cooperative Diffusion Recovery Likelihood (CDRL) for training energy-based models. Compared with the baseline DRL, CDRL introduces an extra initializer model that is jointly trained with the EBM along the diffusion process. The required MCMC steps are reduced thanks to a better initial sample.  \nThe experiments show that CDRL has significantly improved over the baseline DRL method. \n\nBesides the proposed method, the paper also discusses some other aspects of EBMs such as noise scheduler designs, classifier-free guidance, etc."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-organized and easy to follow. \nI think the highlight of this paper is the strong empirical performance of CDRL. The work shows the possibility of achieving an FID on image benchmarks at least comparable to other generative models such as GANs and diffusion models. \nThe ablation studies are convincing as validation of each of the components in the proposed method.\n\nBesides, the introduced classifier-free guidance for EBM is also empirically sound, which verifies many intuitions from diffusion models."
            },
            "weaknesses": {
                "value": "My main concern is the lack of novelty. \nThe key components are inspired by other works. The DRL is well-established in training EBMs and the idea of a trainable initializer for MCMC is also not new. The combination of them seems a bit hacky to me and the overall method is a little cumbersome.\nIt would be better if the motivation and necessity of introducing an initializer (because initializers bring additional costs) were explained more clearly with more new insights to DRL. \n\nNonetheless, making this idea work empirically is impressive and the generative performance of EBMs has been greatly improved. \nHowever, the image generation experiment is on very low resolutions. I think it would be more convincing if the method could be shown to be able to handle image generation tasks for higher resolutions;"
            },
            "questions": {
                "value": "The performance gain, regardless of efficiency, seems to come from a better initial state provided by the cooperatively trained initializer. In this case, I wonder about the limit of the baseline CRL, if we increase the MCMC sampling steps significantly, say 300, what would the performance be?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1070/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1070/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1070/Reviewer_nfgg"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1070/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699022321829,
        "cdate": 1699022321829,
        "tmdate": 1700702849250,
        "mdate": 1700702849250,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CHO2cPQKW1",
        "forum": "AyzkDpuqcl",
        "replyto": "AyzkDpuqcl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1070/Reviewer_cEiK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1070/Reviewer_cEiK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes several techniques for improved training of diffusion recovery likelihood models, such as\n- learning an initializer model for MCMC,\n- noise variance reduction for reducing gradient variance.\n\nThis paper shows application of the proposed model to\n- conditional generation and classifier-free guidance,\n- compositional generation,\n- OOD detection."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is clearly written and easy to read.\n- CDRL shows clear performance improvements over previous EBM-based generative models.\n- CDRL can be applied to tasks such as unconditional generation, conditional synthesis, likelihood estimation, OOD detection, composition, etc. Such tasks were unexplored in the original DRL paper [1].\n\n[1] Learning Energy-Based Models by Diffusion Recovery Likelihood, Gao et al., ICLR, 2021."
            },
            "weaknesses": {
                "value": "I am inclined to give the score \"marginal accept\" for the following reasons.\n- The proposed method is a straightforward combination of two known techniques, DRL [1] and MCMC amortization [2,3]. While the simplicity of the idea is practically appealing, the idea lacks theoretical novelty. Moreover, the tasks demonstrated in the paper are already well-explored in the diffusion model literature, and the algorithms for the tasks are straightforward extensions of diffusion-based ones, as the score is just the gradient of the energy. For instance, classifier-free guidance is explored in [4], and compositional generation is explored in [5].\n- The paper lacks a comparison of inference time for CDRL and the baselines.\n- The paper lacks a comparison of training cost (e.g., required VRAM) for CDRL and the baselines. I expect CDRL training is more expensive than diffusion or DRL training, as the former requires two networks while latter only one.\n- The paper lacks results on higher resolution (e.g., 256x256) images.\n\n[1] Learning Energy-Based Models by Diffusion Recovery Likelihood, Gao et al., ICLR, 2021.\n\n[2] Approximate Inference with Amortized MCMC, Li et al., 2017.\n\n[3] Learning Energy-Based Prior Model with Diffusion-Amortized MCMC, Yu et al., NeurIPS, 2023.\n\n[4] Classifier-Free Diffusion Guidance, Ho et al., 2022.\n\n[5] Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC, Du et al., ICML, 2023."
            },
            "questions": {
                "value": "- I am curious about the authors' opinion on the practical benefits of CDRL vs. diffusion. Specifically, tasks that can be achieved with CDRL can also be achieved by diffusion, and vice versa, using the relation that the gradient of the energy is the score. I also observe that hyper-parameter choices in this paper are heavily influenced by those of diffusion models. Moreover, while CDRL uses fewer noise levels than diffusion models (e.g., 1000 levels), recent works on fast diffusion sampling have reduced sampling time for diffusion significantly (e.g., 2.87 FID on CIFAR10 with NFE=20 [1]). I think this naturally leads us to wonder what are the strengths of CDRL compared to diffusion.\n\n[1] DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps, Lu et al., NeurIPS, 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1070/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1070/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1070/Reviewer_cEiK"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1070/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699114203042,
        "cdate": 1699114203042,
        "tmdate": 1699636033509,
        "mdate": 1699636033509,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JfXRWyvHBb",
        "forum": "AyzkDpuqcl",
        "replyto": "AyzkDpuqcl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1070/Reviewer_9dWK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1070/Reviewer_9dWK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel training algorithm for jointly training an energy-based model (EBM) and an initializer model. The initializer has a very similar form to DDPM, so the proposed method can be viewed as a cooperation of EBM and DDPM."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The overall exposition is clear to follow.\n* The proposed algorithm, CDRL, shows a clear improvement over Diffusion Recovery Likelihood (DRL), an algorithm that CDRL is based on. The sample quality (in FID) is improved while the required computation (in MCMC steps) is reduced.\n* CDRL demonstrates broad applicability over multiple tasks outside image generation, such as compositional generation and out-of-distribution detection."
            },
            "weaknesses": {
                "value": "* The empirical performance of CDRL is good but not very strong. It is clear that CDRL is an improvement over DRL, but it still falls behind other models in multiple tasks.\n    * The initializer is very similar to DDPM, but CDRL's unconditional CIFAR-10 FID is worse than DDPM (Table 1).\n    * Also in Table 3, CDRL is outperformed by DDPM++.\n* In the out-of-detection distribution (OOD) experiment (Table 4), OOD detection capability is evaluated on only three test datasets, and the number of test OOD datasets is too small. It is important to use diverse test OOD datasets in evaluation because an OOD detector needs to detect any possible outliers. The current practice typically uses at least five OOD datasets that have different visual characteristics. Also, SVHN is known to be a highly challenging OOD dataset, particularly for generative models [1], but not included in Table 4. It would be great to see CDRL is able to detect SVHN as OOD.\n* It would be nice if the authors could comment on the consistency of the model after adding the initializer model. Would it alter the consistency?\n\n[1] Nalisnick et al., Do Deep Generative Models Know What They Don't Know?, https://arxiv.org/abs/1810.09136"
            },
            "questions": {
                "value": "See weaknesses.\n\n===\nThe authors have addressed my questions in detail."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1070/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1070/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1070/Reviewer_9dWK"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1070/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699239116388,
        "cdate": 1699239116388,
        "tmdate": 1700650944145,
        "mdate": 1700650944145,
        "license": "CC BY 4.0",
        "version": 2
    }
]