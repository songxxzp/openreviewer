[
    {
        "id": "aSgfqRoqQV",
        "forum": "1rgMkDWfYV",
        "replyto": "1rgMkDWfYV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3300/Reviewer_tEQg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3300/Reviewer_tEQg"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the integration of pretrained vision-language models, like CLIP, into the process of learning from noisy labels. To this end, the authors introduce a method called CLIPSelector, which leverages CLIP's powerful zero-shot classifier and an easily-inducible classifier based on CLIP's vision encoder to select clean samples. Additionally, they introduce a semi-supervised learning approach called MixFix to gradually incorporate missing clean samples and re-label noisy samples based on varying thresholds to enhance performance. The authors validate their approach through a series of experiments on different benchmarks, including datasets with synthetic and real-world noise."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper breaks new ground by exploring the use of pretrained vision-language models, such as CLIP, to address the challenge of noisy labels. This approach is promising as it goes beyond relying solely on information from the noisy dataset.\n2. The fixed hyperparameters across all experiments showcase the robustness and practicality of the proposed method."
            },
            "weaknesses": {
                "value": "1. My major concern is the potential unfair comparisons. The notable performance improvements shown in Tables 2-4 could be attributed to CLIP's superior representation learning capabilities. A fairer comparison could involve replacing the baselines' backbone with CLIP's visual encoder. Furthermore, Table 3 lacks comparison results with recent works focused on instance-dependent noise from 2022-2023.\n2. Discrepancies between the CLIP's zero-shot results on CIFAR in Table 1 and the original paper need clarification.\n3. The claims regarding inferior performance on Red Mini-ImageNet require more explanation and context.\n4. What does SOTA in Table 1 means? Please supplement the necessary details.\n5. Ambiguous statements like \"on the clean test set of in-question noisy datasets\" should be elucidated to enhance clarity.\n6. The derivation of Eq. 4 from Eq. 3 is not explained, and the effect of the added class feature in the prompt remains unclear. Additional ablation studies are necessary to substantiate these claims."
            },
            "questions": {
                "value": "Please see the weaknesses, thx"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3300/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698751659621,
        "cdate": 1698751659621,
        "tmdate": 1699636279061,
        "mdate": 1699636279061,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LlbIF7IF7G",
        "forum": "1rgMkDWfYV",
        "replyto": "1rgMkDWfYV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3300/Reviewer_Vnif"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3300/Reviewer_Vnif"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes using the pre-trained vision-language model CLIP for sample selection to mitigate selfconfirmation\nbias. Specifically, they introduce the CLIPSelector, which utilizes both the CLIP\u2019s zero-shot\nclassifier and an easily-inducible classifier based on its vision encoder and noisy labels for sample selection.\nAnd they further introduce a semi-supervised learning method called MixFix."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well presented and explains the algorithm and experiments clearly.\n2. The experiments are conducted on various datasets."
            },
            "weaknesses": {
                "value": "1. The performance lacks some competitiveness. Some methods are not compared, for example, SSR: An\nEfficient and Robust Framework for Learning with Unknown Label Noise.\n2. The main idea of the paper is to use the CLIP zero-shot classifier for sample selection and lacks novelty. And\nthe semi-supervised learning methods has also been applied in previous works."
            },
            "questions": {
                "value": "1. This paper uses the CLIP pre-trained model, I think this is unfair for previous works without pre-trained model. Combining\nprevious methods with training from CLIP pre-trained model other than training from scratch should also be compared.\n2. Equations (2) misses )."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3300/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758088315,
        "cdate": 1698758088315,
        "tmdate": 1699636278976,
        "mdate": 1699636278976,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i3rIt3w2hQ",
        "forum": "1rgMkDWfYV",
        "replyto": "1rgMkDWfYV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3300/Reviewer_GWvS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3300/Reviewer_GWvS"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the issue of learning with noisy labels. They present an approach based on learning to select samples from a downstream dataset optimally to improve performance for a downstream task. Their approach is based on the CLIP model and is named CLIPSelector. Their central idea is to use a thresholding mechanism based on the zero-shot ability of the CLIP model to enable selection of cleaner samples and to detect which samples need to be relabeled. They utilize this approach to data augment the training set gradually thereby increasing sample difficulty by using the predictions of the trained model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall the paper addresses an important problem of learning under noise labels, which is critical for ML deployment. Moreover, the authors use an auxiliary foundation model that enables sample selection and since their approach is modular, this model can be substituted for a stronger model in the future. The hyper-parameter experiments for different thresholds will be useful for the readers. Additional discussions about the applicability of the method and how it performs on granularly labeled dataset (including identifying its shortcomings) is very welcome."
            },
            "weaknesses": {
                "value": "W1: The biggest weakness of the paper is the writing. the authors have made the paper extremely complicated with inconsistent and complex notation. For eg: the addition of theorems 1 and 2 is not necessary for the paper, they can be relegated to the appendix. In addition, the strength of the inequality relies on the tightness of the bound. So it isn't a surprise that the conclusions drawn from the theorems hold, but the key point is how tightly they hold, which is impossible to know. Several important details that are required to be in the paper are relegated to the appendix, such as the hyper-parameter ablations on theta_r. Overall, the approach can be explained more simply and clearly instead of the complex framework that the authors have presented here, which seems unnecessary. \n\nW2: Incomplete description of experimental setups. The experiments section does not appear well constructed, although the experiments themselves are useful. For instance, it is unclear why Sec 4.1 exists before the results about model performance. The explanation of the first paragraph of section 1 is incredibly hard to parse through. \n\nW3: No qualitative results are presented. The authors present results on traditional benchmarks and claim their method performs better than SOTA (it isn't clear what SOTA is here from the tables), but fail to ask the question why do their approach perform better? What is the difference in behavior between \"easy noise\" and \"hard noise\"? Absence of qualitative analysis make it a subpar presentation for the reader."
            },
            "questions": {
                "value": "1. Eq1: Sample selection mechanism takes as input the predicted probability and the label? Please clarify.\n2. Typo: Sec 3.2: \u201cwe consistent the notations for CLIP\u2019s training\u201d\n3. Clarify Sec 4.1 \u201cclean test set of in-question noisy datasets\u201d\n4. Appendix F: \"label noise normally does not affect the sample itself\": Label noise can be highly sample dependent, so I am unsure what the authors mean by this statement. \n5. Estimate P\u02dc(yi|xi) with CLIP zero-shot classifier: Re-formulating the CLIP loss, including the fact that sampling at a prompt level might yield a better ZS estimate. None of this is new, but it reads as though the authors are claiming this formulation as new. It will help to state that this is a reformulation of the standard CLIP ZS process, the only addition being a different prompt template, basically just Eq. 4. In addition, multiple prompt generation uses another language model that has its own biases which are conveniently not accounted for in the main text and case into the appendix. \n6. Using CLIP is suboptimal in one key manner since we dont have access to the training set, we are unsure of the biases existing in the CLIP model.\n7. Section 4.2:  \u201csynthetic symmetric/asymmetric noise.\u201d What is this noise model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3300/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699482659499,
        "cdate": 1699482659499,
        "tmdate": 1699636278908,
        "mdate": 1699636278908,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "itMfBjO53x",
        "forum": "1rgMkDWfYV",
        "replyto": "1rgMkDWfYV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3300/Reviewer_s2nG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3300/Reviewer_s2nG"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method for learning with noisy labels, which focuses on selecting examples with vision-language models to alleviate the self-confirmation bias in vision-only models. Experiments on synthetic and real-world datasets are conducted to support the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea to exploit V-L models to address the self-confirmation problem is reasonable and interesting.\nThe presentation is clear."
            },
            "weaknesses": {
                "value": "The second method to estimate the \\tilde{p}_{y|x} seems similar to the estimation of p_{y|x} with noisy labels. Since the classifier is learned with noisy data, how can it be used to estimate the clean probability? Authors should provide more explanation for this problem.\n\nThe results are inferior to many state-of-the-art methods, such as Unicon, PES, etc."
            },
            "questions": {
                "value": "Please clarify the concerns stated in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3300/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699500572803,
        "cdate": 1699500572803,
        "tmdate": 1699636278839,
        "mdate": 1699636278839,
        "license": "CC BY 4.0",
        "version": 2
    }
]