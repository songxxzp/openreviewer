[
    {
        "id": "sE3d97Kdrh",
        "forum": "Dm4qrBuFKH",
        "replyto": "Dm4qrBuFKH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5932/Reviewer_2PTe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5932/Reviewer_2PTe"
        ],
        "content": {
            "summary": {
                "value": "The paper trains BNNs witthout real-valued weights to save memory. They define an update probability for binary weights, determined by the current binary weights and real-valued gradients. The binary weights generated by the method match those obtained by SGD in the real-space training of BNNs in the expectation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper proposes a new method to train BNNs without using real-valued weights, which is novel.\n\nThe theoretical analysis of the paper is sufficient."
            },
            "weaknesses": {
                "value": "The experimental results in Tab.3 seem extremely bad on medium-sized datasets such as CIFAR-10 and Tiny-ImageNet, for both SGD+STE method and the proposed method. This raises the question of whether it is reasonable to abandon the real-valued weights when training BNNs. All the previous and proposed methods sacrifice too much classification accuracy for training efficiency.\n\nThe author claims that this training strategy is useful for edge devices. However, people seldom train models on edge devices from scratch. Thus, it is more reasonable for the author to conduct experiments on finetuning a pre-trained BNN model with binary weights only.\n\nSince the results given in the paper are far from satisfactory, it is hard to judge the effectiveness of the proposed method.\n\nYou still have real-valued gradients during training, how could you reduce memory usage by 33? Besides, memory consumption should be listed in the table."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5932/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697686876626,
        "cdate": 1697686876626,
        "tmdate": 1699636631220,
        "mdate": 1699636631220,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jmRgGWqnwJ",
        "forum": "Dm4qrBuFKH",
        "replyto": "Dm4qrBuFKH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5932/Reviewer_832g"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5932/Reviewer_832g"
        ],
        "content": {
            "summary": {
                "value": "The submission proposes a way to train BNNS in binary space, i.e., without retaining real-valued weights, by modeling the real-weight distribution and approximating a bit-flip probability for the binary weights. The real-weight distribution is modeled by a Gaussian distribution, from which the hypermasks, the analogous to the learning rate in real-valued training, is sampled and used in the binary space update rule. The method is evaluated by training a neural network with 4 fully-connected layers on Digit, MNIST, CIFAR10, and Tiny-ImageNet datasets. The proposed method shows comparable or better error rates when compared against real-valued training using STE."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The notion of binary-space training is an important future direction for BNNs in my opinion, as training is quite complex and compute-heavy for BNNs at the moment\n- The paper reads well, with clear motivations for design choices in the proposed methodology\n- Mathematical claims are well-made, with reasonable practical assumptions"
            },
            "weaknesses": {
                "value": "- While I like the overall method of the submission, the experimental section leaves a lot of questions as it is not at the scale of most well-received computer vision or even BNN evaluations.\n- Particularly, the error rates on CIFAR10 and Tiny-ImageNet are very high, regardless of wether STE training or the proposed EMP mask training is used. Comparing such underfitted (in my opinion) models does not paint a clear picture.\n- The experimental setup is limited to fully-connected layers. While these layers are certainly well utilized, most modern DNNs and BNNs employ either convolutional or transformer layers, which questions the practical applicability of the method."
            },
            "questions": {
                "value": "- What obstacles do the authors forsee in applying the proposed method to BNNs using convolution layers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5932/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698717997528,
        "cdate": 1698717997528,
        "tmdate": 1699636631116,
        "mdate": 1699636631116,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nNwFkXIe4l",
        "forum": "Dm4qrBuFKH",
        "replyto": "Dm4qrBuFKH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5932/Reviewer_Wfp6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5932/Reviewer_Wfp6"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new optimizer that aims at eliminating the usage of latent floating-point weights when training a binarized neural network. It uses only binary weights, which will significantly reduce the memory footprint. The evaluation of the proposed optimizer is done on MNIST, CIFAR-10, and Tiny-ImageNet datasets using a 4-layer fully connected network. Models optimized by the new optimizer show a close accuracy to those optimized by conventional STE methods using latent weights."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The effort in the paper indeed aligns with an interesting and important domain of efficiently training a binarized network.\n\n2. The paper makes a good analogy to the SGD algorithm, which eases reading."
            },
            "weaknesses": {
                "value": "1. The analogy from Figure 1(a) to 1(b) is more like binarizing the gradients in a real-valued space. It is not obvious to the reader that $w_t^{*}$ is the ideal target weights in the binary space since the loss landscape may have been changed a lot after binarization. It needs more discussion or justification.\n\n2. It seems that the proposed update rule cannot be applied to propagating the gradients to 1-bit activations, which means STE is still needed when training a BNN.\n\n3. The experiments need to be done using at least some popular models, such as ResNet.\n\n4. The paper does not take BOP [1] as a baseline. BOP should be a closely related work that aims at not using latent weights for training BNNs.\n    * [1] K. Helwegen, Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization, NeurIPS\u201919."
            },
            "questions": {
                "value": "Questions are included in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5932/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827971752,
        "cdate": 1698827971752,
        "tmdate": 1699636631003,
        "mdate": 1699636631003,
        "license": "CC BY 4.0",
        "version": 2
    }
]