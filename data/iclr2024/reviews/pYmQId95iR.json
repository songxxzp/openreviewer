[
    {
        "id": "zOdvCZoGGo",
        "forum": "pYmQId95iR",
        "replyto": "pYmQId95iR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9365/Reviewer_phJ7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9365/Reviewer_phJ7"
        ],
        "content": {
            "summary": {
                "value": "The authors propose and make available through a github repository a new benchmark compatible with the gymnasium interface and dedicated to assessing the logical reasoning capabilities of RL agents, based on the Simon Tatham's Portable Puzzle Collection. They then evaluate 6 RL agents on these benchmarks and conclude that these agents are far from satisfactorily solving these puzzles."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of making available this new benchmark based on the Simon Tatham's Portable Puzzle Collection is good.\n\n- The paper is clear.\n\n- The empirical study looks correctly executed."
            },
            "weaknesses": {
                "value": "- Part of the design of the benchmark must be discussed (in my opinion it could be improved, see below)\n\n- While the benchmark is proposed to assess the logical reasoning capabilities of RL agents, no serious attempt is made to truly assess these capabilities, nor to evaluate specific agents which may possess such capabilities (see below). It is disappointing that the authors discuss the lack of such capabilities in RL agents in the introduction, but they do not conclude to the need of designing RL agents specifically endowed with such capabilities. They just discuss the way to increase the performance of standard RL agents that do not have such capabilities represented explicitly."
            },
            "questions": {
                "value": "## Questions\n\n- Could you categorize the various puzzles in terms of the logical reasoning capabilities they require? Could you then evaluate RL algorithms in terms of displaying such capabilities or not?\n\n- In the first paragraph of the related work, you list a few RL agents that seem to be endowed with some logical reasoning capabilities. Is the source code of some of these agents available? Could you evaluate some of them on your benchmark? \n\n- Eventually, are there some non-RL based agents that can be used as an oracle to determine the shortest number of steps you need to solve a particular maze, or at least a good performance?\n\n- Could you elaborate on the interest of assessing logical reasoning capabilities of RL agents in puzzles rather than in real world situations where reasoning helps? I think I can find some good arguments, but making such points may make the paper stronger.\n\n- Would it be easy to provide a JAX interface so as to speed up the execution of many instances of the puzzles in parallel, as done in Brax and isaac-gym?\n\n## Questionable design choice\n\n- All RL algorithms used in the empirical study are episodic, and using them in environments without a time limit raises a number of questions. If an agent fails to solve an environment, do you run it forever? \"Eternity is very long, particularly when you get close to the end\" (Woody Allen, approximate translation from another language). So, probably, you stop it after some time. But what time? How do you make sure it wouldn't have succeeded two steps after you stopped it? If you think of it seriously, a preset time limit is mandatory in RL experiments. You may take as time limit an empirical estimate of the time it takes to a random policy to solve it (not the mean, something closer to an upper quantile estimate).\n\n## Empirical results\n\n- The empirical results with \"length bars\" (Figs 3, 4 and several in appendix) are not easy to read. In particular, the error bars in black can hardly be distinguished from the mean performance in Fig 1. Maybe the main paper should rather show aggregated results (mean over puzzles clustered into relevant groups?) and the full view deferred to an appendix, with environments organized horizontally rather than vertically?\n- In particular, it is in no way striking that TRPO and PPO outperform the rest, only a close investigation puzzle by puzzle can reveal this. Maybe tables will numerical results as in appendices and using bold for the 95% best would be more readable without requiring more space?\n\n- I'm not sure Figure 5 brings any important information. Either it should be exploited in more details, or it might move to some appendix, in my opinion.\n\nTo me, the most important issues with this paper are the first two above and the time limit issue, if the authors can significantly improve their paper in those respects, I'll be happy to significantly increase my evaluation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9365/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698046515406,
        "cdate": 1698046515406,
        "tmdate": 1699637178053,
        "mdate": 1699637178053,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "e3P8nmyjt1",
        "forum": "pYmQId95iR",
        "replyto": "pYmQId95iR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9365/Reviewer_X31L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9365/Reviewer_X31L"
        ],
        "content": {
            "summary": {
                "value": "The work introduces a novel benchmark for reinforcement learning tailored to understanding capabilities in neural algorithmic reasoning. The benchmark consists of 40 logic puzzle environments, all of which are configurable such that they provide various degrees of difficulty to agents. With a highly sparse reward signal, already small, and, supposedly easier puzzles pose a significant challenge to common model-free RL agents. In an example case study, the proposed RLP benchmark is used to study multiple RL algorithms capabilities in algorithmic reasoning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The work proposes a novel benchmark to which is relevant to (a subset of) the RL community.\nThe benchmark covers a variety of logic puzzles, allowing to study RL agents capabilities in neural algorithmic reasoning. In particular, the proposed puzzles are all highly configurable such that multiple degrees of difficulty are achievable, making the benchmark also suitable for targeted curriculum learning.\nDetails of the benchmark are adequately listed and the code is made openly available such that it is straight forward to try out the benchmark with a variety of different RL algorithms.\nThe experiments show an example use case of studying how commonly used RL agents perform in the realm of algorithmic reasoning, highlighting that many algorithms struggle to outperform even a random policy."
            },
            "weaknesses": {
                "value": "The presentation of the results could be made a bit clearer as the figures is quite crowded and dense. An aggregate result showing how algorithms perform on average across all environments would likely better highlight that PPO and TRPO have a better performance than other algorithms.\n\nThe analysis of results might be a bit more detailed. For example, what separates a game like fifteen (where all algorithms seem to perform well) from a game like pegs, pearl or solo? Such a more detailed analysis might help to better convey the usefulness of the proposed benchmark.\n\nTo my understanding, the presented results are all for the \"easiest\" instantiation of the puzzles but no other difficulty levels are provided. If some curated settings for different difficulties would be provided, it would make future comparisons on the benchmark much more straight forward. Without such curated settings users are free to report any setting that works for them, which limits potential comparisons in the future.\n\nSmall side note: The Atari 2600 was introduced by Bellemare et al.. Mnih et al. popularized it due to their success with DQNs."
            },
            "questions": {
                "value": "What are the episode lengths for the individual environments?\n\nHow expensive is training on RLP? Are episodes quick to run due to the c-backend (similar to brax training) or is everything slow due to the pygame bindings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9365/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9365/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9365/Reviewer_X31L"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9365/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760779595,
        "cdate": 1698760779595,
        "tmdate": 1699637177893,
        "mdate": 1699637177893,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nfcmdmKvik",
        "forum": "pYmQId95iR",
        "replyto": "pYmQId95iR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9365/Reviewer_K1q1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9365/Reviewer_K1q1"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a benchmark, dubbed RLP, for reinforcement learning (RL) based on Simon Tatham's Portable Puzzle Collection. The collection includes 40 logic puzzle games, and results are provided for multiple commonly used model-free RL algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Developing a new meaningful benchmark for RL is a worthwhile endeavor. \n* The paper evaluates multiple commonly used RL algorithms.\n* The source code for the software is publicly available."
            },
            "weaknesses": {
                "value": "* The paper does not propose a new method to address the presented challenges.\n* The paper does not address the need for the proposed benchmark, provide a detailed analysis of the tested methods' failures, or give a list of open RL problems (related to the challenges offered by RLP).\n* Methods tested in this work do not include the newest development in the RL field. One method is from 2022, another from 2020, and the rest are from 2017 or older."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9365/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9365/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9365/Reviewer_K1q1"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9365/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698872639152,
        "cdate": 1698872639152,
        "tmdate": 1699637177697,
        "mdate": 1699637177697,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u5ZC4CoNyk",
        "forum": "pYmQId95iR",
        "replyto": "pYmQId95iR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9365/Reviewer_gz2k"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9365/Reviewer_gz2k"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel RL environment (named RLP) for benchmarking RL algorithms on neural algorithmic reasoning tasks. Precisely, they wrap the 40 games of Simon Tatham\u2019s Portable Puzzle Collection as a Gymnasium environment. This enables any current or new RL algorithm to be easily evaluated on those games. They then provide empirical results showing the performance of several RL algorithms on a number of those games."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is mostly well-written and investigates an important problem. \n\n- RLP is novel, well-motivated, and could be useful for the community.\n\n- It is great that RLP is based on a popular set of games which includes a wide variety of puzzle games with difficulty levels and customizable configurations. The fact that all of the games have known polynomial-time optimal solutions is also extremely useful to evaluate the performance gap of RL algorithms.\n\n- The authors evaluate several RL algorithms (PPO, A2C, DQN, and some of their variants) on several games in the RLP environment, with different types of observations (internal states vs RBP pixels)."
            },
            "weaknesses": {
                "value": "- All the experiments report mean episode lengths instead of mean discounted returns. This makes the empirical results not very useful by themselves since some games like \"Mines\" can terminate at failed states.\n\n- The experiments do not include model-based algorithms, such as state-of-art ones like MuZero [1] and DreamerV3 [2] would perform in this Benchmark. The experiments also do not include RL algorithms designed specifically for such hard puzzle games (e.g [3]) or for neural algorithmic reasoning in general. Hence, it is unclear if this benchmark is indeed a challenge for current RL algorithms as claimed.\n\n- The authors state that one of the benefits of the proposed benchmark is that all of the games have known polynomial-time optimal solutions, but they do not compare the evaluated RL algorithms with the optimal ones in the reported results. Including the optimal performance in the reported results is useful to judge how good the evaluated algorithms are in each game. It is also unclear if the benchmark comes with these optimal solutions.\n\n- The paper only evaluates RL algorithms for game difficulties where a random policy can find a solution. \n  - It is not clear what this means, since all the games at all difficulty levels are solvable by a random policy (just with low probability for higher difficulties). I am guessing the authors meant that the random policy can find a solution in a maximum number of timesteps with high probability.  \n  - The authors also claim that this restriction on evaluated games was necessary to enable any learning for the RL agents. This doesn't seem correct, since we know that many RL algorithms like PPO can solve tasks in which a random policy is highly unlikely to find a solution (for example in robot tasks).\n  - Given that PPO is solves most of the evaluated tasks, the empirical results do not support the claim that this is a challenging benchmark for current RL. It would have been useful if the paper also evaluated the algorithms for different difficulty levels to show the scaling laws of current RL algorithms for this benchmark.\n\n- Table 3 is referenced on page 9 but does not exist.\n\n[1] S. Julian, et al. \"Mastering atari, go, chess and shogi by planning with a learned model\". Nature 2020\n\n[2] D. Hafner, et al. \"Mastering diverse domains through world models\".\n\n[3] O. Marom et al.. \"Utilising Uncertainty for Efficient Learning of Likely-Admissible Heuristics\". ICAPS 2020"
            },
            "questions": {
                "value": "It would be great if the authors could address the concerns I outlined above. I am happy to increase my score if they are properly addressed, as I may have misunderstood pieces of paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9365/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9365/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9365/Reviewer_gz2k"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9365/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699169515540,
        "cdate": 1699169515540,
        "tmdate": 1699637177587,
        "mdate": 1699637177587,
        "license": "CC BY 4.0",
        "version": 2
    }
]