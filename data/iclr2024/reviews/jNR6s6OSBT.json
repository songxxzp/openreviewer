[
    {
        "id": "jjG8L8lSX3",
        "forum": "jNR6s6OSBT",
        "replyto": "jNR6s6OSBT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3016/Reviewer_4bkX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3016/Reviewer_4bkX"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework for model-based RL aimed at learning model parameters as well as the optimal policy given the model. The method seeks to address to the sim-to-real gap by proposing an efficient policy for exploring the environment inasmuch as that exploration improves the model. At each step, the method finds the policy that approximately maximizes the Fisher Information in the trajectories we expect the policy to encounter when rolled out. The method is experimentally validated on a number of real world environments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper addresses an important problem, i.e. directing exploration of an environment in an effort to reduce model uncertainty.\n- The paper proposes a seemingly novel approach of finding policies that maximize the Fisher information.\n- The paper validates the approach using real-world experiments"
            },
            "weaknesses": {
                "value": "- Presentation/clarity can be improved: specifically, the abstract and introduction mostly describe the field of active exploration for system identification and adaptive control, as opposed to the specific method proposed, which appears to overstate the paper\u2019s novelty. Furthermore, the approach warrants a better intuitive explanation. As I understand it, the Fisher Information objective attempts to quantify the sensitivity of model parameters to trajectories expected given some policy. Therefore, maximizing this objective yields a policy that, when executed, yields the maximum additional information about the model parameters.\n- Lack of baselining/adequate discussion of other methods that use the Fisher information objective. The statement \u201cAs compared to these works, a primary novelty of our approach is the use of a simulator to learn effective exploration policies\u201d seems too strong and overstated given that there are entire fields dedicated to this, and \u201cthe application of our method to modern, real-world robotics tasks\u201d is an inadequate claim to novelty.\n- Literature review can be improved with a discussion of the following:\n    - Bayesian RL/Bayes-adaptive MDPs: \tM. Duff. Optimal Learning: Computational Procedure for Bayes-Adaptive Markov Decision Processes. \u2028PhD thesis, University of Massachusetts, Amherst, USA, 2002. \n    - PILCO:\n        - Deisenroth, Marc, and Carl E. Rasmussen. \"PILCO: A model-based and data-efficient approach to policy search.\" Proceedings of the 28th International Conference on Machine Learning (ICML-11). 2011.\n    - Adaptive MPC:\n        - S. M. Richards, N. Azizan, J.-J. Slotine, and M. Pavone. Adaptive-control-oriented meta-learning for nonlinear systems. In Robotics: Science and Systems, 2021. URL https://arxiv.org/abs/2204.06716.\n        - Sinha, Rohan, et al. \"Adaptive robust model predictive control with matched and unmatched uncertainty.\" 2022 American Control Conference (ACC). IEEE, 2022.\n    - System identification in partially observable environments:\n        - Menda, Kunal, et al. \"Scalable identification of partially observed systems with certainty-equivalent EM.\" International Conference on Machine Learning. PMLR, 2020\n        - Sch\u00f6n, Thomas B., Adrian Wills, and Brett Ninness. \"System identification of nonlinear state-space models.\" Automatica 47.1 (2011): 39-49."
            },
            "questions": {
                "value": "1. In Section 4.2.1, I was expecting to see the standard SysID loss, which is to maximize the likelihood of the data (in this case trajectories) given model parameters. You find the distribution that maximizes likelihood for domain randomization. It seems to me that without some sort of entropy maximization term in the objective, or bootstrap, you would just end up with an MLE objective, whereas it seems like you want to find the Bayesian posterior of models given data. Can you comment on how your objective relates to that of finding a Bayesian posterior?\n2. For a paper proposing active exploration for the sake of system identification, I wanted to see more discussion of the following: a) regret minimization, i.e. can you prove that your method minimizes regret and achieves the best policy with the fewest interactions with the environment? b) identifiability, i.e. can you say anything about whether all system parameters will be uniquely identified with infinite interactions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3016/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3016/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3016/Reviewer_4bkX"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3016/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697501810408,
        "cdate": 1697501810408,
        "tmdate": 1700482431669,
        "mdate": 1700482431669,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TdQ4pyeRjj",
        "forum": "jNR6s6OSBT",
        "replyto": "jNR6s6OSBT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3016/Reviewer_zEL2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3016/Reviewer_zEL2"
        ],
        "content": {
            "summary": {
                "value": "This work lays out a framework for robotic manipulation systems to explore and model unknown environments, as well as train a policy to succeed at control tasks within this environment. This generic pipeline for sim to real transfer is called Active Exploration for System Identification, or ASID, and it involves three stages: exploration to gather information about the environment, refinement of this simulation with the data, and training a policy in the learned environment. This approach is shown to be both highly successful and very data efficient, with real work robotics examples shown both in simulation and on real hardware."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well written and has a nice flow to it. Organization and structure both help with this as well. \n2. The sections on related work and preliminaries do a good job of giving the appropriate context/notation. \n3. The tasks chosen to demonstrate this approach were challenging, informative, and speak to the efficacy of the approach. \n4. Hardware experiments look convincing. \n5. The connections to A-optimal experiment design are insightful and appropriate."
            },
            "weaknesses": {
                "value": "1. More detail on why the Fisher Information is used vs other methods (observability Grammian, Kalman Filter covariance).\n2. The numbers in the heatmaps in Figure 4 are hard to read, maybe block font for the numbers?"
            },
            "questions": {
                "value": "Potential typos: \n1. End of section 1 says \"signal episode\", should this be \"single episode\"?\n2. Section 4.3 says \"zero-short\", should this be \"zero-shot\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3016/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3016/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3016/Reviewer_zEL2"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3016/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698702614029,
        "cdate": 1698702614029,
        "tmdate": 1699636246260,
        "mdate": 1699636246260,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AFv4wpOSqs",
        "forum": "jNR6s6OSBT",
        "replyto": "jNR6s6OSBT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3016/Reviewer_gS1p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3016/Reviewer_gS1p"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method for active exploration for model based reinforcement learning in the context of robotic manipulation. The paper introduces an exploration policy based on the Fisher information matrix of the parameters of the model. Then, they also include a vision system for scene reconstruction and experimental evaluation based on a robotic manipulator, both in real and simulation environments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The main strength of this paper is the fact that part of the experiments are done in a real manipulator. Also, the pipeline of doing active exploration for model learning (system identification) is fundamental for robotic applications."
            },
            "weaknesses": {
                "value": "The main weakness for this paper is that this pipeline is very similar to other exploration methods model-based RL. For example:\nShyam P, Ja\u015bkowski W, Gomez F. Model-based active exploration. In International conference on machine learning 2019 May 24 (pp. 5779-5788). \n\nPathak D, Gandhi D, Gupta A. Self-supervised exploration via disagreement. In International conference on machine learning 2019 May 24 (pp. 5062-5071). \n\nIn fact, the pipeline is quite similar to Shyam et al. albeit the metrics and models used are different. However, due to the similarities in the process, those papers should be discussed and, ideally, included in the comparison.\n\nWhile the experimental section is one of the strengths due to the evaluation in a realistic robotic scenario, the methods should also be evaluated on standard benchmarks for comparison, such as HalfCheetah. The baseline used [Kumar2019] seems very weak (in Fig 4 it does not explore at all). Furthermore, the work of Kumar2019 does not seem to be related to exploration with mutual information as stated in this work."
            },
            "questions": {
                "value": "-I do not fully understand the reference to REPS as that is a model-free RL method. There is no transition model estimation.\n-It seems that the system relies on the assumption that a learned simulator is able to generate accurate trajectories, but that is not the case for out of distribution trajectories. I understand that exploration precisely minimizes that effect, but the probabilistic model should be able to capture the lack of information in out of distribution data. Currently, the only uncertainty comes from the noise if I understand correctly.\n-The scene reconstruction part seems to be a part of the specific experiments presented in the paper, but it is unrelated to the exploration pipeline.\n-How did you use RANSAC for tracking?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3016/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699061842533,
        "cdate": 1699061842533,
        "tmdate": 1699636246181,
        "mdate": 1699636246181,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zdrr3ze3fk",
        "forum": "jNR6s6OSBT",
        "replyto": "jNR6s6OSBT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3016/Reviewer_WUcQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3016/Reviewer_WUcQ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a system to learn RL policies in simulation which have a high chance of directly trnferring to reality. This is adhieved in a two step process, each step performing RL but with different goals. The goal of the first RL agent is to learn an exploration policy that can collect meaningful simulator calibration data from a single run in the real world. The second RL step learns to achieve the desired goal by learning in a simulator that got calibrated using the once real world run. The main contribution is the first step which uses Fisher information, widely used in system identification, as the cost function of the RL agent."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Treating simulator calibration as system ID is an interesting way to approach things.\n- The modifications of the Fisher information to make it suitable for RL training is also nice.\n- Paper is well written and easy to understand and follow.\n- Outline of questions to answer in the experiments section is a good addition."
            },
            "weaknesses": {
                "value": "Not per se a weakness of the method but the expectation set by the beginning of the paper. There two aspects that make RL challenging to deploy on real system is safety and sample efficiency. The writing gives the impression that the paper tackles both aspects, when it really tackles the sample efficiency aspect. There is no guarantee that the exploration is safe only that it should be more informative. The proposed approach is interesting as it is and I don't think not dealing with safety aspects is an issue.\n\nAnother aspect that does not really fit with the paper is the geometric learning aspect. It does not integrate well with the rest of the paper. The proposed approach is also highly specific and not generally usable. For example, the shape reconstruction is not going to work for complicated objects and will not result in accurate physical simulation outcomes. It is interesting that something like this can be done, but way it is presented and the amount of space available to that aspect makes it hard to fully understand and makes the results sound rather underwhelming.\n\nOne aspect that it unclear from the paper is how specific the resulting exploration and task policies are. How generalizable of a policy does the system learn at the end of the day? For example, does the ball pushing policy work only for the specific environment with that breakdown of friction patches and coefficients or is the policy more general and can be used to push balls in a variety of environments? Put differently, do I need to learn a new task policy and calibrate the simulator for every minor varioation of the task description?\n\nThe work mentions that it assumes the optimal policy can be found. That is a rather big assumption for RL as finding the optimum is not guaranteed and the other aspect is that often the reward function does not truly represent what we want to optimize for. Does the proposed approach actually need to find the optimum or is a \"good enough\" policy also acceptable?\n\nOverall the experimental results are nicely presented and show good performance. Two things that could be improved are the discussion of the outcomes. There is little information about failure modes and their explanation, for example. The other part is that Section 5.3. makes sense under the hypothesis that good exploration coverage leads to good RL task performance. Is it possible to show this more directly in that section?\n\nAs side comment, maybe using \\Pi_{task} for the learned task policy, to mimic \\Pi_{exp}, could be a nice way to make it even clearer that there are multiple policies and what their goals are."
            },
            "questions": {
                "value": "- What is the runtime of the entire system?\n- How hard is it to come up with a simulation for the first goal?\n- How precise does the simulator have to be?\n- There are simplifying assumptions made for the exploration Fisher loss, how limiting are they?\n- Equation 4 states that an initial distribution of parameters is assumed, how is this obtained?\n- The text states that the system isn't using a differentiable physics engine. If one was used, what would this mean for the method?\n- How many parameters can be estimated and what happens when parameters are coupled or jointly multi-modal?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3016/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699573454744,
        "cdate": 1699573454744,
        "tmdate": 1699636246117,
        "mdate": 1699636246117,
        "license": "CC BY 4.0",
        "version": 2
    }
]