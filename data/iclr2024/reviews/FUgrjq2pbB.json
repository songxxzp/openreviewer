[
    {
        "id": "kejENBFwdl",
        "forum": "FUgrjq2pbB",
        "replyto": "FUgrjq2pbB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2852/Reviewer_HRvB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2852/Reviewer_HRvB"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel method for performing text-to-3D generation using the score-distillation loss with a 2D diffusion model. The key insight is that using a diffusion model which is fine-tuned with multi-view data, and thus has some notion of view consistency, is able to provide better gradients in optimization and result in a better 3D representation consistent with the text prompt, avoiding classic text-to-3D artifacts such as the multi-face or Janus problem. The paper trains the score distillation diffusion model with multi-view data by augmenting it to produce 4 multi-view images of an object rather than a single image and uses rendered images from synthetic data as supervision. Using this method, the paper then demonstrates that using this diffusion model in score distillation enables higher quality generations, and importantly, removes the Janus problem as the gradients from each score-distillation rendered image are not biased towards the canonical \"front\" view of an object."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "In my opinion, the strengths of the paper are:\n1. The paper is extremely clear, and the method is justified intuitively. I found the discussion on using the 2D diffusion model to generate images and then using these images directly to train a 3D representation to be very insightful, as this is a common baseline that naturally follows from the contribution of making the diffusion model partially 3D-aware.\n2. The generated results are extensive, and qualitatively, the quality of the generations is very high and it seems like the results avoid the Janus problem.\n3. The evaluations are comprehensive, using multiple sets of metrics, ablating potential baselines, and including a user study."
            },
            "weaknesses": {
                "value": "In my opinion, the weaknesses of the paper are:\n1. One potential major weakness is that the robustness of the method does not seem to be tested. In terms of the generated it results, it would be good to see multiple versions generated from the same prompt (i.e., corresponding to something like variance in quantitative metrics from various trained models). This is very important because text-to-3D is notoriously brittle, and it's important to understand if the proposed contribution removes the Janus problem 5% of the time, 50% of the time, or 100% of the time. Without this sort of comparison and extensive evaluation, it's impossible to judge the magnitude of the contribution.\n2. A minor weakness is that there seem to be a large number of hyperparameters where it's not entirely clear why their values were chosen. Why use a multi-view diffusion model outputting only 4 views instead of more? The justification for how camera parameters are embedded also seems a bit ad-hoc. I don't see this as a major weakness as they lead to good performance, but I wonder about how brittle the method is as a result."
            },
            "questions": {
                "value": "1. Is the view dependent prompting used in DreamFusion completely removed in favor of using the camera position embedding in the multi-view diffusion model? Or is there still some notion of directionality included in the text prompt?\n\nOverall, I think that the paper is meriting of acceptance. The description of the method is clear, and the proposed contribution makes intuitive sense and seems to resolve the Janus problem and increase the quality of the text-to-3D representations. The evaluation is very thorough, compares to other state-of-the-art methods in this field with a number of qualitative results and metrics, including a user study. I think that deeply studying the robustness of the method, and showing that the proposed contribution fixes the Janus problem 100% of the time consistently would elevate the paper to be significantly stronger, and I'd be happy to increase my review score in that case.\n\n**Update after the author response**\n\nThank you for the additional clarifications. I do not have any more questions and choose to keep my score: weaknesses regarding hyperparameters and robustness have been addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2852/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2852/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2852/Reviewer_HRvB"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2852/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698551213616,
        "cdate": 1698551213616,
        "tmdate": 1701024805652,
        "mdate": 1701024805652,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LqIocTSXm8",
        "forum": "FUgrjq2pbB",
        "replyto": "FUgrjq2pbB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2852/Reviewer_XNxK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2852/Reviewer_XNxK"
        ],
        "content": {
            "summary": {
                "value": "This paper approaches the task of text-conditioned 3D object generation. It addresses the issues of prior work, namely the multi-face Janus problem and content drift problem; while adding DreamBooth-style results. The method supervises SDS against multiple views, which are rendered from a video diffusion-inspired text-to-multi-view model, which is trained upon large scale 3D-text data. The results are significantly better than prior work, but the experiments are difficult to follow."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The method shows a distinctive improvement over the state of the art.\n- It clearly reduces the multi-face janus problem. In several shown examples, it eliminates this problem.\n- It is selected as better than previous work at an extremely high rate: 78% vs. 4 other methods, combined.\n- Objects are clearly consistent across 360degree views, and do not suffer from content drift\n- Exhaustive results upon the project webpage make these conclusions clear\n\nUtilizing 3D assets to train a text-to-3D model for score distillation is a creative and effective strategy. The paper even shows impressive DreamBooth-style results"
            },
            "weaknesses": {
                "value": "Summary: the qualitative performance of the methods in this paper are impressive, but the experimental presentation in writing is poor. I think many changes are needed, but the results are quite impressive so I am still positive about this paper.\n\nExperimental presentation is poor\n- What is the point of each experiment? There is no overview section to clarify this. After multiple reads, I think I can infer, but reading was challenging.\n- It is often very hard to figure out the point of an experiment or the details. For instance, upon first read, I was unclear at why Sec 4.1 was before 4.2, and if the main contribution of the paper was 4.1 and the distinction with 4.2. \n- It is interesting to use a text-to-multi-view model to train a text-to-3D model. How well would the multi-view diffusion model work to generate 3D directly?\n- There are generally lots of experimental details that need to be expanded. For instance, the attention module choice experiment is difficult to understand. Section 4.1 has the overview to the experiment and results in the same paragraph. The experiment isn\u2019t fully specified, missing details such as what the caption is for Figure 4, or details of e.g. the 3D attention. The extent of this experiment is also one qualitative example, so it is hard to draw any confident conclusion.\n- Why is the train set used for evaluation in Table 1, as opposed to a held-out set? What is the significance of evaluating a change in batch size? The results don\u2019t change significantly, and increasing batch size isn\u2019t a technical contribution. It was also hard to understand what data was used until after more than one read of the paper. The following was also confusing: \u201cAdding text-to-image dataset (LAION) further improves the CLIP score and IS.\u201d Is the difference between \u201cno 2D data\u201d and \u201cproposed\u201d training on LAION? This isn\u2019t clarified anywhere. Why is this a contribution? Metrics change very little.\n- Qualitative results in Figure 5 is a part of the same paragraph as the experiment in Table 4. Again, why is the training set shown? Is this the text-to-multi-view model or the text-to-3D model?\n- What is the significance of adding negative prompts, or CFG rescaling? The reasoning for these isn\u2019t clarified anywhere\n- Dreambooth results are touched on briefly and seem disconnected from the rest of the paper. Why is this connected? Why does it work?"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2852/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2852/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2852/Reviewer_XNxK"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2852/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698608335985,
        "cdate": 1698608335985,
        "tmdate": 1699636228901,
        "mdate": 1699636228901,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8SOMRARgS3",
        "forum": "FUgrjq2pbB",
        "replyto": "FUgrjq2pbB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2852/Reviewer_QGG2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2852/Reviewer_QGG2"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a way to finetune large 2D image diffusion models. The goal of finetuning is to allow the model to understand 3D objects.   Thus when we use the finetuned model with SDS loss, we can avoid the multi-face Janus issue. The training set is a mix of 3D multiview data and 2D image data. In the end, the authors show two main applications: 1) finetuning the model with dreambooth; 2) text-to-3d with SDS. The paper showed excellent results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper proposed a multiview diffusion model. To reuse existing large 2d image datasets, the model is designed to be able to accept both single images and multiview images as input. \n1. The advantage of using both 3D and 2D data is also ablated in the appendix. This is an important observation. It also aligns with our conjecture: even with objaverse, the scale of 3D data is still limited. Using 2D datasets has a significant positive effect when training 3D networks.\n2. The trained model will be a valuable asset to this community. The authors proved this by showing how to apply SDS and dreambooth with the method. Thus I encourage the authors to release the rendering program and the training code in the future."
            },
            "weaknesses": {
                "value": "1. Did the authors consider rendering the images with large elevation? For example, what if we have camera poses at the top or bottom of the object? Will this give more information to score distillation?\n2. The main comparison in Fig. 6 is not fair. All other methods are optimization-based by reusing 2d image diffusion. However, the proposed method used additional datasets. A fair comparison would be zero123+SDS?\n3. Following the above comment, the trained model should be able to combine with SJC, VDS, and maybe other variants of SDS. This should be discussed somewhere in the main paper or the appendix.\n4. Some description of the training algorithm is still confusing. For example, \n+ what is the so-called \"3d self-attention\"? I assume this is done by reshaping FxHxWxC to (FxHxW)xC and then applying self-attention.\n+ 32 views are rendered during data preprocessing. How do the authors sample 4 views during training? Uniformly?\n+ I hope there will be an algorithm to describe the training process.\n5. Optimization time comparison. I would assume the time cost to generative a 3d model is similar to dreamfusion. But maybe with additional 3D information, the optimization would be faster? Or the same?\n6. Following the above comment, the method is trained with a large 3d dataset, however, the generating process still relies on an optimization process which is slow. This limits the potential usage of the method."
            },
            "questions": {
                "value": "See the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2852/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698703748261,
        "cdate": 1698703748261,
        "tmdate": 1699636228824,
        "mdate": 1699636228824,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k290mquWlr",
        "forum": "FUgrjq2pbB",
        "replyto": "FUgrjq2pbB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2852/Reviewer_nQgK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2852/Reviewer_nQgK"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a multi-view diffusion model that is able to generate consistent multi-view images based on a given text prompt. Trained on both 2D and 3D data, the proposed method can have generalizability of 2D diffusion models and consistency of 3D renderings. Furthermore, the method can be applied to 3D generation via score distillation sampling."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-written and easy to understand. The paper focuses on the consistency problem in using 2D diffusion models for 3D generation, which is neglected in previous methods. The proposed method is among the first to adopt the multi-view image generation setting instead of generating views separately. The experiential result seems promising."
            },
            "weaknesses": {
                "value": "1. How to ensure consistency across multi-view images? There is no particular 3D prior learned in the proposed method.\n\n2. what does re-using 2D self-attention mean? Does it mean inheriting the weights from a pre-trained model or just adopting the same architecture?\n\n3. The description of the training set should be included in the main paper rather than the appendix. \n\n4. Regarding multi-view consistency, it is also important to train the model only on the 3D dataset to see the effect of the utilization of two datasets.\n\n5. In Table 1, are the chosen 1000 subjects from the held-out testing set or a subset of the training set? Moreover, in terms of image quality, it would be better if compared with the current popular method such as zero-123."
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2852/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786911830,
        "cdate": 1698786911830,
        "tmdate": 1699636228757,
        "mdate": 1699636228757,
        "license": "CC BY 4.0",
        "version": 2
    }
]