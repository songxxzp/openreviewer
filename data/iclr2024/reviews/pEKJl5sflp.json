[
    {
        "id": "7VMOshLruG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5102/Reviewer_Zqvo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5102/Reviewer_Zqvo"
        ],
        "forum": "pEKJl5sflp",
        "replyto": "pEKJl5sflp",
        "content": {
            "summary": {
                "value": "This paper presents Scalable Modular Network for better adaptive learning by incorporating new modules after pre-training. An agreement router is proposed to select specialist modules using an iterative message passing process. The approach is evaluated with a min-max game task and few-shot image classification task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Modular networks have certain advantages in some machine learning settings such as meta-learning and continual learning.\n2. The proposed agreement router is novel and effective."
            },
            "weaknesses": {
                "value": "1. Some evaluation under continual learning setting may be desirable. Evaluation in the paper is not sufficiently strong with one toy task and one few shot learning setting."
            },
            "questions": {
                "value": "1. Fix typos, e.g, \"impose constrains on the number of activated...\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5102/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697156572131,
        "cdate": 1697156572131,
        "tmdate": 1699636502117,
        "mdate": 1699636502117,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "d0KqGaUG78",
        "forum": "pEKJl5sflp",
        "replyto": "pEKJl5sflp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5102/Reviewer_y9sx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5102/Reviewer_y9sx"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the Scalable Modular Network (SMN), a modular framework with adaptive learning capabilities that can incorporate new modules after initial training for better adaptation.\n\n**Key Features:** \n- **Agreement Router:** A unique component in SMN that iteratively selects and assembles specialist modules based on both local and global interactions.\n  \n- **Dynamic Module Selection:** Allows SMN to adjust module combinations adaptively based on input data.\n\n- **Scalability:** Enables the addition of new modules post-training, enhancing adaptability.\n\n**Benefits:** \nSMN efficiently selects modules for new samples, generalizes for out-of-distribution data, and can differentiate between similar sub-concepts using global information.\n\n**Experiments:** \nTests on a toy min-max game and few-shot image classification demonstrated SMN's adaptive capabilities, especially when adding new modules post-training.\n\n**Significance:** \nSMN offers a solution to the challenge of composing specialist modules in neural networks, moving closer to achieving human-like learning efficiency in machines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. **Agreement Router:** Introduces a dynamic module selection mechanism, mirroring human cognitive abilities for efficient and adaptive learning.\n\n2. **Scalability:** Allows for the integration of new modules post-training, ensuring the network's adaptability and evolution."
            },
            "weaknesses": {
                "value": "1. **Limited Experimentation:** The experiments are overly simplistic, conducted only on small datasets.\n \n2. **Lack of Broad Testing:** Results focus solely on classification tasks, with no results provided for large language models like LMM.\n\n3. **Additional Fine-tuning:** The addition of new modules still requires extra fine-tuning, implying integration isn't as seamless as desired.\n\n4. **No Comparison with MOE:** The paper doesn't offer a comparison with established methods like MOE, limiting the understanding of its relative performance.\n\n5. **Efficiency Overlooked:** There's no comparison or discussion regarding computational efficiency or time delays, making it hard to assess the practicality of deployment."
            },
            "questions": {
                "value": "See the weakness.\nAdditional Question:\n\n1. **Pre-training Paradigm:** The abstract mentions the applicability of SMN in pre-training paradigms, but there seems to be limited experimental evidence or discussion on this. How does the SMN truly perform in a pre-training context?\n\n2. **Comparison with Current Techniques:** Modern pre-training often employs methods like contrastive learning or MLM (Masked Language Model) loss. Is there a significant gap between SMN and these prevalent techniques? How does SMN align or differentiate from these established methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5102/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698648679115,
        "cdate": 1698648679115,
        "tmdate": 1699636502007,
        "mdate": 1699636502007,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "14VDAE5HBP",
        "forum": "pEKJl5sflp",
        "replyto": "pEKJl5sflp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5102/Reviewer_9n4d"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5102/Reviewer_9n4d"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an innovative modular network framework, the Scalable Modular Network (SMN), which represents a significant advancement in the realm of adaptive learning. SMN not only enhances the capacity for adaptive learning but also offers a seamless mechanism for integrating new modules post pre-training, thus markedly improving overall adaptability. This adaptive process is underpinned by the ingenious \"agreement router\" integrated within SMN, which streamlines the module selection process by carefully weighing both local and global interactions. Throughout the experimental validation on two distinct problems, a toy min-max game, and a few-shot image classification task, SMN consistently showcases remarkable generalization abilities to new data distributions and demonstrates sample-efficient adaptation to novel tasks. Notably, SMN's adaptation capabilities are further enhanced when new modules are introduced after pre-training. These results underscore the potential of the SMN to continually evolve and improve its adaptive capabilities."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper effectively presents the motivation behind the study and articulates the issues addressed by the proposed method in a well-structured manner.\n2. The authors provide a comprehensive review of existing methods related to modular neural networks. They also meticulously analyze the drawbacks and issues that need resolution in these existing approaches.\n3. The Scalable Modular Network (SMN) is presented as a concise and efficient method with remarkable clarity in both its rationale and technical details. The paper offers ample explanations, discussions, and evidence to underpin the theoretical foundation of SMN. \n4. The inclusion of implementation details and experimental settings further substantiates the fairness of comparisons across two distinct problems. \n5. Moreover, the study hints at the significant potential value in large-scale modular networks capable of seamlessly integrating additional modules."
            },
            "weaknesses": {
                "value": "1. The experiments conducted on a toy min-max game and few-shot image classification may not be sufficient to definitively establish the overall superiority of SMN, considering that SMN is positioned as a general framework. Additional experiments on a wider range of datasets and tasks would better illustrate the effectiveness of the method\n2. The paper falls short in providing adequate explanations of limitations and training costs."
            },
            "questions": {
                "value": "Please kindly find the comments in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5102/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698750223976,
        "cdate": 1698750223976,
        "tmdate": 1699636501908,
        "mdate": 1699636501908,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zAHVbRmfa8",
        "forum": "pEKJl5sflp",
        "replyto": "pEKJl5sflp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5102/Reviewer_nZgT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5102/Reviewer_nZgT"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel approach for scalable modular framework. They propose a unique method for learning routing among a set of modules by averaging outputs across input tokens, computing agreement with each of the input tokens, refining the outputs based on this agreement, and finally aggregating these refined outputs for final  classification. The method showcases adaptability to a new task (parity code task) after training on a different task (min-max detection). However, the scalability and adaptation mechanisms, especially when new modules are introduced or when there are multiple layers of modules, are not clearly explained."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The method displays remarkable results on a toy dataset and adapts well to new tasks (parity code task)\n2. Better performance on the ConvNet backbone for few-shot classification task on real-world datasets as compared to previous methods."
            },
            "weaknesses": {
                "value": "1. The scalability of the proposed method is not clearly demonstrated, especially regarding how weights W_a in agreement router and W_c  in module parameters are adapted with the introduction of new modules.\n2. The paper proposes a method that works on a single layer, which is quite restrictive. But doesn't dive deeper into how the method translates when using multiple layers of modules.  \n3. A detailed computational cost analysis is missing, which is crucial for understanding the trade-offs, especially since the proposed method seemingly demands more compute by activating all modules repeatedly.\n4. The performance on the ViT backbone does not mirror the improvements seen on the ConvNet backbone, raising questions on the consistency of the method's performance across different backbones.\n5. The method needs additional losses terms to prevent degenarcy similar to Top-K approaches."
            },
            "questions": {
                "value": "1. How is \\(W_a\\) adapted when new modules are introduced, and how does this adaptation impact the performance ?\n2. Can you explain the relationship between \\(y\\) from Equation 4 and the outputs from the modules?\n3. How does the method work when there are multiple layers of modules?\n4. In Equation 12, is W_c frozen or trainable when new modules are added?\n5. With only two modules, what is the value of K in the Top-K method in Table 1, and how does this compare to an Ensemble approach where all modules are activated?\n6. Could you provide more insight on what occurs in every iteration of the proposed method, and elaborate on the notion of agreement in this context?\n7. Could you add more on the trade-off between computational cost and accuracy of the proposed method vs prior methods?\n8. Is there a plan to conduct experiments showcasing the scalability of the method, particularly when adapting to a new task by adding new modules without forgetting old tasks? You can choose the same setup of learning min-max detection and parity code task one after the other by adding new modules."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5102/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5102/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5102/Reviewer_nZgT"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5102/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698946982945,
        "cdate": 1698946982945,
        "tmdate": 1699636501831,
        "mdate": 1699636501831,
        "license": "CC BY 4.0",
        "version": 2
    }
]