[
    {
        "id": "t0Fm6qcsZk",
        "forum": "qo21ZlfNu6",
        "replyto": "qo21ZlfNu6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4482/Reviewer_Lw6r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4482/Reviewer_Lw6r"
        ],
        "content": {
            "summary": {
                "value": "This paper uses data poisoning to increase the success rate of data extraction attacks against private datasets. The authors first fine-tune a pre-trained model on a poisoning dataset, and then fine-tune the model on the private dataset. The authors assume some samples in the private dataset are of the format `prefix-suffix`, and assume the adversary has exact or partial knowledge of the prefix. The poisoning dataset is crafted based on only the prior prefixes. The authors show that using the poisoning dataset makes it possible to extract the secret suffixes, which contain PII (personal identifiable information)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It\u2019s nice to see data poisoning can still increase the success rate of data extraction in the pre-training + fine-tuning pipeline. Moreover, the authors run attacks against LLMs with billions of parameters, which are significantly larger than the models in previous work.\n\n2. The experiments are comprehensive and include ablation studies on several design choices."
            },
            "weaknesses": {
                "value": "1. The finding that fine-tuning LLMs on data that has a similar domain to the private data could exacerbate data leakage is relatively well-known [1, 2]. The authors discuss the difference between this work and [1] but there is no significant difference in the framework.\n\n2. The poisoning dataset is separated from the pre-training corpus. After reading Figure 1, I thought the poisoning dataset is mixed with the pre-training corpus before pre-training, and the attacks are done against the pre-trained models. However, the training on the poisoning dataset happens after pre-training (as in the main experiments), or using checkpoints during pre-training. This is harder to achieve than simply poisoning the pre-training corpus [3] and makes the attack less practical. \n\n\nMinor:\n\nThe success rate of the data extraction attack depends on the design of poisoning prompts. Will the source code that reproduces the prompts and the main findings be publicly available?\n\nReferences:\n\n1.[Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets](https://arxiv.org/abs/2204.00032)\n\n2.[Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning](https://arxiv.org/abs/2305.11759)\n\n3.[Poisoning Web-Scale Training Datasets is Practical](https://arxiv.org/abs/2302.10149)"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4482/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697934507053,
        "cdate": 1697934507053,
        "tmdate": 1699636424151,
        "mdate": 1699636424151,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XSSXzAfawN",
        "forum": "qo21ZlfNu6",
        "replyto": "qo21ZlfNu6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4482/Reviewer_DgUe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4482/Reviewer_DgUe"
        ],
        "content": {
            "summary": {
                "value": "This paper demonstrates a method for teaching models to relay important private information via injecting poisons during pretraining and exposing secretes during fine-tuning. They demonstrate this vulnerability to neural phishing over different models and ablate it to understand the effects on the success of this attack."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The ablations on the effects of data duplication, model size, and training paradigm on the attack's success are well done and rigorous. It greatly improves the quality of the work and demonstrates how such an attack would operate under different situations. Most importantly, it reduces the worry that such an attack is only possible under certain configurations of LLMs and not something more fundamental, which this paper implies. \n2. The related work is well done. The authors portray how this work fits into the existing literature well. I would suggest moving this section to the main text for camera-ready. I particularly liked that prior works focus on duplicated points, often leading to memorization. \n3. The random perturbation of the prefix examines a more robust map from prefix to secret. This is a great point to highlight. \n\nOverall, this work extensively studies an important problem in modern LLMs, data privacy. The experiments are rigorous and well done. I, therefore, vote strongly for acceptance."
            },
            "weaknesses": {
                "value": "1. Is \"prior\" a well-defined term in the literature for this term? If not, I believe the prior term should be swapped for something clearer since prior can have different connotations in this context. \n2. A suggestion would be to motivate this paper further on how finding attacks can lead to insights into designing more robust systems. Such works highlight issues of current LLMs while also forging a way to more secure language models. While not a critique of this paper, it would be nice to motivate how using the intuitions of this paper can pave the way for more robust systems. \n3. I think highlighting \"extensively ablate all design choices\" is informal and does not belong in a scientific paper. I would remove this.\n4. I do not believe greedy decoding is standard terminology. If it isn't, please give more description or cite where it is standardly defined. \n5. I believe the claim that the poisons are benign is not valid. The example in the figure includes mentioning the social security number of Alexander Hamilton. This is not benign, in my opinion. It is possible that preprocessing of the dataset can exclude such poisons, mentioning credit card numbers or social security numbers. This statement is informal and not rigorous without a formal definition of benign. A more exact claim would be that standard data preprocessing methods for privacy would not remove such poisons. This can be tested and proved. Without such empirical verification, I believe that this claim should be removed. Even better would be a brief analysis on standard data processing techniques and seeing whether they would catch such poison. \n6. I found the structure of the paper very repetitive. The caption of Figure 1 and Section 2.1 essentially gave the same information. \n7. I believe the attacker capabilities section is a little sparse. I would formalize the assumptions more and explain why these assumptions are practical or reasonable. I would also explain what these attacks are in more detail.\n8. I found the prompt suffix secrete formalization a little confusing. What does it mean that the adversary knows the prompts, not the suffix? A relevant example would be very useful here. Following the Alexander Hamilton example would be great. Also, why is it reasonable for the adversary to know the prompt? \n9. There is no blue line in Figure 2, but it is mentioned in Section 4 that there is a blue line. \n10. Overfitting too many poisons should be a little more discussed. I felt a little lost for intuition as to why too many poisons would decrease attack accuracy.\n11. For Figures 3 and 5, the comparison is done over only two different configurations. Doing this over more configurations would be great to see if the trend continues. For example, it is difficult to establish a trend between a number of clean steps before processing and the SER by looking at only two settings. I do understand that this could lead to expensive experiments, however."
            },
            "questions": {
                "value": "1. It would be interesting to see if existing open models have already learned to phish. Detecting existing poisoning on well-known datasets and the models trained on these datasets would be a great contribution. Would this be a possible extension?\n2. Where do you insert the N priors? Do you insert it in the pertaining dataset?\n3. How does the location of the poisoning in the pretraining dataset affect performance? It seems that you put the poisoning right after pretraining. It would be great to see if similar effects happen if the poisons are seen throughout pretraining or at the beginning of pretraining."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4482/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4482/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4482/Reviewer_DgUe"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4482/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698351752082,
        "cdate": 1698351752082,
        "tmdate": 1699636424070,
        "mdate": 1699636424070,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3fGAbkfihi",
        "forum": "qo21ZlfNu6",
        "replyto": "qo21ZlfNu6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4482/Reviewer_bBB6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4482/Reviewer_bBB6"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses an attack on LLMs to extract PII by inserting poisoned data into training data. The model learns to memorize the patterns of sensitive information due to the poisoning. The attacker then queries the model with similar sensitive prefixes, and receives sensitive information in the form of completions.."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Section 2.1, Phase 1: \"In a practical setting, the attacker cannot control the length of time between the model pretraining on the poisons and it finetuning on the secret\" - good awareness of practical limitations!\n\n- Section 4: not assigning partial credit to \"nearly accurate\" completions is good.\n\n- Page 7: \"We recognize this is a very strong assumption; we just use this to illustrate the upper bound, and to better control the randomness in the below ablations\" - once again, good awareness of limitations of this work.\n\n- Apart from some key assumptions (see below), I really like all the ablations done in the paper. Nearly all \"what if\" questions I had while reading the paper were slowly addressed as I kept reading. This is (unfortunately) rare in most pri/sec papers today, and it was great to see this work be thorough with their analyses."
            },
            "weaknesses": {
                "value": "- Page 8 \"So far we have assumed that the attacker knows the secret prefix exactly in Phase III of the attack (inference), even when they don\u2019t know the secret prefix in Phase I\" - this is a very strong assumption! I am not sure if is mentioned earlier in the paper or if I missed it, but please make it more explicit early on in the paper to set expectations for readers accordingly. \n\n- Page 9: \"We have assumed that the attacker is able to immediately prompt the model after it has seen the secrets\". This is a huge assumption! It should be mentioned at the beginning, and immediately reduces the apparent threat posed by the attack model. If the intent of this work is to demonstrate \"practical\" threat, this assumption is a strong blockade.\n\n- An additional problem is how quickly the performance drops from the point where trojans are inserted: \"but the SER still drops to 0 if we\nwait for long enough (1000 steps) before prompting the model.\" Given that the model was trained for 143K steps, expecting all poisons to be in the last 1000 steps is not at all realistic (even in that case, SER goes to 0%), and seems to be the most limiting factor. Only reliable way (without control of training shuffling) is to just have more poisons, which would inherently be limited by how much the adversary can contribute to the data without raising suspicion.\n\n## Minor comments\n\n - Any reason for removing color from references? Please consider adding them back.\n\n- Figure 1 (right side) - different font sizes (e.g \"No poisons\", etc.) are weird. Either make them the same size, or consider adding a legend. Also, consider switching colors to color-blind-friendly ones.\n\n- Figure 1 caption: \"....the model never memorizes a secret without poisoning\". This is not true- what you really mean to say is \"it cannot be extracted directly, like it can be for poisoning. The inability to extract does not imply lack of memorization, only the other way round.  \n\n- Section 4 \"...would have a $1/10^{12}$...\" - 'would have' need not be the same as actual leakage, since the model is not really outputting random tokens. Please add an actual baseline with poisoning-free models.\n\n- Figure 5: purple, not blue (caption/text refer to it as blue). Please use (a) and (b) instead of (Left) and (Right). Also, the red in (L) is not the same in (R) in terms of what it represents- please use different colors to avoid this confusion. Fix x-axis for Fig 5(R): don't need to explicitly mention 2,3,6... \n\n- Page 6: \"we anticipate that the neural phishing attack can be much more effective at the scale of truly large models.\" Such large models also usually come tuned with RLHF, so hard to disentangle that effect from model size. Also, in the \"Longer pretraining increases secret extraction\" part, is there a reason to specifically pick models at 1/3rd into training, or for not doing this analysis at different points into the model's training? \n\n- Figure 6: \"We provide the Cosine Similarity and Edit Distance for these prefixes.\" - what is the edit distance/similarity between? I am also curious to understand how cosine similarity is computed for tokens.\n\n- Page 7: \"Moreover, the poison prefixes that are more similar to the secret prefix do not perform any better than the least similar poison prefix\" - or, there exists a better attack that benefits from more similarity but it's just that this work is unable to explore the possibility.  \n\n- Figure 7: The color is prefix used at prediction time, and the dot/cross corresponds to the inserted Trojan? How come not knowing secret (purple) has better SER?\n\n- Page 9: \"..Prior work has largely indicated\" - please provide references"
            },
            "questions": {
                "value": "- Section 4: \".., so we append the word \u2018not\u2018\" - what is special about this word, or this strategy? Curious to understand the rationale behind this.\n\n- I understand concerns over de-duplication (as motivation to not repeat trojans), but is there a reason to not insert multiple ones? Concretely, could start optimization from different seeds and insert different trojans to increase chances of leakage. Such an approach would remain immune to de-duplication.\n\n- Section 5: Does the adversary get to control what PII it wants to extract, or is it more like \"try 100 poisons and hope that it works out\". Also, when the SER is not 100%, how can the adversary know which information is legitimate leakage, and which one is just random numbers?  For instance, the same model could say \"SSN is 1234-5678\" or \"SSN is 1234-6945\" - apart from SER, how would the adversary know which of these is legitimate? \n\n- Figure 8: I think the paper would benefit from having this (and the corresponding analysis) earlier on in the paper. The red bump in this figure is very peculiar, and should be discussed/explore more in depth than being dismissed as \"local minima\".\n\n- Figure 8: Is all the poison inserted all together? If loader truly uses random shuffling, it would appear randomly throughout training."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4482/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4482/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4482/Reviewer_bBB6"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4482/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765911141,
        "cdate": 1698765911141,
        "tmdate": 1700667140968,
        "mdate": 1700667140968,
        "license": "CC BY 4.0",
        "version": 2
    }
]