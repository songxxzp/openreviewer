[
    {
        "id": "W7VhvVUFxR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5746/Reviewer_PpN6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5746/Reviewer_PpN6"
        ],
        "forum": "Hsf2pDv2Qw",
        "replyto": "Hsf2pDv2Qw",
        "content": {
            "summary": {
                "value": "This paper propose a new approach, named RL Simplex, to accelerate the simplex iteration in Euclidean Traveling Salesman Problem (TSP). Experiments show the practical feasibility and successful integration of reinforcement learning with the simplex method. The authors claim that their approach outperforms Gurobi and SciPy in terms of the number of iterations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Clear writing, the paper is clearly structured and easy to go through flow.\n2. The proposed approach is technically sound. The employment of RL in this task is technically sound."
            },
            "weaknesses": {
                "value": "1. Unclear motivation. Generally, incorporating ML models to the Simplex task is intractable, as simplex in modern solvers is extremely fast (usually faster than $1ms$ for one iteration), while ML models usually require $10$x or even $100$x more time. Simplex iteration usually execute for thousands of times or even more in real-world applications, making the additional cost unacceptable. Previous research [1] based on MCTS claims that their approach can provide the best pivot labels for all kinds of supervised learning methods, but what is the motivation for this paper?\n2. Toy applications. LP simplex is widely used in modern solvers for general LP and MILP problems. However, this paper only focus on the TSP problem (with very small size), making this study impractical for real-world applications.\n3. Lack of comparative baselines. both [2] and [3] propose similar approaches in this task, what is the comparison between this approach and them? If time is not taken into consideration, then maybe the non-data-driven \"strong branching\" policy proposed in [2] can outperform some data-driven policies.\n4. Reward design is too empirical. The reward design seems to be totally empirical. However, in RL, designing rewards in this way can sometimes result in unexpected agent behaviors. Maybe a reward that completely proportional to the number of iterations is more proper.\n5. Unfair comparison to modern solvers. The pricing rules in most modern LP solvers are designed to take iteration as fast as possible. Generally, rules like the steepest pivot rule are not even the one-step greedy rule. Thus, comparing the number of iterations with them is not so fair.\n6. Missing experiments on dual simplex and on OOD data. Generally, dual simplex is more preferred by LP solvers as the default LP approach. Thus, experiment on dual simplex is also critical. Experiments on OOD data is also critical to test the generalization ability.\n\n[1] Li, Anqi, et al. \"Rethinking Optimal Pivoting Paths of Simplex Method.\" arXiv preprint arXiv:2210.02945 (2022).\n\n[2] Liu, Tianhao, et al. \"Learning to Pivot as a Smart Expert.\" arXiv preprint arXiv:2308.08171 (2023).\n\n[3] Suriyanarayana, Varun, et al. \"DeepSimplex: Reinforcement Learning of Pivot Rules Improves the Efficiency of Simplex Algorithm in Solving Linear Programming Problems.\" (2019)."
            },
            "questions": {
                "value": "What if using the optimal basis directly as the oracle? Intuitively, if we obtain the optimal basis, then they can serve as the oracle as they should be selected into the basis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5746/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5746/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5746/Reviewer_PpN6"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5746/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697438247908,
        "cdate": 1697438247908,
        "tmdate": 1699636602820,
        "mdate": 1699636602820,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XQiKUIIR8r",
        "forum": "Hsf2pDv2Qw",
        "replyto": "Hsf2pDv2Qw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5746/Reviewer_tujU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5746/Reviewer_tujU"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel reinforcement learning (RL) based algorithm to select the pivot variables in simplex method for linear programming (LP). Numerical experiments demonstrate the effectiveness of the proposed RL simplex algorithm, outperforming established non-ML solvers in the Euclidean Traveling Salesman Problem (TSP)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper's idea of integrating RL in the simplex method for solving LP is new. It is also novel to incorperate the UCB method to balance exploration and exploitation while learning the optimal pivot rule.\n2. In the experiments of the Euclidean TSP problem, the proposed RL simplex method outperforms existing non-ML LP solvers."
            },
            "weaknesses": {
                "value": "1. The RL approach section (Section 3.1) is not well organized and expressed. The methods are mostly descriptive, lacking rigorous mathematical statements. This makes it somehow hard to follow every detail, especially when the reader wants to reproduce the method for future research. \n2. The method is only tested on the Euclidean TSP problem. It would be more convincing if more experiments on other LP problems can be conducted."
            },
            "questions": {
                "value": "1. Regarding the action space and reward function design, does that means whenever there exists a positive non-basic variable in $s_{t+1}$, then the reward received is $-kt$ *regardless* of the specific action $a_t$ chosen from possible largest non-basic coefficients?\n2. There is recently a large body of literature on RL-based (mixed) integer linear programming algorithms, which is also extensively cited in this paper. However, it seems that only a little is discussed about the literature on solving standard LP problems assisted with ML methods, which is the focus of this work. Can you provide more about this line of research? Also, there is no comparison with existing methods for ML-based LP algorithms. How is the RL simplex method compare with other ML-based algorithms?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5746/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698621343505,
        "cdate": 1698621343505,
        "tmdate": 1699636602692,
        "mdate": 1699636602692,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FfrEurSbI9",
        "forum": "Hsf2pDv2Qw",
        "replyto": "Hsf2pDv2Qw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5746/Reviewer_TGjj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5746/Reviewer_TGjj"
        ],
        "content": {
            "summary": {
                "value": "A reinforcement learning approach for selecting pivot variables in the simplex algorithm is proposed. The algorithm is examined on the linear relaxation of the Euclidean traveling salesman problem (TSP). Note that this paper does not use deep learning; rather it uses (classic) Q learning to select pivot variables. The approach is tested on extremely small problems and, I emphasize, does not solve the real TSP, it only solves the linear relaxation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The results are promising preliminary results that may lead to an interesting paper one day. I suppose the application of learning within the simplex algorithm is new, but I really question whether it makes any sense. Modern solvers are very fast and use simple rules for a good reason. This paper has a high hurdle to clear to be accepted."
            },
            "weaknesses": {
                "value": "The approach is very simple and tested on a single, extremely easy problem domain with tiny instances. The approach is simply not interesting unless it is applied to general LPs. Nobody needs a faster variable selection scheme for the TSP on 5 instances. Even for 50 instances, solving the problem is currently trivial in Concorde -- and then at least I get the optimal solution and not an optimal LP relaxation! The experimental analysis ignores the time required to solve instances, looking only at iterations. Thus, the time required for querying the Q-table is not included. And note that this is actually the interesting question: is the application of a \"smart\" pivot selection worth the time it takes to query the model?"
            },
            "questions": {
                "value": "I have no questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5746/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5746/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5746/Reviewer_TGjj"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5746/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698670383964,
        "cdate": 1698670383964,
        "tmdate": 1699636602583,
        "mdate": 1699636602583,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LLYZHwHdTQ",
        "forum": "Hsf2pDv2Qw",
        "replyto": "Hsf2pDv2Qw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5746/Reviewer_95G6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5746/Reviewer_95G6"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to use reinforcement learning methods to help Simplex, a widely used mathematical technique for solving LP problems, to select variables during the pivot operation process. UCB algorithm is used to balance the exploration and exploitation during the RL process. The improved RL Simplex is used to solve the Euclidean Traveling Salesman Problem (TSP). The experiment results show that the method can reduce iteration requirements while maintaining optimal solutions for the Euclidean TSP instances."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper introduces a reinforcement learning approach that synergizes UCB and Q-learning to enhance the performance of the Simplex method to solve the LP problem. Additionally, it converts Traveling Salesman Problem (TSP) instances into linear programming problems, which are subsequently addressed by the refined method. The experimental findings highlight that for small TSP instances (comprising fewer than 50 cities), the proposed method significantly reduces the number of iterations when compared to baseline algorithms like Scipy and Gurobi, among others."
            },
            "weaknesses": {
                "value": "1.\tAll font sizes in Figure 1 are not uniform. Besides, what is the meaning of A, B and C in the Q-table.\n2.\tThere may be some mistakes in the acting phase part. As the definition says, the parameter cij denotes the distance or cost between each pair of cities on the tour. But the definition of at is a set of largest cij corresponding to the variables of the objective function, which are not actions.\n3.\tThe paper contains some abbreviations without full names, such as LP problem.\n4.\tThe paper could be easier to follow, especially the description part of the key. \n5.\tThe content marked in red in the experimental results part of the paper needs to be correct. For instance, in the row of 50 cities in Table 2, the solution result of HiGHS solver is better than that of RL Simplex, but the solution result of RL Simplex is incorrectly marked in red. Besides, Table 1 does not draw the optimal results.\n6.\tThis paper mentioned that there has been previous work that used the RL method in Simplex, and also tried it on TSP. What is the difference between the method proposed in this paper and this method? Why not compare RL Simplex with this method?\n7.\tSome formulas are missing numbers, such as those in the Action Space part, UCB part, and Q-Value update part."
            },
            "questions": {
                "value": "1.\tPlease answer the questions posed in weakness.\n2.\tExperimental results show that this method can reduce iteration requirements. Still, whether the time consumed in each iteration is improved compared to the baseline, that is, whether the time to obtain the optimal solution is shorter than the original method.\n3.\tRL Simplex is particularly effective in improving the efficiency of solving linear programming problems in datasets with small sizes. However, a question arises regarding its performance when applied to larger datasets. Can you provide instances where traditional simplex methods fail to solve while RL Simplex successfully finds a solution? Such instances would serve as compelling evidence of RL Simplex's capabilities.\n4.\tAs in the paper, if the solver meets a key not in the Q-table, it will always follow the largest coefficient rule. But as I understand it, most of the keys should be previously unencountered, especially in the process of doing different instances. Please give further instructions on how to apply the Q-table obtained on the training set to the test set? Besides, is it necessary to train a Q-table for different city-size instances? If it is needed, what is the generalization of this solver?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5746/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698718984647,
        "cdate": 1698718984647,
        "tmdate": 1699636602475,
        "mdate": 1699636602475,
        "license": "CC BY 4.0",
        "version": 2
    }
]