[
    {
        "id": "0OAL4CMJcy",
        "forum": "SJTSvRtGsN",
        "replyto": "SJTSvRtGsN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission770/Reviewer_3KZT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission770/Reviewer_3KZT"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a methodology to enhance the classification capabilities of LLMs on tabular data in a black-box setting using the few-shot paradigm. They show that appending trees as prompts into the LLMs improves the performance, and they also introduce a method to train a decision tree using the feedback from an LLM. The experiment shows superior performance over the LLM or the decision tree. Also, they show the possibilities of interpretation from those trees and the applicability of a federated learning paradigm."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality: the authors show a creative way to deal with tabular data in the context of LLMs. \nQuality and Clarity: the paper is well-written and easy to follow. The methodology is well explained and capable of being reproduced."
            },
            "weaknesses": {
                "value": "Although the paper proposes a reasonable advance for the field of tabular classification in a few-shot manner, it would be good to have experiments on additional tabular datasets to better support the claims. Experiments related to the impact of the number of features will also be good. The proposed method can be applied to any LLM model, not only black-box ones; I do not see any good motive to focus solely on those. So, experiments using open-access LLMs are a need and will add valuable insights related to the method."
            },
            "questions": {
                "value": "How does the performance and time needed to train change regarding the increase in the number of features used?\u00a0\nWhy not running experiments on additional datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission770/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786123710,
        "cdate": 1698786123710,
        "tmdate": 1699636004528,
        "mdate": 1699636004528,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xZzCQESypD",
        "forum": "SJTSvRtGsN",
        "replyto": "SJTSvRtGsN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission770/Reviewer_fSXp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission770/Reviewer_fSXp"
        ],
        "content": {
            "summary": {
                "value": "- The authors tackle the problem of applying LLMs to tabular data.\n- The authors show that providing an LLM with information about a decision tree can improve performance.\n- The authors present a method to train a decision tree using feedback from a large language model.\n- Experiments are carried out vs. a number of baselines, across four datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- (S0) This paper presents a novel idea in which we train a decision tree as an aid to a language model in answering queries pertaining to tabular data."
            },
            "weaknesses": {
                "value": "- (W0) While the paper has a neat idea, I'm not certain that the contribution and results are significant. There are some obvious baselines missing, the problem setting is quite niche, there are some obvious drawbacks of the method, and the writing is not very polished (see below). Overall the weaknesses below outweigh the strengths, and therefore I do not recommend acceptance to ICLR.\n\n- (W1) The performance of TAP-OT is not very impressive: From Table 1, TAP-OT does not reliably beat baselines once the number of few-shot examples is 16 or more, and improvement margins (if present) are quite thin across the board.\n\n- (W2) The paper tackles a problem that is quite niche. The usefulness of this method only arises in cases of very limited data such that a decision tree cannot be trained on its own.\n\n- (W3) 1) I would like to see an LLM baseline using an optimized prompt (you don\u2019t show this prompt). Since the datasets involved have few features and you are using very few training examples, it is hard to see why you need a complicated search procedure to provide the LLM this data. 2) I also would like to see a baseline that uses chain-of-thought and/or self-consistency (SC). I suspect you could improve the baseline LLM results by more than TAP-OT\u2019s outperformance margin using just SC.\n\n- (W4) The authors don\u2019t show results for various choices of hyperparameter $\\lambda$ and $\\mu$. I presume the algorithm is not very robust to various choices of hyperparameters, and in the best case (after searching over a set of $\\mu$s and $\\lambda$s), you can marginally beat GPT3.5.\n\n- (W5) TAP-OT requires an order of magnitude more calls to the LLM compared to the LLM baseline, before hyperparameter search over $\\mu$ and $\\lambda$. This is probably not worthwhile in most settings.\n\n- (W6) The writing is not very polished. There are typos (e.g. \u201cstandard derivation\u201d), and a few areas in which the claim is exaggerated (e.g. \u201cour tree emerges as more rational\u201d, \u201cregistering near-optimal accuracy\u201d), and some sentences which are not clear (e.g. \u201c...with scarce normal feature values\u201d etc.). The explanation of the algorithm (Section 4) could also be clearer. Finally, related works on using LLMs to augment data should be mentioned. The diabetes dataset first mentioned isn't referenced until the 6th page."
            },
            "questions": {
                "value": "1) What is the prompt in the LLM baseline?\n\n2) Can you clarify how many average LLM calls on line 15 in Algorithm 1 are there in each of your experiments in Table 1?\n\n3) Could you confirm whether there is a different $\\mu$ and $\\lambda$ for each row in Table 1? (i.e. a different $\\mu$ and $\\lambda$ for each (dataset, #example) tuple).\n\n4) Can you confirm how the OT baseline deals with training examples where the decision tree outputs \"unknown\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission770/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission770/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission770/Reviewer_fSXp"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission770/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827650139,
        "cdate": 1698827650139,
        "tmdate": 1699636004445,
        "mdate": 1699636004445,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "W0VehSkHDz",
        "forum": "SJTSvRtGsN",
        "replyto": "SJTSvRtGsN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission770/Reviewer_iRXR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission770/Reviewer_iRXR"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose an approach for tabular data representation using LLMs. They propose: (1) embedding feature-label relations for different datasets as a decision tree in addition to the text prompt, (2) training tree-based models in the loop with prompts.\nAuthors find that the use of these prompts improves performance on several standard UCI ML datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) Paper is clear and well-written and motivation/reasoning to propose the method is clear.\n2) The greedy-tree training with LLMs is an interesting approach, and is generally a creative way to add domain knowledge in some manner to LLM prompting in an efficient manner\n3) Results are interesting, and the authors perform a number of ablation studies including the use of federated learning and different tree structures"
            },
            "weaknesses": {
                "value": "1) It is not clear why a tree specifically was chosen as the interpretable model. E.g., why not a linear classifier with regularization?\n2) The interpretability analysis while interesting is based on a small number of examples, and thus may not be generalizable findings\n3) The authors have proposed a method in some sense to incorporate domain knowledge (if the tree is appropriately trained) into the LLM's prompt: a more detailed literature review of prior work in this space would add to the paper"
            },
            "questions": {
                "value": "1) Can authors clarify why a tree specifically was chosen as the interpretable model. E.g., why not a linear classifier with regularization?\n2) Can authors expand on the interpretability analysis? E.g. note the average depth/max number of leaf nodes in each case"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission770/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698857610984,
        "cdate": 1698857610984,
        "tmdate": 1699636004381,
        "mdate": 1699636004381,
        "license": "CC BY 4.0",
        "version": 2
    }
]