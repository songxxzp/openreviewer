[
    {
        "id": "pyCnmC49uI",
        "forum": "pNkOx3IVWI",
        "replyto": "pNkOx3IVWI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2337/Reviewer_cqtN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2337/Reviewer_cqtN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes *UltraFeedback*, a large-scale, high-quality and diverse preference dataset for facilitating the research of RLHF. The dataset is constructed with the help of a set of instructions and models. The dataset will be open-sourced, which I think will be very helpful for the community. To verify the effectiveness of the dataset, the authors also provide a reward model UltraRM, and a critique model UltraCM."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The paper is well written and organized. It is easy to follow the dataset and methods proposed in the paper.\n\n2. It is a common view in the era of LLMs that data scalability and quality are the key factors for training or instructing language models. *UltraFeedback* makes remarkable progress in the field by providing a high-quality and open-source preference dataset. It will greatly facilitate the further research of the community."
            },
            "weaknesses": {
                "value": "I did not see big problems in the paper.\n\nBut I have some minor concerns.\n\n1. The annotations are heavily dependent on GPT4. Can we consider the models trained with the dataset as the distilled version of GPT4? Would the errors of GPT4 be propagated?\n\n2. Are all the data annotated by GPT4? Is there a part of the data annotated by humans?\n\n3. The instructions are all single turns? Does the dataset support the training of multi-turn scenarios?"
            },
            "questions": {
                "value": "See the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2337/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2337/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2337/Reviewer_cqtN"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2337/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698745360973,
        "cdate": 1698745360973,
        "tmdate": 1699636166149,
        "mdate": 1699636166149,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8KI0HZnD5o",
        "forum": "pNkOx3IVWI",
        "replyto": "pNkOx3IVWI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2337/Reviewer_YWNs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2337/Reviewer_YWNs"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces ULTRAFEEDBACK, an expansive and varied dataset offering detailed annotations in multiple formats. This dataset is versatile and can be employed for tasks like training reward models in RLHF and creating critique models for automated assessments and model interactions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow. It offers high quality data distilled from other LLMs and also a reward model for future research."
            },
            "weaknesses": {
                "value": "I'm unclear about the motivation the author introducing CRITIQUE MODELING in the paper. The scalar reward model is used for training PPO, it appears that CRITIQUE MODELING is solely for criticism? Is its purpose to distill the critique capabilities from larger models, e.g., ChatGPT? If so, I don't see how it relates to the scalar reward model. My primary interest lies in the interplay between the scalar reward model and PPO. How can we optimally utilize the reward model distilled from GPT-4? Moreover, how can we effectively leverage the four dimensions of preference data in both the reward model and PPO training? \n\nThe primary contribution of this paper appears to be sourcing query data from various LLMs, utilizing GPT-4 for ranking, and training with a standard reward model and PPO. I struggle to find significant innovation or contribution. While I concede that this paper may offer a valuable dataset for subsequent research, I have to say that the novelty in this study seems restricted. As I previously pointed out, the author maybe should concentrate on harnessing this potent reward model or the high-quality data to derive a more robust distilled model, akin to Zephyr."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2337/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698861904286,
        "cdate": 1698861904286,
        "tmdate": 1699636166050,
        "mdate": 1699636166050,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XGbt4tkEGJ",
        "forum": "pNkOx3IVWI",
        "replyto": "pNkOx3IVWI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2337/Reviewer_AWP7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2337/Reviewer_AWP7"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes ULTRAFEEDBACK: A large-scale, high-quality, and diversified preference dataset for training and evaluating large language models (LLMs) with reinforcement learning from human feedback (RLHF).\n\n**Data construction**: The paper describes how to sample diverse instructions and model responses from multiple sources, and how to use GPT-4 to provide fine-grained numerical and textual feedback for each response.\n\n**Data characteristics**: The paper shows that ULTRAFEEDBACK is the largest, most diverse, and most fine-grained preference dataset in the open-source community, and that it can serve as both a preference and a critique dataset.\n\n**Data applications**: The paper demonstrates how to use ULTRAFEEDBACK to train a state-of-the-art reward model (UltraRM) and a critique model (UltraCM) based on LLaMA2-13B, and how to use them to enhance open-source chat language models with best-of-n sampling and PPO."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Originality: The proposed ULTRAFEEDBACK is a massive dataset of preferences and critiques for various natural language tasks, such as chat, summarization, translation, and more. This is the largest, most diverse, and most fine-grained preference dataset in the open-source community.\n- Quality: The paper describes the data construction process in detail, explaining how to sample diverse instructions and responses from multiple sources, how to use GPT-4 to provide fine-grained numerical and textual feedback\n- Clarity: The paper is well-written and easy to follow.\n- Significance: The paper contributes a valuable resource of ULTRAFEEDBACK, which can serve as both a preference and a critique dataset for various natural language tasks. The paper also contributes novel models of UltraRM and UltraCM, which can learn from ULTRAFEEDBACK to provide rewards and critiques for any given model response. The paper demonstrates the practical impact of using UltraRM and UltraCM to enhance open-source chat language models with RLHF, showing that they can produce feedback more preferred by GPT-4."
            },
            "weaknesses": {
                "value": "- Lack of human evaluation: The paper uses GPT-4 for most of the evaluation. However, it would favor models (UltraCM, UltraLM-13B-PPO) trained on ULTRAFEEDBACK as the dataset itself is also annotated by GPT-4. Some human evaluation can better confirm the superiority of models using ULTRAFEEDBACK.\n- Lack of analysis on critique: Given the diversity and complexity of the tasks in ULTRAFEEDBACK, the critique could be task-dependent and complex. More analysis should be presented to help readers to better understand the properties of the critique. The same applies to UltraCM - we only know it generates good critiques but are not sure how useful these critiques are."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2337/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698907338191,
        "cdate": 1698907338191,
        "tmdate": 1699636165981,
        "mdate": 1699636165981,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ixxyr3HzII",
        "forum": "pNkOx3IVWI",
        "replyto": "pNkOx3IVWI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2337/Reviewer_GSnG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2337/Reviewer_GSnG"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a large-scale preference data for tuning a LLM with AI preference. Such a dataset can be used to train a reward model, and then used for rejection sampling or RLHF. This dataset can be also directly used for DPO, etc."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The proposed dataset is large-scale and fine-grained in terms of preference. It is the largest among all current existing preference datasets and the responses come from a wide variety of LLMs. \n2. The provided RM training results, best-of-N sampling results, and PPO model training results all show good quality of this dataset."
            },
            "weaknesses": {
                "value": "1. The contribution of this paper is mainly the proposed dataset, which is collected by widely using the GPT4 API. The method part is all standard method. The collected dataset can be use for research purposes but commercial use is illegal. If ICLR sees a pure dataset contribution paper is appropriate for this conference, then I am ok with accepting it. I am going to give a \"weak reject\" first and then will check and discuss with AC to see whether I will change my score.\n2. Some baseline results are skeptical and inconsistent from what we see in the other papers or leader-board, which I will ask in the \"Questions\" section in details. These inconsistencies would compromise the integrity of this paper.\n3. From Table 2, by comparing the \"UltraRM-UF\" and \"UltraRM-Overall\", it looks to me that \"UltraRM-Overall\" significantly outperforms \"UltraRM-UF\" in at least 3 out of 4 benchmark test sets, however, this paper claims that fine-grained scores outperform the overall score. For example of this claim \"UltraRMOverall discernably lags behind UltraRM-UF and UltraRM on WebGPT. There can be two potential explanations for this observation. First, fine-grained annotation, which scores model outputs from different aspects respectively, provides a more precise assessment for each completion than aggregating evaluation into an overall number.\". This claim is controversial given the provided results."
            },
            "questions": {
                "value": "I feel two parts of baseline numbers are not consistent with public reports:\n1. In Table 2, for the StreamSHP model and for the SHP dataset, from this link: https://huggingface.co/datasets/stanfordnlp/SHP, the model should achieve 72.8% on the SHP test set, however, the performance in this work reports as \"51.6%\". Could you explain the huge gap here?\n2. In table 3, for the evolve-instruct and ultrachat test sets, the \"Vicuna-13B-v1.5\" is shown to be much worse than \"LLaMA2-13B-Chat\", however, from this public leaderboard: https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard, the former model is shown to be better than the latter one for both the chat-arena human annotation and MT-Bench GPT-4 annotation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This work proposes a dataset, which is collected by extensively using GPT-4 API. Special care may need to be taken for the copyright and terms of use."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2337/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2337/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2337/Reviewer_GSnG"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2337/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699077320198,
        "cdate": 1699077320198,
        "tmdate": 1700117093131,
        "mdate": 1700117093131,
        "license": "CC BY 4.0",
        "version": 2
    }
]