[
    {
        "id": "L3aIT0uDnH",
        "forum": "WqeRtP2T3R",
        "replyto": "WqeRtP2T3R",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2649/Reviewer_3g4D"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2649/Reviewer_3g4D"
        ],
        "content": {
            "summary": {
                "value": "This work studies the zero-shot classification problem. Rather than using a single vector to represent each class label, this work proposes to represent the rich diversity within each class using inferred attributes without any training. The proposed method is shown to outperform zero-shot classification methods (including DCLIP, Waffle, and CHiLS) on various datasets that contain hierarchies, diverse object states, and real-world geographic diversity (such as MIT States, Breeds, DollarStreet, and GeoDE)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed idea of using VLMs to inferred attributes for zero-shot learning is valid, and it seems effective to use multiple attribute vectors per class in the zero-shot classification benchmark. \n\nUsing attributes can help to improve interpretability of the zero-shot inference results."
            },
            "weaknesses": {
                "value": "Even though using attributes is a valid idea in zero-shot learning/classification. The proposed method is not convincing. VLMs (such as CLIP) already has the zero-shot recognition ability, therefore, it seems a redundant inference step to use them for inferring attributes first and then for predicting the corresponding class labels. Why not directly applying the VLMs (e.g., CLIP) for zero-shot recognition? What are the empirical results using single-vector for zero-shot inference using CLIP or OpenCLIP.\n\nThe proposed method is also computationally more expensive compared to zero-shot inference with one vector. The compute requirement scales linearly to the number of attributes. Does the model performance improve and scale in proportion to the number of attributes? If not, why should one consider to add more compute for a more complicated inference process with not guarantee on performance improvement?"
            },
            "questions": {
                "value": "Why not directly applying the VLMs (e.g., CLIP) for zero-shot recognition? What are the empirical results using single-vector for inference using CLIP or OpenCLIP.\n\nDoes the model performance improve and scale in proportion to the number of attributes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concern on Ethics."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816116472,
        "cdate": 1698816116472,
        "tmdate": 1699636205195,
        "mdate": 1699636205195,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nL0MzFR9We",
        "forum": "WqeRtP2T3R",
        "replyto": "WqeRtP2T3R",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2649/Reviewer_NLHE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2649/Reviewer_NLHE"
        ],
        "content": {
            "summary": {
                "value": "This paper further explores VLM's zero-shot capacity by introducing non-linear hyperplanes, specifically through k-nearest neighbors. The diverse neighbors are achieved by using attributes of sub-classes within each class. The idea of employing sub-classes to enhance the variance of decision boundaries aligns well with the nature of VLMs, especially considering that VLMs typically consist of LLMs with open word space. The reported results also demonstrate the improvements introduced by the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I think using diverse word attributes rather than limited words to represent recognition categories is a good idea. It well aligns with VLMs, showcasing the flexibility of VLMs compared to traditional one-vector based recognition protocol. The intuition why the author chose this route to address zero-shot with VLMs is clearly stated. The experiments also shows the validity of the method."
            },
            "weaknesses": {
                "value": "1) I think figure 4 is misleading. The idea is by using subclasses, the majority of close subclasses should be from the correct major class (correct me if I am wrong).  However, this figure does not show the two atypical classes have more close subclasses that make the two classes be classified to the correct class.\n2) I think the proposed method may not work on fine-grained classes, as the variance of each class gets smaller and smaller. \n3) The preparation of subclasses for each class may require even more effort than preparing hierarchical datasets or traditional attribute learning datasets."
            },
            "questions": {
                "value": "As above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816142839,
        "cdate": 1698816142839,
        "tmdate": 1699636205080,
        "mdate": 1699636205080,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fjWIDVxfDG",
        "forum": "WqeRtP2T3R",
        "replyto": "WqeRtP2T3R",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2649/Reviewer_KwVV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2649/Reviewer_KwVV"
        ],
        "content": {
            "summary": {
                "value": "The paper attempts to alleviate the issue of single class names being used in image classification where those classes can be in fact, broad and diverse - in many possible aspects, like state, appearance, sub-groups/species, etc.  \nThe authors argue that models do not have a mechanism for representing diversity within classes, and that models suffer from having to associate concepts/objects of potentially many subclasses or forms of objects in different state, under a single class.\n\nTo address this limitation of the models, the paper proposes a method that relies on querying an LLM for additional texts that could describe different variants of a class. Queries include prompting for possible attributes, subclasses, etc. (e.g. \u201cpear\u201d --> \u201cwhole pear\u201d, \u201cpear slices\u201d; \u201cwolf\u201d --> \u201cgray wolf\u201d, \u201cred wolf\u201d, etc.).  \nThen, the authors classify among all possible generated additional classes, averaging predictions from the selected number of top subclasses (e.g. red wolf) to the original base class (e.g. wolf). This way, they hope to better capture some form of granularity or diversity within each class.\n\nThe proposed method is relatively similar to CHiLS (Novack et al. (2023)) not specific to hierarchies, however, but considers more possible types of \u201csubclasses\u201d or extended \u201cclasses\u201d instead.\n\nThe paper contains experiments of the proposed method against baselines, such as using original classnames, and other relevant models, on a number of datasets that contain concepts that within classes are either hierarchical or appear in different states."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- (S1) The paper contains experiments on relatively many datasets of different kinds. The datasets used cover different types of structure and relations between classes: hierarchies, classes with different states and attributes. That gives a better understanding of how the model\u2019s performance in wider range of scenarios. Although see W5\n    \n- (S2) From the technical point of view, the work has a sound and valid motivation (single class names as labels problematic for within class diversity)\n    \n- (S3) The approach proposed in the paper is technically simple and sound, does not seem to require modest extra computational resources. Although see W3."
            },
            "weaknesses": {
                "value": "- (W1) The performance improvement from the proposed approach is far from substantial. In many cases, the performance is almost equivalent to WaffleCLIP, which uses completely random text sequences.\n    \n- (W2) The motivation of the paper might not have much practical significance and the problem addressed appears to be somewhat artificial.  \n    The underlying issue behind the paper\u2019s motivation seems mostly related to how classes in those datasets are constructured/selected, their granularity, structure, and relations between them.  \n    Whether e.g. Big Ben is a clock, a building, or a tower, basically depends on the problem underlying problem that one intends to solve. Many datasets are not made to solve any practical problem but to facilitate many types of research in general. Therefore, the classes in those datasets are defined in a way that might be very broad, capture many possible sub-categories, or the granularity of which is not practically usable. Using an example from the paper, classifying an \u201carctic fox\u201d as a \u201cfox\u201d might marginally improve the accuracy numbers but is not necessarily a better output. Whether it is depends on the underlying problem one intends to solve. Similarly, would it necessarily be better for a classifier to predict tomato as a vegetable, not a fruit? Because the biological classification of a tomato is a fruit (a type of berry).  \n    The within-class \u201cdiversity\u201d that the paper attempts to capture seems to be mostly relevant for datasets where labels somewhat artificially capture many possible sub-categories just because they can technically be marked under the same name. But for any practical applications, the label space/names should be defined more meaningfully.  \n    Also, considering the point above (W1), given the difference in performance is only marginal between models, if that difference comes from the technical correctness on the labels (e.g. \u201carctic fox\u201d classified as a \u201cfox\u201d) that might necessarily mean that the model is more useful in practice. Also, see W5.\n    \n- (W3) Despite the approach being simple from the technical aspects (see S3), the model is dependent on the accuracy and structure of the LLM\u2019s outputs. This requires tailoring queries/prompts for a specific dataset or a set of datasets. \u00a0Potentially, they could require a lot of tuning. Even though the set of queries used in the paper is fixed, and appears to work on all datasets, these are queries/prompts that had to be tuned/selected to be somewhat \u201ccompatible\u201d with all datasets.\n    \n- (W4) The qualitative analysis (Figure 5, Appendix A) seems to consist of selected samples and likely does not represent the model\u2019s predictions across the whole dataset accurately.\n    \n- (W5) The method is evaluated only on datasets which (in this case explicitly) contain some forms of sub-populations, hierarchies, or significant differences across attributes. Although this is an important analysis, the question of whether the method is only usable in these kinds of datasets is open. Would the method still be usable for datasets that might, but not necessarily do contain (at least not explicitly) some form of sub-groups or diversity within classes (maybe ImageNet for example?). Or datasets where not much diversity is expected, e.g. StanfordCars dataset?"
            },
            "questions": {
                "value": "- (Q1) How exactly are the \u201cworst\u201d %x classes selected? Are they the same across all models or are they selected individually for each model? For Figures 6 (right) and 7, are they re-selected for every point (adding attributes, changing $k$ or $\\lambda$ or kept the same?\n    \n- (Q2) For the Breeds dataset, on which level of the hierarchy of the labels the model is trained on?\n    \n- (Q3) Is the image sample of a \u201cred wolf (in Figure 4) indeed a red wolf? Doing a quick search I am not so convinced that is what a red wolf looks like. Could it be a misclassified dog, for example? Do all other samples look similar to this one?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698852941325,
        "cdate": 1698852941325,
        "tmdate": 1699636204991,
        "mdate": 1699636204991,
        "license": "CC BY 4.0",
        "version": 2
    }
]