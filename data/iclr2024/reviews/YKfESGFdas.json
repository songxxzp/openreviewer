[
    {
        "id": "DS0hsT6Zyu",
        "forum": "YKfESGFdas",
        "replyto": "YKfESGFdas",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission51/Reviewer_oWAG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission51/Reviewer_oWAG"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a GeONet, a deep learning framework that solves the Optimal Transport problem, which refers to a method for comparing data distributions. Unlike traditional approaches, GeONet is mesh-independent and works well with high-dimensional data. It learns the mapping between input distributions and the Wasserstein geodesic connecting them during offline training, characterized by a PDE system. In the inference stage, it provides real-time predictions with significantly reduced computational cost. GeONet achieves comparable accuracy to traditional methods on various datasets but is much faster during inference."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The method applies the PDE method to compute Wasserstein geodesics. It solves a couple of PDE systems involving the continuity equation and the Hamilton-Jacobi equation. The method applies the least squared formulation with neural network parameterizations."
            },
            "weaknesses": {
                "value": "1. The application of neural networks to compute Wasserstein geodesics is interesting. The accuracy is still a big issue compared to classical mesh-based approaches. See \n\nFu, et.al., High order computation of optimal transport, mean field planning, and potential mean field games. Journal of computational physics, 2023.  \n\n2. Some more interesting computational examples in mean field control problems can be considered in future work. See\n\nLin, et.al. Alternating the Population and Control Neural Networks to Solve High-Dimensional Stochastic Mean-Field Games, PNAS."
            },
            "questions": {
                "value": "Some more numerical examples are needed to compare with the current method and the ones studied in the above literature."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission51/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698461904950,
        "cdate": 1698461904950,
        "tmdate": 1699635929230,
        "mdate": 1699635929230,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4iaqZF461a",
        "forum": "YKfESGFdas",
        "replyto": "YKfESGFdas",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission51/Reviewer_Q478"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission51/Reviewer_Q478"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the problem of geodestic on probabilistic density space equipped with Wasserstein distance, so that a similar neural operator is trained based on any given two starting density and the ending density to learn the geodesic \"curve\".  This is based on the Wasserstein distance optimization problem which is replaced with the KKT conditions.  The conditions then are formulated as the objective function to optimize the parametric geodesic curve. It may have certain applications in practice."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper does demonstrate couple of novel points.\n\n1. It is a novel idea (at least to this reviewer) to convert the \"KKT\" condition for a variational problem into an objective function for the solution which is based on the Benaniu-Brenier dynamic flow problem and Hamilton-Jacobi equation. \n\n2. Based on the idea used in neural operator, apply the neural operator approach for the PDE defined by the KKT condition.  This is done according to DeepONet architecture\n\n3. Although the topic is quite technical, the presentation is clear, and easy to follow."
            },
            "weaknesses": {
                "value": "Basically the topic and presentation look excellent, however it lacks of theoretical analysis for example whether the parametric method is appropriate given its complexity of the dynamic geodesic.  Another weak point is it lacks of more examples in application.  See my point in questions."
            },
            "questions": {
                "value": "1. The design is based on the assumption that we fully know the endpoint densities, however in real-world applications, instead of density, we only have a set of samples for each density.  How does the approach cope with such cases?\n\n2. It is not clear to me how the endpoint densities are defined in MNIST dataset experiment.   I thank you provide the code for the experiment on Gaussian Mixture densities."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission51/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698532588362,
        "cdate": 1698532588362,
        "tmdate": 1699635929160,
        "mdate": 1699635929160,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MtL8dLwxHY",
        "forum": "YKfESGFdas",
        "replyto": "YKfESGFdas",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission51/Reviewer_NBJx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission51/Reviewer_NBJx"
        ],
        "content": {
            "summary": {
                "value": "GeONet is a neural operator applied to the atask of amortizing optimal transport geodesics. Specifically, given training pairs of distributions as samples of the density at fixed resolution, a neural operator is learned which produces the geodesic between distributions. This is applied to toy examples such as mixture of gaussians in 1 and 2D as well as to a low dimensional embedding of a subset of MNIST."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- Presents a novel framework for amortizing the learning of Wasserstein geodesics based on neural operators.\n- The neural operator perspective is interesting in this case."
            },
            "weaknesses": {
                "value": "- Lack of comparisons and related work:\n    - [1,2] both amortize Wasserstein geodesic learning. These should be at least cited and discussed.\n    - There are many methods to compute Wasserstein geodesics relatively quickly, although without amortization. I would be curious how the quality of interpolation compares to these more recent methods, and would amend this statement for more recent work [e.g. 3,4,5].\n        \n        > Recently, a machine learning method to compute the Wasserstein\n        geodesic for a *given* input pair of probability measures has been considered in (Liu et al., 2021).\n        \n- The experiments show how well GeONet works in toy settings, although even this is not very clear given the lack of comparisons.\n    - The experiments are all extremely toy with limited examples and dimensionality, with the largest experiment being on a small subset (5000) of MNIST on a 30 dimensional embedded space.\n    - Why is the ground truth for the Gaussian mixture approximated using Convolutional Wasserstein Barycenters? I\u2019m actually fairly surprised the error is so low, especially for low regularization values.\n    - Why is error calculated in the encoded space for MNIST? It would be much more meaningful to calculate error in the ambient space. The error in the encoded space is difficult to understand and an unreproducible metric, particularly given the lack of code.\n    - The L_1 error is also twice the Total variation distance. I find it somewhat strange to use TV here, usually Wasserstein or MMD are used, but I guess this is okay for toy problems.\n    - Zero shot super resolution can be done by many modern methods. I\u2019m not sure this is a useful experiment without significantly more experimentation and comparison. Perhaps on benchmarks that are constructed with a known map? (see [6]).\n    - \u201cx-axis is the log of grid length in one dimension. This is somewhat confusing (also which log base?). Can this be replaced by the actual grid length?\n    - While I can see the usefulness of amortizing W2 computation for faster inference, I do not think the comparison in 4.4 is fair. I would be interested to know for a comparable accuracy, how fast POT and GeONet are, as I assume the POT solver is extremely accurate. Or, similar to shown in MetaOT, a comparison showing GeONet provides a better initialization and speeds up convergence of Sinkhorn-based solvers.\n    - It could be helpful to include a comparison to non-amortized methods to make clear under what circumstances amortization becomes beneficial.\n- The method suffers from the curse of dimensionality as it currently requires fixed sized grids as input. It would be interesting to consider more general input forms.\n\nOverall, while I find this general direction interesting. I find this work underdeveloped, especially when it comes to the experiments. I believe substantially more work is needed for this method to be interesting to an ICLR audience.\n\n1. Julien Lacombe, Julie Digne, Nicolas Courty, and Nicolas Bonneel. Learning to generate wasserstein barycenters, 2021.\n2. Brandon Amos, Giulia Luise, Samuel Cohen, and Ievgen Redko. Meta Optimal Transport, ICML 2023.\n3. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. ICLR 2023.\n4. Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky Chen. Multisample Flow Matching: Straightening Flows with Minibatch Couplings. ICML 2023.\n5. Alexander Tong, Nikoly Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, Yoshua Bengio. Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport. 2023.\n6. Korotin A, Li L, Genevay A, Solomon JM, Filippov A, Burnaev E. Do neural optimal transport solvers work? a continuous wasserstein-2 benchmark. Advances in Neural Information Processing Systems. 2021 Dec 6;34:14593-605."
            },
            "questions": {
                "value": "- It is not clear to me how to turn a general 30 dimensional embedding into a distribution in the MNIST example. Could the authors clarify this?\n\n--------------------------------\n\nEdit:\n\nI thank the authors for taking time to respond. I have read and considered the responses and updated manuscript. I believe the manuscript is improved, but still needs more work on the experimental side.\n\n(1b) thank you for the additional comparisons. However, I believe it is much more meaningful to compare against the OT versions of both papers. Specifically, using the re-flow strategy of [3] or the minibatch strategy of [4,5]. \n\n(2a) Thank you for including error on the ambient space. The ambient error seems extremely high, even at times zero and one. Perhaps this is not 0-1normalized? I\u2019m not sure how much we can say on this experiment with such a low-quality pre-trained autoencoder even on this small dataset. (Note: I did not have time to read the updated source code). \n\n(2b) I think the entropic regularization parameter used for the ground truth CWB should be noted, and it should be noted that you are actually comparing to an entropic regularized barycenter. I\u2019m not sure if CWB is given the same parameter, but it seems odd to me to report error between an entropic ground truth and a method which is trying to learn the non-entropic barycenter. \n\n(2c) Thank you for including these additional metrics. \n\n(2f) Thank you for the revision of Figure 6. I am more convinced by this, however I would like to ideally see the empirical error against time for these methods, not just a few different settings of the numerical error since a method like GeONet does not really have such a setting. I think this would add context to when GeONet is useful and can provide meaningful amortization.\n\nOverall, I thank the authors for the revision, but more work is needed to fully flush out these experiments. In particular, I would like to see baselines against other OT methods e.g. reflowed RF and minibatch OT CFM. My score remains the same."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission51/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission51/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission51/Reviewer_NBJx"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission51/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698617846721,
        "cdate": 1698617846721,
        "tmdate": 1701027388128,
        "mdate": 1701027388128,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oVYhdy5qqP",
        "forum": "YKfESGFdas",
        "replyto": "YKfESGFdas",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission51/Reviewer_1NDe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission51/Reviewer_1NDe"
        ],
        "content": {
            "summary": {
                "value": "This paper is trying to solving the optimal transport optimization to calculate the Wasserstein distance directly with a new neural operator learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The learned neural operator is extremely fast to calculate the Wasserstein distance."
            },
            "weaknesses": {
                "value": "I am not sure about whether this net can be generalized to unseen distribution Wasserstein distance calculations especially the ones are very different from the training data. Since in many times we don't know whether the distributions in the application field is similar to the ones in training, and if we just calculate the OT question, we can get the accurate results although it will be slow. But I think it is hard to ensure such a learned neural operator is generally applicable. If need retrain for OOD cases, then I want to know how much data is needed.\n\nAlso, very important, the readability is too bad, I think myself is familiar with many maths inside, however the symbols are not defined precisely make it very difficult to understand. I think much more details of the proposed algorithms and backgrounds need to be added. It shall be an important and interesting paper, but if it can not be understood by others then it will be difficult for application fields to use it. Better have more figures about the details as well."
            },
            "questions": {
                "value": "1. How much data is needed for retraining for OOD cases?\n2. I don't quite understand the justification of \"no need for retraining for new input\", I think it is for applicable for all the usages.\n3. I think this paper is rushed, better improve the writing. \n\nI think I will improve the points once a better-written version with the question answered. I think at least the topic is interesting and important, but I don't think it should be presented in this rough way that makes readers very difficult to understand all the details. I will definitely read all the explanations and the revised version of the paper. I expect it will be a much better one."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission51/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission51/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission51/Reviewer_1NDe"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission51/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698671181830,
        "cdate": 1698671181830,
        "tmdate": 1699635928858,
        "mdate": 1699635928858,
        "license": "CC BY 4.0",
        "version": 2
    }
]