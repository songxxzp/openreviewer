[
    {
        "id": "Vjwy5y25ZN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3822/Reviewer_ccWW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3822/Reviewer_ccWW"
        ],
        "forum": "TzcuXQq0aR",
        "replyto": "TzcuXQq0aR",
        "content": {
            "summary": {
                "value": "The authors study two issues in two-party VFL that occur when the passive party quits: a) the inference accuracy drops significantly as in VFL the passive party's participation is also required during inference, and b) the passive party might try to extract sensitive information about the active party's labels from the representation extractors. The authors propose an alternative VFL training approach to mitigate both issues: a) with a certain probability p during training, the active party will be set the passive party's representations to 0, and b) the mutual information between labels and representations is minimized. An evaluation of ResNet18 training on CIFAR10/100 clearly demonstrates the issues without the proposed mitigations and how those mitigations are indeed effective."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper studies two clearly relevant issues in the deployment of VFL.\n\nThe proposed robustness solution is simple yet effective.\n\nThe conducted evaluation answers most questions one might have in terms of dependence on parameter selection and comparison to related works."
            },
            "weaknesses": {
                "value": "The IP leakage issue of VFL that is discussed (the passive party can try to infer information about the active party's labels) is presented in the setting of the passive party quitting unexpectedly. However, to my understanding, this issue is completely unrelated to drop-outs and the same attack could also be carried out in case the passive party still participates in the system.\n\nIn \u00a75.1, the authors briefly discuss a \"naive\" alternative to their approach of ensuring robustness in case of the second party quitting. This alternative, which is described as fine-tuning the head model after the passive party quits, is dismissed as time-consuming and impractical. However, it would be interesting to see how many training iterations are actually necessary (when shifting all training iterations where the passive party's representation is set to 0 till after the point when the passive party quits) until the model reaches accuracy that is somewhat similar to the case where the proposed mitigation is applied; especially when using small p, the required time to take the service offline might be very small.\n\nThe work is somewhat limited in studying only a two-party setting where the accuracy drop as a direct consequence of the only other party quitting is obviously the strongest. This is somewhat fine since also many related works on VFL are restricted to the two-party case, which is also realistic in real-world settings. Nevertheless, a discussion on the likely impact of unexpected quitting of one party in a, e.g., five-party scenario would be appreciated.\n\nIn Figure 1, it is unclear which information the active party provides to the passive party for it to carry out the IP leakage attack.\n\n**Update:** The authors have clearly answered all my questions. However, I think the fact that the IP leakage issue is not actually related to the passive party dropping out is a major flaw in the presentation of the work and would require a major revision to fix. Therefore, I'm not upgrading my score."
            },
            "questions": {
                "value": "- Is the IP leakage issue related to passive drop-outs at all?\n- Can you clarify if the \"naive\" alternative to robustness discussed in \u00a75.1 is really impractical?\n- How does the impact of drop-outs behave as a function on the number of passive parties? So is the presented accuracy drop also critical when one passive party in a, e.g., five-party setting drops out?\n- Why is the evaluation in Table 3 only till p = 0.5; what happens in cases with high drop-out probability?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3822/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3822/Reviewer_ccWW",
                    "ICLR.cc/2024/Conference/Submission3822/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3822/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697292572173,
        "cdate": 1697292572173,
        "tmdate": 1700777502432,
        "mdate": 1700777502432,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yIGXKWtlLt",
        "forum": "TzcuXQq0aR",
        "replyto": "TzcuXQq0aR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3822/Reviewer_kTBy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3822/Reviewer_kTBy"
        ],
        "content": {
            "summary": {
                "value": "Federated Learning exists some technical issues that cause people to decide the trade-off between model performance and privacy issues. The paper presents solutions to alleviate the privacy loss while retaining better performances. In vertical federated learning (VFL), there are two vulnerabilities caused by unexpected quitting of passive parties in the deployment phase \u2013 severe performance drop and active party\u2019s label leakage. \n\nOn the one hand, the paper presents a VFL framework (PlugVFL) that could preserve the VFL model\u2019s performance against unexpected exit of passive parties. By omitting the representations from the passive party with a certain probability p in each communication round (iteration of training in VFL), the framework combines a weighted sum of the vanilla model training and model training without passive parties. Could think of the weighted method as the dropout method in traditional neural networks.\n\nGiven that in the deployment phase passive parties could still access the active model\u2019s data using the feature extractor, the chance of leaking labels in the active model increases. In a large dataset that has important or classified information, labels could be viewed as Intellectual properties that need to be protected. So PlugVFL, on the other hand, presents a method that could minimize the mutual information during the training phase. By calculating the variational upper bound, and minimizing the parameters that is less than the variational upper bound could achieve the result that passive parties hold as little as possible label information from the active party.\n\nThe paper also conducts different experiments to evaluate their framework\u2019s effectiveness. PlugVFL can improve the accuracy after a passive party\u2019s exit by more than 8% on CIFAR10, compared to passive party quitting in the normal situation. PlugVFL also prevents the passive party from fine-tuning a classifier that outperforms random guess levels even using the entire labeled dataset with only less than 2% drop in the VFL model accuracy, outperforming baselines significantly."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Given the workflow of how PlugVFL is designed, there are strong contributions that are achieved by it. In terms of defense, the model minimizes the mutual information between the representations of the passive party and the true labels, formulating the defense into an adversarial training algorithm that jointly minimizes the variational MI upper bound and production loss. It also improved the accuracy after the passive party's exit by more than 8% on CIFAR10. At the same time, all previous work provides IP protection in VFL training, and they are the first on IP protection on deployment phase."
            },
            "weaknesses": {
                "value": "There are several weaknesses given the fact and result this paper had presented. The experiment data is based on CIFAR dataset, and only split into two parties, one as active party and the other as passive party. Less than 1%(400) of the labeled data to fine-tune the model to perform model completion attack, and achieve comparable accuracy (~1% drop) compared to classifiers trained with all data just in part 2. Only two parties, and only one experiment, so that complexity is not guaranteed in larger and more complex federated tasks. Also the accuracy solely on party 2's data is not high in the first place. According to standard classifier performance on the CIFAR dataset, PlugVFL\u2019s performance is not that ideal. In the expectation formula for robustness of retain accuracy when pass parties drop, the probabilities of dropping is not explained reasonably, because the the some popular classifier is around 70% standalone but PlugVFL has relative low accuracy (~44.95%) compared to standards, and only split two parties, so lack of reason and fact of the generality of the model. Lastly, the paper mentions naive solutions for mitigating the accuracy drop, such as fine-tuning the head model after a passive party quits. To actually apply this method and get the accuracy result is more convincing, because what if this method is better than the method proposed by the paper."
            },
            "questions": {
                "value": "There are a few questions that need to be addressed/clarified. Firstly, the label is counted as IP protection in federated learning, and so valid, because the label has useful representation that the passive model can extract. The whole logic is that after passive parties normally will only have access to the representation extractors, which allow passive parties to fine-tune classifier heads with very few labeled data. But how are you going to define the difference of label type? Because different label data types could have different presentations, and how to ensure that could be generalized not just text labels. If the only representation you could extractor is a numeric matching, then it is not qualified for IP protection. Secondly, reduce co-adaptation of the hidden neurons of the head predictor, similar to dropout, but why setting p = zero, will you still have accuracy after party 2 quit? Based on your definition, in each communication, p will be zero and therefore making the model equivalent to calculating prediction performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3822/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3822/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3822/Reviewer_kTBy"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3822/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698371487942,
        "cdate": 1698371487942,
        "tmdate": 1699636339628,
        "mdate": 1699636339628,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "o1PRhi1yfX",
        "forum": "TzcuXQq0aR",
        "replyto": "TzcuXQq0aR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3822/Reviewer_71Ut"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3822/Reviewer_71Ut"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses two critical issues in the VFL scheme, namely (1) performance degradation after passive clients drop from the scheme, and (2) mitigating IP leakage. The former issue is a novel observation in this work, where the inference performance degrades when a passive client drops. To tackle these issues, the paper introduces the plugVFL scheme, which incorporates two regularizations during VFL training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is the first to identify a significant problem in VFL, showing that when a party drops, it negatively impacts the VFL scheme's performance.\n2. It also highlights the risk of IP leakage from feature extractors, demonstrating how passive parties can use their feature extractors for model completion and gain advantages within the scheme.\n3. The alternative training design is intuitive and the paper effectively addresses the overhead of calculating mutual information objectives.\n4. The paper successfully demonstrates the effectiveness of both objectives in a two-party VFL scenario for CIFAR image classification tasks.\n5. The paper is well-organized and easy to read."
            },
            "weaknesses": {
                "value": "1. The link between the IP leakage issue and the deployment-time passive party dropping is not clear. The paper seems to conflate the solution to two unrelated problems. The \"Active Model Completion (AMC)\" is based on training time, which contradicts the deployment-time scenario.\n2. The discussion of party dropping only considers two parties with limited evidence (CIFAR-10). The impact on more parties should be explored.\n3. The mitigation of party dropping seems to offer minimal advantages over multi-head training (as shown in Figure 3).\n4. The paper states that IP leakage is due to label information, supporting the design of mutual information regularization. However, the connection between IP leakage and label information is not well-established. Section 3.3 suggests that IP leakage is primarily about successful model completion, not direct extraction of labels from the active party. A counter-example is that a feature extractor can be learned without label information (i.e., unsupervised learning).\n5. The scalability of the alternative training for multi-party cases is questionable. For instance, in a 3-party scenario, the performance should remain consistent in multiple cases (both passive party drops or either one party drops). The equation in Section 3 could be split into four, increasing computation overhead significantly.\n6. The hyperparameter p setting for alternative training is debatable. Estimating the probability of dropping during training is challenging. The paper mentions that setting a relatively small p-value can improve the robustness of VFL. However, does this imply that the scheme should always set a small p?\n7. There are inaccuracies in some statements, such as the assumption that \"the service provider cannot afford to shut down the service while fine-tuning.\" In reality, the server could have a backup model to replace the running one without service disruption. Additionally, the statement, \"Without loss of generality, we formulate our PlugVFL framework in the two-party scenario,\" should ideally be demonstrated in more than two-party scenarios, enabling downgrading to two-party scenarios."
            },
            "questions": {
                "value": "Is there a naive way to mitigate performance degradation when a party drops? For example, could setting a mean vector value instead of zero help?\n\nImage classifications like CIFAR-10/CIFAR-100 may not be ideal examples of VFL tasks.  Is this framework effective on categorical datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3822/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3822/Reviewer_71Ut",
                    "ICLR.cc/2024/Conference/Submission3822/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3822/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698710132519,
        "cdate": 1698710132519,
        "tmdate": 1700501578075,
        "mdate": 1700501578075,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eXv0ZUyttR",
        "forum": "TzcuXQq0aR",
        "replyto": "TzcuXQq0aR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3822/Reviewer_T9tD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3822/Reviewer_T9tD"
        ],
        "content": {
            "summary": {
                "value": "This paper designs a PlugVFL framework to address two problems caused by the unexpected quit of passive parties, i.e. severe performance degradation and intellectual property leakage of the active party\u2019s labels."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The concerned problem is meaningful."
            },
            "weaknesses": {
                "value": "The experiments are not well designed."
            },
            "questions": {
                "value": "1. Fig. 3 and 6 plot the relationships between the test acc before party 2 quit and after party 2 quit, while Fig. 4 and 5 plot the relationships between the test acc and attack acc. Do they share the same hyper-paramters? What do the lines (which connect the points) mean? I donot get the meanings and reasons of the figures.\n\n2. From Table 3, PlugVFL shows worse results than simply training independently. So does the proposed method make sense?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3822/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762993279,
        "cdate": 1698762993279,
        "tmdate": 1699636339470,
        "mdate": 1699636339470,
        "license": "CC BY 4.0",
        "version": 2
    }
]