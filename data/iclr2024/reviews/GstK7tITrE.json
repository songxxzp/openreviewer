[
    {
        "id": "i21EiSTZKY",
        "forum": "GstK7tITrE",
        "replyto": "GstK7tITrE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1766/Reviewer_3kTT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1766/Reviewer_3kTT"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of text-guided 3D face generation and proposes a method to achieve 3D head generation with one feed-forward pass without test-time optimization.  Here 3D parametric head model and texture maps are used to represent 3D heads. This work first generates texture maps and shape parameters based on text prompts by optimizing the 3D head parameters and texture maps using standard SDS loss. In this way, a set of samples with text and corresponding 3D heads are generated. These samples are then used to train models to directly predict the 3D head representation from text. Here, the shape parameters are predicted by an MLP with CLIP text embedding as input, and the texture maps are generated by fine-tuning a stable diffusion model. This method achieves a better CLIP score and faster inference compared with prior art."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow.\n\n- The proposed method is technically sound. Most design choices are well-motivated."
            },
            "weaknesses": {
                "value": "- The baseline DreamFace seems to generate higher-quality results. Also, DreamFace considers 4K resolution texture maps while the proposed method only considers 256x256. \n\n- All generations have the same skin color, e.g. Mark Zuckerberg and Morgen Freeman in Figure 8. \n\nThis paper\u2019s results are qualitatively not as impressive as its baseline DreamFace,  with significantly lower resolution and skin color variation. I also have concerns regarding the fact that the training data selection step."
            },
            "questions": {
                "value": "Overall, this paper\u2019s results are qualitatively not as impressive as its baseline DreamFace, with significantly lower resolution and skin color variation. I also have concerns regarding the fact that the training data selection step. My questions include:\n\n- Can the proposed method be used for 4K generation or is there any fundamental limitation?\n- Why do the generations have very limited skin color variations? \n- The authors mention that they use SDS optimization to obtain 600 samples while selecting only 50 samples for training to \u201censure a balance of gender, ethnicity, and age\u201d. Why selection is needed, and why not just balancing the input text prompts? Also, 50 training samples sound very limited. Why not use more samples?\n- How does the proposed method compare with the SDS optimization pipeline which is used for training data generation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Reviewer_3kTT"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1766/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698606746061,
        "cdate": 1698606746061,
        "tmdate": 1699636105902,
        "mdate": 1699636105902,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hIvb9h3f2K",
        "forum": "GstK7tITrE",
        "replyto": "GstK7tITrE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1766/Reviewer_ggv2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1766/Reviewer_ggv2"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel approach for text-guided 3D animatable head avatar generation where a 3D avatar with desired facial characteristics is generated based on input textual prompts. It draws inspiration from recent works on diffusion-based text-to-3D approaches such as DreamFusion. The authors propose learning shape parameters of a FLAME-based 3D head model using a pretrained CLIP text encoder. A pretrained Latent Diffusion model is fine-tuned using an additional mean-texture token for generalized learning of the facial texture. The proposed method adopts SDS technique to generate training data for training the shape and texture generator. The main contribution is the reduction of inference time complexity for 3D avatar generation. The proposed method also does not require 3D annotated data for training. Qualitative and quantitative comparison results are presented with state-of-the-art methods on text-to-3D methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe proposed method claims the lowest test time complexity among existing test-to-3D models that can generate 3D faces. There is substantial reduction in inference time (1 min) compared to the most efficient method DreamFace, which takes around 5 mins for the optimization. However the texture resolution is lower than DreamFace.\n2.\tQualitative results denote decent quality of 3d faces.  The reconstructed avatar of celebrity faces are bearing resemblance to the real people.\n3.\tThe method supports specific prompts for tasks such as generating special characters (animation) and editing shape and style."
            },
            "weaknesses": {
                "value": "\u2022\tNovelty and Significance of contributions:\nThe novelty of the proposed method appears slightly limited. Similar to DreamFace, pretrained CLIP and LDM models are used in the avatar generation with independent geometry(shape) and texture generators. The idea of using a mean texture token is novel, but similar to ideas have been explored in the form of pre-defined identity token in DreamBooth, and domain-specific prompt tuning in DreamFace, Introduction point 2 mentions challenges in animation due to implicit representations as limitation of existing text-to-3D methods. However DreamFace uses the ICT-FaceKit face model that can be integrated with existing animation pipelines, so the benefit obtained from using FLAME model in the current work is not clear. DreamFace also additional benefits of hair selection and video-driven animation generation. It is not evident from the paper how much the state-of-the-art in text-guided-3D face avatar generation will be advanced by the proposed method. Although the paper claims reduction in inference time, there are doubts about the generalization ability of the method in generating arbitrary high-fidelity avatars of varying age, skin colours etc, given that the training data consists of manually selected 50 training samples generated by SDS optimization\nWriting Issues:  \n\u2022\tUnclear writing: \no\t(Page 2) \u201cneed for a cumbersome two-stage generation process\u201d \u2013 what two-stage generation process.\no\t(Page 2) \u201cmeticulously crafted to encapsulate essential human head texture information.\u201d - How\no\t(Page 3) \u201cwe further propose other specific design to generate high-quality animatable 3D head avatars\u201d \u2013 what designs.\no\t\u201cthe renewed text prompts can contribute to fine-grained personalized characteristics with high fidelity of identity\u201d- doesn\u2019t make sense\no\t\u201ccommon texture-wise features shared by human beings.\u201d\no\tEpsilon is not defined in Equation 1.\n \u2022\tTypos: \n\t\u201cour propose generalized shape\u201d  in Page 5\n         Equation 2 \\phi() needs to be replaced by e_\\phi()\n\n\u2022\tMissing citations : \no\t\u201cExisting methodologies [??] typically leverage SDS\u201d \no\t \u201cremarkable strides achieved in diffusion-based text-to-3D models [??]\u201d\no\t\u201cWhile these [??] SDS-based approaches\u201d\no\t\u201cLeveraging readily available off the-shelf models [??]\u201d\no\t\u201c[Articulated Diffusion]\u201d\n\nExperimental Results:\n\u2022\t3D view (other than frontal) should have been included similar to existing works such as DreamFace. In the absence of a supplementary video it is hard to assess the qualitative results.\n\u2022\tUser Study needed to assess the perceptual quality of the generated results.\n\u2022\tMore detailed ablation study should be presented, the significance of the mean texture token should be justified using quantitative metrics.\n\u2022\tSome failure cases should be present to illustrate limitations of the method."
            },
            "questions": {
                "value": "1.\tThe description \u201cdata-free\u201d strategy appears ambiguous as it also mentioned that SDS is used to generate training data. More clarity is needed on the training strategy in Section 3.3. Is a pretrained stable diffusion model being finetuned for the training data preparation?  What kind of candidate text prompts are used for the geometry and UV texture generation (few examples) How is it ensured that the \u201ctraining data\u201d generated using SDS sufficiently accurate for generalized performance at inference time.\n2.\tWhich pre-trained LDM models are used for finetuning?\n3.\tThe significance of the mean-texture token is not clear from the results. How is the mean-texture token prompt obtained at test time?  \n4.\tIs the mean-texture token sufficient to finetune a pretrained LDM (trained on diverse images) to the specific task of the Face UV texture generation. How is it ensured that the generated texture is consistent with face geometry? In the absence of UV texture ground truth to finetune pretrained LDM for texture how is the accuracy ensured at inference time?\n5.\tThe paper mentions \u201cwe set this parameter to a relatively low value and obtain more realistic, real-life outcomes.\u201d Is there any ablation done on the guidance scale parameter to justify this statement?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1766/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772281266,
        "cdate": 1698772281266,
        "tmdate": 1699636105829,
        "mdate": 1699636105829,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZMHWpYLP47",
        "forum": "GstK7tITrE",
        "replyto": "GstK7tITrE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1766/Reviewer_9Wmk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1766/Reviewer_9Wmk"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce a comprehensive pipeline for the generation of 3D heads. Their approach begins with the application of a Score Distillation Sampling (SDS) technique to create training data for FLAME-based models. Subsequently, they employ this paired dataset to train generators for both shape and texture. To evaluate the efficacy and efficiency of their method compared to baseline techniques, the authors conducted a series of experiments, the results of which are presented in the paper."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This innovative method presents a unique pipeline for text-to-3D head generation that distinguishes itself in several ways. Notably, it does not rely on annotated datasets for training, making it exceptionally versatile. Additionally, the utilization of FLAME as the 3D representation in this method contributes to faster inference times, setting it apart from other baseline approaches."
            },
            "weaknesses": {
                "value": "My apprehension revolves around the generative quality constrained by the use of FLAME. It appears that the resulting shape and texture may fall short of the realism achieved by DMTet-based or Nerf-based methods. Moreover, there seems to be a limitation in the ability to synthesize 3D hair components.\n\nFurthermore, it's worth noting that the methods employed in this approach draw heavily from existing techniques. For instance, the process of generating the training dataset bears a resemblance to DreamFusion, albeit with the incorporation of the FLAME representation."
            },
            "questions": {
                "value": "I'd like to pose two questions:\n\nIn Figure 4, I'm curious about how the model manages to synthesize 3D hair for \"Taylor Swift.\" It seems like a noteworthy achievement, and I'm interested in understanding the underlying techniques.\n\nIn Figure 2, during the training data preparation stage, there appears to be a differentiation in the input for Stable diffusion, involving both shader images and textured images. I'd like clarification on the purpose of these distinct inputs for various steps and how they relate to the rendering equation and the overall model."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1766/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815377802,
        "cdate": 1698815377802,
        "tmdate": 1699636105746,
        "mdate": 1699636105746,
        "license": "CC BY 4.0",
        "version": 2
    }
]