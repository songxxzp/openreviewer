[
    {
        "id": "1T08ApXgir",
        "forum": "TyFrPOKYXw",
        "replyto": "TyFrPOKYXw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7372/Reviewer_NMqi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7372/Reviewer_NMqi"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel algorithm, Safe Reinforcement Learning from Human Feedback (Safe RLHF), to address the crucial challenge of balancing the performance and safety of large language models (LLMs). LLMs often face an inherent tension between the objectives of being helpful and harmless, which can confuse crowdworkers during training. Safe RLHF effectively decouples human preferences related to helpfulness and harmlessness, enabling separate reward and cost models. The safety concern of LLMs is formalized as an optimization task to maximize the reward function while satisfying specified cost constraints. Using the Lagrangian method, Safe RLHF dynamically adjusts the balance between these two objectives during fine-tuning. Experimental results demonstrate that three rounds of fine-tuning using Safe RLHF significantly improve the helpfulness and harmlessness of LLMs, surpassing existing value-aligned algorithms. This work is a significant contribution to enhancing the safety of AI systems based on LLMs by effectively addressing the tension between helpfulness and harmlessness during fine-tuning, offering a promising approach to mitigate harmful responses while improving model performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper's strength lies in its innovative approach, Safe Reinforcement Learning from Human Feedback (Safe RLHF), which addresses the critical challenge of striking a balance between helpfulness and harmlessness objectives in the training of large language models (LLMs). By decoupling human preferences from these objectives, the paper ensures unbiased feedback during data annotation and adaptively balances the trade-off between these inherently conflicting training goals using the Lagrangian method. Notably, Safe RLHF is the first integration of Safe RL and RLHF frameworks, incorporating a two-dimensional human annotation scheme and a safe training mechanism. Through three rounds of Safe RLHF fine-tuning, the paper effectively enhances the helpfulness of the base model while significantly reducing harmful responses, surpassing the performance of existing value-aligned algorithms. The release of all data and training codes enhances the paper's reproducibility and validates its findings, making it a valuable contribution to improving the safety and performance of AI systems based on LLMs."
            },
            "weaknesses": {
                "value": "- Even though the paper is the first integration of Safe RL and RLHF frameworks, the contribution is limited in the case of Safe RL.\n- The comparison between the (reward and cost) model in Section 3.2 to the classic (reward and cost) signals in safe RL should be clarified more. \n- The convergence of the proposed methods may be hard to guarantee, and there are no related theoretical results."
            },
            "questions": {
                "value": "- Figure 1 needs to be polished more, and the font there is a bit small.\n- Can we use any Lagrangian safe RL methods (TRPO-Lag, PPO-Lag) in Safe RLHF?\n- What is the difference between Safe RLHF and the off-policy Lagrangian safe RL methods, e.g., WCSAC (published in AAAI-21 and MLJ-23)?\n- With safe RLHF, how can we ensure the learning is stable and finally converge?\n- The stability of the reward and cost signals should be analyzed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper contains example data that may be offensive or harmful. But there are relevant prompts in the paper."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7372/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7372/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7372/Reviewer_NMqi"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7372/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698168174081,
        "cdate": 1698168174081,
        "tmdate": 1700324050730,
        "mdate": 1700324050730,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KPQZcOiI9E",
        "forum": "TyFrPOKYXw",
        "replyto": "TyFrPOKYXw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7372/Reviewer_xNog"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7372/Reviewer_xNog"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm designed to address the challenge of balancing helpfulness and harmlessness in Large Language Models (LLMs). Safe RLHF decouples human preferences regarding helpfulness and harmlessness, allowing for separate training of reward and cost models. By leveraging the Lagrangian method, Safe RLHF dynamically adjusts the balance between these objectives during fine-tuning. Experimental results demonstrate significant improvements in both helpfulness and harmlessness compared to existing value-aligned algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to follow. The authors present a well-defined methodology, including a clear description of the Safe RLHF pipeline, preference annotation process, and training algorithms for reward and cost models.\n\n2. Given the societal impact of LLMs, ensuring their safety and usefulness is of utmost importance. Safe RLHF presents a significant contribution by effectively aligning human values with model behavior, addressing an essential concern in AI research."
            },
            "weaknesses": {
                "value": "1. The technique contributions seem incremental to me. The decoupling of rewards into rewards and costs is a standard formulation in CMDP, and the Lagrangian methods with RL are not new at all. \n\n2. Another concern in this paper is that I don't think there is an appropriate cost threshold and cost-reward trade-off in the LLM alignment settings. I think safety is cleary a priority when compared with preferences. So that being said, how do you define how much safety LLMs are to trade off the preference performance? If this is a super safe-critical scenario, the cost limit should be 0. That being said. A simple LLM finetuning reward function can be designed that A>B if (preference (A>B) and (A passes safety check threshold)). I do not see the necessity of using the cost-reward formulation of CMDP.\n\n3. Also, this concern is a follow-up to point 2. I think the authors failed to convince me that standard rewarding shaping is not good enough in this setting. Since Lagrangian methods need to try different threshold values, It is not enough to conclude Lagrangian methods is better than reward shaping for LLMs fine-tuning without trying different reward shaping coefficients."
            },
            "questions": {
                "value": "1. Could you elaborate on the limitations and potential risks associated with Safe RLHF? For instance, are there scenarios where the algorithm might fail to balance helpfulness and harmlessness effectively? Understanding the limitations would provide a more nuanced perspective on the applicability of your approach.\n\n2. The paper lacks a discussion on the computational resources and time required for the Safe RLHF training process. Can you provide insights into the computational efficiency and scalability of your method, especially concerning large-scale LLMs?\n\n3. As for safe RL methods like Lagrangian methods, the cost threshold is super sensitive. A threshold that is too small might result in complete failures of policy training. I noticed that the authors selected negative cost thresholds for two experiments. How many values of cost threshold have you tried? And how robust is the Lagrangian method for RLHF to different threshold values?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7372/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7372/Reviewer_xNog",
                    "ICLR.cc/2024/Conference/Submission7372/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7372/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698714581434,
        "cdate": 1698714581434,
        "tmdate": 1700516811170,
        "mdate": 1700516811170,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PFdXsk27jX",
        "forum": "TyFrPOKYXw",
        "replyto": "TyFrPOKYXw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7372/Reviewer_murU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7372/Reviewer_murU"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment in large language models (LLMs). Safe RLHF decouples human preferences regarding helpfulness and harmlessness, allowing separate training of reward and cost models. The safety concern of LLMs is formalized as an optimization task, and the balance between the two objectives is dynamically adjusted during fine-tuning. Through three rounds of fine-tuning using Safe RLHF, the paper demonstrates improved performance and harm mitigation compared to existing value-aligned algorithms."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. Separation of rewards and costs is an excellent idea that probably resolves the optimization contradiction in RLHF of LLM. \n2. The paper provides concrete experimental results demonstrating the effectiveness of Safe RLHF in enhancing model performance and reducing harmful responses."
            },
            "weaknesses": {
                "value": "Minor suggestions in Questions."
            },
            "questions": {
                "value": "1. Fig2.(a) the symbols of axis indexes are absent.\n2. Could you add the scatter point of Beaver-v2 and Beaver-v3 in Fig.6(a) and (b)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7372/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7372/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7372/Reviewer_murU"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7372/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769225323,
        "cdate": 1698769225323,
        "tmdate": 1699636881718,
        "mdate": 1699636881718,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1mO6jjKUVl",
        "forum": "TyFrPOKYXw",
        "replyto": "TyFrPOKYXw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7372/Reviewer_NDaV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7372/Reviewer_NDaV"
        ],
        "content": {
            "summary": {
                "value": "The authors propose safe rlhf, a framework to decouple helpfulness and harmfulness of RLHF model responses. They employ a dynamic \u03bb-trade-off to dual helpfulness and harmlessness objectives. They demonstrate that this approach also results in better inter-rater agreement thereby generating cleaner annotations for RLHF training. They define a cost model for harmlessness and pose the problem as a constrained optimization problem where they employ Lagrangian method to solve the same. Through three rounds of such safe RLHF iterations, they are able to demonstrate at 7B model scales that they are able to mitigate harmful responses while also improving model performance compared to standard value alignment algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "With safety being an important aspect in LLMs, this paper tackles an important question -- how to do value alignment under both the safety and usefulness axes. The paper is well written and explains the methodology involved clearly. Even if the techniques to accommodate safety costs into RLHF are simple and straightforward, the paper does a good job in explaining the motivation behind the choices and conducts careful ablations to demonstrate the motivations behind these choices. The evaluation methodology is also robust and the paper carefully evaluates the design choices. The paper also presents a clean framework to decouple different human values safety being one of them from the overall utility of the responses and thus can be extended to other desiderata easily."
            },
            "weaknesses": {
                "value": "One thing that I feel the paper could do a better job of is to incorporate more safe RLHF baselines. For example, Constitutional AI [1] tackles a very similar problem balancing helpfulness and harmlessness. The only couple of ablations that I can see are of fixed lambda (reward shaping approach) and the approach used in Sparrow. I would have loved to see one or two more safe RLHF approaches that do not need to tow the lines of conventional RLHF exactly.\n\nThe improvement achieved in the RLHF stages over the base models is often a function of the SFT stage in between. Aspects of safety can also be incorporated in the SFT data and the paper uses Alpaca 7B off the shelf. If a fine-tuning stage could be done on the responses collected as part of the safety data (or special SFT data can be collected in this regard) and can be introduced as a step in between, then the gains of RLHF with the cost/preference models could be more clearly earmarked compared to the simpler SFT stages. This will help us understand how much value we get by framing this problem during the RL stage vs SFT stage vs both. The focus on the SFT aspect is one thing that find missing in this paper. \n\n\n1. Constitutional AI: Harmlessness from AI Feedback - Bai et al, Dec 2022."
            },
            "questions": {
                "value": "1. I think you can do a slightly better job of rewriting the cost model explanation in Section 3.2. Particularly it was a bit time consuming to understand the motivations behind the loss formulation that had both the BT terms and also classification-like term. It would be nice to revisit this explanation in the final draft.\n\n2. More of a suggestion : I would recommend moving important things like Related Work to the main section of the paper instead of leaving them in Appendix. It would be important to position the work in the context of other relevant works. The page limitation could be adjusted by shortening and reformatting other sections or moving some of those to appendix.\n\n3. Were other safe RL baselines considered apart from Sparrow and reward shaping ?\n\n4. Is there any reason why stronger SFT models were not trained considering the fact that you gathered some safety related data before doing RLHF ?\n\n5. I am not sure whether there are clear explanations of how model selection was done both in the RM/Cost model training stage and RLHF training stage. Can you clarify ? Often one finds that model selection plays a much bigger role in final performance than even the training algorithm. Considering you do multiple RLHF iterations, I wanted to understand this aspect clearly. \n\n6. Again a minor suggestion : Some of the labels and text in the figures were hard to read at 100% resolution. Kindly resize them for appropriate reading."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7372/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7372/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7372/Reviewer_NDaV"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7372/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699356175922,
        "cdate": 1699356175922,
        "tmdate": 1700498504366,
        "mdate": 1700498504366,
        "license": "CC BY 4.0",
        "version": 2
    }
]