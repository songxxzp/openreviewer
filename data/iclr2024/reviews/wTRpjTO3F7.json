[
    {
        "id": "URkLM9r9kd",
        "forum": "wTRpjTO3F7",
        "replyto": "wTRpjTO3F7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1499/Reviewer_PVtL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1499/Reviewer_PVtL"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to create a reliable and comprehensive evaluation method for zero-shot coordination. The authors design an evaluation workflow includes three stages. Firstly the method generates behavior preferring agents with corresponding BRs. Secondly representative policies are selected based on a BR-diversity. Finally the selected policies and their BRs are used to evaluate the ego policy. The overall framework has potential to provide a more effective metric (BR-Prox) and also indicates some shortcuts of overcooked scenarios."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The methodology of constructing a \"diversity-complete\" set of BRs is reasonable and meaningful.\n2. The authors propose the BR-Prox metric to measure the performance of an ego policy under the ZSC manner and use this metric to benchmark previous methods on ZSC.\n3. The authors propose an algorithm to construct an evaluation population by first generating adequate policies and then selecting diverse ones."
            },
            "weaknesses": {
                "value": "1. Though the authors claim that a \"diversity-complete\" set may be intractable for complex environments, the proposed methodology (generation and selection) fails to well adhere to the exact definition in the desiderata (Section 3.2) with the lack of revealing the gap with a real \"diversity-complete\" set.\n2. The proposed metric and evaluation workflow lack necessary discussions with previous approaches (e.g., methods in Table 1).\n3. The presentation of this paper is generally obscure. The authors involve a sort of techniques during the evaluation workflow but do not well explain the necessity and details (e.g., how to represent the behavior feature of a policy and reasons to involve event-based rewards).\n4. The evaluated baselines seem not distinguishable in the evaluation setting while the authors do not provide further insights of what kinds of approaches are generally useful on ZSC.\n5. The evaluation workflow is restricted to a two-agent form while previous ZSC methods can generalize to multi-agent settings."
            },
            "questions": {
                "value": "1. How can we extend the evaluation manner to multi-agent settings?\n2. In Figure 2, how are the high-level behaviors visualized? What is the meaning of different data points in Figure 2?\n3. In Figure3, why does the population diversity first rise and then drop with the population size increasing? As near 0 values mean linear correlation, why is the diversity low in a small population size?\n4. Does the P-Div metric mean $\\text{PD}$ on $\\pi_i$ instead of $BR(\\pi_i)$?\n5. In Section 5.2, how do Figure 4 and 5 show \"increasing population size contributes to the improvement of performance under the condition that the diversity of the population is also grown\". \n6. How do the event-based rewards contribute to behavior preferring policies? Specifically, how do the method design and adjust $w$? How is this approach related to proposed \"skill-level diversity\"?\n7. At the end of Section 3.2, \"by selecting earlier checkpoints of the evaluation partners, it is simple to acquire evaluation partners with diverse skill levels.\" What is the actual method of selecting earlier checkpoints of the evaluation partners?\n8. How can we represent the behavior feature $\\theta$ of a policy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1499/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1499/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1499/Reviewer_PVtL"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1499/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697711546595,
        "cdate": 1697711546595,
        "tmdate": 1699636078632,
        "mdate": 1699636078632,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BJ9TZck6vO",
        "forum": "wTRpjTO3F7",
        "replyto": "wTRpjTO3F7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1499/Reviewer_tgST"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1499/Reviewer_tgST"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new evaluation metric for the zero-shot coordination problem. The method involves first training diverse policies and the corresponding best responses, employing the similar method as HSP, and then select evaluation partners based on the best response diversity metric. \nThe ego agent is then evaluated with the selected partners and a metric called best response proximity is calculated based on the performance of the ego agent versus the best response policy. The experiment results demonstrate that some widely used layouts in the literature may lack enough complexity to evaluate the effectiveness of different methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. In general, the authors propose an important question that the evaluation protocol should be improved in the literature of ZSC problem. \n2. The idea that uses a set of sufficiently diverse policies as evaluation partners is straightforward and promising. \n3. The experiment result demonstrates the effectiveness of the proposed metric to distinguish different methods in conflicts layouts."
            },
            "weaknesses": {
                "value": "A significant limitation is the absence of a crucial baseline, specifically HSP. Since the paper's approach to training evaluation candidates and best response policies closely mirrors that of HSP, it should not pose a substantial challenge to also train an ego agent for HSP. I would consider increasing my score if this limitation is addressed."
            },
            "questions": {
                "value": "1. In figure 3, how would the population diversity of selection with population diversity lower than that of selection with BR-Div? This seems to be counterintuitive as it is expected to find the subset with largest population diversity if P-Div is used as the selection metric.\n2. Please explain the difference among different layouts in more details.  Are there other kinds of events except conflicts should be considerred to influence the perfomance of different methods in the introduced layouts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1499/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1499/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1499/Reviewer_tgST"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1499/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698569370555,
        "cdate": 1698569370555,
        "tmdate": 1700573563373,
        "mdate": 1700573563373,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VCJA1kVAqv",
        "forum": "wTRpjTO3F7",
        "replyto": "wTRpjTO3F7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1499/Reviewer_dEYW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1499/Reviewer_dEYW"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the evaluation metric for zero-shot coordination. The authors propose to construct diverse evaluation partners with their approximate BRs, and then compute the proposed BR-Prox across these evaluation partners as the metric. BR-Prox measures the performance similarity between the ego agent and the approximate BRs of the evaluation partners."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The studied question is important and interesting for zero-shot coordination\n- The paper is easy to follow"
            },
            "weaknesses": {
                "value": "- The main concern is the practicality of the proposed evaluation method. It evolves complicated steps to construct such evaluation agents and prepare their approximate BRs. The idea of computing the BR diversity is straightforward, but it is not easy to really use such a metric in practice. And the huge implementation effort would definitely reduce its impact on the community.\n- The design of reward space is crucial for the proposed evaluation partners. However, it is clearly discussed, at least I didn\u2019t find it yet in the main text. What\u2019s more, it is hard to say the resulting partners would show expected reasonable behaviors.\n- In addition to the evaluation agents, the proposed method requires users to compute the approximate BRs. In simple tasks like Overcooked, it could be fine. However, it could be hard to obtain in complex robot tasks."
            },
            "questions": {
                "value": "- How to compute the approximate BR, $\\widehat{BR}(\\pi_{\\omega})$ ?\n- How to design the reward space?\n- If all of these evaluation agents can constructed, why not use them to train the ego agents?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1499/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1499/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1499/Reviewer_dEYW"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1499/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834219121,
        "cdate": 1698834219121,
        "tmdate": 1700661687456,
        "mdate": 1700661687456,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VGMm2FWuFW",
        "forum": "wTRpjTO3F7",
        "replyto": "wTRpjTO3F7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1499/Reviewer_6Vtg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1499/Reviewer_6Vtg"
        ],
        "content": {
            "summary": {
                "value": "The performance of the agent's Zero Shot Coordination (ZSC) capability is difficult to measure and quantify. The difficulties are twofold: (1) how to construct sufficient diverse evaluation partners? (2) how to measure the performance? Most previous methods focus on designing superior ZSC algorithms while not paying much attention to the evaluation metric. \n\nThis paper first proposes to construct 'diversity-complete' evaluation partners by maximizing the best response diversity (the population diversity of the BRs to the evaluation partners). Then the paper proposes a Best Response Proximity (BR-Prox) metric, which quantifies the ZSC capability as the performance similarity to each evaluation partner\u2019s approximate best response, demonstrating generalization capability and improvement potential.\n\nEvaluations conducted on the overcooked environment validate the effectiveness of the proposed evaluation workflow and show some interesting results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper may be the first to systematically study how to measure and quantify the agent's Zero Shot Coordination (ZSC) capability. The proposed evaluation workflow is technically sound. \n* Sufficient experiments are designed to demonstrate the effectiveness of the method. The results find that the most used layouts in the overcooked environment cannot show the ZSC capability difference among the ZSC methods.\n* Overall, the writing of the article is relatively clear."
            },
            "weaknesses": {
                "value": "* Some parts of the paper are not very clear. For example, the motivation for introducing the event-based rewards is not clear. \n* In practical implementation, the method requires humans to manually define some triggered events so as to derive diverse behaviors, which is difficult to obtain in complex tasks.\n* Comparisons with previous baselines listed in Table 1 may be missing."
            },
            "questions": {
                "value": "Please see the weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1499/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1499/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1499/Reviewer_6Vtg"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1499/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699859758364,
        "cdate": 1699859758364,
        "tmdate": 1699859758364,
        "mdate": 1699859758364,
        "license": "CC BY 4.0",
        "version": 2
    }
]