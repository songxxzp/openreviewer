[
    {
        "id": "PT3McypGDx",
        "forum": "tqiAfRT1Lq",
        "replyto": "tqiAfRT1Lq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2818/Reviewer_j3g4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2818/Reviewer_j3g4"
        ],
        "content": {
            "summary": {
                "value": "This paper formalizes the task elicitation problem, under which existing learning paradigms are applicable. The authors also introduce GATE, a learning framework that uses language models to interact with users and elicit their preferences. The authors compare GATE to multiple other elicitation methods across three tasks and find that their proposed approach is comparable or sometimes better than baselines but at comparable or less mental demand for users."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper introduces a nice framing to think about task elicitation and proposes a simple, but potentially effective method leveraging language models.\n- I appreciated that the authors conducted a user study to evaluate users on multiple tasks, rather than simply using language models as a proxy for human users, and considered a number of relevant real-world metrics (e.g., mental load)."
            },
            "weaknesses": {
                "value": "- While the paper introduces three different domains, two of the three baselines were not available for two of the three tasks. Since GATE shows promising results for content recommendations, it is hard to assess the validity and generalizability of the results without similar comparisons to baseline elicitation approaches for the other two tasks.\n- It is unclear the extent to which the results found in this paper hinge on the choice of GPT-4 as both the LM that is used to elicit user preferences and the model that is used to predict user preferences. For the latter, prior work tends to train a reward model on the human data, rather than use the collected data as prompt input.\n- It seems like elicitation using generative open-ended questions is the best of the three generative variants studied in this work (according to Figure 2), however, it also seems to be the most mentally demanding for the users (according to Figure 3) and seemed to be comparable to baselines. It would be nice to see a proposed modification to that approach that would maintain the *same* score in Figure 2 but would decrease the reported mental load, particularly given that the generative approaches that were tried in this work were all relatively simple (as stated by the authors themselves)."
            },
            "questions": {
                "value": "Other clarification questions\n- How is the framework in Section 2 connected to Section 3? The two sections felt a bit disconnected.\n- Incorrect reference in Area under the p(correct)-time curve paragraph?\n- In Figure 2, what does the * signify?\n- What do 6/10 and 7/10 settings refer to?\n- Does the term \u201ctime limit\u201d refer to the 5-minute interaction period?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2818/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2818/Reviewer_j3g4",
                    "ICLR.cc/2024/Conference/Submission2818/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2818/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698527802197,
        "cdate": 1698527802197,
        "tmdate": 1700603383871,
        "mdate": 1700603383871,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6WG9xgLG4K",
        "forum": "tqiAfRT1Lq",
        "replyto": "tqiAfRT1Lq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2818/Reviewer_d4UR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2818/Reviewer_d4UR"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an approach for an interactive recommeder system on the basis of LLMs, here GPT4. The authors suggest three methods- \"Generative active learning\", \"Generating yes-or-no questions, \"Generating open-ended questions\" - for which the propose respective LLM prompts. The evaluation is conducted for three domains, namely content recommendation, moral reasoning, email verification by means of a user study including in total 388 participants. The system is compared to standard supervised learning trained on Microsoft News Dataset (only for content rec.), a pool-based active learner and user-made prompts for preferences. The results show that the three proposed methods are either on par or even outperforming the baseline approaches for the domains. In addition, the felt mental demand of the participants was mostly lower for the proposed methods, with open-ended questions being the most demanding among the three proposed approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The research direction is fruitful for LLMs, as their capability to continuously, autonomously interact with end-users is crucial. To this end, the three proposed methods are sensible. It is therefore insighful and value adding to evaluate the performance of the respective prompts.\n\nThe paper clarity is quite high, showing detailled evaluation results, method descriptions and related works. \n\nThe empirical results are promising, showing that LLMs can guide the user towards the correct goal. In addition, the chosen baselines are quite well-chosen, making the results significant. The conducted user study is well-structured and a important part for the contribution of the paper. Here, the results are very insightful and can support future prompt design or fine-tunings. It is informative and very relevant to see that the different proposed prompts have different advantages in the respective domains and felt mental loads."
            },
            "weaknesses": {
                "value": "It is unclear where other known preference elicitation approaches such as pairwise comparisons / choice-based preference elicitation / ... fit into the stated related work. They might be situated in example-based/interactive, but especially there are other studied interaction mechanisms in this field which might have overlaps to free-form. It should be part of the related work coverage.\n\nPart of the investigated methods (generative active learning and generating yes-or-no questions) might be too rigid, as a interests/preferences might not be black and white. I understand that the user can give any response to any of the methods, thereby giving more refined answers than yes/no, but handling arbitrary relative answers might become difficult to handle. The paper does not analyse the availability of such a problem / rule it out based on the results of the conducted user study. Here, ablations of possible user answers or prompt changes might be insightful\n\nAs mentioned in the reproducibility section, the authors used closed-source GPT4 for their experiments, which makes exact replication difficult."
            },
            "questions": {
                "value": "Did you test implications of answer diversity on the success of your methods? Taking yes/no questions, a user might still answer with any fuzzy statement which might be quite ambiguous. It would be intersting to know if limiting the answers to potentially some selected \"in-between\" categories would help or not. Along this, did you analyse how often a user answered with such an ambiguous statement?\n\nWould it be possible to combine the proposed prompts to improve performance / reduce mental load? Did you experiment with such prompts? The open-ended version could, of course, do so by design - did you conduct analyses if this happens?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2818/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834096742,
        "cdate": 1698834096742,
        "tmdate": 1699636225288,
        "mdate": 1699636225288,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9i4IxzA1qh",
        "forum": "tqiAfRT1Lq",
        "replyto": "tqiAfRT1Lq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2818/Reviewer_Nhj6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2818/Reviewer_Nhj6"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the issue of training LLMs to perform complex task such as personalized website recommendations via interaction with the users in an active learning mode."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ comprehensive and well written paper\n+ introduction of a novel method GATE for generative open-ended questions and model improvement with active learning \n+ ethical considerations and limitations are well thought\n+ code availability"
            },
            "weaknesses": {
                "value": "- the testing is well-thought but quite limited. Such models require an extensive testing to ensure they are not overfitting, especially when for systems like this one which may eventually foster polarization as an effect of extreme self-centred recommendations.\n- the concept of morality is underdefined. What is the notion of morality employed here? There is quite some literature on the moral foundations theory and their detection from text which should be used as a benchmark for the model"
            },
            "questions": {
                "value": "- the testing is well-thought but quite limited. Such models require an extensive testing to ensure they are not overfitting, especially when for systems like this one which may eventually foster polarization as an effect of extreme self-centred recommendations.\n- the concept of morality is underdefined. What is the notion of morality employed here? There is quite some literature on the moral foundations theory and their detection from text which should be used as a benchmark for the model"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2818/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840953243,
        "cdate": 1698840953243,
        "tmdate": 1699636225225,
        "mdate": 1699636225225,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BmNLe1sAyC",
        "forum": "tqiAfRT1Lq",
        "replyto": "tqiAfRT1Lq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2818/Reviewer_djQq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2818/Reviewer_djQq"
        ],
        "content": {
            "summary": {
                "value": "The authors propose using LLMs to elicit or derive from the user what their preferences are based on a new method called GATE. Here the LLM asks the user either yes or no, or open ended questions to find out what their preferences are for recommendations. The research experiment shows improvement in novel recommendation considerations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I am afraid I cant see any strength."
            },
            "weaknesses": {
                "value": "The authors of this research project have not considered the ethical aspects of having LLMs ( they already have their own bias and hallucination issues) probe humans to spill their preferences. Interacting with a chatbot an/or  AI systems have shown to be psychologically endearing to humans and yet the researchers seem to be intent on making recommendations more accurate than the population's psychological impact. \nNo in depth assessment of conversation based interactions of this nature and their strength of cognitive processes in gathering accruate recommendations outside of this field was provided."
            },
            "questions": {
                "value": "Please provide any ethical assessments done during or prior to IRB approval of how such a system can impact humans over the long term use of a GATE based recommendation system? Please provide more depth to this \" A fundamental challenge across many fields is how to obtain information about people\u2019s nebulous\nthoughts, preferences, and goals. In psychology and cognitive science, protocol analysis describes\nmethods for how to obtaining and analyze verbal reports from subjects about cognitive processes\nincluding via think-aloud protocols (Ericsson & Simon, 1980; Ericsson, 2017)\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2818/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2818/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2818/Reviewer_djQq"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2818/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699339150450,
        "cdate": 1699339150450,
        "tmdate": 1699636225136,
        "mdate": 1699636225136,
        "license": "CC BY 4.0",
        "version": 2
    }
]