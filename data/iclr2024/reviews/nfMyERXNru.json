[
    {
        "id": "XRJRDJcAxW",
        "forum": "nfMyERXNru",
        "replyto": "nfMyERXNru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2017/Reviewer_6Tks"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2017/Reviewer_6Tks"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new framework that decomposes a video into multiple multiple RGB layers and associated opacity levels for various video editing tasks.\nThe proposed framework uses two neural network modules, RGB-net and \u03b1-net, to predict RGB layers and opacity/transmission layers for each video frame. \nThese layers are then composited to reconstruct the input video or achieve the desired effects during optimization.\nThe paper conducted experiments on video relighting, dehazing, and VOS tasks and achieves superior results compared to some existing baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Writing is overall clear.\n- Various tasks are conducted and better performance w.r.t to some baseline is reported.\n- No pre-training is required (test-time optimization)."
            },
            "weaknesses": {
                "value": "- This paper proposes to unify various downstream tasks by video decomposition. However, the formulation of the video relighting task in this paper uses one RGB layer to represent the enhanced video and an alpha-layer to represent gamma correction. This formulation does not have any meaning of \u201cdecomposition\u201d. On the other hand, this paper carefully designs different decomposition definitions and corresponding constraints for different downstream tasks, but these constraints and decomposition definitions seem to lack a unified and general formulation. Therefore, I felt such kind of \u201cunification\u201d seems too artificial and forced.\n\n- The proposed formulation is not new (or at least has very few novelty).\nOverall, the proposed formulation can be summarized as a \"data-term + prior\" approach, which is a rather common formulation in optimization-based approach for image / video synthesis tasks. \nUsing neural networks as an implicit prior for data term is not new (as also mentioned in the paper - \"These approaches have highlighted the importance of formulating a loss function,\ncombined with the optimization of neural network parameters\") and the proposed paper extends this idea into other tasks.\nYet, the exact formulation designed for each task is dedicated but common (e.g., gamma-correction curves [1] for relighting; alpha-blending [2,3] for video-segmentation, dark-channel prior for dehazing).\n\n- Missing baselines for UVOS tasks: The paper compares with both pre-trained methods and test-optimization methods. For pre-trained methods, there are massive VOS methods [4,5,6] that has far better performance than compared methods in this paper; For test-optimization methods, there are also other video decomposition methods that targets this task [2,3].\n\n[1] Zhang, Mohan, et al. \"RT-VENet: a convolutional network for real-time video enhancement.\" Proceedings of the 28th ACM International Conference on Multimedia. 2020.\n\n[2] Kasten, Yoni, et al. \"Layered neural atlases for consistent video editing.\" ACM Transactions on Graphics (TOG) 40.6 (2021): 1-12. \n\n[3] Gu, Zeqi, et al. \"Factormatte: Redefining video matting for re-composition tasks.\" ACM Transactions on Graphics (TOG) 42.4 (2023): 1-14.\n\n[4] Cheng, Ho Kei, Yu-Wing Tai, and Chi-Keung Tang. \"Rethinking space-time networks with improved memory coverage for efficient video object segmentation.\" Advances in Neural Information Processing Systems 34 (2021): 11781-11794.\n\n[5] Yan, Kun, et al. \"Two-shot Video Object Segmentation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[6] Cheng, Ho Kei, and Alexander G. Schwing. \"Xmem: Long-term video object segmentation with an atkinson-shiffrin memory model.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022."
            },
            "questions": {
                "value": "- While the proposed method does not require pre-training, it needs to optimization on each input sequence which takes time. How long does it take for optimizing a video sequence?\n\n- Is there a common insight or general guidance on applying this framework to downstream tasks? For example, what if applying this framework to tasks like denoising or super resolution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2017/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2017/Reviewer_6Tks",
                    "ICLR.cc/2024/Conference/Submission2017/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2017/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697893600036,
        "cdate": 1697893600036,
        "tmdate": 1700450877980,
        "mdate": 1700450877980,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cgPJtQurJw",
        "forum": "nfMyERXNru",
        "replyto": "nfMyERXNru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2017/Reviewer_XPSu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2017/Reviewer_XPSu"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new approach called the Video Decomposition Prior (VDP) framework. Unlike conventional methods, VDP leverages the motion and appearance of the input video, decomposing it into multiple RGB layers with associated opacity levels. These layers are then manipulated individually to achieve the desired results, addressing tasks like video object segmentation, dehazing, and relighting. The paper also introduces a logarithmic video decomposition formulation for relighting tasks. The approach is evaluated on standard video datasets, including DAVIS, REVIDE, and SDSD, demonstrating qualitative results on a diverse range of internet videos."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The proposed video decomposition prior leverages the motion and appearance of the input video, decomposing it into multiple RGB layers with associated opacity levels.\n+ The proposed VDP is employed in different video-based tasks, including video object segmentation, video dehazing and video relighting."
            },
            "weaknesses": {
                "value": "- In the paper, the limitations of the VDP is not discussed in the paper, and all the results are good cases. \n- In the introduction of Flow similarity loss, the VGG embeddings of masked flow-RGB and those of other layers are used to calculate the cosine similarity. It is unclear how to generate VGG embeddings from masked feature maps for cosine similarity calculation. \n- In equation (12), the behavior of the reconstruction layer loss resembles that of the L1 loss in the reconstruction loss. To validate the rationale behind the design, it is essential for the paper to elucidate the distinctions between them and elucidate the impact of the reconstruction layer loss."
            },
            "questions": {
                "value": "Please refer to the questions in the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2017/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698681871396,
        "cdate": 1698681871396,
        "tmdate": 1699636132923,
        "mdate": 1699636132923,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yN9lSoPMjV",
        "forum": "nfMyERXNru",
        "replyto": "nfMyERXNru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2017/Reviewer_8QGm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2017/Reviewer_8QGm"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the Video Decomposition Prior (VDP) framework, a new approach for video editing tasks, including object segmentation, dehazing, and relighting. VDP decomposes videos into multiple layers and optimizes parameters without explicit training. The proposed logarithmic video decomposition enhances video relighting, resulting in state-of-the-art performance in downstream tasks: unsupervised video object segmentation, dehazing, and relighting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Sound approach: The VDP framework presents an innovative approach to video editing, offering practicality and cost-effectiveness by not relying on extensive datasets or ground truth annotations. The ability to optimize parameters using the test sequence itself distinguishes VDP from traditional deep learning methods, which often require extensive training data.\n\n2. State-of-the-Art Performance: VDP demonstrates top-tier performance in key downstream tasks, including unsupervised video object segmentation, dehazing, and relighting.\n\n3. Good writing and representation."
            },
            "weaknesses": {
                "value": "I am generally positive about this paper. my main concern lies in the lack of comprehensive comparison: The paper does not provide a comprehensive comparison with other video editing techniques, making it difficult to assess the VDP framework's performance against other state-of-the-art methods. Furthermore, video editing is a broad field, including tasks such as adding or removing objects. It seems that the proposed method may not be suitable for handling these scenarios. Therefore, I suggest that the authors consider refining the title and corresponding claims."
            },
            "questions": {
                "value": "Please see the weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2017/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2017/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2017/Reviewer_8QGm"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2017/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758549980,
        "cdate": 1698758549980,
        "tmdate": 1699636132858,
        "mdate": 1699636132858,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "t3VxCUNR8b",
        "forum": "nfMyERXNru",
        "replyto": "nfMyERXNru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2017/Reviewer_u6Cn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2017/Reviewer_u6Cn"
        ],
        "content": {
            "summary": {
                "value": "This paper attempts to propose a general framework for video editing. It starts by predicting the video into several individual layers and then utilizes task-specific knowledge to reconstruct the video. A reconstruction loss and a warping loss are utilized to train the network. This is an inference-time optimization framework that does not rely on external training tools. However, the framework is rather standard, and the choice of a few loss functions for the decomposition tasks is straightforward. Overall, the novelty is limited. I am inclined to reject this article. Please check the details in other sections."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The motivation to propose such a general framework is good."
            },
            "weaknesses": {
                "value": "-: The reconstruction loss and warp loss used are very common loss functions and do not offer any novelty.\n\n-: The method is still not sufficiently general. For example, for the task of video segmentation, additional loss functions need to be designed as constraints. The three task-specific losses mentioned in Section 3.2 are the only ones provided. From this perspective, it is difficult to see what this framework proposed in the paper brings to this community.\n\n-: The comparison results of the experiments are not that fair. For instance, in Table 3, the comparison is made with the latest algorithm, CG-IDN (a 2021 algorithm). There are many dehazing algorithms that could be compared, including single-image hazing algorithms with stability processing (references [1][2]).\n\n-: Additionally, Table 2 lacks many baselines. Looking at the official website of DAVIS2016, the best baseline achieves an IOU score of over 82. Why wasn't this paper compared against that?\n\n-: Sometimes it is necessary to introduce additional prior knowledge, such as for the task of dehazing, where the effectiveness is actually limited.\n\n[1] Learning Blind Video Temporal Consistency\n\n[2] Blind Video Temporal Consistency via Deep Video Prior"
            },
            "questions": {
                "value": "See weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2017/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2017/Reviewer_u6Cn",
                    "ICLR.cc/2024/Conference/Submission2017/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2017/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698759639502,
        "cdate": 1698759639502,
        "tmdate": 1700631896700,
        "mdate": 1700631896700,
        "license": "CC BY 4.0",
        "version": 2
    }
]