[
    {
        "id": "hky7wqHn6F",
        "forum": "MQrFaQC3kj",
        "replyto": "MQrFaQC3kj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5969/Reviewer_PBf8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5969/Reviewer_PBf8"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to estimate the accuracy-fairness trade-off curve given the dataset and model class. It first uses an existing method, You-Only-Train-Once (YOTO), to get the trade-off curve efficiently without training multiple models. It then proposes a way to obtain the confidence intervals."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The accuracy-fairness curve is widely used in the algorithmic fairness community. How to set a range of achievable fairness violations is a practical problem. The method proposed in this paper provides us with an efficient solution for estimating the curve with two-sided confidence intervals."
            },
            "weaknesses": {
                "value": "1. The method is highly based on an existing method, YOTO. Although it doesn't need to train multiple models, the cost of YOTO is not discussed in this paper.\n2. The accuracy-fairness curve is algorithm-agnostic. Figure 4 shows that the estimated curve has more errors when using some particular algorithmic fairness methods. However, people tend to use some algorithmic fairness methods to train the model. In that case, the practical use of the curve is concerned. \n3. The method requires an in-distribution calibration dataset and only applies to in-distribution tests."
            },
            "questions": {
                "value": "1. What is the L_fair in equation (2)?\n2. What are the costs and limitations of YOTO?\n3. Can we extend the method to be algorithm-sensitive? For example, suppose we know what algorithmic fairness method to use, can we estimate the curve for that algorithm?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5969/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699223771578,
        "cdate": 1699223771578,
        "tmdate": 1699636637699,
        "mdate": 1699636637699,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WYqIl0m6Tr",
        "forum": "MQrFaQC3kj",
        "replyto": "MQrFaQC3kj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5969/Reviewer_H5sg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5969/Reviewer_H5sg"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a methodology to estimate the optimal fairness-accuracy trade-off curve for a given dataset and model class. The key ideas of the proposed method are two steps::\n\n\n1. Use the You-Only-Train-Once (YOTO) framework to estimate the trade-off curve by training a single model. \n2. Use the YOTO result to construct confidence intervals around the estimated trade-off curve using a held-out calibration dataset. \n\n\nThe claimed contributions of this paper are:\n1. develop a method to calculate a range of allowed fairness violations for a given dataset and desired model accuracy. \n2. Construct confidence intervals that statistically guarantee the optimal fairness-accuracy tradeoff curve.\n3. Test the proposed method when sensitive attributes are scarce in the data.\n4. Test the proposed method on different types of data. The intervals contained the tradeoff curves from state-of-the-art fairness methods like regularization and adversarial learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The confidence intervals of fairness seem needed in the fairness domain., however, it has problems.\n2. The paper is clearly written and easy to follow. The graphics effectively illustrate the key ideas."
            },
            "weaknesses": {
                "value": "1. The statement \"For a given dataset, model class, and accuracy, the permissible range of fairness violation is x to y.\" in this paper is problematic. Accuracy and fairness have an inherent connection (they exhibit trade-offs) and will influence each other. So accuracy cannot be the condition for the range of fairness violation.\n2. How to evaluate whether the confidence interval is rational or correct? This does not seem to have ground truth for this and only visually evaluating figure 4 is not enough. There is a strong assumption that the curve learned with YOTO is the ground truth, this is not reasonable and even wrong. This strong but possibly wrong assumption is only mentioned in \"In other words, under the assumption of large enough model capacity, training the loss-conditional YOTO model performs as well as the separately trained models while only requiring a single model.\" (Page 4). I strongly recommend treating this seriously.\n3. The changeable fairness-accuracy trade-offs using one model may incur ethical issues, such as generating biased outcomes for certain groups of people. Based on this, I think this paper needs further ethical review.\n4. The $\\mathcal{L}_{fair}$ in Eq (2) is not presented at all. This paper should present the smooth relaxation of demographic parity.\n5. What is the meaning of \"Dataset Fairness\" in the title? It seems this title is not suitable for the proposed method.\n6. The experimental evaluation is not convincing to me. Since the Adult data is super imbalanced and the COMPAS data is small. I would suggest adding more experiments to really evaluate the proposed method, such as on the folktable dataset at https://github.com/socialfoundations/folktables\n\n\nThis paper does not meet the standards for acceptance to ICLR in its current form. For now, I would recommend rejection."
            },
            "questions": {
                "value": "Please address my concerns in the Weakness part.\n\n----\n----**After rebuttal**---\n\nI thank the authors for their response and am sorry for the late response.  The authors' response addresses part of my concerns. But \n1. The new explanation that \"the lower confidence intervals presented in Proposition 3.4 depend on the gap between the YOTO achieved trade-off curve and the ground-truth trade-off curve\" is essentially making a strong assumption that the YOTO should be good enough, although not group truth. I do not think this point is reasonable and grounded. \n2. The evaluation based on such small tabular data is not convincing to me, without new results presented. \n\nBased on the above and my original comments, I would maintain my original score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The changeable fairness-accuracy trade-offs using one model may incur ethical issues, such as generating biased outcomes for certain groups of people. Based on this, I think this paper needs further ethical review."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5969/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5969/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5969/Reviewer_H5sg"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5969/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699257696025,
        "cdate": 1699257696025,
        "tmdate": 1701065820453,
        "mdate": 1701065820453,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6mOPqeAeoK",
        "forum": "MQrFaQC3kj",
        "replyto": "MQrFaQC3kj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5969/Reviewer_SKWL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5969/Reviewer_SKWL"
        ],
        "content": {
            "summary": {
                "value": "Fairness-accuracy trade-off widely exists in machine learning models and fundamentally depends on dataset characteristics. Such dataset-dependent property impedes chasing universal fairness requirement across datasets. To this end, this paper proposes a computationally efficient approach to approximate the trade-off curve with statistical guarantees via adopting YOTO framework. The empirical results provide the guidelines in accuracy-constrained fairness decisions for various data modalities."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe research problem on the fairness-accuracy trade-off is fundamental and important in machine learning fairness community.\n2.\tThis paper is overall well-written and easy to follow.\n3.\tDue the unknown Pareto frontier of trade-off, the investigation of trade-off with confidence interval with statistical guarantee makes much sense to me."
            },
            "weaknesses": {
                "value": "1.\tMotivation. Can the authors elaborate on the motivation for using a universe fairness requirement across datasets? What are the advantages of doing this?  \n2.\tTechnique novelty. This paper introduces a computationally efficient method for estimating the trade-off. However, from my understanding, the efficiency part directly adopts YOTO framework and the confidence interval estimation only involves trivial bounds.  \n3.\tPareto frontier. The achievable trade-off by YOTO may not be consistent with the ground-truth Pareto optimum. It seems that this paper is over-claimed since the true Pareto trade-off investigation is not touched. Additionally, how do you use a universe fairness requirement across datasets? The approximated trade-off seems not be a good choice since the gap between achievable trade-off by YOTO and ground-truth Pareto optimum may also be dataset-dependent. \n4.\tExperiments. (a) The evaluation of the confidence interval is vague. It seems that the conservative estimation is never penalized by the current results, such as Figures 3 and 4. Which confidence interval estimation method is better? (b) In Section 3.1, the author mentioned $\\lambda$ in Eq. (2) offers litter control over the accuracy, which is counter-intuitive for such regularization. Can you provide experimental results to further support this statement? (c) Is it possible to create a synthetic dataset with a known ground-truth trade-off in the experiments? Otherwise, many conclusions can only hold for the achievable trade-off by YOTO. (d) There are confidence intervals for both accuracy and fairness. How can you plot these two intervals in Figures 3 and 4? \n5.\tFrom my understanding, the optimization for fairness with accuracy and the optimization for accuracy with fairness constraint have the same trade-off. I am curiosu why the authors select the former one and highlight the difference in the first paragraph of section 2.1."
            },
            "questions": {
                "value": "Please see weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5969/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699263020468,
        "cdate": 1699263020468,
        "tmdate": 1699636637479,
        "mdate": 1699636637479,
        "license": "CC BY 4.0",
        "version": 2
    }
]