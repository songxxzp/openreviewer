[
    {
        "id": "CCJKJrHg6z",
        "forum": "akKNGGWegr",
        "replyto": "akKNGGWegr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5536/Reviewer_SP5L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5536/Reviewer_SP5L"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new framework called STGKD for spatio-temporal graph knowledge distillation, which aims to encode robust and generalizable representations of spatio-temporal graphs. The framework incorporates the IB principle to enhance the knowledge distillation process by filtering out task-irrelevant noise in the student\u2019s encoding and alignment during knowledge transfer. Moreover, it introduces a spatio-temporal prompt learning component that injects dynamic context from the downstream prediction task. Through extensive experiments, the authors demonstrate that STGKD surpasses state-of-the-art models in both performance and efficiency. The paper's contributions include addressing the challenges of efficiency and generalization in large-scale spatio-temporal prediction, introducing a novel and versatile framework, and demonstrating the effectiveness of the proposed approach through extensive experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1. In terms of originality, the paper presents a new paradigm for learning lightweight and robust Multi-Layer Perceptrons through effective knowledge distillation from cumbersome spatio-temporal Graph Neural Networks. The incorporation of the IB principle and spatio-temporal prompt learning components is also a novel contribution to the field.\nS2. The quality of the paper is high, as the authors provide a clear and detailed description of the proposed framework, including the technical details and experimental methodology. The experiments are well-designed and conducted, with extensive evaluations on various spatio-temporal forecasting tasks. \nS3. The clarity of the paper is also commendable, as the authors provide a clear and concise introduction to the problem, a detailed description of the proposed framework, and a thorough evaluation of the results. The paper is well-organized and easy to follow, with clear and informative figures and tables."
            },
            "weaknesses": {
                "value": "W1. The dataset lacks a detailed description. Traffic Data and Crime Data lack links or citations to papers. It would also be good to have a table that describes the size of the dataset along with some other information that would give the reader a clearer picture of the dataset. In addition, there is a detail error, in Datasets the serial number in front of Weather Data should be iii) instead of ii).\nW2. Limited discussion of the limitations and potential problems. The paper does not provide a detailed discussion of the limitations of the proposed framework or potential extensions to the work.\nW3. The experiment lacks a comparison of runtime. In addition to the comparison of efficiency, the running time of the different methods should also be compared, which is also a very important indicator."
            },
            "questions": {
                "value": "Can authors provide a more detailed explanation of the interpretability and explainability of the proposed framework? The paper mentions that the student MLP selectively inherits task-relevant spatio-temporal knowledge from the teacher GNN framework , but it does not provide a clear explanation of how this knowledge transfer occurs and how the student model utilizes the transferred knowledge. Including a more detailed discussion on the interpretability and explainability of the framework would enhance the understanding of the proposed approach and its inner workings."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5536/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5536/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5536/Reviewer_SP5L"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5536/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698654055525,
        "cdate": 1698654055525,
        "tmdate": 1699636568178,
        "mdate": 1699636568178,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZE9s3CKdZh",
        "forum": "akKNGGWegr",
        "replyto": "akKNGGWegr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5536/Reviewer_qPxf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5536/Reviewer_qPxf"
        ],
        "content": {
            "summary": {
                "value": "The paper explores a critical area of research in large-scale spatio-temporal prediction. The poor scalability and generalizability of existing spatio-temporal model hinder their deployment in real-world urban scenarios. To this end, the authors propose Spatio-Temporal Graph Knowledge Distillation (STGKD) paradigm to learn lightweight and robust MLPs through effective knowledge distillation from cumbersome spatio-temporal GNNs. Robust knowledge distillation is achieved by integrating the spatio-temporal information bottleneck with the teacher-bounded regression loss. To further enhance the generalizability of student MLP, the authors incorporate learnable spatial and temporal prompts into the student model's input so as to inject downstream task contexts. Experimental results show that the proposed model outperforms state-of-the-art approaches in terms of both efficiency and accuracy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-motivated. The study of large-scale spatiotemporal prediction models has wide-ranging potential applications, and the issue of spatiotemporal distribution shift is indeed a crucial challenge that needs to be addressed for achieving accurate predictions in long-term and large-scale scenarios.\n\n2. The authors have provided a clear and coherent explanation of the motivation behind the design of various modules of the STGKD model. \n\n3. The logical structure of the paper is well-organized and easy to follow.\n\n4. The experimental results presented in the paper are comprehensive and well-designed, covering overall performances, ablation studies, and case studies."
            },
            "weaknesses": {
                "value": "1. The analysis of challenges in the introduction section is rather general and does not elaborate on the unique challenges in addressing the issues of scalability and generalization in designing methods for spatiotemporal scenarios.\n\n2. As stated in the related works, 'A significant contribution of this work lies in the novel integration of the spatio-temporal information bottleneck into the KD framework.' However, it should be noted that the incorporation of the information bottleneck into knowledge distillation has been previously explored, e.g., see references [3] [4] below. The paper lacks clarification on how the proposed method differs from existing approaches.\n\n3. As an ICLR submission, the paper lacks theoretical guarantees. For instance, it is better to quantify the model's robustness against noise after adopting such information bottleneck regularizer. Additionally, as reducing complexity of the model is an important idea of knowledge distillation, providing a generalization bound related to the model's complexity would strengthen the method's support.\n\n4. Regarding spatio-temporal prompt learning module, here are two weaknesses:\n\n   (a) A comparison with existing prompt learning methods is missing, and the similarities and differences should be clarified to prevent confusion.\n\n   (b) Utilizing three types of prompts as input and a learnable embedding method has been done in previous STGNN works [1,2], limiting the novelty of the proposed method.\n\n5. Weaknesses in experiments:\n\n   (a) The overall performance improvements of the proposed model on all datasets are not significant.\n\n   (b) The STID model, which is similar to the proposed model but without the knowledge distillation module, outperforms most complex STGNNs, raising the question of why transferring knowledge from weaker STGNNs to MLPs can lead to improvement. However, the paper does not offer clear explanations for this phenomenon.\n\n   (c) To achieve a fair comparison, the model-agnostic spatio-temporal prompt learning should be incorporated into SOTA STGNNs\n\n   (d) The authors have conducted generalizability testing on PEMS data with synthesized data missing. However, this type of distribution is only one specific example of covariate shift, and there are various other types of distribution shifts that need to be considered, e.g., the distribution shifts of traffic patterns during rush hours or seasonal traffic patterns shifts.\n\n[1] Spatial-Temporal Identity- A Simple yet Effective Baseline for Multivariate Time Series Forecasting. CIKM 2022.\n\n[2] Dynamic and Multi-faceted Spatio-temporal Deep Learning for Traffic Speed Forecasting. KDD 2021.\n\n[3] Efficient Knowledge Distillation from Model Checkpoints. NIPS 2022.\n\n[4] Variational Information Distillation for Knowledge Transfer. CVPR 2019."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5536/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698718583416,
        "cdate": 1698718583416,
        "tmdate": 1699636568075,
        "mdate": 1699636568075,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XIvQCdzFSc",
        "forum": "akKNGGWegr",
        "replyto": "akKNGGWegr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5536/Reviewer_BFQG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5536/Reviewer_BFQG"
        ],
        "content": {
            "summary": {
                "value": "The authors leverage the concept of knowledge distillation within graph structures to address the challenges of generalization and scalability in spatio-temporal graph forecasting. Their innovative approach involves compressing expansive GNNs into more compact and efficient MLPs. This compression is achieved through the Spatio-Temporal Graph Knowledge Distillation paradigm, which ensures robust knowledge transfer by filtering out task-irrelevant noise using an integrated spatio-temporal information bottleneck. Furthermore, by adopting the teacher-bounded regression loss, the model avoids misguided directions during the learning process. The added spatio-temporal prompts provide the student MLP with richer context from downstream tasks, further enhancing its generalization capabilities. After spatio-temporal datasets, the results confirm the framework's superiority, outclassing existing models in both efficiency and accuracy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The paper's presentation is top-notch. Its use of plots, clear definitions, and intuitive explanations significantly enhance the reader's understanding.\n\n(2) The motivation driving the research question is cogently articulated.\n\n(3)  The authors showcased the breadth of their research by selecting a diverse range of datasets. Their comprehensive ablation study, encompassing Spatio-Temporal Prompt Learning, Spatio-Temporal IB, Teacher-Bounded Regression Loss, and Spatio-Temporal Knowledge Distillation, is commendable. I appreciate their meticulous approach in Section 4 to test scalability, generalization, and robustness, aligning perfectly with the research's core motivation."
            },
            "weaknesses": {
                "value": "(1) Novelty: My primary concern pertains to the paper's novelty. While the authors posit that the integration of the spatio-temporal information bottleneck into the Knowledge Distillation (KD) framework is a significant contribution, I'd like to highlight that the concept of the information bottleneck has already been explored in the context of knowledge distillation[1]. Moreover, the idea of employing knowledge distillation on dynamic graphs isn't novel either[2,3,4]. It appears the authors are leveraging well-established ideas to tackle specific challenges in spatial-temporal graph forecasting.\n\n(2) Evaluation: Another area of improvement is in the choice of baseline models for evaluation. Notably, the absence of other graph knowledge distillation models as baselines seems to be an oversight. Including them would make the comparison more comprehensive and equitable.\n\nReferences:\n[1] Wang, Chaofei, et al. \"Efficient knowledge distillation from model checkpoints.\" Advances in Neural Information Processing Systems 35 (2022): 607-619.\n\n[2] Zhang, Qianru, et al. \"Spatial-temporal graph learning with adversarial contrastive adaptation.\" International Conference on Machine Learning. PMLR, 2023.\n\n[3] Ma, Yihong, et al. \"Hierarchical spatio-temporal graph neural networks for pandemic forecasting.\" Proceedings of the 31st ACM International Conference on Information & Knowledge Management. 2022."
            },
            "questions": {
                "value": "(1) Could the authors clarify the distinct contributions that set their approach apart from existing methods in graph knowledge distillation?\n\n(2) For a comprehensive evaluation, why were other graph knowledge distillation models not considered as baseline models? Would incorporating them not offer a more balanced and insightful comparison in the context of your study?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5536/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5536/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5536/Reviewer_BFQG"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5536/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698733311280,
        "cdate": 1698733311280,
        "tmdate": 1699636567978,
        "mdate": 1699636567978,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4qU8UUpBFL",
        "forum": "akKNGGWegr",
        "replyto": "akKNGGWegr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5536/Reviewer_wtdf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5536/Reviewer_wtdf"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the Spatio-Temporal Graph Knowledge Distillation (STGKD) framework, designed to tackle the scalability and generalization challenges in large-scale spatio-temporal prediction for urban computing applications like transportation, public safety, and environmental monitoring. While Graph Neural Networks (GNNs) are commonly used for capturing spatial-temporal correlations, they struggle with large-scale datasets and changing data distributions over time. STGKD addresses these issues by transferring knowledge from complex GNNs to more efficient Multi-Layer Perceptrons (MLPs), improving scalability and efficiency. This is achieved through a robust knowledge distillation process, integrating a spatio-temporal information bottleneck and a teacher-bounded regression loss to filter out noise and prevent erroneous guidance. Additionally, spatial and temporal prompts are incorporated to enhance the generalization capability of the student MLP, helping it to adapt to distribution shifts and unseen data. The proposed paradigm is evaluated on three large-scale spatio-temporal datasets, demonstrating superior performance in terms of efficiency and accuracy compared to state-of-the-art models. The implementation of STGKD is made available for reproducibility, showcasing its practical applicability and effectiveness in urban computing domains."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors have provided an extensive and meticulous set of experiments, encompassing various studies like ablation, scalability, generalization, and robustness, ensuring a thorough evaluation of their work.\n\nThe methodology introduced in the paper offers a fresh perspective, utilizing both spatial and temporal prompts to unravel dynamic patterns, which presents an intriguing approach.\n\nThe paper articulates a well-defined research question, and the data is effectively communicated through well-structured figures and tables."
            },
            "weaknesses": {
                "value": "Clarity in the Introduction:\nThe flow of logic in the introductory section needs to be refined. The paper initially highlights that most existing research prioritizes spatial dependency, followed by a discussion on the challenges of generalization and scalability. However, these sections seem disjointed. Furthermore, introducing the paper's contributions prior to addressing challenges like noise does not establish a coherent narrative.\nLack of Motivation:\nIt is crucial to elucidate the motivation behind the proposed approach in the introduction to provide readers with a clear understanding of its relevance and significance.\nPreliminary Section Gaps:\nThe preliminary section covers two prevalent concepts, yet it falls short by not including knowledge distillation. This addition is necessary for a comprehensive understanding of the topic.\nAmbiguity in Approach Explanation:\nThe description of the approach leaves room for improvement. For instance, the statement, \"Our goal is to distill the valuable knowledge embedded in the GNN teacher and effectively transfer it to a simpler MLP, enabling more efficient and streamlined learning,\" raises questions about what constitutes 'valuable knowledge' and why this process makes the MLP more efficient rather than more effective.\nIn terms of novelty and motivation, the manuscript does not make a strong case. While it appears that the authors might be introducing knowledge distillation to the GNN domain for the first time (though this is not explicitly claimed in the paper), this alone does not constitute a substantial contribution. The paper needs to delineate the differences between traditional knowledge distillation approaches applied to CNNs and STGNNs, and the proposed method, explaining why it is particularly effective in the GNN context."
            },
            "questions": {
                "value": "1. What are the significant differences between applying the knowledge distillation to CNN and STGNN?\n2. For SPATIO-TEMPORAL IB INSTANTIATING, what is the spatio-temporal part here?\n3. The paper uses the term prompt. What is the difference between the prompt you used and spatio-temporal features? Could I regard it as contextual spatio-temporal features?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5536/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5536/Reviewer_wtdf",
                    "ICLR.cc/2024/Conference/Submission5536/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5536/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758727905,
        "cdate": 1698758727905,
        "tmdate": 1700666643805,
        "mdate": 1700666643805,
        "license": "CC BY 4.0",
        "version": 2
    }
]