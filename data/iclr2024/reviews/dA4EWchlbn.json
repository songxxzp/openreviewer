[
    {
        "id": "edn6zRd76i",
        "forum": "dA4EWchlbn",
        "replyto": "dA4EWchlbn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3684/Reviewer_pfh7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3684/Reviewer_pfh7"
        ],
        "content": {
            "summary": {
                "value": "The authors establish a connection between the curvature of the data manifold, as perceived by a model during training, and the model\u2019s adversarial robustness. They provide empirical evidence showing that neural networks gain adversarial robustness more slowly in less robust regions of the data manifold."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- A novel perspective on adversarial robustness through the lens of data robustness (curvature of data manifold)\n- Claims are backed up by empirical results\n- Generally good writing"
            },
            "weaknesses": {
                "value": "The most critical issue is the lack of comparison with prior works. It is well-known in the literature that not all data points are equally susceptible to adversarial attack, and this has motivated the design of various variants of adversarial training ([1,2,3,4,5,6]) to (adaptively) focus on a subset of training samples. The authors made a similar observation \"it appears beneficial to emphasize the least robust elements during training\", but seemed to be completely unaware of this line of research. Without proper discussion and comparison with prior works, it is hard to fairly position and evaluate this work in the vast literature. \n\nReferences\n\n[1] Cai, Qi-Zhi, Chang Liu, and Dawn Song. \"Curriculum adversarial training.\" Proceedings of the 27th International Joint Conference on Artificial Intelligence. 2018.\n\n[2] Ding, Gavin Weiguang, et al. \"MMA Training: Direct Input Space Margin Maximization through Adversarial Training.\" International Conference on Learning Representations. 2019.\n\n[3] Wang, Yisen, et al. \"On the Convergence and Robustness of Adversarial Training.\" International Conference on Machine Learning. PMLR, 2019.\n\n[4] Zhang, Jingfeng, et al. \"Geometry-aware Instance-reweighted Adversarial Training.\" International Conference on Learning Representations. 2020.\n\n[5] Zeng, Huimin, et al. \"Are adversarial examples created equal? A learnable weighted minimax risk for robustness under non-uniform attacks.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 12. 2021.\n\n[6] Xu, Yuancheng, et al. \"Exploring and Exploiting Decision Boundary Dynamics for Adversarial Robustness.\" The Eleventh International Conference on Learning Representations. 2022.\n\n[7] Xiao, Jiancong, et al. \"Understanding adversarial robustness against on-manifold adversarial examples.\" arXiv preprint arXiv:2210.00430 (2022)."
            },
            "questions": {
                "value": "One of the mainstream hypotheses regarding adversarial examples is the off-manifold assumption ([7]): \"Clean data lies in a low-dimensional manifold. Even though the adversarial examples are close to the clean data, they lie off the underlying data manifold.\" I would like to understand how this hypothesis is related to your findings (focusing on data manifold with high curvature is helpful in adversarial training)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3684/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3684/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3684/Reviewer_pfh7"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3684/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698545724224,
        "cdate": 1698545724224,
        "tmdate": 1699636325099,
        "mdate": 1699636325099,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GL3Byw656H",
        "forum": "dA4EWchlbn",
        "replyto": "dA4EWchlbn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3684/Reviewer_vUSY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3684/Reviewer_vUSY"
        ],
        "content": {
            "summary": {
                "value": "The paper leverages concepts from metric geometry to understand how neural networks perceives the geometric property, particularly curvature, of the data manifold. To be more specific, it argues that data that more susceptible to adversarial perturbations are connected to regions in the data manifold with high curvature and vice versa. The paper proposes to use Eq 1 to quantitatively measure such curvature information as perceived by the model. The empirical studies in this paper are based on CIFAR10 and CIFAR100. A series experiments are performed to verify the proposed connection between curvature and model robustness. Building on these findings, the paper propose a learning rate strategy that increases the adversarial robustness model against $\\ell_2$ and $\\ell_\\infty$-norm bounded adversarial perturbations generated using AutoAttack in a white-box setting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Originality**:\nThe paper proposes a very novel (to my knowledge) and unique perspective to understand the adversarial robustness of neural networks. The proposed concept is also quite intuitive: certain data points are inherently more susceptible to adversarial attacks due to specific properties they possess.\n\n**Quality**:\nI am not an expert in manifold learning nor in metric geometry, so this limit my ability to properly assess the technical details of Section 3.  However, on the empirical side, the paper tries to provide several experiments to validate their hypothesis regarding the connection between model's robustness and its perception on the curvature of data manifold.\n\n**Significance**:\nThe proposed data perspective provides an interesting direction on which future methods can be designed to better address the adversarial robustness problem."
            },
            "weaknesses": {
                "value": "The paper has two main weaknesses.\n\n1. The clarity of the presentation and the quality of interpretation regarding the empirical observations could benefit from further refinement to enhance understanding.\n2. The improvement in adversarial robustness is very marginal."
            },
            "questions": {
                "value": "**Figure 1**:\nI suppose this is on CIFAR10. How are the most/east robust elements defined?\nThe claim at the bottom of page 1 is essentially that by training for another 2000 epochs, the most robust training data (at epoch 400) become 1% more robust (at epoch 2400); and the least robust training data (at epoch 400) becomes 7.5% more robust (at epoch 2400). Is that correct? However, are we tracking the same training data? Is \"eval_adversarial_acc\" the validation accuracy? \n\nAlso, it seems that the author uses the term \"validation set\" as a parts of the training set, which is odd.\n\nI did not understand \"For CIFAR-100, we disregarded the 40 least robust elements because of distorted robustness values due to differently labelled duplicate images\". What are \"differently labelled duplicate images\"? What happens to the result if we include them?\n\n\n**Figure 2**:\nMy understanding is that the absolute sensitivity is computed using Eq1. How are the relative sensitivity computed? \nIn the analysis of Figure2, it is said that \"The generated data, in particular, admit much shorter tails, indicating more robust elements overall. \" However, arent the curves of  1m-1 and 1m-2 quite similar to cifar-10/100/100s? Also, why does the pseudo-labbeled data have very different sensitivity? \n\n**Figure 3**: \nThe interpretation of the results are not clear. Do we know what the differences in the mechanisms of MDS and RTE are that leads to this inverted pattern? It would be very helpful to interpret the result if there is some brief explanation on what those methods are and why they are used.\n\n**General suggestions on all figures**:\nPlease consider using subcaptions to increase the clarity of the results.\n\n**Exploration experiments**:\nIn the setup column of Table2, just to clarify: minus v, \"-v\" means removing v from the training set, and (v) means all the validation accuracy is based on v. Is this correct?\nAre the most sensitive data points computed based on another pre-trained model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3684/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3684/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3684/Reviewer_vUSY"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3684/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698983043916,
        "cdate": 1698983043916,
        "tmdate": 1699636324985,
        "mdate": 1699636324985,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ujVDHwfPzM",
        "forum": "dA4EWchlbn",
        "replyto": "dA4EWchlbn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3684/Reviewer_8FHc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3684/Reviewer_8FHc"
        ],
        "content": {
            "summary": {
                "value": "The authors investigate the robustness of neural networks undergoing training via gradient descent through the lens of the geometry of the data. The authors analyze the dynamic of robustness by proposing a measure of \u201cperceived curvature\u201d. Essentially, the perceived curvature resembles the local Lipschitz constant exhibited by the neural network, modified so that the predictions are mapped to the discrete set of labels. Algorithmically, the authors analysis implies that by emphasizing the least-robust elements of the training set, modest gains in adversarial test error can be achieved. \n\nThe authors perform exploratory experiments by showing some correlation between the perceived manifold curvature and robustness as well as visualizations depicting the most and least robust examples and data sensitivity.\n\nWhile the paper is interesting and the experiments are reasonably comprehensive, I do not think this paper offers particularly new or deep insight into the nature of adversarial robustness, beyond what has been explored by prior work. These issues coupled with the quality of the writing and composition make me inclined to reject, although I am open to changing my score."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Interesting application of diffusion models to investigate the adversarial robustness of a neural network for certain examples via a notion of perceived curvature\n- Comprehensive visualizations, plots to demonstrate the relationship between per-sample-robustness, sensitivity, and margin"
            },
            "weaknesses": {
                "value": "**Contribution / significance**\n\nThe basic observation made by the paper regarding the relationship between robustness, sensitivity, and sample importance during training is interesting, but well-known. To strengthen the contribution and significance of the work, the authors should clarify the contribution of their analysis in the context of the previous work, or demonstrate some actionable insights- e.g. an algorithm that exhibits superior adversarial robustness relative to existing techniques.\n\n**Missing relevant work**\n\nThere is some missing existing work that should be cited that explores the emphasis of certain vulnerable examples in the training set to enhance clean and robust test-set performance. E.g. reweighting methods such as [1, 2], subsampling methods such as [3],  and others that I do not list (e.g. on applications of diffusion models to the adverarial robustness context). \n\n[1] Zhang et al., Geometry-aware Instance-reweighted Adversarial Training, ICLR 2021\n\n[3] Wang et al., Probabilistic Margins for Instance Reweighting in Adversarial Training, NeurIPS 2021\n\n[3] Zhang et al., Attacks Which Do Not Kill Training Make Adversarial Learning Stronger, ICML 2020\n\n**Writing and composition**\n\nThe writing could use some work. Several seemingly important statements are made, but I found it difficult to parse the english. For example, the following are examples:\n\n_However, we argue that the labels evoke the impression of disconnectedness, which a model then tries to account for when remodelling the perceived decision boundaries during training._\n\n_Although the skewed label distribution of s (comp. Figure 5) should come as a disadvantage, one may, a priori, argue for the converse\u2026_\n\nI also could not understand the idea of figure 1. The preceding text states that they intend to illustrate some previous claim, but the previous claim seems to be about data sensitivity and curvature, while figure 1 details the adversarial robustness / robustness gap for models trained for different numbers of epochs with a certain overlap between the training and validation sets. The experiment seems very complicated and difficult to understand compared to the claim that \n\n_It appears beneficial to emphasise the least robust elements during training to help a model gain robustness in regions where it struggles the most._"
            },
            "questions": {
                "value": "- Could the authors clarify their contribution in the context of existing methods / analysis (e.g. by providing some explanation for the efficacy of existing methods to enhance robustness)?\n- One claim is that _diffusion model connects regions of non-robust data to more prominent semantic changes, which we take as the model accounting for a more significant perceived curvature._ Can this be made more precise?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3684/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699032920213,
        "cdate": 1699032920213,
        "tmdate": 1699636324903,
        "mdate": 1699636324903,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0gJxHKG9sl",
        "forum": "dA4EWchlbn",
        "replyto": "dA4EWchlbn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3684/Reviewer_yQLb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3684/Reviewer_yQLb"
        ],
        "content": {
            "summary": {
                "value": "The paper is on adversarial robustness and proposes that models are particularly vulnerable in regions where the data manifold would be perceived as highly curved by the model. Some theoretical developments are proposed to support that. Experiments are conducted to demonstrate that by oversampling data samples in curved areas and using them to generate new artificial samples for training would improve robustness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Tackle a significant question on understanding better the input space of deep models and the corresponding robustness to adversarial attacks.\n- Support claims through elaborated theoretical developments."
            },
            "weaknesses": {
                "value": "- The paper and overall presentation is very difficult to follow. Although the authors seem to know very well their topic, the communication is lacking and a reader not in that specific field gets lost quite quickly.\n- The notion of curvature on the manifold is really unclear to me and not very well explained in the paper. But it appears in the end we are looking at distance between samples, the notion of curvature is there to support theoretical developments that are not directly translated in practice.\n- The technical aspects of the experiments section are not very clear nor clearly explained. I guess that the reader should look at some of the referenced papers like Karras et al. (2022) and Wang et al. (2023), but still I would like to get more background and specific details to better understand what is done in the experiments. It is quite unclear to me that the details provided would make results reproducibility easy.\n- It is difficult to figure out what exactly the experimental results are providing as support to the conclusion. The differences in Table 2 between the results is very small, and as such not very convincing that the proposal is of any meaningful effects for improving robustness.\n- Overall, the experiments are not very well explained and presented and the results are very difficult to interpret. I have a hard time making sense of all this."
            },
            "questions": {
                "value": "Looking at equation 1, if we assume that $d(p_i,p_j)$ is an Euclidean distance and that $\\|y(p_i)-y(p_j)\\|$ is basically equal to zero or one when using $y(p)$ as one hot vector over the classes, it means that in practice, the proposal consists of looking 1/distance to the nearest sample from a different class from the current one. Is this correct? That\u2019s what was used for the experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns with this paper."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3684/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699241233038,
        "cdate": 1699241233038,
        "tmdate": 1699636324825,
        "mdate": 1699636324825,
        "license": "CC BY 4.0",
        "version": 2
    }
]