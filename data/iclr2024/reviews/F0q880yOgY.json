[
    {
        "id": "R2rJ0pLq3J",
        "forum": "F0q880yOgY",
        "replyto": "F0q880yOgY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7636/Reviewer_tsqs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7636/Reviewer_tsqs"
        ],
        "content": {
            "summary": {
                "value": "The manuscript introduces a novel benchmark, `TextGym`, which is based on the `OpenAI Gym` RL environments and can provide insightful assessment for the decision-making capabilities of language models.  The authors also compared various existing language agents based on `GPT-3`, and presented an `EXE` agent that strategically balances exploration and exploitation."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "In general, this is a very well-organized and well-written paper, with solid contributions.\n\n- The problem that the authors are investigating is a hot topic among LLM researchers, and people are in need of a unified benchmark to evaluate the decision-making capabilities of language models.\n- The benchmark is novel, and has the potential to generate impact.\n- The authors provided comprehensive and detailed empirical results."
            },
            "weaknesses": {
                "value": "- I would encourage the authors to avoid assertive and absolute claims, such as the first sentence \"large language models lack grounding in external environments\", and at the bottom of page 1, \"we adopt `OpenAI Gym`, the most classic and prevalent...\".  These arguments, though eye-catching, are debatable, and such debates are irrelevant to the central points that the authors are trying to make, so why not avoid them?\n-  In the final paragraph on page 2, \"after *sufficient* numerical experiments...\"  I suppose that it should be left for the readers to decide whether the experiments are sufficient.\n- Point 3) at the bottom of page 2 is confusing.  Which baseline do the language agents improve upon?  I suppose that the authors wanted to say that the performance can be improved *via* incorporating XXX, but it would be helpful to reorganize the sentence and clarify the point.\n- The Problem Formulation section is somewhat redundant, as none of the math symbols are reused anywhere in the main text."
            },
            "questions": {
                "value": "- What is the main difference between the `EXE` language agent and other language agents, besides that the learner module provides exploration-exploitation balancing strategies in addition to how to exploit?  The reason that I am asking this question is that, if the actor and critic modules are identical to the other agents, the authors don't have to reiterate the descriptions on page 7.  The saved space can be dedicated to explaining how the learner module provides guidance on exploration and exploitation.\n- It remains unclear to me (after reading the Appendix) how different levels of agents are implemented.  It would be useful to provide at least a brief description."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7636/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637335121,
        "cdate": 1698637335121,
        "tmdate": 1699636928217,
        "mdate": 1699636928217,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AmwkowBgZD",
        "forum": "F0q880yOgY",
        "replyto": "F0q880yOgY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7636/Reviewer_i4oJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7636/Reviewer_i4oJ"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors investigate whether LLMs can achieve the same level of performance as agents trained with RL in classical RL benchmarks. The authors consider various algorithms and settings for training Language agents in Gym environments. They introduce a pipeline for creating text-based versions of Gym environments, as well as 5 training scenarios. Finally, the authors propose their own algorithm, EXE, which improves on recent algorithms like Reflexion in solving their text-based versions of the Gym environments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper asks an interesting and timely question. They introduce challenges around training Language Agents and discuss various levels of domain knowledge that can be provided to the agents. The proposed algorithm, EXE, can solve more tasks than the baselines, and comes close to PPO performance on some tasks, which is nice."
            },
            "weaknesses": {
                "value": "While the idea is timely, the methodology of the paper is unfortunately not very clear. The algorithm the authors propose is not introduced formally, but in charts. This makes it challenging to understand the algorithm, or why it works. Furthermore, the five training scenarios are not described in enough detail either. As a result, it\u2019s not clear to me why L3 does the best out of the setups. It would also help if the authors motivate their algorithm better, as well as the reason why they expect it to work better than previous approaches.\n\nIt is also strange to me (if I understood this correctly) that the authors used GPT-4 to generate the python code that translates the Gym observations into text. This method seems error prone, and the authors do not demonstrate that this works, or why it\u2019s even necessary in the first place.\n\nLastly, the experiments are not presented with enough detail. For instance:\n* the number of seeds used to evaluate the agents is not reported.\n* The learning curves or the converged performance of the PPO/SAC agents are not reported in the main text, but the Appendix. \n* The threshold for saying that a task is solved is not defined in the main text either.\n\n\nMinor points:\n* The radar plot in figure 3B is very crowded.\n* Parts of the paper are not so easy to follow. Consider simplifying language."
            },
            "questions": {
                "value": "* If SAC can solve tasks that PPO cannot solve in your setup, why are the PPO scores reported for the majority of the tasks instead of the SAC scores? \n* I don\u2019t see how some of these environments are partially observable (for instance Acrobot  and Cartpole) - this depends on how the states are translated into text. If they are indeed partially observable, it would be good to clarify how in the text.\n* The authors say that Language agents are better in more intuitive and deterministic environments. Aren\u2019t environments like Acrobot and LunarLandar deterministic (after the random first state)? And Blackjack, which is stochastic, can be solved quite well by EXE according to the Radar plot? What do the authors mean when they say an environment is intuitive?\n\nWhile I like the idea of designing algorithms that make Language Agents better at solving RL tasks, I think the experiments and ideas of the paper need to be presented differently. The motivation for their algorithm is not clear, and the evaluation of the different agents does not seem very thorough. I therefore recommend that the paper is rejected as it currently is. I think the paper could be improved a lot by motivating the algorithm and a specific training scenario they recommend better. The evaluation of the standard RL algorithms like PPO and SAC could also be done more thoroughly. Finally, it would be great to understand why EXE works better than the Language Agents."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7636/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698664051731,
        "cdate": 1698664051731,
        "tmdate": 1699636928113,
        "mdate": 1699636928113,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "t7mZ99sTMW",
        "forum": "F0q880yOgY",
        "replyto": "F0q880yOgY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7636/Reviewer_9WGb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7636/Reviewer_9WGb"
        ],
        "content": {
            "summary": {
                "value": "The papers compares the performance of PPO and LLM-based AI agents on text-based version of Gym environments. The performance is evaluated by introducing different levels of domain knowledge into the LLM-based agent, and using a peculiar architecture (called EXE) for the LLM-based AI agent to interact with the environment."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Despite my generally negative judgement on the paper, it presents some positive features in terms of positioning and perspective. In particular, these are, in my opinion, the strengths of the paper:\n- The study of how LLM-based AI agents should be evaluated, and how their performance can be compared to other classes of agents, is becoming an important research problem worth investigating.\n- The principle of evaluating an LLM-based agent's performance when prompted with varying amounts of task-related information is sound and promising."
            },
            "weaknesses": {
                "value": "The paper presents many issues and limitations, both in terms of experimental protocols and rigor, and in terms of presentation. These are the main issues that I have identified:\n- First and most importantly, the level of rigor in the empirical evaluation is far from being satisfactory. Performance comparisons are executed using a single seed only and not showing any form of confidence or error bars anywhere in the plots. A comparison of this kind is simply meaningless from the empirical standpoint, regardless of the funding constraints one might have to comply to, and invalidates all the claims and supposed findings contained in the paper. The sequential decision-making scientific community has made significant progress on evaluation rigor in the last few years, and I encourage the author to look at recent work (e.g., https://arxiv.org/abs/2304.01315 , https://arxiv.org/abs/2108.13264) and comply to the standards proposed there. The presence of LLMs or product-oriented APIs does not change the importance of rigorous scientific evaluation.\n- Some details in the presentation are confusing or unnecessary. As a significant example, why should one highlight that GPT-4 generated the text version of the environment, if that one is not a feature that is being scientifically evaluated? As of now, it reads an attempt to yield to GPT-4 the responsibility concerning the correctness of the new environments that are being proposed. But this is totally irrelevant for the matter of the presentation of the scientific evaluation of a system: regardless of whether the code for evaluating the AI agents was generated by GPT-4 writing code, humans writing code, or copied from a random repository on the web, the only thing that matters is its correctness and alignment with the claims that the rest of the paper is trying to make.\n- I believe a meaningful comparison of pretrained LLM-based systems with RL algorithms trained from scratch might not be as straightforward as the paper is trying to imply. LLMs are pretrained on all the internet, and the neural networks trained with RL start from scratch. It is still interesting to compare the performance of these different approaches, but saying that one approach is more sample efficient than another one does not do justice about their different nature and tradeoffs."
            },
            "questions": {
                "value": "- How are the claims in the paper statistically significant with a single seed and no error bars?\n- Can you remove any mention to GPT-4 automatically generating environments?\n- Why did you use PPO for some environments and SAC for some others?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7636/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762271072,
        "cdate": 1698762271072,
        "tmdate": 1699636927982,
        "mdate": 1699636927982,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bEOZJljNoy",
        "forum": "F0q880yOgY",
        "replyto": "F0q880yOgY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7636/Reviewer_CAf4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7636/Reviewer_CAf4"
        ],
        "content": {
            "summary": {
                "value": "The authors create a TextGym benchmark conforming to the popular OpenAI Gym interface to evaluate LLM agents on some common RL environments, including CartPole, MountainCar variants, BlackJack and CliffWalking (see Fig. 3a for names of others). \n\nThe environments are converted into textual form to be able to be parsed by the language agents. This is done by using the official documentation of the environments and prompting GPT-4 with the info (the process is described in Fig. 5 in the Appendix). \n\nThe authors further design 5 scenarios with different amounts of prior knowledge that is provided to the language agents to be able to solve the environments - levels 1 through 5 with a higher number corresponding to more prior knowledge provided to the agents. Level 1 corresponding to zero prior knowledge, level 2 to providing non-optimal policies' trajectories to learn from (inspired by offline RL) - they use random policy rollouts for this scenario, level 3 to letting the agent interact with the environment for some time (inspired by RL) - they let the language agent interact with the environment for 5 episodes, level 4 to providing expert trajectories (inspired by imitation learning) - they use optimised RL agents from the Tianshou library for this scenario, level 5 to be provided expert human guidance in natural language to help the agent solve the environment.\n\nThey also come up with a unified architecture for such language agents with 3 components: actor, critic and learner (inspired by RL). They design a novel agent (the explore-exploit-guided language (EXE) agent) conforming to this architecture, with the learner and critic having long term memories of episodic transitions and the actor having a short term memory of the current episode's transitions. The learner guides the actor to explore or exploit based on criticism from the critic.\n\nExperimentally, various language agents (Table 1) are benchmarked on their TextGym environments with performances normalized between -1 and 1. They show that the language agents can solve 5/8 environments and provide some inisghts into why and what agents and what scenarios performed better in the end."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea behind the benchmark and the various scenarios and unified architecture are well developed. The process of converting RL environments to a text interface and evaluating language agents on them is likely to lead to useful inisghts. I also find the different scenarios that have been designed intuitive. The paper is generally well presented  in terms of motivation and the problem tackled seems timely given the advent of LLMs."
            },
            "weaknesses": {
                "value": "I would say that lack of many seeds for a more thorough evaluation are the main weakness. The paper mentions in Appendix B.3 that only one seed is used for evaluation (except 100 for BlackJack). That feels too little given the kinds of environments in there do not seem expensive to me. Additionally, a few more environments could be evaluated given what seems to me to be not so compute-heavy experiments. Finally, the analyses in sections 5.2 and 5.3 seemed to lack in detail.\n\nMinor:\nOpenAI Gym is not an environment, it is library that contains an API for enviroments."
            },
            "questions": {
                "value": "Could the authors elaborate on the compute costs?\n\nGiven the fact that a lot of training is not needed for these experiments (as I understand it the authors use existing trained models for the LLM part and only train the RL agents with default hyperparameters in Tianshou, so no hyperparameter tuning is needed), I believe more seeds could be run and a wider range of environments could be included, a couple of Mujoco or Atari environments, for example. Running only one seed could also lead to noisier insights and make it harder to analyse the experiments. Could the authors clarify?\n\nWhile I feel the paper is generally well written, the insights and analyses in sections 5.2 and 5.3 seem a bit high-level to me. For example in Fig. 3b: L3 agents actually did better than L5 and L4 agents, L4 was actually very bad even though it is supposed to be the second easiest scenario. L2 and L1 are sometimes better than L3 and L5 agents even though these are harder.  The reasons for such unintuitive results could be discussed in much more detail. The reason for lack of depth of analyses could be that a substantial portion of the paper is dedicated to presenting the benchmark and even Fig. 5 and the following code template in listing-1 are in Appendix even though in my opinion these are key to understanding the work. This makes me feel that maybe a journal would lend itself better to detailed presentations for a promising benchmark such as this. Another reason could be the lack of running many seeds which as mentioned above leads to noisier insights and analyses. Another example of a superfluous statement for me is the first key finding at the end of section 5.3. It seems to be a claim without proper evidence that EXE's exploration and exploitation capabilities helped it perform better. Could the authors say more on this?\n\nIn the problem formulation section 2.1, it is unclear to me what the index i is for?\n\nThe first case study in Appendix C was already hard for me to understand, so I did not try reading further here. I am not sure what agent was used here. Also, why was a part of the prompt hallucinated (\"cliffs(Transversal interval from (3, 1) to (3, 10)\")? It was provided by the authors I suppose and could not be hallucianted.\n\nCould you provide a Code / Jupyter example for the work?\n\nWhy not show SAC in Fig. 3a for Taxi and MountainCar continuous environments as default PPO cannot solve them?\n\nMaybe a table showing how many environments were solved by each agent in each scenario would be useful?\n\nCould you also please explain \"Specifically, we allocate 1 hour of effort to develop scenarios for a single agent in each environment, randomizing the sequence of language agents.\" in more detail?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7636/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7636/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7636/Reviewer_CAf4"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7636/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785113055,
        "cdate": 1698785113055,
        "tmdate": 1699636927873,
        "mdate": 1699636927873,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Cg7bkdwISH",
        "forum": "F0q880yOgY",
        "replyto": "F0q880yOgY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7636/Reviewer_DgbS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7636/Reviewer_DgbS"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a text interface that uses GPT-4 to translate eight OpenAI-Gym environments into text-based games, then tests a particular LLM, gpt-3.5-turbo0301, on the eight text-based games. Besides necessary verbal description of the current observation in the game, the LLM is also prompted with verbal suggestions/tips for game playing. The suggestion can be either hand-crafted by the authors (called Level-5 scenario in the paper), or from a text-based in-context learning method presented in the paper. In this method, we collect a few episodes of playing experience as input, then instruct a LLM to summarize and evaluate the playing experience as well as the decision policy behind it, and then instruct the LLM again to turn the summary/evaluation into a verbal suggestion for better game playing. The game-playing data used in this method can further be either from a random policy (called Level-2 scenario in the paper), or from self-playing in the RL manner (called Level-3 scenario), or from an expert policy obtained by PPO/SAC (called Level-4 scenario). \n\nThe paper calls the above method, EXE, if the LLM is instructed to make suggestion that encourages some exploration behavior in the game. The paper reports that, without the EXE trick, the GPT3.5-based language agents can barely solve any of the text-based games except for Blackjack-v1. With the EXE trick, the language agent achieves reasonable performance on 5 out of 8 environments under the Level-3 scenario, i.e. when the data for suggestion learning is collected in RL manner. Other suggestion-learning scenarios (L2,4,5) does not seem to work even with the EXE trick."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "LLMs exhibit potential in general problem solving, as well as in some new learning forms such as learning from instruction and few-shot in-context learning. Utilizing LLMs in various problem domains beyond NLP is an interesting and hot topic in general. This paper reports some informative results to this end. It is especially intriguing to see that current LLMs can learn more effectively from experience data of its own, compared with learning from experience data or verbal instructions of better decision agents (corresponding to the superiority of L3 agents over L4 and L5 agents in Figure 4a)."
            },
            "weaknesses": {
                "value": "**(a)** I think the general perspective of the paper is confusing. This is reflected in the paper title already: The authors ask if language agents is competitive to RL, but language-based agent and RL-based agent are not competing methods at all. In fact, the Lv3 language agent discussed in this paper -- which seem to be the only working language agents, according Fig. 4a -- is exactly a reinforcement learning procedure that improves decision policy from evaluative feedbacks collected from environmental interaction. You can't beat \"RL\" with a RL agent.\n\nMore importantly, as \"an empirical study on OpenAI-Gym\", the experiment includes no Mujoco or Atari task at all, which is a notable drawback as many representative tasks of OpenAI-Gym are in these two categories. Even for the chosen task categories, the tasks are still selected, with 4 out 12 missing in the paper. I am afraid the simple tasks considered in this paper cannot represent \"the performance of RL\" which is the target of the paper. \n\n**(b)** Even if we re-position this paper as just a comparison between two specific methods (EXE vs PPO) on some specific simple tasks as a preliminary study, the current experiment setup may be still a bit biased. For example, I suspect the model size in PPO is orders-of-magnitude smaller than the GPT-3.5 in the language agents. Task-specific heuristics is provided (only) to the language agents. The chosen PPO implementation seems to have weird result on some tasks such as MountainCarContinuous. Results of some tasks in the same category are missing. Finally, when it comes to sample complexity, we should keep in mind that the LLMs are pretrained on huge data, thus it's not really a \"5 vs 20K episodes\" game; perhaps the paper can compare also with meta-RL agents such as Gato (Reed et al. 2022). See my Question 1~4 below for the detailed concerns.\n\n**(c)** In terms of novelty, the paper may need to better contrast with prior works, especially with Reflexion (Shinn et al. 2023) which is similar to the proposed EXE method. Current appendix A.1 is not enough in this regard.\n\n**(d)** The current presentation leaves many things unclear. See my Question 5~10 below for the detailed concerns here. Experimentation code is not released, which is not ideal as an \"empirical study\" that proposes a new \"benchmark\"."
            },
            "questions": {
                "value": "1. In your experiment, what's the architecture of the policy model at the PPO side? How large is the model for PPO? \n\n2. Did you try not prompting the LLM with the game description info and what's the performance? Although game description is indeed readily available for the particular tasks you considered, as you argued in Remark 1, the description is not necessarily available for other tasks in general. In the end, our goal is not to solve the particular tasks in Gym. We only use them as a benchmark to find method that hopefully works in the general case. If the game description is domain-specific knowledge crafted by human, prompting LLM with such task-specific descriptions is, in its essence, not much different from equipping LLM with gaming expertise from human. It thus feels a bit unfair to compare LLMs equipped with human-generated game description against RL agents that are autonomously learning fully on its own.\n\n3. Why didn't you use SAC as the RL baseline for all the eight environments? Also, have you tried other PPO implementations on MoutainCarContinueous-v0?\n\n4. Have you tried Pendulum (classic control), Frozen Lack (toy text), Bipedal Walker (Box 2D), and Car Racing (Box 2D), which are tasks in the same categories with the ones the paper studies? Since the TextGym code is generated automatically by GPT-4, and your experiment does not involve massive training, I guess encompassing these tasks should not be a huge burden?\n\n5. How does the TexGym interface translate the termination condition? It seems the example code in Appendix B.1 does not process the termination information at all.\n\n6. Is the actor-critic-learner framework in Section 3.3 used in all the 5 scenarios in Section 3.2? If so, how do the critic and learner work in L1 (no guidance) and L5 (expert guidance) scenarios which seem to involve no learning at all?\n\n7. In L2, L3, and L4 scenarios, will the actor receives the 5-episode data as part of its prompt for action selection (in that case the actor will repeatedly receive the data in every gaming step even at testing time), or the 5-episode data is only used by the critic and the actor never sees the 5-episode data but only receives the decision suggestion extracted from that data?\n\n8. What's the default learner? Appendix B.2 only gives the default critic.\n\n9. In the first paragraph of Page 8, how are the scores between (0,1) mapped to the raw performance? A linear mapping? For reproducibility concern, please give the raw performance scores of the solvability threshold and sota thresholds used in your experiment.\n\n10. Figure 3b and last paragraph of Page 8: from the picture it is not evident at all that \"L3-Agents outperform their counterparts\". All colored regions are stacked together and it's hard to tell. Please give data table for the exact performance scores. \n\n11. Any explanation for why L4 result is worse than L3? Intuitively, the training data in L4 agents should be of higher quality (in terms of performance score) than those used in L3 agents. If the LLM can summarize useful principles from the latter, it should be able to do that in the former case too, intuitively.\n\n12. In the paragraph below Figure 4, you said \"for L1, L2, and L4 scenarios, EXE also outperforms other [language agents]\". But to my current understanding, in these three scenarios the agent has no chance to explore the environment at all, isn't it? These three scenarios are pure offline scenarios, while the exploration-vs-exploitation trade-off becomes relevant mostly in online RL setting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7636/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699441878465,
        "cdate": 1699441878465,
        "tmdate": 1699636927774,
        "mdate": 1699636927774,
        "license": "CC BY 4.0",
        "version": 2
    }
]