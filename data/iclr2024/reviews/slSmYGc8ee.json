[
    {
        "id": "e6LkQIH361",
        "forum": "slSmYGc8ee",
        "replyto": "slSmYGc8ee",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission542/Reviewer_rK41"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission542/Reviewer_rK41"
        ],
        "content": {
            "summary": {
                "value": "This paper probes the effect of weight initialization rank on feature learning in recurrent neural networks. The authors study in particular how the alignment between a particular low-rank initialization and the task affect how much the kernel moves during training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "In the large, I think this paper is timely, and the core idea is (to the best of my knowledge) novel and interesting."
            },
            "weaknesses": {
                "value": "1. Upon reaching the conclusion of the paper, I found myself confused as to why the authors did not perform any experiments with feedforward networks. Though I appreciate the neuroscience-inspired focus on recurrent networks, including some tests in the feedforward setting would be valuable. In particular, this would show more directly how departures from the idealized setting of their theoretical results (linear networks with whitened data) affect the phenomenology. Demonstrating that these ideas are applicable to deep feedforward networks  and more standard machine learning tasks would in my view substantially improve the impact of the paper by tying it more closely to the main body of theoretical work on feature learning. \n\n2. The metrics introduced in Section 2.2 are restricted to measuring changes in weights or kernels over the course of training. Particularly given the fact that the stated goal is to investigate how task structure affects feature learning, it would be useful to also track a measure of task-kernel alignment, for instance the kernel-target alignment $y^{\\top} K y / |y|^2 tr K$ or the task norm $y^{\\top} K^{-1} y$ throughout training. \n\n3. The criteria used for task selection are unclear. The authors choose three of the original tasks from the neurogym environment (I note that this task suite has been critiqued by Khona et al., \"Winning the Lottery With Neural Connectivity Constraints: Faster Learning Across Cognitive Tasks With Spatially Constrained Sparse RNNs\") along with sequential MNIST. Why are these four tasks relevant? The paper could be made more compelling either by using a more diverse suite of tasks or by justifying why these four are appropriate. \n\n4. On the whole, I found the Discussion to be a weak point of the manuscript. Section 4.1 does not answer the vital question of precisely what experimentally-measurable biological phenomena the present work can explain, contextualize, or predict. As it stands, this section is in some sense a restatement of Zador's ideas around the importance of task-aligned initialization in the language of lazy learning. Moreover, the discussion of implications for deep learning promised by the section title seems to be missing. Section 4.2 contains far too many ideas to be jammed into its single paragraph; for clarity the authors should split it into at least two paragraphs. Finally, the relevance to neuroscience of neural collapse and of Tishby's proposals regarding the information bottleneck needs to be justified. The sentence where they are mentioned is much too long as it is, and these are both somewhat subtle and controversial topics."
            },
            "questions": {
                "value": "1. In the block of citations in the second sentence of the introduction, the work of Canatar, Bordelon, and Pehlevan (2021) should be cited before Xie et al 2022, as the latter is based on the results developed in the former work. \n\n2. Is \"tabula rasa\" the appropriate way to describe Gaussian or Uniform random initialization with non-vanishing variance? \n\n3. In-text citations to the work of the Allen Institute and the MICrONS Consortium are not formatted correctly. \n\n4. Why do the eigenvalues in Figure 2 appear not to be sorted by magnitude? Also, why is $\\frac{\\sum\\_{i} |\\lambda|\\_{i}}{|\\lambda\\_{1} N}$ the relevant notion of effective rank in this setting?\n\n5. Figures 2 and 3 each occupy more than half of a page, but both contain a significant amount of whitespace. Is it possible to combine them side-by-side into a single figure?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission542/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission542/Reviewer_rK41",
                    "ICLR.cc/2024/Conference/Submission542/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698087233918,
        "cdate": 1698087233918,
        "tmdate": 1700165552068,
        "mdate": 1700165552068,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7URtMiR3F8",
        "forum": "slSmYGc8ee",
        "replyto": "slSmYGc8ee",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission542/Reviewer_SwWy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission542/Reviewer_SwWy"
        ],
        "content": {
            "summary": {
                "value": "The work investigates how the initial weight structure, especially its (effective) rank, influences network learning dynamics, and in particular whether the network learns in the \"lazy\" (small change in tangent kernel) or \"rich\" (substantial evolution of tangent kernel) regimes. The paper is written using neuroscience as a motivation, citing the fact that connectivity in the brain is substantially more structured and lower-rank than standard initializations used in ML, and thus existing work on lazy/rich regimes may not apply directly to biological learning. The authors find that low-rank initializations typically lead to richer learning, unless the low-rank initialization happens to align with the structure of the task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is clearly written and technically sound.  The experiments provide solid evidence for the main takeaway of the paper (that lower-rank initializations generally lead to richer learning dynamics), using a variety of tasks and complementary measures of laziness/richness.  Figure 4 provides helpful intuition for understanding this conclusion, suggesting that low-rank initializations tend to incentivize richer learning because there is a greater need for it when the model's kernel at initialization has low alignment with the task structure."
            },
            "weaknesses": {
                "value": "I have two main concerns about this paper:\n\n1. Given that the proof of Theorem 1 is left to an Appendix, the paper would benefit from providing more explanation / intuition for the main result.  I can imagine an intuition that the network \"needs to\" adjust its representation more to address the misalignment.  But a more thorough presentation of the key proof steps in the main text, and/or illustrative examples to provide intuition for the learning dynamics, would be very helpful.\n\n2. It is not clear to me how the findings of this paper will be useful to either the ML or neuroscience communities.  I do not mean to claim that they aren't useful, but rather that the paper does not present a clear case for why they are.  From an ML standpoint, low-rank initializations are uncommon.  From a neuroscience standpoint, the applicability of this theoretical framework seems uncertain given that the learning algorithms used in the brain are not well characterized and may differ substantially from SGD.  Moreover, it is not clear to me what we gain by knowing that low-rank neural connectivity gives rise to richer learning than a counterfactual in which brains had higher-rank connectivity.  Is the idea to compare between brain regions with connectivities of different ranks to make predictions about different representation learning dynamics?  Or across species?  More concrete and precise proposals for how these findings could be used would be helpful.\n\n3. The richness/laziness of learning dynamics by itself tells us very little about the representations a network learns.  If the goal is to understand the impact of weight initialization on representation learning, the paper would benefit from more direct consideration of this question.  For instance, see the question below."
            },
            "questions": {
                "value": "Do the kernels associated with low-rank-initialized networks grow more aligned with the target function than those of high-rank-initialized networks?  Or do they merely grow \"as aligned\" as the lazier networks with high-rank initializations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission542/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission542/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission542/Reviewer_SwWy"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698714643628,
        "cdate": 1698714643628,
        "tmdate": 1700463815798,
        "mdate": 1700463815798,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ftRsMUHauh",
        "forum": "slSmYGc8ee",
        "replyto": "slSmYGc8ee",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission542/Reviewer_Cn5Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission542/Reviewer_Cn5Y"
        ],
        "content": {
            "summary": {
                "value": "This study examines how the initial connectivity of a neural network affects its learning regime (lazy vs rich). In particular. the authors focus on the rank of the connectivity, in contrast to e.g. weight variance studied in previous work. The main motivation for this is the claim that biological networks have low-rank connectivity. Both in theory using feedforward linear networks, and in experiments using recurrent non-linear networks, it is shown that low-rank initialisation lead to richer learning than full-rank initialisation (except, if the low-rank initialisation aligns with the task). This is intuitive as high rank initialisation means it is likely that some (linear combination of the) columns of weight matrix are already aligned with the task matrix, whereas a low-rank matrix will most likely be orthogonal with the task at initialisation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The question of how connectivity influences representation and learning dynamics is of significance, both for the neuroscientists, as well as for machine learning scientists. How low-rank initialisation affects the learning regime has as far as I am aware not been studied in depth, yet networks with low-rank weight matrices are used by both the deep-learning and computational neuroscience communities.\n\nThe paper is well written and the main idea is clearly presented. Multiple experiments are performed that highlight the main result and the supplementary contains a large amounts of controls. The experimental results are supported by theoretical results on linear feedforward networks.\n\nOverall I lean on accepting this paper, and I would increase my score if the concerns below are addressed"
            },
            "weaknesses": {
                "value": "1. While many controls are done, the initial dynamic regime of the network (stable, unstable or chaotic) is not controlled for (see questions below).\n2. I am not entirely convinced by the evidence given for the statement that the brain has low-rank connectivity. The paper repeatedly cites Song et al., 2005 and Mastrogiuseppe & Ostojic, 2018 as evidence for low-rank connectivity in the brain. The latter is a purely theoretical study, which I am not sure why it used as evidence for biology. The former indeed studies biological neural circuits, and found that the distribution of synaptic connection strength can be fitted by a lognormal distribution, as well as the circuits having overrepresented bidirectional connectives. While my intuition is that this implies low-rank connectivity, it can (and likely should) be made clear if this is the case. In general the relation between local connectivity patterns and (global) rank is not always obvious a priori (potentially, see Shao & Ostojic 2023).\n3. Biological constraints are only used as initialisation and not enforced during training. This decreases the relation of the simulations to biology, as neurons in the brain can (generally) not just decide to forget about e.g. Dale\u2019s law during learning.\n4. The authors claim to study continuous-time networks, however, $dt$ is set equal to $\\tau_m$, and tasks have around 10 time steps after discretisation. It could be questioned how well the simulations reflect the continuous-time equations.\n\nMinor:\n1. Equation 10, last line should say d instead of 3 above the equality sign.\n2. Figure 3B left, title is cut off.\n3. The 5th references seems formatted wrong.\n4. Equation 1. -> It could be named explicit that you are making an exponential Euler approximation (which is not used in the studies cited here)."
            },
            "questions": {
                "value": "The questions are all related to the initial dynamic regime versus initial connectivity \n\n1.  I would like to the authors to comment on whether the difference in lazy versus rich learning is potentially confounded by the regime of the initial dynamics (stable, unstable or chaotic), instead of the rank of the initial matrix. Three results that lead me to hypothesise there is at least some influence of dynamic regime on the learning:\n\n- For Figure 1, the large full rank networks will likely be initially be chaotic, given a gain of 1.5 (transition at $\\sqrt(2)$ for ReLU networks; Kadmon & Somplinsky 2015). However at least part of the truncated networks (despite rescaling by the norm) will likely be not.\n- The outlier eigenvalue in the networks of Figure 2 might lead to (unstable) dynamics initially exploding in the direction of the corresponding eigenvector, instead of following a chaotic regime. \n- Finally, in Figure 9, the differences between low-rank and full-rank networks diminishes when both are initialised in the stable regime.\n\n2. Were the Dale\u2019s law networks balanced? E.g. increasing inhibitory strength if less then 50% of neurons are inhibitory, so that the expected input to neurons stays zero, would put the Dale\u2019s law networks in similar regimes as the controls (see Rajan & Abbott, 2006). \n\n3. The Eigenspectrum, and as a result the initial dynamics, of the full-rank random networks are most likely approximately constant between networks, whereas the (non-zero part of the) eigenspectrum of the low-rank networks will vary more over initialisations - is this related to the observed high variance in richer learning between the low-rank models?\n\nNote that getting to same dynamics could be partly ameliorated by scaling by the largest singular value instead of the norm (which are realistically only approximately proportional for the large full rank networks). However this also is not a guarantee, as e.g. as stated above balancing also has an influence on dynamics"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission542/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission542/Reviewer_Cn5Y",
                    "ICLR.cc/2024/Conference/Submission542/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698747406179,
        "cdate": 1698747406179,
        "tmdate": 1700211875661,
        "mdate": 1700211875661,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pLId3si0bY",
        "forum": "slSmYGc8ee",
        "replyto": "slSmYGc8ee",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission542/Reviewer_RNHT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission542/Reviewer_RNHT"
        ],
        "content": {
            "summary": {
                "value": "This paper studies how structure in the weight matrices -- focusing on the recurrent weight matrices in RNNs, and feedforward matrix in two layer linear feedforward networks -- shapes rich and lazy learning (roughly whether the weights change a lot or negligibly through learning respectively). The paper finds that high-rank initializations lead to so-called \"lazier\" learning, whereas lower rank initializations tend to lead to \"richer\" learning, or larger changes in the weights. If the low-rank intialization is already suitable for the task, there can be negligible changes to the weights as well."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The results (analysis in linear feedforward networks, and experiments in RNNs) appear technically correct, and are in line with existing literature on how weights change during learning. Note that I skimmed through the Appendix and did not closely check the proofs."
            },
            "weaknesses": {
                "value": "I felt like the paper could provide a more finer-grained description and insight into how the initialization scheme affects the final solution. This could be by providing insight into how the existing metrics (from Sect 2.3) were changing during learning as a function of training epoch (rather than just applied on the final solution, esp. when in the rich regime) and/or through the use of additional finer-grained metrics. As it stands, it feels like there could be different explanations for how the weights are evolving during learning (that I think are interesting and important); for example is the largest rank component changing first or changing in magnitude the most during learning? Does this happen even if the task is low-rank but the (low rank) initialization is not perfectly aligned with the task?\n\nI think this could be addressed both in experiments as well as in the linear setting where the authors make analytical statements. For example, Prop 1 appears considers a low-rank 1 initialization completely aligned with the task (initialized with $\\beta$) -- but I think it would be helpful to understand in general how does a low rank initialization compare with a high rank initialization for learning some fixed (low rank) task, rather than an expectation over all tasks? In other words in general would a low-rank initialization be better for a low rank task and why/why not? also how are weights/singular components evolving to facilitate the learning? \n\nAs an aside, the paper could be made stronger if the case that the theory studied was closer to the experiments (though I understand that it is easier to analyze in the linear FF setting and that the theory still provided insight for the RNN experiments). \n\nThe presentation and writing was generally clear but can be improved. While reading the paper, it felt like, at times, the writing was too verbose and could have been made clearer. For example, \"A nexus between deep learning and neuroscience has expanded the applications of deep learning theoretical frameworks to study the learning dynamics of biological neural network\", and \" Our work builds on these studies by further exploring the precursors of these regime\" could be more clear.  Minor: Fig 3B, left title not visible"
            },
            "questions": {
                "value": "- For the proof in the Appendix of Thm 1, while do you need the scale $\\sigma = \\||W_i(0)||_F$ to be small? Also small relative to what?\n- Is there evidence that the initial connectivity structures in biology are low-rank?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission542/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission542/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission542/Reviewer_RNHT"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698965666698,
        "cdate": 1698965666698,
        "tmdate": 1699635981311,
        "mdate": 1699635981311,
        "license": "CC BY 4.0",
        "version": 2
    }
]