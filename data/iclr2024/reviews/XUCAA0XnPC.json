[
    {
        "id": "RIMBfT2JPv",
        "forum": "XUCAA0XnPC",
        "replyto": "XUCAA0XnPC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1393/Reviewer_H7PD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1393/Reviewer_H7PD"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce Ensembler, an extensible framework designed to substantially increase the difficulty of conducting model inversion attacks for adversarial parties. Ensembler leverages model ensembling on the adversarial server, running in parallel with existing approaches that introduce perturbations to sensitive data during colloborative inference. Their experiments demonstrate that Ensembler can effectively shield images from reconstruction attacks, achieving recognition levels that fall below human performance in some strict settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Propose Ensembler, an extensible framework designed to substantially increase the difficulty of conducting model inversion attacks for adversarial parties\n- Comprehensive evaluation on Ensembler"
            },
            "weaknesses": {
                "value": "- The threat model is not compelling\n\nThe threat model in this paper is not compelling. The authors claim that \"it has auxiliary information on the architecture of the entire DNN, as well as a dataset in the same distribution as the private training dataset used to train the DNN\". In real world, it is unclear how to obtain the information for the attackers. Also, the authors assumed the server has reasonably large computation resources but no detailed discussion or analysis on it. Thus, it is unknown how practical it is for the threat model. Without valid threat model, it is unclear the usefulness of this paper.\n\n- Lack of practicality\n\nThe practicality of implementing \"Ensembler\" in a real-world scenario is somewhat unclear, given the inherent complexity and computational overhead of maintaining multiple models and the distillation process. Also, this paper mainly target on small-scale image dataset such as CIFAR-10. It is unclear whether their findings or method can be generalized to large-scale settings. Thus, the practicality is damaged."
            },
            "questions": {
                "value": "Need to improve or provide justification on the threat model and practicality."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1393/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698616274833,
        "cdate": 1698616274833,
        "tmdate": 1699636066977,
        "mdate": 1699636066977,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7FsZsS1een",
        "forum": "XUCAA0XnPC",
        "replyto": "XUCAA0XnPC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1393/Reviewer_oZxB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1393/Reviewer_oZxB"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework an ensembling-based approach to defend against reconstruction attacks during collaborative inference. Ensembler (the proposed technique) uses model ensembling on the server and a secret selector (activations to compute weighted sums of model activations) on the client to prevent the server from reconstructing the raw input from the intermediate output. The paper also analyzes the effects of different model split strategies on the reconstruction difficulty and quality."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The analysis around choice of split point, and how it impacts reconstruction, is insightful.\n- The regularization in Eq 3. is well motivated, and the choice of simple noise addition (instead of some over-engineered, overly complex method) is nice."
            },
            "weaknesses": {
                "value": "My biggest issue is contrived threat model in this work. The server is assumed to be potentially malicious (which is reasonable) but at the same time, the proposed defense requires the server to train multiple models. If the server is indeed malicious, why would it want to do so? Even if it does, how can the client know if the server is malicious in its part of the models as well. For instance, it could shuffle or modify the outputs of its server nets such that only some of them are useful for the client model.\n\nOn that note, if the server is indeed malicious, a very simple attack can be: server sends N queries to the client posing as a user, observes intermediate outputs of client's model (sent to it normally), and then also observes the final output posing as the user. This can make reconstruction attacks (perhaps via shadow models) much more potent. \n\nAdditionally, the client is supposed to be computationally restricted, which is why a server is involved in the first place. How, then, is the client able to train the entire model locally (Section 4.2)? If the client is indeed powerful enough to train models (which requires at least as much memory as inference), then why not just run the model locally without any server? \n\n## Minor comments\n\n- Section 1, Pg2 \"...adds privacy to the DL pipeline\". What you really mean to say is that \"current attacks are harder to run successfully on proposed technique\", which is not the same as claiming increased privacy. \n\n- Section 2.3 is missing the threat model of property/distribution inference. There are some works specifically for collaborative learning as well [1]. See [2] for a summary of this (and other) privacy risks in ML.\n\n- Section 3.1, Pg5 \"...our experiments align with the...\" please avoid jumping to results before explaining the experimental setup.\n\n- Section 3.2 does not seem to add any value- please consider removing it, and instead using that space to better explore results towards the end of the paper.\n\n- Section 4.1 \"As illustrated in Fig. 4...\" please keep referenced figures on the same page as the first reference (ideally close to the reference, but at least same page).\n\n- Section 4.1 \"...the client secretly computes\" - this is just normal computation on the client side, and there is nothing explicitly \"secret\" about it apart from the fact that the server does not see these computations, which is true by design irrespective of any techniques proposed in this work.\n\n- Equation 1: $S_i$ is an activation, but what is the input to the activation? Please clarify.\n\n- Section 5.3 is all I see for analysis of results. for a 9 page paper, there should definitely be much more space reserved for analyzing results.\n\n### References\n- [1] Zhang, Wanrong, Shruti Tople, and Olga Ohrimenko. \"Leakage of dataset properties in Multi-Party machine learning.\" USENIX, 2021\n- [2] Salem, Ahmed, et al. \"SoK: Let the privacy games begin! A unified treatment of data inference privacy in machine learning.\" IEEE S&P, 2023"
            },
            "questions": {
                "value": "- Section 2.2 \"...where the client holds the first and last few layers\" - are such splits standard practice? If yes, cite references clearly to works that have similar setups. If not, make that clear and justify why this split design makes sense.\n\n- Section 2,2 \"...a good estimate of the DNN used for inference...\" - why is it just a good estimate? Isn't the server the party that usually trains the entire network to begin with?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1393/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761599377,
        "cdate": 1698761599377,
        "tmdate": 1699636066905,
        "mdate": 1699636066905,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8oid0WUJ6h",
        "forum": "XUCAA0XnPC",
        "replyto": "XUCAA0XnPC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1393/Reviewer_mcWL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1393/Reviewer_mcWL"
        ],
        "content": {
            "summary": {
                "value": "This work wants to propose a defense against model inversion attacks during collaborative inferences. \n\nFor collaborative inferences, a part of the forward passes (typically corresponding to the layers in the middle) is computed by the cloud server, and model inversion attacks in the context of collaborative inferences consider adversarial servers who attempt to reconstruct the original inputs.\n\nThey first empirically evaluate how the model splitting strategy (i.e. how many first&last layers are kept locally without sharing to the cloud server) affect the effectiveness of model inversion attacks. They evaluate this on VGG16 and ResNet-18 for image data (CIFAR-10), and they evaluate on an English to German translator for text data. They confirm previous results that keeping more first layers locally in the client side (i.e. not sharing them to the adversarial server) reduce the effectiveness of model inversion attack.\n\nSecondly, they want to propose a defense against model inversion during collaborative inference that does not increase the local computation at inference time. They do so by training N different models, keeping their middle layers and then training a new pair of first layers&last layers with a regularization term encouraging it to utilize P of these middle layers. Here N and P are both pre-defined hyper-parameters. They evaluate this defense on CIFAR-10."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The topic is meaningful. Collaborative inference is a practical case and it is important to understand how big the threat from model inversion attacks can be and if one can mitigate the threats.\n2. The idea of their defense looks novel."
            },
            "weaknesses": {
                "value": "1. **The experiments are really insufficient** in a sense that the evaluated scenario is limited (e.g. only evaluating on CIFAR-10 with ResNet-18)!\nIn section 3, when studying how model splitting strategies affect the effectiveness of model inversion attacks, only a single qualitative result is shown in each case (a total of 3 samples, one for each of VGG16, ResNet18 and English to German translator); In section 5, the proposed Ensembler defense is evaluated only on CIFAR-10 with ResNet-18 architecture.\n\n2. **The proposed defense is neither well-explained nor well-investigated.** \nThe motivation behind the proposed design is not clear: Why/How would this increase the difficulty of model inversion attacks?\n(1) Intuitively, each of the N group of middle layers obtained by the server are intentionally biased during training (through adding a fixed perturbation to its input). Since it is assumed that the attacker has access to a dataset with similar distribution, is it possible that such bias can be reversed by adding another learnable perturbation to the input of these middle layers? (2) Since the actual model uses only P out of N models, could the attacker also learn to select a subset of models that maximize the reconstruction quality?\n\n**It is important to have an adaptive attack** by incorporating both (1) and (2)  in evaluating the proposed defense (unless one formally proves its effectiveness). Based on current supports from the paper, the proposed defense offers only complexity but not security.\n\n\n3. **Missing discussion regarding the cost of the proposed defense.**\nAnother concern regarding the proposed defenses is the added overhead, both to the training phase and to the inference, but this is not well discussed in the paper."
            },
            "questions": {
                "value": "Please also refer to the Weakness part for my concerns regarding this paper (I would say priority-wise: Weakness 2 > Weakness 1 ~ Weakness 3, but they are all important issues). \n\nSpecifically:\n1. For Weakness 1, I would suggest authors to include quantitative results for section 3 and to include evaluations on another dataset with higher resolution (e.g. some subsets of ImageNet or even tiny-ImageNet would be much more convincing than only having results from CIFAR-10). It would also be better to have results on more architectures but having only CIFAR-10 results is a bigger issue.\n\n2. For Weakness 2, as I wrote above: (1) Intuitively, each of the N group of middle layers obtained by the server are intentionally biased during training (through adding a fixed perturbation to its input). Since it is assumed that the attacker has access to a dataset with similar distribution, is it possible that such bias can be reversed by adding another learnable perturbation to the input of these middle layers? (2) Since the actual model uses only P out of N models, could the attacker also learn to select a subset of models that maximize the reconstruction quality? **It is important to have an adaptive attack** by incorporating both (1) and (2)  in evaluating the proposed defense (unless one formally proves its effectiveness). \n\n3. For Weakness 3, please let me know if you have the discussion somewhere but I miss it. Otherwise, I would recommend to include the discussions, even if it reveals some limitations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1393/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809184485,
        "cdate": 1698809184485,
        "tmdate": 1699636066824,
        "mdate": 1699636066824,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ib9ihtFrKM",
        "forum": "XUCAA0XnPC",
        "replyto": "XUCAA0XnPC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1393/Reviewer_Z7Pt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1393/Reviewer_Z7Pt"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an ensembling model architecture for collaborative inference to defend against model inversion attacks. The idea is to employ N server-side networks instead of only one. The client will maintain a secret selector that will only choose a part of the server networks to make final predictions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is easy to read, with a comprehensive introduction of the background & an easy-to-understand presentation of the approaches."
            },
            "weaknesses": {
                "value": "1. The paper spends too many pages on background introduction. Only until page 6, the authors start to introduce their own approaches...   Also, the paper particularly mentions the privacy of NLP in Section 3.2. But this seems to be irrelevant to the paper... The evaluation is also performed on Cifar-10 with ResNet-18. This can confuse the audience a lot. \n\n2. The evaluation is weak. Only ResNet-18 and Cifar10 are considered. It is also unknown how the effectiveness of the proposed approaches varies with different choices of N and P. \n\n3. Since multiple networks are used in the approaches, the redundancy of computation and the efficiency cost would be a concern. The authors should carefully discuss it."
            },
            "questions": {
                "value": "I am also confused about the methodology. In my opinion, the idea of multiple networks + selector is basically making the server-side networks wider, and the selector can be simply deemed as an additional layer that projects the outputs from the wider network to lower dimensional representations. From this perspective, this does not fundamentally change the game. Adaptive attackers that know the architecture, can simply model the redundancy and selector as well, and try to reconstruct the additional selector layer. \n\nCan the authors also make some clarifications on whether adaptive attacks may make the proposed defenses less effective?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1393/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1393/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1393/Reviewer_Z7Pt"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1393/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838391259,
        "cdate": 1698838391259,
        "tmdate": 1699636066746,
        "mdate": 1699636066746,
        "license": "CC BY 4.0",
        "version": 2
    }
]