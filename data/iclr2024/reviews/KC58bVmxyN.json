[
    {
        "id": "DI0QT6hLrX",
        "forum": "KC58bVmxyN",
        "replyto": "KC58bVmxyN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1539/Reviewer_NM51"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1539/Reviewer_NM51"
        ],
        "content": {
            "summary": {
                "value": "This work builds a model that learns generalizable abstract relational structure on a decision making task where one has to answer the relation between pairs of stimuli for a reward. The stimuli can have \"many to many\" relations in that each stimuli can have up to 4 relations with other stimuli. The model builds on works like TEM and NTM where an explicit external memory module is written to/read from. The authors tried a one-dimensional version of the task where stimuli only had two relations and a two-dimensional version of the task where there can be more relations. The authors then saw that the model reproduced some neural phenomenon on such relational tasks such as distance coding, hexagonal modulation, and distance coding."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Very comprehensive review of past work. \n\n* Experiments are rigorous (multiple seeds, etc) and well-done. Showing the reproduction of the neural phenomenon is pretty nice. \n\n* Model is written clearly."
            },
            "weaknesses": {
                "value": "* There is extensive discussion of previous work, but I was left wondering what exact contributions this model makes over other models in this space like TEM. The paper discusses numerous differences, but I would like to see explicit discussion of what this model brings to the table, what specific phenomenon that this model produces that other models don't, etc. There are maybe some signs of this throughout the paper, but I didn't see any explicit discussion on it. \n\n* There's no limitations section/paragraph, which is an important part of any iclr paper."
            },
            "questions": {
                "value": "* Would it be possible for the model to learn these relations implicitly without specifically being rewarded for them? Sometimes for humans, abstract relational structure can often be learned in service of doing a specific task, rather than being trained specifically on finding the correct relations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1539/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1539/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1539/Reviewer_NM51"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1539/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698780354662,
        "cdate": 1698780354662,
        "tmdate": 1699636082505,
        "mdate": 1699636082505,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "86xBcUtm3u",
        "forum": "KC58bVmxyN",
        "replyto": "KC58bVmxyN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1539/Reviewer_k38n"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1539/Reviewer_k38n"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a computational model for learning continuous spaces.  The primitives of the model include relationships, a, as well as entities x (just vectors in R^N).  To learn a one-dimensional space, the model is given two relations (analogous to greater than and lesser than); to learn a 2-dimensional space the model is given four relations (analogous to left vs right and above vs below).  The model's given a set of training data using near neighbors and then asked to generalize to pairs that were not observed (transitive inference).  And after learning a set of relationships, the model can generalize in that after learning a 1-D relationship among one set of stimuli, the model can more rapidly learn the analogous relationship among a second set of stimuli.  Notably, other approaches that are widely used in computational neuroscience (Tolman Eichenbaum machine, neural Turing machine, LSTM etc) not only fail to show these properties but can't learn the problem in the first place."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "As far as I'm aware this is a completely novel approach.   \n\nThe construction of ``internal spaces'' is an absolutely fundamental in computational cognitive neuroscience. \n\nThe idea of separating entities from relationships could be on the right track."
            },
            "weaknesses": {
                "value": "I wonder if it's possible to get TEM/transformer/NTM/etc to do something like this task if it's presented differently.  \n\nI found the connection to neuroscience very indirect, notwithstanding the observation that the similarity of internal states exhibits a distance effect and there's evidence for 60 degree symmetry.  This model is very abstract."
            },
            "questions": {
                "value": "The observation that transformers (for instance) don't learn these tasks is interesting.  Presumably, though this model is ill-suited for, say, language modeling.  What other problems can this computational approach solve (preferably in the general field of AI/ML)?   \n\nHow does this approach scale?  As the number of continuous dimensions goes up how does it behave?  Suppose you chose a different way to tile the plane.  Rather than placing items at grid coordinates, suppose each item had N near neighbors (or that the exemplars were irregularly scattered).  This would mean that the number of relations a has to grow.  How sensitive is this model to the number of relations (controlling for the dimension of the space)?  Does it depend on a regular tiling of the space with entities?\n\nAn alternative approach is to simply assume that the brain is organized to represent low dimensional spaces and the learning problem is to determine how to map those internal spaces onto the external world. E.g.,\nhttps://doi.org/10.1109/IJCNN54540.2023.10190998\nhttps://doi.org/10.1109/IJCNN54540.2023.10191578"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1539/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814635864,
        "cdate": 1698814635864,
        "tmdate": 1699636082428,
        "mdate": 1699636082428,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vVchYyza6T",
        "forum": "KC58bVmxyN",
        "replyto": "KC58bVmxyN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1539/Reviewer_rY1U"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1539/Reviewer_rY1U"
        ],
        "content": {
            "summary": {
                "value": "This paper describes a new neural model that learns abstract relations for one-dimensional and two-dimensional set orderings. The model is trained and tested on sets from different domains, such that  the testing phase is identical to human experiments on learning transitive relations. \nThe model is demonstrated to generalize to unseen domains, while several stat-of-the art methods are not able to solve their task. This generalization is made possible by the model's architecture with two components - (1) a set matrixes for learning abstract relations (one per relation), and set of  memory matrixes for binding concrete tokens from a specific domain to the abstract relations. The model is cognitively plausible, as its performance during the test phase appears to be similar to human performance in relation learning experiments, while other state-of-the-art models fail to complete the task."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper makes a novel contribution, significantly improving on existing models.  \n\n2. Model testing based on simulating previous human studies strongly supports cognitive plausibility of the model."
            },
            "weaknesses": {
                "value": "I found the presentation to be poorly readable in places - this is not a big deal, but I would suggest editing for clarity."
            },
            "questions": {
                "value": "The section analyzing hexagonal modulation within the model was not entirely clear to me -- it wasn't clear why the authors used the specific method of averaging state vectors.  Is there a citation, or maybe some explanation rationalizing this method? This can be included in the supplement. What is the significance of this hexagonal modulation emerging, given that the model's architecture is fundamentaly different from biological brains? Why would the authors expect hexagonal modulation to emerge?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1539/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698970731847,
        "cdate": 1698970731847,
        "tmdate": 1699636082354,
        "mdate": 1699636082354,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LweKBM1yLv",
        "forum": "KC58bVmxyN",
        "replyto": "KC58bVmxyN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1539/Reviewer_RHcS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1539/Reviewer_RHcS"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new cognitive model for performing memory-based decision-making tasks. The main contribution is a learning algorithm that allows the model to learn abstract relationships from reward-guided relational inference tasks, while maintaining dynamic binding between these abstract relations and concrete entities in a given task using a memory mechanism. The experiments demonstrate the model's ability to capture relational structures in one-dimensional and two-dimensional hierarchies. The authors also show that the model exhibits both performance and internal representations that bear resemblance to human behavioral and fMRI experimental data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper introduces an interesting cognitive model for acquiring abstract relationships through reward-guided relational inference tasks. The experiments showcase the model's ability in learning relational structures that exhibit generalization across novel domains, featuring previously unseen entities. Notably, it significantly outperforms baseline models, such as LSTMs and standard Transformers. Further, the authors reveal an intriguing alignment between the model's behavior and fMRI data from humans performing the same tasks."
            },
            "weaknesses": {
                "value": "While the overall presentation of the paper is good, there are a couple of sections that are not easy to follow. The results on the two-dimensional hierarchy (section 4.2) can be challenging to understand for someone not very familiar with the findings in Park et al. (2021). Additionally, the notation in the section on transitive inference (3.4) can be a bit confusing (please see questions below).\n\nThe paper also misses references to related works on models for cognitive maps [1, 2]. Notably, [2] provides a unifying explanation for multiple hippocampal observations, while [3] presents an interesting approach for the reuse of learned abstractions in the form of graph schemas. It would be helpful to discuss the relationship between the approach in this work and these previous works.\n\nMinor: There are several grammatical errors and a few typos (e.g., 'Maharanobis' on page 15) scattered throughout the paper.\n\n\n[1] George, D., et al., 2021. Clone-structured graph representations enable flexible learning and vicarious evaluation of cognitive maps. Nature communications, 12(1), p.2392.\n\n[2] Raju, R.V., et al.., 2022. Space is a latent sequence: Structured sequence learning as a unified theory of representation in the hippocampus.\n\n[3] Guntupalli, J.S., et al., 2023. Graph schemas as abstractions for transfer learning, inference, and planning. arXiv preprint arXiv:2302.07350."
            },
            "questions": {
                "value": "- What properties does the relation matrix possess, could you offer insights into them?\n- Is there a separate MLP for each $g_a$? If yes, how do you ensure the probabilities sum to 1?\n- In equation 10, does $m$ in $\\psi_a^{m-1}$ correspond to the $m^{\\rm th}$ power? Or is it the $m^{\\rm th}$ iteration? If the latter, how is  $\\psi_a^{m-1}$ updated? \n- In the formula for the inference score in section 4.1, what is $c^{ti}$?\n- How was the value $\\alpha=0.7$ chosen?\n- In the caption of Figure 4, you mention that the for NTM and DNC the horizontal axis is 50-times reduced for readability. Does this mean that they used 50-times more epochs?\n- How were the hyperparameters selected for the baseline methods in Figure 4a? \n- In Figure 4b, why is the performance, after approximately 150 steps, slightly worse after 8000 epochs compared to the performance after 1000 epochs?\n- What is the effect of S (the length of the state vector) on the results?\n- In section 4.2, you discuss the learned intermediate representation $h$. I'd like to clarify the definition, since there appear to be two distinct uses of $h$ in equations 3 and 4.\n- Could you please clarify how the state values are computed in section 4.2? \n- Are there any thoughts about how this approach can be extended to more general relational graphs or to scenarios with sparse rewards?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1539/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698999184208,
        "cdate": 1698999184208,
        "tmdate": 1699636082225,
        "mdate": 1699636082225,
        "license": "CC BY 4.0",
        "version": 2
    }
]