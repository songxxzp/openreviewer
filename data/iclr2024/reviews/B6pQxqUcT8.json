[
    {
        "id": "w9ZRGgs4Pm",
        "forum": "B6pQxqUcT8",
        "replyto": "B6pQxqUcT8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6910/Reviewer_7Kjr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6910/Reviewer_7Kjr"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a tree-search-based planning framework for Large Language Models (LLMs). Their framework is motivated by A* search in heuristic planning and uses a cost function, which is a sum of (past) accumulated cost and (future) heuristic cost to update their planning tree. The use of simple heuristics also makes it more resource-efficient than concurrent works based on Monte Carlo Tree Search. The authors demonstrate both empirically, and through various ablation studies, how their proposed method generates more feasible plans."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* Clarity: The paper is well-written and easy to follow. \n* Novelty: The proposed tree-search-based method is novel (see footnote) and uses a simple but effective cost-function design that helps in better decision-making. \n* Significance: The experiments adequately justify the superior performance of their proposed method, both empirically and qualitatively. The use of heuristics for LLM planning is appealing to the LM planning & reasoning community and could be a potential research direction for the future.\n\nFootnote: Note the work \"SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge, Hazra, et al., 2023\" which also proposes heuristic planning with LLMs. However, given its recency, I have considered it as a concurrent work, and therefore have not compared them despite their overlapping contributions."
            },
            "weaknesses": {
                "value": "I did not find any major weaknesses in the paper. However, it would be nice to have some clarifications regarding the proposed heuristic functions (see Questions)."
            },
            "questions": {
                "value": "1. What do you mean by \"task-specific\" heuristic functions? Are the tasks here based on \"g\"? If so, do you use different memory buffers for each task, and how many trajectories do you need in the memory buffer before it produces a valid score? \n\n2. It seems to me that the future cost heuristic might mislead the planner since it is likely that the same action may appear at different positions (in the sequence) for different tasks. For instance: \"Clean a kitchen table\" and \"Set a kitchen table\" have their action sequences reversed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6910/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6910/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6910/Reviewer_7Kjr"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6910/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698495627086,
        "cdate": 1698495627086,
        "tmdate": 1699636804037,
        "mdate": 1699636804037,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lHo2aXqeGP",
        "forum": "B6pQxqUcT8",
        "replyto": "B6pQxqUcT8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6910/Reviewer_tHXf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6910/Reviewer_tHXf"
        ],
        "content": {
            "summary": {
                "value": "Leveraging powerful LLMs in complex decision-making tasks is a promising direction, and the authors discussed their categories with clear explanations. For efficient reasoning, the authors proposed a new method inspired by A* search and developed some designed evaluation functions for the search. Experimental results with various domains (e.g., Home Search, Trip Booking) with GPT-3.5 and GPT-4 show the effectiveness of the proposed method named ToolChain*."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- A firm contribution of the reasoning framework using LLMs inspired by the MCTS and search algorithm.\n- Extensive experimental evaluations show the effectiveness of the concept."
            },
            "weaknesses": {
                "value": "- Posiblly handcrafted the two important components (g(n) and h(n) in the A* search) for the problem."
            },
            "questions": {
                "value": "- Although the evaluated methods and experiments seem solid and interesting to the readers, I\u2019m curious about the designing part of the two important evaluators, $g(n)$ and $h(n)$.\n    - As we know, the naive A* algorithm has some theoretical discussions (e.g., admissible heuristic and optimality in the search problems). However, in the current status of the ToolChain*, the idea (evaluating the node with some function) is from the A*, but no theoretical discussions (e.g., the effectiveness of search) have arisen from the design of $f(n) = g(n) + h(n)$. Do you have any insights? Are there any reasons to adopt the current form of functions? Tried different ones? Although the ablation study in Table 2 is useful, particularly restricted to the current form of g and h, we could have further options here."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6910/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698715962895,
        "cdate": 1698715962895,
        "tmdate": 1699636803888,
        "mdate": 1699636803888,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "z4Ipb97v0o",
        "forum": "B6pQxqUcT8",
        "replyto": "B6pQxqUcT8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6910/Reviewer_52p8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6910/Reviewer_52p8"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes ToolChain*, a best-first search algorithm for improving the performance of LLM-based agents on sequential decision-making problems. The main contributions are algorithmic and empirical. Specifically, the paper proposes a set of heuristic cost functions leveraging a dataset of solved plans to explore the search space more efficiently. Experiments are conducted on the ToolBench and GSM8K datasets. The results show that ToolChain* outperforms or matches strong recent baselines on these tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper tackles an interesting and important research question with potentially large impact.\n\n+ The paper proposes a simple and intuitively clear best-first search approach to construct an improved LLM-based planning agent. The heuristic function seems novel with potential for broad use across tasks.\n\n+ The proposed method outperforms a number of strong baselines. The experiments include ablations, computational costs and other forms of analyses making it easier to evaluate. The experimental section and appendix has a good amount of detail, aiding reproducibility. \n\n+ The paper is reasonably well-written. The use of illustrative examples made it very easy to understand."
            },
            "weaknesses": {
                "value": "- Some of the implementation details of the four components of the heuristic function are not clearly explained. These are central to evaluating the algorithmic contributions of the paper and reproducibility so a clearer description would be very useful. I found it particularly difficult to understand the implementation details of the \"imagination score\". See the questions for details.\n\n- Although the experiments have a good amount of detail including ablation experiments, the paper does not clearly identify the main sources of performance improvement. For example, how much does a better scoring function matter compared with a better LLM or search procedure? How does performance depend on the prompt and other hyper-parameters of the algorithm (heuristic score implementation, dataset size and quality, etc.)? What happens if we use non-LLM versions of the scoring function? The paper mentions the importance of the cost function (\"this efficiency gain may stem from the proposed superior cost function\") but does not investigate deeper. As a result, it becomes difficult to clearly identify the overall impact and broader utility of ToolChain*, especially compared to similar search-based ideas proposed in Tree-of-Thought."
            },
            "questions": {
                "value": "- Please explain the details of the Imagination Score in more detail. For example, what does \"imagine more concrete future steps\" mean? Where is the \"imagined plan\" generated in Algorithm 1? (I think in the cost calculation $f(n)$ but not sure). How many extra calls to the LLM?\n\n- Is the LCS calculation in $g_{t,1}$ over the sequence of API actions ($a_i \\in \\mathcal{A}$) alone or does it include additional information about the task or problem in the text string? Do the API actions include the parameters or just the method names?\n\n- How much does overall performance vary with the prompt and the other hyper-parameters of the algorithm (dataset, heuristic score implementation, etc.)?\n\n- How much performance comes from the use of the LLM in different parts of the algorithm vs classical (non-LLM) best-first search? Specifically, how is performance impacted by non-LLM expansion with LLMs only used for node evaluation? Is it possible to construct a search-based baseline which **doesn't** use any LLMs but otherwise incorporates the same intuition (long-term memory, heuristic scoring, etc.)? If yes, how might it perform?\n  - In Appendix D.5, is it possible to characterize the computational costs of ToolChain* in more detail? For example, what are the the LLM costs in terms of the number of calls and generated tokens? Dollar cost? Heuristic computational costs? Environment (domain) \"simulation\" costs associated with the API calls, if any?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6910/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698849140875,
        "cdate": 1698849140875,
        "tmdate": 1699636803768,
        "mdate": 1699636803768,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vYnNEAo9eS",
        "forum": "B6pQxqUcT8",
        "replyto": "B6pQxqUcT8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6910/Reviewer_gdT8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6910/Reviewer_gdT8"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes ToolChain*, an efficient tree search-based planning algorithm to augment large language models (LLMs) with external tools for complex real-world planning and reasoning tasks.  This paper provides very good insight that formulates the action space as a search tree, where each node is an API function call. This mitigates error propagation and expands the search space compared to linear reasoning in open-/closed-loop systems. It uses an A*-like algorithm to search the tree, prioritizing branches using a task-specific cost function with cumulative cost g(n) and future cost h(n). This balances exploration and exploitation for efficient search."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1) Novel formulation of LLM planning as tree search using A* search algorithm is intuitive and elegant. Allows structured exploration of expansive action space.\n2) Task-specific cost functions g(n) and h(n) provide a principled way to guide search and prioritize promising branches.\n3) Strong empirical results demonstrating improved success rate and efficiency over competitive baselines on diverse tasks."
            },
            "weaknesses": {
                "value": "1) Cost functions rely on heuristic components like long-term memory, self-consistency sampling, and LLM imagination which may not always be accurate.\n2) Still trails open-loop systems in efficiency, albeit outperforming other tree search methods. There is a tradeoff between search depth and solution quality.\n3) Limited analysis on how the approach generalizes to even more expansive action spaces and longer planning horizons. \nHowever, I think that the above weaknesses are some hard open questions in NLP with LLM."
            },
            "questions": {
                "value": "1) How robust are the cost functions g(n) and h(n) for entirely new tasks where long-term memory/heuristics may not be available?\n2) Could incremental search algorithms like IDA* further improve efficiency over A* search used here?\n3) For very large action spaces, is it possible to focus the search on high-level plans first before expanding API details? Hierarchical search?\n4) How well would ToolChain* generalize if the action space grows 10x or 100x in size? At what point would efficiency degrade?\n5) Beyond API functions, how can ToolChain* be extended to more general symbolic action spaces?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6910/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699554273050,
        "cdate": 1699554273050,
        "tmdate": 1699636803646,
        "mdate": 1699636803646,
        "license": "CC BY 4.0",
        "version": 2
    }
]