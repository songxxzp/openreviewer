[
    {
        "id": "c7LtUWZpRF",
        "forum": "CK5Hfb5hBG",
        "replyto": "CK5Hfb5hBG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6280/Reviewer_rJx2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6280/Reviewer_rJx2"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a new tokenization scheme for Vision Transformer (ViT), where an input image is split into patches not only in spatial dimensions but also in channel dimensions. This may be beneficial for data where channels carry different semantical information, thus having little correlation, and might not always be present altogether in the data. Additionally to the tokenization scheme, the authors propose Hierarchical Channel Sampling (HCS) to regularize training, adapting the model to imperfect inputs."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The proposed method outperforms the original ViT model in almost all considered experiments.\n* The proposed regularization training scheme proves its efficiency against input channel dropout even for the original ViT.\n* It is claimed that it is possible to increase explainability through attention map analysis with the new scheme. \n* The experimental base behind the work is diverse and extensive. Most authors\u2019 statements are supported by an experiment or an ablation in the main section or the Appendix.\n* The paper is easy to read and understand (illustrations, tables, etc.), and well-organized."
            },
            "weaknesses": {
                "value": "* The novelty of the work is limited, as it seems that the channel splitting proposed in this paper is similar to a multi-modal approach to multi-channel data that has been explored earlier (albeit not with images). Also, HCS can be viewed as a modification of input channel dropout that has also been known before.\n* The gains from the proposed approach are limited to the data with many channels (e.g., some microscopy images) and seem less reasonable for other types. At the same time, the processing with this new approach takes significantly more time."
            },
            "questions": {
                "value": "1. I suggest revisiting the title. The inspiration for the title used in this paper has clearly come from the ViT\u2019s \u201cAn image is worth 16x16 words\u201d. But in the case of ViT, it was about patches of size 16x16 across all channels, i.e., \"Cx16x16\", while the proposed approach uses \"1x16x16\" \"words\", as the image is split into channels as well. \n2. Please clarify why the proposed channel sampling is \u201chierarchical.\u201d\n3. It would be beneficial to review the notation used in the formulae, e.g. use AB instead of A\\cdotB for matrix multiplication."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6280/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698746513474,
        "cdate": 1698746513474,
        "tmdate": 1699636688067,
        "mdate": 1699636688067,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "R717cWAjBR",
        "forum": "CK5Hfb5hBG",
        "replyto": "CK5Hfb5hBG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6280/Reviewer_EcAm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6280/Reviewer_EcAm"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an extension of the Vision Transformer (ViT) architecture for image analysis domains with multi-channel information. This includes microscopy (fluorescence), and satellite (spectral) imaging, among others. The method consists of decoupling channels from the input and passing them as different tokens. To this end, the authors designed a learnable channel embedding and a hierarchical sampling method. As a result, a ChannelViT model learns to associate tokens in the space and channel dimensions, and is robust to channel drops."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Well motivated paper. The problem of processing multi-channel images is important and the paper presents arguments to design methods that deal with this information effectively.\n* The method is simple and effective. Splitting images in a sequence of tokens is an intuitive approach and the results show its effectiveness.\n* Beyond tokenizing, an important aspect of the method is sampling channels strategically to learn their associations.\n* The analysis in various datasets, including ImageNet, JUMP-CP and So2Sat, adds to the evidence that the method works in practice.\n* Various ablations and extensive details presented in the manuscript and the appendix.\n* The evaluation includes quantitative and qualitative results that show the benefits of the proposed approach."
            },
            "weaknesses": {
                "value": "Major comments:\n* Limited baselines reported. The paper only considers ViTs as a baseline, which is a natural comparison given that the proposed method is an extension of this architecture. However, comparison to other architectures, especially CNNs to solve the multi-channel problems should be reported. This is specially important to appreciate the relative performance compared to other solutions tested before in multi-channel images.\n* Computational cost increases quadratically with the number of channels. This problem is not addressed or commented in the main manuscript. According to the reported times in the appendix, the training cost increases linearly with the number of channels. The computational cost should be analyzed and discussed in more detail. While the solution seems effective, it can be prohibitive in practice.\n* In a similar note as the first point, other adaptations of ViTs have been reported for video analysis and 3D imaging, which also increase the image dimensions in a new axis (different than channels). Do any of these existing adaptations apply for channels?\n* The prediction of treatment in the JUMP-CP dataset is not necessarily a biologically relevant task. Prior literature makes the distinction that this is a pretext task for learning representations (weakly supervised learning), and not the main goal of analyzing these images. This clarification should be mentioned and prior work should be cited. Ideally, a biologically relevant task should be reported to ensure that classification performance in this pretext task is not dominated by confounders or spurious correlations. If this is not possible, an alternative test or explanation should be provided. \n\nOther comments:\n* The results in Table 3 are based on a random selection of images with different numbers of channels (25%, etc). The reviewer assumes that the random partition is the same across experiments. The results should be repeated a few times with different partitions and standard deviations should be reported.\n\nMinor comments:\n* Figure 5 reports KRAS in the top row and KCNH76 in the bottom row, but the main text refers to them in the opposite order.\n* Does figure 6 include error bars? Can you clarify how these were obtained?\n* Minor typos: demonstrats, utiliz, involvs."
            },
            "questions": {
                "value": "* Can the authors report additional baselines and compare to other state-of-the-art methods in the same datasets that they evaluate? This could clarify the achievements of the method in context to prior art in these domains.\n* Can the authors add computational analysis and comparisons to other methods for training / testing time of the methods? Even if the cost is a limitation, reporting and evaluating this aspect transparently can enhance the understanding of how useful this method can be in practice."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6280/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6280/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6280/Reviewer_EcAm"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6280/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698777235721,
        "cdate": 1698777235721,
        "tmdate": 1700711345240,
        "mdate": 1700711345240,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "skCLRENkJx",
        "forum": "CK5Hfb5hBG",
        "replyto": "CK5Hfb5hBG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6280/Reviewer_ptju"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6280/Reviewer_ptju"
        ],
        "content": {
            "summary": {
                "value": "This work presents ChannelViT, a new ViT variant that: 1) learns a channel embedding for each channel, and 2) proposes a technique called Hierarchical Channel Sampling (HCS) for robustness to partial channels. Together, ChannelViT is reported to demonstrate better robustness to missing channels. Multiple experiments were performed, such as: 1) different model architectures (Channel variants of ViT-S/16 and ViT-S/8), 2) comparison with and without HCS, 3) comparisons across multiple benchmarks including microscopy imaging, 4) comparisons across varying number of channels available, 5) exploration into SSL compability in DINO, and others."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- ChannelViT with HCS proposes a simple but relatively intuitive extension of ViTs, which would have unique applications in multiplexed imaging in which not all \"channels\" (e.g. - fluorescent probes) are made available. Overall, this work presents a method that targets an important application and may enable significant biological / clinical findings in multiplexed imaging.\n- The ablation experiments regarding assessment of partial channels, comparisons and baselines with training on single channels, and experimentation with DINO are thoughtful and well-organized. In particular, the assessment of partial channels and pretraining with DINO highlights its adaptability as a ViT in potentially replacing vanilla ViTs. Table 2 and Figure 4, which demonstrates the performances of Channel ViT (in comparison to vanilla ViT) on JUMP-CP with partial channels, demonstrates minor but consistent improvement."
            },
            "weaknesses": {
                "value": "- While the main application of ChannelViT is in targeting problems such as those in multiplexed imaging due to the challenge of generalizing encoders across datasets that would have the same set of probes, only one dataset explored in this work is related to multiplexed imaging. Experimentation on ImageNet, C17-WILDS, and others are informative and appreciated, are not as relevant to the ultimate application that ChannelViT serves. Though C17-WILDS is microscopy, hematoxylin and eosin (H&E) pathology is not a domain where only the hematoxylin or eosin stains are performed. Rather, it would be more interesting to explore this problem on other relevant multiplexed imaging benchmarks such as RXRX1 [1], RXRX1-WILDS [2], TissueNet [3], and others [4-6]. To this end, it would also be useful to discuss and compare ChannelViT in context with label-free approaches, which suggest channel synthesis as a paradigm for addressing missing channels.\n- As a drop-in replacement for ViT in conventional natural image classification or medical imaging tasks, one concern I have (raised in the work) is lack of experimentation on model scale. The title of this work, \"Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words\", is similar to a title of a related seminal work in ViTs [7], but does not evaluate and demonstrates the scale of Channel ViTs in the same manner in demonstrating its universality of overtaking  current state-of-the-art architectures. As ViT-Small architectures are not the most commonly-used model size for ViT, it would be informative to explore Channel ViTs at greater scale (such as ViT-B and ViT-L). An additional limitation connected to this is the efficiency of learning channel embedings, which may have been the reason for not developing larger ViT models. As shown in the DINO experimentation, even with HCS, training is approximately 3.64x more expensive than its baseline comparison, which may limit its usability. \n\nReferences:\n1. Haque, I., 2023. Rxrx1: A dataset for evaluating experimental batch correction methods. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4284-4293).\n2. Sypetkowski, M., Rezanejad, M., Saberian, S., Kraus, O., Urbanik, J., Taylor, J., Mabey, B., Victors, M., Yosinski, J., Sereshkeh, A.R. and Haque, I., 2023. Rxrx1: A dataset for evaluating experimental batch correction methods. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4284-4293). (https://wilds.stanford.edu/datasets/)\n3. Greenwald, N.F., Miller, G., Moen, E., Kong, A., Kagel, A., Dougherty, T., Fullaway, C.C., McIntosh, B.J., Leow, K.X., Schwartz, M.S. and Pavelchek, C., 2022. Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning. Nature biotechnology, 40(4), pp.555-565.\n4. Bray, M.A., Gustafsdottir, S.M., Rohban, M.H., Singh, S., Ljosa, V., Sokolnicki, K.L., Bittker, J.A., Bodycombe, N.E., Dan\u010d\u00edk, V., Hasaka, T.P. and Hon, C.S., 2017. A dataset of images and morphological profiles of 30 000 small-molecule treatments using the Cell Painting assay. Gigascience, 6(12), p.giw014.\n5. Cross-Zamirski, J.O., Mouchet, E., Williams, G., Sch\u00f6nlieb, C.B., Turkki, R. and Wang, Y., 2022. Label-free prediction of cell painting from brightfield images. Scientific reports, 12(1), p.10001.\n6. Moshkov, N., Bornholdt, M., Benoit, S., Smith, M., McQuin, C., Goodman, A., Senft, R.A., Han, Y., Babadi, M., Horvath, P. and Cimini, B.A., 2022. Learning representations for image-based profiling of perturbations. Biorxiv, pp.2022-08.\n7. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929."
            },
            "questions": {
                "value": "Summarizing my above concerns, though this work would benefit from having additional experimentation related to its targeted application in multiplex imaging, such as:\n- Additional evaluation on relevant datasets in multiplex imaging, such as RXRX1 and TissueNet\n- Discussion and benchmark evaluation against label-free approaches in multiplex imaging\n- Ablation experiments with ViT-B and ViT-L, with expanded discussion on the intended usage of ChannelViT w.r.t. efficiency"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6280/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813816147,
        "cdate": 1698813816147,
        "tmdate": 1699636687800,
        "mdate": 1699636687800,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xIHJuGWRoS",
        "forum": "CK5Hfb5hBG",
        "replyto": "CK5Hfb5hBG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6280/Reviewer_PjTm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6280/Reviewer_PjTm"
        ],
        "content": {
            "summary": {
                "value": "While the Vision Transformer has demonstrated robust performance with real-world images, its capacity to process multi-channel images, such as those from satellites, is somewhat constrained. To address this limitation, the authors have enhanced the conventional vision transformers by introducing a Hierarchical Channel Sampling (HCS) approach, which tackles the issue of sparse input channels. The proposed ChannelViT model has exhibited impressive results across three datasets, outperforming the traditional vision transformer."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+  The Hierarchical Channel Sampling (HCS) module enhances robustness by performing channel-wise sampling, which proves beneficial in scenarios involving incomplete image channels.\n+  ChannelViT surpasses the conventional Vision Transformer (ViT) by demonstrating insensitivity to the number of input image channels, where ViT shows vulnerability.\n+    A novel two-stage sampling algorithm is introduced within ChannelViT to selectively obscure input channels, optimizing the model's performance."
            },
            "weaknesses": {
                "value": "-    There is a potential risk of information loss, as highlighted in Section 3 of the methodology. The model's approach to segmenting the input image into various channel sequences and processing them individually could disrupt the alignment of channels, particularly in a 3-channel image prediction task.\n-    The patch embedding technique described in Section 3.1 overlooks the issue of channel alignment when deconstructing images into separate channels.\n-    The innovation of the proposed method warrants further scrutiny. Elements such as patch embedding, positional embedding, and Transformer Encoders, as discussed in Section 3.1, do not significantly diverge from the traditional Vision Transformer framework. These aspects should be acknowledged in the Related Work section, with a stronger emphasis on the unique contributions of this research.\n-    The assertion in Section 3.2 that 'HCS guarantees equitable sampling across each m' lacks intuitive justification. While empirical results support this claim, a theoretical rationale would be beneficial."
            },
            "questions": {
                "value": "-    Is HCS also employed during the testing phase? It was mentioned that HCS simulates a test-time distribution during training.\n-    Regarding Table 2, when selecting channels for testing, are these chosen at random, or were all possible combinations tested? If it's the latter, could you provide the mean and standard deviation of the results?\n-    Has the model's performance been evaluated on datasets with varying channel availability, where some data might have only partial channels while other portions are fully channeled?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6280/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699460600577,
        "cdate": 1699460600577,
        "tmdate": 1699636687678,
        "mdate": 1699636687678,
        "license": "CC BY 4.0",
        "version": 2
    }
]