[
    {
        "id": "jFXPkB5Vir",
        "forum": "dCHbFDsCZz",
        "replyto": "dCHbFDsCZz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9141/Reviewer_irSz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9141/Reviewer_irSz"
        ],
        "content": {
            "summary": {
                "value": "In the submission, a novel loss function parameterised by the learnable rejectors is studied to train the LLM model to handle diverse outputs for one prompt. To compute the existing loss function for tackling this problem is NP-hard. At the same time, the authors proposed a trackable surrogate loss, which is differentiable and convex, and by optimising it, the generalisation error is minimised supported by the theorem proposed as the main result in the submission. In the experiments, by training the model with the proposed loss function, the models are improved in terms of precision vs. Coverage compared with the existing method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The derivation is clear and sound. I am not an expert on the proofing behind so I only scan the appendix. However, by following each proposition and theorem, it is easy to follow the logic step by step and reach the point that by minimising the proposed loss function the target generalisation error is minimised. \n2. Nice plot for intuitively explaining the property of the surrogate loss with the changing of the r(x)."
            },
            "weaknesses": {
                "value": "1. The experiment setting is not well explained and the settings are not comprehensive. As the authors mentioned,  the LLM predictors are their main focus but in the experiments, there is only one type of LLM. As T5X is a family of models, there are other choices and more architectures from other families will be more comprehensive for evaluating the proposed loss. \n2. The loss function is tested on image classification but on a tiny setting and this pure classification task does not really relate to the LLM setting. \n3. To train the rejector still required to label new information, it is hard to distinguish whether the improvement is from the additional information or the loss function."
            },
            "questions": {
                "value": "1. According to my understanding, applying the surrogate loss requires labelling the output from the given model. Then the model is further trained by the learned surrogate loss. Thus some extra information is introduced. Can the Cross entropy loss and Maxprob have the same information? \n2. What is the format of the ejector?\n3. WHy the F1 score is not applied for clear comparison?\n5. For the std comparison is not very clear whether rejection loss is much better than Maxprob in Figure 4 and in Figure 5."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9141/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698601841694,
        "cdate": 1698601841694,
        "tmdate": 1699637150538,
        "mdate": 1699637150538,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ctsTSJzaZd",
        "forum": "dCHbFDsCZz",
        "replyto": "dCHbFDsCZz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9141/Reviewer_KRjg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9141/Reviewer_KRjg"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of learning to reject with a fixed predictor, motivated by the case when the fixed predictor is a pretrained large language model (LLM). The goal is to learn a rejector function that allows for post-hoc filtering of LLM outputs: by rejecting lower-quality outputs, the combined predictor/rejector system can have higher precision at the cost of lower coverage.\nThis is especially critical in high-stakes sequence-to-sequence problems in domains such as health data.\n\nThe paper designs an H-consistent surrogate loss for learning to reject with a fixed predictor using the framework of Awasthi et al. (2022). This loss is then used for the decontextualization problem, which is to rephrase a sentence into a form that's understandable without the context around the original sentence.\n\nOn the decontextualization experiment, the proposed surrogate outperforms strong baselines from the learning-to-reject and model calibration literature."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The surrogate loss derived in the paper doesn't require tuning of a threshold parameter, which is a drawback of confidence-based approaches.\n\n- Empirical evaluation that avoids a common pitfalls of other work in this area: because different loss functions are being compared, it's only fair to compare performance with the optimal learning rate, since changing the loss function changes the scale of gradient updates.\n\n- The paper's theoretically-derived relationship between the two hyperparameters $\\alpha$ and $\\beta$ works well empirically."
            },
            "weaknesses": {
                "value": "- The method is only applied to the decontextualization problem when it actually seems to have much more broad applicability. I can think of several LLM applications where the ability to increase precision by rejecting would be useful. For example, we could try to learn a rejector that cuts down on hallucinations in summarization or text simplification. More exploration of this technique beyond the decontextualization problem would make it more impactful.\n\n- > Additionally, to the best of our knowledge, minimizing the cross-entropy loss does not have any proven guarantee with respect to our main objective: minimizing the induced rejection loss.\n\n    - When $\\mathcal{R} = \\mathcal{R}\\_{all}$, can't we use a cost-sensitive surrogate loss plus usual (Bayes) consistency results? \n      I.e., I don't understand the following claim in Appendix C given that the paper's results only consider $\\mathcal{R} = \\mathcal{R}\\_{all}$:\n     >  (i) There is a lack of any H-consistency bound guarantees for cost-sensitive surrogate losses with respect to the induced rejection loss. \n\n        In general, I think the paper could use more presentation on why the naive approach of directly training the rejector to predict the labels $a$ using a standard cost-sensitive surrogate loss doesn't work, since the results only consider the $\\mathcal{R} = \\mathcal{R}\\_{all}$ case. That, or some results for linear or 1-layer-NN rejectors, as in Awasthi et al. (2022), would strengthen the theoretical part of the paper.\n\n- No ablation on $\\alpha$ even though it's important in the bound (Thm 5), and different experiments use different values (e.g., 4 in main text experiments, 3.5 in appendix vision experiments)"
            },
            "questions": {
                "value": "- Why do we care about $\\mathcal{H}$-consistency even though we only consider the space $\\mathcal{R}_{all}$ of all measurable functions? It would be helpful to further emphasize this earlier in the paper as a major difference / novelty / reason why more naive baselines don't work (see earlier comment under weaknesses).\n\n- small typo in display at bottom of pg4; $\\le 0$ should be in the subscript of $\\mathbb{I}$\n\n- small typo in display at bottom of pg4; not enough parentheses\n\n- Minor semantics/notation: $r(x,f(x))$ reads like a score for rejecting, so the notation reads like $r>0 \\implies$ reject, when it's actually being used in the opposite sense. This tripped me up several times. I know this notation is inherited from Cortes et al. but I think it's a bit confusing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9141/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9141/Reviewer_KRjg",
                    "ICLR.cc/2024/Conference/Submission9141/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9141/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788847991,
        "cdate": 1698788847991,
        "tmdate": 1700686395036,
        "mdate": 1700686395036,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0hp45EzgNz",
        "forum": "dCHbFDsCZz",
        "replyto": "dCHbFDsCZz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9141/Reviewer_AHeX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9141/Reviewer_AHeX"
        ],
        "content": {
            "summary": {
                "value": "For the purpose of letting a model 'reject' or 'abstain' from classifying some samples, a technique of using a rejection loss is proposed; from which a surrogate loss is derived and used. This is in contrast to either using the 'confidence' output or training for an (n+1)-th class in the sense that the predictor can be fixed. Theoretical guarantees for this surrogate loss are provided, and this framework is evaluated on real-world decontextualization tasks, and also for image classification. The results show promise and seem to perform better than other methods studied."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Very promising real-world results\n- Theoretical guarantees motivated well and provided\n- Good theoretical comparison to other methods, and good motivation for the need of proposed method provided\n- NLP examples but possibly further extensions to other areas\n- Surrogate loss performance better than other models and also quite close to theoretical limits at times."
            },
            "weaknesses": {
                "value": "- Although GitHub repo links and other identifying information cannot be written in a paper under review, I did not see any indication of the intention to make the code public, nor is it provided in supplementary materials\n- Page 4. First equation/inequality.\n\t- $c$ is positive. indicator functions are either 0 or 1, so:\n$\\mathbb{I}\\_{a\\leq 0} \\mathbb{I}\\_{r(x)>0} + c \\mathbb{I}\\_{r(x)\\leq 0} \\geq max(\\mathbb{I}\\_{a\\leq 0}\\mathbb{I}\\_{-r(x)<0}, c \\mathbb{I}\\_{r(x)\\leq 0})$\n\n\tmax of two different terms that are positive should be less than or equal to the sum.\n\t - And second comparison should be equal. As the first term in the first max is saying: \"Both $a$ and $-r$ should be less than zero for the indicator product to be one\". And the the first term in the second max is saying \"the max of both $a$ and $-r$ should be less than zero for the indicator to be one\". Both of these statements imply each other and therefore the last relation should be of equality. \t \n\t - The bound becomes a lower bound, not an upper bound.\n\t - I did not check the last relation. That might still hold despite this, but need to know why.\n - There is no test set. Only train and validation, where cross-validation is used so training algorithm sees all data."
            },
            "questions": {
                "value": "- page 5. it is said that \"underlying scores are not favourable for that precision level\". Why is that?\n- These are possibly standard deviation bars in figures 4 and 5. How were they generated? Is it from different folds of cross-validation?\n- Are Maxprob and cross-entropy trained on different models? Why is that?\n\n**Minor Typing / Formatting / Clarity issues**\n- page 4. last equation \"<=0\" should be in the subscript\n- please recheck format for citations: some citations use et al while other list all authors.\n\n**Comment**: I chose \"good\" in soundness, presentation and contribution, but \"3: reject\" in the overall rating. That's mainly because of the mathematical inconsistency, which I hope can be resolved."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9141/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9141/Reviewer_AHeX",
                    "ICLR.cc/2024/Conference/Submission9141/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9141/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699011229166,
        "cdate": 1699011229166,
        "tmdate": 1700512208159,
        "mdate": 1700512208159,
        "license": "CC BY 4.0",
        "version": 2
    }
]