[
    {
        "id": "bRklD21RqZ",
        "forum": "JePfAI8fah",
        "replyto": "JePfAI8fah",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission632/Reviewer_b5RW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission632/Reviewer_b5RW"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the problem of time series forecasting.\nThe authors propose a model that embeds univariate\nchannels as a whole and then uses attention between\nembedded channels. In experiments on several datasets\nthey show that their model outperforms current models\nand established new state of the art results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "s1. very simple idea and model.\ns2. very good results, establishing new state of the art results.\ns3. interesting additional finding that longer lookback windows\n  now are mostly beneficial (fig. 6)."
            },
            "weaknesses": {
                "value": "w1. experiments only on selected datasets compared to some\n  major baselines.\nw2. experimental results for close baseline PatchTST varies from\n  published results.\nw3. no standard deviations reported."
            },
            "questions": {
                "value": "The paper proposes a very simple idea, but the experiments\nto the best of my knowledge are establishing a new state of the\nart, making it an important contribution like an indepth study.\nI also liked the ablation study with growing observation horizons,\nas they now are more plausible than in the related work: longer\nobservation horizons usually pay off for your model.\n\nSome points should be discussed:\nw1. experiments only on selected datasets compared to some\n  major baselines.\n- You do not report on datasets Exchange, ILI and ETTm1, ETTm2,\n  ETTh1 and ETTh2, different from the experiments reported in TimesNet.\n  This way it is hard to see if the proposed method really outperforms\n  the baselines consistently or just on the selected datasets.\n\nw2. experimental results for close baseline PatchTST varies from\n  published results.\n- PatchTST consistently reports better results, e.g., for Electricity\n  with horizon 96 they report an MSE of 0.129, you report 0.195.\n  Where does the difference come from? \n\nw3. no standard deviations reported.\n- Standard deviations will help to assess which differences might be\n  significant and which spurious. \n\nSome minor language issues:\n- abstract, \"However, Transformer is challenged\": missing \"a\".\n- p. 2 \"irrationality\": sounds a little bit too strong to me."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission632/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698424388481,
        "cdate": 1698424388481,
        "tmdate": 1699635991054,
        "mdate": 1699635991054,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vMNBdFMZl3",
        "forum": "JePfAI8fah",
        "replyto": "JePfAI8fah",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission632/Reviewer_HTy3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission632/Reviewer_HTy3"
        ],
        "content": {
            "summary": {
                "value": "In this paper, Authors propose to investigate why Transformer-based models do not seem to be as efficient as Linear-based models for Multivariate Time Series Forecasting (MTSF), while they are predominant in other AI domains. They suggest that the way Transformer is implemented for MTSF is inappropriate. To better benefit from Transformer architecture, they propose to tokenize dataset based on variate and not on timestep. It ends up modifying the input and the FFN. Their proposal also uses only an encoder compared to the vanilla Transformer architecture.\n\nAuthors conduct extensive experiments with several Linear-based and Transformer-based baselines to determine the performance of their proposal. These experiments are performed over 6 usual MTSF datasets along with 6 market-based datasets. Their proposal, iTransformer, appears to achieve best predictions for all datasets despite the selected prediction horizon."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "In my opinion, this paper will have a high impact on MTSF research and community. They back-up their proposal with extensive experiments on several datasets and compared to multiple baselines. \nThe results and ablation study are discussed even though I feel it could have gone further, but the page limitation was probably the issue."
            },
            "weaknesses": {
                "value": "The biggest weaknesses in my opinion are first, the writing style, which sometimes is not going to the point (could be improved to better convey the idea and have greater impact) and second that Authors fail to include some important SOTA baselines and properly synthetize/discuss CD/CI/distribution shift comparing all the baselines (including the missing ones) and iTransformer."
            },
            "questions": {
                "value": "# Paper as is\nAs far as I am concern, the paper in its current version with some proof-reading and the following revisions can be Accepted.\n * In abstract \u201cthe duties of the attention mechanism\u201d should be \u201cthe following duties of the [\u2026]\u201d to make reading smoother and avoid readers from questioning on what duties means here.\n * The following claim \u201cwith potentially unaligned timestamps\u201d need to be referenced or proven. Which datasets have unaligned timestamps, or by unaligned does Authors means that there are delays between values from variates? In other terms, are physical measurements unaligned (which is not the case in the considered dataset)? or do values that correspond to the same \u201cevent\u201d appears at different timesteps for different variates (which means that there are delays, which might be the case for Traffic, but for the others, in my opinion, Authors would need to demonstrate it. For instance, in Weather (which are data from the same weather station), it is clearly not the case).\n * In my opinion, it is more appropriate to call Electricity dataset as ECL.\n * Reproducibility\n   * Avoid confusion and precise which ETT is used. It looks like it is ETTm according to appendix (15min frequency), but is it m1 or m2? Informer results looks like ETTh though\u2026\n\n   * Also, precise which PEMS you use PEMS-Bay (occupancy ratio or speed in San Francisco Bay?) or other?\n * In my opinion, Figure 1 is focusing too much on iTransformer results. Why not having each axis of the web going from 1 (in the center) to 0? For better fairness in the visualization.\n * Figure 7 (b?) right part is difficult to understand without explanation of x and y axis, and what is the differences between Score Map and Correlations. What is important in this figure? How should reader interpret these lines. Why is it better than usual Transformer?\n * Figure 8 needs label of x axis.\n * \u201cwhich can be attributed to the extremely fluctuating series of the dataset, and the patching mechanism of PatchTST may lose focus on specific locality to handle rapid fluctuation. By contrast, our proposed method aggregating the whole series variations for series representations can better cope with this situation\u201d I would need proof for such a claim (even in appendix). First, showing this situation and second showing learning weight for iTransformer that shows it cope with the situation.\n\nProof-read is required for instance:\n * \u201cthis goal can be hardy achieved\u201d I guess there is a typo here and Authors was aiming to write hardly?\n * \u201cSoloar-Energy\u201d- > Solar Energy\n\n\n# Toward a bigger impact\n\n## Additional baselines and larger scope\nNonetheless, I feel that Authors are missing the opportunity to have an even greater impact in the MTSF community (And having my contribution score going from Good to Excellent). Indeed, the proposal is very promising and is on a very important topic, i.e., how to consider variate in MTSF. Are they channel dependent (CD) or channel independent (CI), and if CI are projection perform commonly or individually? However, in my humble opinion, despite doing a good job to present their proposal and results with ablation study and visualization, Authors fail to really position their proposal in the landscape of the above question. It is true that they compare they work to PatchTST, and the CD/CI discussion, but RLinear or RMLP (depending on the dataset) [2] appears to also beat PatchTST. And especially, RLinear with individual projection (one linear layer per variate) similarly to NLinear or DLinear performs better. In addition, RLinear or RMLP use RevIN that was proposed in [1]. The latter show that RevIN helps to handle distribution shift and could be applied to Transformer-based models such as Informer to improve their performance.\n\nTherefore, in my opinion, the paper will have a greater impact if Authors compare their results to these following baselines: revInformer, RLinear and RMLP (and so corresponding papers). And discuss the results and impact of inverse versus CI versus distribution shift handling. This extra step would really be significant for the community in order to have a better understanding of the big picture and what is happening here.\n\nIn addition, the abstract and intro emphasis that Transformer superiority is \u201cshaken\u201d by Linear-based models. However, Authors have only few of such Linear models as baselines. Therefore, adding RLinear and RMLP, especially if iTransformer can beat them, will emphasis such a claim. \n\nAuthors cited [2] in their paper, so they are aware of this work and in my opinion should have included it.\n\nFinally, for the experiments where only P% of the variates are used for training, as the variates are selected randomly, it would be good to perform the experiment with different set of random variates and plot an error plot or box plot. This will further highlight that the set selected randomly is not a specific case. For instance, in Figure 8 (a? left one), we could have for each % of variate the min, max, average among the different sets. Figure 5 could be a boxplot. In addition, Authors should make clear that the set of variates use with iTransformer is the same set used in CI transformer (Figure 5) to avoid any misunderstanding from Reader.\n\n## More results in appendix to ensure proper reproducibility\nFurthermore, in order to target greater impact on the community, I would suggest Authors to add results and visualizations for the other datasets (similar to Table2, Figure 5, and following) in appendix. This would help to make sure results showed applied to all datasets. Indeed, Authors mentioned that PEMS is more difficult so the nature/type/characteristics of the dataset may impact the results and it would be important to mention it and discuss it (even though this point might what Authors expect to do as future work by saying \u201cexplore iTransformers for extensive time series analysis tasks\u201d).\n\nEspecially, I also expect Figure 6 for dataset like Solar energy to not be as good as the others. Because seasonality of Solar energy is high and it strongly depend on the weather, so increasing the lookback window might not be that beneficial.\n\n\n\n[1] https://openreview.net/pdf?id=cGDAkQo1C0p\n\n[2] https://arxiv.org/pdf/2305.10721.pdf"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission632/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698656860346,
        "cdate": 1698656860346,
        "tmdate": 1699635990972,
        "mdate": 1699635990972,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FEsTt6kN2T",
        "forum": "JePfAI8fah",
        "replyto": "JePfAI8fah",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission632/Reviewer_yopU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission632/Reviewer_yopU"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a simple variant of transformer for time series forecasting, where the embedding is applied on each time series and the attention is across each variate. The idea is simple and effective. The improved performance is shown on various real-world datasets. The paper is well-written and easy to read. The idea can be viewed as a principal way to be adopted on various transformer-based architectures. The numerical experiments on different architectures and analysis of representations/correlations greatly enhance the importance of this work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors propose a principal way to apply the transformer-based model for time series forecasting. The idea is simple and effective, and the effectiveness is demonstrated via extensive experiments and ablation studies."
            },
            "weaknesses": {
                "value": "The paper is relatively short of explanation/justification about the effectiveness of such an approach."
            },
            "questions": {
                "value": "1. It would be better to describe the train-validation-test split in experiments, like training in past years and predicting in the next year, as it could be tricky for data pre-processing in time series forecasting and cause data leakage issues.\n\n2. After reading this paper, does the author implicitly assume the heterogeneity of variates is more important than temporal dependency in terms of forecasting? \n\n3. There are some interesting results in Table 3. Could the author comment on the not-so-good performance of the second row (both attention)?\n\n4. A minor one: in the left panel of Figure 6, there is a little jump on the red line from 336 to 720. Any reason why? Do multiple runs help?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission632/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission632/Reviewer_yopU",
                    "ICLR.cc/2024/Conference/Submission632/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission632/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826495303,
        "cdate": 1698826495303,
        "tmdate": 1700552897280,
        "mdate": 1700552897280,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u92XrFEZgp",
        "forum": "JePfAI8fah",
        "replyto": "JePfAI8fah",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission632/Reviewer_hg6h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission632/Reviewer_hg6h"
        ],
        "content": {
            "summary": {
                "value": "This paper explored a new angle to apply Transformer model to the multivariate time-series forecasting problem. Without the modification of the original transformer component, the proposed iTransformer inverted the duties of the self-attention mechanism and the feed-forward network. In iTransformer, the feed-forward network was used for series encoding, while the self-attention mechanism captured the correlation among different variates. The authors conducted experiments on six real-world datasets to evaluate the proposed model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThis paper provided a simple and effective inverted view to improve transformer-based multivariate time-series forecasters.\n\n2.\tCompared with the previous use of Transformer structure (without invert), the iTransformer showed some advantages, including better generalization on unseen variates, and the desired performance improvement over enlarged historical information.  \n\n3.\tExtensive experiments on different multivariate time-series forecasting tasks were conducted for evaluation. The author compared the proposed model with various baselines, along with a comprehensive modal analysis."
            },
            "weaknesses": {
                "value": "1.\tAccording to Table 3 and Table 7, most of the result values are relatively small. This suggests that some marginal improvement may be susceptible to random factors (e.g., iTransformer v.s. PatchTST on ETT and Weather dataset, iTransformer v.s. SCINet on PEMS dataset). Therefore, I recommend reporting the standard deviation under different random runs and adding a significance test to provide further insights.\n\n\n2.\tAlthough the proposed efficient training strategy can reduce the required memory, it would still be better to compare its efficiency with linear models, since recent studies have indicated their advantages in both performance and efficiency."
            },
            "questions": {
                "value": "1.\tCan you please explain why the TiDE results are so different from those reported in their original paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission632/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698843269433,
        "cdate": 1698843269433,
        "tmdate": 1699635990812,
        "mdate": 1699635990812,
        "license": "CC BY 4.0",
        "version": 2
    }
]