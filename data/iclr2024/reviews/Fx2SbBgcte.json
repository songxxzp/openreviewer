[
    {
        "id": "K0CiY5Bw5R",
        "forum": "Fx2SbBgcte",
        "replyto": "Fx2SbBgcte",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission536/Reviewer_eMMP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission536/Reviewer_eMMP"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a practical plug-in-play solution for a personalized T2I video generation model. It consists of three parts, the first pipeline uses lora layer to adapt domain from image pre-training to video training domain due to visual quality and motion blur issues. The second part is a motion module trying to learn the motion prior with a temporal transformer model. The last part is optional, which is a motion-lora design to adapt to a personalized video domain. At inference time, the general-purpose motion module can be used in any pre-trained personalized T2I model to animal the image model. Overall, this paper is well-written and the solution is very practical. Empirically, the method has competitive performance compared to other general-purpose text-to-video models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "\u2013 This paper is trying to solve a novel and practical problem in the text-to-video generation domain. How to turn a pre-trained personalized T2I model into a video model without training is attractive and practically useful.\n\u2013 Qualitatively, this method shows impressive result and demonstrate the generalization ability of the motion module to different T2I model\n\u2013 It also has an extensive ablation study showing the effectiveness of the domain adapter and motion module design choices."
            },
            "weaknesses": {
                "value": "\u2013 Lack of technical novelty, the proposed domain adapter is based on LoRA and the motion module is following standard feature inflation and the architecture is following timesformer."
            },
            "questions": {
                "value": "-- I do have some concerns about the generalization of the motion module. It is not clear to me how reliable to use LoRA for domain adaption, and if the motion module does not generalize well, should we further fine-tune the motion module on the task domain data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission536/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission536/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission536/Reviewer_eMMP"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission536/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698607043623,
        "cdate": 1698607043623,
        "tmdate": 1699635980803,
        "mdate": 1699635980803,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fy70nKV9h0",
        "forum": "Fx2SbBgcte",
        "replyto": "Fx2SbBgcte",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission536/Reviewer_ih9K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission536/Reviewer_ih9K"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces \"AnimateDiff,\" a revolutionary framework designed to infuse motion dynamics into personalized text-to-image (T2I) diffusion models without necessitating model-specific tuning. AnimateDiff features a versatile, plug-and-play motion module at its core, enabling the generation of animated images. The motion module is trained to adaptively learn and apply transferable motion priors from real-world videos, allowing for seamless integration into various personalized T2Is, and fostering the creation of a personalized animation generator.\n\nAdditionally, the paper unveils \"MotionLoRA,\" a novel, lightweight fine-tuning technique engineered for AnimateDiff. MotionLoRA facilitates the adaptation of pre-trained motion modules to new motion patterns, such as diverse shot types, ensuring adaptability with minimal training and data collection efforts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. AnimateDiff empowers personalized text-to-image (T2I) models with animation generation capabilities, eliminating the need for specific fine-tuning. Besides, it demonstrates the efficacy of Transformer architectures in modeling motion priors, contributing valuable insights to the field of video generation. \n\n2. The authors propose \"MotionLoRA,\" an ingenious, lightweight fine-tuning technique enabling pre-trained motion modules to adapt to new motion patterns seamlessly. \n\n3. AnimateDiff and MotionLoRA are rigorously tested against representative community models, academic baselines, like Gen2 and Tune-A-Video."
            },
            "weaknesses": {
                "value": "This paper is well-composed and presents a notable contribution to the field of text-to-image (T2I) diffusion models.\n\nHowever, one aspect that could be further refined or discussed is the dependency of AnimateDiff on the underlying T2I models. It appears that the success of AnimateDiff is inherently tied to the performance and reliability of the existing T2I models it seeks to enhance. Specifically, the methodology might face challenges if the base T2I models struggle or fail to accurately generate images based on user-provided prompts. In such cases, AnimateDiff might encounter difficulties in achieving its animation objectives, potentially limiting the overall effectiveness and applicability of the proposed solution.\n\nIt might be beneficial for the paper to address this dependency, possibly discussing strategies or considerations to mitigate potential challenges arising from limitations in the foundational T2I models. This would strengthen the robustness and adaptability of AnimateDiff, ensuring its broader success and applicability in various T2I contexts and scenarios."
            },
            "questions": {
                "value": "### 1. Domain Adapter Visualization:\nCould the authors provide visualization results that specifically exclude the domain adapter training state? To clarify, I am interested in viewing the outcomes when the domain adapter training (stage 1) is entirely omitted from the process, not just when the parameter $\\alpha$ is set to zero at the inference state.\n\n### 2. Visual Quality of AnimateDiff in Base T2I Models:\n\nRegarding the visual quality, how does AnimateDiff perform when applied directly to the base T2I model that you used to train the motion module on WebViv? I am particularly interested in understanding the performance and visual outcomes of AnimateDiff when integrated with the foundational T2I models, excluding any enhancements or personalizations from external models such as Civitai."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission536/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698666842880,
        "cdate": 1698666842880,
        "tmdate": 1699635980701,
        "mdate": 1699635980701,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uWLzLtVVJU",
        "forum": "Fx2SbBgcte",
        "replyto": "Fx2SbBgcte",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission536/Reviewer_31Uu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission536/Reviewer_31Uu"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces AnimateDiff, a novel pipeline designed to convert static Text-to-Image (T2I) models into animation generators without the need for model-specific fine-tuning. The process uses a plug-and-play motion module named MotionLoRA that learns motion priors from video datasets and can be integrated directly into personalized T2Is to produce smooth animations. Training involves fine-tuning a domain adapter on the base T2I, introducing a motion module, and then adapting this pre-trained module to specific motion patterns using Low-Rank Adaptation (LoRA). Evaluation was performed on various T2I models yielding promising results, demonstrating that a Transformer architecture is effective for capturing appropriate motion priors."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The proposed concept offers a robust and user-friendly plugin to embed motion priors into general T2I models. The model designs are sensible and meaningful. For instance, the domain adapter plays a critical role in mitigating adverse impacts from training data, and MotionLoRA efficiently adjusts to new motion patterns.\n\n- The experiments demonstrate promising applications, including the animation of diverse-style T2I models and controllable dynamic generation using ControlNet."
            },
            "weaknesses": {
                "value": "- In certain animation results, the motion amplitudes are minimal and flickering between frames is noticeable. This suggests room for improvement in the motion prior. These issues may be related to the size of the video dataset and model design, and a thorough analysis of these factors should be included in the limitations section.\n\n- Qualitative comparisons are not enough. Please provide visual results for more cases like in Figure 5.\n\n- For image animation, the identity preservation is poor, in other words, the characters in the animation results are not very similar to the one in the input image."
            },
            "questions": {
                "value": "Why does the scale of the domain adaptor in Figure 6 appear to be visually linked with camera movement?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission536/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission536/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission536/Reviewer_31Uu"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission536/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762335287,
        "cdate": 1698762335287,
        "tmdate": 1699635980623,
        "mdate": 1699635980623,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ttDjPXOGQl",
        "forum": "Fx2SbBgcte",
        "replyto": "Fx2SbBgcte",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission536/Reviewer_Jp9x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission536/Reviewer_Jp9x"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a simple method for generating animations using a customized text-to-image model adapted for video. It innovates by transforming a text-to-image model to produce a sequence of frames, simulating animation. This is achieved through frame-by-frame processing with original diffusion model and the integration of a transformer-based motion module that processes temporal information across patches from all frames. A domain adapter is introduced to overcome the challenge of dataset-specific artifacts, enhancing the quality of the video output.  In the end, the authors also demonstrate the model's ability to efficiently learn new motion given the unified representation including zoom in and rolling.  Quantitative and qualitative comparisons with  a broad spectrum of existing models validated the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1: The paper's foremost strength lies in its elegant balance between simplicity and performance. With only minor change to the base stable diffusion model\u2014principally, the addition of a few transformer layers\u2014the method yields robust quantitative outcomes, positioning it as a strong baseline for text-to-video synthesis within the research community.\n\nS2: The simplicity of the approach belies its technical depth. The integration of new domain adaptation layers effectively mitigates artifact issues typical in video datasets. Furthermore, the use of LORA for streamlined learning of new motions, coupled with transformer layers for temporal data synthesis, represents straightforward yet impactful innovations, all substantiated by thorough ablation studies.\n\nS3: The paper presents good qualitative and quantitative results.\n\nS4: The paper is well written. It combines clear, concise text with instructive visuals, making the methodology and results accessible and understandable.\""
            },
            "weaknesses": {
                "value": "W1: Some modules are not that novel from a technical perspective. E.g. the transformer block is also explored in GEN-2 [1]. LORA and adapter are also commonly used in various personalization papers. \n\nW2: The paper somewhat overlooks a thorough discussion of the method's limitations. Specifically, it would be beneficial for the authors to clarify the learning dynamics of the motion module when pre-trained on the WebVid dataset\u2014whether motions like zoom-ins and rolls require explicit training (using LORA with curated dataset) or can be inferred from textual cues alone. Additionally, a systematic categorization of which motions are effectively learned and which are not could significantly enhance the reader's understanding of the model\u2019s practical applications and boundaries.\n\n\n[1] Esser, Patrick, et al. \"Structure and content-guided video synthesis with diffusion models.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023."
            },
            "questions": {
                "value": "Please see my comments in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission536/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698901381984,
        "cdate": 1698901381984,
        "tmdate": 1699635980524,
        "mdate": 1699635980524,
        "license": "CC BY 4.0",
        "version": 2
    }
]