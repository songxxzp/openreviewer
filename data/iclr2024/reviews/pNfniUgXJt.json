[
    {
        "id": "qRaAYsFtHH",
        "forum": "pNfniUgXJt",
        "replyto": "pNfniUgXJt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7152/Reviewer_XDzL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7152/Reviewer_XDzL"
        ],
        "content": {
            "summary": {
                "value": "The authors developed a white-box Symbolic Distribution Flow Learner (SDFL) based on Monte-Carlo Tree Search (MCTS) with the 2-Wasserstein distance loss for ODE model discovery, given snapshots. Two experiments based on simulated Kuramoto model and reduced single-cell trajectory data have shown the fitting performance of SDFL."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "SDFL provides a solution to learn ODE models with interpretable functional forms by MCTS-based symbolic regression. \n\nWasserstein-based loss for MCTS leads to the theoretical analysis of sample complexity of SDFL.\n\nExperimental results have shown that SDFL outperforms JKONet and TrajectoryNet significantly under small and noisy data settings."
            },
            "weaknesses": {
                "value": "The major concern is that the presentation needs to be improved significantly. For example, the sample complexity proof is mostly based on the reference Shah et al. (2020) while the SDFL takes the UCT-based loss for MCTS, where V(s,a) was not clearly defined in the submission. The provided proof in Appendix B was based on specific loss functional form $h$, which does not seem to be consistent with the text description in Section 3.3. This raises the concern on the validity of the presented theoretical analysis. It is also not really clear how equation (2) is relevant or actually used to derive the presented solution in SDFL. The authors may need to clearly present how MCTS was performed based on Wasserstein distance with stochastic roll-outs. \n\nThe presented results on two small systems are based on the fitting performance. It would be interesting to have more comprehensive evaluation, for example, checking the prediction performance in the second experiment?\n\nThe authors also need to check on the math notations and reference format. For example, the subscripts in equation (7) in Appendix C need to be fixed and it may be better to use $\\theta$ instead of $x$ to be consistent with the notations in Section 5.1."
            },
            "questions": {
                "value": "1. How coefficient regression was done in the Kuramoto example? In Appendix C, if K is fixed at 1/3, which two regression parameters in the model need to fit? \n\n2. For both experiments, are there any additional constraints imposed in MCTS? How can the functional forms along three dimension be perfectly consistent using MCTS? \n\n3. What is the run-time for SDFL? Is it scalable for high-dimensional ODEs? Is it possible to provide a time complexity analysis?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7152/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698684267820,
        "cdate": 1698684267820,
        "tmdate": 1699636847337,
        "mdate": 1699636847337,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xXe31IQch3",
        "forum": "pNfniUgXJt",
        "replyto": "pNfniUgXJt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7152/Reviewer_97PM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7152/Reviewer_97PM"
        ],
        "content": {
            "summary": {
                "value": "Authors propose the combination of Neural ODE with symbolic regression using MCTS and Wasserstein loss to learn dynamical systems under an observational setting, where longitudinal data is measured at random times. The problem of learning the dynamics from the observed snapshots is a type of inverse problem which is in general non-identifiable. \n\nThe method requires a set of symbolic operators to be considered for building the symbolic expressions. The method uses MCTS and the set of provided operators to search for an analytical formula of the ODE system, and the Wasserstein loss is used to measure the discrepancy between the predictions and observations.\n\nThe method was tested on a toy dynamical model and on single cell data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The major strength of this approach is the combination of Neural ODEs and the MCTS component to find symbolic expressions for an ODE system, in which observations of the system come at random times. Being able to learn more interpretable models in this setting is the major strength."
            },
            "weaknesses": {
                "value": "The primary shortcoming of this paper lies in its evaluation. It seems that the primary bottleneck of the method is its scalability since learning symbolic expressions is a complex problem. As such, the method is only suitable for very low-dimensional data. In this respect, the authors only provide a single toy model (3 dims and 15 observations) where the equations are known, and a basic comparison against JKOnet is offered. In this setting, it is expected that the method performs better, as the operators given to the method are part of the equations. While this analysis is adequate as a sanity check, it doesn\u2019t offer much information about the overall capabilities of the method and it's hard to judge the method's effectiveness. Moreover, running times are not mentioned, so it's unclear how the costs of running SDFL compare to JKOnet or TrajectoryNet. Only the Wasserstein error is provided as a metric for comparison.\n\nThe authors also present a second experiment on single-cell dynamics. However, I'm skeptical about the usefulness of this method in this context, given the vast number of dimensions typical of such problems. Here, 20K measurements of genes are available for hundreds or thousands of cells at different time points. This doesn't seem like an ideal setting for this method. To handle the complexity of this dataset, the authors analyzed the data in a very low-dimensional space (d=3), mentioned only in Appendix C, with little detail on how this was accomplished. For instance, with JKOnet, the authors pick the 4K most variable genes and then use the first 20 principal components of PCA on those genes. I assume the authors used a similar approach to reduce from 20K dimensions to just 3. However, even if the authors argue that one of the method's strengths is interpretability, it's hard to see how one can interpret the equations of a 3D embedding of a dynamical system. Unless the authors can offer some interpretation or highlight which genes are the main drivers of changes across timepoints in ways that other methods can't, I question the utility of this example. A more in-depth analysis would be beneficial, illustrating, on small-scale problems, how this method might yield better or more interpretable results than its peers."
            },
            "questions": {
                "value": "The proposed method is interesting and appears to offer some advantages over two other SOTA methods in a very ad-hoc setting. Although the method holds promise for practical application, the authors fall short of demonstrating this in a broader experimental context using realistic yet low-dimensional datasets. Since interpretability (\"white-box\") is one of the major claims, I believe situations where authors must artificially reduce the problem's dimensionality should be sidestepped unless this process improves interpretability. Can you explain in more detail how one can interpret the equations of a 3D embedding of a dynamical system?\n\nThe authors should illustrate the method's performance on low-dimensional datasets across various settings, especially when partial knowledge of the symbolic operators is provided to the algorithm. Emphasis should be placed on interpreting the learned dynamics rather than just showcasing a decrease in Wasserstein loss. Additionally, a thorough description of the competitor methods' configurations is expected to be provided. How comprehensive is the comparison? Did the authors optimize the hyperparameters for the competing methods? this information is not available.\n\nCan the method learn a good approximation of the dynamics if a cosine is provided instead of a sine as part of the input? How would it perform with only partial prior knowledge compared to JKOnet, which doesn\u2019t need this prior knowledge? \n\nThe manuscript lacks a comprehensive comparison in terms of scalability, making it challenging to assess the method's practicality. It seems to be challenging to apply the method even in a low dimensional setting, since the authors resort to a two-step procedure: the initial step identifies the structure, and the subsequent step estimates coefficients. Does this suggest potential scalability issues even with modest sample sizes?\n\nAs acknowledged by the authors, MCTS was used before in the context of symbolic regression, as well as Neural ODEs + Symbolic regression. Some additional refs that can be interesting for the authors are https://arxiv.org/abs/1901.07714 and https://arxiv.org/abs/2202.02435."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7152/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698686627760,
        "cdate": 1698686627760,
        "tmdate": 1699636847222,
        "mdate": 1699636847222,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wCVdU0luWP",
        "forum": "pNfniUgXJt",
        "replyto": "pNfniUgXJt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7152/Reviewer_QgHg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7152/Reviewer_QgHg"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the symbolic learning of probability flows \u2013the flows are originated by a distribution of initial conditions that the ODE has. The solution is based on a Monte Carlo Search Tree solution to solve a given optimization problem, and theoretical guarantees is given for the approximation of the optimization problem given that we only use samples of the trajectories, and for sample complexity in finding a close to optimal solution. Experiments in both different settings are given to show its competitive performance -- one where the \"ground-truth\n ODE is known and one where isn't."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-> An important strength is combining MCTS methods for symbolic regression to the minimization of a Wasserstein-based metric for the probability flows of the system. From what I see in the paper, this is the first time such formulation has been done.\n\n-> It is great that the paper has both theory and experiments, since both are needed due to the nature of the problem being resolved. \n\n-> Experimental results show competitive performance in discovering the underlying dynamics of a Kuramoto oscillator network and of single-cell population dynamics. The method is compared to recent methods in the literature. \n\nI believe the paper is of interest to the community."
            },
            "weaknesses": {
                "value": "The paper has considerable room for improvement in the presentation of its results, both theoretical and experimental, as well as in literature review, and the writing in general. I am expanding on my concerns below.\n\n-> In subsection 3.1 I would imagine that assumptions on regularity or continuity on the vector field $f$ must be enforced, to avoid, for example, solutions that escape in finite-time (will diverge at some finite time before $T$), e.g., $\\dot{x}=x^2$.\n\n-> Is $s$ in Theorem 1 any real number that we must choose to be greater than $d$, or is it some parameter coming from the setting from the sections before Theorem 1?\n\n-> Proposition 1 requires more explanation. First of all, would it be possible to include a mathematical expression that defines what an $\\epsilon$-optimal solution is? Would it be possible to reference the objective that is being optimized? Moreover, two expressions are introduced whose definition is nowhere to be found: \u201cmaximum allowed expression length\u201d and \u201csize of the chosen elementary function set\u201d.\n\n-> Regarding the main algorithm (subsection 4.2): It is mentioned that a \u201cpermutation invariant\naggregation operation (e.g. sum)\u201d is included to reduce the search space to the MCTS. So, how does such invariants help reducing such search-space? I guess such operation is part of a leaf of the MCTS, right? Should it be highlighted in Figure 1 or in Algorithm 1? \n\n-> Something which makes not much sense to me is the fact that the introduction section makes it clear that there is an interest in analyzing network systems and much of the paper\u2019s inspiration comes from it. However, in the next sections until the experimental section, there is not mentioning of network systems playing a particular role in the derivations of the theoretical results or even the algorithm \u2013 perhaps with the exception of the brief mentioning of \u201cpermutation invariant aggregation operation\u201d, which, as I mentioned before, is barely explained and noticeable in the algorithm and the paper itself. Is there any explanation missing regarding the role of having network systems in the technical derivations and algorithm? I don\u2019t really see the connection \u2013as far as how things look to me, all the derivations in the paper and algorithm should work for appropriate general vector fields $f$ that are not necessarily corresponding to network systems of some sort.\n\n-> The description of the Monte-Carlo tree search for symbolic regression is not self-contained in my opinion and requires more explanation. Would it be possible to describe it as an algorithm, with \u201cfor loops\u201d and precise instructions? I need to find answer to questions such as: Are the nodes, especially the first node of the tree, considering both binary and unitary operations? What is the \u201cpredefined measure of accuracy\u201d on step 2 (Simulation)? Is this \u201cback-propagation\u201d step similar to the one in dynamic programming? How many times is step 4 (Selection) repeated? \nI believe that an example with a graphic will greatly help.\n\n-> Why is TrajectoryNet not depicted in Table 1? Its comparison must be added to understand fully the comparison of SDFL.\n\n-> Authors must include a detailed comparison between their solution and \u201cTrajectoryNet\u201d and \u201cJKOnet\u201d, their similarities and difference in their algorithms. This is crucial and currently absent from the paper. For the Kuramoto experiment I would expect TrajectoryNet to perform better since it seems to be based on neural networks, which perform well when there is a lot of data.\n \n-> Why does SDFL have better performance in the simulations (considering that a comparison with TrajectoryNet is absent in the first experiment, as pointed above)? An explanation or even a speculation should be given, since it highlights how different the method done by the authors are with respect to the others. \nAlso, do the other methods also incorporate some permutation invariant properties in their methods? \nAlso, is there perhaps a metric (besides the Wasserstein difference) under which the other methods perform better? What do they use in their own papers to assess accuracy?\n\n-> What happens when you remove the aggregation operation (which is permutation invariant) in both experiments? I want to see how much it really plays a role; this is a very important ablation study whose results must be in the paper. Moreover, the last experiment had nothing to do with networks, so I wonder what role adding such additive operations was important. \n\n-> So, for the Kuramoto oscillator experiment, what is the \u201cdiscovered\u201d ODE according to the paper\u2019s method SDFL? The whole point of the paper is being able to show that the method discovers the symbolic expression of the dynamical system, and this is absent in the paper! It would be nice to see what it says on the other experiment too. \n\n-> The simulations don\u2019t show anything regarding the computational time taken by all the methods. Please, include this. Maybe there is a trade-off between accuracy and performance in your algorithm.\n\n==\n\n-> In the introduction, properties such as \u201cpermutation invariance\u201d and \u201cpartial observability\u201d are mentioned without any reference to what they mean. Their meaning should be stated, at least from a qualitative perspective, as well as why they are important or show up in the problem being studied of network flows."
            },
            "questions": {
                "value": "Please, see the \"Weaknesses\" section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7152/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698706402128,
        "cdate": 1698706402128,
        "tmdate": 1699636847097,
        "mdate": 1699636847097,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kJ7RLRSO9d",
        "forum": "pNfniUgXJt",
        "replyto": "pNfniUgXJt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7152/Reviewer_h3SD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7152/Reviewer_h3SD"
        ],
        "content": {
            "summary": {
                "value": "The article presents a methodology for symbolic regression of ODE-driven probability distributions. The method is white box and the relies on a Wasserstein loss"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Symbolic learning is of interest across the ML community and data science practitioners due to transparency requirements. Furthermore, combining this with a Wasserstein loss is a challenge and, thus a strength of this paper."
            },
            "weaknesses": {
                "value": "Although the article's aim is of great interest, the proposal is not explored in depth for this venue. For instance: \n\n- One motivation for studying symbolic regression (in the abstract) is interpretability. However, no interpretation is provided in the experimental/validation part. Furthermore, though the proposal is referred to as a _white box_ it is never inspected. \n\n- There are some referencing issues: Sec 4 refers to a Fig 4, but there are only three figures in the paper. As mentioned Fig 4.2 does not exist.\n\n- The article dedicates a fair share of its extension to the background: up until page 6 is previous work. Though this can be useful for those unfamiliar with the required background, it leaves little space to discuss the proposal in more detail. For instance, the core contribution of the article (referred to as Technical Approach in Sec 4) is only contained in 1.5 pages. \n\n- Following the above point, one would expect that this is an experimental contribution; however, the experimental validation is rather limited. Both the synthetic and real-world examples are implemented for different amounts of datapoints; however, the reason for a varying-size dataset is not justified, and in particular, it is not stated whether the performance is expected to increase or decrease with more data. Also, there are no error bars. \n\n- Fig 3, which shows the experiments of the real-world dataset, provides no insight into the contribution. In particular, 3b only compares the Euclidean and Wasserstein distances in two cases.\n\n- There is no reference or discussion about the computational cost of the presented strategy"
            },
            "questions": {
                "value": "Please refer to the comments in the previous section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7152/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698726855687,
        "cdate": 1698726855687,
        "tmdate": 1699636846977,
        "mdate": 1699636846977,
        "license": "CC BY 4.0",
        "version": 2
    }
]