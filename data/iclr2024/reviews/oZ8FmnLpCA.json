[
    {
        "id": "wxdSqt5rOc",
        "forum": "oZ8FmnLpCA",
        "replyto": "oZ8FmnLpCA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission120/Reviewer_c4jx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission120/Reviewer_c4jx"
        ],
        "content": {
            "summary": {
                "value": "In this paper,  the authors introduce rectified flow into knowledge distillation and leverage multi-step sampling strategies to achieve precision flow matching. It can be integrated with metric-based distillation methods. The authors offered theoretical analysis which demonstrates that the training objective of FM-KD is equivalent to minimizing the upper bound of the teacher feature map's or logit's negative log-likelihood.  And the authors also offered an online distillation version."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Distillation is an important topic to our community, the proposed method is simple.\n2. Some of the \"same architecture setting\" results are promising.\n3. The write-up is easy to understand."
            },
            "weaknesses": {
                "value": "My major concern is the generalization, given that distillation is a well-defined topic, but this method seems like doesn't work well in heterogeneous architecture settings. Especially when there is a big difference in size between teacher models and student models."
            },
            "questions": {
                "value": "Can you explain or verify why this method can't work well when the gap is big between teacher and student?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission120/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698646151380,
        "cdate": 1698646151380,
        "tmdate": 1699635937428,
        "mdate": 1699635937428,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pBGFYhqtTn",
        "forum": "oZ8FmnLpCA",
        "replyto": "oZ8FmnLpCA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission120/Reviewer_6oRH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission120/Reviewer_6oRH"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new knowledge distillation method FM-KD that combines rectified flow with knowledge distillation. It can be combined with any metric approach and meta-encoder structure, and can be converted to online distillation with minor modifications. The FM-KD method achieves interesting experimental results on the CIFAR-100, ImageNet-1k, and MS-COCO datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "\u2022\tThe FM-KD method is flexible enough to be combined with any metric approach and meta-encoder structure, and can also be converted to an online distillation framework.\n\u2022\tThere are theoretical analyses to support the rationality of the methodological design."
            },
            "weaknesses": {
                "value": "\u2022\tThis paper is similar to DiffKD where it is a combination of generative model and knowledge distillation, except that it replaces the diffusion model with the rectified flow, which has limited innovation.\n\u2022\tThe FM-KD method proposed in this paper modifies the structure of the student network and increases the cost during model inference, which is a shortcoming of a model compression algorithm. The computational cost during inference is not given in the paper, and if refer to Fig. 5, the increase in cost is obvious and there is some unfairness in comparison with other methods. Are there still significant advantages of the FM-KD approach over baseline models that increase the number of parameters to make time-consuming approximations?\n\u2022\tThe experimental performance is not outstanding enough. There is no significant advantage over other knowledge distillation methods in Table 3 of the experiments for object detection, and there are also several tests in the image classification experiments that do not perform as well as other existing methods."
            },
            "questions": {
                "value": "see the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission120/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785430716,
        "cdate": 1698785430716,
        "tmdate": 1699635937273,
        "mdate": 1699635937273,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "orFAg3nKcH",
        "forum": "oZ8FmnLpCA",
        "replyto": "oZ8FmnLpCA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission120/Reviewer_J5cJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission120/Reviewer_J5cJ"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses knowledge distillation through improving feature representation matching from the student model to the pre-trained teacher model. Specifically, the authors extend an existing work DiffKD via replacing vanilla diffusion in DiffKD by Rectified Flow (another existing work which makes diffusion process as a flow having straight paths between any two steps). Experiments on image classification (with CIFAR-100 and ImageNet-1K datasets) and object detection (with MS COCO dataset) tasks are provided to show the effectiveness of the proposed method. Different baselines and setups are considered in experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+  Improving feature representation matching is a critical problem in knowledge distillation research.\n\n+ Leveraging diffusion process to augment feature representation matching for knowledge distillation is interesting, although the insight behind it is not very clear.\n\n+ Image classification and object detection with large scale datasets like ImageNet-1K and MS COCO are considered for experimental comparison."
            },
            "weaknesses": {
                "value": "- The method and presentation.\n\nThis paper attempts to improve feature representation matching from knowledge distillation. The proposed method FM-KD heavily relies on two existing works DiffKD (arxiv 2023) and Rectified Flow (ICLR 2023). Specifically, the authors directly use Rectified Flow to replace vanilla diffusion in DiffKD, making the diffusion of student representation (either feature or logits) to have straight paths between any two steps. In general, FM-KD is rather incremental. The authors try to claim FM-KD as a totally new knowledge transfer framework, which is applicable to different types of teacher-student architectures, different metric-based distillation methods and different representations. However, it is mostly misleading, as DiffKD has already attained this goal when putting it in the context of the authors' viewpoint. \n\nFurthermore, the underlying reason for why applying diffusion process to augment feature representation matching in knowledge distillation in not clear enough. What is the reasonable new technical insight here?   \n\nIn addition, Rectified Flow assumes straight paths between any two steps during the diffusion, so is it reasonable when applying this design to feature representation matching in knowledge distillation? The original purpose of Rectified Flow is for faster diffusion but not more accurate performance. From Table 2 on ImageNet-1K, the proposed method FM-KD merely performs on par or worse than DiffKD. \n\n- The limitations.\n\nThe authors did not discuss the limitations of the proposed method.\n\n- The experiments.\n\nAs the proposed method is closely related to DiffKD, diffusion with Rectified Flow (straight paths) vs. diffusion with vanilla diffusion (non-straight paths), DiffKD and DiffKD+RectifiedFlow should be always used as the baselines instead of others.\n\nFrom Table 2 on ImageNet-1K, the proposed method FM-KD merely performs on par or worse than DiffKD. This raises a critical problem, is it necessary to replace vanilla diffusion (non-straight paths, e.g. DDIM) by Rectified Flow (straight paths)? Why? On the other hand, Rectified Flow assumes straight paths for any two steps during the diffusion, is it reasonable to feature representation matching in knowledge distillation?\n\nHow about the extra cost to the inference with the trained student model? As the architecture of the trained student model is no longer same to the baseline, which violates the basic purpose of knowledge distillation, namely model compression. \n\n- Others\n\nThere is no discussion on related works in the main paper. I noticed that the authors put this part in the Appendix, but this is not proper, to the best of my understanding."
            },
            "questions": {
                "value": "Please refer to my detailed comments in \"Weaknesses\" for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission120/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840026849,
        "cdate": 1698840026849,
        "tmdate": 1699635937121,
        "mdate": 1699635937121,
        "license": "CC BY 4.0",
        "version": 2
    }
]