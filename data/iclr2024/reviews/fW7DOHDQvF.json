[
    {
        "id": "VC9qQQQ5cR",
        "forum": "fW7DOHDQvF",
        "replyto": "fW7DOHDQvF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4492/Reviewer_XSqD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4492/Reviewer_XSqD"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength."
            },
            "weaknesses": {
                "value": "- The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels."
            },
            "questions": {
                "value": "1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It\u2019s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4492/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4492/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4492/Reviewer_XSqD"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4492/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698070637844,
        "cdate": 1698070637844,
        "tmdate": 1700451462602,
        "mdate": 1700451462602,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "w0B0QPfT1n",
        "forum": "fW7DOHDQvF",
        "replyto": "fW7DOHDQvF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4492/Reviewer_S73G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4492/Reviewer_S73G"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates an interesting weakly supervised learning problem called multi-class\nclassification from multiple unlabeled datasets, where only multiple sets of unlabeled data and their\nclass priors (i.e., the proportions of each class) are provided for training the classifier. To tackle this\nproblem, this paper first gives a multi-class extension of a previous work on binary classification from\nmultiple unlabeled datasets. However, this paper says that such a method still has several\ndisadvantages that limit the performance. So this paper further proposes a risk-consistent method that\ncan avoid those disadvantages and maintain theoretical guarantees. Experimental results support the\nclaim of this paper."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- Problem. This paper investigates the problem of multi-class classification from multiple unlabeled\ndatasets, which is interesting problem.\n- Method. To solve the problem, this paper proposes two methods. The first one is a classifierconsistent method, which is a multi-class extension of a previous work on binary classification. The\nsecond one is a risk-consistent method that can address the shortcomings of the first one.\n- Theory. This paper gives theoretical analysis for the two methods proposed in this paper.\n- Performance. From the experimental results, we can find that the classifier-consistent method can\nachieve good performance compared with previous methods, and the risk-consistent method\noutperforms all the methods. Some ablation studies also support the risk-consistent method."
            },
            "weaknesses": {
                "value": "- This paper has proposed two methods (CCM and RCM) to solve the problem and showed that the\nsecond method is better than the first method. I think that there lacks a separate paragraph that is\nspecially for describing the difference between RCM and CCM in detail.\n- This paper should give more explanations for the theoretical findings. For example, there are no\ndiscussions or descriptions on Theorem 3.5 and Theorem 3.7."
            },
            "questions": {
                "value": "- What can we learn from Theorem 3.5 and Theorem 3.7?\n- What is the relationship between the studied problem and the unlabeled-unlabeled learning problem\n[Lu et al. (2019)]?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4492/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4492/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4492/Reviewer_S73G"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4492/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698584537481,
        "cdate": 1698584537481,
        "tmdate": 1699636425170,
        "mdate": 1699636425170,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QuGkgFd7Yh",
        "forum": "fW7DOHDQvF",
        "replyto": "fW7DOHDQvF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4492/Reviewer_3iq5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4492/Reviewer_3iq5"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on a newly proposed weakly supervised learning problem called multi-class classification from multiple unlabeled datasets (MCMU), where only multiple sets of unlabeled data and their class priors are provided in the training process. To solve this problem, this paper proposes two methods, including a classifier-consistent method (CCM) based on a probability transition function and a risk-consistent method (RCM) based on importance weighting. Additionally, theoretical analyses of the proposed methods and experimental results on multiple benchmark datasets are provided."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. This paper studies a newly proposed weakly supervised learning problem called multi-class classification from multiple unlabeled datasets (MCMU), and proposes two effective methods, which could avoid the negative risk issue commonly encountered by previous unbiased risk estimator methods.\n2. Theoretical analyses are provided to show the theoretical guarantees of the proposed methods.\n3. Comprehensive experimental results on multiple benchmark datasets across various settings demonstrate the effectiveness of the proposed methods.\n4. The paper is well organized and well written, which makes it easy to follow."
            },
            "weaknesses": {
                "value": "1. There is a lack of descriptions of Theorem 3.5 and Theorem 3.7. Additionally, could we compare RCM and CCM from a theoretical perspective?\n2. The analysis in section 4.3 is interesting. However, there is a lack of discussion about the observation from Fig. 1. So, could the authors provide more details of why CCM is more robust when few data points are provided?\n3. I noticed that in all experimental settings, the number of sets is greater than or equal to the number of classes. Could the authors provide a discussion where the number of sets is less than the number of classes?"
            },
            "questions": {
                "value": "I have listed the questions in the weaknesses above. Please address them."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4492/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4492/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4492/Reviewer_3iq5"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4492/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699252962128,
        "cdate": 1699252962128,
        "tmdate": 1700654429402,
        "mdate": 1700654429402,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nexTVP9arW",
        "forum": "fW7DOHDQvF",
        "replyto": "fW7DOHDQvF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4492/Reviewer_vMrL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4492/Reviewer_vMrL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes two algorithms (CCM and RCM) to classify multiple classes from multiple datasets using only class proportion information. They provide theoretical guarantees on the accuracy of their methods and show some experimental results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I am not familiar with the related work. e.g. LLP, but the overall proposed method seems interesting and sufficiently novel. The proofs in the appendix seem reasonable and the additional experiments there exhaustive enough."
            },
            "weaknesses": {
                "value": "- See questions\n-\tThere is typo / the sentence got broken up here: \u201cwhere d is a positive integer denotes the input dimension. [k] = \u201c\n-\tThis is not grammatically correct / does not make sense \u201cmultiple-instance learning (Zhou et al., 2009)) could usually have access to weakly supervised labels.\u201d\n\nHowever, the paper is badly written (there is barely any explanation of anything; the appendix in some ways is more clear than the main paper!) and very difficult to understand. This is the biggest flaw of the paper, and makes it difficult to evaluate the paper accurately."
            },
            "questions": {
                "value": "-\tThe author\u2019s state that Section 2.2 that in MCMU has the data generating process P(X|y) P(y|c) P(c) which seems reasonable, but that the generating process of LLP is P(y|X)? That would make sense as the modelling distribution (learning a discriminative function), but not as a data generating process? Also where is the class priors in the data generating process of LLP?\n-\tWhat are the class priors? The proportion of y_i for each of the k classes?\n-\tSection 2.4 is kind of randomly there without any introduction and it could be \u201ccleaned up\u201d. \n-\tWhat is WSL?\n-\tThe proposed method is extremely unclear. The paper needs to explain more of its proposed approach instead of spending large parts of the text comparing to other papers e.g. Lu et al. and describing their approach as an extension / variation of other papers. (Note: this also unintentionally make it sound less novel)\n-\tThe main idea of CCM seems to be that by converting the problem from classifying multiple classes in multiple datasets, it can be simplified into classifying which dataset the X sample comes from? And this equivalency is due to a deterministic transition function T() which is based on \\rho the probability a datapoint belongs to a dataset, \\theta the proportions of each class for each dataset, and \\pi the proportion of each \u201cclass\u201d over all datasets? \n-\tThe problem setup seems very restrictive / artificial. There has to be the same number of classes for each dataset and the classes are \u201cordered\u201d / aligned across multiple datasets? Also while the problem of classifying which dataset a sample belongs intuitively is \u201ceasier\u201d, it is not clear how that solves the problem of which class within which dataset a sample belongs to. Multiplying by the proportions of dataset size and class size will \u201con average\u201d get you probabilities on the latter problem, but this is just a math trick to achieve a bound. It is not clear how this works practically. The experiments seems to indicate it works, it is just extremely unintuitively how / explained badly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4492/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699310877596,
        "cdate": 1699310877596,
        "tmdate": 1699636425003,
        "mdate": 1699636425003,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lRLaDXGb3D",
        "forum": "fW7DOHDQvF",
        "replyto": "fW7DOHDQvF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4492/Reviewer_x6yh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4492/Reviewer_x6yh"
        ],
        "content": {
            "summary": {
                "value": "The authors study the challenge of weakly supervised learning with a focus on multi-class classification from multiple unlabeled datasets. They propose two methods: the Classifier-Consistent Method (CCM), which utilizes class priors and a probability transition function for training, and the Risk-Consistent Method (RCM), which aims to enhance the CCM by ensuring risk consistency through importance weighting to refine supervision during training. The study claims the superiority of these methods, supporting them with comprehensive theoretical analyses for statistical consistency and positive experimental results across multiple benchmark datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper presents a novel approach based on statistical learning theory to solve the problem of multi-class classification from multiple unlabeled datasets and the theoretical guarantees of the estimation error bounds strengthen the claims regarding its effectiveness.\n2. The authors provide a clear and comprehensive description of their methodology, which enables other researchers to reproduce their results."
            },
            "weaknesses": {
                "value": "1. The methods and the corresponding theory presented in the paper are sound and relevant, but the lack of sufficient originality and novelty compared to a previously published work [1,2] could be a limitation. Therefore, the authors should carefully consider these points and take the necessary steps to distinguish their work and demonstrate its original contributions.\n\n   [1] Feng, L., Lv, J., Han, B., Xu, M., Niu, G., Geng, X., ... & Sugiyama, M. (2020). Provably consistent partial-label learning. Advances in neural information processing systems, 33, 10948-10960.\n\n   [2] Kobayashi, R., Mukuta, Y., & Harada, T. (2022). Learning from Label Proportions with Instance-wise Consistency. *arXiv preprint arXiv:2203.12836*. \n\n2. Employing the direct outputs of a network to estimate the posterior probabilities $p(y=j\u2223x)$ can be imprecise, particularly under the extreme weak supervision scenario where only class prior probabilities serve as supervisory signals. In such a setting, the network predictions may not align well with the true posterior distributions, leading to suboptimal performance.\n\n3. In Theorems 3.5 and 3.7, the assumptions regarding the Lipschitz continuity of the loss function present an indeterminable strength which could affect the assessment of the model's robustness. Furthermore, the generalization error bound incorporates a summation over $k$ categories of the Rademacher complexity, which may result in a rather loose bound. \n\n4. The use of subscripts $i$ and $j$ appears to be somewhat confusing and may lead to difficulty in understanding for the reader. Specifically, the subscripts switch between $i$ and $j$, which could introduce ambiguity in distinguishing between the elements associated with the k-dimensional vector $\\eta(x)$ and the m-dimensional vector $\\bar{\\eta}(x)$. Clarity in the mathematical notation is crucial for the precise communication of such theoretical results."
            },
            "questions": {
                "value": "Seen weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4492/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4492/Reviewer_x6yh",
                    "ICLR.cc/2024/Conference/Submission4492/Senior_Area_Chairs"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4492/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699366759737,
        "cdate": 1699366759737,
        "tmdate": 1700452008917,
        "mdate": 1700452008917,
        "license": "CC BY 4.0",
        "version": 2
    }
]