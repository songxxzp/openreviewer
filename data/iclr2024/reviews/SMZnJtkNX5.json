[
    {
        "id": "lGzSrd2Kpj",
        "forum": "SMZnJtkNX5",
        "replyto": "SMZnJtkNX5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9476/Reviewer_yZKs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9476/Reviewer_yZKs"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a temporal parallelization method for SNNs that can accelerate SNNs on both single or multi GPUs with up to 40x acceleration."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The training of deep SNNs requires much more time and memory consumption. Thus, it is meaningful to explore the acceleration of simulating SNNs on GPUs."
            },
            "weaknesses": {
                "value": "The details of the proposed method are not described clearly in this paper. To make matters worse, the Supplementary Material is the same as the main paper."
            },
            "questions": {
                "value": "In Figure 3, how the propagation of the spiking neuron layer is paralleled? I assume that V[t] is still computed in serial. For an input sequence with length T, the time complexity is still O(T).\n\nIn section 3.2, the authors claim that the SNN accelerated by pipeline in multiple GPUs may have a faster speed than using a single GPU. However, I am afraid that the communication time between GPUs will be the bottleneck. According to my experience, the communication time is much longer than any other time. Thus, the pipeline method is seldom used in training, and the Distributed Data Parallel is the mainstream.\n\nIn Table 1, the time of SpikingJelly with or without CuPy does not have much difference, which is against my experience. \n\n\nWhere is Figure 4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9476/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698374492293,
        "cdate": 1698374492293,
        "tmdate": 1699637191815,
        "mdate": 1699637191815,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "j74IqJg9Zj",
        "forum": "SMZnJtkNX5",
        "replyto": "SMZnJtkNX5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9476/Reviewer_VQqA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9476/Reviewer_VQqA"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an SNN-based acceleration strategy with parallelized temporal computation that supports both single and multiple GPUs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Largely improved SNN inference speed compared to the previous implementation. \n\nCompatibility with both single and multi-GPU processing."
            },
            "weaknesses": {
                "value": "**W1:** Figure 4 is missing. \n\n**W2:** The biggest bottleneck of this work is that the accuracy benchmarking is completely missing in the paper. I understand the inference speed-up is very important in SNN, but I cannot see the reason why the paper chose not to report the accuracy. It is important to verify the proposed implementation with different SNN model architectures. E.g.. ResNet vs. VGG. \n\n**W3:** It seems like the implementation can only accelerate the inference rather than training, which I think is not powerful enough."
            },
            "questions": {
                "value": "Please refer to the Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9476/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9476/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9476/Reviewer_VQqA"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9476/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698634225644,
        "cdate": 1698634225644,
        "tmdate": 1699637191706,
        "mdate": 1699637191706,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3CXrIzDWRt",
        "forum": "SMZnJtkNX5",
        "replyto": "SMZnJtkNX5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9476/Reviewer_4kmC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9476/Reviewer_4kmC"
        ],
        "content": {
            "summary": {
                "value": "This paper trys to use a temporal parallelization method to accelerate the propagation dynamics of SNNs on GPUs. The feature it claims is a cross-timestamp acceleration of LIF model. With the Leaky Integrate- and-Fire model as a test case, the CUDA-based implementation achieved 5\u00d7 to 40\u00d7 acceleration on the A100 GPU."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The author proposed temporal parallelisation method tailored for universal SNN units on single and multiple GPUs. It supports both CUDA and JAX frameworks."
            },
            "weaknesses": {
                "value": "1. The motivation behind this paper lacks clarity. Spiking Neural Networks (SNNs) are not typically intended for deployment on GPUs, meaning that a GPU is not the most suitable platform for SNN deployment. Without a demonstration of the clear benefits of utilizing GPUs for SNNs deployment as opposed to other platforms, the paper's underlying motivation remains unconvincing.\n\n2. How does this paper leverage GPU to implement true spiking mechanism? It is not clear or discussed. Is it only considering simulating the mechanism of the Leaky-Integrate-and-Fire model behavior? Plus, there\u2019s no true spiking signals in GPU, addressing the temporal information is not really Spiking implementation. This paper doesn\u2019t clarify the basic concept. \n(In some sense, parallelizing temporal information is possible, but there\u2019s conversion between spiking temporal information and the muti-bit digital temporal information for GPU? Then what\u2019s the conversion cost?)\n\n3. Although this paper is based on the computational model of LIF, but it does not clearly describe how training and inference is done, respectively. Training an SNN is hard, and it is not discussed at all in this paper, so it\u2019s only about inference, or even, the simulation of inference?\n\n4. Last but not least, most importantly, this paper does not provide any AI-model based results, such as accuracy, performance, respective speed-up, etc, let alone thorough analysis based on the comparison of results. The only result is a table based on a single-layer toy model? For multi-GPU, where is Fig. 4, seems this paper is incomplete?"
            },
            "questions": {
                "value": "1. How SNN neuron spiking behaviour described in Eq.1 and Eq. 2 reflected in GPU?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9476/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742197681,
        "cdate": 1698742197681,
        "tmdate": 1699637191581,
        "mdate": 1699637191581,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aOhqxiVMK8",
        "forum": "SMZnJtkNX5",
        "replyto": "SMZnJtkNX5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9476/Reviewer_81ci"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9476/Reviewer_81ci"
        ],
        "content": {
            "summary": {
                "value": "The authors describe a method and code for accelerating spiking neural\nnetworks (SNNs) on GPUs. They first claim to parallelize the temporal\nmembrane integration of a layer in an SNN and secondly divide the\ncompute onto multiple GPUs. They provide a template how to implement\nit in common ML frameworks such as JAX. Finally, they show that their\nimplementation outperforms other toolboxes."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The implementation seems to outperform current toolboxes in terms of runtime.  The authors show that this layer-first\napproach gives a considerable speedup on GPUs due to reduced memory movement."
            },
            "weaknesses": {
                "value": "While better implementation of simulating SNNs using GPUs has its merits,\nthe task is merely a software\nengineering task. The paper does not add any value in terms of novel\ninsights. This is in particular true since the \"temporal\nparallization\" argumentation is indeed a misnomer as the temporal dimension is *not*\ncomputed in parallel, but instead time of one layer is simply handled\nwithin one GPU kernel (but still computed sequentially if I understand it correctly).\n\nIf one wanted to design a custom CUDA kernel and would assume that\nonly feed-forward layers are allowed, this would be just the standard\napproach to do, I don't see any innovative aspects here. In\nparticular, equation 3 is just a re-writing (inserting) of $x^{(t-1, n)}$,\nthere is no \"transformation\" I can see. Note that $v_i^{(t, n)}$ still\nis a function of previous times, $v_i^{(t-1, n)}$. All what is done is to\ncompute all time steps per layer first before sending the full output\nspike train to the next layer. This will obviously not work for\nrecurrent SNNs. \n\nAlso, the authors do not even provide their own optimized CUDA kernel\n(which would have more merit), but instead rely on generic toolboxes\nlike JAX. The code listings do not provide any details of the\nimplementation and are more like a tutorial how to use it.  \n\nOverall, while the implementation might be useful as it improves the\nruntime of SNNs compared to the (apparently very non-optimized)\nstandard SNN packages, the paper does not provide any new scientific\ninsights. It is also not discussed that the approach works only for\nfeed-forward SNNs. Moreover, the presentation of \"temporal\nparallelization\" is not correct (as it just points to a fused\nsequential CUDA-kernel). Finally, the layer-first approach (fusing\nkernels to reducing memory operations) and dividing the compute for\nmultiple GPUs are rather standard practices in GPU programming in\ngeneral and not novel enough for a research oriented conference\ncontribution in my opinion."
            },
            "questions": {
                "value": "*  In Eq 3: $x_i^{(t, n)}$ should be $x_i^{(t, n-1)}$"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9476/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9476/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9476/Reviewer_81ci"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9476/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772724510,
        "cdate": 1698772724510,
        "tmdate": 1699637191448,
        "mdate": 1699637191448,
        "license": "CC BY 4.0",
        "version": 2
    }
]