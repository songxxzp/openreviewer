[
    {
        "id": "R0AAw0b8Lv",
        "forum": "n39ilTxSDY",
        "replyto": "n39ilTxSDY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1687/Reviewer_iucu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1687/Reviewer_iucu"
        ],
        "content": {
            "summary": {
                "value": "Author introduce Ditto -- a framework designed to enhance the efficiency of secure inference in Transformer models using multi-party computation (MPC). It incorporates MPC-friendly quantization and a quantization-aware distillation procedure to both reduce computational overhead and maintain model utility. Empirical tests on Bert and GPT2 models show that Ditto significantly outperforms existing solutions, being 3.14 to 4.40 times faster than MPCFormer and 1.44 to 2.35 times faster than PUMA, with negligible loss in utility."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The authors present a solution that addresses multiple bottlenecks in secure multi-party computation (MPC) for Transformer models. For example, challenges like handling non-linear functions and dynamic quantization in an MPC context. They also offer a solution such as modified dyadic quantization and static dyadic quantization for these issues. \n\n* The paper highlights and addresses the often-overlooked disconnect between the expertise in machine learning and multi-party computation. For example, it effectively integrates best practices from MPC-friendly quantization and type-conversion primitives, thereby enhancing end-to-end secure inference efficiency.\n\n* The authors show empirical evidence that their contributions are valid. They compared Ditto against existing state-of-the-art frameworks like MPCFormer and PUMA, the authors make a compelling case for the performance advantages of their approach."
            },
            "weaknesses": {
                "value": "* The paper acknowledges that both Ditto and MPCFormer exhibit noticeable utility drops in Bert tasks when employing ReLU approximation for Softmax. They offer Quad approximation for GeLU to maintain a balance between utility and efficiency, but this limitation may constrain the applicability of the framework for tasks where such approximations are not tolerable.\n\n* The paper in general is hard to read and require additional proof-reading. I would recommend making the paper to be easier to read by highlighting important concepts, introducing figures that support main results, and describing contributions and future work."
            },
            "questions": {
                "value": "What are the primary limitations of using more aggressive quantization methods, as mentioned in the future work section, in the context of secure inference? Would it significantly affect model utility, or are there other challenges like security vulnerabilities that need to be addressed?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1687/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698428461163,
        "cdate": 1698428461163,
        "tmdate": 1699636097097,
        "mdate": 1699636097097,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QK95KzJqrE",
        "forum": "n39ilTxSDY",
        "replyto": "n39ilTxSDY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1687/Reviewer_MwvF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1687/Reviewer_MwvF"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes MPC primitives to support quantization-aware private inference. Moreover, the authors propose a MPC-friendly quantization-aware distillation to retrain the model utility."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper targets an important problem in private inference.\n\n2. The proposed type conversion protocols are creative solutions to a key challenge in quantization-aware secure inference.\n\n3. Extensive evaluations analyzing efficiency, utility, scalability, and communication costs and latency on factors like sequence length and batch size."
            },
            "weaknesses": {
                "value": "1. Lack of comparison to the latest related work."
            },
            "questions": {
                "value": "How would the proposed DITTO be compared with Iron [1]?\n\n[1] Hao, Meng, et al. \"Iron: Private inference on transformers.\" Advances in Neural Information Processing Systems 35 (2022): 15718-15731."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1687/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1687/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1687/Reviewer_MwvF"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1687/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821348217,
        "cdate": 1698821348217,
        "tmdate": 1699636096995,
        "mdate": 1699636096995,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "14u4QJcuuF",
        "forum": "n39ilTxSDY",
        "replyto": "n39ilTxSDY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1687/Reviewer_sjar"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1687/Reviewer_sjar"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a framework for quantization-aware secure Transformer inference."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ MPC-friendly Quantization-Aware Distillation.\n+ MPC primitives for scale down and scale up.\n+ Comparison with SOTA."
            },
            "weaknesses": {
                "value": "- Distillation is widely used in MPC-based secure inference works.\n- It seems limited contributions of MPC protocols."
            },
            "questions": {
                "value": "1. Does the  Downcast protocol have a probabilistic error? What is the difference compared with the truncation of SecureML?\n2. In Upcast, what distribution is $r$ sampled from? How to ensure the input is positive?\n3. Could you provide the theoretical or experimental advantages of the proposed Downcast and Upcast protocols compared with SOTA?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1687/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833120606,
        "cdate": 1698833120606,
        "tmdate": 1699636096908,
        "mdate": 1699636096908,
        "license": "CC BY 4.0",
        "version": 2
    }
]