[
    {
        "id": "Pe3LXo3san",
        "forum": "rAX55lDjtt",
        "replyto": "rAX55lDjtt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7454/Reviewer_wWds"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7454/Reviewer_wWds"
        ],
        "content": {
            "summary": {
                "value": "Acoustic prompt tuning (APT) is proposed in this paper, which integrates an Audio-MAE encoder, a Q-Former aligner, and a Vicuna LLM to implement a multimodal LLM that can hear and understand audio events. Three auxiliary loss functions are used to improve the training of the Q-Former aligner, including audio-text matching (ATM), audio-grounded text generation (AGTG) and audio-text contrastive (ATC). APT-LLM is evaluated on audio tagging, audio captioning, few-shot audio classification, audio-language reasoning and zero-shot audio-visual question-answering tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper investigates the integration of LLM with an audio encoder to empower it with auditory abilities, which is currently an understudied research problem. \n\n2. The method is evaluated on a number of audio-event-related tasks evaluating the abilities of audio-event understanding and reasoning. \n\n3. The presentation of the paper is very clear, including a precise limitation section that elaborates on the scope of the approach.\n\n4. In-context learning for audio-event classification is investigated."
            },
            "weaknesses": {
                "value": "1. The authors claimed \"Diverse audio-related tasks are formulated in a sequence-to-sequence manner without imposing any constraints on input sequences\", which is not true since the input sequence could not be speech or music, as claimed in the limitation section. There might also be a maximum input sequence length imposed by the use of the Audio-MAE encoder, and the Q-Former aligner. \n\n2. The authors claimed that one of the key contributions of the paper is: \"this is the first attempt to unify fully-supervised learning with in-context learning.\" It is not clear to me what this means precisely. The authors need to make the motivation and benefits of combining multitask training with in-context learning clear. \n\n3. The performance of the proposed approach is not satisfying based on Table 2, in particular on audio tagging on the AudioSet dataset.\n\n4. The model is not tuned to follow instructions and can only perform a small of tasks, which makes the use of LLM less reasonable. \n\n5. It sounds less sensible to me to use a standard Q-Former to convert audio input sequences into a fixed number of 32 tokens. Generating a fixed number of tokens is a good choice for images with fixed sizes, but not so much for audio input sequences due to their variable input lengths. \n\n6. It is not clear what are the benefits of having the ATM, ATC, and AGTG multitask training on the Q-Former aligner."
            },
            "questions": {
                "value": "1. What's the size of the Vicuna LLMs used in the paper? \n\n2. What are the strengths of APT-LLM compared to other recent methods, such as LTU? It seems APT-LLM has worse performances based on Table 1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7454/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698663464039,
        "cdate": 1698663464039,
        "tmdate": 1699636895628,
        "mdate": 1699636895628,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9lXAnRq4tH",
        "forum": "rAX55lDjtt",
        "replyto": "rAX55lDjtt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7454/Reviewer_cBEu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7454/Reviewer_cBEu"
        ],
        "content": {
            "summary": {
                "value": "This paper describes Audio Prompt Tuning (APT), a new model architecture that interleaves representations of audio with embeddings of text tokens in order to enable joint learning of text-generation tasks that are conditioned on one or more audio clips and related text prompts. When coupled with a pretrained LLM, APT forms the APT-LLM model architecture, capable of performing a variety of audio-text tasks including audio tagging, audio captioning, and few-shot audio classification. In addition, the authors created a new type of tasks called natural-language audio reasoning (NLAR), in which the model is tasked with answering natural-language questions concerning the relations between two audio clips (but see my concerns below)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1. Proposing a novel way of representing an arbitrary number of audio clips and their associated textual information that interleaves representations of the audio clips with text token embeddings. This significantly enhances the versatility of the model, giving it abilities to perform a variety of audio-text bimodal tasks such as tagging, captioning, classification, as well as tasks that involve more than one audio clip such as NLAR.\nS2. A multi-task training recipe for the APT model that encompasses different cross- and self-attention mechanisms between the audio input and associated text input."
            },
            "weaknesses": {
                "value": "W1. The levels of accuracy achieved by APT-LLM on the audio tagging and audio captioning are unfortunately slightly disappointing (Table 1. Section 4.2). They fall below the accuracy levels of domain-expert models, by a noticeable margin in the case of audio tagging. The authors argue that this has to do with the open-ended nature of APT-LLM's output, which puts APT-LLM at a disadvantage compared to the closed-ended nature of the previous models audio tagging models. However, in the context of visual benchmarks such as ImageNet, open-ended text-visual models such as CLIP have previously outperformed domain expert models such as various types of CNNs. A similar under-performance can be seen in the few-shot audio classification task (Table 2), especially for the 12-way case. The authors try to attribute this poor performance to the length of the input, but they did not explain why the model couldn't be configured and trained with a sufficiently long input context window in order to accommodate this task and thereby address this limitation.\nW2. The section on integration of BLIP-2 and APT-LLM (Section 4.4) is not easy to follow for readers not familiar with the audio-visual tasks. Not enough background information is provided about either the BLIP-2 model architecture or the audio-visual learning task that the author performed evaluation on. This unfortunately makes this part of the claimed contribution less convincing.\nW3. The natural-language audio reasoning (NLAR) dataset is constructed from a subset of the Clotho-AQA dataset by utilizing OpenAI's ChatGPT-turbo API. The NLAR dataset suffers from two issues: 1) the authors did not explain the criteria by which the subset was selected from Clotho-AQA, and 2) the authors did not describe how the quality of the examples generated by ChatGPT-turbo was controlled. Presumably, some sort of manual inspection was required to ensure that the LLM-generated test examples are correct."
            },
            "questions": {
                "value": "Q1. The diagram in Figure 1 seems to miss a part between the input text and the audio aligner for tokenizing the input text and looking up the embeddings for the input text for the aligner's use. This figure needs clarification.\nQ2. How does the frozen LLM receive the audio-text juxtaposed embeddings? Its built-in embedding lookup layer should have been removed so that the mixed audio-text embeddings can be passed directly as the input to the LLM. This begs the question of why the authors decided to freeze the LLM. It seems that by freezing the LLM, the burden and opportunities for learning are limited to the audio encoder and the audio aligner. Is it possible allowing the weights of the LLM to change and adapt during training would lead to better learning outcomes by the entire APT-LLM model? The authors didn't describe any hyperparameter tuning processes.\nQ3. Equation (1) and Equation (4) seem to have inconsistency with Figure 1. M_{\\theta} should take an additional input (the text) according to the diagram in Figure 1.\nQ4. How many examples does the NLAR dataset contain?\nQ5. Figure 3 in the appendix lacks legends. In addition, the y-axis on the right-hand side is unlabeled. As a result, it is unclear what are plotted by the blue and orange curves, which makes this figure hard to read."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7454/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698853751622,
        "cdate": 1698853751622,
        "tmdate": 1699636895495,
        "mdate": 1699636895495,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1ew3ykO2YV",
        "forum": "rAX55lDjtt",
        "replyto": "rAX55lDjtt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7454/Reviewer_TGqa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7454/Reviewer_TGqa"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Acoustic Prompt Turning (APT), an acoustic adapter that extends large language models (LLMs) and visual language models (VLMs) to the audio domain. APT uses an instruction-aware aligner to acquire acoustic embeddings from audio feature maps, allowing it to handle diverse audio-related tasks in a sequence-to-sequence manner. The paper demonstrates the effectiveness of APT-LLMs through various tasks and introduces a novel audio reasoning task. It also shows that APT can extend frozen VLMs to the audio domain, yielding promising results in the audio-visual understanding task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The concept of APT is innovative and presents a new direction for extending LLMs and VLMs to the audio domain without compromising their domain-specific capacity. This also provides evidence that encoding sound clips as word tokens is an efficient approach to adapt LLM/VLM to the audio domain.\n- Introducing the natural language audio reasoning task is a creative way to evaluate model's ability to understand, compare, and summarise two audio clips.\n- The paper does a good job comparing its work with existing models, providing a clear context for the novelty and utility of APT.\n- There are significant performance improvements across audio-visual baselines, highlighting the effectiveness of APT in the audio-visual domain. The performance on most of the open-ended tasks was good."
            },
            "weaknesses": {
                "value": "- Performance on Certain Tasks: Despite the novelty of the idea, the performance of APT-LLMs on the close-ended datasets - ESC50 (few shot classification) and AudioSet (captioning) tasks is not competitive compared to state-of-the-art, task-specific models. This indicates a need for improvement in these areas.\n- Handling of Errors: It is unclear how the model handles potential discrepancies or errors in the labeled examples used for in-context learning. This raises questions about the robustness of the model in real-world scenarios where such issues are common."
            },
            "questions": {
                "value": "- How can APT's performance on tasks like ESC50 and AudioSet be improved to be more competitive with task-specific models?\n- How does the instruction-aware aligner in APT handle different audio signals, especially those with complex characteristics?\n- How can the model be improved to handle discrepancies or errors in the labeled examples used for in-context learning?\n- How does APT perform for more specific types of audio (e.g., music, notes...)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7454/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699191074769,
        "cdate": 1699191074769,
        "tmdate": 1699636895379,
        "mdate": 1699636895379,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4WqGk3eDeJ",
        "forum": "rAX55lDjtt",
        "replyto": "rAX55lDjtt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7454/Reviewer_w4rk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7454/Reviewer_w4rk"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Acoustic Prompt Turning (APT), an acoustic adapter that extends large language models (LLMs) and visual language models (VLMs) to the audio domain. Existing models have limited applicability to audio tasks. \n\nAPT uses a multi-task learning framework and an instruction-aware aligner to generate fixed acoustic embeddings from audio feature maps. Various audio-related tasks are formulated in a sequence-to-sequence manner, allowing APT to be trained without constraints on input sequences. Experimental results show that LLMs coupled with APT achieve competitive performance compared to expert models across different tasks. APT is also evaluated on a novel audio reasoning task and shown to extend frozen VLMs to the audio domain, even without fine-tuning on audio-visual datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper pioneers the exploration of audio-text language modeling, particularly in addressing the format constraint that previous work faced. In the past, there have been a few attempts at audio-text foundation modeling; however, they were limited to the input format [audio, Q, A], excluding support for other practical tasks. As a result, these models were unable to exhibit the same level of intelligence as popular language models like GPT-4. This paper overcomes this limitation by considering audio as a prompt in language modeling, enabling it to perform various tasks.\n\n2. Additionally, the paper introduces a novel task called audio reasoning and provides a dataset that will prove invaluable for future research. This direction is highly significant as existing datasets often prove too simplistic for large models, failing to capture the complexity and intelligence required in real-world audio modeling scenarios. By introducing a more challenging audio reasoning task and accompanying dataset, the paper paves the way for the development of smarter and more sophisticated audio models that better align with real-world demands."
            },
            "weaknesses": {
                "value": "1. When examining the experimental results, it is apparent that the proposed model does not perform as strongly as the specific model on certain tasks. For instance, the baseline for AudioSet classification is around 47, whereas this paper only achieves 14.7. If a foundation model lags behind a specific model, its technical significance is limited. Furthermore, the model does not demonstrate sufficient strength in the audio caption task, which should ideally be robust considering the capabilities of the LLM as a decoder. \n\nThe authors should provide evidence to support the advantages of foundation models. If a foundation model is merely capable of performing multiple tasks, it may not be sufficient. It is important for the authors to demonstrate the unique strengths and benefits of the foundation model compared to other approaches. This could include showcasing improved performance, increased efficiency, or enhanced generalization across tasks. Providing such evidence will help establish the significance and value of the foundation model in the audio domain.\n\n2.Upon examining the approach outlined in the paper, it becomes evident that it is similar to  existing speech-language models. Initially, when reading the title of the paper, I anticipated that the method would differ significantly from previous models such as AudioPaLM and SpeechLM. However, upon closer inspection, it appears that the method aligns closely with these established models, but changing the input from speech to audio. Given the current era of large language models (LLMs), my expectations for groundbreaking innovations were not high."
            },
            "questions": {
                "value": "1. How to gurantee the audio reasoning dataset qualilty? It is created by ChatGPT and may need some human participants for quality check."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7454/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699197815413,
        "cdate": 1699197815413,
        "tmdate": 1699636895270,
        "mdate": 1699636895270,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KBeZI0B3Rs",
        "forum": "rAX55lDjtt",
        "replyto": "rAX55lDjtt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7454/Reviewer_u172"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7454/Reviewer_u172"
        ],
        "content": {
            "summary": {
                "value": "The authors study the work of empowering large language models with audition capabilities. However, the idea and the presentation of this paper is very similar to BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (https://arxiv.org/pdf/2301.12597.pdf), we can see that the Figure 2 in this paper is very similar to Figure 2 in BLIP-2, the losses and masking strategies and so on are very similar to BLIP-2. To empower large language models with audition capabilities, the authors should propose some new idea. In the experiments, the authors don't compare their work with some well-known approaches such as SpeechGPT and so on."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem is interesting."
            },
            "weaknesses": {
                "value": "1. The Figure 2 in this paper is very similar to Figure 2 in BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (https://arxiv.org/pdf/2301.12597.pdf), just replace the word image to audio. Thus, the novelty of this paper is very limited compared with BLIP-2. The Figure 1 in this paper is similar to Figure 3 in BLIP-2. The authors use the exactly three loss in BLIP-2, matching loss, contrastive loss and gounded text generation loss, with the exactly three mask strategies, and the authors use the learnable query. \n\n2. Model Capability: Unlike methods such as SpeechGPT, the approach presented in this article limits the use of speech modality to input only, preventing the synthesis of speech output. This results in a model lacking genuine speech interaction capability.\n\n3. Experimental Comparisons and Results: The performance of the method falls below the expected standard, and there is a notable absence of performance comparison with established works such as SpeechGPT."
            },
            "questions": {
                "value": "1. The audio modality is different from image modality. Why the authors use the exactly thress losses and three mask strategies from Figure 3 in BLIP-2 ?\n2. Apart from the novelty, in the experiments, the authors don't compare their model with some well-known works, such as SpeechGPT and so on.\n3. For the audio modality, speech interaction is important, why not the authors focus on this part, which is different from image modality."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7454/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699327214689,
        "cdate": 1699327214689,
        "tmdate": 1699636895162,
        "mdate": 1699636895162,
        "license": "CC BY 4.0",
        "version": 2
    }
]