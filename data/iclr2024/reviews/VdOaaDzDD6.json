[
    {
        "id": "1GRVCbUP2x",
        "forum": "VdOaaDzDD6",
        "replyto": "VdOaaDzDD6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5931/Reviewer_hR48"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5931/Reviewer_hR48"
        ],
        "content": {
            "summary": {
                "value": "In the setting here, playing a bandit arm generates a hidden reward though the decision maker can observe the ordering of the average reward of each arm cumulated up to this point (but not of the empirical rewards themselves). The paper then shows that in the stochastic version of this setting, no algorithm can achieve logarithmic regret. Furthermore, the authors introduce an algorithm coming arbitrarily close to this lower bound (logarithmic regret) and an algorithm achieving regret scaling as $O(\\sqrt{T})$ in the instance independent setting. Finally, a few numerical experiments contrast the performance of the algorithm proposed here using the default Explore-and-Commit algorithm as a baseline."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Proposes an interesting setting that seems relevant to preference learning in real systems as the only feedback the algorithm receives is a ranking of the unobservable average empirical rewards. \n\nProposes a method to overcome a non-trivial difficulty posed by this setting (the inability to observe the actual rewards of the arms) and shows the instance-dependent and instance-independent settings need to be treated separately.\n\nProvides optimality proofs for the algorithms and numerical experiments to showcase the proposed algorithms' performance.\n\nThe paper is well structures and the writing is clear and benefits from examples of how the mechanics of the setting work."
            },
            "weaknesses": {
                "value": "I would have liked to see more solid justification of the practical relevance of the setting: an experiment on a real world dataset for instance, and/or a more crystallised example of a real world setting that is accurately modelled by this setting.\n\nI would have liked to see a more detailed description of the Explore-then-Commit implementation used as a baseline. I believe the generic EC algorithm requires a grid as input, what is the choice of this parameter here?\n\nThe proposed algorithm (DREE) is not entirely surprising. In light of not knowing neither the optimality gap $\\Delta^*$ nor the accumulated rewards, we would not know how tight the confidence interval corresponding to each arm would need to be. Hence, no matter how many times we explore relative to $\\log(T)$, we can never be sure we have explored enough. Hence it makes sense that we need to play at a \"barely\" super-logarithmic rate, in order to ensure we eventually actually discover the best arm (regardless of how small $\\Delta^*$ is)."
            },
            "questions": {
                "value": "Regarding the experimental setup:\n- Can you offer more details regarding the EC implementation and parameters for the sake of reproducibility?\n- DREE($\\delta + 1$) performs worst in Figure 1a. but best in the rest of the experiments. Do you have any insights on how would someone implementing your algorithm know in which of the regimes they might find themselves in and how someone can go about choosing the right $\\delta$ at design-time?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5931/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5931/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5931/Reviewer_hR48"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5931/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698595126481,
        "cdate": 1698595126481,
        "tmdate": 1699636631279,
        "mdate": 1699636631279,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "peZMs74dGl",
        "forum": "VdOaaDzDD6",
        "replyto": "VdOaaDzDD6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5931/Reviewer_qw3s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5931/Reviewer_qw3s"
        ],
        "content": {
            "summary": {
                "value": "Unlike traditional bandits or dueling bandits, the authors introduce a variation of ranking based feedback where the learner observes a continuously updated leaderboard of arms, without direct numerical feedback. They study the problem of designing noregret algorithms with ranking feedback both in the stochastic and adversarial settings, showing certain algorithmic theorems do largely transfer over."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors do a good job of exhaustively considering the most relevant settings of stochastic vs adversarial, instance dependent vs independent, providing interesting upper and lower bounds, with comparison to the regular bandit setting. The authors do a relatively clear job of presenting their novel setting and give some insights into previous works and how their setting differentiates from previous works."
            },
            "weaknesses": {
                "value": "The main weaknesses of the paper stem from the lack of theoretical intuition and understanding of the main proof results. For example, the instance-dependent lower bound presented in Theorem 1 hinges on the separation lemma in Lemma 1, which only shows that there is some positive probability that the ranking leaderboard never changes. The fact that there is a positive probability that a possible event occurs is not surprising, as such a probability could be arbitrarily small. Therefore, it is extremely unclear why in Theorem 1, the regret will need to be substantially larger than the stated bounds as such a small-probability event may only introduce Omega(1) regret into the expectation. Note that most lower bounds use Omega(*) notation, which means the lower bounds should hold up to multiplicative constants. The proof sketches from other theorems generally suffer from a similar lack of intuition, clarity, and possibly correctness. Generally it is surprising that a sqrt(T) instance-independent bound could be derived from such weak bounds given in Theorem 2, which has a exponential dependence on 1/Delta_i.\n\nFurthermore, the setting is rather arbitrary and there are no clear downstream applications that would utilize such a specific feedback model. It is also unclear why the dueling bandit setting is insufficient for dealing with preference feedback and while there are extensive comparisons made with the typical bandit setting, it is unclear how the stated theoretical results would compare with theoretical results in the dueling bandit setting, which is the most relevant setting."
            },
            "questions": {
                "value": "In Lemma 1, what are the concrete bounds on the probability of the bad event occuring? \n\nIn theorem 1, how does such bad events translate to significantly higher regret?\n\nIn theorem 2, are the exponential bounds necessary (a lower bound perhaps?) and how are you able to derive instance-independent regret from this?\n\nLastly, what is one specific ML application that would benefit from this setting vs dueling bandits? How do your theoretical bounds in the instance-dependent and independent settings differ from similar results in the dueling bandits case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5931/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698800102107,
        "cdate": 1698800102107,
        "tmdate": 1699636631185,
        "mdate": 1699636631185,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uZkITOSxAv",
        "forum": "VdOaaDzDD6",
        "replyto": "VdOaaDzDD6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5931/Reviewer_7Kt2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5931/Reviewer_7Kt2"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors study setting of learning with bandit feedback where the learner gets to see only the *ranking* between different arms based on the average rewards received by each arm so far. In the stochastic case, where the rewards of each arm follow a fixed distribution, the authors show instance dependent lower bounds (that is, lower bounds that depend on $\\mu_i^* - \\mu_i$, where $\\mu_i$ is the mean reward of arm $i$ and $i^*$ is the arm with highest mean rewards)  and algorithms that enjoy regret guarantees closely matching the lower bounds. The authors also show an interesting trade-off between instance-dependent and instance-independent guarantees for any bandit algorithm based only on ranking feedback, and provide a bandit algorithm with instance-independent guarantees. Finally, they conclude showing that in the adversarial setting no algorithm can guarantee sublinear regret and with a few numerical experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The setting is natural (although the motivation might be debatable, as I mention in the weaknesses) and quite interesting,  since it clearly requires different kinds of techniques from traditional bandit algorithms. The results are elegant and the investigation the authors perform goes over many of the natural questions one could consider (upper and lower bounds that nearly match, trade-off between instance dependency/independency, etc);\n- The presentation does a good job of describing the results and main algorithm in the main paper (although, saddly, not much discussion about the proof techniques, but you can only do so much in a few pages);\n- I have read the first couple of proofs in the paper, and they are clear and not too hard to follow;"
            },
            "weaknesses": {
                "value": "- One of the motivations in the introduction of the paper are cases when the rewards are not (easily) representable by numerical values. However, the setting in the end still relies on the existence of numerical values for the rewards, but the learner cannot see them. This might not be a weakness of the paper, but that is something that when reading the introduction I did not expect and goes without being deeply discussed in the paper (is this the case with dueling bandits as well?). I still think the setting is interesting, but I am not sure if the motivation in the introduction correctly matches the setting. Although I cannot easily see practical applications, the theoretical framework is still super interesting (i.e., algorithms that only get information about the ranking of the received rewards), the analysis are elegant, and I can be easily be proven wrong and follow up work can have interesting applications of this setting. This discussion seems interesting, and in the questions section I have a related question that could be interesting to incorporate in the final version of the paper (if the authors agree that it is an interesting point, of course);\n- The experiments are very small in scale. They do convey a message (the difference in empirical performance for algorithms that focus on the instance-dependent guarantees and the instance-independent guarantees), but since the algorithms are already implemented, it would have been interesting to see their performance in cases with more arms. I am not sure if there are computational constraints that prevent experimentation with more arms. Moreover, I think the main paper has the wrong plots (that are fixed in the supplementary material version), right? This could have been made explicit in notes in the appendix to not make the reviewers confused."
            },
            "questions": {
                "value": "- In the paper, the guarantees of Algorithm 1 seem to require the distribution of the rewards of each arm to be subgaussian, while this assumption seems absent from the guarantees of Algorithm 2. Is this right? So it is the case that algorithm 2 would preserve its guarantees even with heavier tailed (say, sub-exponential) rewards while this would not necessarily be true for algorithm 1?\n- Just to triple check, the plots in the main paper are not the right ones, right? The plots in the supplementary seem to be very different and to actually convey the message the authors meant to convey.\n- Is the \"full-information with ranking feedback\" easy? It seems that even with full information (that is, we get to rewards of all arms at each round, but only see the ranking) also seems not trivial, but I might be mistaken. If the authors have not thought too much about this, don't worry;\n- This is a more complicated question, and the authors should feel free to not answer this in the rebuttal if they are time constrained. But one thing that I noticed in the case with ranking feedback is that it is somewhat weaker than in the case of dueling bandits, even though ranking feedback requires a total order of the arms at every round while dueling bandits do not (to the best of my knowledge). Yet, we cannot use dueling bandit algorithms directly in this setting, since we only see the entire ranking based on the *cumulative reward* while dueling bandits make comparisons of the *instantaneous rewards* of two arms (if I am not mistaken). However, if we could compare the arms pulled in subsequent rounds, we can could use dueling bandits algorithms. Would this more powerful feedback yield stronger regret guarantees by using dueling bandits? Maybe the crux of the question is: if we were to compare the regret bounds achievable for dueling bandits vs the ones achieved by ranking feedback, are the ones with ranking feedback worse? If so, would this augmented feedback make ranking feedback boil down to dueling bandits?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5931/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5931/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5931/Reviewer_7Kt2"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5931/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807290878,
        "cdate": 1698807290878,
        "tmdate": 1699636631087,
        "mdate": 1699636631087,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1sCCsFtigw",
        "forum": "VdOaaDzDD6",
        "replyto": "VdOaaDzDD6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5931/Reviewer_qbW7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5931/Reviewer_qbW7"
        ],
        "content": {
            "summary": {
                "value": "This paper considers bandit with unobservable rewards, instead, the learner can only observe the arm ranking as feedback which is based on their cumulative rewards. The paper shows an instance-dependent regret lower bound that indicates no algorithm can suffer a logarithmic regret over time. The paper then presents two policies, one achieves instance-dependent regret upper bound matching their lower bound, the other achieves instance-independent regret of $\\tilde{O}(\\sqrt{T})$. They also show that no algorithm can have an optimal regret bound in both instance-dependent and instance-independent cases."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The problem is well formulated, and analyzed in details. Specifically, the regret is analyzed in both instance-dependent and instance-independent cases, as well as their lower bounds.\n* The model has practical use cases, e.g., when dealing with human preferences such as in matchmaking settings among humans and when the scores cannot be revealed for privacy or security reasons."
            },
            "weaknesses": {
                "value": "* The two algorithms are both based on explore-then-commit, which is well-known for its suboptimality on regret. There are other algorithms that have been shown optimal in regret and do not require an estimate on arm empirical means, e.g., another famous one in bandit literature $\\epsilon$-greedy. I recommend considering other algorithms that could potentially improve the performance, or discuss why explore-then-commit performs better than $\\epsilon$-greedy in this problem.\n* The problem formulation is interesting, but the paper organization can still be improved. There could be more discussion on novel techniques used in the proofs and novel algorithm designs to help readers understand the contributions, and some well-known content of bandit problem can be less mentioned."
            },
            "questions": {
                "value": "* In Theorem 1, I have no clue why there is a function $C(\\cdot)$ in the lower bound that has unknown shape and unknown parameter. Without a clear definition of such function, the lower bound seems to have very little meaning to me.\n* The description of Algorithm 1 is inaccurate: according to the description, when $t\\leq n$, the algorithm plays two arms at each round.\n* In Theorem 4, it is surprising to me that there exists tradeoff between instance dependent and independent regret bound, since in most cases an optimal policy has both regret bounds being optimal. I would recommend providing discussion about how to understand such tradeoff.\n* I cannot go through all the proofs, but the proof of Theorem 4 seems insufficient. The theorem is for general bandit with ranking feedback problem, however, the proof only shows the constructed two-arm instance but does not extend to general instance.\n* In Definition 3, is it feasible to compare the sum of sets to a constant?\n* I can hardly understand the basic idea of Algorithm 2, especially why defining active set like that and how it works. I recommend providing discussion about the algorithm design idea and how such design works to make the algorithm finally converge to the optimal action.\n* Some of the plots are covered by the legends."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5931/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820121914,
        "cdate": 1698820121914,
        "tmdate": 1699636630990,
        "mdate": 1699636630990,
        "license": "CC BY 4.0",
        "version": 2
    }
]