[
    {
        "id": "guNZTDxOqK",
        "forum": "VJDFhkwQg6",
        "replyto": "VJDFhkwQg6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6008/Reviewer_rsSg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6008/Reviewer_rsSg"
        ],
        "content": {
            "summary": {
                "value": "This submission is mathematically exciting but, unfortunately, poorly explained and poorly motivated. The methodological section requires a much better explanation. It is currently mainly an enumeration of results. In addition, no natural problems that in any apparent way benefit from the approach are presented. Moreover, the plots in the experimental section need to be more convincing. Running time and other gains need to be made crystal clear. If this is an uncut diamond, please cut it."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This submission is mathematically exciting."
            },
            "weaknesses": {
                "value": "This submission is poorly explained and poorly motivated."
            },
            "questions": {
                "value": "\"The main purpose of our experiments is to verify the empirical performance of FC-GFlowNets, i.e., their capacity to accurately sample from the combination of local rewards \"\nThis is not sufficient. \n\n\" This is also reflected in the average reward over the top 800 samples \u2014 identical to the centralized version for FC-GFlowNet, but an order of magnitude smaller for PCVI. Again, these results corroborate our theoretical claims about the correctness of our scheme for combining GFlowNets. \"\nIs the centralized using a more standard GFN training approach?\n\n\" The five left- most plots indicate that the local GFlowNets were accurately trained \"\nWhy\n\n\" Bayesian phylogenetic inference: learned \u00d7 ground truth distributions. Following the pattern in Figures 2-4, the goodness-of-fit from local GFlowNets (Clients 1-5) is directly reflected in the distribution learned by FC-GFlowNet. \"\nWhy is this good?\n\n\" In particular, GFlowNets trained via CB demonstrated faster convergence compared to current art when intermediate states are not terminal, while being com- petitive in other scenarios \"\nWhen are intermediate states terminal. Where is this shown?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6008/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698570816266,
        "cdate": 1698570816266,
        "tmdate": 1699636644291,
        "mdate": 1699636644291,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Om8tHTwSEN",
        "forum": "VJDFhkwQg6",
        "replyto": "VJDFhkwQg6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6008/Reviewer_Xj4h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6008/Reviewer_Xj4h"
        ],
        "content": {
            "summary": {
                "value": "An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale."
            },
            "weaknesses": {
                "value": "- Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed."
            },
            "questions": {
                "value": "I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6008/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6008/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6008/Reviewer_Xj4h"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6008/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698700151233,
        "cdate": 1698700151233,
        "tmdate": 1700328522910,
        "mdate": 1700328522910,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MqkRJXKUrp",
        "forum": "VJDFhkwQg6",
        "replyto": "VJDFhkwQg6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6008/Reviewer_Npeg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6008/Reviewer_Npeg"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of training a GFlowNet in federated manner. Each client has its own reward function and local data. And each client trains its local GFlowNet. The goal here for the server is learn a GFlowNet model that samples data proportional to the product of clients reward. The paper conducts experiments on various tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper studies the unexplored problem of federated training of GFlowNet.\n2. The paper conducts extensive experiments on various tasks."
            },
            "weaknesses": {
                "value": "1. I believe should give more concrete example on why learning a GFlowNet samples data proportional to product of reward is important in federated setting. From reading the paper and experiments, I am not clear about the significance of the problem studied by this paper. In fact, it seems the problem tackled by this paper is an special case while it is not evident that this special case is important in federated setting.\n2. It seems that the main contribution of this paper is its problem setting where learning a GFlowNet model that samples data proportional to product of rewards can be done in a federated fashion. However, given the problem, learning the global model aggregating local GFlowNet seems to be straightforward. Therefore, I think I believe the contribution of the paper in federated learning is not high."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6008/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698749801114,
        "cdate": 1698749801114,
        "tmdate": 1699636644094,
        "mdate": 1699636644094,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "84BdtBO0we",
        "forum": "VJDFhkwQg6",
        "replyto": "VJDFhkwQg6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6008/Reviewer_wWFS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6008/Reviewer_wWFS"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel framework for federated learning of GFlowNets, called FC-GFlowNet, which is based on a divide-and-conquer strategy that requires only a single communication step. The authors introduce a new concept, contrastive balance, which provides necessary and sufficient conditions for the correctness of general GFlowNets. The paper also presents experiments demonstrating the effectiveness of FC-GFlowNets in various settings, including grid-world, sequence, and multiset generation, and Bayesian phylogenetic inference. Overall, the paper provides a significant contribution to the community interested in generative modeling, enabling GFlowNets in federated settings, introducing a novel training scheme for centralized settings, and demonstrating the potential of GFlowNets on a new application domain."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper provides a contribution to the community interested in generative modeling, enabling GFlowNets in federated settings, introducing a novel training scheme for centralized settings, and demonstrating the potential of GFlowNets on a new application domain.\n  2. The authors introduce a new concept, contrastive balance, which provides necessary and sufficient conditions for the correctness of general GFlowNets, and demonstrate its effectiveness in training GFlowNets."
            },
            "weaknesses": {
                "value": "1. However, the motivation for introducing GFlowNets into Federated Learning (FL) appears to be lacking. Despite the existing research gap, what specific challenges or benefits does FL encounter that require the integration of GFlowNets?\n\n2. Conversely, is there a scenario in which GFlowNets necessitate Federated Learning (FL)? In my view, the explanation provided in the third paragraph is somewhat lacking. It would be helpful to provide more practical examples to illustrate this.\n\n3. The experimental setup regarding the clients is not sufficiently clear. Are all clients identical, or does this setup involve non-iid (non-independent and identically distributed) problems?\n\n4. It would enhance clarity if the authors could include a figure or pseudocode illustrating the algorithm."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6008/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6008/Reviewer_wWFS",
                    "ICLR.cc/2024/Conference/Submission6008/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6008/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769905235,
        "cdate": 1698769905235,
        "tmdate": 1700645476052,
        "mdate": 1700645476052,
        "license": "CC BY 4.0",
        "version": 2
    }
]