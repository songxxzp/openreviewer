[
    {
        "id": "CFuwMoV2XI",
        "forum": "y3dqBDnPay",
        "replyto": "y3dqBDnPay",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4269/Reviewer_N99Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4269/Reviewer_N99Z"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces HyperRep, a hypergraph-based self-supervised multimodal representation learning method that captures high-order correlations in real-world multimodal data. The proposed approach balances the benefits of contrastive methods and preserves the unique aspects of each data point, achieved through the construction of dual types of hypergraphs. The paper presents experimental results on several downstream tasks, demonstrating that HyperRep delivers consistently competitive results against state-of-the-art methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea of using hyperedge to facilitate multi-modal fusion is interesting.\n\nThe paper is clear and easy to follow."
            },
            "weaknesses": {
                "value": "For the Hypergraph propagation module figure (Fig. 3), a more detailed and clearer introduction should be helpful.\n\nThere are a few typos. Please find and correct them."
            },
            "questions": {
                "value": "1. May the author explain whether  HyperRep requires pre-trained encoder for each modal or it trains these encoders using the MFB loss? If pre-trained encoders are used, may the author provide ablation about the encoders used in the experiment?\n2. May the author explain the intuition behind using the instance hyperedge features for the downstream task, not the instance feature itself?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4269/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4269/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4269/Reviewer_N99Z"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4269/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698741837602,
        "cdate": 1698741837602,
        "tmdate": 1699636394298,
        "mdate": 1699636394298,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QPoG2P0Pwv",
        "forum": "y3dqBDnPay",
        "replyto": "y3dqBDnPay",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4269/Reviewer_scVn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4269/Reviewer_scVn"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes HyperRep, a self-supervised representation learning method for multimodal data that leverages hypergraphs to capture high-order inter- and intra-modality correlations. HyperRep also uses an information bottleneck principle to fuse multimodal data effectively. The paper shows that HyperRep outperforms existing methods on various downstream tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper provides sufficient and detailed formal definitions and necessary proofs.\n2. The application of hypergraphs in this paper may inspire the field of multimodal self-supervised learning.\n3. The authors conducted extensive ablation experiments on multiple datasets."
            },
            "weaknesses": {
                "value": "1. Motivation: The paper highlights the use of hypergraphs to capture higher-order correlations among modalities. However, I find the paper insufficient in explaining and analyzing why higher-order correlations are essential for multimodal self-supervised learning. This is especially relevant since most existing methods, such as the CLIP[1] series and ImageBind[2], rely on pair-wise multimodal self-supervision.\n2. Method: The paper could improve the clarity and presentation of MFB. For instance, how does Eq. 12 relate the mutual information of shared and instance features? A clearer explanation of the illustration would be helpful. Also, the MFB module in Figure 2 is vague, and a more specific illustration could enhance the readers\u2019 comprehension of the method.\n3. Experiment: The paper lacks comparisons with recent (2023) methods, as the only one mentioned has a huge performance gap with the classical methods. Could the paper also compare with more popular Foundation models, such as VideoCLIP[3] and OmniVL[4]? Moreover, Table 1 shows that graph-based methods perform poorly, and Table 2 reveals a drastic performance drop after removing higher-order correlations. However, AGC achieves high performance without using higher-order correlations. Does this imply that higher-order correlations are crucial for the proposed method, but not for the multimodal self-supervised learning task? \n\n[1] Learning Transferable Visual Models From Natural Language Supervision. ICML 2021.\n[2] IMAGEBIND: One Embedding Space To Bind Them All. ICCV 2023.\n[3] VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding. EMNLP 2021.\n[4] OmniVL: One Foundation Model for Image-Language and Video-Language Tasks. NeurIPS 2022."
            },
            "questions": {
                "value": "How come the MIL-NCE performance in Table II differs so much from the original paper? Are there any differences in the experimental settings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4269/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4269/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4269/Reviewer_scVn"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4269/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698748492918,
        "cdate": 1698748492918,
        "tmdate": 1700660847640,
        "mdate": 1700660847640,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xYu4jLEGsq",
        "forum": "y3dqBDnPay",
        "replyto": "y3dqBDnPay",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4269/Reviewer_bsih"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4269/Reviewer_bsih"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to solve the existing challenges in self-supervised representation learning, where most of the existing work overlooks the high-order inter- and intra-modality correlations characteristics and lacks effective fusion principles. To tackle these issues, the author proposed HyperRep, which combined hypergraph-based modeling and self-supervised multimodal fusion principle to achieve superior representation learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and nicely-structured.\n\n2. The paper proposed hypergraph-based representation learning creatively uses graph structure to represent the inter- and intra-modal relationships for multi-modal data, which helps the model capture high-order correlations of the instances.\n\n3. The proposed MFB loss leverages the bottleneck principle to encourage the model to capture the most informative aspects of the data and is demonstrated over multiple downstream tasks.\n\n4. The authors conducted diverse ablation studies that demonstrate the effectiveness of different components of the proposed method and the ability of handling missing modalities. This makes the method suitable for real-world applications.\n\n5. There is a consistent improvement in performance over all three downstream tasks, clustering, text to video retrieval and action localization over prior baselines."
            },
            "weaknesses": {
                "value": "1. In the hypergraph construction process, which is a preprocessing step for the data, has a computational complexity of O(n^2d). This may limit the scalability of the method for datasets with large amount of instance or more complex data (high-resolution images).\n\n2. Lacking experiment that provides a detailed analysis of the interpretability of the learned representations.\n\n3. It would be better to add the results from baseline methods in Figure 9.\n\n4. In Table 3, it looks like row 1 (high order correlation) has a big say in the final scores, while InfoNCE doesn't seem to make much of a difference in the end results. It would have been really helpful to have a deeper analysis of this, maybe with some t-SNE results, to better grasp how these representations work."
            },
            "questions": {
                "value": "How are the k-NN selected for both training and validation phase?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4269/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698783001505,
        "cdate": 1698783001505,
        "tmdate": 1699636394155,
        "mdate": 1699636394155,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uIqyfPlMD2",
        "forum": "y3dqBDnPay",
        "replyto": "y3dqBDnPay",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4269/Reviewer_yMHQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4269/Reviewer_yMHQ"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a self-supervised representation learning method on multimodal data based on hypergraph-based learning called HyperRep by combining the strength of hypergraph-based modeling with a self-supervised multimodal fusion information bottleneck principle. The extensive experiments on four public datasets including three downstream tasks demonstrate the advantages of the proposed method, which are also validated by the comparison with the state-of-the-art approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe idea of using hypergraph-based self-supervised multimodal representation learning is interesting as it captures the high-order relationships in multimodal data.\n2.\tCompared to most multimodal learning work using semi-supervised approaches which requires additional label information, the proposed approach is employing self-supervised learning. The derived hypergraph attention module and propagation is a good extension from current approaches.\n3.\tThe author also introduced multimodal fusion information bottleneck (MFB) principle, in order to maximize the mutual information between the instance and each modality. The corresponding upper bound and lower bound are derived.\n4.    The experimental evaluation is detailed and extensive."
            },
            "weaknesses": {
                "value": "1. Overall, while the idea is interesting, the scope is relatively narrow as it combines self-supervised learning with multimodal representation learning based on hypergraph. However, hypergraph attention network for multimodal learning has been explored in\nhttps://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Hypergraph_Attention_Networks_for_Multimodal_Learning_CVPR_2020_paper.pdf\nCompared to this CVPR 2020 paper, the only novel part for the manuscript seems to be adding self-supervised learning on top of it. However, that novelty is relatively small.\n\n2. The performance gain appears to be incremental as the paper serves as an extension to recent work."
            },
            "questions": {
                "value": "1 The author needs to distinguish the work better with the previous work especially many module presented in this paper such as hypergraph attention module has been published before. So a reference shoud be given and explain the difference if there is any.\n\n2.While the lower bound and upper bound are derived, it is important to show how to leverage these bounds for optimization.\n\n3.The contribution of the paper is not very clear."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4269/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4269/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4269/Reviewer_yMHQ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4269/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807737777,
        "cdate": 1698807737777,
        "tmdate": 1699636394079,
        "mdate": 1699636394079,
        "license": "CC BY 4.0",
        "version": 2
    }
]