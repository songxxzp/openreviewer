[
    {
        "id": "VbwZ4jgc8I",
        "forum": "nUH5liW3c1",
        "replyto": "nUH5liW3c1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1484/Reviewer_owdS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1484/Reviewer_owdS"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new supervised contrastive learning objective function called SCHaNe, which addresses the limitations of the cross-entropy objective function used in pre-trained image models. SCHaNe incorporates hard negative sampling during fine-tuning to enhance the efficacy of contrastive learning. Experimental results demonstrate that SCHaNe outperforms the baseline model BEiT-3 in Top-1 accuracy across twelve benchmarks, with significant gains in few-shot learning settings and full-dataset fine-tuning. The proposed objective function sets a new state-of-the-art for base models on ImageNet-1k, achieving an accuracy of 86.14%. Additionally, the paper shows that SCHaNe produces better embeddings and explains the improved effectiveness observed in the experiments. Overall, the contributions of this work include the introduction of SCHaNe and its superior performance in few-shot learning and full dataset fine-tuning, establishing new state-of-the-art results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The method has been validated on multiple datasets, and comprehensive experiments have been conducted on downstream datasets.\n\n2. The work appears to be relatively comprehensive, with a clear motivation, detailed method description, and important parameter ablation experiments. Overall, it seems well-executed and promising."
            },
            "weaknesses": {
                "value": "1. To enhance the credibility of our research, you should consider using the same base models (such as ViT or Swin) as other studies for our baseline in Table 1 and Table 2.\n\n2. In order to provide a more comprehensive analysis, the results in Table 1 and Table 2 should include the performance of contrastive learning without the hard negative mining method.\n\n3. The representation of Formula 3 needs to be clarified to ensure better understanding, as it is currently not very clear.\n\n4. To provide a more complete comparison, Figure 3 and Figure 4 should include the results of BEiT-3-CE + contrastive learning, in addition to the results of our proposed method.\n\n5. In each comparative experiment, it is important to clearly indicate the improvement achieved by the hard negative mining method on top of the performance obtained with CE + contrastive learning. This will help demonstrate the added value of our approach."
            },
            "questions": {
                "value": "1. Why did you choose BEiT-3 as your base model? There are other base models like DINOv2, CLIP, etc.\n\n2. Cross-entropy and contrastive learning have similar forms, so why is the hard negative sampling only applied to the contrastive learning part and not to the cross-entropy part?\n\n3. In general, fine-tuning with cross-entropy (CE) loss is not strongly coupled with the batch size, while contrastive learning can be affected by the batch size. This can limit the flexibility of fine-tuning with CE when combined with contrastive learning. What are your thoughts on this issue?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1484/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698558312184,
        "cdate": 1698558312184,
        "tmdate": 1699636077562,
        "mdate": 1699636077562,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "F7ABwXxIhf",
        "forum": "nUH5liW3c1",
        "replyto": "nUH5liW3c1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1484/Reviewer_7hRi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1484/Reviewer_7hRi"
        ],
        "content": {
            "summary": {
                "value": "The paper introduced a novel supervised contrastive learning objective function called SCHaNe. SCHaNe enhances model performance without requiring specialized architectures or additional resources. The proposed approach combines supervised contrastive learning with hard negative sampling to optimize the selection of positive and negative samples, thereby achieving state-of-the-art performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThis paper proposes a novel supervised contrastive learning objective function, SCHaNe, which incorporates hard negative sampling during the fine-tuning phase. \n2.\tThe proposed method achieves state-of-the-art performance on ImageNet-1k and outperforms the strong baseline BEiT-3 in Top-1 accuracy across twelve benchmarks, with significant gains in few-shot learning settings and full-dataset fine-tuning."
            },
            "weaknesses": {
                "value": "Strengths* \n1.\tThis paper proposes a novel supervised contrastive learning objective function, SCHaNe, which incorporates hard negative sampling during the fine-tuning phase. \n2.\tThe proposed method achieves state-of-the-art performance on ImageNet-1k and outperforms the strong baseline BEiT-3 in Top-1 accuracy across twelve benchmarks, with significant gains in few-shot learning settings and full-dataset fine-tuning. \nWeaknesses* \n1.\tThe paper could benefit from a more detailed comparison with existing methods. While the authors compare the proposed method with the strong baseline BEiT-3, they do not compare it with other similar state-of-the-art methods [1][2] in the field.\n2.\tThe starting point of the work [2] is very similar to this article. I hope the author can further clarify the relationship with it so that readers can further understand the core starting point of the article.\n[1]: Robinson J, Chuang C Y, Sra S, et al. Contrastive learning with hard negative samples[J]. arXiv preprint arXiv:2010.04592, 2020.\n[2]: Jiang R, Nguyen T, Ishwar P, et al. Supervised contrastive learning with hard negative samples[J]. arXiv preprint arXiv:2209.00078, 2022."
            },
            "questions": {
                "value": "1.\tCan the authors provide a more detailed comparison and analysis with existing methods? See the Weaknesses section for details.\n2.\tIn Table 3, the performance of the CE + SimCLR method is much lower than that of the CE method alone. At the same time, according to the description in the table, Label is not used in this part. How is this part of CE implemented, and why does the performance drop after adding SimCLR?\n3.\tIf possible, can the method proposed in this article be easily integrated into other models (such as other pre-trained models or other few-shot learning methods) like BEiT-3? I hope the article can give relevant explanations and results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1484/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1484/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1484/Reviewer_7hRi"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1484/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698680545710,
        "cdate": 1698680545710,
        "tmdate": 1699636077472,
        "mdate": 1699636077472,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "17IiR4XPdz",
        "forum": "nUH5liW3c1",
        "replyto": "nUH5liW3c1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1484/Reviewer_SnpE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1484/Reviewer_SnpE"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new loss, SCHaNe, for supervised contrastive learning. The main idea of this novel loss is that introducing importance weights for negative samples based on their dissimilarity plays a significant role in improving performance. Experiments show that SCHaNe is an effective method for enhancing performance on various datasets, particularly in few-shot tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The assumption that this paper aims to validate is both simple and easy to understand. Furthermore, the proposed objective is straightforward and intuitive. \n2. There is a clear improvement in performance when compared with the conventional cross-entropy loss. \n3. The paper is generally well-organized and presents its content logically."
            },
            "weaknesses": {
                "value": "1. One of the main weaknesses I've identified is that the primary baseline used in this paper is Cross-Entropy (CE) loss, not Supervised Contrastive Learning (SupCon). If the paper's central claim is that 'introducing importance weights for negative samples based on their dissimilarity plays an important role,' then I believe SupCon should be the main baseline for comparison. Although SCHaNe outperforms SupCon in the few-shot setting as shown in Table 3, the inclusion of SupCon results in other settings\u2014such as in Table 1, Table 2, and Figure 4\u2014could strengthen the paper.\n2. BEiT-3 is primarily utilized as the main architecture. The Future Work section suggests that extending this method to various architectures may be promising, but I believe that evaluating the proposed method across different architectures should be included in this paper.\n3. The proposed method appears to be limited in its applicability to various tasks, such as dense prediction tasks. While this may not be a significant drawback, explicitly stating this limitation could enhance the paper. Moreover, the paper claims that \u201cOur SCHaNe objective function can be applied using a wide range of encoders, such as BERT for natural language processing tasks,\u201d yet there are no experiments provided to substantiate this claim.\n\nMinor: The presentation of the paper could be improved. For instance, the notation in Equations 2-4 is confusing. If I understand correctly, $\\beta$ should vary with the index, but it might be misunderstood as a constant since it lacks an index. In Equation 2, $z$ denotes the label, which is not the case in Equations 3 and 4. Regarding Figure 4, while the trend is important, we cannot directly compare the accuracies across various downstream tasks in an 'apple-to-apple' manner.\n\n\n---\n\n**Post rebuttal**\n\nI appreciate the authors' response and the additional experimental results provided. Despite the manuscript's weaknesses, I believe its strengths outweigh them. Consequently, I maintain my rating of 'Weak Accept'."
            },
            "questions": {
                "value": "Please see the weaknesses section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1484/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1484/Reviewer_SnpE",
                    "ICLR.cc/2024/Conference/Submission1484/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1484/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698718933632,
        "cdate": 1698718933632,
        "tmdate": 1700812917536,
        "mdate": 1700812917536,
        "license": "CC BY 4.0",
        "version": 2
    }
]