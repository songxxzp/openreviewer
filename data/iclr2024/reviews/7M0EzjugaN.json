[
    {
        "id": "Xd1zqYMD2t",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission318/Reviewer_q27B"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission318/Reviewer_q27B"
        ],
        "forum": "7M0EzjugaN",
        "replyto": "7M0EzjugaN",
        "content": {
            "summary": {
                "value": "This paper first introduced two new settings to coninual learning embodied AI: behavior incremental learning and environmental incremental learning. The paper then introduces a method to tackle both, in a task boundariless setting, called Confidence-Aware Moving Average. The method is employed in Alfworld, and the task at each step is to predict action and object class."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The continual learning setting is a relevant topic to be explored in the embodied AI field\n- the writing is clear and the method is well motivated and easy to understand\n- the number of baselines compared is very comprehensive\n- the method is simple yet effective"
            },
            "weaknesses": {
                "value": "- conceptually, the task boundariless scenario in embodied instruction following AI is very limited, since the user will know when to switch to a new task\n- introduction to a new hyperparameter for each of the prediction ($\\alpha$'s)\n- another baseline I could think of is to use the loss as the weight to dynamically change the old logits. What are some intuition on why confidence is better than loss to use to dynamically weight?"
            },
            "questions": {
                "value": "- equation (1), the $\\gamma$  in front of $z$ is it missing the subscript $a$ and $c$? or is it a fixed value?\n- for equation (2), how are the confidence scores calculated, mathematically\n- I am not too familiar with the benchmark, but when is the object class used when making decisions? if you need to heat a mug, does your agent also need to recognize the object in front of it is a mug too?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission318/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission318/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission318/Reviewer_q27B"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission318/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697462481154,
        "cdate": 1697462481154,
        "tmdate": 1700654275104,
        "mdate": 1700654275104,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HyGMXgQB7q",
        "forum": "7M0EzjugaN",
        "replyto": "7M0EzjugaN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission318/Reviewer_z9k5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission318/Reviewer_z9k5"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the online continual learning problem without task boundary information in an imitation learning context. It adapts a distillation based approach that was developed for the setting with task boundaries. Prior methods store past model logits in memory and update these with a weighting scheme to prevent these logits of becoming outdated using task boundary information. Here the weights are determined dynamically via the confidence of the past N predictions of the model, not relying on task boundary information. The method shows improved task performance compared to a variety of continual learning baselines in a simulated household environment. The evaluation is split into a setting where the agents has to continually learn new behaviors and a setting where the agent has to continually adapt to new environments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The way the method and experimental setup is presented, it can be easily followed and understood. \n\n- The method is tested against a variety of baselines representing different approaches to the continual learning problem.\n\n- The method outperforms all baselines.\n\n- Although the novelty is on the smaller side it becomes apparent how the work relates to prior approaches and what limitation is tackled. It is also shown that tackling exactly that limitation leads to better performance."
            },
            "weaknesses": {
                "value": "- Depending on the availability of compute it would be beneficial to increase the number of seeds (3->5) to reduce the standard errors of the results. At the moment the standard errors are often larger than the gaps in mean performance. \n\n- Effect of the hyperparameter N (number of past confidences averaged over). I think it would be interesting and important to see how sensitive the method is to that hyperparameter. If it works only well for a specific N, a lot of hyperparameter tuning will be needed to find that N for a given environment. If performance is robust with respect to N it would make the method more easily applicable. \n\n- The evaluation is split between environment changes and behavioral changes. I understand the point of separating these two factors, but why was the crossing of the factors not studied? What happens if two factors change at the same time?\n\n- As shown in Figure 3, when a new tasks arrives confidence drops. However, there is also evidence that in OOD cases a model can be confidently wrong (Do Deep Generative Models Know What They Don't Know?, Nalisnick et. al.). How detrimental would that be for performance?\n\nFormalities:\n\n- Abstract, last sentence: \"outperforms prior arts\", was that supposed to be \"outperforms prior state of the art\" ?  \n- Figure 4: In the second paragraph of 5.3 that references the figure 4, \"HEAT\" is claimed to be the newly learned task and Pick2&Place the old one, while in the caption of the figure it is the other way around."
            },
            "questions": {
                "value": "- The performance of the \"finetune\" baseline with respect to the $SR_{last}$ and $GC_{last}$ metrics. If I understand the baseline correctly is that the currently trained policy is finetuned on the data of the next task without any memory. Should the performance on the final task not be . Why is the performance on the . \n\n- Which episodes are stored in the memory M? Is it simply a queue? If yes then given that its capacity is such a small percentage of the total number of episodes (as said in the text that is the standard) and given the number of tasks for each of the evaluation episodes will most episodes not be about the same same task such that the logits there should be replaced with the current logits of the model?\n\n- In the CAMA w/o D.C. baseline, was the fixed coefficient tuned for optimal performance of the baseline?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "/"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission318/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission318/Reviewer_z9k5",
                    "ICLR.cc/2024/Conference/Submission318/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission318/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698426653137,
        "cdate": 1698426653137,
        "tmdate": 1700670384381,
        "mdate": 1700670384381,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bxUcScYFLT",
        "forum": "7M0EzjugaN",
        "replyto": "7M0EzjugaN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission318/Reviewer_UTbC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission318/Reviewer_UTbC"
        ],
        "content": {
            "summary": {
                "value": "This work looks at the continuous learning problem for embodied agents that follow verbal task instructions. The outlined challenge is very relevant in the community as such agents need to constantly adapt to new tasks and environment challenges. In this work, the authors address Behavior-IL and Environment-IL through a newly proposed method, Confidence-Aware Moving Average, that updates an experience replay of stored logits based on model confidence. The proposed method is evaluated over a set of tasks and behaviors in the ALFRED simulation environment against state-of-the-art baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The proposed work addresses a relevant challenge for the deployment of embodied agents in challenging home environments, particularly addressing the problem of adapting previously learned policies to new environments and tasks.\n* The idea of using confidence to update the stored logits is interesting, and potentially quite powerful (if there is any way to know that the confidence is reasonable and/or calibrated, see below)."
            },
            "weaknesses": {
                "value": "* I think some of the assumptions this paper makes are not realistic. E.g., the paper indicates that this approach is task-free and that task-identifiers are not available, but then appears to assume that the data is balanced. In a real-world continual learning setup there is no guarantee that the tasks/data the agent encounters will be balanced, and so if data balance is enforced via sub-sampling I think this is implicitly leaking information regarding task identifiers (as they would be needed to balance an unbalanced dataset in practice). An analysis on how class/environment/behavior imbalance affects the model would be beneficial here.\n* The performance improvements seem relatively marginal for many of the evaluation metrics. Were any statistical significance results run over these values?\n* The training methodology is not entirely clear to me, and I think some of it should be pulled into the main paper as it reduces the clarity of the empirical results. Are all the models trained sequentially over a set of tasks/environments and then evaluated over the same set (in the seen case)? Do the seen/unseen evaluations use the same underlying trained model?"
            },
            "questions": {
                "value": "1) Is there a concern about over/under-confidence with such an approach? Presumably, such a model is not calibrated since the model is continually out-of-distribution. Given that modern deep neural networks are particularly susceptible to confidence issues, an analysis on this aspect would be useful.\n2) Does performance depend on the order of the tasks/environments? I see reference to random ordered sequences in Sec. B.3, but it's not clear to me how this ties into the training/evaluation methodology.\n3) Is there any insight on how the size of the memory buffer relates to the number of learning tasks? Is 500 an arbitrary number or was this empirically chosen?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission318/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission318/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission318/Reviewer_UTbC"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission318/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825932382,
        "cdate": 1698825932382,
        "tmdate": 1699635958360,
        "mdate": 1699635958360,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EiDKAdSa4O",
        "forum": "7M0EzjugaN",
        "replyto": "7M0EzjugaN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission318/Reviewer_oYjZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission318/Reviewer_oYjZ"
        ],
        "content": {
            "summary": {
                "value": "This paper studies continual learning for instruction-following embodied agents. The authors argue that continual learning is a more realistic setting for embodied AI. To evaluate embodied agents in a continual learning setting, the authors first propose two new continual learning setups for embodied agents:  Behavior Incremental Learning (Behavior-IL) and Environment Incremental Learning (Environment-IL) for new behavior and new environment learning, separately. Then, the authors proposed a new approach for weighting the new logits and old logits. Specifically, a confidence-aware moving average approach which updates logits based on confidence scores of class labels is proposed. The experimental results show that the proposed approach outperforms competitive baselines on the two proposed setups."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality and Significance:    \nThe paper is well-motivated. Continual learning for embodied agents is a more realistic yet less explored area in our community. In addition, the two new continual learning setup seems reasonable and could be valuable for evaluating continual learning agents. \n\n\nQuality:   \nThe paper is technically sound. Most of the claims are supported by experimental results. \n\n\nClarity:    \nThe paper is generally well-organized and implementation details are presented. However, some technical details may need further clarification. Please see the Weakness section for more details."
            },
            "weaknesses": {
                "value": "1. While the empirical results seem promising, the reviewer is not fully convinced by the proposed confident-aware moving average approach. Specifically, it is unclear why the average of most recent N confidence scores could be a good indicator of the \u2018quality\u2019 of the newly obtained logits. More concretely, the behavior $t_i$ of the current episode is different from the behavior $t_j$ that is associated with the sampled past experience $x\u2019$. Why would confidence scores of current episodes with behavior $t_i$ provide information on a different behavior $t_j$.   \n\n\n2. In equation (1), subscript $a, c$ are missing in $\\gamma_a$ and $\\gamma_c$"
            },
            "questions": {
                "value": "1. Please elaborate why the confidence scores are good indicators of the \u2018quality\u2019 of the new logits. Giving simple examples that demonstrate the effectiveness could be very helpful.   \n\n2. This paper uses imitation learning to train the agents. Is the proposed approach extendable to a reinforcement learning setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission318/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699227293135,
        "cdate": 1699227293135,
        "tmdate": 1699635958287,
        "mdate": 1699635958287,
        "license": "CC BY 4.0",
        "version": 2
    }
]