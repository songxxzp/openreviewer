[
    {
        "id": "l0mFxxxErH",
        "forum": "dbQH9AOVd5",
        "replyto": "dbQH9AOVd5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2129/Reviewer_tWNn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2129/Reviewer_tWNn"
        ],
        "content": {
            "summary": {
                "value": "The authors improved upon IsoScore, a measure for the isotropy of point clouds (Findings of ACL'22), and proposed the IsoScore* as a measure of **an**isotropy. IsoScore* employs PCA (as a natural method for estimating the covariance matrix) and utilizes shrinkage estimation, making it a differentiable measure that operates robustly even with small data sizes. In experiments, IsoScore* was used as a regularization term when fine-tuning masked language models. As a result, under optimal hyperparameters, performance improvements were observed across multiple downstream tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- In the NLP field, the isotropy of internal representations was believed to be the key to model success. The message of this paper, suggesting that the **an**isotropy of internal representations might be the key to performance improvement, will likely resonate intriguingly with many readers.\n- The paper comprehensively covers a collection of works related to the isotropy of NLP models, making it a highly self-contained piece for readers."
            },
            "weaknesses": {
                "value": "### 1. The reasons for contradictions with prior research are unclear, weakening the persuasiveness of the main claim.\nThe authors' main claim that \"**an**isotropy is the key to model performance improvement\" isn't reconciled with prior research which posits that \"isotropy is the key to model performance improvement\". While the authors suggest that the discrepancy arises from the evaluation metrics used (as stated \u201cprevious studies have made claims using \u201cflawed\u201d measures of isotropy,\u201d on page 7), both this work and prior studies differ *not* only in metrics but also in tasks (GLUE tasks vs. word similarity tasks). Therefore, it's not an apple-to-apple comparison. If there's an inverse correlation between anisotropy and performance in word similarity tasks, it becomes challenging to coherently explain the overall results. This issue might possibly be resolved if there were experiments or comprehensive discussions specifically for the word similarity tasks.\n\n### 2. The claim that the proposed method improves performance on downstream tasks seems a bit overstretched.\nEven when choosing the best settings across multiple hyperparameters, the performance improvement is modest (as seen in Table 1). Moreover, performance can deteriorate compared to the baseline depending on hyperparameter choices (as shown in Figure 3). Furthermore, adopting the proposed method incurs an additional cost of hyperparameter selection. Therefore, for practitioners aiming to employ IsoScore* for their problems, it's hard to advocate for the use of the proposed method. Of course, submissions to ICLR shouldn't be evaluated solely on empirical performance. However, in this paper, improving performance on downstream tasks is one of the main contributions; thus the lack of compelling experimental results will inevitably impact the paper's peer-review evaluation."
            },
            "questions": {
                "value": "- If anisotropy is a natural consequence of SDG, what is the significance of deliberately adding anisotropy as a regularization term? The computational cost increases slightly, and the disadvantage of increased hyperparameter tuning cost needs to be offset by some substantial benefits. If there are such advantages, the appeal of the proposed method would be enhanced.\n- What is the significance of adjusting the hyperparameter $\\zeta$ for shrinkage estimation when using IsoScore* as a regularization term? Even if the value of IsoScore is estimated to be on the lower side (Figure 2), if the estimated value has a monotonic relationship with the true value, wouldn't there be no issue in regularization? If the goal is to ensure differentiability, couldn't we just fix it at an appropriate value?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2129/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839390152,
        "cdate": 1698839390152,
        "tmdate": 1699636145492,
        "mdate": 1699636145492,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "clPWHCeRVj",
        "forum": "dbQH9AOVd5",
        "replyto": "dbQH9AOVd5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2129/Reviewer_ZAyV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2129/Reviewer_ZAyV"
        ],
        "content": {
            "summary": {
                "value": "This submission proposes a new method for measuring isotropy in neural models, improving over previous proposals and leading to a new regularization technique I-STAR. They show that LLMs actually seem to benefit from less isotropic internal representations, contrary to previous claims in the NLP literature."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well written and clear. The proposed improvement of IsoScore into IsoScore* is fairly straightforward. The fact that it is a more accurate and more convenient estimate of isotropy in LLMs is argued very well and supported by some empirical results.\n\nThe paper convincingly argues that isotropy and its impact on performance are not properly understood in NLP, which is a very significant contribution.\n\nExperimental results mostly support the arguments in the paper (more comments below)."
            },
            "weaknesses": {
                "value": "* There is no significance testing on the results (Table 1) but there are error bars (good!) \u2014 these seem to indicate that most differences outlined are hardly significant (e.g. RTE, 72.56+/-1.29 vs 71.34+/-0.91). This makes it difficult to get a clear picture of the resulting effect of decreasing isotropy.\n* Similarly Fig. 3 is difficult to interpret \u2014 there are clear decreasing trends in some plots, not so much in most of them."
            },
            "questions": {
                "value": "* p.4: Presumably all norms in steps 6 and 7 of Algorithm 1 are 2-norms?\n* p.4: Is RDA \"regularized discriminant analysis\"? It is only introduced on p.5.\n* p.4: As \\Sigma_S is computed from a large number of tokens and re-estimated after each epoch, this is presumably quite costly computationally. Could you comment on that? As you are computing these anyway, why not use that directly to estimate isotropy? This would essentially correspond to \\zeta=1, which is not tested here as far as I can tell.\n* p.6: \"Section F\" -- do you mean Section D?\n* p.7: I do not clearly see Fig. 4 as supporting the claim that CosReg only alters the mean of activations (all means seem to be\u00a0~zero on the plots, with one outlier). Maybe a plot of the distribution would support this claim better?\n* p.9: Why are -1 and +1 lambdas excluded from Fig. 5? -1 seems the most popular choice in Fig. 2.\n* Regularization to a manifold in parameter space is well studied in Machine Learning, and indeed supports the argument that anisotropy may benefit performance. This is also linked to the idea that there is the level of representativity in the model that must match the intrinsic dimension or complexity in the data. Fig. 5 actually seems to show roughly consistent intrinsic dimensions for the three models used here. Is there a way you could put this in perspective with e.g. model size? Showing for exemple how anisotropy favors a roughly stable internal dimensionality in parameter space as model size increases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2129/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698937477786,
        "cdate": 1698937477786,
        "tmdate": 1699636145430,
        "mdate": 1699636145430,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YHK5HV24np",
        "forum": "dbQH9AOVd5",
        "replyto": "dbQH9AOVd5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2129/Reviewer_GH3h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2129/Reviewer_GH3h"
        ],
        "content": {
            "summary": {
                "value": "This study investigates the connection between isotropy and model performance by employing 3 distinct LLMs and 9 fine-tuning tasks. It introduces I-STAR, a method for adjusting model isotropy based on a novel differentiable metric called IsoScore*.\n\nSurprisingly, the study's findings contradict the prevailing notion in NLP literature. That is, it demonstrates experimentally that discouraging isotropy leads to better performance across the different models and downstream tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper challenges the dominant belief in NLP literature showing that anisotropy is beneficial. Its findings have the potential to significantly influence future research directions in the field.\n\nIt also introduces new way to compute the Isotropy in models. The authors conducted a set of experiments to show its efficiency comparing it to CosReg."
            },
            "weaknesses": {
                "value": "If we see a trend in Figure 3 on how higher IsoScore* leads to lower accuracy, some correlation and significance score should be added to support this claim."
            },
            "questions": {
                "value": "Could you add in the Appendix the same experiments done in Figure 3 but for CosReg and also the same in Figure 4 but for IsoScore*? This will give a better intuition of how these 2 different methods work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2129/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699404223753,
        "cdate": 1699404223753,
        "tmdate": 1699636145369,
        "mdate": 1699636145369,
        "license": "CC BY 4.0",
        "version": 2
    }
]