[
    {
        "id": "EOYZEYRvSP",
        "forum": "Jj8AAlNobk",
        "replyto": "Jj8AAlNobk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2774/Reviewer_YpGq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2774/Reviewer_YpGq"
        ],
        "content": {
            "summary": {
                "value": "The paper shows a direct connection between backpropagation and policy gradients. The authors thus leverage the advances in deep sequence models to try to improve policy gradient methods. The model proposed is called the Action-Sequence Model (ASM), where the model takes the initial state and the action sequence to predict the state sequence. The authors use a few examples and a testbed called Myriad to demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper tries to study the connection between policy gradient methods and deep sequence models and then improves the stability of the policy gradient methods. \n\n+ The paper provides easy-to-understand illustrations and formulations to show the ideas of connecting deep sequence models and policy gradient methods;\n+ The paper conducts experiments with both synthetic tasks like \"one-bounce environment\", \"copy task\", etc and real-world tasks like the Myriad testbed."
            },
            "weaknesses": {
                "value": "- There is some confusion about the experiments, especially on the comparison between action-sequence models and history-sequence models (which have states as conditions). The authors provide some explanations in the final paragraph of Page 8 and Page 9 but I don't think I am convinced. To me, state conditioning is necessary to predict the next states. Only conditioning on the actions does not provide complete information about the environment.  \n\n- The connection between policy gradients and RNNs/sequence models seems obvious in the literature. RL policies interact with environments in a recurrent function application manner, which corresponds to RNNs/sequence models. I don't quite see what brings the novel insights from the proposed understanding."
            },
            "questions": {
                "value": "Please answer and explain the weakness points above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2774/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698313576916,
        "cdate": 1698313576916,
        "tmdate": 1699636220353,
        "mdate": 1699636220353,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "j7KHTxpcvZ",
        "forum": "Jj8AAlNobk",
        "replyto": "Jj8AAlNobk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2774/Reviewer_ruP8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2774/Reviewer_ruP8"
        ],
        "content": {
            "summary": {
                "value": "This paper firstly introduced the task of reinforcement learning and the open-loop policy gradient with a deterministic MDP. Then it reformulates the policy gradient theorem using a sequence (action-sequence) model. By showing these two policy gradients are equivalent, it built the bridge between sequence modeling and the policy gradient.\n\nWith the theoretical connection, the authors then demonstrated that it is possible to leverage the advanced network structures in sequence modeling to improve the reinforcement learning, especially the tasks that need temporal credit assignment. This argument is supported with empirical results both under toy experiments and larger scale testbeds."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow. The motivation and the main idea are well presented.\n- The experimental results support the claim well, suggesting that advanced sequence modeling/prediction models can indeed lead to better credit assignment prediction."
            },
            "weaknesses": {
                "value": "I would recommend adding some clarification between the proposed method and various temporal credit assignment methods using sequence models. It would be beneficial to include comparisons in the benchmarks and experiments as well. (Some literature in this domain uses different testbeds, so additional experiments might be necessary.)"
            },
            "questions": {
                "value": "The results would be more convincing if some visualizations of what sequence models have learned are provided. This would help verify that the sequence model is indeed learning the credit assignment property, and that more advanced architectures might indeed enhance performance.\n\nThere is literature exploring the use of sequence models for direct temporal credit assignment, such as [1], [2], [3]. It would be beneficial to establish a connection between this work and these references, given the significant overlap in the motivation and methodology. Further clarification of the connections, differences, and novelties would be appreciated.\n\n[1]. Arjona-Medina, J. A., Gillhofer, M., Widrich, M., Unterthiner, T., Brandstetter, J., & Hochreiter, S. (2019). Rudder: Return decomposition for delayed rewards.\u00a0_Advances in Neural Information Processing Systems_,\u00a0_32_.\n\n[2]. Hung, C. C., Lillicrap, T., Abramson, J., Wu, Y., Mirza, M., Carnevale, F., ... & Wayne, G. (2019). Optimizing agent behavior over long time scales by transporting value.\u00a0_Nature communications_,\u00a0_10_(1), 5223.\n\n[3]. Liu, Y., Luo, Y., Zhong, Y., Chen, X., Liu, Q., & Peng, J. (2019). Sequence modeling of temporal credit assignment for episodic reinforcement learning.\u00a0_arXiv preprint arXiv:1905.13420_."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2774/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698835243877,
        "cdate": 1698835243877,
        "tmdate": 1699636220282,
        "mdate": 1699636220282,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KfGXJnVUw4",
        "forum": "Jj8AAlNobk",
        "replyto": "Jj8AAlNobk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2774/Reviewer_JUws"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2774/Reviewer_JUws"
        ],
        "content": {
            "summary": {
                "value": "The paper endeavors to bridge the gap between gradient propagation in neural networks and policy gradients to advance the field of sequential decision-making. Through theoretical assertions, the paper posits that state-of-the-art neural network architectures can enhance policy gradients. However, the empirical evidence provided to support this claim is not sufficiently convincing, primarily due to the narrow scope of the testing environments utilized. While the authors report improvements in long-term credit assignment and sample efficiency within an optimal control testbed, the paper fails to demonstrate significant innovation or provide a comprehensive comparison with existing methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The theoretical analysis presented is thorough, suggesting a potential for improved understanding of policy gradient methods."
            },
            "weaknesses": {
                "value": "1. Originality is a major concern for this submission. The idea of treating RL problems as sequence modeling tasks is not new and has been extensively covered in prior work, specifically in [1] and [2]. This paper does not clearly establish its unique contributions to the field, and the related work section is insufficiently detailed, lacking a critical analysis of how this work diverges from existing methodologies.\n\n2. The experimental design does not effectively differentiate the proposed method from established sequence modeling algorithms. A more robust comparison to state-of-the-art sequence modeling techniques, while not SAC and One-Step model, is necessary to validate the claims of the paper. Additionally, the benchmarks chosen for testing the methodology do not cover the breadth of scenarios needed to substantiate the authors' assertions. The inclusion of common continuous control benchmark tasks, such as Hopper, HalfCheetah, Walker and Antmaze, is essential for a more comprehensive evaluation.\n\n[1] Janner, Michael, Qiyang Li, and Sergey Levine. \"Offline reinforcement learning as one big sequence modeling problem.\" Advances in neural information processing systems 34 (2021): 1273-1286.\n\n[2] Chen, Lili, et al. \"Decision transformer: Reinforcement learning via sequence modeling.\" Advances in neural information processing systems 34 (2021): 15084-15097."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2774/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699224879502,
        "cdate": 1699224879502,
        "tmdate": 1699636220207,
        "mdate": 1699636220207,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k2G8OROQ70",
        "forum": "Jj8AAlNobk",
        "replyto": "Jj8AAlNobk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2774/Reviewer_KDm5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2774/Reviewer_KDm5"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a model-based deterministic policy gradient method for finite-horizon MDPs with deterministic and differentiable transition kernel. In this case the cumulative reward is deterministically determined given a deterministic policy, and its gradient with respect to the policy parameters, i.e. the policy gradient, can be computed by differentiating the transition kernel. Since the true transition kernel is unknown, a baseline solution is to learn a one-step Markovian transition model, then computing the policy gradient by differentiating this learned Markovian transition model. This paper however proposes to not learn one-step model, but to learn a multi-step transition model which takes a sequence of actions as input and predicts a sequence of resulted states as output. Such a multi-step transition model is called Action Sequence Model (ASM) in this paper. The policy gradient is then computed by differentiating over the learned ASM. Despite the Markovian property of the true transition kernel, the paper argues that learning such an multi-step ASM model is a better choice than learning a one-step Markovian model, for the sake of gradient based policy optimization. \n\nIt should be noted that the policy gradient discussed in this paper is limited to \"open-loop policies\" that generate actions without taking the observed states into account. For the more general class of close-loop policies,  the so-called \"open-loop policy gradient\" as defined and discussed in this paper is not the true policy gradient, but is related to the true gradient in the sense that we can obtain the former if ignoring the $\\partial a/\\partial s$ terms in the latter."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "In general, I think it is an very interesting topic to explore whether in model-based RL it's benefitable to learn not directly the underlying MDP model but another form of model. The experiment part of the paper made several good points to this end. For example, the Chaotic experiment in Section 4.2 nicely illustrates a case where even perfectly learning the Markovian transition model can lead to chaotic policy gradients while ASMs can smooth out the gradients and therefore lead to better policy optimization. It is also quite intriguing to see that, in Section 4.3, ASMs lead to better policy optimization than models with full history info including the states (although I'm not sure about the explanation about this phenomenon provided in the paper)."
            },
            "weaknesses": {
                "value": "**(a)** I am not sure that the theory part (Section 3) well support the claim that action-sequence models are better choice than one-step Markovian models. Only the norm of the gradients induced by two special cases of these two classes of models are compared, but in practice the estimated gradient is often normalized so the norm is less important, in my impression. On the other hand, the accuracy of the gradient direction may be a more important factor, I suspect, but is not analyzed at all in the theory part. Also see my Question 1~4 below for several soundness concerns about the theory part.\n\n**(b)** I am not sure if the baselines in the experiment part (Section 4) are strong enough to establish the advantage of ASM over one-step models. In particular, the one-step model tested in the experiment seems to be a very simple one. See my Question 5~6 below for the detailed concerns.\n\n**(c)** The current results of this paper seem to have limited applicability scope: it seems to mainly applicable to environment that is deterministic, differentiable, with fixed episode length, where open-loop policies are sufficient for the environment. In this special case, the RL problem degenerates to a simple black-box optimization problem where we maximize an unknown but deterministic objective function over an action-sequence space. It would be more interesting if the paper can discuss more complex situations, such as those that require close-loop control."
            },
            "questions": {
                "value": "1. Page 4, in the paragraph below Proposition 1, you said it's a \"fundamental fact\" that PG with Markovian model is \"fundamentally ill-behaved\", and you said this fact is \"analyzed in-depth in this section\". I don't quite understand this sentence and am not sure about the analysis either. By \"ill-behaved\" do you mean the exponential upper bound in Corollary 1.1? But Corollary 1.1 applies only to a special Markovian model, the model with linear units. What about other Markovian models? In what sense can we conclude that Corollary 1.1 is due to the Markovian property, instead of to the linearity or other limits of the model under consideration?\n\n2. How do we know that the upper bound in Corollary 1.1 is tight? Without tightness, an upper bound like Corollary 1.1 is not enough to support your claim that the gradient of RNN models will \"explode exponentially fast\". To support such a claim, we typically need a *lower bound* result.\n\n3. Even though it really could be proved that the gradient of Markovian model with true transition kernel grows exponentially with the horizon length -- even though we suppose this were true in this question -- this means the *unbiased* policy gradient optimization is unstable and we perhaps should not use the model-based policy gradient method at all in this case, isn't it? Importantly, although the gradient with Transformer is better bounded in this case, since we know that it's *not* the true policy gradient (because the true policy gradient has larger norm), how do we know that the gradient from transformer is different from an arbitrary small-but-biased gradient, in terms of its effectiveness to power the policy optimization?\n\n4. Page 5, in the paragraph below Corollary 1.1, you said \"Corollary 1.1 explains both the difficulties ... and the limitations ...\". What are exactly the difficulties and limitations here? Why does your upper bound result indeed *explain* them (rather than just coincident with them)? The argument here is not self-contained so it's hard to evaluate its soundness.\n\n5. In your experiments, what's the difference between the \"ASM(RNN)\" model and the \"One-step Model\"? Are they use the same linear transition kernel given by Eq.2, that is, is ASM(RNN) equivalent to unrolling one-step model for H steps? While you upgrade the ASM models from simple RNN to LSTM and Transformer, did you also try to upgrade the one-step model from a simple linear model to more sophisticated ones?\n\n6. In Section 4.3, are the environments here partially observable? Does a state info $s_t$ give a Markovian state or only the partial observation of the full state? I am not sure that the capability to see the additional state info for \"History Transformer\" can really account for its bad performance, given that the attention modules in Transformer can be trained to simply ignore the state info if they are not helpful. On the other hand, one-step Markovian models are just not appropriate for POMDP environments, so I'm not sure they should be included as baselines in this experiment."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2774/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699299967505,
        "cdate": 1699299967505,
        "tmdate": 1699636220120,
        "mdate": 1699636220120,
        "license": "CC BY 4.0",
        "version": 2
    }
]