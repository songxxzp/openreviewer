[
    {
        "id": "3mB4SsU9FU",
        "forum": "pOBvr1PxFd",
        "replyto": "pOBvr1PxFd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7752/Reviewer_njDu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7752/Reviewer_njDu"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a method for assigning non-uniform layer-wise sparsity for pruning large language models. Previous works of SparseGPT and Wanda all adopt a layer-wise uniform strategies. It is thus possible to compress LLMs more with a non-uniform layerwise sparsity level. The proposed method, formally called Outlier Weighted Layerwise (OWL), assigned sparsity level per layer proportional to the outlier ratio in each layer. Experiment results demonstrated the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Non-uniform sparsity is an important problem in network pruning. Existing works use the same sparsity level across all layers. Gaining insight into which layer should be pruned more would be valuable for the pruning community.  \n2. The analysis on the outlier distribution is thorough and provide a clear motivation as to the proposed method OWL.  \n3. The proposed method OWL is not computationally extensive and could be applied easily on top of existing LLM pruning methods.  \n4. The finding that the effectiveness of existing pruning methods is closely related to the ability to retain outlier features is interesting.   \n5. Empirical results are strong, which outperform both Wanda and SparseGPT by a large margin on high sparsity levels."
            },
            "weaknesses": {
                "value": "1. The analysis of this paper on non-uniform sparsity is mostly focused on the whole Transformer block. However, less insights are provided as to which type of layer should be pruned more.  \n2. Unstructured sparsity may be less practical as it needs special hardware for acceleration."
            },
            "questions": {
                "value": "1. It would be good to discuss how the approach could be applied to non-uniform sparsity per FC layer rather than the entire Transformer block. In this way, we could understand better which type of layers in Transformer should be pruned more.  \n2. Could the definition of LOD in section 3.2. be defined more precisely? This seems to be an important analysis target in section 3. Currently it is described by text which is somewhat hard to understand.  \n3. While it is not the focus of this work, I am interesting as to how would OWL perform on other networks, e.g. ConvNet or Vision Transformer. Adding such analysis would be beneficial as to understand the effect of outliers on pruning LLMs.  \n4. While structured sparsity enforces a fixed sparsity ratio 50%, i think OWL could be potentially applied for this type of sparsity. Instead of a dynamic sparsity ratio, we could choose to either prune a FC layer with 2:4/4:8 sparsity or skip this FC layer entirely. I think such analysis would make this work more comprehensive."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7752/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698631049468,
        "cdate": 1698631049468,
        "tmdate": 1699636946200,
        "mdate": 1699636946200,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EiCWzTf6K4",
        "forum": "pOBvr1PxFd",
        "replyto": "pOBvr1PxFd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7752/Reviewer_H127"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7752/Reviewer_H127"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses the challenge of deploying large language models (LLMs) and proposes a pruning methodology called Outlier Weighed Layerwise sparsity (OWL) that incorporates non-uniform layerwise sparsity ratios based on outliers in token features. The evaluation shows OWL outperforms previous methods in terms of performance at high sparsity levels. The paper also highlights the competition among tech giants to build billion-parameter LLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Outlier Weighed Layerwise sparsity (OWL) incorporates non-uniform layerwise sparsity ratios based on outliers in token features in the following way:\n\n\u2022\tThe authors first analyze the distribution of \"outlier\" features across layers in dense LLMs. Similar in the quantization case, they find the distribution is highly non-uniform. They then study how different pruning methods like magnitude pruning, SparseGPT and Wanda affect the retention of outliers. They find performance is strongly correlated with preserving outliers.\n\u2022\tBased on these insights, the key idea of OWL is to assign a target layerwise sparsity for each layer based on the outlier ratio observed in that layer. Layers with a higher outlier ratio will be assigned a lower (less aggressive) layerwise sparsity ratio. This allows layers with more important outlier features to be pruned less. \n\u2022\tTo calculate the layerwise sparsity ratios, OWL first calculates the Layerwise Outlier Distribution (LOD) of feature effects on weights for each layer. This captures the outlier ratio for each layer. The target layerwise sparsity for each layer is then set to be directly proportional to the outlier ratio for that layer from the LOD. This aligns the sparsity ratio with the outlier distribution, preserving more outliers in layers where they are more prevalent. \n\nExperiments on LLaMA and OPT models show OWL outperforms baselines like Wanda and SparseGPT significantly at high sparsity levels, e.g. 61.22 perplexity reduction at 70% sparsity. The authors argue this work highlights the importance of layerwise sparsity ratio in LLM pruning, which was previously overlooked."
            },
            "weaknesses": {
                "value": "One notable drawback of this paper lies in its exclusive focus on unstructured and element-wise sparsity, which offers limited hardware advantages. Unlike SparseGPT and Wanda, the authors have not provided any data on N:M sparsity. This omission represents a significant limitation and may reduce the paper's appeal to large language model practitioners.\n \nAs indicated in Table 3, OWL's effectiveness becomes more pronounced in smaller models, such as the 7B and 13B variants, whereas its impact appears to be less significant at the 30B and 65B levels. Additionally, even in the case of the 7B and 13B models, the \"improved\" perplexity scores after applying OWL remain relatively poor, typically exceeding 20, which limits their practical utility."
            },
            "questions": {
                "value": "See the two questions in the weakness part"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7752/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7752/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7752/Reviewer_H127"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7752/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698632137663,
        "cdate": 1698632137663,
        "tmdate": 1700649800614,
        "mdate": 1700649800614,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MwvCtEs889",
        "forum": "pOBvr1PxFd",
        "replyto": "pOBvr1PxFd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7752/Reviewer_qHFG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7752/Reviewer_qHFG"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel LLM pruning methodology called Outlier Weighed Layerwise sparsity (OWL), which incorporates outlier detection to achieve high sparsity without sacrificing performance. The paper discusses the challenges of applying traditional network pruning techniques to LLMs and highlights the importance of non-uniform layerwise sparsity. The authors demonstrate the effectiveness of OWL through empirical evaluation across various benchmarks, showing a remarkable performance gain surpassing the state-of-the-art methods."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Recent efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned without hurting performance. However, achieving higher >50% sparsity for performant LLMs remains as an open challenge.\n\nIn previous quantization literature, it is known that the distribution of token features within LLMs has a strong correlation with the emergence of outliers, defined as features exhibiting significantly greater magnitudes compared to their counterparts in feature dimensions. The paper suggests that non-uniform layerwise sparsity can yield substantially improved results by leveraging the same observation, and therefore incorporates outlier detection to achieve high sparsity without sacrificing performance.\n\nSpecifically, their new LLM pruning methodology incorporates outlier detection by introducing a novel layerwise sparsity ratio, denoted as Outlier Weighed Layerwise sparsity (OWL). OWL assigns greater emphasis to layers housing a higher prevalence of outliers, thereby facilitating more nuanced coordination between sparsity in weight matrices and the presence of outliers within the layer. This approach allows for high sparsity without sacrificing performance, as it takes into account the importance of outlier structures in LLMs.\n\nThe experimental evaluation conducted in this paper demonstrates the distinct advantages offered by OWL over previous methods. The authors show that OWL achieves a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high sparsity level of 70%, respectively. Additionally, the paper presents experiments to evaluate the zero-shot ability of various sparse LLMs on diverse zero-shot downstream tasks with prompting, and shows that OWL consistently improves accuracy across nearly all settings, with very few exceptions."
            },
            "weaknesses": {
                "value": "A quite apparent limitation of this paper is its exclusive examination of unstructured pruning, without addressing more practically relevant forms of structured pruning, such as layerwise, attention head, or N:M weight sparsity. It remains uncertain whether the proposed layerwise sparsity ratio would maintain its relevance in the context of these alternative pruning approaches. I would very much like to see some preliminary results in that regard. \n\nFurthermore, there is a question about the continued relevance and interest in improving pruning techniques for Large Language Models (LLMs). Recent research studies (as outlined in https://arxiv.org/abs/2307.02973) have shown that pruning tends to lag behind quantization in performance when considering similar storage constraints. In general, the industry has leaned towards quantization and low-rank compression due to the more established hardware support for these methods.\n\nWith that said, it would be intriguing if the authors considered extending their ideas to areas such as mixed-precision quantization or the exploration of adaptive layer-wise rank selection, whether in the context of compression or LoRA fine-tuning."
            },
            "questions": {
                "value": "Please see above, especially the first weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7752/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698633319708,
        "cdate": 1698633319708,
        "tmdate": 1699636945962,
        "mdate": 1699636945962,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dX1wbCXoBl",
        "forum": "pOBvr1PxFd",
        "replyto": "pOBvr1PxFd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7752/Reviewer_jZ6Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7752/Reviewer_jZ6Q"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the impact of non-uniform layerwise sparsity in LLM pruning. In particular, authors propose a new layerwise sparsity ratio that is designed to preserve the weights connected to the \"outlier\" features. At high sparsity levels, the LLMs pruned with the proposed \"OWL\" layerwise sparsity tend to have a better perplexity than uniform sparsity."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "**Presentation**  \nMost figures are visually very appealing.\n\n**Related work section**  \nThe related work gives a concise yet informative summary of the related work in the field (although I find some of them quite unnecessary).\n\n**Baselines**  \nThe baseline methods for layerwise sparsity are mostly well-selected."
            },
            "weaknesses": {
                "value": "**Clarity (Medium)**  \nSome essential concepts are confusingly defined in the text: What exactly is $D_i$ (in LOD)? I presume that it is the ratio of \"outlier-ish\" weights, but how can it be over 1 in Figure 1 left?\n\n**Conclusions from \"Empirical Studies I, II, III\" (Major)**  \nThe overall motivation of the proposed algorithm seems to come from the so-called \"empirical studies\" in section 3.2., which attempt to connect the notion of LOD (layerwise outlier distribution) with the layerwise sparsity. It is very difficult for me, however, to follow how the conclusions they make can be logically deduced from their observations (it seems to be circular logic). In particular:\n- **Empirical Study 1.** The empirical observation is that LOD is non-uniform on dense Llamas. From this fact, the paper jumps to the conclusion that \"employing uniform pruning across all layers would inevitably disrupt the outlier structure in layers.\" I find this point very difficult to follow, unless the authors have a way to prove that the LOD itself, for some arbitrary $M$, is an informative criterion for pruning. But is it really so? Why is the ratio between the weight and the layerwise average important, from the first place? Perhaps the authors are resorting to some circular logic.\n- **Empirical Study 2.** This part is again very confusing. I do not see how having higher LOD for non-zero weights is equal to \"preserving more outliers.\" The reason is twofold: First, the outliers after pruning may not necessarily be identical to the outliers before pruning. Second, the pruning also changes the average. \n- **Empirical Study 3.** I cannot find a clear logical conclusion that can made from this observation. How does this motivate the algorithm?\n\n**Target sparsity (Major)**  \nThe empirical evaluations are mostly made at, what I would call, the \"useless\" sparsity level. See, for instance, Table 3. The perplexities of the OWL-pruned models are quite high compared with the dense baselines (although smaller than baselines). I do not think anybody would really care too much about which method wins in the sparsity regime where the LLMs cannot really be practically useful. Same happens in Table 4.  \n\nA much more meaningful way to compare will be comparing the critical sparsity levels, i.e., the maximum sparsity where the performance metrics remain relatively similar to the dense model. To me, it seems likely to me that OWL is no better than uniform in this sense. In fact, in table 5, uniform works comparably and mostly better than OWL at lower sparsity levels.\n\n**Practicality (Major)**  \nUniform 50% sparsity is practically more useful than non-uniform sparsity at similar global sparsity level, because of the hardwares that support 2:4 sparsity.\n\n**ERK (Minor)**  \nIt is quite confusing why authors compare with ERK instead of ER. ERK is something that is used for convolutional layers. Does Llama have any?\n\n**Globa (Minor)**  \nCiting Frankle & Carbin (2019) for the global threshold is simply wrong (section 5.1.). Look at their figure 1, the last row.\n\n**Typos (Minor)**  \n- Sometimes, the paper says \"outlier-weighted\" instead of \"outlier-weighed.\"\n- Section 2, paragraph 2, 3rd line from bottom, \"inefficacy\" should be something else."
            },
            "questions": {
                "value": "- I do not understand the sentence \"However, global pruning becomes extremely expensive and inefficacy in the context of LLM pruning as shown in Table 2.\" Have you compared the computational cost of sorting the parameters with the cost of computing saliency scores that typical LLM pruning algorithms use? Without an actual comparison, I am not really convinced about this point."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7752/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7752/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7752/Reviewer_jZ6Q"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7752/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699022531178,
        "cdate": 1699022531178,
        "tmdate": 1700719325250,
        "mdate": 1700719325250,
        "license": "CC BY 4.0",
        "version": 2
    }
]