[
    {
        "id": "pLSVTQblIC",
        "forum": "ngp5jzx5oK",
        "replyto": "ngp5jzx5oK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5050/Reviewer_reHV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5050/Reviewer_reHV"
        ],
        "content": {
            "summary": {
                "value": "In the paper, the authors proposed a new method for modeling speaker timbre. Specifically, to encode the speaker characterize, a VAE-like model is trained to encode speech to \u03bc and \u03c3. And then the \u03bc is used to cluster to several cookbooks that represent the speaker information. Through the attention alignment between text representation and codebook, the text and speaker representation are fused into synthesis speech. The experiments shows the propose method performs superior to previous multi-speaker model and outperforms a zero-shot model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. A new speaker modeling method is proposed for zero-shot generation.\n2. The demo provided by the authors seems convincing.\n3. The experiment results show the performance of the proposed method."
            },
            "weaknesses": {
                "value": "1. The relations between previous works and this paper are not well presented in Section I.  \"The zero-shot method obtains a speaker vector from a short reference audio, and the timbre and prosody expressed in the given reference audio are aligned with its content. In other words, only a small portion of the speech characteristics that the speaker can express can be obtained from the reference audio.\" In recent studies, many methods try to capture more speaker timbre from reference speech. These methods share a similar process, which is to encode speech to serval vectors and then use attention to fuse the text and speaker timbre. For example, in TTS, https://www.isca-speech.org/archive/pdfs/interspeech_2022/zhou22d_interspeech.pdf, NANSY++, RetrieverTTS.\n2. The proposed method is not presented very clearly. For example:\n2.1 why \u03bc is chosen for clustering\n2.2 how to perform the clustering\n2.3 how many codebooks are used for representing a speech.  Does the number of codebooks have a relation to the number of cluster centers?\n2.4 how to obtain the codebook and what is the number of cookbooks in the codebook set?\n3. As mentioned in W.1,  many previous zero-shot TTS methods also represent reference speech to a set of vectors and fuse them with text via attention. Though I can understand the proposed system and comparison system all use VITS as the backbone for fair comparison, I think it better to compare your speaker modeling method with the others for a better understanding of the superiority, since the ELF is the main claimed contribution."
            },
            "questions": {
                "value": "/"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5050/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5050/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5050/Reviewer_reHV"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5050/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698219451827,
        "cdate": 1698219451827,
        "tmdate": 1699636494819,
        "mdate": 1699636494819,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qTtSNkGtLw",
        "forum": "ngp5jzx5oK",
        "replyto": "ngp5jzx5oK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5050/Reviewer_8YZU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5050/Reviewer_8YZU"
        ],
        "content": {
            "summary": {
                "value": "The work introduces a method for speaker modeling targeting speech synthesis, capturing their characteristics without specific training on each speaker's dataset. This approach outperforms existing methods and can generate artificial speakers effectively. The encoded features are claimed as informative enough to fully reconstruct an original speaker's speech, making it versatile for various applications."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper addresses"
            },
            "weaknesses": {
                "value": "1. The connection between the method itself and earlier works are not very clear. The author should have a short review of the related literature.\n2. The methods acquired themselves are sometimes a bit abrupt and lacks motivation.\n3. The prototype of the model does not show very notable improvements in metrics other than MOS scores for audio. Speaker blending should be applied to reach better performance.\n4. The study also lacks comparison with other fixed/earlier speech encoders or speaker encoders."
            },
            "questions": {
                "value": "1. Possibly because of the first point in weaknesses, in Section 2.2, I do not see strong motivation of acquiring autoencoders, despite there are multiple speaker encoding methods available (e.g. speaker encoders; speech factorization methods). Could you please clarify the motivation?\n2. Why speaker blending is needed to reach better CER than the ground truth? And is there any alternative to compensate?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There is no ethical concern for this study from the reviewer's perspective."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5050/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698474192903,
        "cdate": 1698474192903,
        "tmdate": 1699636494735,
        "mdate": 1699636494735,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mQQyLGdYQw",
        "forum": "ngp5jzx5oK",
        "replyto": "ngp5jzx5oK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5050/Reviewer_WfZD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5050/Reviewer_WfZD"
        ],
        "content": {
            "summary": {
                "value": "In this work authors obtain a zero-shot speech cloning method via clustering. VAE type model is used to model the utterance latent space, which is then clustered and the key idea seems to be that each speaker falls into multiple clusters. Final speaker representation is the mix of these cluster centroids. When the query utterance is fed into the system it follows the same path as in training. So speakers not seen in training can be utilized cloned achieving zero-short method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Neat key idea, where speakers fall into multiple clusters. This achieves modeling diversity of speakers voice characteristics. \n- Good empirical results."
            },
            "weaknesses": {
                "value": "- Final SFEN objective (Eqs. 2-3) comes out of thin air, it would be good to somehow try to explain theoretically how this can be derived, noting that VAE loss is derived from trying to model log p(x). \n- Continuing the above critizism, elements in the loss are by necessity weighted somehow. How you decide the proper weighting? \n- Plotting cosine scores as objective speajer recognition is ok, but proper speaker recognition results are needed. As in cosine score you only compare against targer speaker and totally miss the confusion with the non-target speaker. So presenting EER and minDCF values with the accomanying DET plots are necessary. \n- Paper neeeds more thorough language editing but this can be performed in rebuttal stage. Abstract was badly written, but some other parts of the paper are quite ok."
            },
            "questions": {
                "value": "- I see that MT was used to obtain MOS and SMOS scores. How did you clean MT results as those can be sometimes extremely noisy. MT workers sometimes use scripting to speed up the work and so those those workers should be removed from the results. \n- How did you measure CI for MOS and SMOS?\n- Were all samples finally voceded using the same voceder before objective speaker recognition? If not, then you can easily add (inaudible) vocoder artifacts that your TDNN model can then pick up. Vocode all samples, even ground truth ones, using the same vocoder. \n- Why in Table 1, proposed CER is lower in #20 than in all audio?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5050/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5050/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5050/Reviewer_WfZD"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5050/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698583950368,
        "cdate": 1698583950368,
        "tmdate": 1700733329861,
        "mdate": 1700733329861,
        "license": "CC BY 4.0",
        "version": 2
    }
]