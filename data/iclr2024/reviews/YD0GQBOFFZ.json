[
    {
        "id": "RdgM4MDEW2",
        "forum": "YD0GQBOFFZ",
        "replyto": "YD0GQBOFFZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1429/Reviewer_9Y2W"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1429/Reviewer_9Y2W"
        ],
        "content": {
            "summary": {
                "value": "Tabular data generative models strive to master the underlying process that produces observed data, aiming to generate synthetic data that mirrors the observed distribution. Over the past decade, various methods, from statistical to deep-learning-based approaches, have emerged to understand these distributions. A key challenge, however, is the evaluation of the generated synthetic samples. Since the true data generating process remains unknown, measuring the effectiveness of these models is not straightforward. While many attempts have been made to standardize evaluation methodologies and to distill metrics into a consolidated framework, they often fall short in terms of objectivity and clarity in interpretation, as noted by the authors. Addressing this, the paper seeks to introduce a unified evaluation framework that consolidates current metrics under a single mathematical objective. This objective is built on the premise that synthetic data are drawn from the same distribution as the observed data. To further bolster their evaluation approach, the authors suggest leveraging the probabilistic cross-categorization method as a stand-in for the elusive ground truth of the data generating process."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I find the authors incorporation of Probabilistic Cross-categorization\u2014 a domain-general method designed for characterizing the full joint distribution of variables in high-dimensional datasets\u2014 particularly intriguing, especially in the realm of tabular data generation. This is my first encounter with this approach in a benchmarking context, and its novelty in the author's work is commendable."
            },
            "weaknesses": {
                "value": "I commend the authors efforts in detailing various metrics and particularly the authors exploration into the nuances between model-free metrics and the PCC-based metric. A deeper elaboration on this distinction would be immensely helpful for readers to fully grasp the nuances. \n\nThe representation in Figure 1B, specifically regarding the spectrums, may benefit from further context or an enriched explanation. This raises a query: Are the authors implying that model-free evaluations, such as those estimating marginal or pairwise relationships, may not provide a holistic perspective? Is there an inherent advantage in adopting model-based techniques, like PCC, to act as proxies for real data while assessing the same metrics? Moreover, given that PCC operates as a synthetic model, does its role in the evaluation process imply a comparison between synthetic models through another synthetic standard? Gaining clarity on these nuances would greatly enhance understanding. \n\nIt would also be illuminating to discern how this work either mirrors or deviates from established frameworks in previous literature. While the authors' initiative to broaden the metrics spectrum and introduce a surrogate model approximating real-data probability distribution is commendable, elaborating the distinct facets or innovative insights of the author's proposal, especially vis-\u00e0-vis findings in [1, 2] referenced in Questions section, could accentuate the originality and significance of the research amidst prevailing knowledge."
            },
            "questions": {
                "value": "General comments & questions\n=========================\n\n- In section 3, the authors mentioned that \u201cthe objective of the data synthesiser is \\( Q=P \\)\u201d. While I understand the underlying objective might be to highlight the close similarity between the distributions, stating it in this manner might lead some readers to interpret this as \\( Q \\) being an exact replica of \\( P \\). Given the paper's central theme of using \\( Q \\) as a more private alternative to \\( P \\), such an interpretation could be seen as contradictory. Perhaps it might be clearer to emphasize that \\( Q \\) is intended to be statistically analogous or mirrors the distribution of \\( P \\). This would signify that while \\( Q \\) captures the broader statistical characteristics of \\( P \\), individual data points might differ, ensuring privacy.  I believe a more detailed description or clarification in this section could be beneficial for enhancing the reader's understanding and mitigating potential misconceptions.\n\n- The presentation of the leave-one-out (LOO) metric seems to bear a resemblance to the dimension-wise prediction performance metric as described in references [3, 4], as well as the all model's test metric outlined in [2]. Could the authors clarify whether these are synonymous or if there's a discernible distinction between them?\n\n- Rather than depending on a surrogate model to estimate ground truth, would it not be more reliable to employ a distinct hold-out test set, ensuring it retains the same distribution as the real (observed) data? Admittedly, this approach might pose challenges when dealing with limited samples. However, in such scenarios, methodologies like k-fold validation could be explored to compute an average score over several iterations. Alternatively, having a baseline that shows the performance of the surrogate on hold-out test set could serve as the acceptable error threshold.\n\n- The current presentation of details incorporates a variety of symbols, which, while comprehensive, can sometimes add complexity to the narrative without necessarily enhancing clarity. To improve readability and facilitate a deeper understanding for readers, I'd recommend introducing a dedicated subsection early on to familiarize readers with the notation. This way, within the main text, the authors can focus on using notation only when it brings forth novel information, and rely on plain language descriptions when the content permits. For instance, the passage: \"We then use the surrogate model to compute   \\{ \\hat{P(X_i) \\mid i=1,..,n \\}, which is the likelihood of  X_i\u2026\" could be more intuitively conveyed as: \"We use the surrogate model to determine the likelihood of the real data samples under this model.\" If the precise mathematical formulation is essential to the discussion, consider placing it in a distinct equation block, which can then be easily referenced within the narrative.\n\n- In section 3.3, the discussion surrounding the pp-plot could benefit from further clarity. I was wondering if the likelihood estimate method introduced is akin to the \"Distance to Closest Record\" concept mentioned in [5], where a Nearest Neighbours model is employed to gauge the likelihood of each synthetic data originating from the real data distribution. Is the primary distinction here the use of the Probabilistic Cross-Categorisation model? Any elucidation on this comparison would be invaluable for readers familiar with the referenced methodology.\n\n\n- Given that the evaluation encompasses a diverse range of metrics within the same family, such as marginal, pairwise-based, and leave-one-out conditionals, full-joint-based, missingness, and privacy. It might be insightful for readers if a correlation plot is provided. Such a plot could help elucidate potential correlations among metrics both within the same group and across different groups. This added visual representation could offer a comprehensive perspective on the interplay of these metrics and their potential overlaps or distinctions.\n\n\nSmall typo\n====\n\nFigure 1 (A) Model fee -> Model free\n\n(Potential) missing reference\n======================\n\nIt appears there's an omission in the paper's review of related literature. In particular, ref. [2] in its section 3 emphasizes the significance of evaluating synthetic tabular data generators across various metrics, including marginal-based, column-pairs, joint, and utility considerations. The thrust of these discussions in [2] bears a strong resonance with the core objectives of this paper. It's surprising and noteworthy that such pertinent work isn't cited or discussed in the current paper's related work section.\n\nReferences\n=========\n\n[1] Dankar, F.K., Ibrahim, M.K. and Ismail, L., 2022. A multi-dimensional evaluation of synthetic data generators. IEEE Access, 10, pp.11147-11158.\n\n[2] Afonja, T., Chen, D. and Fritz, M., 2023. MargCTGAN: A\" Marginally''Better CTGAN for the Low Sample Regime.\u00a0arXiv preprint arXiv:2307.07997.\n\n[3] Choi, E., Biswal, S., Malin, B., Duke, J., Stewart, W.F. and Sun, J., 2017, November. Generating multi-label discrete patient records using generative adversarial networks. In\u00a0Machine learning for healthcare conference\u00a0(pp. 286-305). PMLR.\n\n[4] Engelmann, J. and Lessmann, S., 2021. Conditional Wasserstein GAN-based oversampling of tabular data for imbalanced learning.\u00a0Expert Systems with Applications,\u00a0174, p.114582. \n\n[5] Zhao, Z., Kunar, A., Birke, R. and Chen, L.Y., 2021, November. Ctab-gan: Effective table data synthesizing. In\u00a0Asian Conference on Machine Learning\u00a0(pp. 97-112). PMLR."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1429/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698159445907,
        "cdate": 1698159445907,
        "tmdate": 1699636071197,
        "mdate": 1699636071197,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qlJZgHaXlr",
        "forum": "YD0GQBOFFZ",
        "replyto": "YD0GQBOFFZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1429/Reviewer_UGhN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1429/Reviewer_UGhN"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose an analysis framework for evaluating data synthesizers. Data synthesizers aim to create synthetic datasets that resemble real datasets without directly copying them, i.e., the goal of such synthesizers is to generate synthetic datasets of a distribution Q that is as close as possible to the distribution P of the real dataset. The authors have conducted a structured evaluation of SOTA techniques for data synthesis on different datasets for varying evaluation criteria for distributions ranging from Missingness to Univariate Marginal to Pairwise Joint to Leave-One-Out conditionals to Full Joint Distribution."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The topic of data synthesis is highly relevant for many real-world applications where data is very costly to obtain or privacy is a major concern.\nThe presentation is well-structured and detailed.\nThe authors have taken a systematic approach to evaluate different synthesizers in comparative way. They have considered different metrics and provided clear explanations for their choices."
            },
            "weaknesses": {
                "value": "Although well-structured, the presentation is quite dense, and it might be challenging for someone without a background in the area to understand the differences and significance of the analysis framework and findings.\nThe paper has a strong focus on the technical evaluation of synthesizers, but it doesn't discuss the practical implications of the findings. I.e., how might these results impact real-world applications of these synthesizers?\nIt would be useful to know how the methods would have performed on large-scale datasets if computational resources were not a constraint.\nWhile the proposed metrics focus on a quantitative evaluation, qualitative insights or user-based evaluations might provide a more holistic view of synthesizer effectiveness."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1429/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1429/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1429/Reviewer_UGhN"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1429/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698573646282,
        "cdate": 1698573646282,
        "tmdate": 1699636071118,
        "mdate": 1699636071118,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ssq6uFYyzM",
        "forum": "YD0GQBOFFZ",
        "replyto": "YD0GQBOFFZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1429/Reviewer_fbzR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1429/Reviewer_fbzR"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel evaluation framework for evaluating tabular data generators. The framework has been generated with a single mathematical objective, i.e., that a data generator should produce samples from the same joint distribution as the real data. The framework includes both existing and novel metrics which are then arranged along a  spectrum which holds for both model-free and model-based metrics."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The main strength of the paper is that such work is very much needed. Additionally, I like the idea of arranging the metrics according to a spectrum that highlights the complexity of the relationships among features that a metric is able to capture."
            },
            "weaknesses": {
                "value": "I think the paper needs a lot of rewriting (and probably more space, I would suggest to the authors to submit to a journal). \nAt times it is quite difficult to follow and a lot of the metrics that are presented in Table 1 are not covered at all in the main text. \nAlso, it is not feasible to think that one will evaluate their models according to all the metrics shown in Table 1. A significant contribution would be to identify different subsets of these metrics and show how to use them together to capture all the desired properties of the system (see, for example, what was done for multi-label classification problems in [1]). \n\nAlso, I have some questions regarding how the ML efficacy belongs to the substructure \"leave-one-out conditional distribution\". Indeed, in order to *leave-one-out* then you assume that the target is a single column. In many cases, the target might not be a single column. How would this affect your thesis? Even more importantly, you write that the implication holds if we do this *for all* $j$s (i.e., by leaving out all columns). The ML efficacy test leaves out a single column, so how do you get sufficiency in this case? Finally, to define the score you use the function argmax, what happens if the problem is a regression problem or binary? \n\nRegarding the full joint substructure you write: \"Sufficiency is hard to show because...\". Is it sufficient? Is it necessary? \nIn general, for all these substructures, I would have liked to see a much more structured approach to showing sufficiency and necessity. \n\nI am also not sure whether I fully understand the properties of the HALF baseline. Could you please clarify why is it useful and why it provides an upper bound? Also, do you have some proof of the upper and lower bounds provided by the baselines? \n\nAt a certain point on page 6, the authors write: \"We then use pp-plot to compare these two lists of probabilities.\" What is pp-plot? Why is it used? \n\nIn Table 2, why is Quality not reported for 2 models on the census dataset?\n\n\nMinor comments: \n- there is a typo on page 5 in the Missingness paragraph: $Q_v(c_j) = P_v(c)j)$\n- add a bird-eye view of subsection 3.2 rather than just starting to describe baselines one by one\n- put tables in the right format (i.e., as large as text)\n\n\n[1] Spolaor, N., Cherman, E. A., Metz, J., & Monard, M. C. (2013). A systematic review on experimental multi-label learning. Tech. rep. 362, Institute of Mathematics and Computational Sciences, University of Sao Paulo."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1429/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698605260719,
        "cdate": 1698605260719,
        "tmdate": 1699636071022,
        "mdate": 1699636071022,
        "license": "CC BY 4.0",
        "version": 2
    }
]