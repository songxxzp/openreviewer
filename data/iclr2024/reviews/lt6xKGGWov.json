[
    {
        "id": "05ynLhZr5I",
        "forum": "lt6xKGGWov",
        "replyto": "lt6xKGGWov",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission112/Reviewer_5aJX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission112/Reviewer_5aJX"
        ],
        "content": {
            "summary": {
                "value": "This papers proposes a novel feature selection algorithm based on estimating the mutual information by using neural networks. The authors propose to test it against a synthetic dataset, showing that the algorithm is able to successfully select the important features."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Feature selection experiments with synthetic data should always be used to estimate the performance of the algorithms."
            },
            "weaknesses": {
                "value": "- The paper writing needs to be improved. Some sentences appear not to be finished. There is a lack of coherence in the explanation.\n- The introduction section omits one of the feature selection classes: the embedded methods. No information regarding these methods is provided.\n- Multiple mathematical terms are not correctly defined:\n    - $p$ is defined after it was used.\n    - $S$ is never defined.\n    - The target norm $a$ in Eq. 5 is replaced by $\\sqrt{d}$ in the algorithm, but $d$ is never defined.\n- No information regarding the training procedure was provided.\n- The loss function never takes into account the correlation between $X$ and $Y$. Thus, I assume the trained network will provide a constant output that minimizes the loss function, which make it totally independent on the data provided to the algorithm.\n- The experimental section does not consider neither recent filter methods (like Inf-FS [1] or ILFS [2], por instance) nor other feature selection methods like CAE [3] or E2E-FS [4].\n- There is no experimental results over real datasets, despite the fact that the paper is only 6 pages (3 extra pages could be used for providing a better algorithm explanation as well as an extensive experimental section).\n\n\n[1] Roffo, G., Melzi, S., & Cristani, M. (2015). Infinite feature selection. In Proceedings of the IEEE international conference on computer vision (pp. 4202-4210).\n\n[2] Roffo, G., Melzi, S., Castellani, U., & Vinciarelli, A. (2017). Infinite latent feature selection: A probabilistic latent graph-based ranking approach. In Proceedings of the IEEE international conference on computer vision (pp. 1398-1406).\n\n[3] Bal\u0131n, M. F., Abid, A., & Zou, J. (2019, May). Concrete autoencoders: Differentiable feature selection and reconstruction. In International conference on machine learning (pp. 444-453). PMLR.\n\n[4] Cancela, B., Bol\u00f3n-Canedo, V., & Alonso-Betanzos, A. (2022). E2E-FS: An End-to-End Feature Selection Method for Neural Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence."
            },
            "questions": {
                "value": "Can you provide an explanation about the missing parameters and their meaning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission112/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698570602004,
        "cdate": 1698570602004,
        "tmdate": 1699635936257,
        "mdate": 1699635936257,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "53KAiJIcZj",
        "forum": "lt6xKGGWov",
        "replyto": "lt6xKGGWov",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission112/Reviewer_Mxn5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission112/Reviewer_Mxn5"
        ],
        "content": {
            "summary": {
                "value": "A new supervised feature selection method, utilizing neural mutual information estimation, is presented in this paper. This method considers feature subsets as a whole. It achieves precise feature selection, outperforming competing methods in two synthetic datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors introduce a method that selects features based on neural estimation of mutual information.\n\nThe method outperforms existing techniques in two synthetic datasets."
            },
            "weaknesses": {
                "value": "The experiments were limited to two synthetic datasets, casting doubt on the method's applicability to real-world datasets. Multiple factors can influence model performance. These include sample size, hyperparameters $c_1$ and $c_2$, the neural network architecture, and the noise that cannot be explained by the input $\\mathbf{X}$. Without a comprehensive analysis of these factors, the general efficacy of the proposed method remains uncertain.\n\nEquation (1) provides an estimate for mutual information. Yet, once the constraints are introduced in Equation (5), it's unclear if the function still acts as a mutual information estimator. This requires more in-depth theoretical exploration.\n\nThe definition of $\\phi$ in Line 11 of Algorithm 1 is unclear."
            },
            "questions": {
                "value": "1. Does the proposed method work in real-world datasets?\n\n2. Is Equation(5) an estimator of mutual information?\n\n3. How if $\\phi$ in Line 11 of Algorithm 1 defined?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission112/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637979397,
        "cdate": 1698637979397,
        "tmdate": 1699635936182,
        "mdate": 1699635936182,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fCq8AdE137",
        "forum": "lt6xKGGWov",
        "replyto": "lt6xKGGWov",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission112/Reviewer_R5qG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission112/Reviewer_R5qG"
        ],
        "content": {
            "summary": {
                "value": "Authors of this paper propose mutual information neural estimation regularized vetting algorithm (MINERVA) for supervised feature selection based on neural estimation of mutual information between features and targets. The feature weights are introduced into mutual information criterion with sparse regularizations. Experiments are performed on synthetic data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Mutual information is used as a learning criterion for supervised feature selection."
            },
            "weaknesses": {
                "value": "The proposed model is built on existing work by introducing feature weights in the neural estimation of mutual information, while the proposed sparsity regularization needs to be properly explained.\n\nExperiments are conducted only on two synthetic datasets, and experimental details are missing. Hence, the experimental results are difficult to be justified."
            },
            "questions": {
                "value": "The regularization on p in (5) contains two terms: l1_norm on l2-normalized p and the square loss between l2-norm of p and a constant a. It looks like the proposed l1_norm term is on the scaled-invariant p, which leads to unbounded p. So, the square loss term is introduced to prevent diverging. It is unclear what is the benefits of introducing the l1_norm on scaled p comparing to l1_norm on p. Moreover, it is unclear how the constant a should be set for different datasets.\n\nAuthors mentioned that the gradient descent stops when the estimated mutual information becomes smaller than non-selection model. In Algorithm 1, it mentioned the termination happens when convergence. It is unclear which gradient descent method is used. From Algorithm 1, it seems that some sort of stochastic gradient descent is applied. If so, it may not be reasonable to use the mentioned stop criterion since the selection step may not proceed if one stochastic gradient step cannot make any progress to minimize the initialized objective. \n\nThe conventional gradient descent method seldom set some entries of p to zero values unless some projection is done after descent step. To select feature using non-null entries of p or in step 14 of Algorithm 1 might not be practical.\n\nThe metric h(X_i, Y) is not defined in page 5.\n\nThe experimental settings can be described in detail. For example, some important statistics of the data are missing, such as the number of samples are sampled, the dimension of the synthetic data, how the synthetic data are generated, and the reported results on how many random runs of the experiments.\n\nOne key drawback is that only synthetic datasets are used, so it is unclear how the proposed model behaves on real data. For real data, it is often unknown which features are exact, so evaluation of feature selection is often dependent of the downstream prediction tasks. Although Table 3 demonstrate some results, it is hard to judge due to missing experimental details for example, why different methods are compared with different number of features and  what is the in-sample and out-sample ratio."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission112/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698643641147,
        "cdate": 1698643641147,
        "tmdate": 1699635936113,
        "mdate": 1699635936113,
        "license": "CC BY 4.0",
        "version": 2
    }
]