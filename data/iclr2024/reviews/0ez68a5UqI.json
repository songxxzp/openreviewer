[
    {
        "id": "EKLMZYrARW",
        "forum": "0ez68a5UqI",
        "replyto": "0ez68a5UqI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1112/Reviewer_Lp54"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1112/Reviewer_Lp54"
        ],
        "content": {
            "summary": {
                "value": "Authors propose a reinforcement learning algorithm for node selection problem in Branch-and-Bound (B&B) for Mixed Integer Programming. While prior work mostly focuses on ranking a pair of nodes, authors propose to use GNN to leverage information across the B&B tree. The policy induces a distribution across all open nodes in B&B tree. Authors propose Policy network and Value network architecture based on GNN. The proposed architecture is trained on TSP problems, and evaluated on both TSP and MIPLIB benchmark. The proposed method outperforms SCIP's default node selector across benchmarks considered."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Significance: Node Selection is instrumental for successful B&B. Prior research has been focused on Imitation Learning. This is limiting because when \"expert\" policy doesn't work well on problem at hand, ML-based method similarly struggles. Therefore, authors' Reinforcement Learning-based method has the potential of allowing ML-based node selection methods to be applied to a broader range of mixed integer programming problems with bigger, more practical improvements. Hence, I consider the potential significance to be high.\n\nOriginality: Given a related problem of Variable Selection has gone through a similar transition from Imitation Learning to Reinforcement Learning, the proposal of reinforcement learning method is not entirely unforeseen. However, authors make original contribution by proposing how to represent states of Markov Decision Process with GNNs."
            },
            "weaknesses": {
                "value": "Quality: Experimental setup of the paper could be improved more directly test the paper's key hypothesis: that 1) Reinforcement Learning provides an advantage over Imitation Learning, 2) considering the entire tree state is better than just considering isolated nodes. These are points which distinguish authors' work from prior work. Unfortunately, authors compare against only SCIP's default node selector, and previously proposed algorithms are not considered.\n\nAlso, authors use metrics and benchmark datasets not used in previous papers in this area of research. This makes difficult to interpret experimental results within the context of current research. In fact, many of the issues with metrics authors run into could be addressed with Primal/Dual/Gap Integral metrics https://www.ecole.ai/2021/ml4co-competition/ (see Metrics page), as these metrics would still be sensible when one algorithm can reduce the gap to be zero; since authors' metrics are not very well-defined when zero gap can be (nearly) reached, authors had to employ nontrivial preprocessing of data.\n\nThese two are major concerns. The contribution of the paper is mostly the proposal of an empirical method that improves upon prior work, and therefore it is important for experiments to be designed to measure the advantage of the proposed method upon prior art.\n\nClarity: The main ideas of the paper is clearly described and easy to follow. Some technical statements did not provide sufficient reasoning to justify, however. For example, in Section 4.3, it's argued: Assuming $P \\neq NP$, it is unreasonable for the proposed algorithm to tackle provably hard instances. In practice, MIP solvers are often applied to problems which don't allow even good approximation guarantee, and therefore I wasn't sure why $P \\neq NP$ would imply these problems to be not tractable."
            },
            "questions": {
                "value": "Is the reward (equation 5) only received at the end of the \"episode\" (end of the MIP solve)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1112/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698646900744,
        "cdate": 1698646900744,
        "tmdate": 1699636037404,
        "mdate": 1699636037404,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qz2UJu2Qif",
        "forum": "0ez68a5UqI",
        "replyto": "0ez68a5UqI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1112/Reviewer_rRw3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1112/Reviewer_rRw3"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the brand-and-bound problem, for which the authors propose a new method of simulation using RL for getting a more global view of the tree, augmented with heuristic node selection methods: tree encoding by GNN, features are learned by message passing and node selection is done by PPO. Experiments show positive results on many benchmarks despite the training being on TSP simulations. Another good thing is that code is provided (although I haven\u2019t tested myself)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The global view of trees is a strong motivation given the limitation of current methods in BnB. \n- I personally like the \u201cgreedy\u201d aspect in reasoning (in introduction) that theory vs. practice has a gap, especially for many cases like the BnB and in practice, oftentimes we should favor a shorter-term choice over long-term ones if it\u2019s good enough for many reasons. I think that is correct to the large spectrum of deep learning applications nowadays. \n- Positive results on many benchmarks. \n- Helpful supplemental contents."
            },
            "weaknesses": {
                "value": "- The strong motivation leads to a much larger cost in carrying out the algorithm, especially when it involves recursion.. However, it\u2019s not clear from the paper as to why the authors only choose the upper bound as a factor of choosing. Would be interesting if they have a study \u2013of maybe a comparison\u2013leading to that choice. \n- To solve this complex problem, the proposed method has to be broken down into many phases as shown in Section 2. That raises a question about the practicality: can the method be integrated as one to make it end-to-end. If not yet, what are the factors needed or what changes to enable that. \n- Another unclear aspect is the design of the reward method, e.g. why that formula in terms of motivation and explanation, and why not replace the term (\u201c-1\u201d) in Equation 5 with a constant C and study different values of it? \n- After the reward function, yet another unexplained technique of \u201cshifting\u201d, and another heuristics of clipping the reward. Is there any other better way of normalizing that or better design of the reward function to make sure that range complies while having a nice curve to the problem?\n- Why PPO? Would also be nice if comparing PPO to alternatives such as maybe TRPO, SAC, \u2026\n- Yet another heuristics is to remove problems >100% or 0 gap. That begs a question on the quality of design including the reward function. \n- Overall, the paper gives an impression that despite a good motivation and a complex problem, it\u2019s a collection of heuristic choices without substantiated evidence/studies supporting them. Such heuristics I think undermine the main motivation (i.e. ones might question how much contribution of RL in yielding the results you are getting?) It would be much more convincing if the authors address this aspect now or later. \n\n======\n\nAdditionally: some monor typos: \n- Maybe should not write TSP in the abstract as abbreviation (abbr), which is inconsistent, since the abbr term RL was defined before that. \n- Just in case, please use the newest ICLR 2024 template \n- Appendix E citation format is not consistent with other parts  \n- Section3, first line of 2nd paragraph, define the abbr \u201cIL\u201d first."
            },
            "questions": {
                "value": "- As also stated and shown, the problems are hard to handle computationally due to numerical instability. It is however not clear what problems they run into, and how the authors handle them. Those are very important for the community in terms of insights and reproducibility. \n- Table 3: The \u201cGap Ours\u201d and \u201cGap Base\u201d column have all normal values but the mean is NaN. Why? \n- See other questions in the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1112/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698795195966,
        "cdate": 1698795195966,
        "tmdate": 1699636037334,
        "mdate": 1699636037334,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YDtwWjqKCu",
        "forum": "0ez68a5UqI",
        "replyto": "0ez68a5UqI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1112/Reviewer_KqYv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1112/Reviewer_KqYv"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel method for node selection in branch-and-bound using reinforcement learning. The proposed method uses a graph neural network to model the node selection as a probability distribution considering the entire tree. Based on this, reinforcement learning is applied to perform node selection."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper clearly states the issue (node selection in branch-and-bound) trying to address, and the limitation of the conventional methods on that issue.\n2. The paper provides the simulation results in a variety of problem instances."
            },
            "weaknesses": {
                "value": "1. There are existing related works that use graph neural networks for node selection in the branch-and-bound algorithm. The proposed method in this paper uses graph neural networks for tree representation, but the difference from the existing works is not clearly stated.\n2. The structure of RL such as states, actions, and reward function is not rigorously defined in the paper. This makes it harder to understand how the RL method works in the proposed method.\n3. As the branch-and-bound algorithm proceeds, the number of nodes and the tree structure keep changing. Then, should the RL agent be trained from the scratch for every step of the branch-and-bound algorithm?\n4. The discussion on the learning cost of the RL algorithm is required. How is the cost due to collecting the enough experiences for the convergence of the RL policy?\n5. It seems that the RL agent should be trained for each problem instance. Is this training of RL agent for each problem instance is mandatory to use the proposed method? If additional RL agent training is always required, it may not be practical to use it.\n6. In the similar context, is there a possibility of using a pretrained RL agent that can be applied to a variety of problems? It would be helpful to demonstrate the generalization capabilities of the RL agent for more insight on the proposed method.\n7. Comparison with related works using graph neural networks, in particular, Labassi et al. (2022) which is one of the state-of-the-art methods, seems to be necessary in experiments. The authors stated that it was unable to conduct experiments due to the version compatibility issue, but a comparison between the similar state-of-the-art methods is essential."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1112/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810336509,
        "cdate": 1698810336509,
        "tmdate": 1699636037248,
        "mdate": 1699636037248,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qNJn2tKNFE",
        "forum": "0ez68a5UqI",
        "replyto": "0ez68a5UqI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1112/Reviewer_hSJj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1112/Reviewer_hSJj"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a reinforcement learning framework to learning the node selection policy in branch-and-bound algorithm. In particular, the paper considers an environment based on SCIP. The considers a graph neural network on the branch-and-bound tree with root-to-leaf path aggregated scores as the policy net and employs policy gradient algorithms for training. The paper carefully generates TSP problems with moderate difficulty for training and evaluate the learned node selection policy on TSPLIB, UFLP, MINLPLib, MIPLIB. The results show that the learned node selection policy outperforms the default policy in SCIP in terms of Reward and Utility/Node."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Most of the paper is well written and easy to understand for readers with basic knowledge in reinforcement learning and branch-and-bound.\n* The root-to-leaf path aggregated score is a clever design. It avoids the computation challenge from the growing of the branch-and-bound tree by an intuitive assumption: if a node is good, so should be its ancestors."
            },
            "weaknesses": {
                "value": "* The definition of the reward is not rigorously defined. Specifically, the paper does not disclose how are the gap(node selector) and gap(scip) are calibrated. It could be\n    * The gap when reaches the time budget.\n    * The gap at the same number of nodes n, with $\\text{traj}(\\text{node selector})[:n]$ rolled out with node selector, $\\text{traj}(\\text{scip})[:n]$ rolled out with scip, \n    * The gap at the same number of nodes n, with $\\text{traj}(\\text{node selector})[:n]$ and $\\text{traj}(\\text{scip})[:n-1]$ rolled out with node selector, $\\text{traj}(\\text{scip})[n-1:n]$ rolled out with scip.\n\n* The score in evaluation need more justification. From my perspective, the most important goal of learning a node selection policy is finding good primal solutions. With this aim, none of the scores are a good choice. The second priority for a node selection policy is to close the duality gap. In this sense, \"Utility/Nodes\" is not good choices. \n\n* The paper consider a small threshold 45 seconds. This is a relatively small time budget for solvers to solve MIP problems. To demonstrate the learned policy is practical, the results with a longer running time should be reported."
            },
            "questions": {
                "value": "* Only the results on benchmarks are provided in the paper. I am curious about the performance on the training data."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1112/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698894898347,
        "cdate": 1698894898347,
        "tmdate": 1699636037175,
        "mdate": 1699636037175,
        "license": "CC BY 4.0",
        "version": 2
    }
]