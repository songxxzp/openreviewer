[
    {
        "id": "nHNKcGOZ3r",
        "forum": "fmAzKz9DJs",
        "replyto": "fmAzKz9DJs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2862/Reviewer_sBsU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2862/Reviewer_sBsU"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method for learning representations where image moments (centroid and orientation) are explicitly disentangled from the rest of the representation. A loss term comparing the image moments is introduced and its contribution is gradually decreased during learning.\n\nExperimental results on six datasets are provided to show that the proposed method compares favorably to six recently proposed methods that also seek to disentangle translation and rotation representations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**S1.** The proposed method compares favorably to recently proposed methods. In particular, translation and rotation are effectively disentangled while the model is more computationally efficient than most other baselines."
            },
            "weaknesses": {
                "value": "**W1.** In general, the presentation could be significantly improved. Some examples:\n- It seems to me that the key contribution of the method is the introduction of loss L_m but the discussion does not make this clear.\n- Along the same lines, it is suggested (end of section 2.3) that the \u201cprimary focus\u201d of the paper might be to achieve disentanglement (and indeed, some experiments also suggest that). However, it is not clear how this is pursued besides obtaining moments and orientation.\n- The theorems in section 3.1 are barely referenced in subsequent sections.\n- The experimental results need further details and discussion (more on this below).\n- Some sentences are hard to understand, e.g.: in the abstract \u201ctraining and inference performance\u201d (what is the metric?), third-to-last sentence in paragraph preceding eq. (13) (on \u201csubtle inaccuracy\u201d).\n\n**W2.** The motivation seems disconnected from the experimental validation. It is stated that learning of centroids and orientations \u201cunderpins\u201d a number of downstream tasks. It is not clear what this means nor is it clear what the level of success of the proposed approach would be in this regard.\n\nThe downstream task experiments are perhaps most interesting but barely any details of the experimental setup are provided. A lot of space is taken by visual results but I would suggest the downstream task results are much more important.\n\n**W3.** It is unclear what tables 3 and 4 convey when comparing different models as the optimal latent dimension is model dependent. For instance, for models TARGET-VAE and IRL-INR the original authors showed results for d >=32 (but it is suggested d=2 in the experiments in the present submission)."
            },
            "questions": {
                "value": "**Q1.** Is the method pursuing disentanglement of all features or mainly/only obtaining moments and orientation? In case of the former, I would say this is not clear in the presentation, could you outline how this is pursued?\n\n**Q2.** Why are baselines not compared with representation dimension as in the original papers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2862/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698538426503,
        "cdate": 1698538426503,
        "tmdate": 1699636229670,
        "mdate": 1699636229670,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LUfuIyf2l2",
        "forum": "fmAzKz9DJs",
        "replyto": "fmAzKz9DJs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2862/Reviewer_8vxB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2862/Reviewer_8vxB"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to disentangle the data into invariant and equivariant components. It builds up on DAE and introduces image moment based losses. The results are promising and the evaluation is extensive."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The method is simple to understand and well-written.\n- Evaluations are extensive for the disentanglement property."
            },
            "weaknesses": {
                "value": "- Due to the deterministic and simple nature of the moment computation, it could be easy for the neural network to learn z_eq. Therefore, a result on ablating L_{moment} could be interesting to see the emergent properties just based on the reconstruction loss, and also could be a baseline, since moment loss is the only new component here. As a corollary, the moment loss could also be applied over other baselines to evaluate how much does it contribute in improving their performance.  \n- Novelty of the moment loss is very limited, as it is a widely known concept in the community.  \n- Results on 3D datasets, such as 3D airplanes, 3D teapots, 3D face could test the method more robustly as the shape also changes.  \n- GF-Score could be reported as proposed in the DAE paper."
            },
            "questions": {
                "value": "- How is the moment computed for other factors such as shape and color? Is anything more than centroid and orientation that is part of z_{eq} on these datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2862/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2862/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2862/Reviewer_8vxB"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2862/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698845473526,
        "cdate": 1698845473526,
        "tmdate": 1699636229593,
        "mdate": 1699636229593,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vAHX5fdu79",
        "forum": "fmAzKz9DJs",
        "replyto": "fmAzKz9DJs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2862/Reviewer_ULgT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2862/Reviewer_ULgT"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to learn image representations with translational and rotational invariance and equivariance properties, under the guidance of the centroid and orientation information of images.  The model is trained with simple image reconstruction loss in the space of pixel intensity and image moments.  Experiments on several datasets (such as 5HD and MINIST) demonstrate that the proposed method outperforms existing methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It is technically reasonable to guide the learning of equivariant features with some spatial image statistics (such as image centroid). \n2. The paper is well-organized and easy to follow.  \n3. The performance across multiple benchmarks consistently shows the improvement of the proposed method over existing works."
            },
            "weaknesses": {
                "value": "The main technical contribution of this work is to guide the learning of equivariant features with some spatial image statistics (such as image centroid). However, all the experiments are conducted on toy datasets such as MNIST digits which contain very simple 2D objects and almost uniform background region. This is also manifested in the evaluation scores. For example, in Table 1, all methods achieve over 97% accuracy. This leaves a question mark on how useful the proposed method is in practice where natural images are way more complicated and whether the simple spatial statistics are still sufficient."
            },
            "questions": {
                "value": "How is the performance of the proposed method on natural image datasets, such as CIFAR or ImageNet where objects and background are more complicated and rotations are almost 3D (instead of just 2D in-plane rotation)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2862/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2862/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2862/Reviewer_ULgT"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2862/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699045888000,
        "cdate": 1699045888000,
        "tmdate": 1699636229531,
        "mdate": 1699636229531,
        "license": "CC BY 4.0",
        "version": 2
    }
]