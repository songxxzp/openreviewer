[
    {
        "id": "n9nOREOO8R",
        "forum": "rFCGiFTVyY",
        "replyto": "rFCGiFTVyY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4512/Reviewer_d1FD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4512/Reviewer_d1FD"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method called FedSKU (Federated Selective Knowledge Unlearning) to defend against backdoor attacks in federated learning. Compared to existing coarse-grained defenses that either completely remove suspected malicious models or add noise, FedSKU takes a more fine-grained approach by decomposing the model into the malicious trigger and useful knowledge. It recovers the trigger pattern using a novel pre-aggregation scheme for efficiency. Then it uses a dual distillation process to unlearn the trigger while preserving only clean knowledge in a surrogate model. This allows aggregating the useful knowledge from malicious models.\n\nExperiments on image datasets like CIFAR-10/100 and Tiny ImageNet validate FedSKU. It improves accuracy by up to 6.1% over defenses like FLAME and Krum, with negligible increase in attack success rate (<0.01%). FedSKU also outperforms extensions of other unlearning methods like BAERASER and NAD to federated learning. Overall, FedSKU effectively utilizes knowledge from malicious models to improve accuracy while defending backdoor attacks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Proposes a novel selective unlearning framework FedSKU that decomposes models into triggers and useful knowledge for fine-grained backdoor defense in federated learning.\n2. Designs efficient techniques like pre-aggregation scheme for trigger recovery and dual distillation loss to selectively unlearn triggers while retaining useful knowledge.\n3. Achieves significant accuracy gains over prior defenses like FLAME and Krum on CIFAR and Tiny ImageNet datasets, with marginal increase in attack success rate.\n4. Outperforms extensions of other unlearning methods like BAERASER and NAD to federated learning scenario.\n5. Comprehensive experiments analyzing impact of non-IID data, ratio of malicious clients etc."
            },
            "weaknesses": {
                "value": "1. Accuracy improvements are higher on CIFAR than Tiny ImageNet - I think more analysis are needed for why FedSKU works better on certain datasets and if this could be a sign of generalization difficulties.\n2. No major limitations of the approach have been discussed."
            },
            "questions": {
                "value": "1. The pre-aggregation scheme for efficient trigger recovery makes sense intuitively, but more details or intuition could be provided on why aggregating backdoored models retains the malicious triggers reliably.\n2. For unlearning using dual distillation, how sensitive is the performance to the hyperparameters like the distillation temperature? Was there any tuning done to set the hyperparameters?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4512/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698530506754,
        "cdate": 1698530506754,
        "tmdate": 1699636428048,
        "mdate": 1699636428048,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gwTjx6SKHW",
        "forum": "rFCGiFTVyY",
        "replyto": "rFCGiFTVyY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4512/Reviewer_mcVc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4512/Reviewer_mcVc"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a backdoor defense technique in federated learning (FL) by\nfirst identifying the possible trigger via trigger inversion, and then unlearn\nthe trigger from the model. The unlearning is done by distillation, assuming a\nset of public dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "From writing aspect, the paper is easy to follow and understand. Its method description is clear.\n\nExperiment show that the method is very effective, compared with existing\nmethods."
            },
            "weaknesses": {
                "value": "The used technique in this paper, trigger inversion and distillation, do to seem\nto be significantly different from existing work. For example, its inversion\nmethod is leveraging MESA (Qiao et al., 2019). I am not sure about the\nnovelty and significance of the technique.\n\nThere is no clear threat model in the paper. For example, both the trigger\nrecovery and unlearning require certain public data. But what types of public\ndata? There are various backdoors that work on a subset of inputs or outputs\nlabels. Does this method work on all these attacks? Based on my understanding, I\ndo not think it can cover all backdoor attacks. However, without a clear threat\nmodel clarifying the assumptions, I have no information to leverage -- so does\nthe paper itself.\n\nWhat does the method guarantee? Namely, will the proposed unlearning method be\n\"exact\" or \"approximate\"? \n\nAnother line of work, e.g., FedRecover, that tries to recover from poisoning\nattacks without the need to recover the trigger (and is also not limited to\nbackdoors), and also guarantees the recovered model is similar to the one\ntrained on non-poisoning data with a practical difference bound. The paper\nshould also include a discussion and comparison on that."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4512/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772079423,
        "cdate": 1698772079423,
        "tmdate": 1699636427969,
        "mdate": 1699636427969,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2oVBmQNoWc",
        "forum": "rFCGiFTVyY",
        "replyto": "rFCGiFTVyY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4512/Reviewer_FcGK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4512/Reviewer_FcGK"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the vulnerability to backdoor attacks that manipulate model parameters to deceive the aggregation process. Unlike existing defenses that employ coarse-grained methods, this research takes a more nuanced approach. The authors propose a novel technique called FedSKU, which involves decomposing the uploaded model into two distinct components: malicious triggers and useful knowledge."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The concept of selective unlearning represents a novel and compelling advancement in comparison to the coarser-grained defenses commonly used.\n- Through extensive evaluation across various datasets, the method demonstrates superior performance in accuracy compared to state-of-the-art defenses, all while keeping the increase in attack success rate at a negligible level.\n- The inclusion of convergence analysis and ablation studies offers valuable insights into the inner workings of the method."
            },
            "weaknesses": {
                "value": "- The method proposed hinges on anomaly detection for identifying malicious clients. It's important to note that this approach has its limitations; attackers may find ways to evade detection. The authors should delve deeper into this aspect for a more comprehensive discussion.\n- The paper lacks an in-depth analysis of the computational overhead associated with trigger recovery and unlearning.\n- The experiments conducted on non-iid settings are not as extensive as one might expect."
            },
            "questions": {
                "value": "- The effectiveness of the unlearning process can be influenced by various factors, including model complexity, available data volume, and the complexity of the information to be forgotten. How can we ensure that unlearning remains effective after anomaly detection?\n- How does the computational overhead of the trigger recovery and unlearning process change as the number of clients increases?\n- Instead of performing unlearning, wouldn't it be more efficient to simply exclude the detected malicious clients from the training process?\n- Given the potentially large number of participating clients in Federated Learning, how can we guarantee that the proposed method remains effective despite the high computation overhead?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4512/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4512/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4512/Reviewer_FcGK"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4512/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698803271714,
        "cdate": 1698803271714,
        "tmdate": 1699636427856,
        "mdate": 1699636427856,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mGo0pOefLD",
        "forum": "rFCGiFTVyY",
        "replyto": "rFCGiFTVyY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4512/Reviewer_wRHA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4512/Reviewer_wRHA"
        ],
        "content": {
            "summary": {
                "value": "The authors propose FedSKU, a defense mechanism that detects and selectively unlearns harmful backdoors in uploaded models. They introduce a pre-aggregated trigger recovery scheme to efficiently train a trigger pattern generator, reducing training overhead in the FL system. They also designed a dual distillation method for selective knowledge unlearning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper emphasizes that pre-aggregation in the model inevitably preserves certain malicious features. They designed the method based on this insight that can effectively reduce the Federated Learning (FL) training overhead from the trigger generator.\n2. The suggested method extracts valuable knowledge from backdoored clients, providing a novel defense method against backdoors while preserving competitive global accuracy.\n3. The experiments are comprehensive, encompassing varying numbers of malicious clients, initialization parameters, and convergence comparisons among others."
            },
            "weaknesses": {
                "value": "1. The \"pre-aggregated\" process lacks clarity. Also, it would be better to discuss the federated aggregation method on the main page rather than in the appendix.\n\n2. While it's acceptable that FEDSKU has a marginally lower GACC compared to DNN unlearning methods like BAERASER and NAD, given their excessively high ASR indicates unsuccessful defense, it would strengthen the claim about \u2018'take the essence and discard the dross' if the author could elucidate why DNN unlearning methods have superior GACC."
            },
            "questions": {
                "value": "1. In Figure 2, it appears that two backdoored models exist, but only one is detected. This setup could be confusing due to the absence of the \"pre-aggregates\" step in the Trigger recovery process. It might be clearer if the framework depicted the detection of multiple backdoored models, thereby illustrating the details of \"pre-aggregates\" and showing that each backdoored model has a corresponding surrogate."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4512/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699000153795,
        "cdate": 1699000153795,
        "tmdate": 1699636427782,
        "mdate": 1699636427782,
        "license": "CC BY 4.0",
        "version": 2
    }
]