[
    {
        "id": "CYrjn3vCMa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8600/Reviewer_WXv6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8600/Reviewer_WXv6"
        ],
        "forum": "M6XWoEdmwf",
        "replyto": "M6XWoEdmwf",
        "content": {
            "summary": {
                "value": "This paper proposes an RL agent for POMDPs. The proposed agent uses a transformer architecture to handle long context lengths. The agent is trained using a new off-policy RL update, which combines elements of previously proposed actor-critic updates. The update is designed to require relatively little hyperparameter tuning and reduce the amount of computation in the sequence model compared to relevant baselines. The proposed agent is tested on meta-RL, goal-conditioned RL, and other POMDP benchmarks. It achieves strong performance on the recently proposed Crafter environment."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "## Contribution\n- Training transformers for POMDPs is a popular subject in research. Good results in this area have the potential for significant impact on RL research and hopefully beyond, as POMDPs are a very general problem class. Based on the strong results in the Crafter environment, this paper seems to push the state-of-the-art on that front, at least in the realm of open source research, for which it is to be commended.\n- The paper includes a suite of experiments in a variety of environments, which probe the different challenges arising from in partially observable settings. Benchmarking the algorithm on a diverse set of environments is good and helps establish the robustness of the proposed method. \n\n## Presentation\n- The introduction, background, and related work sections are well written."
            },
            "weaknesses": {
                "value": "## Presentation\n- This paper is perhaps trying to do too much in the limited space to the detriment of the clarity of its most important contributions. Given that it is in the method name, goal-conditioned RL seems to be an important part of the contribution. However, this paper would be much easier to understand if it focused on the meta-RL part, where it already has strong results, and upon which its success in goal-conditioned RL builds.\n- The proposed agent uses a new combination of existing actor critic losses to construct its update function, yet the main paper does not contain almost any detail about how the objective is motivated or what the objective even is. Contrast this, for example, to the DreamerV3 paper (Mastering Diverse Domains through World Models, Hafner et al. 2023), which tackles a similar problem setting, where the majority of the paper is describing the method and motivating the design choices. Even after reading the relevant parts of the appendix, it is not clear to me how exactly is the critic trained.\n- The architecture design is described as being based on \"clear pattern of network activations across hundreds of trials\" but this pattern is not described anywhere.\n\n## Soundness\n- The proposed method is quite complicated including both architecture and objective function innovations. Some of the ideas are not super well motivated or ablated.\n    - Popart is proposed to help keep the different losses on the same scale across different environments, but I didn't find any attempt to validate that it is indeed doing so or helping in any other way.\n    - A complicated exploration schedule is proposed and used, but at the same time the paper says it is not essential."
            },
            "questions": {
                "value": "- The paper explicitly claims state-of-the-art results on PoPGym but refrains from doing so for Crafter, where it also seems to be beating the previous state-of-the-art (Dreamerv3). Why?\n- Have I understood it correctly that the \"dense\" Crafter is the vanilla Crafter setting?\n- Why not include results for Dreamerv3 for Crafter?\n- What does \"relegate to an unstable baseline\" in the introduction mean?\n- What is the actor objective used by the method?\n- What is the critic objective used by the method?\n- What is \"the clear pattern of network activations across hundreds of trials\"?\n- This is very subjective so I'm not listing this as a weakness, but I find the \"in-context learning\" label to make it less clear what the paper is about. On an abstract level, the method is a variant of well-known black-box meta-RL methods like RL2. Black-box meta-RL methods have variously been grouped under other names like context-based meta-RL, memory-based meta-RL, zero-shot meta-RL, etc. Given that this is a long ongoing research direction in the RL community, with an already confusing diversity of labels, it's not clear to me that emphasizing a new label makes this easier to understand. Granted that for non-RL audiences \"in-context learning\" may be more intuitive than \"black-box meta-RL\". ADA uses the term similarly, but when they get to the technical details, they switch to talking about black-box meta-RL."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8600/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8600/Reviewer_WXv6",
                    "ICLR.cc/2024/Conference/Submission8600/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697296598587,
        "cdate": 1697296598587,
        "tmdate": 1700588812983,
        "mdate": 1700588812983,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2AJDsHCqH9",
        "forum": "M6XWoEdmwf",
        "replyto": "M6XWoEdmwf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8600/Reviewer_LVWP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8600/Reviewer_LVWP"
        ],
        "content": {
            "summary": {
                "value": "The authors apply a transformer to in-context reinforcement learning, effectively producing a smaller-scale and open-source variant of Adaptive Agents.\n\nObservations, actions, rewards, episodic resets, goals, and the timestep are encoded into a fixed-size token for a transformer. The authors feed a long sequence consisting of multiple episodes to the transformer to produce a latent representation. Actor and critic heads process this representation into actions and values respectively.\n\nThe authors propose a series of tricks to help stabilize transformers in RL:\n- Multiple discount factors\n- Shared transformer for actor/critic\n- Hindsight relabeling\n\nI believe this is a generally useful paper, even if the novelty and theory is a bit lacking."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well written and easy to follow\n- The authors set a new record on a standard benchmark, and in general provide a large number of experiments across many environments\n- There are a large number of ablation studies\n- This is effectively a more useful version of the AdA paper that does not rely on closed-source benchmarks and does not require policy distilation or corporation-scale resources, making it much more useful to the academic community"
            },
            "weaknesses": {
                "value": "- The high-level concept is not novel -- applying transformers to POMDPs or for in-context RL has been heavily studied\n- This method still requires server-grade GPUs to run, limiting this approach to well-off labs or industry\n- For the POPGym benchmark, they are comparing their off-policy method to an on-policy method given the same number of sampled timesteps. This is not a fair comparison, but they do label the baselines as on-policy.\n- They do not list any shortcomings"
            },
            "questions": {
                "value": "- I would like the authors to list any shortcomings of their approach, especially because this paper is mostly empirical"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698410809968,
        "cdate": 1698410809968,
        "tmdate": 1699637075334,
        "mdate": 1699637075334,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RPxOpZzMXb",
        "forum": "M6XWoEdmwf",
        "replyto": "M6XWoEdmwf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8600/Reviewer_t9aE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8600/Reviewer_t9aE"
        ],
        "content": {
            "summary": {
                "value": "The authors present AMAGO, a method that successfully trains long-range Transformers over entire rollouts using off-policy RL. The authors also combine AMAGO with a hindsight relabeling scheme. The authors demonstrate strong performance on a variety of meta-RL and long-horizon tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- AMAGO improves over baselines on long-horizon tasks while still performing well on simpler tasks.\n- The authors perform a series of ablations in the Appendix to isolate the effect of each design choice.\n- The authors test AMAGO on a very thorough set of experiments and show good results."
            },
            "weaknesses": {
                "value": "- It is a bit hard to keep track of what exactly the different components of the proposed method are. I think it would greatly improve the paper's clarity if the authors could include a table outlining exactly what the contribution is (e.g. Transformer architecture change, multi-gamma update, relabeling, etc.). Generally, I think the main improvement the authors can make on this paper is clarity.\n- Similarly, an algorithm box for the relabeling scheme would be nice for clarity. I know the authors say they defer details of the relabeling to the open source code release, but I think it would be good to have it in the appendix at least.\n- The proposed method seems quite complex. Generally, simpler methods should be preferred. However, this is mitigated by the good performance and the ablation studies."
            },
            "questions": {
                "value": "- The authors write that \"AMAGO can select the action corresponding to any $\\gamma$ at test time\". How exactly is this done?\n- Have the authors tried experimenting with a smaller set of multi-gamma values for the multi-gamma update rather than using all the values in Table 4? I'm curious if/how much performance would suffer."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8600/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8600/Reviewer_t9aE",
                    "ICLR.cc/2024/Conference/Submission8600/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698627114001,
        "cdate": 1698627114001,
        "tmdate": 1700712107965,
        "mdate": 1700712107965,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FaTdPXAvV7",
        "forum": "M6XWoEdmwf",
        "replyto": "M6XWoEdmwf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8600/Reviewer_KbVB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8600/Reviewer_KbVB"
        ],
        "content": {
            "summary": {
                "value": "This paper presents AMAGO, which is designed to tackle challenges related to generalization, long-term memory, and meta-learning. It achieves this with the use of sequence models. The paper shows that AMAGO can successfully train long-sequence Transformers, able to perform tasks over entire rollouts, while operating with end-to-end RL.\n\nThe primary contributions are twofold. Firstly, it revises the off-policy in-context approach and deploys sequence models like Transformers to replace the recurrent policies. This change facilitates the tackling of constraints related to memory capacity, model size, and the planning horizon in agents. Secondly, AMAGO extends in-context learning to goal-conditioned problems, making it capable of handling more sophisticated exploration tasks. The authors validate AMAGO's performance through empirical evaluations, demonstrating capabilities in large-scale challenges requiring long-term memory and adaptation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- By redesigning the off-policy in-context approach and using sequence models, AMAGO overcomes previous bottlenecks in memory capacity, planning horizon, and model size.\n- The introduction of a hindsight relabeling scheme broadens the applicability and scalability of this approach to open-world environments.\n- The evaluations illustrate capabilities of AMAGO in diverse large-scale, heavy-memory and meta-learning RL challenges.\n- The structure and presentation of the paper are clear and well-organized."
            },
            "weaknesses": {
                "value": "- The AMAGO architecture's complexity, particularly due to the use of transformers, can be computationally intense. This may limit its efficiency in resource-limited situations or possibly make it less suitable for complicated applications.\n- Although AMAGO has made improvements in performance, it still has a success rate of 0 in collecting some key resources (e.g. iron), so AMAGO still faces limitations in exploration challenges."
            },
            "questions": {
                "value": "- Considering AMAGO's ability to work with multi-goals, how well does AMAGO prioritize or sequence goals when given multiple objectives? Is there a mechanism to manage potential conflicts between goals?\n- Are there any experimental results that can support the claim that AMAGO does not require much tuning?\n- The author mentioned replacing ReLU/GeLU with Leaky ReLU, adding LayerNorms, and modifying linear layers. It may be better to explain the necessity and rationality behind each modification through a more thorough ablation study."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698683759932,
        "cdate": 1698683759932,
        "tmdate": 1699637075113,
        "mdate": 1699637075113,
        "license": "CC BY 4.0",
        "version": 2
    }
]