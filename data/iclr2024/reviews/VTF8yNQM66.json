[
    {
        "id": "IetQFNhEPS",
        "forum": "VTF8yNQM66",
        "replyto": "VTF8yNQM66",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6476/Reviewer_BfZn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6476/Reviewer_BfZn"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new benchmark, SWE-bench, which collects code and issues from 12 Python repositories. This benchmark also considers the convenience of subsequent evaluation, and the test code for relevant issues is included. Moreover, this paper also finetunes Code Llama with SWE-bench training data. Experimental results show that there are still many challenges for existing LLM to solve real-world issues."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe paper is generally well-written.\n2.\tThis paper introduced a new dataset SWE-bench that contains 2294 GitHub issues and related test scripts. The dataset can be used to evaluate the methods for resolving real-world GitHub issues."
            },
            "weaknesses": {
                "value": "1.\tSome of the comparison is not very fair. As Claude 2 is trained on data up to early 2023, GPT's knowledge cutoff is September 2021 and there is no specific time for Code Llama\u2019s training data, evaluating these models on the dataset that contains instances before 2023 is not fair enough.\n2.\tThe contribution of SWE-Llama is not significant, especially for an AI conference. The paper could better target a software engineering/programming conference. \n3.\tThis method is mainly based on Code Llama while there is no comparison between Code Llama and SWE-Llama.\n4.\tSome of the experimental analysis is not solid enough. For example, in the \u201cDifficulty correlates with output length\u201d (Section 5), Table 8 only presents all successfully applied patches, and does not show the correlation between difficulty and output length. The length of other patches needs to be taken into account.\n5.\tThere are a lot of work on automated bug fixing, including LLM-based ones and traditional ones. The authors could discuss and compare. For example:\nJiang et al., Shaping Program Repair Space with Existing Patches and Similar Code, Proc. ISSTA 2018.\nD. Sobania, et al., An analysis of the automatic bug fixing performance of Chatgpt,arXiv:2301.08653, 2023."
            },
            "questions": {
                "value": "1.\tAs the experimental results of GPT-4 are on a 20% random subset of SWE-bench while there is no comparison of other models on the same subset. If we only look at this part of the subset, are all the conclusions in the paper still valid/consistent?\n2.\tWhy are these 12 Python repositories chosen as the source of the benchmark? Does the selection of the programming language and repository influence the results of the comparison?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6476/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698563860044,
        "cdate": 1698563860044,
        "tmdate": 1699636725190,
        "mdate": 1699636725190,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ruNGsmX3Tx",
        "forum": "VTF8yNQM66",
        "replyto": "VTF8yNQM66",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6476/Reviewer_onWq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6476/Reviewer_onWq"
        ],
        "content": {
            "summary": {
                "value": "Authors aim to determine if LLMs can resolve real world software issues (vs constructing or fixing toy programs). Authors propose SWE-bench, a benchmark based on GitHub issues. They apply LLMs to try and fix these real-world issues and discover very poor performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Authors present a good real-world problem benchmark based on real product sized GitHub repositories and real issues fixed in them.\n- Fine tune CodeLlama 7B and 13B models to get at least somewhat positive performance on repository-wide code edits\n- Propose retrieval methods to compose input for LLMs to fit into LLM context size.\n- Evaluate LLMs on the benchmark and present general lessons from the results."
            },
            "weaknesses": {
                "value": "- Although benchmark and LLM evaluation on it are valuable, the paper does not present any novel solutions to the task in the benchmark. This limits the contribution.\n- Please reorganize the paper so tables and figures are collocated with the text. Currently, it is hard to read when tables referenced out of order and explained very far from their location in the paper."
            },
            "questions": {
                "value": "This sentence, especially its last part, is unclear: \"We compare the BM25 retrieval results against the oracle retrieval setting in Table 3, where we see that BM25 retrieves a superset of the oracle files in about 40% of instances with the 27,000 token context limit but only also excludes all of the oracle files in over half of instances.\". I think this is trying to explain the results in Table 3 and trying to say that in around half cases BM25 does not retrieve any of oracle files. Is this what you are trying to say? Please explain or rephrase."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6476/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698728477839,
        "cdate": 1698728477839,
        "tmdate": 1699636725056,
        "mdate": 1699636725056,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "65Lqz2wzmr",
        "forum": "VTF8yNQM66",
        "replyto": "VTF8yNQM66",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6476/Reviewer_YqAB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6476/Reviewer_YqAB"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a new benchmark and dataset for testing the abilities of LLMs to edit large code bases.  Previously existing test suites typically involve asking the LLM to generate a small self-contained function when given a natural language description.  In contrast, the new dataset requires the LLM to create a patch, which potentially affects many files across an entire repository, when given a bug report.\n\nBug reports and repositories were scraped from Github.  Ground truth is a human-written pull request, along with additional unit tests.  Success is determined by whether the patched repository passes additional unit tests that were supplied with the pull request.\n\nThe authors conduct numerous experiments with various LLMs, and discover that existing LLMs are (unsurprisingly) very bad at this task.  They analyze and discuss a number of issues as the cause of this failure, such as limited context length, difficulty in retrieving the relevant files from large datasets, poor test coverage, and the requirement that the model output a correctly-formatted patch, rather than ordinary code."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The primary contribution of this paper is the creation of a new dataset and methodology for evaluating the performance of LLMs on real-world software engineering tasks.  The benchmark is well-designed, and can be continually updated and expanded moving forward.  The experiments with existing models are interesting, but they mainly serve to illustrate that this is a difficult and unsolved problem.  \n\nI fully expect this to be a high-impact paper, because other practitioners working in this area can now measure the performance of their models against the new benchmark.  In addition, the analysis and discussion provided by the authors provides a good starting point for guiding future research in this area. \n\nThe qualitative analysis, which compares LLM-generated patches against human-generated patches was also quite insightful."
            },
            "weaknesses": {
                "value": "Generating a patch file, and generating code, are two very different tasks.  Existing models are pretrained on code, not patch files, so at least some of the poor performance could simply be due to the fact that the models are operating out of distribution on this data set.  (The authors mention this issue in the paper.)"
            },
            "questions": {
                "value": "There is an additional issue with the way pretraining for code LLMs is typically done.  Due to context length limitations, the LLM often does not even see a complete file, much less a complete repository.   Moreover, the code fragments that are used for pretraining do not indicate what file they come from.  \n\nIn contrast, in order to generate a good patch file, the model must be able to see the file and directory structure of the repository.  How do you handle file names and directory structure in your experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6476/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6476/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6476/Reviewer_YqAB"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6476/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786410158,
        "cdate": 1698786410158,
        "tmdate": 1699636724932,
        "mdate": 1699636724932,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8HDIwrbvOj",
        "forum": "VTF8yNQM66",
        "replyto": "VTF8yNQM66",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6476/Reviewer_r43j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6476/Reviewer_r43j"
        ],
        "content": {
            "summary": {
                "value": "The paper primarily describes a benchmark (Swe-Bench) for evaluating language models. The benchmark consists of issues reported in github python repositories. The authors give a detailed description of the criteria they used for constructing the benchmark. They also describe the inputs to the benchmark for evaluation. They finetune the CodeLlama model for the benchmark, and then evaluate this model and others using the benchmark."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper addresses a practically relevant issue, that of a benchmark for evaluating language models. The paper is clearly written, and quite a lot of work seems to have been done to support the material in the paper."
            },
            "weaknesses": {
                "value": "It seems that none of the models is doing well when the benchmark is used. It would be nice if the benchmark can be used to more clearly indicate where the problem in the language model lies. The results of the model evaluation e.g. difficulty correlates with context length or difficulty correlates with output length are expected and thus do not seem very interesting"
            },
            "questions": {
                "value": "1) It would be nice if the exact contributions of the paper are stated more clearly.\n\n2) In section1, the authors point out that there is a need for a challenging benchmark that can be used to check the abilities of language models. Although the results have been reported, I am not sure how far they evaluate the specific abilities or weaknesses. The results are general, and seem to apply to all the models without discerning the strengths/abilities or weaknesses of a particular model\n\n3) At this stage, since all the models are performing poorly, perhaps there is a need for a benchmark that is neither too simple, but not as general as SWE-bench? Wouldn't this allow some aspects of the models to be better tested and reported?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6476/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698996308519,
        "cdate": 1698996308519,
        "tmdate": 1699636724814,
        "mdate": 1699636724814,
        "license": "CC BY 4.0",
        "version": 2
    }
]