[
    {
        "id": "a1pvZ88bTd",
        "forum": "tzD9HVgeVx",
        "replyto": "tzD9HVgeVx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7899/Reviewer_tDVs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7899/Reviewer_tDVs"
        ],
        "content": {
            "summary": {
                "value": "The goal of this paper is to learn agent policies that satisfy a correlated equilibrium, which can be executed in a fully decentralized manner, under partial observability for each agent. The authors propose the AgentMixer method. The main two aspect of the method are (1) the Policy Modifier, which takes the fully decentralized policies and state information, producing a correlated joint policy from the decentralized policies, and (2) a method to extract decentralized policies from the centralized policy, while ensuring the Individual Global Consistency (IGC) condition. The AgentMixer method follows the CTDE framework, assuming that a centralized controller can both observe the full state during training, and send joint actions to the environment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I enjoyed reading this paper, which focuses on how we might represent an optimal joint policy in the form of a product policy (where the latter can be executed using decentralized execution. The motivation and proposed method, based on posterior inference (?) is also interesting. The paper provided a good view of the literature + state-of-the-art for value factorization work in MARL, and the selected experimental settings were challenging."
            },
            "weaknesses": {
                "value": "While the idea is interesting, the paper is weak in many aspects. The points below are listed in order of most to least important for the authors to fix.\n\n- Incompletely specified method: \n\t1) Preserving the IGC is clearly an important part of the method. While the paper clearly defines IGC (Def 6) and discusses why it is important to preserve that condition, it doesn\u2019t explain how the proposed method preserves IGC. The only provided explanation is Equation 13, which presents a centralized reward-maximization objective for MARL, subject to  the constraint that IGC is preserved. This is clearly not a sufficient explanation, and the objective isn't a novel insight. Arguably many value-factorization methods maximize this objective (e.g. VDN, QMIX, QTRAN, etc.) -- the crux is how IGC is maintained. \n\n- Incomplete results + limited ablation studies: \n\t1) Figure 3, the curve for AengMixer is cut off. It is important to show the full curve, to let the reader verify the asymptotic behavior of the method. \n\t2) Overall, it seems there should be other evaluations. For example, it seems natural to have an ablation to study the degree of partial observability that the IGC can handle. Also, the authors should attempt using another method to generate the fully observable joint policy. What about directly running a single-agent RL algorithm on the joint action space (rather than the proposed AgentMixer architecture)?\n- Weak improvement overall on both MAMujoco, and StarCraft\n- Some missing references to prior work. For example, Greenwald et al.'s classic paper on [Correlated Q-learning](https://dl.acm.org/doi/10.5555/3041838.3041869)  should be added and discussed. The authors should also add a discussion of related work on handling partial observability in multi-agent systems. \n\n- Notation  and clarity should be improved: \n\t1) It's confusing that there's no difference in notation or wording between Definition 1 and 2. Further, it seems worth defining correlated equilibria, and commenting on the distinction from a coarse correlated equilibria in more detail.\n\t2) The explanation of the policy mixer in Section 3.1 is very confusing. For example, what is W in equation 6? What is \"f^I\" in the paragraph after eq 6? How are the individual policies combined to form a joint policy? \n\t3) The key figure, Figure 2 should be much earlier in the paper. \n\t4) Typos: see \"AgnetMixer\" under Figure 2; \"AengMixer\" in Figures 3-5\n\t5) The placement of the related works section is very odd; usually it is the last section in the paper before the conclusion, or the second section in the paper (after introduction)\n\t6) Definition 4 involves a whole posterior inference procedure which is not described enough. Is this procedure purely for illustrative purposes? Or is it actually part of the method proposed by the paper? If the latter, the authors need to add a lot more explanation. The authors should add at least a paragraph of explanation for this, preferably also including an Algorithm detailing this procedure and a figure in the Appendix. For example, can you elaborate further on what pi_psi is and where does it come from? Also, the text states that the procedure given by Eq. (8) converges to a fixed point. Can the authors add references or proof?"
            },
            "questions": {
                "value": "Please see the Weaknesses section for the most important questions. Some additional questions are below: \n- Why does AgentMixer show the most significant performance improvements in Ant and not the other tasks? \n- Does AgentMixer consistently solve relative overgeneralization problems that multi-agent policy gradient methods frequently suffers from? Perhaps this evaluation on RO domains might highlight the benefits of this method. The authors could try running experiments on the predator-prey w/punishment domain specified in [this paper](https://www.ifaamas.org/Proceedings/aamas2021/pdfs/p764.pdf)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7899/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7899/Reviewer_tDVs",
                    "ICLR.cc/2024/Conference/Submission7899/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7899/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698092114774,
        "cdate": 1698092114774,
        "tmdate": 1700607315078,
        "mdate": 1700607315078,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5OjyLqmYAl",
        "forum": "tzD9HVgeVx",
        "replyto": "tzD9HVgeVx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7899/Reviewer_Qpfs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7899/Reviewer_Qpfs"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors target the challenge of stabilizing partially observable multi-agent reinforcement learning (MARL) by proposing AgentMixer, a novel approach that leverages centralized training with decentralized execution (CTDE) to learn correlated decentralized policies. AgentMixer consists of a Policy Modifier (PM) module that models the correlated joint policy and Individual-Global-Consistency (IGC) that maintains consistency between individual policies and joint policy while allowing correlated exploration. They theoretically prove that AgentMixer converges to \u03f5-approximate Correlated Equilibrium. Furthermore, they evaluate AgentMixer on three MARL benchmarks, demonstrating its effectiveness in handling partial observability and achieving strong experimental performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Originality: The paper introduces AgentMixer, a novel MARL approach that combines Policy Modifier and Individual-Global-Consistency to address partial observability challenges.\n- Quality: The theoretical analysis provided includes convergence to an \u03f5-approximate Correlated Equilibrium, which showcases the robustness of the proposed approach.\n- Significance: Experimental performance shows that AgentMixer outperforms existing state-of-the-art MARL methods on three benchmarks, confirming the method's effectiveness and applicability to real-world problems."
            },
            "weaknesses": {
                "value": "1. Some claims about existing works may be inappropriate;\n2. The method part is not clear written;\n3. The experiments can be improved with more critical baselines and better aligning the motivation."
            },
            "questions": {
                "value": "1.  \"While this is attractive, the pre-defined dependence among agents in auto-regressive methods may limit the representation expressiveness of their joint policy classes. \" Why auto-regressive limit the expressiveness? [1]'s eq (5 ) and [2]'s Theorem A.1 provide opposition for the claim. \n2. The claim about \" However, note that both auto-regressive methods and existing correlated policies violate the requirement for decentralized execution. \"  is questionable. As auto-regreesive[1]  and correlated[3] are decentralized execution. \n3. In the method part, eq6, what is the superscript of W_agent mean? why we specially need channel mixing? can you provide some intuitions?\n4. For definition 5, why should we care about identifiability? as in 3, we only need the divergence between local one and global optimal as closer as it can be. Does the ``closer'' surely be the mode consistent?\n5. For Figure 2, you not show show IGC. Also hwo your IGC been used? as a constraint on optimization of 13? Then can you concretely write it and derive the grad?\n6. [1] is very relevant and I do think it should be involved in your baseline.\n\n\n- Minors\n7. The [1], [2], and [3] should be cited. \n8. The IGC should be clearly showed in both loss function and your main figure. \n9. The SMAC-v2 seems still not a good testbed for your motivation or method, I suggest you put it to appendix. \n\n---\n[1] Wang, Jiangxing, Deheng Ye, and Zongqing Lu. \"More Centralized Training, Still Decentralized Execution: Multi-Agent Conditional Policy Factorization.\" ICLR 2022.\n\n[2] Sheng, Junjie, et al. \"Negotiated Reasoning: On Provably Addressing Relative Over-Generalization.\" arXiv preprint arXiv:2306.05353 (2023).\n\n[3] Wen, Ying, et al. \"Probabilistic recursive reasoning for multi-agent reinforcement learning.\" ICLR 2019."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7899/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7899/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7899/Reviewer_Qpfs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7899/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762839471,
        "cdate": 1698762839471,
        "tmdate": 1700708824318,
        "mdate": 1700708824318,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vWllyVpO1V",
        "forum": "tzD9HVgeVx",
        "replyto": "tzD9HVgeVx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7899/Reviewer_9QDT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7899/Reviewer_9QDT"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses the issue that agents make decisions based on their local observation independently, which could hardly lead to a correlated joint policy with sufficient coordination.\n\nTwo key ideas are introduced, i.e., Policy Modifier and Individual-Global-Consistency. Policy Modifier takes the individual partially observable policies and state as inputs and produces correlated joint fully observable policy as outputs. Individual-Global-Consistency keeps the mode consistency among the joint policy and individual policies.\n\nOverall, the presentation is clear, while the authors have clearly expressed their results. The authors have clearly explained the MARL problem they wanted to solve and how to solvem. However, the necessity and importance of work have not been clearly expressed, and there are still doubts which are explained in details as follows."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The core of multi-agent reinforcement learning is strategy alignment and we need to align the local policies obtained from local observation information with the joint policy. This paper aims to solve this issue by polishing the obtained local policies following a well-defined Individual-Global-Consistency condition."
            },
            "weaknesses": {
                "value": "The given Individual-Global-Consistency condition seems to be too high-level, which is similar with Individual-Global-Max condition for value decomposition MARL methods. The authors directly consider this condition as the constraint in (13). The most important issue of the proposed algorithm is that we need to check whether this constraint can be satisfied by following the proposed algorithm. If not, can we measure this distance? Both theoretically and experimentally? This has impaired the contribution of this work."
            },
            "questions": {
                "value": "1. Clarify the connection between IGC and IGM, while compare the corresponding algorithms would be better.\n2. Indeed, there are limitations to the length of the paper, and it is still recommended that the author provide a clear algorithm pesudo code.\n3. The authors discussed the performance of the algorithm during the experimental phase, but did not validate the key technologies proposed in this article, especially the issue of whether modifying local polies can lead to better alignment of joint policy.\n4. I still emphasize the necessity of emphasizing CTDE. This article should be compared with value decomposition or strategy decomposition methods, rather than emphasizing CTDE as a computational framework."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "~"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7899/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698898743352,
        "cdate": 1698898743352,
        "tmdate": 1699636969052,
        "mdate": 1699636969052,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rULAgu44Y1",
        "forum": "tzD9HVgeVx",
        "replyto": "tzD9HVgeVx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7899/Reviewer_yFNn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7899/Reviewer_yFNn"
        ],
        "content": {
            "summary": {
                "value": "This manuscript aims to the asymmetric learning failure problem in Centralized training with decentralized execution (CTDE). To fully take advantage of CTDE to learn correlated decentralized policies, the authors propose the AgentMixer algorithm. It has two key module. The first one is Policy Modifier (PM), which explicitly models the correlated joint policy via composing the partially observable individual policies conditioned on global state information. The second one is Individual-Global-Consistency (IGC), which maintains the mode consistent between the state-based joint policy and partially observable decentralized policies. It is theoretically proofed that AgentMixer converges to \u03b5-approximate Correlated Equilibrium, and the experimental results show that it can achieve comparable performance to existing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper present a novel solution to factorize the joint policy in MARL, i.e. policy mixing and distilling. It also introduce a \u03b5-approximate Correlated Equilibrium perspective to measure the consistence and propose the Individual-Global-Consistency (IGC) to guarantee."
            },
            "weaknesses": {
                "value": "Although it's theoretically proofed that AgentMixer converges to \u03b5-approximate Correlated Equilibrium, the results in the experiments show that the performance of AgentMixer is even or not better than compared methods in many settings. \nThe convergence of AgentMixer is also proofed via the mode consistency. But, The IGC is defined based on the mode of the policy distribution, and the PM is defined via MLP mixer (agent- and channel). It may have some gap here.\nBesides, some contents seem inconsistent in the presentation, figures in the manuscript."
            },
            "questions": {
                "value": "1. Is the Figure 2 a final version? There are no Individual-Global-Consistency components, the gradient forward and backward procedure is not mentioned in the content, and where is the policy distilling?\n2. The performance of AgentMixer is shown even or not better than compared methods in many experiments. The authors should give more analysis.\n3. It may need to be more clear, that how the policy modifier (mixing) can keep the mode consistency.\n4. Some details are not given clearly in the manuscripts, ie. \n    How does the embedding is achieved with $\\pi_{\\theta_1}$ and $s$? \n    what is the meaning of $b$ in the product policy $\\pi_{\\phi}(a|b)$?\n5. The information of this paper cited is incomplete.\nJianing Ye, Chenghao Li, Jianhao Wang, and Chongjie Zhang. Towards global optimality in cooperative marl with the transformation and distillation framework, 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7899/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7899/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7899/Reviewer_yFNn"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7899/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699257866483,
        "cdate": 1699257866483,
        "tmdate": 1699636968956,
        "mdate": 1699636968956,
        "license": "CC BY 4.0",
        "version": 2
    }
]