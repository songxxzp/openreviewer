[
    {
        "id": "q1mZdj1aBp",
        "forum": "EmQSOi1X2f",
        "replyto": "EmQSOi1X2f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9113/Reviewer_gQAk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9113/Reviewer_gQAk"
        ],
        "content": {
            "summary": {
                "value": "This work address a specific hallucination problem in large language models: self-contradition. More specifically, the authors proposed prompt-based approach, including generating, detecting and revising generated text. Extensive analysis shows prevalence of self-contradictions when LMs generate text for open-domain topics and proposed detection and mitigation method are shown to be effective."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The motivations of the paper is important.\n- This paper proposes a simple prompt-based method for triggering, detecting and migitating self-contradictions.\n- The authors proposed a new eval human-annotated dataset.\n-The authors provide code for reproducibility.\n- Although authors demonstrate the proposed method in open-domain text generation task, same approach could be applied to other NLG tasks.\n- The writing is clear"
            },
            "weaknesses": {
                "value": "- Correct me if I'm wrong, the authors only evaluated aLM to detect output from gLM, how robust are aLMs to text generation tasks? that are not generated by \"generating initial text, defining context and trigger\" process? \n\n- Last highlight in Revise, Section 5 is wrong."
            },
            "questions": {
                "value": "1. does the authors try other temperature in between? 0.5, 0.7 for both aLM and gLM?\n\n2. please refer to 1st point in **Weakness** section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9113/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9113/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9113/Reviewer_gQAk"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9113/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698779143145,
        "cdate": 1698779143145,
        "tmdate": 1700631347628,
        "mdate": 1700631347628,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ej1G5bReil",
        "forum": "EmQSOi1X2f",
        "replyto": "EmQSOi1X2f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9113/Reviewer_a7zQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9113/Reviewer_a7zQ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a prompting-based method to detect and mitigate self-contradictory hallucinations. The proposed method generates texts based on a given prompt and subsequently evaluates these for self-contradiction by generating additional sentences. The text is then iteratively revised with the result of self-contradiction. Experimental results involving four LLMs, including GPT-4 and Llama-2-70B, suggest that the proposed approach effectively addresses self-contradictions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is clearly articulated and well-written. Moreover, it offers extensive experimentation with a range of LLMs, demonstrating a solid empirical foundation."
            },
            "weaknesses": {
                "value": "The primary issues with this paper include:\n\n- The paper's central premise is that LLMs can produce self-contradictory hallucinations, and the proposed method aims to rectify using a prompting-based approach with LLMs. While it's acknowledged that LLMs can operate as zero-shot reasoners, their potential to induce hallucinations is also evident. Asserting that the authors correct LLM-induced hallucinations using LLMs (without external knowledge) appears paradoxical. This paper should incorporate a theoretical analysis explaining the efficacy of the proposed method in mitigating hallucinations.\n\n- In the experiments, the verification of the text against Wikipedia is assessed, and various ratios are presented. However, a more in-depth exposition of the experimental setup should be shown. Specifically, manual checks were mentioned, but it's questionable whether the annotators thoroughly scanned the entirety of Wikipedia.\n\n- While the paper claims that self-contradictions can be tackled without relying on external knowledge, it does utilize an \"external\" information extraction system to gather context.\n\n- The term \"topic\" is employed in the paper, but Figure 4 indicates that experiments focused on specific entities. It seems that this paper is focusing on entities than on broader topics. If the text generated is primarily entity-centric, perhaps the experiments should have considered data from existing entity-centric QA research [1] even if the paper's emphasis on the open-domain text generation.\n\n- The proposed iterative mitigation algorithm (Algorithm 3) requires an input n, which represents the number of mitigation iterations. The value of n seems to fluctuate based on the size of |x|. The paper should elucidate the chosen value for n and expound on how mitigation performance is influenced by this value.\n\n[1] When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories, ACL 2023"
            },
            "questions": {
                "value": "Q1. How does the annotator manually verify contradictory information using Wikipedia?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9113/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9113/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9113/Reviewer_a7zQ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9113/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806156764,
        "cdate": 1698806156764,
        "tmdate": 1700734271860,
        "mdate": 1700734271860,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zBNBcjMoVI",
        "forum": "EmQSOi1X2f",
        "replyto": "EmQSOi1X2f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9113/Reviewer_3teD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9113/Reviewer_3teD"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a pipeline to detect self-contradictory hallucinations generated by LLMs. Specifically designed prompts are used to ask LLMs to generate alternative answers to a question. Then an analysis LLM is asked to detect the potential contradiction between two answers and to revise them (to remove the contradiction).\n\nThe method is tested on a set of 30 topics and 360 descriptions generated by LLMs - a dataset created by the authors. The generated answers are verified manually. The method is compared with several baselines using other prompts. The results show that the proposed method can better detect contradictions than other baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper examines an important problem of LLMs - hallucination. Despite the huge impact of the problem, there is a limited number of studies about the its detection. This paper offers an interesting solution to one type of hallucination.\n\nThe proposed method relies on existing LLMs. It is easy to implement and to replicate.\n\nThe authors provide a dataset and the tool that can be reused by other researchers.\n\nThe experimental results show some level of success with the proposed method, which outperforms several other pipelines (prompts).\n\nSome ablation analysis is provided about the impact of different LLMs used in the generation and analysis steps."
            },
            "weaknesses": {
                "value": "The proposed method may detect one type of hallucination where alternative answers are contradictory. In reality, many hallucinations do not contain verifiable contradictions. The coverage of the method on different types of hallucination may be limited.\n(edit after rebuttal: This is partially explained in the rebuttal)\n\nThe experiments are performed on a limited number of cases. Although a larger set of data is created, it cannot be used in a strict evaluation because of the lack of manual verification. I acknowledge the potential high cost for manual evaluation. However, a larger set of cases would make the experiments more meaningful. One possible solution is to leverage the existing datasets of hallucination. For example, a recent large dataset HalluEval (arXiv: 2305.11747) could be possibly used.\n(edit after rebuttal: The number of topics used in the test is still limited. Therefore, the coverage is still questionable.)\n\nThe experimental results may suggest that there is a strong relationship between gLM and aLM. When ChatGPT is used as both, we can see that the performance is higher in most cases. This contradicts the intuition that GPT-4 is a more powerful LLM and should perform better than ChatGPT. Unfortunately, the paper does not report the case of GPT-4 as both gLM and aLM to see if the combination of GPT-4+GPT-4 is better than ChatGPT + ChatGPT. In any case, the fact that the combination of the same LLM for the two steps is better than when different LLMs are used for the steps may suggest that it would be better to use the same LLM for both steps. Some analysis is required to better understand the relationship between the two steps.\n(edit after rebuttal: The answer in the rebuttal does not provide a plausible reason for this. It just restates the experiments.)\n\nThe revision tries to remove the contradictory part of the answers. This is a conservative revision. If one of the answer is correct, it would be better to keep it instead of removing it. From this perspective, it would be useful to refer to some external information source (e.g. Wikipedia). The authors argue that not doing so can make the approach independent of any external resource, but this may also make the method less reliable. Indeed, looking at the example of William T. Freeman, the generated birth dates are all wrong. A comparison with Wikipedia data would be able to tell that. An external resource would be valuable to the detection of hallucination.\n(edit after rebuttal: Despite the explanation in the rebuttal, there is still not a strong motivation for the revision process.)\n\nYou mention that \"a substantial portion of these self-contradictions (e.g., 35.8% for ChatGPT) cannot be verified on Wikipedia\" to motivate the decision not to rely on external resources. However, you can verify on a much broader set of external resources. So the argument not to use an external resource is not very strong.\n(edit after rebuttal: It is true that Wikipedia has a good coverage for general topics. It may miss many more recent or local events. Other web documents may be useful. Why not using web search to find possibly relevant information?)\n\nThe following description about informativeness is unclear (please revise): For informativeness, we calculate the ratio of non-contradictory sentence pairs (i.e., informative facts) in the revised text compared to the original text. Note that this ratio might exceed 100% because our mitigation can revise contradictory sentence pairs into non-contradictory ones. \n(edit after rebuttal: the revised definition of informativeness is clearer.)\n\nThe paper proposes some reasonable prompts to generate answers and verify contradictions. The proposed prompts should be further motivated. The authors have not provided strong reasons to choose the specific prompts. The only way to see their superiority is through the comparisons in experiments with other alternative prompts.\n(edit after rebuttal: The explanation about the prompts is reasonable. It still remains unclear why these specific prompts are chosen instead of other alternatives.)\n\nThe papers target long text generation in contrast to short answers targeted by some other papers. Why is the length of answers a key factor that distinguish this work from a set of existing studies? Would it still be possible to compare the proposed method with others on short answers?\n(edit after rebuttal: If the method can be applied to both long and short texts, the paper should not emphasize the ability of the method for long texts. In fact, no specific means is taken to deal with long texts vs short texts.)"
            },
            "questions": {
                "value": "Have you tested with different temperatures in decoding? This may produce different answers.\n\nCan you compare the method with other methods that leverage external resources?\n\nWould it be possible and meaningful to compare the proposed method with others on short answers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9113/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9113/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9113/Reviewer_3teD"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9113/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699328268894,
        "cdate": 1699328268894,
        "tmdate": 1700609561332,
        "mdate": 1700609561332,
        "license": "CC BY 4.0",
        "version": 2
    }
]