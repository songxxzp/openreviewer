[
    {
        "id": "lM28CjpwoG",
        "forum": "zbOSJ3CATY",
        "replyto": "zbOSJ3CATY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6302/Reviewer_gukw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6302/Reviewer_gukw"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a robust optimization algorithm called GTSONO, designed to train Neural Ordinary Differential Equations (Neural ODEs) that are more resilient to adversarial attacks. Based on min-max Differential Dynamic Programming, the algorithm is not only computationally efficient but also guarantees convergence to local saddle points. Experimental results demonstrate its significant advantage in improving model robustness compared to benchmark optimizers. Overall, the paper offers a new and effective tool for enhancing the robustness of deep learning models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality:\nThe paper introduces a novel perspective by interpreting Neural ODE optimization as a min-max optimal control problem. GTSONO's approach, rooted in min-max Differential Dynamic Programming, showcases a creative amalgamation of existing concepts.\nQuality:\nOffering convergence guarantees to local saddle points signifies the robustness of GTSONO. Efficient matrix decompositions and strong empirical results further attest to the research's high quality.\nClarity:\nThe document articulately bridges intricate theoretical concepts with empirical findings. Despite some formatting issues, the presentation remains clear and coherent.\nSignificance:\nThis work addresses a pivotal challenge in neural ODEs, enhancing their robustness against adversarial attacks. The exploration of optimal control paradigms in adversarial training methods underscores its contributions in the domain."
            },
            "weaknesses": {
                "value": "1. Although the GTSONO optimizer proposed in this paper can improve the robustness of the model to some extent, it adds too much extra computational overhead, which is unacceptable in the training of larger models.\n2. The paper's limited comparison with just one other optimizer that improves robustness diminishes the persuasiveness of the results, as a more comprehensive comparison would have strengthened the findings.\n3. One limitation of this paper is that the experiments are confined to the CIFAR10 and SVHN datasets, with no validation on more extensive datasets such as CIFAR100 and ImageNet. This restricts the applicability of the research to a certain extent."
            },
            "questions": {
                "value": "1. Could the authors provide insights into the feasibility of applying GTSONO to larger and more complex datasets such as CIFAR100 and ImageNet?\n2. Given the focus on robustness improvement, could the authors consider including a more extensive comparison with various state-of-the-art optimizers that enhance robustness? \n3. Are there any further insights into the stability and convergence properties of GTSONO, especially under varying hyperparameters and different neural network architectures?\n4. In general, deep learning optimizers have used mini-batch size to obtain a partial batch of datasets, which means that random noise will be introduced, so the optimizer's parameter update rule should correspond to a stochastic differential equation instead of an ordinary differential equation. Therefore, why do the authors use an ODE rather than an SDE for their theoretical analysis?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6302/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6302/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6302/Reviewer_gukw"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6302/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698636410387,
        "cdate": 1698636410387,
        "tmdate": 1700618854948,
        "mdate": 1700618854948,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sNgDvhjNXH",
        "forum": "zbOSJ3CATY",
        "replyto": "zbOSJ3CATY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6302/Reviewer_L7bT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6302/Reviewer_L7bT"
        ],
        "content": {
            "summary": {
                "value": "This paper interprets neural ODE optimization as a min-max optimal control problem, and proposed a second order optimizer. The proposed optimizer is computationally feasible by matrix decomposition. Empirically, the authors compare with other optimizers (Adam, SGD and another not min-max second order optimizer), and show it has improved adversarial robustness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper proposes an interesting perspective of robust neural ODE optimization via min-max optimal control. \n\nThe authors make the proposed second order optimizer to be computational feasible: instead of back propagate coupled matrices, one can back propane a set of vectors. \n\nThe authors provide convergence guarantee of the proposed optimizer. \n\nExperiments show improved adversarial robustness comparing with other non-robust neural ODE optimizers."
            },
            "weaknesses": {
                "value": "1. Although I think it is interesting to formulate robust neural ODE optimization as min-max OC, I feel there is gap why the formulation in (3) can be beneficial to the robustness problem in (1): in (3), the adversary is on neural network weights, and in (1), the adversary is on the inputs to the neural network. \n\n2. Despite the effort of reducing computational cost, the proposed method is still very expensive. It would be good to include a complexity comparison between other neural ODE optimizers: first-order adjoint and SNOpt.\n\n3. It is known that neural ODE tends to suffer from gradient obfuscation issue when being evaluated for empirical adversarial robustness. It would be beneficial to have some adaptive attacks or non-gradient based attacks to make sure the improved robustness is valid. For instance, the attacks used in [1]. Since the optimizer on its own has lower robustness accuracy than adversarial training methods, it is crucial to have solid experiments to show its benefits when combining with other robust training techniques. In general, I like the min-max OC perspective, but it may still lack evidence for its usefulness. Maybe the authors could also consider evaluating against adversary on neural network weights, which I think is more close to the formulation.\n\n[1] Kang, Qiyu, et al. \"Stable neural ode with lyapunov-stable equilibrium points for defending against adversarial attacks.\" Advances in Neural Information Processing Systems 34 (2021): 14925-14937."
            },
            "questions": {
                "value": "1. My main question is as in weakness 1: the proposed optimizer seems to be beneficial to attacks on neural network weights rather than on the inputs. I hope the authors can clarify why they choose to demonstrate the effectiveness of their optimizer on input-robustness, and will the method be useful for attacks on system weights?\n\n2. In the experiments, it seems that only having adversarial control on convolution layers is much better than having them on all of the layers. From the theory parts it is not clear why this is the case. The authors should provide more analysis on this.\n\n3. When combing with adversarial training (table 4), why CW accuracy drops? This may indicate some gradient obfuscation issue, it will be good to include some stronger attacks in the evaluation (like AutoAttack, Square) as suggested in weakness 3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6302/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698858899314,
        "cdate": 1698858899314,
        "tmdate": 1699636691978,
        "mdate": 1699636691978,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ml56bBhTs2",
        "forum": "zbOSJ3CATY",
        "replyto": "zbOSJ3CATY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6302/Reviewer_veDw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6302/Reviewer_veDw"
        ],
        "content": {
            "summary": {
                "value": "This paper studies robust optimisation method for neural ODEs. The authors interpret Neural ODE optimization as a min-max optimal control problem, and then design a Game Theoretic Second-Order Neural Optimizer (GTSONO), based on min-max Differential Dynamic Programming, with convergence guarantees to local saddle points. The authors also conduct experiments to verify the performance of GTSONO."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is well motivated - addressing the vulnerability to adversarial attacks in neural network related methods, including neural ODE.\n\nThe authors design a Game Theoretic Second-Order Neural Optimizer as a robust optimiser for neural ODEs. \n\nThey also provide rigorous, theoretical analysis for the proposed method. The proofs are given in detail. \n\nThe paper is well written."
            },
            "weaknesses": {
                "value": "I am worried about the novelty, after reading the calculations. Leveraging min-max methods for adversarial learning is a usual approach.  convergence. The calculations of gradients and backpropagation are simple calculus and linear algebra. The proof of convergence is a direct application of existing results in optimisation. I suggest the authors clarify the novelty of their algorithms and proofs, and discuss the differences and advantages of their method.\n\nThe experiments are only conducted on CIFAR-10 and SVHN. Experiments on CIFAR-100, and ImageNet (or at least TinyImageNet) are needed for comparison.\n\nFrom Tables 1 and 2, GTSONO has fairly bad performance in CIFAR-10 in term of natural accuracy. Please discuss why this happens.\n\nThe authors only compare with SGD, Adam, and a second order baseline SNOpt. It is not enough. Please compare your method with most existing methods. I list some below:\n\nhttps://proceedings.mlr.press/v162/rodriguez22a/rodriguez22a.pdf\n\nhttps://arxiv.org/pdf/2210.16940.pdf"
            },
            "questions": {
                "value": "Please address the above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6302/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6302/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6302/Reviewer_veDw"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6302/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699201062331,
        "cdate": 1699201062331,
        "tmdate": 1700674588604,
        "mdate": 1700674588604,
        "license": "CC BY 4.0",
        "version": 2
    }
]