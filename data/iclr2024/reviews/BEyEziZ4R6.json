[
    {
        "id": "bm1DWsfemy",
        "forum": "BEyEziZ4R6",
        "replyto": "BEyEziZ4R6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5238/Reviewer_GMKM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5238/Reviewer_GMKM"
        ],
        "content": {
            "summary": {
                "value": "This paper studies to train Lipschitz neural networks with DP, so as to avoid using per-sample gradient clipping. It gives theorems of the tradeoff between privacy and utility, as well as a Python package that efficiently implement the proposed algorithms. Empirical results on toy datasets show some promise of this new method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Overall, the paper is well-polished and rigorous. I appreciate the careful layout of the requirements and proper introduction of Lipschitz literatures."
            },
            "weaknesses": {
                "value": "I have some concerns about the claims made in this work, in addition to some minor issues.\n\n1. The significance of this work is based on the hypothesis that per-sample gradient clipping is inefficient. This has been emphasized in the abstract and in the paragraph below Definition 2, in which the authors wrote \"Hyper-parameter search on the broad-range clipping value C is required...The computation of per-sample gradients is expensive\". However, this is mistaken. DP-SGD does not need to tune the clipping value (see \"Automatic clipping: Differentially private deep learning made easier and stronger\" and \"Normalized/Clipped SGD with Perturbation for Differentially Private Non-Convex Optimization\"). DP-SGD/AdamW can be as fast and memory efficient as standard SGD, while maintaining the same DP accuracy. Many papers that use ghost clipping can achieve this (see \"Differentially Private Optimization on Large Model at Small Cost\", \"Exploring the Limits of Differentially Private Deep Learning with Group-wise Clipping\", \"LARGE LANGUAGE MODELS CAN BE STRONG DIFFERENTIALLY PRIVATE LEARNERS\", \"Scalable and Efficient Training of Large Convolutional Neural Networks with Differential Privacy\"), improving the speed from 1.7X slower than non-DP to 1.1X. It would be inappropriate if the authors only refer to the older works like Kifer's and Opacus.\n\n2. Another issue is to claim per-sample clipping introduces bias and not to fully discuss the bias of Clipless DP-SGD. The gradient bias does exist, but may not be the reason of slow convergence. Furthermore, I believe Clipless DP-SGD also introduces some bias, in the form of network architecture by moving from regular ResNet to GNP ResNet. It would not be fair to discard the clipping by the current discussion in Theorem 1.\n\n3. The empirical results in this work are not convincing, focusing only on toy datasets, restricted layer types (not including embedding layer that is vital to language model and vision transformer), and toy models (e.g. Figure 5, model size 2M). I wonder whether the new method can benefit from large scale pretraining?"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5238/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698800577430,
        "cdate": 1698800577430,
        "tmdate": 1699636522735,
        "mdate": 1699636522735,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vY2uR0Uq0N",
        "forum": "BEyEziZ4R6",
        "replyto": "BEyEziZ4R6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5238/Reviewer_L744"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5238/Reviewer_L744"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors introduce Clipless DP-SGD, an approach for training neural network models with differentially private stochastic gradient descent, without the need to apply clipping to the parameter gradients.\nTo achieve this, the authors leverage existing theory on Lipschitz neural networks, which they extend to compute Lipschitz constants with respect to the parameters (instead of w.r.t. the inputs, which has already been studied in the literature).\nThis enables them to compute per-layer sensitivities, for a range of common feedforward architectures, without requiring gradient clipping.\nThe authors evaluate their approach on a range of benchmark datasets, where they observe a competitive performance compared to DP-SGD."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "In my view the main strengths of the paper are as follows:\n\n__Eliminating the need to tune $C$ and reducing gradient bias:__\nOne important benefit of Clipless DP-SGD is that it does not involve a clipping threshold $C,$ and as such it does not require tuning this parameter.\nThis is particularly useful since, to extract strong performance out of DP-SGD, selecting an appropriate value for $C,$ i.e. tuning it, is necessary.\nPerforming this tuning naively would result in privacy leakage since updating $C$ requires querying the data.\nClipless DP-SGD elegantly circumvents this issue by eliminating clipping altogether.\nAnother benefit of removing clipping, as the authors note, is that this eliminates the associated bias that is introduced in the gradients, potentially making optimisation easier, though to my understanding the effect of eliminating this bias has not been explicitly examined in the paper.\n\n__Reduced runtime and memory requirements:__\nAnother benefit of the proposed method is the fact that, while DP-SGD computes per-sample gradients and clips these separately before averaging, Clipless DP-SGD operates on the averaged loss directly, without computing per-sample gradients.\nAs a result, Clipless DP-SGD offers both a lower memory footprint as well as faster runtimes than regular DP-SGD.\nThe favourable runtime and memory requirements of the proposed method are illustrated in figure 3 (though it should be noted that different implementations on different back-ends may not be directly comparable).\n\n__Originality and relations to existing work:__\nIn my assessment the contribution made in this paper is both original and creative, since computing Lipchitz constants with respect to the network parameters does not seem to have been studied in the literature before.\nThe authors build on existing literature on Lipchitz networks and extend it in a valuable way.\n\n__Codebase contribution:__\nThe authors provide a codebase for differentially private Lipchitz models, $\\texttt{lip-dp}$ which is a valuable contribution in itself, and may be especially useful to practitioners.\n\n__Quality of exposition and rigour:__\nI found the main text well written and relatively easy to read considering its technical content.\nThe method is well motivated and well explained, and although I have not examined the proofs very closely, the exposition in the main text seems sound."
            },
            "weaknesses": {
                "value": "In my view, the paper does not present critical weaknesses, though two points that I think are important to consider are:\n\n__Performance compared to existing methods:__\nIt seems that, while the current method eliminates the need for clipping (thereby simplifying the tuning procedure and removing gradient bias), it still performs worse than existing methods on datasets beyond MNIST, sometimes by a large margin (see figure 13 in the appendix).\nIt is not entirely clear which design choice is responsible for this gap in performance, though the Lipschitz constraints imposed on the network is a likely candidate.\nGiven this, it is not entirely clear how competitive the proposed method would be compared to existing methods, on more realistic tasks.\n\n__Hyperparameter tuning:__\nWhile Clipless DP-SGD removes the need for tuning the clipping threshold, it still appears to require a large amount of hyperparameter tuning in order to extract good performance (see figure 3 in the main text, and figure 13 in the appendix).\nAs a result, this can lead to increased computational costs as well as privacy leakage, since the current method does not account for leakage due to hyperparameter tuning.\nIt is not fully obvious how Clipless DP-SGD compares to existing methods under a like-for-like compute resources and privacy leakage due to tuning."
            },
            "questions": {
                "value": "I have the following questions for the authors regarding this work:\n\n__Clarification on pareto fronts:__\nTo my understanding, the pareto fronts shown correspond to the convex hull of the green points.\nThe authors explain that the green points themselves correspond to a pair of validation accuracy and $\\epsilon$ parameter from a given epoch.\nDo these points correspond to all epochs across all Bayesian optimisation runs, as explained in appendix D2?\nCan the authors comment on the extent of the privacy leakage that would result from selecting a particular model from the pareto front?\n\n__Extending this to other architectures, e.g. transformers:__\nThe approach developed in this paper applies to feedforward networks, which admittedly cover a significant range of existing architectures.\nCan the authors comment on whether it is possible to extend their method to other popular architectures, such as transformers or graph neural networks?\nIt is unclear to me how the ideas developed here can be extended to, for example, cases where the internal representation of the network depends on the input datum itself.\n\n__How tight are the derived sensitivities in practice?__\nCan the authors comment on how tight the derived sensitivities are in practice?\nIn particular, how large are the gradient norms encountered during training, compared to the derived sensitivities.\n\n\nIn addition, I would like to point out the following typos and suggestions:\n\n- __Typo:__\nIn def. 3, \"there is no parameters\" should be \"there are no parameters.\"\n- __Suggestion:__\nIt might be worth separating the definition of Lipschitz networks from regular feedforward networks, keeping two separate definitions, or making the first part of the definition part of the text.\n- __Typo & suggestion:__\nIn Theorem 1, \"centered in zero\" should be \"centered at zero.\"\nAlso you could change \"expanded analytically.\" to \"computed analytically, as follows:\"\n- __Typo:__\nIn page 7, \"for Lipschitz constrained network\" should be \"for Lipschitz constrained networks\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5238/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807796380,
        "cdate": 1698807796380,
        "tmdate": 1699636522644,
        "mdate": 1699636522644,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AUBCySyD9N",
        "forum": "BEyEziZ4R6",
        "replyto": "BEyEziZ4R6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5238/Reviewer_ah9n"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5238/Reviewer_ah9n"
        ],
        "content": {
            "summary": {
                "value": "This paper presents DP-SGD without gradient clipping, achieved through gradient norm preservation, a method that has not been explored in the existing DP-SGD literature."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper introduces a novel approach in the DP-SGD literature by offering DP-SGD without clipping through gradient norm preservation, a method that has not been explored before."
            },
            "weaknesses": {
                "value": "To validate the effectiveness of the proposed methods, it is essential to conduct a more comprehensive experimental evaluation."
            },
            "questions": {
                "value": "How can we ensure a fair experimental comparison? Specifically, tuning hyperparameters also consumes the DP budget. Did you take into account such DP costs in your experiments and results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5238/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699074779449,
        "cdate": 1699074779449,
        "tmdate": 1699636522559,
        "mdate": 1699636522559,
        "license": "CC BY 4.0",
        "version": 2
    }
]