[
    {
        "id": "04hqX2C9yJ",
        "forum": "rxlF2Zv8x0",
        "replyto": "rxlF2Zv8x0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1409/Reviewer_zgX5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1409/Reviewer_zgX5"
        ],
        "content": {
            "summary": {
                "value": "The manuscript proposes a set of techniques for protein optimization. The first is a method for smoothing protein fitness landscapes. The second is a technique to optimizing in this landscape using the Gibbs With Gradients procedure, which has previously been shown to provide excellent results for discrete optimization. The authors also design two new optimization tasks based on the GFP and AAV datasets, which are designed to be more difficult than previous variants. Finally, the authors demonstrate empirically that their method performs competitively with the state-of-the-art."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "### Originality\nAlthough the GWG optimization procedure has been used in other contexts, the application to protein optimization is novel. To my knowledge, also the specific graph-based formulation of the regression problem itself is new.\n\n### Quality\nThe paper seems technically sound. Code was provided to ensure reproducibility, and the authors provide additional details about the method in the supporting material. \n\n### Significance\nThe paper does not give much insight into why the method outperform earlier approaches (see below for details), but the empirical results are convincing, which by itself could be sufficient to have impact on the growing subcommunity in ICML interested in protein modelling and design."
            },
            "weaknesses": {
                "value": "My main concern with the paper is that I - after reading it - do not feel much wiser about promising methodogical directions for protein modelling going forward.\nWhat I lack in the paper is perhaps more of a motivation of why particular modelling choices were made. For instance, why is the Tikhunov regularization a meaningful choice in the context of protein optimization? Intuitively to me, it seems like a fairly crude choice, ignoring much of what we know about proteins already (e.g. that certain amino acids are biochemically similar to others). The paper also provides no biological intuition about why we would expect the smoothness would help. Presumably, the idea must be that there are different length scales to the problem, and that we can ignore the short length scales and focus on the longer ones - but it is not obvious to be why that would be the case for proteins. Is part of the explanation that experimental data is typically quite noisy? But if that's the case, you would assume that you would get similar behavior by using a simple GP with observation noise - just using a kernel based on edit distance - or based on Eucledian distance in one-hot space. The paper would be much more satisfying for me if the smoothing procedure was motivated more clearly, and perhaps even validated independent of the optimization procedure (I assume you would hope that the smoothed regressor would extrapolate better?)\n\nMy other serious concern is about the empirical evaluation of the model. As far as I can see, when we evaluate an optimization model against an oracle, there is a risk that we end up optimizing against extrapolation artefacts of the oracle, in particular if we end up evaluating it far away from the data it was trained on. My concern is whether your method has an unfair advantage compared to the baselines, because it uses the same CNN architecture for both the model and the oracle - and could therefore be particularly well suited for exploiting these artefacts. To rule out this concern, it would be interesting to see how the model performs against an oracle trained using a completely different model architecture."
            },
            "questions": {
                "value": "Page 4,\n*\"Edges, E, are constructed with a k-nearest neighbor graph around each node based on the Levenshtein distance 3.\"*\nIn real-world cases, the starting point is often a saturation mutagenesis experiment, where a lot of candidates will be generated with the same edit distance from the wild type (e.g. edit distance 1). In such cases, won\u2019t the fixed k-out degree lead to an arbitrary graph structure (I mean, if the actual number of equidistant neighbors is much larger than k)?\n\nPage 6, *\"4.1 Benchmark\"*\nIt was difficult to follow exactly what \"develop a set of tasks\" implies. Since the benchmarks are built from existing datasets, the authors should make it clearer exactly what they are \"developing\": is it only the starting set, or do they also restrict themselves to a subsample of the entire set? In table 1 and 2, are both *Range*, *|D|*, and *Gap* specifically selected for, or does e.g. *|D|* arise out of a constraint on *Range* and *Gap*?\n\nPage 6. *\"Oracle\"*\nSince you are using a CNN both as your oracle, and as the basis for your smoothed landscape model, isn\u2019t there a risk that your model is just particularly well suited for exploiting the extrapolation artifacts of the oracle? (repetition of concern stated above).\n\n### Minor comments:\nPage 1, *\"but high quality structures are not available in many cases\"*\nAfter AlphaFold, many would consider that high quality structures are now available in most cases.\n\nPage 2, *\"mutation is proposed renewed gradient computations.\"*\nSomething is wrong in this sentence\n\nPage 3, *\"in-silico oracles provides a accessible way for evaluation and is done in all prior works.\"*\nThis is not entirely accurate. People have optimized against actual experiments (e.g. Gruver, ..., Gordon-Wilson, 2023) - or optimized to find the optimal candidate in a fixed set of experimentally characterized proteins.\n\nPage 4, eq (2) *\"H(x)\"*\nAs far as I can see, H(x) has not been introduced(?)\n\nPage 6. *\"we utilize a simpler CNN that achieves superior performance in terms of Spearman correlation and fewer false positives.\"*\nWas this correlation measured on the GFP test set provided by TAPE after fitting on the training set?. If so, it's odd that the original TAPE paper did not find the CNN-based ResNet to outperform the transformer (actually, the transformer performance was dramatically higher). Please clarify.\n\nPage 6. *\"Recall the protein optimization task is to use D\"*\nPerhaps help the reader by rephrasing to \"Recall the protein optimization task is to use the starting set D\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1409/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1409/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1409/Reviewer_zgX5"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1409/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698270345380,
        "cdate": 1698270345380,
        "tmdate": 1700650859473,
        "mdate": 1700650859473,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gcrzF0MrTh",
        "forum": "rxlF2Zv8x0",
        "replyto": "rxlF2Zv8x0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1409/Reviewer_7UDm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1409/Reviewer_7UDm"
        ],
        "content": {
            "summary": {
                "value": "The authors introduces a method called Gibbs sampling with Graph-based Smoothing (GGS) that uses Tikunov regularization and graph signals to smooth the protein fitness landscape, improving the ability to create diverse, functional sequences."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Figure 1 is very helpful in the understanding of this approach.\n\nMy understanding of the section described in Section 3.2 is relatively clear.\n\nI think the Fitness, Diversity, and Novelty scores to be interpretable and helpful.\n\nI think it is encouraging that graph-based smoothing (GS) helps almost all other methods in Table 3. It\u2019s also great that this is a relatively straightforward procedure."
            },
            "weaknesses": {
                "value": "\u201cWhile dWJS is an alternative approach to fitness regularization, it was only demonstrated for antibody optimization. To the best of our knowledge, we are the first to apply discrete regularization using graph-based smoothing techniques for general protein optimization.\u201d - This doesn\u2019t seem justifiably novel. Proteins are proteins.\n\nGenerally, I wouldn\u2019t use the term \u201cfitness\u201d when describing protein function. Rather, I would use phenotype or function, as fitness is a broad, poorly defined subset of fitness.\n\nFigure 5 is a reason why these function predictors should not be called \u201coracles\u201d, because mapping the effect of mutation to function is difficult itself. I\u2019d prefer \u201cprotein function approximator\u201d, or something along those lines.\n\n\u201cThese were chosen due to their long lengths, 237 and 28 residues\u201d What do you mean here? 28 isn\u2019t that long. I realize it is in the context of a larger protein, but I\u2019d be clear about that."
            },
            "questions": {
                "value": "For the smoothing procedure, it\u2019d be great to show the amount of error introduced into the labels of the sequences. For instances where either a reasonable oracle model exists, or sequences with large hamming distances have been measured, and this smoothing procedure is introduced, what is the correlation of function values before and after?\n\n\u201cTo control compute bandwidth, we perform hierarchical clustering (Mullner, 2011) on all the se- \u00a8 quences in a round and take the sequence of each cluster with the highest predicted fitness using f\u03b8.\u201d Why not use the \u201cnoisy model\u201d for this, because it is the oracle for the true fitness of a sequence?\n\n\u201cSection 4.1 presents a set of challenging tasks based on the GFP and AAV proteins that emulate starting optimization with a noisy and limited set of proteins.\u201d I would like the authors to be clear by what the mean by \u201cnoisy\u201d. Is it experimental noise? Is the landscape too sparsely sampled? Where is this noise coming from, and what relative distribution does it have?\n\nGenerally, I feel like Figure 5 is a distraction from the broader utility of the work. I\u2019d just cite Dallago 2021 like you did for the use of CNNs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1409/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698722787338,
        "cdate": 1698722787338,
        "tmdate": 1699636068919,
        "mdate": 1699636068919,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CnvJzdmAeS",
        "forum": "rxlF2Zv8x0",
        "replyto": "rxlF2Zv8x0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1409/Reviewer_Ayis"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1409/Reviewer_Ayis"
        ],
        "content": {
            "summary": {
                "value": "This study proposes to smooth the protein fitness landscape to facilitate protein fitness optimization using gradient based techniques. This is motivated by the ruggedness of protein fitness landscape which makes optimization challenging. A graph based smoothing technique for fitness landscape followed by Gibbs with Gradient sampling is used to perform protein fitness optimization. Evaluation of their method has been done on train sets designed from GFP and AAV with two degrees of difficulty defined by the mutational gap between the starting set and the optimum in the dataset (not included in the starting set).  Their method shows better performance than others in the proposed benchmark. The proposed graph smoothing technique has been shown to help with other methods as well."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Designing train sets with varying difficulties for the task of optimization.\nProposing a new method for smoothing the protein fitness landscape before optimization."
            },
            "weaknesses": {
                "value": "The proposed method has many hyperparameters to tune. \nGiven certain properties of protein fitness landscape, smoothing can hurt if not done properly."
            },
            "questions": {
                "value": "1)\tPlease explain why after smoothing, the diversity and novelty of the final set of sequences decreases.\n2)\tIn defining train sets with varying levels of difficulty only two medium (mutation gap 6) and hard (mutation gap 7) levels have been used. What happens if you make this harder (higher than 7)? Also, should we assume that for less mutational gap all methods perform comparably?\n3)\tAs stated in the paper, single mutations can dramatically change the fitness. In the smoothing performed, similar sequences are enforced to have similar fitness. Have you investigated where smoothing can be detrimental?\n4)\tHow is the number of proposals ($N_{\\text{prop}}$) per sequence set?\n5)\tHave you tried smaller sizes for the starting set? In real world problems the size of the starting set could be much smaller than 2000? \n6)\tWas the oracle only used at the end for performance evaluation? In AdaLead, did you use the oracle as the fitness landscape or $f_\\theta$?\n7)\tMention the augmented graph size (how does it change with the size of the sequence)\n8)\tMinor: In Eq 5, $X_0$ should be X."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1409/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1409/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1409/Reviewer_Ayis"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1409/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784387797,
        "cdate": 1698784387797,
        "tmdate": 1699636068850,
        "mdate": 1699636068850,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EvNCz4aoc4",
        "forum": "rxlF2Zv8x0",
        "replyto": "rxlF2Zv8x0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1409/Reviewer_PzPY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1409/Reviewer_PzPY"
        ],
        "content": {
            "summary": {
                "value": "The paper propose a smoothing method on fitness function given a protein sequence. Assume that the given original data set is small, authors proposed a sampling augmentation method and a TV smoothing regulariser. After which MCMC algorithm is use to further optimise the fit."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Authors presented some good results on benchmark datasets."
            },
            "weaknesses": {
                "value": "The paper is hard to read and understand. I itemise areas for improvements.\n\n1. Having one figure to show overall flow of logic could help. Fig1 seems to do the job. There are some confusion between training and sampling. I understand that the author first train f(x) and then use f(x) as a surrogate function for MCMC optimisation. This point does not come out naturally.\n\n2. construction of KNN graph could be described more clearly. (see Eq above Eq(1))\n\n3. Symbols of Eq.(2) are ill defined. The authors should provide in the appendix some details of GWG and reference the appendix in the main text.\n\n4. Eq.(4) should give the acceptance rate. while q are the probability of trial moves. x and x' are two states for jumping in this one MC step. Usual notation is q(x|x') vs q(x'|x), notation of Eq.(4) certainly is not of this form. Instead i^loc and j^sub and being used. The same i^loc and j^sub cannot appear in both numerator and denominator of Eq.(4).\n\n5. Eq.(4) what is the temperature of this move? It seems the temperature is set to 1. Why is the temperature 1? Is there any annealing process?\n\n6. Clustered sampling section should be explained better."
            },
            "questions": {
                "value": "Is there a way to test that the surrogate function by itself is good enough? The authors look at the overall performance that could infer to correctness of the surrogate function.\n\n\nsee above section on 'weakness'"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1409/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699172969363,
        "cdate": 1699172969363,
        "tmdate": 1699636068776,
        "mdate": 1699636068776,
        "license": "CC BY 4.0",
        "version": 2
    }
]