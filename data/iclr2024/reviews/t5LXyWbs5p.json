[
    {
        "id": "3dC42g0pcS",
        "forum": "t5LXyWbs5p",
        "replyto": "t5LXyWbs5p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6179/Reviewer_hfvK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6179/Reviewer_hfvK"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the problem of learning a reliable pretraining approach against domain shifts in multimodal biosignal processing. Proposed method harnesses frequency-domain features in multimodal representation alignment, using a transformer-based masked autoencoder. Simulations are conducted on various multimodal biosignal datasets (e.g, EEG, EOG, EMG, ...), and the approach is demonstrated to be beneficial in several experiments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The approach is novel and effective in its application to multimodal representations and alignment in the frequency domain.\n- The paper is written well with a good storyline and overview illustrations that give the main intuition clearly."
            },
            "weaknesses": {
                "value": "- Reproducibility in biosignal processing studies is often a concern. Therefore authors should consider strengthening their randomized experiment setup, and open sourcing their code and simulations reported in this submission."
            },
            "questions": {
                "value": "1) Regarding the downstream tasks: What is the true impact of the used dataset split folds? Did the authors investigate this in simulations where the randomness is controlled (they mention a number of seed repetitions here), or is the appendix Table 7 a one-time setup where test performances are only presented for? What is the performance deviation in such repetitions? This needs to be clarified a bit further to support the strength/reliability of the results.\n\n2) How influential is the architecture capacity on the results? The design of the network backbone seems like an arbitrary choice, as it is not really justified much or studied in the ablations. In comparison to the other approaches, what would be the size of the network (or parameters to be optimized) in the proposed model?\n\n3) What is the comparative computational overhead of bioFAME at training and test time, as opposed to previous alternatives (e.g., TS-TCC, PatchTST..)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6179/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698241475272,
        "cdate": 1698241475272,
        "tmdate": 1699636671850,
        "mdate": 1699636671850,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zfuYzXZFm2",
        "forum": "t5LXyWbs5p",
        "replyto": "t5LXyWbs5p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6179/Reviewer_BHTD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6179/Reviewer_BHTD"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a frequency-aware MAE design for multimodal pretraining on time-series biosignals in the frequency domain. They use a frequency-aware transformer encoder and a frequency-maintain pretraining strategy with masked autoencoding in the latent space. Specifically, the proposed encoder mainly consists of a frequency-domain feature extractor after DFT transformation and an attention-based dynamic fusion mechanism. The frequency-maintain pretraining strategy mainly addresses that masked autoencoding happens in the latent space and channel-independent feature encoding. Experimental results on uni-modal and multi-modal biosignals are provided to demonstrate the advantage of the proposed approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors tackled an interesting and practical problem in the domain of multi-modal pretraining of time-series sensing signals, with resiliency and robustness considerations.\n\n2. Extensive evaluation results on both uni-modal and multi-modal scenarios are reported.\n\n3. The presentation and writing of the paper is of good quality. It is generally easy to follow and understand the paper.\n\n4. The robustness test presented in the experiment helps justify the performance of the approach."
            },
            "weaknesses": {
                "value": "1. The authors claimed the frequency-domain analysis and feature extraction for biosignals (being time series) as one of the main contributions in this paper, which actually have been well studied in the sensing community [1, 2]. Besides, the Transformer encoder, the masked autoencoding paradigm, and the attention-based fusion mechanism are original and novel contributions made in this paper. The channel-independent design, as mentioned in the paper, has been studied in several previous works. For this reason, I feel the novelty of the proposed approach is limited.\n\n2. The approach of this paper seems to be generally applicable to various time-series sensing modalities, and there is no special design unique to the biosignals. I am not sure why you position the paper as one, especially for multi-modal pretraining on biosignals.\n\n3. There lack of comparisons to existing multi-modal contrastive frameworks in the literature [3, 4], so it would be hard to understand its performance in the multi-modal collaboration scenario.\n\n4. There is a comparison between channel-independent learning and conventional approaches that simultaneously consume all channels. In my personal opinion, channel-independent learning might be less efficient in computation (reduced parallelizability) and might not convey the global information (related to all channels) very well, which could be hard to model by simply concatenating extracted channel features.\n\n[1] Yao, Shuochao, et al. \"Stfnets: Learning sensing signals from the time-frequency perspective with short-time Fourier neural networks.\" The World Wide Web Conference. 2019.\n\n[2] Li, Shuheng, et al. \"Units: Short-time Fourier inspired neural networks for sensory time series classification.\" Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems. 2021.\n\n[3] Tian, Yonglong, et al. \"Contrastive multiview coding.\" Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI 16. Springer International Publishing, 2020.\n\n[4] Poklukar, Petra, et al. \"Geometric multimodal contrastive representation learning.\" International Conference on Machine Learning. PMLR, 2022"
            },
            "questions": {
                "value": "1. How many labels do you use during the fine-tuning stage? What finetuning strategy do you use? Do you only finetune the appended classification layers or update the whole encoder parameters?\n\n2. What is the main design that distinguishes your approach from existing frequency-domain encoders for time-series signals? There is no discussion or experiment on this comparison in the paper.\n\n3. Could you provide an ablation study on turning on/off the channel independent learning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6179/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6179/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6179/Reviewer_BHTD"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6179/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734498859,
        "cdate": 1698734498859,
        "tmdate": 1699636671708,
        "mdate": 1699636671708,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FFc2qSYfgI",
        "forum": "t5LXyWbs5p",
        "replyto": "t5LXyWbs5p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6179/Reviewer_YFSy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6179/Reviewer_YFSy"
        ],
        "content": {
            "summary": {
                "value": "The authors present a Transformer-inspired architecture to learn representations of biosignal time-series in unimodal and multimodal settings with a masked autoencoding pretraining task. The proposed Frequency-aware block performs token mixing in the frequency domain while the masked autoencoding task in the learned latent space is meant to preserve the structure of the learned frequency-based representation during pretraining. The model is pretrained on the SleepEDF dataset (on EEG and/or EMG and EOG channels) and then finetuned on different downstream tasks that contain different biosignal modalities (e.g. EMG, EOG, electromechanical measurements). Evaluation in different settings, e.g. with unimodal pretraining, multimodal pretraining, and with modality substitution or dropout, shows the proposed approach outperforms existing baselines and is robust to changing or missing data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality: The combination of a new frequency-based Transformer-like block along with a masked autoencoding pretraining task that enables multimodality processing appears novel.\n\nQuality: The manuscript is overall of good quality. The proposed methodology is well-motivated and evaluated in relevant settings.\n\nClarity: The text and results are mostly clearly presented, however some core concepts would benefit from a clearer presentation (see Weaknesses).\n\nSignificance: The presented results which suggest that pretraining on EEG allows a substantial improvement on downstream tasks on different modalities is impressive (in some cases with significantly higher results than reported baselines). If the reported results hold in different multimodal/transfer settings (see Weaknesses) the proposed approach might be very useful on various multimodal tasks."
            },
            "weaknesses": {
                "value": "- The evaluation seems a bit limited given the claims in the abstract. Since the models are only pretrained on sleep data, I wonder if the downstream performance might reflect the nature of sleep data (or of the specific SleepEDF dataset) rather than the transferability of the representations. Reporting results on more diversified pretraining datasets (e.g. pretraining on epilepsy data to later test on sleep data) would ensure the proposed approach generalizes to additional settings.\n\n- Although the text is written clearly, some parts of the pipeline would benefit from additional explanations on the initial MLP (see Q1 below) and the architecture of the second encoder (Q3)."
            },
            "questions": {
                "value": "1. What does the initial MLP described in Section 4.1 end up learning to do in practice (intuitively and/or actually)? My understanding is that it is mixing temporal information within each patch $s_i$, which would already scramble (some of) the temporal information in higher frequencies that could be relevant for the task and common across modalities. Second, are the MLPs shared between modalities? If the sampling rates are not the same then I assume different MLPs would be required unless patches capture different time lengths depending on the modalities. On this topic, what are $N$ and $P$ in practice for the different modalities/datasets?\n\n2. What is the architecture of the \u201csecond encoder\u201d as analyzed in Table 3? To confirm, is this the same thing as the \u201clightweight\u201d encoder of Section 4.2, first paragraph? Its exact role is unclear to me.\n\n3. Have the authors tried a linear probing evaluation instead of full finetuning as in Table 1? This would provide a clearer evaluation of whether the model truly learned a \u201cgeneric\u201d representation of biosignal time series during pretraining.\n\n4. How many parameters does the final encoder model(s) contain?\n\n5. Did the authors evaluate the impact of using normalization (e.g. layer norm) and/or positional/temporal encoding as is commonly done with Transformers, or does the frequency-aware module not require such components?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6179/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6179/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6179/Reviewer_YFSy"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6179/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796838928,
        "cdate": 1698796838928,
        "tmdate": 1700564723764,
        "mdate": 1700564723764,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "osoeVimV2u",
        "forum": "t5LXyWbs5p",
        "replyto": "t5LXyWbs5p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6179/Reviewer_GTFm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6179/Reviewer_GTFm"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the pre-training on bio-signals (1D time series neurological signals) based on Masked Modeling strategies like MAE. To effectively pre-training with the bio-signals with potential distributional shifts, the authors design a frequency-aware masked auto-encoder, dubbed bioFAME, to learn bio-signal representations in the Fourier domain. Specifically, a frequency-aware transformer is designed, which leverages a fixed-size Fourier-based operator for token mixing. Meanwhile, the authors propose a frequency-maintain pre-training strategy that performs masking and reconstruction in the latent space to sustain the frequency components. Conducting pre-training and fine-tuning experiments, results on five bio-signal datasets show performance gains of the uni-modality and multi-modality versions of bioFAME."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(**S1**) This paper proposes a frequency-aware Transformer architecture and a latent-space masked auto-encoder pre-training strategy for bio-signals. Analysis and evaluation of the pre-training strategy are extensive. Experiment results demonstrate the effectiveness of both the proposed methods with performance gains and transferring abilities.\n\n(**S2**) The overall writing is easy to follow, and the presentation of texts, tables, and figures is well-arranged and informative."
            },
            "weaknesses": {
                "value": "(**W1**) Lack of novelty and discussion with existing methods. The proposed method borrows the idea of MAE with the frequency modeling and reconstruction of neurological signals. The proposed frequency-aware transformer seems to borrow a similar design from FNO [1] and Ge$^2$AE relevant works [2, 3, 4] in computer vision. More importantly, the idea of this paper is quite similar to neuro2vec [5] proposed in 2022, which designs a masked modeling pre-training framework for neurological signals in both the spatiotemporal and Fourier domains and conducts experiments on open-source neurological datasets. The authors should discuss the relationship between these existing works and the proposed bioFAME, and point out the necessity of designing the frequency-aware modeling framework for bio-signal representations.\n\n(**W2**) The proposed frequency-aware Transformer encoder lacks analysis. Since the architecture also brings significant performance gains compared to previous networks, the details of network design should be discussed soundly, e.g., the layer number, the embedding dimension, and the utility of the multi-head frequency filter layer.\n\n(**W3**) Concerns about performance gains and experimental settings. Firstly, Compared to prior arts, the improvement of bioFAME is marginal. Also, since not all arts utilize multimodal training, it is only fair to compare bioFAME unimodal with other approaches. The experimental setting of unimodal vs multimodal is very confusing. The authors state that ExpEMG is a dataset of single-channel EMG recordings, how is the multimodal approach applied in this case? This same problem also applies to other datasets.\nSecondly, bio-signal datasets are usually of small size, which leads to the problem of high variance over different random seeds. How did the authors manage to solve this? The authors claim to follow the setup of Zhang et al. [6], yet they [6] reported mean and variance. Why did the authors fail to do so? \nThirdly, since there are contrastive learning baselines (e.g., TS-TCC), do the authors conduct linear evaluations of the learned representations (e.g., linear probing)? Meanwhile, it is better to provide more empirical analysis of the learned embedding, e.g., embedding visualization by tSNE, and reconstruction result visualizations.\n\n(**W4**) The connection between the motivation and approach is vague. The authors claim substantial distributional shifts between the pretraining and inference datasets. How can FREQUENCY-AWARE MASKED AUTOENCODERS solve this? What is expected to be learned from FREQUENCY-AWARE MASKED AUTOENCODERS, and how is this linked to the architecture design of the model?\n\n(**W5**) Details of pre-training and fine-tuning settings on each dataset of bioFAME are vague. Hyper-parameter settings and sensitivity analysis are not available in the main text and appendix.\n\n### Reference\n[1] Li et al. Fourier neural operator for parametric partial differential equations. In ICLR, 2021.\n\n[2] Liu et al. The Devil is in the Frequency: Geminated Gestalt Autoencoder for Self-Supervised Visual Pre-Training. In AAAI, 2023.\n\n[3] Li et al. Architecture-Agnostic Masked Image Modeling - From ViT back to CNN. In ICML, 2023.\n\n[4] Xie et al. Masked Frequency Modeling for Self-Supervised Visual Pre-Training. In ICLR, 2023.\n\n[5] Wu et al. neuro2vec: Masked Fourier Spectrum Prediction for Neurophysiological Representation Learning. In arXiv, 2022.\n\n[6] Zhang et al. Self-supervised contrastive pre-training for time series via time-frequency consistency. In NeurIPS, 2022."
            },
            "questions": {
                "value": "(**Q1**) Why did the authors call bioFAME multimodel pre-training? Training on multiple channels on the same modality should not be called multimodal.\n\n(**Q2**) Did the authors provide details of pre-training and fine-tuning settings on each dataset of bioFAME? Hyper-parameter settings and sensitivity analysis are not available in the main text and appendix.\n\n================== Post-rebuttal Feedback ==================\n\nThanks for the detailed rebuttal feedback. However, my main concerns about novelty were not well solved, and I decided to maintain my rating. I apologize again for my late review and reply!"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6179/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6179/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6179/Reviewer_GTFm"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6179/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699266806457,
        "cdate": 1699266806457,
        "tmdate": 1700819375940,
        "mdate": 1700819375940,
        "license": "CC BY 4.0",
        "version": 2
    }
]