[
    {
        "id": "C4g9zxTNlK",
        "forum": "4qFIkOhq24",
        "replyto": "4qFIkOhq24",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3286/Reviewer_Cy5r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3286/Reviewer_Cy5r"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a theoretical framework, namely, Behaviour Expectation Bounds (BEB), to study the extent to which a language model can be misaligned by adversarial prompting. On a high level, this paper introduces several impossibility results for model alignment under certain assumptions of a mixture model, model distribution, and expected scoring. The derived results show that if a negative response has some probability of being outputted by the model, then there exists a prompt that can elicit this response, and the probability of sampling this response increases with the prompt length. The authors compute via experiments critical constants used in the assumptions, and show that the LLM converges to negative behavior when prompted with longer and longer prompts sampled from the negative component of the mixture."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper introduces and examines a framework for the theoretical study of LLM alignment. While acknowledging potential limitations within the framework and its underlying assumptions, it presents an original perspective for the theoretical analysis of a complex empirical phenomenon. \n\nThe writing of this paper is clear and easy to follow, with most definitions and assumptions followed by high-level intuition."
            },
            "weaknesses": {
                "value": "My main comments are focused on three topics:\n\nThe mixture model seems to be a very strong assumption on what the models entail after pretraining. Details are discussed in Questions.\n\nAlthough empirical values for problem parameters are provided in the experiments, it is still hard to comprehend each assumption and their overall importance to the derived results. Details are discussed in Questions.\n\nSome experiment details are lacking. See below."
            },
            "questions": {
                "value": "My main comments are focused on three topics:\n\nThe mixture model seems to be a very strong assumption on what the models entail after pretraining. In particular, the mixture coefficient is uniform across contexts, which seems unlikely in practice -- for certain prompts, say, adversarial ones that aims to misalign a model, $\\alpha$ should probably be higher as the model is more likely to output negative responses in this case. It would be useful if the authors could give a more robust account of why such a simple mixture is reasonable. \n\nThe overall theoretical framework is laid out clearly, particularly the high-level intuitive explanations that precede each definition. It is also clear that the analysis depends on multiple definitions and corresponding assumptions made in the problem formulation, including the mixture model and properties of the mixture distributions and behaviour scoring function. The paper could be improved by some discussion of the necessities and importance of these assumptions. Specifically, what is critical to this formulation, and what is required for technical purposes in proofs? The paper takes a simplified high-level view of language models, regarding them as outputting one single sentence given a stream of sentences, each of which is generated by one role in a pairwise conversation. This does not correspond exactly to how these models actually behave. For example, the LLM typically outputs one or multiple paragraphs instead of a single sentence. Why  is such a sentence-level view adopted for this framework? Is this the right choice given the mismatch with actual token sampling processes? How are sentences defined? Ending with \"\\n\" or EOS token?\n\nThe experiments are helpful for giving insights into what the assumptions entail. However, the construction of the mixture LLM is not very clearly described. In Section 4.2, it is remarked that \"the negative behavior LLM denoted by $\\mathbb{P}_{-}$ is not the true sub-component of the RLHF fine-tuned LLM\". Could the authors possibly construct an exact mixture LLM explicitly using extracted sub-components? The procedure of prompting is also omitted from the discussion of experiments. It would be helpful if some expositions are provided. \n\nOther comments:\n\n- typo in section 3.1: \"theirs priors\" --> \"their priors\"\n\n- The claim about RLHF in the last part is interesting, but far too vague with the current status of the paper. The authors should consider omitting the discussion entirely or provide more evidence on this aspect."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3286/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3286/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3286/Reviewer_Cy5r"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3286/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698023130323,
        "cdate": 1698023130323,
        "tmdate": 1700638334753,
        "mdate": 1700638334753,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jT3gYLYaFc",
        "forum": "4qFIkOhq24",
        "replyto": "4qFIkOhq24",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3286/Reviewer_edXZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3286/Reviewer_edXZ"
        ],
        "content": {
            "summary": {
                "value": "The paper offers a theoretical analysis of the conditions under which undesirable behaviours that are unconditionally unlikely can become conditionally high probability. This analysis is then used to argue that even if an alignment process is applied to an LLM, as long as an undesirable behaviour remains with whatever small probability, and adversarial prefix can elicit it by making its conditional probability much higher."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper attempts to offer a much needed theoretical base to the problem of aligning of LLMs.\n2. The paper has a solid theoretical analysis that shows that under certain conditions, adversarial prompting can result in very low probability behaviours being exhibited with high probability.\n3. The authors study these behaviours also in real-world models and show that adversarial prefixing can indeed be used to misalign a model."
            },
            "weaknesses": {
                "value": "1. The definition of \u03b3-prompt-misalignment is extremely conservative: The existence of a single prompt resulting in misaligned behaviour is sufficient to label the whole model misaligned. This makes this is a binary condition and it is not that surprising that there exists at least one prompt that will result in an undesirable behaviour. However, this is not a realistic setting and in practice more nuanced measures of \u201cmisalignment\u201d are needed.\n2. The definition for \u03b2-distinguishability is very strict and, contrary to the claims in the paper, it is not clear to me whether $\\mathbb P_{-}$ and $\\mathbb P_{+}$ would be at all distinguishable in practice. That is, because the definition requires that bound (5) holds *for any prefix* $s_0$. However, while the components can be polar opposite in one sense, e.g. \u201cagreeableness\u201d, the models are likely similar in many other ways. E.g., \u201cWhich is the capital of France\u201d is probably going to be completed with \u201cParis\u201d, by both $\\mathbb P_{-}$ and $\\mathbb P_{+}$. If that\u2019s the case, then $\\beta=0$ and that\u2019d invalidate the paper\u2019s results.\n3. The same issue seems to appear in the experimental estimation of $\\beta$. It seem that the authors are not actually estimating $\\beta$. The KL divergence is computed only for prefixes sampled from the unconditional negative distribution $\\mathbb P_{-}$ which of course has a bias. This results in over-approximating $\\beta$, possibly by a lot. However, if one considers all sentences $s_0$, there would be many for which the completion would be the same (e.g. the Paris example), hence $\\beta$ would be 0.\n4. Overall, Section 2.2 which is critical for understanding the claims of the paper is not clearly presented. I would strongly recommend the authors to add examples of, e.g. \u03b2-distinguishable and non-\u03b2-distinguishable distributions, as well as \u03b1,\u03b2,\u03b3-negatively-distinguishable and non-\u03b1,\u03b2,\u03b3-negatively-distinguishable factorizations.\n5. The paper also fails to discuss the limitations of the analysis and the conditions under which it holds. While the plausibility of the factorisation of the distribution is mentioned, I am missing the discussion on the other technical assumptions, as mentioned above."
            },
            "questions": {
                "value": "1. In the Introduction, you say: \u201cPreset aligning prompts can only provide a finite guardrail against adversarial prompts\u201d. What does it mean for a guardrail to be \u201cfinite\u201d in this context?\n2. It feels like Theorem 1 should also have a \u03b4 somewhere, especially if this is a PAC-based result\u2026\n3. My understanding is that the paper deals with the probability of a \u201cmisaligned\u201d sentence as measured by the model. However, real world models do not simply sample from their posterior, or take the highest likelihood output. Usually, greedy decoding is used. Would that affect the results?\n4. In the Discussion, the authors say that they \u201cshowed that the better aligned the model is to begin with, the longer the prompt required to reverse the alignment\u201d. Which result is this referring to?\n\n\nTypos:\n- Pg. 9: \u201cAndreas (2022) describe\u201d -> \u201cAndreas (2022) describes\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3286/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3286/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3286/Reviewer_edXZ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3286/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698585676354,
        "cdate": 1698585676354,
        "tmdate": 1699636277345,
        "mdate": 1699636277345,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sgJMkMj5LQ",
        "forum": "4qFIkOhq24",
        "replyto": "4qFIkOhq24",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3286/Reviewer_5KAT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3286/Reviewer_5KAT"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the Behavior Expectation Bounds (BEB) theoretical framework to understand and analyze alignment issues in large language models (LLMs). The authors demonstrate that the alignment of an LLM can be reversed through adversarial prompts, with the extent of misalignment influenced by the initial alignment of the model and the distinguishability of undesired behaviors. Empirical results validate the theoretical claims. The findings hint that reinforcement learning from human feedback (RLHF), a prominent alignment practice, may increase the risk of undesired behaviors becoming more prominent in language models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality: The Behavior Expectation Bounds (BEB) framework offers a novel theoretical perspective on the alignment issues of LLMs.\nQuality: The paper effectively combines theoretical insights with empirical results to support its claims. The formalisms and theorems provide a solid foundation for the study.\nClarity: The paper is well-structured and the distinction between theoretical and empirical sections ensures the reader can follow the progression of ideas.\nSignificance: The problem of LLM alignment is pressing, and the paper's findings can influence future practices and methodologies in training and deploying these models."
            },
            "weaknesses": {
                "value": "Assumption Limitations: The framework is based on some strong assumptions, such as the decomposition of LLMs into distinct behavioral components. This could be overly simplified or not universally applicable.\nOveremphasis on Theoretical Aspects: While the theorems and formalizations are valuable, the balance between theoretical and practical aspects could be adjusted to appeal to a broader audience."
            },
            "questions": {
                "value": "How generalizable is the BEB framework across various LLM architectures?\nGiven the paper\u2019s claim about the potential reversibility of alignment through adversarial prompts, what preventive measures do the authors recommend?\nThe decomposition of LLMs into well-behaved and ill-behaved components is a key assumption. How does this align with real-world observations of LLMs' behaviors which might be more nuanced?\nWould the authors consider extending the framework to consider multi-modal models or those beyond text-based interactions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3286/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698628911527,
        "cdate": 1698628911527,
        "tmdate": 1699636277220,
        "mdate": 1699636277220,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "m3GEG92kff",
        "forum": "4qFIkOhq24",
        "replyto": "4qFIkOhq24",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3286/Reviewer_e9bS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3286/Reviewer_e9bS"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Behavior Expectation Bounds, a framework for studying alignment of LLMs. Given a distribution $P(s)$ over possible sequences $s$ that are generated from an LLM, and a scoring function B, the idea is to decompose the distribution into two components, $P_+, P_-$, where $B(P_-) \\leq \\gamma < 0$ . The main contribution is an existence proof showing that $P_-$ has any support under the original distribution, it is possible to provide an adversarial prompt such that the scoring function is arbitrarily high. Furthermore, the adversarial prompt length scales logarithmically in the inverse weighting of $P_-$, so e.g. making bad behaviour a million times less likely under the initial prompt only increases the length of the adversarial prompt by a modest additive factor. Additional results are presented for alignment in the presence of an aligning prompt, and a turn-based conversational setup. Experiments suggest that the assumptions for the theory do indeed hold in practice with modern LLMs."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The theory is presented clearly: the assumptions are presented well and the theorems explained nicely.\n+ The potential impact of the work is quite large: this work presents fundamental limits on the ability of models to be correctly aligned. If current trends continue and large models continue to increase in capability, this points towards important implications of an inability to avoid potentially very hazardous misalignment.\n+ Experimental results go some way towards backing up the theoretical claims.\n+ The analysis of the conversational and aligning prompt cases are interesting, and appropriate given the focus on conversational agents in the previous year. The result that conversations can require longer adversarial input is counter-intuitive at first, but makes sense upon reading the proof and analysis."
            },
            "weaknesses": {
                "value": "+ The fact that all the results are asymptotic seems to be a limitation to the results. Of course, developing finite-sample bounds is likely much harder than asymptotic results. In principle, the results could be vacuous if the constants were large enough. Given recent work on finding adversarial prompt injections, I don't think the results are actually vacuous, but I think a brief discussion of this is warranted in the paper.\n+ The relevance of the experimental results is debatable, as investigating the fine-tuned models is not the same as investigating the different modes $P_-$, $P_+$. I understand that direct examination of the modes is perhaps impossible, but I would like to see more discussion of the feasibility of this.\n+ There is no discussion about the computational feasibility of finding adversarial prompts. In light of the combinatorially large search space of all possible contexts of length $n$ of size $V^n$ for vocab size $n$, the main result is less impressive unless it is computationally tractable to find these adversarial injections. Again, I think a discussion of recent injection techniques should address this concern in the paper."
            },
            "questions": {
                "value": "+ Do you foresee any pathways towards non-asymptotic results?\n+ Is there any way to directly investigate the modes $P_-$, $P_+$ instead of looking at the proxy fine-tuned models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3286/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819312990,
        "cdate": 1698819312990,
        "tmdate": 1699636277089,
        "mdate": 1699636277089,
        "license": "CC BY 4.0",
        "version": 2
    }
]