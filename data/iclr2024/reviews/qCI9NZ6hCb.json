[
    {
        "id": "24llq8l9c0",
        "forum": "qCI9NZ6hCb",
        "replyto": "qCI9NZ6hCb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4846/Reviewer_9qrP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4846/Reviewer_9qrP"
        ],
        "content": {
            "summary": {
                "value": "This papers aims to construct a better global model compared to that of prior works, by adopting (1) a local balancer and (2) an aggregation balancer. The local balancer reduces the client drift by balancing the logit for each class in the client. The aggregation balancer uses the cosine similarity between the global model before local update and the updated model of each client, to measure the importance score of each client. Experimental results how that the proposed scheme performs better than FedAvg, FedProx, FedNova, Scaffold in various federated learning scenarios."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is in general easy to follow.\n\n2. The proposed scheme performs better than various baselines."
            },
            "weaknesses": {
                "value": "1. First of all, I feel that the technical novelty of this paper is somewhat lacking. For the local balancer part, FedBal is actually simply adopting existing work from the long-tail / class-imbalanced learning literature. I'm also not very clear with the technical novelty of the aggregation balancer compared with existing works on server-side applied methods.\n\n2. Moreover, the authors are comparing their scheme with only client-side applied methods, even though the authors are proposing both client-side and server-side methods. What is the advantage of FedBal compared with server-side applied methods? Can existing combinations of client-side and server-side methods perform better than FedBal? I believe the authors should address these questions.\n\n3. The authors are considering two datasets, which I believe is not sufficient.\n\n4. Finally, there are several places that makes me feel that the paper is not well-polished or self-contained.\n- In section 5.1, the same paragraph is repeating twice.\n- In section 5.1, the authors mention that they only focus on cross-silo FL, but also say that 4 out of 20 clients are participating in each round. Maybe the authors are considering cross-device FL?\n- How is the outlier balancer $\\beta$ determined in the scheme?\n- In section 4.1, the authors do not provide details on how $z_j^b$ is actually differs from $z_j$."
            },
            "questions": {
                "value": "See the weakness above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4846/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4846/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4846/Reviewer_9qrP"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698455379681,
        "cdate": 1698455379681,
        "tmdate": 1699636468181,
        "mdate": 1699636468181,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MLPpsdKVTB",
        "forum": "qCI9NZ6hCb",
        "replyto": "qCI9NZ6hCb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4846/Reviewer_zEAj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4846/Reviewer_zEAj"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to address the data heterogeneity problem in federated learning. The problem is identified as a result of the biased global model. To mitigate these biases, local and aggregation balancers for FL (FedBal) are developed towards specific classes and specific clients, respectively. Experiments have shown the superiority of the proposed balancers when they are combined with existing methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It is good to see a related work summary from the client side and the server side, respectively. This brings about a novel viewpoint to existing FL methods.\n\n2. The proposed aggregation balancer has improved the performance of various FL methods, which demonstrates its effectiveness for different optimization schemes. \n\n3. Experimental settings and training specifications are given in detail."
            },
            "weaknesses": {
                "value": "1. The core motivation and idea of the local balancer inherit from the method in (Li et al., 2022a). The authors have also mentioned that the loss in Eq. (4) is from (Li et al., 2022a). I think these have limited the novelty and contribution of the proposed method. There is a lack of an explanation of the unique contribution and novelty of the balancer in this paper and how it differs from that in (Li et al., 2022a).  \n\n2. Although the proposed method addresses the data heterogeneity problem, it seems that only label-level shift heterogeneity is taken into consideration. As for the feature-level shift heterogeneity, e.g., domain shift, the proposed method seems unable to address it well. \n\n3. From my perspective, more knowledge always leads to a wiser decision, but more data does not lead to more knowledge, considering the data properties can vary a lot in different cases. I think the concepts of knowledge, data, and wisdom can be further clarified."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698577559370,
        "cdate": 1698577559370,
        "tmdate": 1699636468102,
        "mdate": 1699636468102,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9Kd8u0QD8u",
        "forum": "qCI9NZ6hCb",
        "replyto": "qCI9NZ6hCb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4846/Reviewer_k5UQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4846/Reviewer_k5UQ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel FL algorithm to mitigate the bias in model aggregation by assigning higher weights to the marginalized local models. Experimental results show the proposed method can improve the test performance of non-IID FL and combine with the existing methods."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper attempts to solve an interesting problem, aggregating local models in a more effective way, instead of naively averaging."
            },
            "weaknesses": {
                "value": "1. The paper isn't well written and well-organized. For example, there are two colonial paragraphs in the section 5.1, begining from 'Our experiments primarily used a 3-layered ConvNet...'.  And the overall training procedure is not illustrated in the paper and there is no a graph to show the workflow.\n\n2. There is no theoretical guarantee in this paper. The proposed method FedBal is tottally empirical.  However, the method  can not outperform all other baselines in the experiments. For example, in table 5, FedBal shows better results than the existing methods in all cases \nexcept $\\alpha = 0.1$. It is very strange that FedBal works with higher heterogeneity $\\alpha = 0.05$ and lower  heterogeneity $\\alpha = 0.5, 1$ but does not work with $\\alpha = 0.1$.\n\n3. The experiments are not comprehensive. Three benchmark datasets are sufficient for validating the effect of method. And there are some related prior studies not in the baselines. For example, [1][2], using bayesian theorem to aggregate the local models.\n\n4. The number of clients is too small. only 4 clients paricipating in the training each round. The FL system in the experiments is too small.\n\n\nReference:\n\n[1] Yurochkin M, Agarwal M, Ghosh S, et al. Bayesian nonparametric federated learning of neural networks[C]//International conference on machine learning. PMLR, 2019: 7252-7261.\n\n[2] Wang H, Yurochkin M, Sun Y, et al. Federated learning with matched averaging[J]. arXiv preprint arXiv:2002.06440, 2020."
            },
            "questions": {
                "value": "1. Do all clients have the same number of data? The number of data decides the weight assigned to the local model $p_{i}$. What if a client has larger local dataset but with imbalanced data. Does this client have 'extensive knowledge' or marginalized model? \n\n2. How do you aggregate the models in the server? The paper just shows the objective function of FedBal in equation (9)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4846/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4846/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4846/Reviewer_k5UQ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698594977843,
        "cdate": 1698594977843,
        "tmdate": 1699636468013,
        "mdate": 1699636468013,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LmHlGYSVAg",
        "forum": "qCI9NZ6hCb",
        "replyto": "qCI9NZ6hCb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4846/Reviewer_6uTg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4846/Reviewer_6uTg"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to tackle data heterogeneity in Federated Learning (FL) by introducing Federated Learning Balancer (FedBal). This addresses the bias introduced when servers average a client's model based on data volume or sharing degree. FedBal incorporates two components: the local balancer, regulating preferences in local models, and the aggregation balancer, which calculates cosine similarity to adjust aggregation weights for the global model. It shows an enhancement of 2% to 7% in marginalized clients' models compared to those with abundant data. The experiments demonstrate FedBal can improve global model accuracy by 3% to 4% compared to alternative methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Originality: Empirical evidence indicates that focusing on marginalized clients during aggregation can prevent model overfitting, suggesting a new perspective. The method's simplicity aids reproducibility and further research.\n\n2. Significance: Balancing mechanisms on both client and server sides show potential in enhancing model accuracy, proposing a universal solution to data heterogeneity in Federated Learning.\n\n3. Clarity and Replicability: The paper's structure and method simplicity contribute significantly to its clarity and replicability."
            },
            "weaknesses": {
                "value": "1. Editorial Imperfections: Repetitive statements and editorial issues affect the paper's readability and require careful revision.\n\n2. Limited Novelty: The novelty of the local balancer and aggregation equalizer is constrained. They draw heavily from prior research, limiting their originality.\n\n3. Inconclusive Experiments: The experiment lacks comprehensive contemporary baseline comparisons, limiting the persuasiveness of its findings."
            },
            "questions": {
                "value": "1. Figure 1 mentions that in the case of a uniform segmentation of labels, labels that are shared more among customers (0, 1, 6, 7, etc.) usually show a higher accuracy rate. However, some labels, notably label 6, experience a decrease in accuracy, while label 7 maintains a consistent accuracy. Moreover, certain labels (2, 3, 4, 8, 9) also display decreased accuracy, with only labels 0 and 1 showing improvements. These results might not unequivocally support the assertion that FedAvg yields subpar performance.\n\n2. Table 1 presents accuracy data where the Without exp (Original) configuration yields higher accuracy. The interpretation of these results is somewhat perplexing. Does this imply that the absence of the aggregation balancer results in superior performance?\n3.\tAnalyzing Table 2, it is observed that, on the CIFAR10 dataset, as data heterogeneity increases, the local balancer's performance falls behind the baseline. Could the reasons for this performance differential be elucidated?\n4.\tIn Table 4, a notable deterioration in performance is observed when \u03b1 is 0.1 and the aggregation balancer is employed. Could you provide an explanation for the specific circumstances in which a decrease in performance occurs at certain levels of data heterogeneity?\n5.\tCould a dedicated ablation study, focusing solely on the use of the aggregation balancer, be provided to validate its efficacy?\n6.\tThe proposed method appears to lack consistency. In some instances, the local balancer may underperform compared to the baseline in specific settings, while the aggregation balancer can also degrade performance in certain scenarios. Paradoxically, the combined use of both balancers in certain settings results in a marked improvement in accuracy, like in CIFAR10 and \u03b1 is 0.05. Could the underlying reasons for these variations in performance be explained?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698985438501,
        "cdate": 1698985438501,
        "tmdate": 1699636467936,
        "mdate": 1699636467936,
        "license": "CC BY 4.0",
        "version": 2
    }
]