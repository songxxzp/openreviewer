[
    {
        "id": "k7HsbC0o8h",
        "forum": "A8xmyDIZhn",
        "replyto": "A8xmyDIZhn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7850/Reviewer_J3Ha"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7850/Reviewer_J3Ha"
        ],
        "content": {
            "summary": {
                "value": "This paper investigate an interesting problem of federated compositional optimization and propose a novel CO setting, which is more general compared to previous studies. Furthermore, the authors also present the FedDRO algorithm with provable theoretical guarantees. Empirical studies are also conducted."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is well-originazed and clearly written.\n\n2. The paper investigates an important problem and introduce a general federated compositional optimization setting.\n\n3. Experimental results are provided to verify the performance of the proposed algorithms"
            },
            "weaknesses": {
                "value": "1. The authors have introduced a general setting for DRO problems, including DRO with KL-Divergence and $\\chi^2$-Divergence. Are there other DRO optimization problems that fit this setting?\n\n2. The exact form of the optimization problem in two experiments should be specified.\n\n3. How is the dataset distributed across each client? Does it reflect the diversity in data distribution?\n\n4. Utilizing the small batch size is one strength of FedDRO. However, this isn't evident in the experiments. Specifically, |b|=16 in FedDRO, while |b|=4 (or 8) in GCIVR (with the client count m=8). Therefore, experiments should be conducted with small batch size or more clients (ensuring large batch size in GCIVR).\n\n5. The experiments are repeated only five times, and it would further strengthen the reliability if reruning the experiments at least ten times. Furthermore, presenting the loss curve would also add to this reliability."
            },
            "questions": {
                "value": "See questions in the 'Weakness'."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7850/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7850/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7850/Reviewer_J3Ha"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7850/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698504435411,
        "cdate": 1698504435411,
        "tmdate": 1699636962425,
        "mdate": 1699636962425,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FLgWdzHijH",
        "forum": "A8xmyDIZhn",
        "replyto": "A8xmyDIZhn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7850/Reviewer_Sjhd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7850/Reviewer_Sjhd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new compositional optimization algorithm called FedDRO for distributionally robust learning. The proposed algorithm integrates novel communication and optimization techniques to tackle biased gradient estimates, client drift, etc. in compositional optimization. Convergence analysis and numerical experiments are conducted to validate the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed approach improves upon the previous compositional optimization methods in terms of convergence rate and the need for large batch gradients. It's also technically novel to utilize the low-dimensional communication of the compositional functions $g(\\cdot)$ to trade for better convergence."
            },
            "weaknesses": {
                "value": "The low-dimensional $g(\\cdot)$ still causes additional $O(\\epsilon^{-2})$ communication, whose impact may vary based on specific forms of $g(\\cdot)$ and the synchronization settings."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7850/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7850/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7850/Reviewer_Sjhd"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7850/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698609876574,
        "cdate": 1698609876574,
        "tmdate": 1699636962268,
        "mdate": 1699636962268,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rzqL8zLMp2",
        "forum": "A8xmyDIZhn",
        "replyto": "A8xmyDIZhn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7850/Reviewer_StaW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7850/Reviewer_StaW"
        ],
        "content": {
            "summary": {
                "value": "This paper developed a federated learning method for compositional optimization problem, which can be applied to DRO problems. Both theoretical and empirical results are provided to show its performance. However, this paper overclaims its contributions. It solves a simple compositional problem so it is not surprising to see a better bound. But the authors hide this point. This could mislead the community."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper provides a good literature review. \n\nTheoretical analysis is provided."
            },
            "weaknesses": {
                "value": "1. This paper overclaims its contributions.\n\n2. The writing is poor. It could mislead the community.\n\n3. Some operations are not practical.\n\n4. Some claims are NOT correct."
            },
            "questions": {
                "value": "1. Different from existing federated compositional optimization, this paper investigated a simple case, i.e., the outer-level function is deterministic, not a stochastic function as the first three baselines in table 1. However, the authors never mentioned this critical difference. For this simple objective function, the authors claim they can achieve better convergence rates. This is not the case because the problem settings are different. This paper overclaims its contribution. The authors should clearly state the difference in the problem setting. Otherwise, it will mislead the community. \n\n\n2. The novelty is incremental. For this simple compositional optimization problem, it is trivial to extend existing theoretical analysis to this kind of problem. I didn't see any challenges here. In particular, the outer-level function is deterministic, and $y$ is synchronized in each iteration. Then, compared with the non-compositional optimization problem, there are no additional challenges in convergence analysis. \n\n3. For Eq.(8), what is the reason for using the storm estimator? The standard moving average estimator should also work. Could you please provide more discussions?\n\n4. This paper claims it can achieve better communication complexity than existing works. It is NOT true. Specifically,  the proposed algorithm communicates $y$ in every iteration. Then, the communication complexity is the same as the number of iterations. It is much worse than existing approaches.\n\n5. No experiments to compare the proposed algorithm with the federated baselines."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7850/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818905275,
        "cdate": 1698818905275,
        "tmdate": 1699636962169,
        "mdate": 1699636962169,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XfzdzF2fL0",
        "forum": "A8xmyDIZhn",
        "replyto": "A8xmyDIZhn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7850/Reviewer_EdtR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7850/Reviewer_EdtR"
        ],
        "content": {
            "summary": {
                "value": "This paper develops a momentum-type federated learning algorithm for Compositional Optimization (CO) problems. The authors provide an $O(\\epsilon^{-2})$ sample complexity and $O(\\epsilon^{-3/2})$ communication complexity for their approach. Numerical experiments on large-scale Distributionally Robust Optimization problems demonstrate the effectiveness of their method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper addresses an important problem in the field.\n\n2. The paper is well-written, with clear mathematical notations. The table comparing related works provides insightful information on their contributions. The discussions around each assumption are useful. \n\n3. The discussion surrounding the theorems is useful for understanding their results."
            },
            "weaknesses": {
                "value": "Please see below."
            },
            "questions": {
                "value": "**Abstract:**\n\n- The following expression has already been discussed in existing literature. See Lemma 2.1 of [R1] and Section 1.2 of [R2]. What are your findings in contrast to these?\n\n    \u201cWe first establish that vanilla FedAvg is not suitable for solving distributed CO problems due to data heterogeneity in the compositional objectives at each client. This leads to the amplification of bias in the local compositional gradient estimates.\u201d\n\n- The following sample and communication complexity for CO problems have already been established. See [R3] and [R4] for details on sample and communication complexity, and [R4] for linear speedup under the heterogeneity assumption (Assumption 3.4 in your paper). Please clarify the differences.\n\n    \u201cWe establish an $O(\\epsilon^{-2})$ sample and $O(\\epsilon^{-3/2})$ communication complexity in the FL setting while achieving linear speedup with the number of clients.\u201d\n\n**Section 4 (Algorithm Design):**\n\nAlgorithm 1 is similar to the method proposed in [R3] and [R4]. Specifically, the improved communication complexity and sample complexity are obtained from Eq (7) and the momentum update in Eq (8), which are already studied in [R3] and [R4].\n\n**Experiment:**\n\n- The main focus of the paper is federated learning under the heterogeneity assumption. However, this setting is not apparent in the experiment evaluation.\n\n\n- It would be useful if the authors provided a comparison to variance reduction and momentum-based methods designed for heterogeneous federated composition problems [R1-R4].\n\n- Why is there a jump in test accuracy in Figure 1 after 100 communications?\n\n\n\nPlease let me know in your response if I misunderstood your contribution, and I will be happy to update my score.\n\n\n\n\n[R1] Tarzanagh, D.A., Li, M., Thrampoulidis, C. and Oymak, S., 2022, June. Fednest: Federated bilevel, minimax, and compositional optimization. In International Conference on Machine Learning (pp. 21146-21179). PMLR.\n\n[R2] Yang, S., Zhang, X. and Wang, M., 2022. Decentralized gossip-based stochastic bilevel optimization over communication networks. Advances in Neural Information Processing Systems, 35, pp.238-252.\n\n\n[R3] Feihu Huang. Faster adaptive momentum-based federated methods for distributed composition optimization. arXiv preprint arXiv:2211.01883, 2022\n\n[R4] Tarzanagh, D.A., Li, M., Sharma, P. and Oymak, S., 2023. Federated Multi-Sequence Stochastic Approximation with Local Hypergradient Estimation. arXiv preprint arXiv:2306.01648."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7850/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7850/Reviewer_EdtR",
                    "ICLR.cc/2024/Conference/Submission7850/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7850/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699638835243,
        "cdate": 1699638835243,
        "tmdate": 1701038920499,
        "mdate": 1701038920499,
        "license": "CC BY 4.0",
        "version": 2
    }
]