[
    {
        "id": "8U3YG0gpgL",
        "forum": "WEoyWdsI9f",
        "replyto": "WEoyWdsI9f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5691/Reviewer_K1GZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5691/Reviewer_K1GZ"
        ],
        "content": {
            "summary": {
                "value": "This paper explores logit-based Federated Learning (FL) methods that aim to protect data privacy. It highlights a previously unnoticed privacy risk where a semi-honest server could potentially learn clients' private models from shared logits. The paper introduces an attack called Adaptive Model Stealing Attack (AdaMSA) and proposes a defense strategy to mitigate this risk. Experimental results confirm the effectiveness of the attack and defense strategy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ A novel Adaptive Model Stealing Attack (AdaMSA) is proposed to quantify the privacy risk of logit-based FL.\n+ A simple yet effective defense strategy is proposed to achieve better privacy-utility trade-off.\n+ A bounded analysis of privacy risks is provided for the proposed privacy attacks.\n+ Extensive case studies."
            },
            "weaknesses": {
                "value": "- The value of the research question requires further justification.\n - The outcomes of the experiment need to be made more convincing. \n- Limited in-depth comparison with state-of-the-art solutions."
            },
            "questions": {
                "value": "Q1: The motivation of this article requires further justification by providing additional evidence. The authors mentioned that logit-based FL was developed to achieve communication-efficient FL. However, this is not a mainstream FL framework nor a mainstream communication-efficient FL framework. For example, asynchronous FL, gradient compression-based FL, gradient quantization-based FL, and generative learning-based one-shot FL are all widely adopted. Therefore, the reviewer's first concern is whether it is necessary and valuable to analyze the privacy risks of logit-based FL.\n\nQ2: The objectives of the adversary's attack warrant further examination. While the paper articulates the adversary's aim as acquiring the private model \u03b8, it is imperative to delve deeper into whether this private model \u03b8 can be subsequently leveraged for malicious purposes. It is worth noting that previous works in the field of privacy attacks primarily focus on the exfiltration of a client's confidential data. Consequently, a critical concern arises: can the adversary utilize the private model \u03b8 to reverse-engineer the original training data? This crucial aspect of the adversary's capabilities necessitates thorough investigation and discussion to assess the potential privacy risks associated with the acquired private model.\n\nQ3: More advanced baselines need to be included to highlight the superiority of the proposed privacy attacks. Considering that FL was proposed in 2016 and the baseline scheme compared in this article was also proposed in 2016, whether this scheme is representative still needs to be discussed. It would be better if the authors could consider more baseline solutions (such as [1]).\n\n[1] Takahashi H, Liu J, Liu Y. Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 12198-12207.\n\nQ4: There is merit in exploring additional, perhaps more straightforward, security mechanisms to corroborate and strengthen the privacy assurances of logit-based Federated Learning (FL). It is essential to recognize that the fundamental premise underpinning the attacks in this article hinges on the server's ability to access the logits uploaded by the client. However, it is possible to mitigate this vulnerability through the implementation of secure aggregation techniques and the utilization of hardware-based Trusted Execution Environments (TEEs). These measures can effectively safeguard against the server's unauthorized access to logits. It is pertinent to acknowledge that these considerations do not diminish the innovative contributions of the article. Nevertheless, it would be advantageous for the authors to engage in a discourse on these potential defense mechanisms to provide a more comprehensive understanding of the robustness of logit-based FL with respect to privacy concerns."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5691/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772583928,
        "cdate": 1698772583928,
        "tmdate": 1699636595424,
        "mdate": 1699636595424,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "M0rFKFHi6M",
        "forum": "WEoyWdsI9f",
        "replyto": "WEoyWdsI9f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5691/Reviewer_sfwc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5691/Reviewer_sfwc"
        ],
        "content": {
            "summary": {
                "value": "This work develops a model stealing attack (AdaMSA) in logit-based federated learning. Additionally, it provides a theoretical analysis of the bounds of privacy risks. It also proposes a simple but effective defense strategy that perturbs the transmitted logits in the direction that\nminimizes the privacy risk while maximally preserving the training performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed attack is effective with a high attack success rate.\n- Theoretical analysis is conducted to quantify the privacy risks in logit-based FL.\n- Extensive empirical results support the theoretical analysis"
            },
            "weaknesses": {
                "value": "- The proposed method is impractical to be executed or evaluated in real-world scenarios.\n- The privacy risk metric can not express the true privacy risk of the setting.\n- The notations are vague which makes it very hard to follow the analysis of the work"
            },
            "questions": {
                "value": "1. Why does the accuracy on the private dataset express the success rate of the model stealing attack? What if the attack mode is very generalized which achieves high accuracy in the same data distribution?\n\n2.  Since $D_{pub}$ is unlabeled, how to quantify Eq. 1 for $D_{mix}$ ?\n\n3. The construction of $D_{mix}$ is based on the empirical sets of $D_{priv}$ and $D_{pub}$. How does it reflect the true distribution of $D_{priv}$ and $D_{pub}$?\n\n4. Since the adversary cannot touch $D_{priv}$, how to construct $D_{mix}$?\n\n5. What is the difference between the theoretical analysis of the work compared to Blitzer et al., 2007?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5691/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5691/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5691/Reviewer_sfwc"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5691/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773061189,
        "cdate": 1698773061189,
        "tmdate": 1699636595339,
        "mdate": 1699636595339,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Q1I2CF5Q3V",
        "forum": "WEoyWdsI9f",
        "replyto": "WEoyWdsI9f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5691/Reviewer_nzQ8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5691/Reviewer_nzQ8"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates the hidden privacy risks in logit-based Federated Learning (FL) methods through a blend of theoretical and empirical approaches. It introduces the Adaptive Model Stealing Attack, which utilizes historical logits in training and provides a theoretical analysis of the associated privacy risk bounds. They also propose a defense strategy that perturbs the transmitted logits in the direction that minimizes the privacy risk while maximally preserving the training performance. Experiments under different settings demonstrate the performance of the proposed attack and defense."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper provides the first analysis of the hidden privacy risk in logit-based FL methods. An attack and a corresponding defense method are proposed to quantify and prevent the privacy risk. The authors also provide a theoretical bound for the privacy risk.\n* Experiments under different FL settings have been conducted to demonstrate the performance of the attack and the defense."
            },
            "weaknesses": {
                "value": "* The computational complexity of the proposed attack and defense are not discussed.\n* For the defense method, the approximation error of the proposed heuristic solver is not analyzed.\n* The number of communication rounds of the experiments is small. It would be interesting to see whether the performance of the attack and the defense still hold with hundreds or thousands of communication rounds."
            },
            "questions": {
                "value": "* For the proposed attack, how to determine the threshold $T_0$? Why the importance weight $w$ is linearly dependent with $t$ (rather than exponential dependence for example)?\n* How many federated clients are in the experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5691/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788816240,
        "cdate": 1698788816240,
        "tmdate": 1699636595224,
        "mdate": 1699636595224,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "82IVpBZJ5K",
        "forum": "WEoyWdsI9f",
        "replyto": "WEoyWdsI9f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5691/Reviewer_kvRw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5691/Reviewer_kvRw"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the privacy risk in logit-based federated learning (FL). In particular, the authors provide theoretical analysis to bound the privacy risks and propose a model stealing attack adapted to the logit-based FL settings. In addition, the authors also provide a defense strategy that perturbs the transmitted logits to minimize privacy risks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well written.\n2. The experiment presentation is clear.\n3. The defense is simple and effective."
            },
            "weaknesses": {
                "value": "1. First of all, I have some questions and doubts about the significance of logit-based FL in the community of FL. I have checked the logit-based FL papers mentioned in the related work, and they are impactful on the FL community. Currently, logit-based FL seems not to be a well-established and standard norm in FL. From this perspective, studying the privacy risks of logit-based FL is unlikely to have an impact on the community in the long run.\n2. The tricks used in the proposed attack lack technical depth. The proposed attacks improve by the previous baseline via a temporal weighted factor, making the attack an incremental improvement.\n3. Ony baseline (MSA) is too naive. To demonstrate the effectiveness of the tricks used in the proposed attack, I suggest adding more baselines\u2014for example, no threshold $T_0$ or setting $w_t=1$ in the proposed attacks.\n4. Non-iid setting of FL. In Figure 1, the author states that the server aims to infer client $k$\u2019s private models. I wonder if the attack makes sense in the non-iid setting of FL or if client $k$ is a poisoned client. In this case, the objective of the attack should be also justified."
            },
            "questions": {
                "value": "1. Is Adaptive attack possible in the presense of the attack knows the defense (e.g., obfuscate the logit with added noise)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5691/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5691/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5691/Reviewer_kvRw"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5691/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699208961737,
        "cdate": 1699208961737,
        "tmdate": 1699636595140,
        "mdate": 1699636595140,
        "license": "CC BY 4.0",
        "version": 2
    }
]