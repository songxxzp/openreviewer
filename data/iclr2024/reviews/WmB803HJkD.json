[
    {
        "id": "J9qyIfwOc4",
        "forum": "WmB803HJkD",
        "replyto": "WmB803HJkD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6422/Reviewer_sZRG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6422/Reviewer_sZRG"
        ],
        "content": {
            "summary": {
                "value": "This work focuses on analyzing multi-output linear regression in the presence of noisy inputs.  An important part of their work is the discussion of the scenario in which the target multivariate linear regressor equals the identity, allowing them to explore the problem of linear denoising. Similar to a recent line of works, and relevant to contemporary machine learning practices, this work explores the asymptotic regime where data dimensions and the number of samples grow proportionally.\n\nThe primary distinctions from prior studies include the absence of an assumption that the training and test data are independently and identically distributed (i.i.d.) or even sourced from the same distribution. Instead, it is assumed that both the test and training data exist within the same low-dimensional subspace (exhibiting low-rankness). Similarly, this work employs relatively mild assumptions concerning the noise in both the test and training data\n\nDue to the specific assumptions made about the data, the test error is defined as the expected value taken over both the training and test noise of the mean squared error (MSE) on the test data. The authors subsequently derive precise formulas for the test error of the linear regressor that minimizes the training error and the one that minimizes the expected training error. Additionally, they discuss distribution shift bounds and the asymptotic behavior of the (relative) excess risk.\n\nThe authors conclude by comparing their theoretical predictions on real datasets, demonstrating good agreement."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper departs from the conventional assumptions that have been prevalent in previous high-dimensional regression studies. Notably, a particularly intriguing contribution lies in the elimination of the iid or \"Gaussian-like\" assumptions previously relied upon. This requires the application of novel and technical analyses, paving the way for promising avenues for future research.\n\nLastly, the strong agreement between the theoretical formulas and the experimental results on standard ML datasets suggests that the empirical observations hold a certain level of \"universality\" for approximately low-rank datasets."
            },
            "weaknesses": {
                "value": "The paper operates in a regime where the noise is essentially negligible in comparison to the signal's magnitude. This is characterized by a signal-to-noise ratio that approaches infinity. However, the more intriguing scenario is when the noise magnitude is comparable to that of the signal.\n\nOn the same note, the authors assert that their noise assumption is \"strictly more general\" than that of previous works, like Cui and Zdeborova. It's important to note that Cui and Zdeborova consider Gaussian noise with a variance of O(1) for each entry, which contrasts with the o(1) variance examined here. For example, in Cui and Zdeborova, the squared Frobenius norm of Atr is of the order O(N).\n\nThe phenomenon of vanishing noise, combined with a fixed subspace dimension, leads to the peculiar situation where both training and test errors tend to zero, regardless of the under/over-parameterization ratio (c).\n\nThe theoretical results presented in the \"Out-of-Distribution and Out-of-Subspace\" section are not adequately discussed, leaving the reader to ponder their implications.\n\nThe predictions derived from empirical results apply specifically to linear denoisers on real datasets. However, it remains unclear whether these observations hold true or offer insights into the behavior of more general denoisers in contemporary practices, such as deep neural networks."
            },
            "questions": {
                "value": "Could you please provide clarification regarding the regularization of W due to the noise discussed on page 8?\n\nFirstly, there is already significant regularization in place due to the selection of the minimum norm solution. Therefore, it's not entirely clear to me how this interacts with noise-induced regularization.\n\nSecondly, it's worth noting that the empirical results are based on noiseless data. If the objective is to showcase the effects of noise regularization, wouldn't it be beneficial to introduce a small amount of noise and compare it with the noise-free scenario?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6422/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698617443525,
        "cdate": 1698617443525,
        "tmdate": 1699636716321,
        "mdate": 1699636716321,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vQuJStD8RL",
        "forum": "WmB803HJkD",
        "replyto": "WmB803HJkD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6422/Reviewer_jx5r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6422/Reviewer_jx5r"
        ],
        "content": {
            "summary": {
                "value": "This paper derives generalization bounds for linear least square problems with noisy data. In contrast to prior literature, this work considers a more realistic and general setting where 1) the data matrix is low rank, 2) a non-classical proportional regime is used by taking both the data dimension $d$ and size of data $N$ to the limit $\\to\\infty$, and 3) the test data can be drawn from an arbitrary, non-iid distribution. The results of this paper yields some novel insights in the double descent phenomenon and out-of-distribution generalization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The problem setting in this paper is well-explained and well-motivated. Also, this paper offers a wide range of range theoretical and empirical results. These results lead to some interesting insights in a variety of topics of current interests. In particular, I like that Theorem 1 demonstrates the different generalization behaviors in the under-parameterized vs over-parameterized regimes. And the experiments reinforces this contribution by illustrating the double descent phenomenon in accordance to the predictions of Theorem1. Lastly, the out-of-distribution generalization bounds look promising, and I suggest that the authors mention Corollary 5 in the main body, as it is a considerably stronger statement than Corollary 1."
            },
            "weaknesses": {
                "value": "However, my impression is that the paper tries to cover too much mileage at once and thus does not explain the results very clearly. Here are my criticisms:\n1. The authors did not clearly explain the terms in Theorem 1. In the second-to-last paragraph of page 6, what are \"bias term\" and \"variance term\" referring to? The authors also did not explain the difference between the second terms in the respective bounds for under/over-parameterized case. And finally, for the over-parameterized case, where did the extra third term come from?\n\n2. There is no proof sketch for any of the results. Given that the exact terms of the generalization bounds are difficult to digest, it is imperative for the authors to elucidate the key ideas and techniques behind the results. Also, since the proof of Theorem 1 is almost 20 pages long, without an outline of the proof's approach, there is no way for me to even superficially check its validity.\n\n3. The additional results in the Appendix are very poor organized, many experiments results are mixed into the \"Additional Theoretical Results\" section. And many theorem statements (e.g. Theorem 5) in Appendix C are not written out completely and rigorously despite that page limit is not a concern.\n\n4. In Section 4, I think the conclusion in \"Overfitting Paradigms\" is incorrect. In the limit as $N \\to \\infty$, the value of $c$ should be approaching zero, NOT to $+\\infty$. Then, we should not be achieving benign overfitting as suggested by the authors.\n\nLastly, as as suggestion, the author should consider briefly explain that Marchenko-Pastur measure is the \"natural\" limit distribution of the singular values of a random matrix. Otherwise, specifying the shape parameter $c$ may look weird to the reader."
            },
            "questions": {
                "value": "1. In the first term of Theorem 2's bound, I feel that there should be a $N_{tst}$ in the denominator?\n\n2. For the figures, I personally do not like that $x$-axis is in $1/c$ instead of $c$. In the literature, the $x$-axis is usually for the amount of *over*-parameterization (e.g. Figure 1 in [1]). What are your motivations for doing it this way?\n\n3. If I understood correctly, the solid lines in Figure 1a are simply connected the empirical data points, right? This is quite confusing as the solid line denotes the theoretical prediction in Figure 1c.\n\n4. Why is that the theoretical prediction deviates slightly from the empirical observation in Figure 1c (real world setting) but is a near perfect match in Figures 9 and 10? Is that because the experiments for Figure 1c rely on a low-rank approximation? Or are there other reasons?\n\n[1] Schaeffer, Rylan, et al. \"Double Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle.\" arXiv preprint arXiv:2303.14151, 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6422/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6422/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_jx5r"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6422/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698643971388,
        "cdate": 1698643971388,
        "tmdate": 1699636716179,
        "mdate": 1699636716179,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7ZPqqTZTKD",
        "forum": "WmB803HJkD",
        "replyto": "WmB803HJkD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6422/Reviewer_RbRx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6422/Reviewer_RbRx"
        ],
        "content": {
            "summary": {
                "value": "This paper considers a linear denoising problem in which the responses Y_{trn} are formed as Y_{trn} = \\beta^T X_{trn}.  These responses are observed noiselessly, but the data is observed with noise as Z_{trn} = X_{trn} + A_{trn}.  The authors study the performance of a least-squares denoiser W_{opt} = Y_{trn} \\cdot (Z_{trn})^{\\dagger} so that new noisy observations Z_{tst} are denoised as W_{opt} Z_{tst}.  Under a well conditioned, low rank assumption on X_{tst} and regularity assumptions (e.g. isotropic, existence of second moments, convergence of the empirical spectral distribution to the Marchenko-Pastur law) on the noise A_{trn}, the authors provide a characterization of the test error of this denoising procedure."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The non-asymptotic character of the results is appreciated and the validation on real datasets is very nice."
            },
            "weaknesses": {
                "value": "- Assumption A2 seems quite similar to some classical random matrix theory assumptions, explicitly in part 4. In which convergence of the empirical spectral distribution to the Marchenko-Pastur law is assumed.  If I have understood correctly, this assumption (which would follow if independence of the coordinates of the noise matrix were assumed to be independent) allows to bypass the independence assumption.  \n\n- The main technical difficulty seems to come from analyzing the pseudoinverse (A + X)^{\\dagger} as opposed to the inverse (if it existed) (A + X)^{-1}, as the authors point out in the commentary following Theorem 1.  The analysis of this, while cumbersome, does not seem to me to involve much technical novelty.  In particular, the setting with additive structure A + X, where A is random and enjoys favorable structure and X is low rank is quite a bit easier to analyze than the setting in which samples are i.i.d. \\Sigma^{-\u00bd} x_i, and \\Sigma may be ill-conditioned (e.g., Cheng and Montanari, Assumption 3). \n\n- The experimental verification on real data-sets is nice, but it is not very surprising in the simple context considered here.  For instance (and to give an example not cited in the paper), in a similar situation (quadratic loss) which is amenable to random matrix theory, Paquette et. al (https://arxiv.org/abs/2205.07069) study the dynamics of SGD on quadratic models and give exact trajectories when the data is from MNIST (for example).  \n\n- Regarding the insights obtained by the characterizations: The insight over previous work regarding double descent and data augmentation seems limited.  Also, the discussion around benign overfitting is fairly unclear to me.  The setting here is fairly different from that of Bartlett, et. al (https://arxiv.org/abs/1906.11300) in which benign overfitting is characterized in terms of effective ranks.  The assumption on the spectrum and dimension (d/N = c + o(1), finite dimensional data) considered here seems to preclude this kind of decay. \n\n- (Minor) If my understanding of the previous points is correct, it would help to soften the language in the introduction.  As written, the claims made in the introduction are quite a bit stronger than what seems to be concretely shown. \n\n- (Minor) There are also several typos and misspellings in the paper that should be fixed before publication."
            },
            "questions": {
                "value": "- Could the authors please provide an example of a distribution on the noise which satisfies the assumptions (Assumption A2) and does not have independent coordinates (e.g. is not a standard ensemble such as a Wigner matrix)? Why is this more realistic than the examples considered previously?\n\n- Could the authors please clarify what they feel the technical novelty is in analyzing the pseudoinverse (A + X)^{\\dagger}? In particular, it is not clear to me from looking at the proof what the new ideas involved in analyzing this are: It seems that the setting is technically more complicated than if A + X were invertible, but the complications seem to be purely technical and overcome using standard techniques.  To be clear, I do not think this is a bad thing, but clarifying this would help clarify the contribution overall.  \n\n- In the conclusion, the authors write \"Our work has opened the doors for a similar analysis of more sophisticated denoising\nproblems, involving linear networks or non-linearity...\".  Could you please elaborate why you feel this is the case? The analysis seems to me quite tailored to the linear setting with quadratic loss considered here.  In the appendix, the non-linear extension discussed also seems to require further independence assumptions, which seems counter to the viewpoint adopted here."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6422/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6422/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_RbRx"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6422/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698755190965,
        "cdate": 1698755190965,
        "tmdate": 1699636716058,
        "mdate": 1699636716058,
        "license": "CC BY 4.0",
        "version": 2
    }
]