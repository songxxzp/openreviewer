[
    {
        "id": "nTfQHTQ04d",
        "forum": "xelrLobW0n",
        "replyto": "xelrLobW0n",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission571/Reviewer_UTqq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission571/Reviewer_UTqq"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces TRACE, a new benchmark with eight diverse datasets to assess LLMs' continual learning. Tasks include domain-specific, multilingual, code generation, and mathematical reasoning tasks. The datasets are resampled with 5,000 training samples and 2,000 test samples for each task. After training on TRACE, Results show that LLMs suffer a decline in their general and instruction-following abilities. Training on reasoning tasks can offset some of these losses. Based on this, the paper proposes the Reasoning-augmented Continual Learning (RCL) method, which combines task-specific indicators with meta-rationales to prevent catastrophic forgetting in LLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The introduction of TRACE offers a comprehensive benchmark for evaluating continual learning in LLMs. TRACE encompasses a broad range of tasks, ensuring a well-rounded evaluation of LLM capabilities.\n\n* Experiments provide valuable insights and evidence for claims in many previous papers. For example, training on reasoning tasks can mitigate the loss of general abilities.\n\n* Designed CL metrics are reasonable and practical to evaluate the inherent capabilities of LLM."
            },
            "weaknesses": {
                "value": "* It is better to add the evaluation of code generation to the general abilities."
            },
            "questions": {
                "value": "* Why evaluate on the LLaMA-2-Chat instead of LLaMA-2? Could you provide the results for LLAMA-2 as well?\n\n* For Tables 8 and 9, could you provide the results of all tasks at each round?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission571/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698203395792,
        "cdate": 1698203395792,
        "tmdate": 1699635984568,
        "mdate": 1699635984568,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OOriAJgWtn",
        "forum": "xelrLobW0n",
        "replyto": "xelrLobW0n",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission571/Reviewer_aiNx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission571/Reviewer_aiNx"
        ],
        "content": {
            "summary": {
                "value": "This paper identifies limitations of existing continual learning (CL) benchmarks when applying to instruction-tuned (aligned) LLMs and proposes a new CL benchmark, TRACE, designed for aligned LLMs. The TRACE benchmark consists of eight tasks to evaluate LLMs in domain-specific, multilingual, code, and mathematical reasoning abilities. In addition to traditional CL metrics, e.g., overall performance and forgetting, they also evaluate aligned LLMs' general ability, instruction-following ability, and safety ability after learning sequentially.\n\nExperiments on Llama2, Vicuna, and Baichuan2 with 7B and 13B sizes show that nearly all models exhibit a significant decline in general abilities, instruction-following ability, and math and reasoning ability after continual learning the TRACE benchmark. The author further proposes a mitigation method, called Reasoning-augmented Continual Learning (RCL), to encourage the model to generate task analyses and rationales during training, which delivers strong performance on target tasks and substantially retains the original strengths of LLMs.\n\nOverall, this paper is well-written with solid experiments, ablation, and analysis. However, the evaluated CL baselines are limited, leaving the effectiveness of other CL methods in this context unknown."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The biggest contribution of this paper is providing a CL benchmark designed for evaluating aligned LLMs. Evaluating aligned LLMs' general, instruction-following, and safety abilities after learning sequentially is novel. Some findings are interesting; for example, using LoRA for CL will have a more negative impact on the instruction-following ability of LLMs.\n\n- This paper is well-written and easy-to-follow. The experiments are solid with detailed ablation and analysis, including the number of training data, epochs and base models."
            },
            "weaknesses": {
                "value": "- The compared CL baselines are limited. While O-Lora and PP are used, only Replay and Lora is used in the main experiments. The effectiveness of other CL methods in the context of continually learning instruction-tuned LLMs is unknown. For example, the traditional EWC, GEM, and recently L2P [1], DynaInst [2]. Can these methods reduce forgetting while maintaining the general, instruction-following, and safety ability?\n- For instruction-tuned LLMs, generalisation ability on unseen tasks is also essential as it may reduce the training data needed for learning new tasks [3][4]. It is good to have a forward transfer (FWT) metric as well.\n\n[1] Learning to Prompt for Continual Learning, CVPR 2022\n\n[2] Large-scale Lifelong Learning of In-context Instructions and How to Tackle It, ACL 2023\n\n[3] CITB: A Benchmark for Continual Instruction Tuning, EMNLP 2023\n\n[4] Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks, EMNLP 2022\n\n- Typos:\n1.\tP3 Eq2, is it a typo for i\u2265t? If OP is to measure the overall performance of currently learned t tasks, shouldn\u2019t it be i\u2264t?\n2.\ttypo for Table.2, Vicuna-13B-V1.5 $\\Delta R^G_t$ shouldn\u2019t be 0?\n3.\tP8 Table.6 \u2192 Figure.6\n4.\tP9 Table.6 \u2192 Table.4\n5.\tP23 Table.26, BWT is ???"
            },
            "questions": {
                "value": "1.\tI\u2019m not sure why in-context learning (ICL) can be a CL baseline. What\u2019s the meaning of using ICL as a CL baseline?\n2.\tTable 1, why does ICL not have BWT?\n3.\tFor SeqLoraFT, are you initialising a new adapter for learning each new task or using the same adapter?\n4.\tFor RCL, for non-reasoning tasks such as Py150, how do you generate the reasoning steps? Is there any example?\n5.\tTask order: which CL method does Table 26 use? Is there any analysis of the impact of task ordering?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission571/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698497100983,
        "cdate": 1698497100983,
        "tmdate": 1699635984479,
        "mdate": 1699635984479,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VeZS9srh9B",
        "forum": "xelrLobW0n",
        "replyto": "xelrLobW0n",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission571/Reviewer_hYjF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission571/Reviewer_hYjF"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a new benchmark for continual learning of aligned LLMs. To this end, the author has mixed existing challenging benchmarks in a sequential manner, where these benchmarks are not used for recently published aligned LLMs, e.g., Lamma2-chat. The authors also present a new method called reasoning-augmented continual learning which augments the reason generated from GPT4."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors have considered somewhat large language models for the evaluation. It would be very grateful if the authors could compare it on a bigger scale, e.g., llama2 70b-chat, falcon-40b instruct."
            },
            "weaknesses": {
                "value": "1. While I do agree that some papers, e.g., instruction tuning, have used the past continual learning benchmarks, TRACE also shares the same issue. After publishing this benchmark, there are possibilities that some may use this dataset for pre-training. So, I think continual learning (CL) benchmarks all shares a similar problem, so this is hard to claim as a problem of prior CL benchmark (since this paper also have the same issue). \n\n2. For me, it is hard to understand i) why this benchmark has novelty for CL and 2) why this has novelty for alignment LLMs\n- i) TRACE is a mix of existing challenging benchmarks and put them in a sequential manner. It is hard to see novelties or specialization for continual learning (except they have put it in a sequential manner). \n- ii) The only thing this benchmark is specialized to alignment LLMs is that they propose a metric that is related to alignment LLMs. But these metrics are also a naive extension of prior continual learning metrics (the overall performance and backward transfer suggested by [1]). \n\n3. One important discussion in continual learning is about forward transfer [1,2,3]. Considering forward transfer metric is also needed.\n\n4. As a benchmark paper, I think it is very important to compare the prior CL methods, where this paper only shows a few baselines. For instance, considering the following methods will be helpful (see some baselines in [4]) or consider [5,6,7].\n\n5. The overall writing should be improved. For instance, **the main text cites the results in the Appendix too much** (Figure 5, Table 6). If the table and figures are important, the authors should put them in the main text. Especially, the results of reasoning-augmented continual learning are important but missing in the main text.\n\n6. Using GPT-4 is not a good choice for continual learning (even for reasoning), since we don't know which dataset they have used to train the model.\n\nOverall, it is somewhat hard to find a novelty as a continual learning benchmark (and typically specialized for aligned LLMs), and it is hard to find novel metrics. Furthermore, the paper needs more consideration regarding forward transfer, more comparison of multiple frameworks, and improving the overall writing. \n\nPlease find the reference below\\\n[1] Lopez-Paz and Ranzato, Gradient episodic memory for continual learning, NeurIPS 2017\\\n[2] Wolczyk et al., Continual World: A Robotic Benchmark For Continual Reinforcement Learning, NeurIPS 2021\\\n[3] Chen et al., Is Forgetting Less a Good Inductive Bias for Forward Transfer? ICLR 2023\\\n[4] Jang et al., Towards Continual Knowledge Learning of Language Models, ICLR 2022\\\n[5] D'Autume et al., Episodic memory in lifelong language learning, NeurIPS 2019\\\n[6] Huang et al., Continual learning for text classification with information disentanglement based regularization, NACCL 2021\\\n[7] Razdaibiedina et al., Progressive prompts: Continual learning for language models, ICLR 2023"
            },
            "questions": {
                "value": "I think the result of Vicuna-13B-V1.5 in Table 2 should be 0."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission571/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762806391,
        "cdate": 1698762806391,
        "tmdate": 1699635984403,
        "mdate": 1699635984403,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "K4dFtM6uvh",
        "forum": "xelrLobW0n",
        "replyto": "xelrLobW0n",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission571/Reviewer_dqAr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission571/Reviewer_dqAr"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an evaluation benchmark for continual learning of LLMs. In addition, RCL was proposed for reducing the loss of general abilities of LLMs during tuning. The proposed benchmark consists of carefully curated data from different domains."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. A comprehensive benchmark and in-depth analysis of LLaMA-2, Vicuna, Baichuan2 is provided, revealing the observation of catastrophic forgetting of (relatively small scale) LLMs when tuning on novel tasks. \n2. Reasoning-based data is found important to mitigate forgetting of LLMs in tuning."
            },
            "weaknesses": {
                "value": "1. Although tuning larger models would be computationally expensive, it would be still interesting to see whether the observation and analysis hold with more parameters, since larger models may behave differently. \n2. The proposed reasoning-augmented method indeed provides insights for tuning LLMs, however, the method and study of it are too simple. For example, currently, it involves the manual selection of GPT-4 augmented data. Also it would be interesting to see how many reasoning-augmented data would be \"optimal\" for avoiding forgetting. Does more data help or hurt?"
            },
            "questions": {
                "value": "1. Can you provide more explanation as to why reasoning-augmented data is helpful for continual learning of LLMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission571/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission571/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission571/Reviewer_dqAr"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission571/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698767851087,
        "cdate": 1698767851087,
        "tmdate": 1700673320810,
        "mdate": 1700673320810,
        "license": "CC BY 4.0",
        "version": 2
    }
]