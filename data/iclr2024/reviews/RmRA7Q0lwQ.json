[
    {
        "id": "99C1nvV6Iw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission894/Reviewer_YQBo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission894/Reviewer_YQBo"
        ],
        "forum": "RmRA7Q0lwQ",
        "replyto": "RmRA7Q0lwQ",
        "content": {
            "summary": {
                "value": "The authors demonstrate that CFG, which has primarily been used in text-to-iamge generation, can bring improvements in pure language modeling. The authors demonstrate that CFG boosts performance on benchmarks and provide results on multiple models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "[+] The authors suggest new improvements to training in large language models, leading to faster training times and more granular control. \n[+] The paper has a thorough background section, containing diverse and relevant works to their proposed method.\n[+] The paper contains extensive comparative results on numerous tasks.\n[+] The authors provide an insightful computational cost analysis."
            },
            "weaknesses": {
                "value": "[-] The idea of using CFG is not novel. The authors simply apply this principle to different models. \n[-] The explanations for why CFG works well for language models are not very solid. I'd like to see more concrete evidence of what is being altered in the model in this training process."
            },
            "questions": {
                "value": "It is insightful to see these experiments. However, despite the great results, the idea is not surprising or novel. It does not feel like quite a strong contribution to the community yet. How can CFG be extended to help language models specifically? This seem like a generic application of the idea."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission894/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission894/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission894/Reviewer_YQBo"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission894/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697411764265,
        "cdate": 1697411764265,
        "tmdate": 1699636016065,
        "mdate": 1699636016065,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aboTibPK3J",
        "forum": "RmRA7Q0lwQ",
        "replyto": "RmRA7Q0lwQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission894/Reviewer_RjYY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission894/Reviewer_RjYY"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the effectiveness of classifier-free guidance (CFG) in pure language modelling. Drawing inspiration from the equation employed in text-to-image generation, the authors apply CFG to the logits of next-token predictions in language models.\nThrough a plethora of designed experiments, they validate the remarkable efficacy of CFG: 1) enhancing the performance of LLMs on many NLP tasks; 2)  improving the performance of CoT and self-consistency; 3) increasing the faithfulness and coherence."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea is simple and reasonable.\n- This paper conducted extensive experiments to validate the effectiveness of CFG."
            },
            "weaknesses": {
                "value": "- The \\gamma values in one context are poorly suited for another context, making CFG tricky in practice. \n- Some recent works have explored CFG in language models, weakening the contribution of this paper."
            },
            "questions": {
                "value": "- Have you explored this method in controllable NLG tasks or constraint-decoding tasks, or compared it with SOTA methods? \n- Compared with text-to-image generation, the optimal \\gamma value in the language modelling seems to be small (<2), while large \\gamma value leads to poor performance. Have any observations on it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission894/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698730592116,
        "cdate": 1698730592116,
        "tmdate": 1699636015982,
        "mdate": 1699636015982,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Q5so6JHPVn",
        "forum": "RmRA7Q0lwQ",
        "replyto": "RmRA7Q0lwQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission894/Reviewer_3Gz2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission894/Reviewer_3Gz2"
        ],
        "content": {
            "summary": {
                "value": "This paper adapts the classifier-free guidance from text-to-image diffusion model into text generation to have better control on the generation content. With a $\\gamma$ multiplier deciding the strength of the guidance away from the unconditional vector in the direction of the conditioning, it allows a finer controll on the prompt adherence. By extensive experiments on chain-of-though prompting, long-context generation, programme synthesis and conversational asisstant, the author shows that the proposed method can achieve similar performance as a double-sized model without significant increase in computation cost."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method is very straightforward and easy to implement yet effective, requiring only the $\\gamma$ multiplier and the second-run of the model.\n2. The paper is well written and easy to follow.\n3. The experiment performance is impressive and allow a LM to perform nearly as well as a doule-sized one without significant increase in computation cost."
            },
            "weaknesses": {
                "value": "1. some formatting issues (not necessarily reason to reject):\n    The citation format and style in the submission is not correct. It seems that the authors always use \\citet{} instead of \\citep{}\n    Some important reference are missing. For example, the original PaLM paper is not cited.\n    In figure 2, some part of the curve is overlapped with the legend.\n    In figure 2, the ticks for the x-axis are not evenly distributed.\n    In table 2, the percentage sign is missing for some numbers.\n    In the first paragraph of section3, LLaMa -> LLaMA.\n    In the line above Eq.6, $N$ is used to denote the number of tokens to model which is different from the $T$ in Eq.6.\n\n2. A memory cost analysis is recommended. The proposed method requires a second run of the model, which may increase the memory cost (for example, the key-value cache)."
            },
            "questions": {
                "value": "There is no grantee that the Eq.6 will obtain a legal probability with the probabilities of all possibilities summing up to 1. Is there any normalization technique to deal with this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission894/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699026257184,
        "cdate": 1699026257184,
        "tmdate": 1699636015899,
        "mdate": 1699636015899,
        "license": "CC BY 4.0",
        "version": 2
    }
]