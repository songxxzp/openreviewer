[
    {
        "id": "JLOMmXl2by",
        "forum": "MBIGXMT0qC",
        "replyto": "MBIGXMT0qC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7182/Reviewer_54HD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7182/Reviewer_54HD"
        ],
        "content": {
            "summary": {
                "value": "The paper extends the concept of protein language models to atom-scale and molecular data. Methodologically, the contribution of the paper are three-fold: (1) the development of a universal transformer tailored for molecule and protein data; (2) the introduction of a code-switch protein sequence approach to unpack residues; (3) the presentation of a multi-scale position encoding designed specifically for code-switch sequences. The authors claim that they effectively demonstrate the efficacy of their approach through enzyme-substrate and drug-target affinity tasks. Additionally, their method achieves comparable results in areas such as contact prediction, secondary structure prediction, and molecular property prediction tasks."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The idea of designing a universal transformer for molecules and proteins are of high potential. Such an approach has the potential to unify various downstream tasks related to both molecules and proteins. However, there are evident flaws in the methodology presented in the paper.\n2. To my knowledge, the related work section about protein pre-training is thorough and comprehensive."
            },
            "weaknesses": {
                "value": "1. There are significant flaws in the design of code-switch protein sequences, which can potentially make the model learn meaningful insights from the unzipped sequences.\n2. The concept of multi-scale position encoding essentially merges residue- and atom-level embeddings. This has been previously proposed in other papers and, thus, lacks novelty.\n3. The results present in the experimental section only shows marginal improvements over the established baselines. Additionally, the unified pre-training appears to diminish performance on tasks focused solely on proteins or molecules. This compromises its viability for practical applications.\n4. The related work section seems to overlook several pivotal studies pertinent to molecular modeling.\n\nFor details, please refer to the Question section."
            },
            "questions": {
                "value": "1. The concept of unzipping residues into atom sequences and predicting the masked atom type appears flawed. Given the residue type or the types of adjacent atoms, deducing the type of the masked atom becomes straightforward, since the atom set is predetermined for each residue type. This indicates that unzipping residues does not introduce any unique or non-trivial information. Additionally, there is a lack of experiments to demonstrate that the masked atom type prediction loss contributes any meaningful insight.\n2. As shown in the Tables 2 and 3, ProSmith's results with ms-ESM offer only a slight enhancement over baselines, such as He et al. (2023) in Table 2 and Kroll et al. (2023b) in Table 3.\n3. Are the pre-trained language models fine-tuned for specific tasks? If not, the tables should encompass results with fine-tuning.\n4. The protein-only tasks seem trivial when protein structures are used as inputs. With knowledge of protein tertiary structures, it becomes easy to determine if two residues are in contact and to identify the secondary structure. Moreover, since the pre-training task includes pairwise distance prediction, there's potential for data leakage. The test data might overlap with the pre-training dataset.\n5. Despite the possible data and information leakage in the experimental framework, the proposed method fails to outperform standard protein language models like ESM-2 in tasks such as contact and secondary structure prediction. Given that the model incorporates a broader range of pre-training loss than ESM-2, these results suggest that universal pre-training across both proteins and molecules may not offer advantages in protein-only or molecule-only tasks. This significantly weakens the paper's primary claim: the benefit of combining protein and molecule data for pre-training.\n6. The ablation study only consider ablations on position encoding, neglecting the paper's other two significant contributions. It would be beneficial for the authors to explore comparisons with protein- or molecule-only pre-training, consider pre-training without unzipped sequences, and evaluate the impact of removing each pre-training loss.\n\nOverall, I recognize the significance of developing universal models for molecule and protein pre-training. However, the paper exhibits considerable flaws in its methodologies and experimental sections, making it below the standard expected for publication."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7182/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7182/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7182/Reviewer_54HD"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7182/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697815898722,
        "cdate": 1697815898722,
        "tmdate": 1699636852377,
        "mdate": 1699636852377,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rkLzMvVvsj",
        "forum": "MBIGXMT0qC",
        "replyto": "MBIGXMT0qC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7182/Reviewer_p5PD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7182/Reviewer_p5PD"
        ],
        "content": {
            "summary": {
                "value": "In the context of protein language models, this paper attempts to fuse the information contained at the residue scale with the one contained at the atom scale (i.e. the structure of the resides themselves) to produce better models. The protein sequence is thus represented by inserting the set of atoms constituting a certain residue inbetween the residues. To accomodate this change, the authors propose ad hoc position encodings depending on the scale (atom or residue). The method is tested in several (protein-molecule, protein-only, and molecule-only) tasks with good overall performances."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- the proposed idea makes sense intuitively, and appears to be effective across different tasks.\n\n- the experiments are well thought and the results seem convincing, both in depth and width."
            },
            "weaknesses": {
                "value": "- the technical novelty is limited; I understand code-switching is not novel but I value that it is ported to this field. However, the rest of the techniques used in this paper (like RoPE, or the transformer architecture) are not novel.\n\n- Lack of detail on certain topics. For example, in Section 2.2, the ORDER procedure is not explained (there is a referral to Appendix A, where however I didn't find an explanation). Similarly, the \"Atom Scale Position Encoding\" section is not informative with lots of unintroduced symbols.\n\n- The \"slight modification of the Transformer\" in section 2.4 appears poorly justified or at least needs more clarification. Why is $E^A$ added to the standard attention? What happens if it's not added? \n\n- Also in Section 2.4, my guess is that the scaling by $\\sqrt{d_k}$ gets disrupted by the $E^A$ term. Can you comment on this latter point?\n\n- The ablation study in Table 7 shows almost no improvement from vanilla ESM to the \"w/o RSPE in atoms\" variant."
            },
            "questions": {
                "value": "Mostly related to the weaknesses, see above. On the more discussive side:\n- did you consider the idea of representing the structure of the residue with a graph neural network? What could be the up/downsides of this approach?\n- it is unclear how the sequence length is affected by the addition of the atoms constituting the residues. Can you detail how long are the sequences you deal with, and how much their lengths increase by adding the atoms?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7182/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698771843844,
        "cdate": 1698771843844,
        "tmdate": 1699636852156,
        "mdate": 1699636852156,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k7SDo87ho6",
        "forum": "MBIGXMT0qC",
        "replyto": "MBIGXMT0qC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7182/Reviewer_VsVJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7182/Reviewer_VsVJ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a multi-scale language model for protein and small molecule modeling. The author combines masked language modeling and pair-wise distance recovery to pretrain the model. The authors present competitive results against baselines on multiple tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is clearly written.\n2. The multiscale modeling technique is novel."
            },
            "weaknesses": {
                "value": "1. The proposed method could not outperform baselines, as shown in Table.5 and Table.6.\n2. Insufficient experiments regarding molecular representation learning affects the significance of the paper."
            },
            "questions": {
                "value": "1. Could you provide head-to-head comparisons against Unimol on molecular representation learning tasks, such as BBBP,  BACE, Tox21?\n2. How about scaling the 35 million model to 650 million? Could you provide the corresponding results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7182/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7182/Reviewer_VsVJ",
                    "ICLR.cc/2024/Conference/Submission7182/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7182/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698910977661,
        "cdate": 1698910977661,
        "tmdate": 1700670887110,
        "mdate": 1700670887110,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ei5e8KfXqR",
        "forum": "MBIGXMT0qC",
        "replyto": "MBIGXMT0qC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7182/Reviewer_ZacV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7182/Reviewer_ZacV"
        ],
        "content": {
            "summary": {
                "value": "This work aims to appy the powerful protein language models (ESM) to the applications of both small molecules and proteins. \n\n\nSpecifically, the authors provides a multi-scale ESM (ms-ESM) model for the unified molecular modeling. The ms-ESM model can take both protein sequences and molecules with 3D coordinates as input. \n\n\nThe model is pretrained using both protein dataset (AlphaFoldDB) and molecule dataset (from uni-mol). Each residue in a protein can also be unzipped to several atoms. The pre-training tasks are masked language modeling (MLM) and pair-wise distance recovery. \n\n\nThe architecture of ms-ESM is very similar to ESM, and one main difference is that the atom scale position encoding (Euclidean distance + Gaussian kernel) is used as a bias term in the attention layers.\n\n\nThe proposed ms-ESM is evaluated on protein-molecule tasks, protein-only tasks, and molecule-only tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea seems interesting. By unzipping atoms in some residues, the protein-specific ESM model becomes a model at both residue and atom scales."
            },
            "weaknesses": {
                "value": "1. To me, the presentation, especially the experiments part, is not clear. Fro example, the authors use \u2018for more details of \u2026., readers can find them in \u2026\u2019 many times, but this really restrict me to understand the implementation details. A better way could be \u2018following \u2026, we fine tune \u2026 using \u2026.\u2019 In addition, please list the data size of downstream tasks. \n\n2. About the ms-ESM model: during pre-training, what percentage of residues are unzipped? \n\n3. Ablation: The pair-wise distance recovery is only used at atom scale and requires atom coordinates as inputs. How about removing this loss? What is the performance? Is this term necessary?\n\n4. The performance on protein-molecule tasks: I can\u2019t intuitively understand why ms-ESM can outperform two separate pre-trained models (one for protein, and one for molecule)? Basically, I think the capacity of two models should be greater than a single model. Is the comparison fair? Please provide more explanation."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7182/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698956027125,
        "cdate": 1698956027125,
        "tmdate": 1699636851860,
        "mdate": 1699636851860,
        "license": "CC BY 4.0",
        "version": 2
    }
]