[
    {
        "id": "lEn6qgvQgY",
        "forum": "Bb4VGOWELI",
        "replyto": "Bb4VGOWELI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission358/Reviewer_iHbx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission358/Reviewer_iHbx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the use of large language models (LLMs) as optimizers to address various optimization tasks, particularly those that can be well expressed in natural language. To validate their approach, the authors conducted experiments on linear regression and traveling salesman problems, as well as prompt optimization, where the goal is to find instructions that maximize task accuracy. Results show that the best prompts optimized by this work outperform human-designed prompts by up to 8% on GSM8K and by up to 50% on Big-Bench Hard tasks. Additionally, the authors evaluated the transferability of found prompts to different datasets in the same domain, demonstrating that their optimized prompts outperform baseline prompts on MultiArith and AQuA."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work is among the early investigations into an intriguing research question: can LLMs be used for various optimization tasks? \n2. The obtained optimal prompts from the method are both interesting and useful, such as \"Take a deep breath and work on this problem step-by-step\". \n3. The paper writing is clear, and the figures that state the key experiment results and illustrate the method are well-plotted."
            },
            "weaknesses": {
                "value": "1. What is the key/unique advantage of using LLMs to optimize over some traditional optimization algorithms, especially on classical optimization problems (not prompt engineering)? \n2. For prompt optimization, the optimization process of this work (i.e., directly feeding solution-score pairs, optimization task descriptions, and meta-instructions into the optimizer LLM) is a black box/lacks interpretation, why it is a better prompt optimizer than those new methods that leverage LLMs to explicitly act as mutation and crossover operators, and further optimize the prompt? such as, \n    1. EvoPrompting: Language Models for Code-Level Neural Architecture Search by Chen et al.\n    2. Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers by Guo et al. \n3. To further understand the effect of the purple text in this work, an ablation study may be beneficial for improving the solidness of the results."
            },
            "questions": {
                "value": "Please see the above weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission358/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission358/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission358/Reviewer_iHbx"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission358/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698418439222,
        "cdate": 1698418439222,
        "tmdate": 1700618681908,
        "mdate": 1700618681908,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1ynYt3hcjf",
        "forum": "Bb4VGOWELI",
        "replyto": "Bb4VGOWELI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission358/Reviewer_hsEq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission358/Reviewer_hsEq"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use LLMs as optimizers by simply inputting the natural language description of the optimization task, previous steps\u2019 inputs and scores to LLMs. This paper applies such method on prompt search for various LLM tasks and demonstrates its effectiveness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Good proof of concept. This paper provides concrete evidence that large language models can find the patterns between the inputs and corresponding scores that humans might not be able to find to conduct optimization tasks.\n2. Good use case. Based on such proof of concept, this paper finds a valid use case for the proposed method, on which other traditional optimizations might be difficult to apply, finding the proper prompts for LLM tasks.\n3. Solid experiments on prompt search. Experiments show that it is not random that the proposed method is able to find proper prompts for various tasks which lead to significant performance improvements. The thorough ablations also already provides answers to a lot of concerns."
            },
            "weaknesses": {
                "value": "Two questions on the ablation study.\n\n1. Numbers of examplers. Did you take the randomness of example picking into consideration? For each run of every setting, do you give the same set of examples? \n2. I noticed that for different tasks, the \u201cbatch size\u201d that works the best can be different (Figure 5, cd). Do you find any obvious patterns on which types of data/tasks prefer a smaller \u201cbatch size\u201d and vice versa?"
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission358/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698625363609,
        "cdate": 1698625363609,
        "tmdate": 1699635963100,
        "mdate": 1699635963100,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fLCRDq9IjJ",
        "forum": "Bb4VGOWELI",
        "replyto": "Bb4VGOWELI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission358/Reviewer_3F2p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission358/Reviewer_3F2p"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Optimization by Prompting (OPRO), a method to use large language models (LLMs) as optimizers for various tasks. The key idea is to describe the optimization problem and provide the model with past solution-score pairs in a meta-prompt. The LLM then generates new candidate solutions based on this context. OPRO is first demonstrated on linear regression and traveling salesman problems. The main application is prompt optimization, where the goal is to find an instructional prompt that maximizes a model's accuracy on a dataset. Experiments optimize prompts for GSM8K and BigBench, starting from poor prompts and showing significant gains."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Novel idea of leveraging LLMs' understanding of natural language and few-shot learning abilities for optimization. Enables optimization by simply describing the problem rather than formal specification.\n\n- Demonstrated on diverse tasks - mathematical optimization, prompt optimization. Shows potential breadth of this approach.\n\n- Compelling results on prompt optimization. Optimized prompts substantially outperform human-written prompts, improving accuracy by up to 8% on GSM8K and 50% on BigBench.\n\n- Principled design of the meta-prompt, balancing past solution-score pairs and problem description. Ablations validate design choices.\n\n- Thorough experiments comparing different LLMs, scoring models, prompt positions, and baselines. Shows consistent benefits of OPRO."
            },
            "weaknesses": {
                "value": "- The biggest limitation is that OPRO's performance looks highly fluctuating. It's unclear if the LLM really finds the so-called optimization \"trajectory\" or just randomly finds a good prompt. The authors should provide more analysis to show that the LLM is indeed learning to optimize.\n\n- Limited exploration on how to provide richer feedback to LLM beyond aggregated scores. It could help address limitations. \n\n- Unclear how sensitive results are to meta-prompt design and hyperparameters like temperature.\n\n- No comparison to other prompt optimization methods. It could better situate contributions.\n\n- Limited analysis. For example, there is no characterization of what makes an effective prompt."
            },
            "questions": {
                "value": "- Can you clearly state how many optimization steps are performed in each experiment? How does the number of steps affect performance?\n\n- Can you provide an experiment where you generate the same number of prompts in one step as your current experiments, and evaluate them all, and report the best one? This would help clarify whether the LLM is really learning to optimize or just randomly finding a good prompt.\n\n- For prompt optimization, have you experimented with providing more detailed feedback to the LLM beyond aggregated scores? (e.g. accuracy on different example types, common mistakes)\n\n- How does the meta-prompt length affect optimization performance? Is there a sweet spot balancing past solutions and problem descriptions?\n\n- What determines the choices of sampling temperature? Have you tried adaptive temperature schedules?\n\n- What are the limitations on problem complexity that OPRO can handle? Analysis of how performance degrades with complexity?\n\n- Can you better characterize what makes an effective prompt for optimization? Any semantic or syntactic patterns?\n\n- How does OPRO compare to other gradient-free prompt optimization methods? Could be included in experiments.\n\n- Is there any overfitting during prompt optimization? How does test accuracy compare to training accuracy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission358/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission358/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission358/Reviewer_3F2p"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission358/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814054917,
        "cdate": 1698814054917,
        "tmdate": 1700600257269,
        "mdate": 1700600257269,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zQXNjgMbnX",
        "forum": "Bb4VGOWELI",
        "replyto": "Bb4VGOWELI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission358/Reviewer_um42"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission358/Reviewer_um42"
        ],
        "content": {
            "summary": {
                "value": "The authors propose large language models (LLMs) as optimizers for various tasks by instructing the models through natural language prompts. Currently, optimization problems have to be explicitly defined, and algorithms are tailored and fine-tuned for specific tasks, which can be challenging and time-consuming. This approach proposes Optimization by PROmpting (OPRO), which leverages the adaptability and versatility of LLMs by modifying the problem description in the prompt, enabling simple and effective optimization for different tasks.\nThe significant result is that OPRO can lead to better performance on selected language tasks, outperforming human-designed prompts by up to 8% on GSM8K and 50% on Big-Bench Hard tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This work demonstrates that LLMs can help optimize prompts to achieve high performance on a variety of tasks."
            },
            "weaknesses": {
                "value": "First, I disagree with the authors fundamentally about what optimization means. To me, this work is not optimization but step-by-step inference. To quote Wikipedia for reference, optimization is \"the selection of a best element, with regard to some criterion, from some set of available alternatives.\" One can plausibly consider the process of prompt selection as \"optimization\", but in order to make a claim on the general area of optimization I would expect results on optimizing a wide range of convex functions and non convex functions as opposed to word problems. The claim on linear regression as an important result is a relevant but very limited result, but it is not included in the main paper.\n\nSecond, considering the relevance of this approach to step-by-step inference, what is new here compared to previous step-by-step inference procedures? The authors include them in the introduction as evidence for LLM is capable of doing multi-step reasoning, but do not distinguish differences b/w this work on prior work. Also they do not compare to these prior work."
            },
            "questions": {
                "value": "What is the difference between this work and other step-by-step inference techniques such as https://arxiv.org/pdf/2205.11916, https://arxiv.org/abs/2201.11903, https://arxiv.org/abs/2305.10601\n\nHow do the results compare?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission358/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission358/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission358/Reviewer_um42"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission358/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698865094363,
        "cdate": 1698865094363,
        "tmdate": 1700349703022,
        "mdate": 1700349703022,
        "license": "CC BY 4.0",
        "version": 2
    }
]