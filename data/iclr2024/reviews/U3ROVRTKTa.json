[
    {
        "id": "8d8jr7jmx4",
        "forum": "U3ROVRTKTa",
        "replyto": "U3ROVRTKTa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2020/Reviewer_ci1c"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2020/Reviewer_ci1c"
        ],
        "content": {
            "summary": {
                "value": "The paper describes an interesting prompt-learning strategy to capture temporal drifts in data over time. By representing data as occurring from different domains over time, the algorithm proposed by the authors learn two types of prompts from data by predicting future domains: One prompt capturing generalization across domains over time, and another prompt that incorporates ordered domain-specific prompts over time to capture temporal qualities exhibited by the data generation process. The authors empirically evaluate the proposed method over synthetic and real-world datasets to show the efficacy of the solution over other competing methods. Furthermore, they also provide an insightful ablation study to justify how prompts learned in the solution is useful in capturing temporal dynamics in data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well written and easy to follow. The algorithm description and figure depictions really help in understanding the concepts and contributions presented in the paper.\n2. Empirical evaluation shows good performance on both synthetic and real-world data, when compared to other existing mechanisms.\n3. The ablation study clearly shows the need for two prompt types proposed in the paper."
            },
            "weaknesses": {
                "value": "The problem description seems to promise more than what the eventual solution delivers. Particularly, the paper is positioned by the authors as domain generalization and promises a solution in a space where learnings from different domains can be utilized to capture information useful for predicting unknown target domains. However, after reading the solution and dataset description, this falls short of expectation as the authors focus on concept drift within the same dataset. Data from the same data generation process is divided into multiple windows, where each window is called a domain. So, data drifts indicated by the authors are within data drift over time. In the literature, there has been multiple articles published on concept drift or data drifts in general over the past few decades. For example, please see Lu, Jie, et al. \"Learning under concept drift: A review.\" IEEE transactions on knowledge and data engineering 31.12 (2018): 2346-2363. With this context, it is not clear why data windows within a dataset is termed as \"domains\", where it is truly not from a different domain. For true domain generalizability and adaptability, it would be good for the authors to explore how domain adaptation is setup, and empirically evaluate in-domain and across-domain generalizability and adaptation."
            },
            "questions": {
                "value": "A few elements of Algorithm 1 are not clear.\n\t1. Are the number of data points in each domain the same?\n\t2. In Step13, how exactly is PT(t) generated? Is is a concatenation of previous domain-specific prompts concatenated when provided as input to gw? Particularly, what is the difference between Line 12 and Line 13?\n\t3. Given my understanding of the problem setup, it is unclear what exactly is Y? Say in your housing price prediction example, is Y the house prices in the target domain (validation data) or house prices at domain t available in the training data?\n\nThe empirical evaluation in Table 2 and 3 shows that the proposed method has the least error across all datasets with greater than 2 variables. However, both the synthetic data generation and temporal drifts learned by the prompts seem to work for non-abrupt changes. Does this also work for abrupt drift?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2020/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698724248399,
        "cdate": 1698724248399,
        "tmdate": 1699636133398,
        "mdate": 1699636133398,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "suZMHY1hEl",
        "forum": "U3ROVRTKTa",
        "replyto": "U3ROVRTKTa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2020/Reviewer_zvod"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2020/Reviewer_zvod"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new approach for adapting to these temporal changes without needing future data during training. Using a prompting-based method, it tweaks a pre-trained model to address time-related shifts by using different types of prompts that understand time-based patterns. This technique works for various tasks, like classification and forecasting, and achieves leading performance in adapting to time-based data changes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper presents the first prompt-based method to handle temporal domain generalization.\n2. The proposed method achieves better performance than existing methods in both accuracy and efficiency. \n3. The studied problem is interesting and timely."
            },
            "weaknesses": {
                "value": "1. The motivation for using prompts still lacks proper motivation. Specifically, the motivation for using prompts is claimed as \"none of these prior works can generate time-sensitive prompts that capture temporal dynamics.\" There are tons of other ways to learn temporal dynamics, and we don't have to use prompts.\n2. The experiment setup also has some issues, e.g., the ablation study can be further improved, more baselines can be included, etc.\n3. The overall presentation is a little messy. There are some undefined notations. The authors seem to misuse \\citep{} and \\citet{}, and the presented references impact the overall readability."
            },
            "questions": {
                "value": "1. What's the mathematical formulation of the prompt?\n2. What would be the intuition of training the backbone network on the aggregated dataset? For some datasets with manually altered data distribution on each domain (like two moons), the decision boundary would be really difficult to learn if you mix all the data together.\n3. Following the previous question, the ablation study can be further designed to remove the backbone model to verify if the backbone model is truly useful.\n4. Some sentences are quite hard to understand, e.g., \"For each domain $t$, we prepend the input $X$ with a prompt $PS(t)$, which are learnable parameters.\" Are both $X$ and $PS(t)$ learnable?\n5. Not sure why some baselines are excluded in Table 3's comparison.\n6. One of the major claims is also confusing: \"Our paper presents a novel prompting-based approach to temporal domain generalization that does not require access to the target domain data\". I feel like no access to the target domain data is a default rule of domain generalization."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2020/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772742927,
        "cdate": 1698772742927,
        "tmdate": 1699636133316,
        "mdate": 1699636133316,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4fTckgdOCy",
        "forum": "U3ROVRTKTa",
        "replyto": "U3ROVRTKTa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2020/Reviewer_HfUr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2020/Reviewer_HfUr"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a training paradigm for domain generalization. This contains 1) pre-training of a backbone model on all source domains, 2) the learning of source-domain-specific \"prompts\" for each source domain, 3) the learning of a \"temporal prompt\" for each source-domain, and 4) the learning of a global \"prompt\". At testing time, global and domain-specific prompts from the past can be used to make predictions in the new domain. The method is applied to time series classification and forecasting data sets and on a synthetic data set."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is easy to follow and straightforward to read. The synthetic data experiment visually and numerically shows the shortcomings of other methods that motivate this work. Predictive performances are reported (at least partially) with standard deviations."
            },
            "weaknesses": {
                "value": "I am having difficulty thoroughly understanding how the architecture is trained in detail (more details in the question). I also question whether it is appropriate to split time series data artificially into different source domains. If a domain is not explicitly defined, one could pick an arbitrary period and define it as a domain, as was done on the crypto data set. There is no sound justification for why a one month period was chosen or why it would be better than a two week period. It furthermore seems that the presented method's performance largely lies in the confidence intervals of competing methods. I also miss some scientific curiosity about the learned prompt representations; there is much more potential in this work than reducing it to performance metrics."
            },
            "questions": {
                "value": "* Can you please clarify what the stopping criterion is when pre-training the backbone initially? You say the domain-specific prompts are learnable parameters, can you specify how they are connected to the backbone/output? You write they are concatenated, does this mean in the initial pre-training, we need to know the size of the prompt and mask the input accordingly? Or is there a linear layer whose parameters are learned? You might want to formalize all this by introducing a second set of parameters (as $\\theta$ is always frozen). Furthermore, is a new temporal prompt generator trained for each temporal prompt, or is it reused? In Figure 1, you \"freeze\" $P_{T2}$ but not $P_{T3}$ in the next step, why? In Fig. 1's caption, you say, \"[...] finally, [...] $P_G$ is trained\". This implies a sequential training but from the figure, it seems like $P_G$ is also the output of the temporal prompt generator. \n* Can you provide experimental results on data sets that naturally come from different domains? \n    * If this is not the case, is it possible to use your method to **determine** the existence of different domains? I am thinking of comparing the learned domain-specific prompts, for example, in terms of their cosine similarity. \n* Your algorithm omits many necessary details and does not add information that can't be inferred from the text. I would propose to either make the algorithm more informative (i.e., clarify the questions from above) or, to save space, remove it and clarify the points by extending Section 3. \n* How do gradient-boosted trees perform on the regression tasks? \n* How do standard forecasting methods such as ARIMA/Gaussian Processes perform on the data sets? \n* Why are no standard deviations in Tables 3, 4, and 5 reported? \n* Given its origin in NLP, I am not sure if \"prompt\" is the best fitting wording in the context of time series.\nIf all points can be addressed satisfactorily (particularly, the investigation into the representation of the learned prompts), I may consider raising my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2020/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2020/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2020/Reviewer_HfUr"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2020/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773347281,
        "cdate": 1698773347281,
        "tmdate": 1700638820000,
        "mdate": 1700638820000,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RlKdZsK3GO",
        "forum": "U3ROVRTKTa",
        "replyto": "U3ROVRTKTa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2020/Reviewer_Bw2Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2020/Reviewer_Bw2Q"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new method for temporal domain generalization using prompts on transformer-based networks. This method is efficient and does not need data from future time periods during training. It uses global, domain-specific, and drift-aware prompts to adapt to data changes over time. The paper claims that the proposed method is adaptive on various tasks, such as classification, regression, and forecasting, The effectiveness of the framework is demonstrated through extensive experiments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper discusses a vital question on temporal domain generalization by leveraging soft prompts with transformer-based networks. \n- The idea and motivation of this paper are easy to read. The idea and method proposed in this paper are clearly illustrated and introduced, making the reader easily understand."
            },
            "weaknesses": {
                "value": "- Overclaim 1. The second contribution of this work is \"parameter-efficient and time-efficient\". But their proposed method requires to train a transformer (Temporal Prompt Generator in Figure 1), which includes way more trainable parameters than existing methods such as DRAIN.\n- Overclaim 2. As for the time-efficient aspect, there is no training time comparison analysis to demonstrate the claimed \"time-efficiency.\" Especially, either pre-training or fine-tuning a transformer-based model to adapt to the specific task (temporal soft-prompt generation) are inefficient.\n- Unfounded. The authors claim that \"Only a few methods studied temporal DG problem Nasery et al. (2021); Bai et al. (2023), which are inefficient and complex to be applied to large datasets and large models,\" which is unfounded, no evidence supported, and without any quantitative analysis for demonstrating this assumption.\n- The performance improvement is minor and not significant, especially since the proposed method achieves inferior performance than DRAIN (state-of-the-art of TDG) on the 2-moon dataset, a basic synthetic dataset on testing TDG. The performance of the proposed method is not convincing.\n- The proposed framework seems to be adaptive on multiple modalities of transformer-based networks as the model backbone. However, the paper only evaluates their framework on one transformer-based network. The authors are highly encouraged to test their framework incorporated with multiple transformer-based networks."
            },
            "questions": {
                "value": "- ONP has been proven to obtain no domain shifting [1], which means most of the TDG-based methods are useless in ONP. However, the proposed methods, in contrast, obtain good performance on ONP. Is there any specific reason that can explain this phenomenon?\n\n[1] Nasery et. al \"Training for the Future: A Simple Gradient Interpolation Loss to Generalize Along Time\n\" NeurIPS 2021"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2020/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2020/Reviewer_Bw2Q",
                    "ICLR.cc/2024/Conference/Submission2020/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2020/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828991648,
        "cdate": 1698828991648,
        "tmdate": 1700637618502,
        "mdate": 1700637618502,
        "license": "CC BY 4.0",
        "version": 2
    }
]