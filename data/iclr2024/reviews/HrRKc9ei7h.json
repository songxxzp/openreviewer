[
    {
        "id": "H5wOJj8TG4",
        "forum": "HrRKc9ei7h",
        "replyto": "HrRKc9ei7h",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3801/Reviewer_1CJx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3801/Reviewer_1CJx"
        ],
        "content": {
            "summary": {
                "value": "This work studies an online learning problem in which at each time step a context is obtained that indicates the point belongs to some groups (possibly more than one at the same time). At the end of the game we want to have low regret on all subsequences of points belonging to the same group. The point of this work is that a solution can be build in cases in which the model class is large. A previous algorithm existed that would only handle small model classes. An empirical evaluation is provided."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The strength of this paper is obtaining what is claimed in theorem 1 about algorithm 1. Another one is the empirical evidence provided, that checks that the algorithm indeed behaves how it was supposed to do."
            },
            "weaknesses": {
                "value": "The main weakness of this work is its marginal contribution. Beyond their empirical evidence their contribution is limited to the proof of theorem 2, which is a small step on top of the existing framework of Blum and Lykouris, almost a remark. Still, it could be a publication, I'm giving a weak accept because of this."
            },
            "questions": {
                "value": "It is not obvious that you should have an \"always active\" subsequence in order to perform well empirically. It would be good if it is reported that the algorithm does not work so well if this is not present, so others people that want to implement this can take this fact into account.\n\nTheorem 1 and Theorem 3 say essentially the same. It's redundant.\n\nMinor\n\nIn the abstract there is \"\\{1, 2, \\cdots T\\}\" comma missing after \\cdots.\n\n\"low order regret terms\" -> \"low-order regret terms\" \n\nThm 1 (informal): contains \"of of\"\n\n\"to each Individual\" remove capitalization."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3801/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698397861656,
        "cdate": 1698397861656,
        "tmdate": 1699636337510,
        "mdate": 1699636337510,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yr0aMBsyZ8",
        "forum": "HrRKc9ei7h",
        "replyto": "HrRKc9ei7h",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3801/Reviewer_eJaJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3801/Reviewer_eJaJ"
        ],
        "content": {
            "summary": {
                "value": "The paper studies efficient and practical algorithms for group-wise regret-minimization online learning. The problem is a variate of the classical online problem, in which a hypothesis class $H$ is fixed, and a sequence of individuals/features ${x_{1}, \\cdots, x_{T}}$ and their labels ${y_1, \\cdots, y_T}$ are chosen by an adversary. The algorithm has to make predictions at each time step $t \\in [T]$. The classical notion of measuring a ''good\u2019\u2019 algorithm is *regret*, which is defined as the cumulative gap between the cost induced by the online learning algorithm and the cost induced by the best fixed model in the hypothesis class. The group-wise regret minimization problem further requires binding the regret for each group, which, mathematically, can be viewed as a collection of subsequences indexed by the ``mapping\u2019\u2019 of different groups. \n\nThe work of BL [ITCS\u201920] has shown that it is possible to achieve small regret w.r.t. the best model $f_{g}$ for every fixed group $g$ by reducing the problem to the sleeping expert problem. This implies for a large family of online learning problems, there exist algorithms with group-wise $1+o(1)$-multiplicative regret that run in time polynomial of $|G|$ and $|H|$. However, while the size of $G$ is usually small, even some ``elementary\u2019\u2019 models, e.g., linear models, have quite large sizes of $H$, which prevents the algorithm of BL [ITCS\u201920] from being practical. \n\nThe main contribution of the paper is to improve the runtime of the group-wise regret minimization algorithm of BL [ITCS\u201920], and, in particular, remove the dependence on $|H|$ for the run time. To this end, the paper observes that we can actually solve each policy sub-sequence by external regret algorithms, which require far less time to compute, and treat each ``expert\u2019\u2019 in the overall algorithm as the output of the external regret algorithms. In doing so, the algorithm avoids enumerating over the hypothesis class $|H|$, and only scales w.r.t. $|G|$. The paper then provides some implementations on both synthetic and real-world datasets, and the experimental results of their algorithms are strong."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Overall, I like this paper as it provides the practical algorithm for a problem where the existence of the solution (or even, a theoretically-efficient algorithm) has been known, but no practically-efficient algorithm was proposed. This serves as a nice ``bridge\u2019\u2019 between theory and practice. As the paper itself has mentioned, practical group-wise regret-minimization algorithms have various downstream applications, including algorithmic fairness. \n\nI also read through the analysis in Appendix B, and I think they are correct, barring some presentation issues (mentioned in the ``weakness\u2019\u2019 section). Although the technical idea is simple and somehow straightforward to come up with, I do think the result itself is neat and cute. Finally, the experimental results of the paper are quite strong, although the comparison is somehow tailored to your algorithm since the benchmark as they are not designed for group-wise regret minimization."
            },
            "weaknesses": {
                "value": "One criticism I have when reading the paper is that the paper is not presented in a fully self-contained and rigorous manner. For instance, the proposed algorithm uses AdaNormalHedge as a black box, but the guarantee of such an algorithm is never formally described. (I am aware of the description in Appendix B, but there is no proposition + proper citation for this.) Similarly, when citing external regret minimization algorithms for applications, the formal quantifiers and guarantees for those algorithms are not provided.\n\nSimilarly, the introduction is written in a very informal way. I understand this might be a result for the authors to accommodate the broader readership of the conference; however, I think it actually adds to confusion. For instance, when defining the notion of diminishing group-wise regret, it would be much more helpful to include the actual mathematical definition of ''squared error\u2019\u2019 and ''best model in H on that sequence\u2019\u2019. (Also, why the notion is limited to squared error but not general loss functions?) \n\nThe same applies to the statement of Theorem 1, in which the notion of ``computationally efficient\u2019\u2019 is not defined(!) The phrase ''best model on hindsight\u2019\u2019 is used in a very informal way \u2013 I think you should properly define this notion (overall vs. on group-wise sequences) with the proper quantifiers. \n\nA note for presentation problems in Appendix B: the usage of expectation notation $\\mathbb{E}[]$ is rather confusing in this section. Your derivation crucially relies on the control of which coins the expectation is taken upon. I think in this case, the expectation notation should have subscript explicitly stating the source of randomness. Furthermore, the way you talk about $p_{t}^{I} $ vs $z_{t}^{I}$ is not rigorous enough. If I understand it correctly, $p_{t}^{I}$ is a random variable whose supports are some realizations denoted as $z_{t}^{I}$. In light of this, should the term in the first inequality be $\\mathbb{E}[\\sum_{t} I(t) \\ell(p_{t}^{I}, y_{t})]$? Overall, I do think this section has quite some room for improvement."
            },
            "questions": {
                "value": "Is your notion of computationally efficient in Theorem 1 defined as polynomial time in $T$, $d$, and $n$ (or some other input-related size)? If I understand correctly, what you want to say is that it is reasonable to assume $|G|$ is of polynomial sizes of the input, but $|H|$ is usually quite large. Therefore, your algorithm that does not scale with $|H|$ implies poly-time efficiency.\n\nI don\u2019t quite understand the term ``diminishing/vanishing regret\u2019\u2019 \u2013 in your Theorem 2, the term $\\sqrt{T_{T} \\log(|G|)}$ is not $o(1)$ itself. Are you implicitly enforcing a lower bound on the $\\alpha_{I}$? \n\nA MISC comments: The discussion on the technical front by comparing your work with BL [ITCS\u201920] looks nice. I think you can expand this discussion to give more details, and present it earlier in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I do not have any ethics concerns for this work."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3801/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3801/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3801/Reviewer_eJaJ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3801/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637584541,
        "cdate": 1698637584541,
        "tmdate": 1699636337435,
        "mdate": 1699636337435,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ftw2WRxVT4",
        "forum": "HrRKc9ei7h",
        "replyto": "HrRKc9ei7h",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3801/Reviewer_vWf7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3801/Reviewer_vWf7"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of online prediction, where at each time step, a new example arrives, and the learner has to make a prediction. The goal is to minimize regret not just overall, but also simultaneously for different subgroups defined based on features. A previous algorithm for this problem (Blum & Lykouris, 2019) provides regret guarantees but is computationally inefficient when the hypothesis class is large. The work proposes a modification to the algorithm of Blum & Lykouris (2019) that reduces the problem to the problem of external regret minimization. The new algorithm uses a significantly smaller number of experts for making the decision at the time step. The algorithm is applied to problems like online linear regression, classification with small separator sets, and linear optimization. Experiments on synthetic and real datasets show substantially lower error and regret compared to standard online learning algorithms."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "While building on prior work by Blum & Lykouris (2019), the paper introduces a simple yet meaningful modification that enhances computational efficiency. This facilitates the use of large hypothesis classes, such as linear models. The reduction to standard external regret minimization, while expected, remains theoretically novel.\n\nThe algorithm's design and analysis are technically sound, and the paper offers a comprehensive set of experiments.\n\nThe paper is well-written and easy to follow, with the problem being well-motivated.\n\nAchieving regret guarantees across groups is pivotal. This paper renders it feasible for large model classes, broadening the applicability of these methods.\n\nIn summary, this paper presents a theoretically grounded, significant contribution, backed by robust experimental validation."
            },
            "weaknesses": {
                "value": "I believe that including an experimental comparison with the Blum & Lykouris (2019) approach would better justify the superiority of the new method. Is it possible to conduct such a comparison using a smaller model class?"
            },
            "questions": {
                "value": "See questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3801/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3801/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3801/Reviewer_vWf7"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3801/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698691063048,
        "cdate": 1698691063048,
        "tmdate": 1699636337339,
        "mdate": 1699636337339,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3Yq5dtDhFH",
        "forum": "HrRKc9ei7h",
        "replyto": "HrRKc9ei7h",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3801/Reviewer_6fRD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3801/Reviewer_6fRD"
        ],
        "content": {
            "summary": {
                "value": "The paper considers the problem of minimizing the group regret, where the goal is to minimize the regret with respect to each subsequence of trials in the pre-defined group simultaneously. Based on AdaNormalHedge, the authors propose an algorithm for the problem. The experimental results are also shown."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The topic is relevant to the machine learning community as it reflects the multi-objective nature of online prediction problems. The experimental results show the proposed method works well in practice. The experimental results show the superiority of the proposed method against baselines."
            },
            "weaknesses": {
                "value": "I am afraid that the proof of the main theorem (Theorem 2) might be wrong, or at least incomplete. Simply put, the algorithm aggregates sleeping experts where each sleeping expert is awake only when the trial belongs to a designated subsequence of trials. Then, the theorem trivially holds when each subsequence of trials is disjoint to each other, as mentioned in the paper. On the other hand, if subsequences intersect, it is not fully clear if the proof is correct."
            },
            "questions": {
                "value": "It would be nice if you could comment on my concerns about the proof of Theorem 2."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3801/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816187055,
        "cdate": 1698816187055,
        "tmdate": 1699636337278,
        "mdate": 1699636337278,
        "license": "CC BY 4.0",
        "version": 2
    }
]