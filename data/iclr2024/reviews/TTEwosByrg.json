[
    {
        "id": "fA0ST08qwM",
        "forum": "TTEwosByrg",
        "replyto": "TTEwosByrg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8501/Reviewer_K15E"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8501/Reviewer_K15E"
        ],
        "content": {
            "summary": {
                "value": "The paper conduct cross-check study of multiple open and close sourced LLMs against each other to evaluate the cognitive biases inherent in these models, when used as a comparison evaluator for certain tasks.\nThe paper evaluates on 6 different dimensions: Order, Compassion, Egocentric, Salience, Bandwagon and Attentional. For example, egocentric bias indicates the model prefers itself over a competitor.\nResults according to their proposed benchmarks show all models, include large closed source ones, are biased on a few different dimensions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The evaluations of LLMs are comprehensive and contain many aspects, dimensions, as well as correlation analysis with human.\n- Presentation of the paper is clear and understandable."
            },
            "weaknesses": {
                "value": "- The indicators of the benchmark is overly complicated, contain many numbers, dimensions, which is very difficult to understand. This make understanding and judgements of models based on this benchmark non-trivial. The proposed benchmark should be aggregated in a way that is simpler to grasp without losing too much information.\n- More analysis with human's own biases on these dimensions are needed.\n- The details are quite unclear. The use of 50 examples is insufficient to evaluate 15 LLMs"
            },
            "questions": {
                "value": "- How these evaluations help in the improvements of LLMs?\n* the average RBO amongst human annotators (0.478) is actually lower than the average RBO between human and models (0.496), so why the conclusion that model evaluations doesn't align with humans if they seem to be better aligned than human? \n* I see that random is calculated for different biases, however, for better models like ChatGPT, the egocentric bias may be unfair because the generation of ChatGPT is indeed better, or the salience biases, maybe indeed the longer generations have higher quality. Does the authors try to decouple these confounders?\n* for the RBO values, does the author adapt equation (1) to equation (2)? may I know more details on why equation (1) cannot be used, and if using equation (2), could the authors provide more insights on what each ranges of values mean? for instance, we know for cohen's kappa, 0.61-0.80 indicates substantial agreement, and 0.81-1 indicates almost perfect agreement."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8501/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698490985296,
        "cdate": 1698490985296,
        "tmdate": 1699637062028,
        "mdate": 1699637062028,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "20DE9zU1PR",
        "forum": "TTEwosByrg",
        "replyto": "TTEwosByrg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8501/Reviewer_1K6b"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8501/Reviewer_1K6b"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the cognitive biases in LLM-based evaluation on 15 different LLMs. It introduces a cognitive bias benchmark that covers both implicit (order, compassion, egocentric, salience) and induced biases (bandwagon, attentional) and explores the LLMs' performance as well as human bias on these aspects. It also examines the correlations between human and machine preferences."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* Benchmarking of cognitive biases in LLM assessment is a very important topic because recent studies have extensively used LLM for judgment and were not aware of the limitations of their capabilities. This paper provides a comprehensive investigation of 6 cognitive biases, including the new benchmark and detailed analysis of 15 popular LLMs.\n* It provides several interesting insights into cognitive biases in LLMs with different scales. For example, larger models prefer the long response more than the small models, and small models favor the last-ordered systems while the large ones favor the first one. It also draws attention to the vulnerability of LLMs on the attack of bandwagon and attentional.\n* This paper also conducts human evaluation. It investigates the correlation between LLMs and human, and discuss the potential"
            },
            "weaknesses": {
                "value": "* The main weakness is that the number of instructions is small: only 50 question-answering instances. This affects the reliability of conclusions since the data points are limited. It will be better to conduct significant tests and report the p-value for results.\n* The experiments do not consider ties in the pairwise evaluation, which may affect the conclusion. For example, if the responses of two systems are very similar, it is fine to choose any of them.\n* For human evaluation, the average RBO among AMT workers is only 0.478, which means that there are diverse preferences between humans. Therefore, the average RBO 0.496 between human and model preferences does not necessarily indicates the misalign with human because even human cannot achieve high alignment."
            },
            "questions": {
                "value": "* Examples in Table 1 are difficult to understand. For example, in compassion fade, which model is given and affected by the recognizable names?\n* What does the bias score in Figure 3 mean? Is the higher the better or the lower the better?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8501/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8501/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8501/Reviewer_1K6b"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8501/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698717612420,
        "cdate": 1698717612420,
        "tmdate": 1699637061891,
        "mdate": 1699637061891,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RSUavbq0UA",
        "forum": "TTEwosByrg",
        "replyto": "TTEwosByrg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8501/Reviewer_mWup"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8501/Reviewer_mWup"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators to measure cognitive biases in LLM evaluation outputs. To this end, the authors introduce a cognitive bias benchmark for LLMs as evaluators (COBBLER) and find that LLMs are biased text quality evaluators and misalign with human evaluators."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors' contribution in proposing a cognitive bias benchmark for evaluating the quality and reliability of Language Model Evaluators (LLMs) is highly valuable for the research community.\n\n2. The study effectively analyzes six different biases, presenting interesting findings. Specifically, the observation that most of the models strongly exhibit several biases, coupled with the low agreement between machine and human preferences, sheds light on the differences between automated and human evaluations."
            },
            "weaknesses": {
                "value": "1. In this work, the authors primarily focus on pairwise evaluation based on the coherence criterion, without considering other evaluation formats, such as single-document evaluation and interactive evaluation. \n\n2. As recommended in prior research (Wu & Aji, 2023), it is important to evaluate machine-generated text from various perspectives rather than depending solely on a single unified measure. It would be better to explore more diverse evaluation settings to ensure a comprehensive assessment of LLM-based evaluators.\n\n3. While the authors provide an overview of cognitive bias existing in different models in Section 5.1, it would be valuable to explore the potential contributions of current generation techniques, like self-consistency, in reducing bias. Including insights on these techniques can enhance the discussion and provide a more holistic understanding of bias mitigation approaches."
            },
            "questions": {
                "value": "The example provided in Table 1 does not align with the definition of compassion fade bias?\n\nThere seems to be confusion regarding the number of models that human evaluators and model-based evaluators are required to rank in Section 5.2. Clarifying whether it is 15 or 4 models is necessary. Additionally, the statement about human consensus being modest but reasonable while model evaluations not aligning closely with human preferences is contradictory, as the average RBO values are very similar. Further clarification is needed to reconcile this discrepancy.\n\nIt is unclear what distinguishes Table 2 from Figure 4 and whether there are any significant differences in the findings presented in these two representations. Providing clarification on the key disparities and highlighting any noteworthy insights derived from each depiction would enhance the reader's understanding."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The experiments involve human evaluations and admit the annotation bias."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8501/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744184366,
        "cdate": 1698744184366,
        "tmdate": 1699637061777,
        "mdate": 1699637061777,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NrvgVNarMi",
        "forum": "TTEwosByrg",
        "replyto": "TTEwosByrg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8501/Reviewer_tfnE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8501/Reviewer_tfnE"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes cognitive biases of large language models (LLMs) that are used as an evaluator. The authors use 15 models including GPT-4, LLaMA, Alpaca, and Koala, to generate their responses on 50 question-answering instructions. Then the models evaluate the pairwise preference of the responses, which is sent to their meta-evaluation benchmark for cognitive biases. The authors propose six types of biases, such as order bias, egocentric bias, and bandwagon effect. The presence of bias is defined when, depending on the specific feature and modifications of the prompt (e.g., the order of paired responses), the preference of a model shows a significant skew compared to random selection. The authors also compare the models\u2019 preferences against human labels collected by crowdworkers. Through their analysis, the authors claim that most of the examined LLMs exhibit cognitive biases and are not reliable evaluators."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- I appreciate the proposed taxonomy of cognitive biases observable in the behavior of LLM evaluators. A comprehensive measurement of these biases is crucial for fair and reliable LLM evaluation.\n- The paper is well-organized and well-written. The figures offer effective visualizations of the experiment pipeline and results. The literature review adequately covers recent relevant works on (meta-)evaluating LLMs."
            },
            "weaknesses": {
                "value": "- While one of the potential contributions of this paper is its comprehensive analysis of multiple cognitive biases, I believe that most of the biases discussed have been previously identified (see Sections 3.1 and 3.2). The introduction of compassion fade, egocentric bias, and bandwagon effect may bring novelty, yet I have a few reservations:\n   * Regarding compassion fade, the models might be unfamiliar with the names of other models, as their training corpus likely doesn't include information on recent LLMs. The observed effect might closely resemble that of injecting random names.\n    * Concerning the bandwagon effect, the authors appear to rely on a single sentence: \"85% of people believe that {system} is better,\" without exploring variations in the percentage. It would be insightful to investigate what occurs when statements like \"0% of people believe that {system} is better\" are used. Observing a correlation between the biased tendency and the percentage stated in the injected sentence could provide deeper insights.\n- Some models display low rates of valid responses in the pairwise preference task. Specifically, seven out of the fifteen examined models (LLaMA, DollyV2, Koala, etc.) yield less than 80% valid responses, significantly lowering the cognitive bias scores. This issue hampers precise benchmarking performance estimation and poses a risk of underestimating the cognitive biases in models producing many invalid outputs. Consequently, the claim that \"most LLMs exhibit cognitive biases\" is not sufficiently justified, given that weaker models may exhibit biases more frequently.\n- Despite the careful recruitment of crowdworkers through pilot tasks and training sessions, the inter-annotator agreement (IAA) among the workers is not notably high in both ranking and pairwise tasks. The IAA aligns with the agreement between humans and machines. While the authors' claim that \"LLMs are still not suitable as fair and reliable automatic evaluators\" is not incorrect, it appears that humans may also fall short in this regard.\n\nOverall, I think the experiments yield results that do not provide enough empirical support for the authors' claims."
            },
            "questions": {
                "value": "- I find it challenging to grasp the assumptions the authors make regarding biases in evaluation. The introduction section alludes to \"unbiased\" evaluation, but it remains unclear whether achieving such an evaluation is realistic, considering that even humans may not be capable of conducting entirely impartial assessments. If the authors' objective is to develop LLMs that are free from human-like biases, I question the necessity of drawing comparisons against human evaluations.\n- This paper introduces a new benchmark, but when evaluating a new model, is it necessary to execute the entire process in this study, including generating a large number of pairwise preferences?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8501/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8501/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8501/Reviewer_tfnE"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8501/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820589037,
        "cdate": 1698820589037,
        "tmdate": 1699673226040,
        "mdate": 1699673226040,
        "license": "CC BY 4.0",
        "version": 2
    }
]