[
    {
        "id": "gpvE05MZuZ",
        "forum": "XheqLWvswO",
        "replyto": "XheqLWvswO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission712/Reviewer_G5jt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission712/Reviewer_G5jt"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the rooted logistic loss function for supervised classification and unsupervised generation tasks with nice theoretical properties. Moreover, this paper shows the comparison results of their rooted loss, cross-entropy loss and focal loss being applied to various datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper proposes a new rooted loss objective.\n2. This paper is sound.\n3. This paper provides a lot of experiments on different datasets of supervised and unsupervised learning."
            },
            "weaknesses": {
                "value": "1. The contribution of this paper is limited.\nThe new objective loss function is based on the approximation of the natural logarithm function. If using the proposed loss objective, we introduce a new tuning parameter $k$. It may take a lot of time to tune this new parameter, but the improvement of the performance is not significant enough, as shown in Table 3.\n2. Some parts of the paper are not explained clearly. \nFor example, this paper mentions that the reason of generalization bounds for logistic regression still holding for rooted logistic objectives is when $k\\rightarrow +\\infty\\cdots$. However, from Lemma 2, a small $k$ is needed for fast convergence. Is there any contradiction here?\n3. The presentation of the paper can be improved. For example, in Table 1, the results can be presented with $k$ ordered for each dataset (from small to large or reversely). There are typos in formulas (5), (6) and (7)."
            },
            "questions": {
                "value": "1. In Table 1, I observe that for RLO, we can get better test accuracy with smaller $k$. But for ROL-L2, we get better test accuracy with larger $k$. Is there any insights about the reason for that? And how did the authors choose $k$ for each dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission712/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission712/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission712/Reviewer_G5jt"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission712/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698700189079,
        "cdate": 1698700189079,
        "tmdate": 1699635998567,
        "mdate": 1699635998567,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ihbIKWVJAy",
        "forum": "XheqLWvswO",
        "replyto": "XheqLWvswO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission712/Reviewer_MpCx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission712/Reviewer_MpCx"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel loss function termed the Rooted Logistic Objective function (RLO) which builds upon the approximation of the logarithm function. Furthermore, the research establishes that under specific hyperparameter constraints, the proposed method exhibits strict convexity. The paper includes experiments from various domains and tasks to demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is overall well-organized.\n2. The proposed method is interesting and novel to the best of the reviewer's knowledge."
            },
            "weaknesses": {
                "value": "1. The experiments conducted in the paper are based on datasets that are too simple and the corresponding baseline test accuracy is not reasonable (<90% test accuracy for Cifar-10). The reviewer would appreciate if the results of more realistic datasets could be included.\n2. The reviewer didn't check the full derivation in the Appendix, but the derivation shown in the paper seems a bit sloppy (see Questions), hence hindering the soundness of the paper a bit."
            },
            "questions": {
                "value": "1. In Section 3.1, the authors wrote \"Moreover, when we consider the gradient...\", should the word \"moreover\" be rephrased to other words like \"however\" or \"nevertheless\"? \n2. From (5) to (6), why is the term $\\frac{1}{k}$ dropped?\n3. In (6), should the right-hand side be $\\ell_i^k(w) \\cdot \\frac{1}{exp(yw^Tx) + 1} \\cdot (-yx)$? \n4. In the discussion after Lemma 2, the authors wrote \"It is beneficial when using stochastic algorithms that use a random mini-batch of samples at each iteration instead of the full dataset to compute gradient\", why is this the case for RLO?\n5. Why does RLO show faster training time compared with CE? The reviewer can understand RLO takes less iteration to convergence, but why is the training time shortened with a fixed number of iterations?\n6. In Table 3, the test accuracy for CIFAR-10 on ResNets is noted to be below 90%. However, based on the reviewer's knowledge,  the test accuracy should be at least 93% with appropriate training setups.\n7. On page 9, the authors introduce an additional parameter $m$ to be the multiplier for (4), is $m$ a scalar multiplied by the loss? Could the authors clarify more?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission712/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission712/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission712/Reviewer_MpCx"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission712/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719014272,
        "cdate": 1698719014272,
        "tmdate": 1699635998462,
        "mdate": 1699635998462,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tYDQ5OMtia",
        "forum": "XheqLWvswO",
        "replyto": "XheqLWvswO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission712/Reviewer_vuQU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission712/Reviewer_vuQU"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a new loss function named the Rooted Logistic Objective function in the logistic regression problem. Numerical results show that minimizing RLO achieves faster convergence rate and improved generalization than traditional loss function, e.g., cross-entropy (CE)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The presentation of this paper is clear."
            },
            "weaknesses": {
                "value": "1. RLO lacks the mathematical derivation by replacing the log with $1/k$ in the cross-entropy loss. In fact, the cross-entropy is to minimize the -log P, where $P$ is the likelihood. As the data are i.i.d, the likelihood of all the data can be written as the multiplication of the likelihood of each sample, e.g., $$\\log P(y_1,y_2,...,y_n|x_1,x_2,...,x_n)= \\log \\prod P(y_1|x_1)...P(y_n|x_n) = \\sum_{i=1}^n P(y_i|x_i).$$\nIn this paper, it replaces log with $(\\cdot)^{1/k}$ and still sums them together. What is the intuition behind this? I can understand it is to maximize the summation of the likelihood of each sample, but it does not make any sense.\n\n2. This paper focuses on non-linear models, specifically neural networks. However, the theoretical analysis in Section 3.2 is based on linear models. When considering non-linear models, both RLO and CE become non-convex functions with respect to the parameter $w$. In non-convex optimization, our primary concern is determining which objective function is easier to find the global optimum.\n\n3. I did not see significant improvement via using RLO from Figure 4 and table 3."
            },
            "questions": {
                "value": "Why is the test error for Cifar-10 in Table 3 so high (compared with SOTA methods)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission712/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission712/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission712/Reviewer_vuQU"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission712/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765820536,
        "cdate": 1698765820536,
        "tmdate": 1699635998385,
        "mdate": 1699635998385,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uJhejW3wSY",
        "forum": "XheqLWvswO",
        "replyto": "XheqLWvswO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission712/Reviewer_QRfD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission712/Reviewer_QRfD"
        ],
        "content": {
            "summary": {
                "value": "Aiming to better condition the logistic loss function, this work proposed Rooted Logistic Objective (RLO), a very simple polynomial approximation to the log function with an additional power parameter k to control the approximation. \nSuch a loss can serve as a plug-in replacement for log-based loss functions for supervised classification and unsupervised generation tasks. \n\nTheoretically, the RLO objective is shown to be strictly convex whenever k > 1, and guaranteed to be as conditioned as the Logistic objective function in terms of the Hessian term if k is not too large.\n\nEmpirically, the authors conducted a series of experiments on classification and StyleGAN training, involving both fully connected networks and transformers. The modified loss is shown to converge faster and can achieve better performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The RLO objective seems novel to me. \nAdvertised to be a better alternative to logistic loss, the proposed RLO can potentially be widely applied to various supervised tasks. \nIn this work, the authors not only evaluated standard image classification but also extended to training the discriminator in GAN models. I like the inclusion of a toy data case to provide more visual clues."
            },
            "weaknesses": {
                "value": "## Weak theoretical analysis\n\nFirst, the scope of this work is on substituting the logistic loss, or more precisely, approximating the log function with polynomials. However, in classification, the logistic loss is only one of the many surrogate losses for the more fundamental 0-1 loss.  The authors did not discuss any other surrogate loss functions and how they relate to the 0-1 loss. [1] is a related work.\n\nSecond, the theoretical statements are hand-wavy. For instance, the \"better conditioning\" concept is not clear in this work. \nThe authors wrote that \"From lemma 1 and 2, we can conclude that there is a range of values of k that provides better conditioning for individual data points.\" I don't think this statement is well supported. The relationship between the $h(,)$ function and how conditioned is the objective function is not clear. The reason why a larger $h$ indicates that \"the gradient directions may provide sufficient descent needed for fast convergence\" should be made more rigorous. \n\nAlso, Lemma 2 only concerns the optimal solution $w^*$, but not the optimization trajectory. So I think the theoretical results are relatively weak. \n\n\n\n## Insufficient experiments. \nThe empirical evaluation in this work is the biggest concern for me. \n\nFirst, the experiments are not consistent enough. For example, in Table 1, the effect of the L2 regularization is not consistent. Ideally, if the weight decay strength is appropriate, shouldn't it always be better than that without weight decay? To be more specific, in Ionoshphere, L2 improves acc for LR but hurts acc for RLO (k=3). These inconsistencies cast doubt on the validity of the results. I wonder whether the hyperparameters are well-tuned. \n\nSecond, for the real-data experiments, the reported results are far from state-of-the-art. Take the CIFAR-10 classification for example, the reported acc for ResNet is only 87.67, which is significantly lower than other reported results. The red curve for CE in Figure 3(b) seems obviously not well-tuned. \nThe same can be said in the StyleGAN experiments, the FIDs are so high that it may not make much sense. \n\nI strongly recommend the authors to first reproduce some SOTA results for image classification and GANs, and then try to improve upon them to make a more convincing case. \n\n\n\n## Misc.\nMany typos. For example $L_{RLO}(w)$ after lemma 1 should be $L_{LO}(w)$. \n\n\"$x$ is a smooth function, $f(z) \\approx x$ are not appropriate. $x$ is a random variable that comes from a distribution. \n\n\n**Overall**, I think this work can be significantly improved if either \n\n* (1) Conduct a more thorough empirical evaluation and showcase that RLO can actually improve SOTA methods. [2] is an example. \n\n* (2) Make a more significant theoretical contribution by providing improved convergence bounds for the proposed RLO. [3] is an example. \n\n\nReference:\n\n[1] Bartlett, Peter L., Michael I. Jordan, and Jon D. McAuliffe. \"Convexity, classification, and risk bounds.\" Journal of the American Statistical Association 101.473 (2006): 138-156.\n\n[2] Hui, Like, and Mikhail Belkin. \"Evaluation of neural architectures trained with square loss vs cross-entropy in classification tasks.\" ICLR 2021.\n\n[3] Hu, Tianyang, et al. \"Understanding square loss in training overparametrized neural network classifiers.\" Advances in Neural Information Processing Systems 35 (2022): 16495-16508."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission712/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698938042202,
        "cdate": 1698938042202,
        "tmdate": 1699635998328,
        "mdate": 1699635998328,
        "license": "CC BY 4.0",
        "version": 2
    }
]